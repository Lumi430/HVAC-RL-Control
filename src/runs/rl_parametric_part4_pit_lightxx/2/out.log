Using TensorFlow backend.
[2019-04-03 21:51:49,373] A3C_AGENT_MAIN INFO:Namespace(action_repeat_n=1, action_space='part4_v1', activation='linear', agent_num=5, check_args_only=False, clip_norm=5.0, debug_log_prob=0.0005, decay_steps=1000000, dropout_prob=0.0, end_e=0.0, env='Part4-Light-Pit-Train-v1', eval_act_func='part4_v1', eval_env_res_max_keep=50, eval_epi_num=1, eval_freq=100000, forecast_dim=0, gamma=0.99, h_decay_bounds=[], h_regu_frac=[0.0], init_e=0.0, isNoisyNet=True, isNoisyNetEval_rmNoise=True, is_greedy_policy=False, is_learning_rate_decay_staircase=False, is_warm_start=False, job_mode='Train', learning_rate=0.0005, learning_rate_decay_rate=1.0, learning_rate_decay_steps=100000, max_interactions=5000000, metric_func='part4_v1', model_dir='None', model_param=[12, 1], model_type='nn', num_threads=16, output='./Part4-Light-Pit-Train-v1-res1', p_loss_frac=1.0, raw_state_prcs_func='cslDx_1', reward_func='part4_v1', rmsprop_decay=0.99, rmsprop_epsil=1e-10, rmsprop_momet=0.0, rwd_e_para=1.0, rwd_p_para=1.0, save_freq=500000, save_max_to_keep=5, save_scope='all', sharedNet_type='Dense', state_dim=10, test_env=['Part4-Light-Pit-Test-v1', 'Part4-Light-Pit-Test-v2'], test_mode='Multiple', train_act_func='part4_v1', train_freq=5, v_loss_frac=0.5, violation_penalty_scl=10.0, weight_initer='glorot_uniform', window_len=20)
[2019-04-03 21:51:49,373] A3C_AGENT_MAIN INFO:Start compiling...
2019-04-03 21:51:49.405208: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
[2019-04-03 21:52:05,120] A3C_AGENT_MAIN INFO:Start the learning...
[2019-04-03 21:52:05,120] A3C_AGENT_MAIN INFO:Prepare the evaluation environments ['Part4-Light-Pit-Train-v1', 'Part4-Light-Pit-Test-v1', 'Part4-Light-Pit-Test-v2'] ...
[2019-04-03 21:52:05,143] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation worker starts!
[2019-04-03 21:52:05,168] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation worker starts!
[2019-04-03 21:52:05,191] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation worker starts!
[2019-04-03 21:52:05,192] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:05,192] A3C_AGENT_WORKER-Thread-2 INFO:Local worker starts!
[2019-04-03 21:52:05,264] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:05,265] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run1
[2019-04-03 21:52:06,193] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:06,195] A3C_AGENT_WORKER-Thread-3 INFO:Local worker starts!
[2019-04-03 21:52:06,272] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:06,273] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run1
[2019-04-03 21:52:07,196] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:07,197] A3C_AGENT_WORKER-Thread-4 INFO:Local worker starts!
[2019-04-03 21:52:07,274] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:07,275] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run1
[2019-04-03 21:52:08,198] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:08,199] A3C_AGENT_WORKER-Thread-5 INFO:Local worker starts!
[2019-04-03 21:52:08,292] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:08,293] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run1
[2019-04-03 21:52:09,199] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:09,200] A3C_AGENT_WORKER-Thread-6 INFO:Local worker starts!
[2019-04-03 21:52:09,307] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:09,309] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run1
[2019-04-03 21:52:09,362] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-03 21:52:09,362] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-03 21:52:09,363] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-03 21:52:09,363] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:09,363] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-03 21:52:09,363] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:09,364] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:09,367] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run1
[2019-04-03 21:52:09,376] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run1
[2019-04-03 21:52:09,377] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run1
[2019-04-03 21:52:10,201] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:10,202] A3C_AGENT_WORKER-Thread-10 INFO:Local worker starts!
[2019-04-03 21:52:10,287] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:10,288] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run1
[2019-04-03 21:52:11,203] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:11,203] A3C_AGENT_WORKER-Thread-11 INFO:Local worker starts!
[2019-04-03 21:52:11,311] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:11,312] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run1
[2019-04-03 21:52:12,204] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:12,211] A3C_AGENT_WORKER-Thread-12 INFO:Local worker starts!
[2019-04-03 21:52:12,291] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:12,293] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run1
[2019-04-03 21:52:13,211] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:13,212] A3C_AGENT_WORKER-Thread-13 INFO:Local worker starts!
[2019-04-03 21:52:13,326] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:13,327] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run1
[2019-04-03 21:52:14,213] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:14,214] A3C_AGENT_WORKER-Thread-14 INFO:Local worker starts!
[2019-04-03 21:52:14,340] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:14,342] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run1
[2019-04-03 21:52:15,215] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:15,216] A3C_AGENT_WORKER-Thread-15 INFO:Local worker starts!
[2019-04-03 21:52:15,446] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:15,447] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run1
[2019-04-03 21:52:16,216] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:16,246] A3C_AGENT_WORKER-Thread-16 INFO:Local worker starts!
[2019-04-03 21:52:16,911] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:16,912] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run1
[2019-04-03 21:52:17,247] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:17,253] A3C_AGENT_WORKER-Thread-17 INFO:Local worker starts!
[2019-04-03 21:52:17,784] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:17,786] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run1
[2019-04-03 21:52:18,253] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:18,254] A3C_AGENT_WORKER-Thread-18 INFO:Local worker starts!
[2019-04-03 21:52:19,219] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:19,221] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run1
[2019-04-03 21:52:19,269] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:19,270] A3C_AGENT_WORKER-Thread-19 INFO:Local worker starts!
[2019-04-03 21:52:19,893] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:19,894] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run1
[2019-04-03 21:52:20,285] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:20,293] A3C_AGENT_WORKER-Thread-20 INFO:Local worker starts!
[2019-04-03 21:52:20,838] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:20,840] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run1
[2019-04-03 21:53:12,040] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-03 21:53:12,041] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [2.633333333333334, 69.66666666666667, 226.6666666666667, 199.3333333333333, 19.5, 21.02514176778423, -0.5013038979696127, 1.0, 1.0, 0.0]
[2019-04-03 21:53:12,041] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-03 21:53:12,042] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [0.29535767 0.05710881 0.14783882 0.06952234 0.23516577 0.15613434
 0.03887217], sampled 0.7370836540922121
[2019-04-03 21:53:51,425] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-03 21:53:51,425] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [18.33333333333334, 27.66666666666667, 113.3333333333333, 807.5, 19.0, 25.47998487573564, 0.5130360313132977, 0.0, 0.0, 0.0]
[2019-04-03 21:53:51,425] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-03 21:53:51,426] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [0.24825431 0.15387644 0.07108331 0.03398669 0.26426855 0.1600439
 0.06848678], sampled 0.48126357533880193
[2019-04-03 21:54:00,273] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7487.7648 180804475.9848 157.5327
[2019-04-03 21:54:16,992] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7238.7653 205192080.8023 -462.1879
[2019-04-03 21:54:18,848] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7029.0849 219185844.7234 -571.2267
[2019-04-03 21:54:19,872] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 0, evaluation results [0.0, 7238.765252242774, 205192080.8023241, -462.1879363835367, 7487.76483304243, 180804475.98477572, 157.5327408785089, 7029.084874054795, 219185844.72339004, -571.2267313975015]
[2019-04-03 21:54:22,598] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.05218945 0.10030949 0.2340054  0.15904444 0.25305936 0.13520248
 0.06618941], sum to 1.0000
[2019-04-03 21:54:22,608] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6494
[2019-04-03 21:54:22,700] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.7, 93.0, 0.0, 0.0, 23.0, 20.59993950745187, -0.6797576976732419, 0.0, 1.0, 63207.95701647342], 
current ob forecast is [], 
actual action is [23.5], 
sim time this is 21000.0000, 
sim time next is 21600.0000, 
raw observation next is [7.7, 93.0, 0.0, 0.0, 23.5, 20.68806440247398, -0.6628033567031629, 0.0, 1.0, 48080.04928047442], 
processed observation next is [0.0, 0.2608695652173913, 0.6759002770083103, 0.93, 0.0, 0.0, 0.4583333333333333, 0.22400536687283168, 0.2790655477656124, 0.0, 1.0, 0.22895261562130675], 
reward next is 0.7710, 
noisyNet noise sample is [array([0.99304634], dtype=float32), -0.3451928]. 
=============================================
[2019-04-03 21:54:24,661] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00331325 0.09501556 0.3812245  0.14290395 0.15536426 0.20748755
 0.01469096], sum to 1.0000
[2019-04-03 21:54:24,664] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5656
[2019-04-03 21:54:24,704] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.7, 93.0, 82.0, 0.0, 22.0, 22.71075411741278, -0.2570272608354004, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 42600.0000, 
sim time next is 43200.0000, 
raw observation next is [7.7, 93.0, 85.5, 0.0, 22.0, 22.64199669795142, -0.2677039851741841, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.6759002770083103, 0.93, 0.285, 0.0, 0.3333333333333333, 0.3868330581626183, 0.410765338275272, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3307431], dtype=float32), 1.3370479]. 
=============================================
[2019-04-03 21:54:27,480] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.7720619e-07 2.4961217e-03 8.4030437e-01 9.9043354e-02 5.0874963e-02
 6.9374945e-03 3.4333099e-04], sum to 1.0000
[2019-04-03 21:54:27,484] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6390
[2019-04-03 21:54:27,574] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-4.733333333333333, 74.83333333333334, 0.0, 0.0, 19.0, 19.33621231722455, -1.004916573329498, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 103800.0000, 
sim time next is 104400.0000, 
raw observation next is [-5.0, 74.0, 0.0, 0.0, 19.0, 19.232438681472, -1.029638788483528, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.32409972299168976, 0.74, 0.0, 0.0, 0.08333333333333333, 0.10270322345600007, 0.15678707050549065, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.70230705], dtype=float32), 0.89377916]. 
=============================================
[2019-04-03 21:54:27,911] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.7218360e-08 8.5537927e-04 9.1076672e-01 7.2108164e-02 1.3164518e-02
 3.0514137e-03 5.3729033e-05], sum to 1.0000
[2019-04-03 21:54:27,911] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9649
[2019-04-03 21:54:27,922] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.199999999999999, 69.16666666666667, 0.0, 0.0, 19.0, 18.84353651059873, -1.072882648077402, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 111000.0000, 
sim time next is 111600.0000, 
raw observation next is [-7.3, 68.0, 0.0, 0.0, 19.0, 18.86413336498549, -1.087634815353061, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.26038781163434904, 0.68, 0.0, 0.0, 0.08333333333333333, 0.07201111374879095, 0.13745506154897966, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.42249033], dtype=float32), -1.3635674]. 
=============================================
[2019-04-03 21:54:32,121] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.5116667e-09 1.1921683e-03 8.1775254e-01 1.6922353e-01 9.5153833e-03
 2.3050890e-03 1.1239213e-05], sum to 1.0000
[2019-04-03 21:54:32,122] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0086
[2019-04-03 21:54:32,224] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-8.4, 71.0, 0.0, 0.0, 19.0, 18.60049842110701, -1.192838908478353, 0.0, 1.0, 31213.29031416882], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 168600.0000, 
sim time next is 169200.0000, 
raw observation next is [-8.4, 71.0, 0.0, 0.0, 19.0, 18.56338789566691, -1.198792198660052, 0.0, 1.0, 55948.85257508064], 
processed observation next is [1.0, 1.0, 0.2299168975069252, 0.71, 0.0, 0.0, 0.08333333333333333, 0.04694899130557584, 0.10040260044664932, 0.0, 1.0, 0.266423107500384], 
reward next is 0.7336, 
noisyNet noise sample is [array([0.7703871], dtype=float32), 0.058684394]. 
=============================================
[2019-04-03 21:54:34,885] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3073475e-05 1.5752574e-02 7.4386108e-01 1.9941944e-01 1.7058205e-02
 2.2265976e-02 1.6296246e-03], sum to 1.0000
[2019-04-03 21:54:34,886] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0934
[2019-04-03 21:54:34,906] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 19.0, 19.19547341570266, -1.214270970483244, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 234600.0000, 
sim time next is 235200.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 19.0, 19.17664315847131, -1.243893967551409, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.368421052631579, 0.65, 0.0, 0.0, 0.08333333333333333, 0.09805359653927592, 0.0853686774828637, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.45725167], dtype=float32), 0.3428187]. 
=============================================
[2019-04-03 21:54:39,159] A3C_AGENT_WORKER-Thread-2 INFO:Local step 500, global step 7695: loss 2.7383
[2019-04-03 21:54:39,214] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 500, global step 7695: learning rate 0.0005
[2019-04-03 21:54:39,258] A3C_AGENT_WORKER-Thread-6 INFO:Local step 500, global step 7721: loss -3.8198
[2019-04-03 21:54:39,261] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 500, global step 7721: learning rate 0.0005
[2019-04-03 21:54:39,429] A3C_AGENT_WORKER-Thread-11 INFO:Local step 500, global step 7781: loss -2.0301
[2019-04-03 21:54:39,430] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 500, global step 7781: learning rate 0.0005
[2019-04-03 21:54:39,501] A3C_AGENT_WORKER-Thread-12 INFO:Local step 500, global step 7801: loss 2.7259
[2019-04-03 21:54:39,504] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 500, global step 7802: learning rate 0.0005
[2019-04-03 21:54:39,707] A3C_AGENT_WORKER-Thread-3 INFO:Local step 500, global step 7880: loss 2.4261
[2019-04-03 21:54:39,709] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 500, global step 7881: learning rate 0.0005
[2019-04-03 21:54:39,887] A3C_AGENT_WORKER-Thread-20 INFO:Local step 500, global step 7952: loss 2.4591
[2019-04-03 21:54:39,887] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 500, global step 7952: learning rate 0.0005
[2019-04-03 21:54:39,904] A3C_AGENT_WORKER-Thread-4 INFO:Local step 500, global step 7958: loss -1.2203
[2019-04-03 21:54:39,905] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 500, global step 7958: learning rate 0.0005
[2019-04-03 21:54:39,944] A3C_AGENT_WORKER-Thread-5 INFO:Local step 500, global step 7978: loss 2.5477
[2019-04-03 21:54:39,945] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 500, global step 7978: learning rate 0.0005
[2019-04-03 21:54:40,145] A3C_AGENT_WORKER-Thread-14 INFO:Local step 500, global step 8051: loss -2.6873
[2019-04-03 21:54:40,146] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 500, global step 8051: learning rate 0.0005
[2019-04-03 21:54:40,448] A3C_AGENT_WORKER-Thread-19 INFO:Local step 500, global step 8147: loss -4.4444
[2019-04-03 21:54:40,448] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 500, global step 8147: learning rate 0.0005
[2019-04-03 21:54:40,563] A3C_AGENT_WORKER-Thread-13 INFO:Local step 500, global step 8166: loss -3.4737
[2019-04-03 21:54:40,564] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 500, global step 8166: learning rate 0.0005
[2019-04-03 21:54:40,718] A3C_AGENT_WORKER-Thread-18 INFO:Local step 500, global step 8194: loss -0.1526
[2019-04-03 21:54:40,718] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 500, global step 8194: learning rate 0.0005
[2019-04-03 21:54:40,907] A3C_AGENT_WORKER-Thread-10 INFO:Local step 500, global step 8228: loss -4.8200
[2019-04-03 21:54:40,908] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 500, global step 8228: learning rate 0.0005
[2019-04-03 21:54:40,966] A3C_AGENT_WORKER-Thread-17 INFO:Local step 500, global step 8238: loss -2.8366
[2019-04-03 21:54:40,966] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 500, global step 8238: learning rate 0.0005
[2019-04-03 21:54:41,277] A3C_AGENT_WORKER-Thread-15 INFO:Local step 500, global step 8284: loss 2.3607
[2019-04-03 21:54:41,278] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 500, global step 8284: learning rate 0.0005
[2019-04-03 21:54:41,937] A3C_AGENT_WORKER-Thread-16 INFO:Local step 500, global step 8396: loss -5.0837
[2019-04-03 21:54:41,942] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 500, global step 8396: learning rate 0.0005
[2019-04-03 21:54:42,014] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.2356952e-06 8.1025576e-03 4.8982650e-01 5.0221648e-02 5.9171814e-02
 3.9220253e-01 4.7176442e-04], sum to 1.0000
[2019-04-03 21:54:42,019] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6620
[2019-04-03 21:54:42,221] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.5, 44.0, 93.0, 652.1666666666666, 25.5, 23.83663131679204, -0.08831889038324951, 1.0, 1.0, 84680.4561043097], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 307200.0000, 
sim time next is 307800.0000, 
raw observation next is [-9.5, 44.0, 95.0, 631.0, 26.0, 24.22910840677069, -0.1490813370999395, 1.0, 1.0, 83159.1465372545], 
processed observation next is [1.0, 0.5652173913043478, 0.1994459833795014, 0.44, 0.31666666666666665, 0.6972375690607735, 0.6666666666666666, 0.5190923672308907, 0.4503062209666868, 1.0, 1.0, 0.39599593589168813], 
reward next is 0.6040, 
noisyNet noise sample is [array([1.0638667], dtype=float32), -0.44152853]. 
=============================================
[2019-04-03 21:54:44,789] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.1775933e-16 2.3203254e-06 9.9717391e-01 3.0496615e-04 4.1352917e-04
 2.1052177e-03 2.3362579e-09], sum to 1.0000
[2019-04-03 21:54:44,789] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5863
[2019-04-03 21:54:44,868] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-12.9, 77.83333333333334, 0.0, 0.0, 19.0, 19.56069961200904, -1.000949568726879, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 335400.0000, 
sim time next is 336000.0000, 
raw observation next is [-13.0, 78.66666666666667, 0.0, 0.0, 19.0, 19.43289207290363, -1.029003323046685, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.10249307479224376, 0.7866666666666667, 0.0, 0.0, 0.08333333333333333, 0.11940767274196921, 0.15699889231777164, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.05505029], dtype=float32), 1.3918334]. 
=============================================
[2019-04-03 21:54:44,881] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[66.749504]
 [67.60857 ]
 [66.22649 ]
 [66.06203 ]
 [65.86723 ]], R is [[67.54194641]
 [67.86653137]
 [68.18786621]
 [68.50598907]
 [68.82093048]].
[2019-04-03 21:54:58,486] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.8444718e-16 8.9500209e-05 2.5202323e-02 7.6089107e-04 6.5231508e-01
 3.2163221e-01 1.1703200e-08], sum to 1.0000
[2019-04-03 21:54:58,486] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7506
[2019-04-03 21:54:58,581] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-11.1, 52.0, 0.0, 0.0, 26.0, 21.93353702713187, -0.4992256650301629, 0.0, 1.0, 47190.31800176897], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 447000.0000, 
sim time next is 447600.0000, 
raw observation next is [-11.0, 52.0, 0.0, 0.0, 26.0, 21.85814974008795, -0.5065307884382473, 0.0, 1.0, 47292.97456245552], 
processed observation next is [1.0, 0.17391304347826086, 0.15789473684210528, 0.52, 0.0, 0.0, 0.6666666666666666, 0.32151247834066243, 0.3311564038539176, 0.0, 1.0, 0.22520464077359773], 
reward next is 0.7748, 
noisyNet noise sample is [array([2.3387072], dtype=float32), 0.8756776]. 
=============================================
[2019-04-03 21:55:12,581] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1000, global step 15178: loss 0.3405
[2019-04-03 21:55:12,581] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1000, global step 15178: learning rate 0.0005
[2019-04-03 21:55:13,645] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1000, global step 15420: loss -1.8555
[2019-04-03 21:55:13,647] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1000, global step 15420: learning rate 0.0005
[2019-04-03 21:55:14,062] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1000, global step 15529: loss -1.7098
[2019-04-03 21:55:14,063] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1000, global step 15529: learning rate 0.0005
[2019-04-03 21:55:14,545] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1000, global step 15642: loss -1.4616
[2019-04-03 21:55:14,560] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1000, global step 15642: learning rate 0.0005
[2019-04-03 21:55:15,221] A3C_AGENT_WORKER-Thread-6 INFO:Local step 1000, global step 15816: loss -0.0574
[2019-04-03 21:55:15,222] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 1000, global step 15816: learning rate 0.0005
[2019-04-03 21:55:15,604] A3C_AGENT_WORKER-Thread-5 INFO:Local step 1000, global step 15924: loss -0.2240
[2019-04-03 21:55:15,623] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 1000, global step 15924: learning rate 0.0005
[2019-04-03 21:55:15,839] A3C_AGENT_WORKER-Thread-4 INFO:Local step 1000, global step 15973: loss -0.0214
[2019-04-03 21:55:15,842] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 1000, global step 15973: learning rate 0.0005
[2019-04-03 21:55:15,999] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1000, global step 16017: loss -0.1738
[2019-04-03 21:55:16,001] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1000, global step 16017: learning rate 0.0005
[2019-04-03 21:55:16,764] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1000, global step 16214: loss -0.0811
[2019-04-03 21:55:16,765] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1000, global step 16214: learning rate 0.0005
[2019-04-03 21:55:16,815] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1000, global step 16231: loss -0.2142
[2019-04-03 21:55:16,816] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1000, global step 16231: learning rate 0.0005
[2019-04-03 21:55:16,820] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1000, global step 16232: loss -0.0017
[2019-04-03 21:55:16,823] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1000, global step 16232: learning rate 0.0005
[2019-04-03 21:55:17,402] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1000, global step 16387: loss -0.6230
[2019-04-03 21:55:17,403] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1000, global step 16387: learning rate 0.0005
[2019-04-03 21:55:17,512] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1000, global step 16423: loss -0.5630
[2019-04-03 21:55:17,513] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1000, global step 16423: learning rate 0.0005
[2019-04-03 21:55:17,649] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1000, global step 16453: loss -0.2076
[2019-04-03 21:55:17,650] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1000, global step 16453: learning rate 0.0005
[2019-04-03 21:55:17,769] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1000, global step 16478: loss -0.0006
[2019-04-03 21:55:17,782] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1000, global step 16478: learning rate 0.0005
[2019-04-03 21:55:18,590] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1000, global step 16645: loss -1.4431
[2019-04-03 21:55:18,605] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1000, global step 16645: learning rate 0.0005
[2019-04-03 21:55:26,411] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.8930017e-19 4.0539046e-05 9.8234916e-04 2.3665933e-05 8.0735862e-01
 1.9159484e-01 9.1761543e-11], sum to 1.0000
[2019-04-03 21:55:26,411] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2065
[2019-04-03 21:55:26,548] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.3, 76.0, 0.0, 0.0, 26.0, 24.03143698967301, -0.01745961037973359, 0.0, 1.0, 41862.8588172094], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 712200.0000, 
sim time next is 712800.0000, 
raw observation next is [-2.3, 76.0, 0.0, 0.0, 26.0, 24.06282913246363, -0.01297802031459702, 0.0, 1.0, 41895.58742310471], 
processed observation next is [1.0, 0.2608695652173913, 0.3988919667590028, 0.76, 0.0, 0.0, 0.6666666666666666, 0.5052357610386359, 0.49567399322846767, 0.0, 1.0, 0.1995027972528796], 
reward next is 0.8005, 
noisyNet noise sample is [array([-0.9479938], dtype=float32), 2.0474946]. 
=============================================
[2019-04-03 21:55:34,289] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.1955319e-21 7.2804169e-06 6.3914587e-05 2.1344792e-07 4.4341391e-01
 5.5651474e-01 2.2226226e-13], sum to 1.0000
[2019-04-03 21:55:34,317] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1056
[2019-04-03 21:55:34,505] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.266666666666667, 54.66666666666667, 0.0, 0.0, 26.0, 25.04682075577638, 0.3064695124447858, 1.0, 1.0, 28458.79941226137], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 760800.0000, 
sim time next is 761400.0000, 
raw observation next is [-4.45, 55.5, 0.0, 0.0, 26.0, 25.05760531118658, 0.2970658248499397, 1.0, 1.0, 30448.97023440519], 
processed observation next is [1.0, 0.8260869565217391, 0.3393351800554017, 0.555, 0.0, 0.0, 0.6666666666666666, 0.588133775932215, 0.5990219416166466, 1.0, 1.0, 0.14499509635431043], 
reward next is 0.8550, 
noisyNet noise sample is [array([-0.22677079], dtype=float32), -0.87427735]. 
=============================================
[2019-04-03 21:55:49,173] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.24930635e-17 1.14188697e-05 5.73471698e-05 6.65195671e-07
 7.92694807e-01 2.07235739e-01 5.72066439e-12], sum to 1.0000
[2019-04-03 21:55:49,176] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4326
[2019-04-03 21:55:49,212] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.4, 83.0, 0.0, 0.0, 26.0, 24.88027426770575, 0.2625381709491256, 0.0, 1.0, 42555.07319005419], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 853800.0000, 
sim time next is 854400.0000, 
raw observation next is [-3.4, 83.0, 0.0, 0.0, 26.0, 24.85566671565665, 0.2565097974399529, 0.0, 1.0, 42308.78922784868], 
processed observation next is [1.0, 0.9130434782608695, 0.368421052631579, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5713055596380542, 0.5855032658133176, 0.0, 1.0, 0.20147042489451755], 
reward next is 0.7985, 
noisyNet noise sample is [array([0.40640062], dtype=float32), -1.2390872]. 
=============================================
[2019-04-03 21:55:50,323] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1500, global step 22954: loss 4.7164
[2019-04-03 21:55:50,323] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1500, global step 22954: learning rate 0.0005
[2019-04-03 21:55:50,518] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1500, global step 22994: loss 4.5903
[2019-04-03 21:55:50,518] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1500, global step 22994: learning rate 0.0005
[2019-04-03 21:55:51,121] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1500, global step 23169: loss 9.9971
[2019-04-03 21:55:51,124] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1500, global step 23169: learning rate 0.0005
[2019-04-03 21:55:53,024] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1500, global step 23669: loss 3.8820
[2019-04-03 21:55:53,025] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1500, global step 23669: learning rate 0.0005
[2019-04-03 21:55:53,653] A3C_AGENT_WORKER-Thread-6 INFO:Local step 1500, global step 23801: loss 6.8059
[2019-04-03 21:55:53,654] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 1500, global step 23801: learning rate 0.0005
[2019-04-03 21:55:53,926] A3C_AGENT_WORKER-Thread-4 INFO:Local step 1500, global step 23866: loss 10.0501
[2019-04-03 21:55:53,934] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 1500, global step 23866: learning rate 0.0005
[2019-04-03 21:55:54,357] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1500, global step 23964: loss 3.0662
[2019-04-03 21:55:54,358] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1500, global step 23964: learning rate 0.0005
[2019-04-03 21:55:54,657] A3C_AGENT_WORKER-Thread-5 INFO:Local step 1500, global step 24025: loss 6.0603
[2019-04-03 21:55:54,659] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 1500, global step 24025: learning rate 0.0005
[2019-04-03 21:55:54,920] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1500, global step 24083: loss 5.0944
[2019-04-03 21:55:54,921] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1500, global step 24083: learning rate 0.0005
[2019-04-03 21:55:55,580] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1500, global step 24263: loss 7.8492
[2019-04-03 21:55:55,605] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1500, global step 24263: learning rate 0.0005
[2019-04-03 21:55:55,917] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1500, global step 24358: loss 9.4083
[2019-04-03 21:55:55,924] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1500, global step 24358: learning rate 0.0005
[2019-04-03 21:55:56,701] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1500, global step 24535: loss 7.6114
[2019-04-03 21:55:56,742] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1500, global step 24543: learning rate 0.0005
[2019-04-03 21:55:56,881] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1500, global step 24572: loss 3.8669
[2019-04-03 21:55:56,881] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1500, global step 24572: learning rate 0.0005
[2019-04-03 21:55:57,009] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1500, global step 24608: loss 8.1436
[2019-04-03 21:55:57,036] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1500, global step 24608: learning rate 0.0005
[2019-04-03 21:55:57,581] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1500, global step 24736: loss 3.6839
[2019-04-03 21:55:57,607] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1500, global step 24736: learning rate 0.0005
[2019-04-03 21:55:58,295] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1500, global step 24930: loss 3.1396
[2019-04-03 21:55:58,296] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1500, global step 24930: learning rate 0.0005
[2019-04-03 21:56:00,031] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.2251097e-18 3.0252854e-06 7.2642310e-05 6.9722313e-07 3.0281276e-01
 6.9711089e-01 3.2385376e-11], sum to 1.0000
[2019-04-03 21:56:00,031] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7886
[2019-04-03 21:56:00,177] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.5, 89.0, 0.0, 0.0, 26.0, 25.2349940183401, 0.4058565743650567, 0.0, 1.0, 38085.93055891249], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 954000.0000, 
sim time next is 954600.0000, 
raw observation next is [5.683333333333334, 87.83333333333334, 0.0, 0.0, 26.0, 25.25876206675157, 0.410687643100103, 0.0, 1.0, 38058.27010464832], 
processed observation next is [1.0, 0.043478260869565216, 0.6200369344413666, 0.8783333333333334, 0.0, 0.0, 0.6666666666666666, 0.604896838895964, 0.6368958810333677, 0.0, 1.0, 0.18122985764118246], 
reward next is 0.8188, 
noisyNet noise sample is [array([1.6095223], dtype=float32), 0.22733755]. 
=============================================
[2019-04-03 21:56:01,283] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.6311515e-19 1.3189878e-06 1.3516865e-04 1.4556162e-05 6.8105765e-02
 9.3174320e-01 4.0980330e-11], sum to 1.0000
[2019-04-03 21:56:01,283] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5152
[2019-04-03 21:56:01,466] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.8, 83.0, 0.0, 0.0, 26.0, 25.45628984959813, 0.4643143628584527, 0.0, 1.0, 40171.03299944349], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 971400.0000, 
sim time next is 972000.0000, 
raw observation next is [8.8, 83.0, 0.0, 0.0, 26.0, 25.56846046993575, 0.4759921567979018, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.7063711911357342, 0.83, 0.0, 0.0, 0.6666666666666666, 0.6307050391613126, 0.6586640522659672, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.51849633], dtype=float32), -2.172516]. 
=============================================
[2019-04-03 21:56:01,469] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[84.18127 ]
 [84.214935]
 [84.14237 ]
 [84.22881 ]
 [84.28426 ]], R is [[84.17595673]
 [84.14290619]
 [84.04769897]
 [84.11793518]
 [84.27675629]].
[2019-04-03 21:56:05,162] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.1004504e-21 1.5623508e-09 6.2154828e-07 1.4041652e-09 2.6282910e-06
 9.9999678e-01 4.6914119e-14], sum to 1.0000
[2019-04-03 21:56:05,162] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5837
[2019-04-03 21:56:05,283] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.4, 76.33333333333334, 0.0, 0.0, 26.0, 25.88694223208595, 0.6276386118926739, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1027200.0000, 
sim time next is 1027800.0000, 
raw observation next is [14.4, 76.0, 0.0, 0.0, 26.0, 25.94285774835221, 0.6289108342290571, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.8614958448753465, 0.76, 0.0, 0.0, 0.6666666666666666, 0.6619048123626842, 0.7096369447430191, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.24892], dtype=float32), -0.57858694]. 
=============================================
[2019-04-03 21:56:07,302] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [9.0565752e-24 5.2870305e-08 3.0180078e-07 3.6780047e-08 1.2724071e-03
 9.9872726e-01 1.0606773e-14], sum to 1.0000
[2019-04-03 21:56:07,302] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7156
[2019-04-03 21:56:07,312] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.55, 79.0, 0.0, 0.0, 26.0, 25.7293761269718, 0.6069222718372939, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1056600.0000, 
sim time next is 1057200.0000, 
raw observation next is [13.46666666666667, 79.33333333333333, 0.0, 0.0, 26.0, 25.8405609423404, 0.6107631339609298, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.8356417359187445, 0.7933333333333333, 0.0, 0.0, 0.6666666666666666, 0.6533800785283667, 0.70358771132031, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.13368121], dtype=float32), 0.87120336]. 
=============================================
[2019-04-03 21:56:11,578] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.2868948e-23 2.1373585e-07 1.5458285e-05 1.2973233e-07 9.4435003e-05
 9.9988973e-01 3.1990095e-13], sum to 1.0000
[2019-04-03 21:56:11,578] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7551
[2019-04-03 21:56:11,590] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.46666666666667, 79.33333333333333, 0.0, 0.0, 26.0, 25.84077930929694, 0.6108564138892248, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1057200.0000, 
sim time next is 1057800.0000, 
raw observation next is [13.38333333333333, 79.66666666666667, 0.0, 0.0, 26.0, 25.88046920676895, 0.6074323790349297, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.8333333333333334, 0.7966666666666667, 0.0, 0.0, 0.6666666666666666, 0.6567057672307458, 0.70247745967831, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9758724], dtype=float32), 1.2294745]. 
=============================================
[2019-04-03 21:56:13,664] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2000, global step 30186: loss 0.9056
[2019-04-03 21:56:13,674] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2000, global step 30188: learning rate 0.0005
[2019-04-03 21:56:13,917] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2000, global step 30311: loss 1.0124
[2019-04-03 21:56:13,927] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2000, global step 30311: learning rate 0.0005
[2019-04-03 21:56:14,056] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2000, global step 30389: loss 3.4251
[2019-04-03 21:56:14,057] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2000, global step 30389: learning rate 0.0005
[2019-04-03 21:56:15,923] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.7607441e-18 2.1983790e-06 6.3931680e-06 6.3433308e-06 1.4034408e-02
 9.8595065e-01 5.3942167e-11], sum to 1.0000
[2019-04-03 21:56:15,929] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4899
[2019-04-03 21:56:15,933] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [18.55, 64.0, 171.0, 0.0, 26.0, 25.10197675501551, 0.5069373756860273, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1168200.0000, 
sim time next is 1168800.0000, 
raw observation next is [18.46666666666667, 64.33333333333333, 169.0, 0.0, 26.0, 25.11459217225383, 0.5066780451774603, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.9741458910433982, 0.6433333333333333, 0.5633333333333334, 0.0, 0.6666666666666666, 0.5928826810211524, 0.6688926817258202, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2379901], dtype=float32), 0.24448161]. 
=============================================
[2019-04-03 21:56:16,052] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2000, global step 31405: loss 0.4849
[2019-04-03 21:56:16,053] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2000, global step 31405: learning rate 0.0005
[2019-04-03 21:56:16,864] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2000, global step 31724: loss 0.3420
[2019-04-03 21:56:16,872] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2000, global step 31725: learning rate 0.0005
[2019-04-03 21:56:17,291] A3C_AGENT_WORKER-Thread-4 INFO:Local step 2000, global step 31897: loss 0.2110
[2019-04-03 21:56:17,292] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 2000, global step 31897: learning rate 0.0005
[2019-04-03 21:56:17,374] A3C_AGENT_WORKER-Thread-6 INFO:Local step 2000, global step 31925: loss 0.2657
[2019-04-03 21:56:17,374] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 2000, global step 31925: learning rate 0.0005
[2019-04-03 21:56:17,871] A3C_AGENT_WORKER-Thread-5 INFO:Local step 2000, global step 32112: loss 0.2046
[2019-04-03 21:56:17,872] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 2000, global step 32112: learning rate 0.0005
[2019-04-03 21:56:17,899] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2000, global step 32127: loss 0.2462
[2019-04-03 21:56:17,899] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2000, global step 32127: learning rate 0.0005
[2019-04-03 21:56:18,493] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2000, global step 32375: loss 0.1879
[2019-04-03 21:56:18,499] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2000, global step 32375: learning rate 0.0005
[2019-04-03 21:56:18,920] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2000, global step 32528: loss 0.1866
[2019-04-03 21:56:18,921] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2000, global step 32528: learning rate 0.0005
[2019-04-03 21:56:18,960] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2000, global step 32546: loss 0.2274
[2019-04-03 21:56:18,961] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2000, global step 32546: learning rate 0.0005
[2019-04-03 21:56:19,565] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.31697165e-23 4.13439144e-11 4.70017596e-07 1.01670681e-08
 2.82006404e-05 9.99971271e-01 4.44651470e-15], sum to 1.0000
[2019-04-03 21:56:19,566] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4095
[2019-04-03 21:56:19,584] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.5, 100.0, 0.0, 0.0, 26.0, 25.47961741114141, 0.586298205088504, 0.0, 1.0, 36960.12672906319], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1292400.0000, 
sim time next is 1293000.0000, 
raw observation next is [5.316666666666667, 99.33333333333334, 0.0, 0.0, 26.0, 25.45752934350334, 0.588322216630032, 0.0, 1.0, 46872.99723456703], 
processed observation next is [0.0, 1.0, 0.6098799630655587, 0.9933333333333334, 0.0, 0.0, 0.6666666666666666, 0.6214607786252783, 0.6961074055433439, 0.0, 1.0, 0.2232047487360335], 
reward next is 0.7768, 
noisyNet noise sample is [array([-2.5129247], dtype=float32), -0.3925216]. 
=============================================
[2019-04-03 21:56:19,592] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2000, global step 32789: loss 0.1012
[2019-04-03 21:56:19,618] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[106.510086]
 [106.35057 ]
 [106.27151 ]
 [106.24533 ]
 [106.274086]], R is [[106.42341614]
 [106.18318176]
 [105.95742035]
 [105.80856323]
 [105.66117859]].
[2019-04-03 21:56:19,636] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2000, global step 32789: learning rate 0.0005
[2019-04-03 21:56:20,020] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2000, global step 32914: loss 0.0219
[2019-04-03 21:56:20,020] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2000, global step 32914: learning rate 0.0005
[2019-04-03 21:56:20,806] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2000, global step 33105: loss 0.0060
[2019-04-03 21:56:20,811] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2000, global step 33105: learning rate 0.0005
[2019-04-03 21:56:21,465] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2000, global step 33270: loss 0.0281
[2019-04-03 21:56:21,506] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2000, global step 33270: learning rate 0.0005
[2019-04-03 21:56:36,008] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.8927056e-19 1.1603905e-09 5.0980844e-07 7.4582713e-09 3.1024034e-03
 9.9689710e-01 3.0017913e-13], sum to 1.0000
[2019-04-03 21:56:36,010] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7735
[2019-04-03 21:56:36,027] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 26.0, 25.28023041050545, 0.5113809897651479, 0.0, 1.0, 43183.50296995221], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1378800.0000, 
sim time next is 1379400.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 26.0, 25.30731211376429, 0.5094672343771376, 0.0, 1.0, 42095.64461868651], 
processed observation next is [1.0, 1.0, 0.46260387811634357, 0.95, 0.0, 0.0, 0.6666666666666666, 0.6089426761470241, 0.6698224114590459, 0.0, 1.0, 0.20045545056517383], 
reward next is 0.7995, 
noisyNet noise sample is [array([0.05579865], dtype=float32), 0.30761316]. 
=============================================
[2019-04-03 21:56:41,872] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2500, global step 38419: loss 3.8335
[2019-04-03 21:56:41,872] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2500, global step 38419: learning rate 0.0005
[2019-04-03 21:56:42,265] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2500, global step 38522: loss 3.8511
[2019-04-03 21:56:42,267] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2500, global step 38522: learning rate 0.0005
[2019-04-03 21:56:43,055] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2500, global step 38741: loss 3.3702
[2019-04-03 21:56:43,065] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2500, global step 38741: learning rate 0.0005
[2019-04-03 21:56:44,791] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.4389605e-24 9.3831117e-13 5.5633307e-09 1.1835782e-12 2.5888650e-07
 9.9999976e-01 2.5293987e-16], sum to 1.0000
[2019-04-03 21:56:44,794] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0117
[2019-04-03 21:56:44,834] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.966666666666667, 75.33333333333333, 0.0, 0.0, 26.0, 26.17735463620768, 0.7069521436153606, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1546800.0000, 
sim time next is 1547400.0000, 
raw observation next is [6.783333333333333, 75.66666666666667, 0.0, 0.0, 26.0, 26.16000857858801, 0.6981999176658906, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.6505078485687905, 0.7566666666666667, 0.0, 0.0, 0.6666666666666666, 0.680000714882334, 0.7327333058886302, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.53029364], dtype=float32), 0.6288674]. 
=============================================
[2019-04-03 21:56:45,498] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2500, global step 39592: loss 2.5987
[2019-04-03 21:56:45,500] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2500, global step 39593: learning rate 0.0005
[2019-04-03 21:56:45,682] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2500, global step 39660: loss 2.8688
[2019-04-03 21:56:45,683] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2500, global step 39660: learning rate 0.0005
[2019-04-03 21:56:46,565] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2500, global step 40002: loss 3.2121
[2019-04-03 21:56:46,566] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2500, global step 40002: learning rate 0.0005
[2019-04-03 21:56:46,645] A3C_AGENT_WORKER-Thread-4 INFO:Local step 2500, global step 40031: loss 2.8636
[2019-04-03 21:56:46,646] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 2500, global step 40031: learning rate 0.0005
[2019-04-03 21:56:46,655] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.6474290e-20 2.8822089e-10 5.7286974e-07 1.0240390e-08 2.4494257e-06
 9.9999690e-01 2.7238555e-13], sum to 1.0000
[2019-04-03 21:56:46,655] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5146
[2019-04-03 21:56:46,789] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.183333333333333, 92.0, 0.0, 0.0, 26.0, 25.48548859529517, 0.4891777182183658, 0.0, 1.0, 55018.53451209464], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1462200.0000, 
sim time next is 1462800.0000, 
raw observation next is [1.266666666666667, 92.0, 0.0, 0.0, 26.0, 25.44535404002222, 0.4829982295905602, 0.0, 1.0, 63926.20109956854], 
processed observation next is [1.0, 0.9565217391304348, 0.4976915974145891, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6204461700018516, 0.66099940986352, 0.0, 1.0, 0.3044104814265169], 
reward next is 0.6956, 
noisyNet noise sample is [array([-0.03405539], dtype=float32), 1.5687543]. 
=============================================
[2019-04-03 21:56:46,806] A3C_AGENT_WORKER-Thread-6 INFO:Local step 2500, global step 40056: loss 3.1413
[2019-04-03 21:56:46,809] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 2500, global step 40058: learning rate 0.0005
[2019-04-03 21:56:47,166] A3C_AGENT_WORKER-Thread-5 INFO:Local step 2500, global step 40217: loss 2.7958
[2019-04-03 21:56:47,166] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 2500, global step 40217: learning rate 0.0005
[2019-04-03 21:56:47,252] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2500, global step 40260: loss 2.5832
[2019-04-03 21:56:47,272] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2500, global step 40273: learning rate 0.0005
[2019-04-03 21:56:48,421] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2500, global step 40813: loss 2.7363
[2019-04-03 21:56:48,424] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2500, global step 40815: learning rate 0.0005
[2019-04-03 21:56:48,686] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2500, global step 40932: loss 2.7997
[2019-04-03 21:56:48,689] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2500, global step 40933: learning rate 0.0005
[2019-04-03 21:56:48,734] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2500, global step 40952: loss 2.9652
[2019-04-03 21:56:48,735] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2500, global step 40952: learning rate 0.0005
[2019-04-03 21:56:48,882] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2500, global step 41025: loss 2.6248
[2019-04-03 21:56:48,883] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2500, global step 41025: learning rate 0.0005
[2019-04-03 21:56:49,057] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2500, global step 41114: loss 2.7561
[2019-04-03 21:56:49,058] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2500, global step 41114: learning rate 0.0005
[2019-04-03 21:56:49,511] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2500, global step 41345: loss 3.1113
[2019-04-03 21:56:49,511] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2500, global step 41345: learning rate 0.0005
[2019-04-03 21:56:51,159] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.3337507e-19 8.5089439e-08 1.7210397e-04 3.8160854e-07 3.9961070e-02
 9.5986634e-01 4.1669813e-11], sum to 1.0000
[2019-04-03 21:56:51,159] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7326
[2019-04-03 21:56:51,196] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.8, 79.0, 37.0, 35.0, 26.0, 25.82473932130492, 0.5597598565371857, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1585800.0000, 
sim time next is 1586400.0000, 
raw observation next is [6.066666666666666, 78.0, 50.0, 51.83333333333333, 26.0, 26.05797675397686, 0.5804240930148894, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.6306555863342568, 0.78, 0.16666666666666666, 0.057274401473296495, 0.6666666666666666, 0.671498062831405, 0.6934746976716298, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.8160363], dtype=float32), -1.2426319]. 
=============================================
[2019-04-03 21:56:58,474] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.9884628e-23 5.2565050e-11 8.4651322e-07 8.4337970e-10 5.8278232e-07
 9.9999857e-01 1.7881704e-14], sum to 1.0000
[2019-04-03 21:56:58,474] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6959
[2019-04-03 21:56:58,717] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.75, 92.0, 30.0, 0.0, 26.0, 25.61125527404931, 0.514790247793765, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1672200.0000, 
sim time next is 1672800.0000, 
raw observation next is [2.566666666666667, 92.0, 33.83333333333333, 0.0, 26.0, 25.72068154524737, 0.5391253285103353, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.5337026777469991, 0.92, 0.11277777777777777, 0.0, 0.6666666666666666, 0.6433901287706142, 0.6797084428367784, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6804553], dtype=float32), -0.3316243]. 
=============================================
[2019-04-03 21:57:04,835] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3000, global step 46786: loss 0.1972
[2019-04-03 21:57:04,835] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3000, global step 46786: learning rate 0.0005
[2019-04-03 21:57:04,973] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3000, global step 46818: loss 0.1279
[2019-04-03 21:57:04,979] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3000, global step 46820: learning rate 0.0005
[2019-04-03 21:57:05,381] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3000, global step 46922: loss 0.0401
[2019-04-03 21:57:05,381] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3000, global step 46922: learning rate 0.0005
[2019-04-03 21:57:07,954] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3000, global step 47549: loss 0.1498
[2019-04-03 21:57:07,954] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3000, global step 47549: learning rate 0.0005
[2019-04-03 21:57:09,735] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3000, global step 47867: loss 0.0104
[2019-04-03 21:57:09,735] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3000, global step 47867: learning rate 0.0005
[2019-04-03 21:57:10,078] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3000, global step 47928: loss 0.0162
[2019-04-03 21:57:10,095] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3000, global step 47928: learning rate 0.0005
[2019-04-03 21:57:10,665] A3C_AGENT_WORKER-Thread-4 INFO:Local step 3000, global step 48034: loss 0.0042
[2019-04-03 21:57:10,665] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 3000, global step 48034: learning rate 0.0005
[2019-04-03 21:57:10,960] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.7519339e-20 7.9586232e-10 1.7563737e-07 2.5255384e-09 1.1930944e-06
 9.9999857e-01 3.5720222e-13], sum to 1.0000
[2019-04-03 21:57:10,962] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0078
[2019-04-03 21:57:11,002] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 86.0, 0.0, 0.0, 26.0, 25.02952870423989, 0.3158426263707014, 0.0, 1.0, 46021.95165223091], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1803600.0000, 
sim time next is 1804200.0000, 
raw observation next is [-5.0, 86.0, 0.0, 0.0, 26.0, 25.05256506632329, 0.3093053554774092, 0.0, 1.0, 45981.37811129295], 
processed observation next is [0.0, 0.9130434782608695, 0.32409972299168976, 0.86, 0.0, 0.0, 0.6666666666666666, 0.5877137555269408, 0.6031017851591364, 0.0, 1.0, 0.21895894338710928], 
reward next is 0.7810, 
noisyNet noise sample is [array([0.05378729], dtype=float32), -0.28019023]. 
=============================================
[2019-04-03 21:57:11,045] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3000, global step 48107: loss 0.0028
[2019-04-03 21:57:11,051] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3000, global step 48107: learning rate 0.0005
[2019-04-03 21:57:11,518] A3C_AGENT_WORKER-Thread-6 INFO:Local step 3000, global step 48203: loss 0.0139
[2019-04-03 21:57:11,518] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 3000, global step 48203: learning rate 0.0005
[2019-04-03 21:57:11,741] A3C_AGENT_WORKER-Thread-5 INFO:Local step 3000, global step 48263: loss 0.0312
[2019-04-03 21:57:11,741] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 3000, global step 48263: learning rate 0.0005
[2019-04-03 21:57:13,033] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3000, global step 48574: loss 0.0169
[2019-04-03 21:57:13,034] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3000, global step 48574: learning rate 0.0005
[2019-04-03 21:57:14,074] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3000, global step 48791: loss 0.0493
[2019-04-03 21:57:14,078] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3000, global step 48791: learning rate 0.0005
[2019-04-03 21:57:14,496] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3000, global step 48874: loss 0.0511
[2019-04-03 21:57:14,497] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3000, global step 48874: learning rate 0.0005
[2019-04-03 21:57:14,654] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3000, global step 48918: loss 0.0218
[2019-04-03 21:57:14,654] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3000, global step 48918: learning rate 0.0005
[2019-04-03 21:57:14,743] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3000, global step 48930: loss 0.0329
[2019-04-03 21:57:14,744] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3000, global step 48930: learning rate 0.0005
[2019-04-03 21:57:15,273] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3000, global step 49057: loss 0.0082
[2019-04-03 21:57:15,278] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3000, global step 49057: learning rate 0.0005
[2019-04-03 21:57:18,552] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [9.7862517e-20 3.6541822e-10 6.1582845e-08 2.4996110e-09 1.8277998e-06
 9.9999809e-01 2.1337892e-13], sum to 1.0000
[2019-04-03 21:57:18,552] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1095
[2019-04-03 21:57:18,730] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 75.0, 183.3333333333333, 76.66666666666667, 26.0, 25.02770247555555, 0.2848621361801081, 0.0, 1.0, 41963.60892076751], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1866000.0000, 
sim time next is 1866600.0000, 
raw observation next is [-4.5, 77.0, 186.0, 84.0, 26.0, 25.02297283625713, 0.288091452417594, 0.0, 1.0, 45361.88558615628], 
processed observation next is [0.0, 0.6086956521739131, 0.3379501385041552, 0.77, 0.62, 0.09281767955801105, 0.6666666666666666, 0.5852477363547607, 0.596030484139198, 0.0, 1.0, 0.21600897898169655], 
reward next is 0.7840, 
noisyNet noise sample is [array([0.52442026], dtype=float32), -0.27761677]. 
=============================================
[2019-04-03 21:57:19,418] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.3103565e-20 4.9614972e-11 4.4990571e-08 3.5422887e-10 2.9692858e-08
 1.0000000e+00 5.3056396e-14], sum to 1.0000
[2019-04-03 21:57:19,419] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4953
[2019-04-03 21:57:19,506] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 81.66666666666667, 110.0, 27.99999999999999, 26.0, 25.07953803100467, 0.2778965265293961, 0.0, 1.0, 21248.72017079855], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1869000.0000, 
sim time next is 1869600.0000, 
raw observation next is [-4.5, 80.33333333333334, 91.0, 14.0, 26.0, 25.06347685767314, 0.2680606223869741, 0.0, 1.0, 38962.26795501552], 
processed observation next is [0.0, 0.6521739130434783, 0.3379501385041552, 0.8033333333333335, 0.30333333333333334, 0.015469613259668509, 0.6666666666666666, 0.5886230714727617, 0.589353540795658, 0.0, 1.0, 0.18553460930959773], 
reward next is 0.8145, 
noisyNet noise sample is [array([0.10124384], dtype=float32), 0.68732774]. 
=============================================
[2019-04-03 21:57:25,062] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0608634e-19 2.8442665e-10 3.8933152e-07 1.9274566e-08 3.2649245e-07
 9.9999928e-01 2.2783615e-12], sum to 1.0000
[2019-04-03 21:57:25,062] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2461
[2019-04-03 21:57:25,183] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.116666666666666, 78.33333333333334, 0.0, 0.0, 26.0, 24.45068155726197, 0.1163091935857354, 0.0, 1.0, 44905.18684656543], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1896600.0000, 
sim time next is 1897200.0000, 
raw observation next is [-7.3, 79.0, 0.0, 0.0, 26.0, 24.4143822534751, 0.1078136825152412, 0.0, 1.0, 44906.43099290472], 
processed observation next is [0.0, 1.0, 0.26038781163434904, 0.79, 0.0, 0.0, 0.6666666666666666, 0.5345318544562584, 0.5359378941717471, 0.0, 1.0, 0.2138401475852606], 
reward next is 0.7862, 
noisyNet noise sample is [array([-0.8761958], dtype=float32), -0.78770614]. 
=============================================
[2019-04-03 21:57:37,383] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3500, global step 54529: loss 0.5048
[2019-04-03 21:57:37,383] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3500, global step 54529: learning rate 0.0005
[2019-04-03 21:57:37,394] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3500, global step 54533: loss 0.5233
[2019-04-03 21:57:37,405] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3500, global step 54533: learning rate 0.0005
[2019-04-03 21:57:38,280] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3500, global step 54738: loss 0.5845
[2019-04-03 21:57:38,297] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3500, global step 54738: learning rate 0.0005
[2019-04-03 21:57:38,391] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3500, global step 54759: loss 0.5683
[2019-04-03 21:57:38,392] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3500, global step 54759: learning rate 0.0005
[2019-04-03 21:57:40,965] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.8203791e-18 2.2640703e-09 4.8615956e-08 1.1775810e-09 2.3306939e-06
 9.9999762e-01 1.4946625e-12], sum to 1.0000
[2019-04-03 21:57:40,965] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8068
[2019-04-03 21:57:40,996] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 86.0, 0.0, 0.0, 26.0, 25.3273155443786, 0.4321740232455049, 0.0, 1.0, 46960.85105736881], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2062200.0000, 
sim time next is 2062800.0000, 
raw observation next is [-3.9, 86.0, 0.0, 0.0, 26.0, 25.37493476057239, 0.4347458125985291, 0.0, 1.0, 42443.2345292986], 
processed observation next is [1.0, 0.9130434782608695, 0.3545706371191136, 0.86, 0.0, 0.0, 0.6666666666666666, 0.6145778967143659, 0.6449152708661764, 0.0, 1.0, 0.2021106406157076], 
reward next is 0.7979, 
noisyNet noise sample is [array([-1.1152136], dtype=float32), -0.57238173]. 
=============================================
[2019-04-03 21:57:41,099] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3500, global step 55329: loss 0.7468
[2019-04-03 21:57:41,099] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3500, global step 55329: learning rate 0.0005
[2019-04-03 21:57:41,961] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3500, global step 55523: loss 0.6869
[2019-04-03 21:57:41,963] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3500, global step 55523: learning rate 0.0005
[2019-04-03 21:57:44,012] A3C_AGENT_WORKER-Thread-6 INFO:Local step 3500, global step 55967: loss 0.9497
[2019-04-03 21:57:44,014] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 3500, global step 55967: learning rate 0.0005
[2019-04-03 21:57:44,086] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.3715253e-20 4.9149701e-10 5.3443699e-07 5.7687771e-08 7.1873746e-06
 9.9999225e-01 1.3005092e-12], sum to 1.0000
[2019-04-03 21:57:44,086] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2962
[2019-04-03 21:57:44,120] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.2, 87.66666666666667, 0.0, 0.0, 26.0, 24.46761407463516, 0.1703216865033971, 0.0, 1.0, 43302.02121605805], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2085600.0000, 
sim time next is 2086200.0000, 
raw observation next is [-5.3, 88.5, 0.0, 0.0, 26.0, 24.45800821621918, 0.1579698437008022, 0.0, 1.0, 43376.19038947848], 
processed observation next is [1.0, 0.13043478260869565, 0.31578947368421056, 0.885, 0.0, 0.0, 0.6666666666666666, 0.5381673513515984, 0.552656614566934, 0.0, 1.0, 0.20655328756894514], 
reward next is 0.7934, 
noisyNet noise sample is [array([-1.1088495], dtype=float32), 0.59227264]. 
=============================================
[2019-04-03 21:57:44,301] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3500, global step 56041: loss 1.0368
[2019-04-03 21:57:44,303] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3500, global step 56041: learning rate 0.0005
[2019-04-03 21:57:44,541] A3C_AGENT_WORKER-Thread-4 INFO:Local step 3500, global step 56114: loss 1.0229
[2019-04-03 21:57:44,557] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 3500, global step 56114: learning rate 0.0005
[2019-04-03 21:57:44,945] A3C_AGENT_WORKER-Thread-5 INFO:Local step 3500, global step 56223: loss 1.2643
[2019-04-03 21:57:44,947] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 3500, global step 56226: learning rate 0.0005
[2019-04-03 21:57:45,239] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3500, global step 56293: loss 1.2782
[2019-04-03 21:57:45,241] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3500, global step 56293: learning rate 0.0005
[2019-04-03 21:57:46,416] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3500, global step 56613: loss 0.9838
[2019-04-03 21:57:46,420] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3500, global step 56613: learning rate 0.0005
[2019-04-03 21:57:46,985] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3500, global step 56738: loss 1.1136
[2019-04-03 21:57:46,986] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3500, global step 56738: learning rate 0.0005
[2019-04-03 21:57:47,289] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3500, global step 56801: loss 1.1972
[2019-04-03 21:57:47,290] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3500, global step 56801: learning rate 0.0005
[2019-04-03 21:57:47,793] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3500, global step 56886: loss 1.1606
[2019-04-03 21:57:47,800] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3500, global step 56886: learning rate 0.0005
[2019-04-03 21:57:48,469] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3500, global step 56988: loss 1.2173
[2019-04-03 21:57:48,473] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3500, global step 56988: learning rate 0.0005
[2019-04-03 21:57:55,285] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.8720508e-20 1.6188451e-10 5.0966509e-08 8.5234159e-10 5.0904042e-08
 1.0000000e+00 1.6507902e-13], sum to 1.0000
[2019-04-03 21:57:55,285] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3730
[2019-04-03 21:57:55,312] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.9, 78.33333333333334, 0.0, 0.0, 26.0, 24.30472019758839, 0.1492212143056924, 0.0, 1.0, 42594.85055250865], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2166000.0000, 
sim time next is 2166600.0000, 
raw observation next is [-6.800000000000001, 78.16666666666666, 0.0, 0.0, 26.0, 24.27622089527996, 0.1490203256972489, 0.0, 1.0, 42612.04233808369], 
processed observation next is [1.0, 0.043478260869565216, 0.2742382271468144, 0.7816666666666666, 0.0, 0.0, 0.6666666666666666, 0.5230184079399965, 0.549673441899083, 0.0, 1.0, 0.20291448732420803], 
reward next is 0.7971, 
noisyNet noise sample is [array([0.6093931], dtype=float32), 1.5226696]. 
=============================================
[2019-04-03 21:58:05,089] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.1445805e-19 1.7385138e-10 1.8762446e-08 5.9314789e-11 1.6548107e-08
 1.0000000e+00 1.9068662e-13], sum to 1.0000
[2019-04-03 21:58:05,089] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6578
[2019-04-03 21:58:05,172] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 68.0, 0.0, 0.0, 26.0, 25.24302040476955, 0.4156225489422677, 0.0, 1.0, 47026.5790330975], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2235600.0000, 
sim time next is 2236200.0000, 
raw observation next is [-5.100000000000001, 68.5, 0.0, 0.0, 26.0, 25.28257068139613, 0.4137176416608617, 0.0, 1.0, 45955.16680279129], 
processed observation next is [1.0, 0.9130434782608695, 0.32132963988919666, 0.685, 0.0, 0.0, 0.6666666666666666, 0.6068808901163442, 0.6379058805536205, 0.0, 1.0, 0.21883412763233948], 
reward next is 0.7812, 
noisyNet noise sample is [array([1.3190421], dtype=float32), -0.04745185]. 
=============================================
[2019-04-03 21:58:17,208] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.1898624e-20 1.2251912e-10 8.7270111e-09 1.0939606e-09 7.0833039e-08
 9.9999988e-01 8.8484382e-14], sum to 1.0000
[2019-04-03 21:58:17,209] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1580
[2019-04-03 21:58:17,282] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.09999999999999999, 46.66666666666667, 81.33333333333333, 152.3333333333333, 26.0, 24.9896586896549, 0.2918495943636563, 0.0, 1.0, 36720.25447046908], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2391000.0000, 
sim time next is 2391600.0000, 
raw observation next is [-0.2, 46.33333333333334, 80.16666666666667, 105.1666666666667, 26.0, 24.96868696804399, 0.2869227872639658, 0.0, 1.0, 47496.06029924707], 
processed observation next is [0.0, 0.6956521739130435, 0.4570637119113574, 0.46333333333333343, 0.26722222222222225, 0.11620626151012894, 0.6666666666666666, 0.5807239140036659, 0.5956409290879886, 0.0, 1.0, 0.22617171571070033], 
reward next is 0.7738, 
noisyNet noise sample is [array([1.5386945], dtype=float32), 0.8195619]. 
=============================================
[2019-04-03 21:58:18,697] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4000, global step 62501: loss 0.2174
[2019-04-03 21:58:18,698] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4000, global step 62502: learning rate 0.0005
[2019-04-03 21:58:19,982] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4000, global step 62770: loss 0.1616
[2019-04-03 21:58:19,982] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4000, global step 62770: learning rate 0.0005
[2019-04-03 21:58:20,657] A3C_AGENT_WORKER-Thread-11 INFO:Local step 4000, global step 62905: loss 0.1741
[2019-04-03 21:58:20,747] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 4000, global step 62922: learning rate 0.0005
[2019-04-03 21:58:20,975] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4000, global step 62970: loss 0.2073
[2019-04-03 21:58:20,975] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4000, global step 62970: learning rate 0.0005
[2019-04-03 21:58:22,096] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.1144214e-23 3.0739885e-12 6.8362804e-10 1.5706455e-11 1.3380346e-09
 1.0000000e+00 1.1963047e-15], sum to 1.0000
[2019-04-03 21:58:22,101] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5764
[2019-04-03 21:58:22,324] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.7, 55.33333333333333, 0.0, 0.0, 26.0, 25.40240332550685, 0.4230556299071875, 0.0, 1.0, 51787.16157976544], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2324400.0000, 
sim time next is 2325000.0000, 
raw observation next is [-1.7, 55.66666666666667, 0.0, 0.0, 26.0, 25.3721759175675, 0.4236471988311393, 0.0, 1.0, 60818.98671828004], 
processed observation next is [1.0, 0.9130434782608695, 0.4155124653739613, 0.5566666666666668, 0.0, 0.0, 0.6666666666666666, 0.6143479931306249, 0.6412157329437131, 0.0, 1.0, 0.28961422246800017], 
reward next is 0.7104, 
noisyNet noise sample is [array([-1.7938111], dtype=float32), -0.38066885]. 
=============================================
[2019-04-03 21:58:22,431] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[80.82168]
 [80.75235]
 [80.71194]
 [81.06478]
 [81.28295]], R is [[81.06459045]
 [81.00733948]
 [81.04356384]
 [81.14379883]
 [81.24301147]].
[2019-04-03 21:58:23,213] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4000, global step 63461: loss 0.1663
[2019-04-03 21:58:23,223] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4000, global step 63461: learning rate 0.0005
[2019-04-03 21:58:23,804] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4000, global step 63577: loss 0.1320
[2019-04-03 21:58:23,804] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4000, global step 63577: learning rate 0.0005
[2019-04-03 21:58:26,272] A3C_AGENT_WORKER-Thread-6 INFO:Local step 4000, global step 63987: loss 0.1006
[2019-04-03 21:58:26,273] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 4000, global step 63987: learning rate 0.0005
[2019-04-03 21:58:27,690] A3C_AGENT_WORKER-Thread-4 INFO:Local step 4000, global step 64219: loss 0.0549
[2019-04-03 21:58:27,693] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 4000, global step 64219: learning rate 0.0005
[2019-04-03 21:58:28,827] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4000, global step 64424: loss 0.0020
[2019-04-03 21:58:28,827] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4000, global step 64424: learning rate 0.0005
[2019-04-03 21:58:29,344] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4000, global step 64506: loss 0.0084
[2019-04-03 21:58:29,358] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4000, global step 64506: learning rate 0.0005
[2019-04-03 21:58:29,729] A3C_AGENT_WORKER-Thread-10 INFO:Local step 4000, global step 64573: loss 0.0067
[2019-04-03 21:58:29,729] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 4000, global step 64573: learning rate 0.0005
[2019-04-03 21:58:30,180] A3C_AGENT_WORKER-Thread-5 INFO:Local step 4000, global step 64655: loss 0.0004
[2019-04-03 21:58:30,182] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 4000, global step 64655: learning rate 0.0005
[2019-04-03 21:58:31,877] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4000, global step 64959: loss 0.0131
[2019-04-03 21:58:31,884] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4000, global step 64959: learning rate 0.0005
[2019-04-03 21:58:31,944] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4000, global step 64969: loss 0.0058
[2019-04-03 21:58:31,944] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4000, global step 64969: loss 0.0052
[2019-04-03 21:58:31,944] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4000, global step 64969: learning rate 0.0005
[2019-04-03 21:58:31,945] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4000, global step 64969: learning rate 0.0005
[2019-04-03 21:58:32,999] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4000, global step 65189: loss 0.0075
[2019-04-03 21:58:33,042] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4000, global step 65195: learning rate 0.0005
[2019-04-03 21:58:37,944] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.3065224e-20 6.8406586e-10 3.2680461e-07 4.1216071e-09 2.5363420e-07
 9.9999940e-01 1.2606595e-12], sum to 1.0000
[2019-04-03 21:58:37,945] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7631
[2019-04-03 21:58:38,289] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.016666666666667, 48.83333333333334, 54.0, 582.0, 26.0, 25.21421835214917, 0.2521129642014204, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2452200.0000, 
sim time next is 2452800.0000, 
raw observation next is [-6.733333333333333, 47.66666666666667, 57.5, 623.5, 26.0, 25.29693728395682, 0.2570157779971627, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.2760849492151431, 0.47666666666666674, 0.19166666666666668, 0.6889502762430939, 0.6666666666666666, 0.6080781069964015, 0.5856719259990543, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.08229618], dtype=float32), 0.009040223]. 
=============================================
[2019-04-03 21:58:43,352] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.860985e-21 6.469950e-11 3.888488e-08 6.808046e-09 5.972162e-08
 9.999999e-01 9.294492e-14], sum to 1.0000
[2019-04-03 21:58:43,352] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5981
[2019-04-03 21:58:43,383] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.7, 38.0, 0.0, 0.0, 26.0, 24.97821870063637, 0.1891552842617701, 0.0, 1.0, 38901.04798505783], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2512800.0000, 
sim time next is 2513400.0000, 
raw observation next is [-1.7, 39.0, 0.0, 0.0, 26.0, 24.95760769203838, 0.1971406201867428, 0.0, 1.0, 38844.48013329619], 
processed observation next is [1.0, 0.08695652173913043, 0.4155124653739613, 0.39, 0.0, 0.0, 0.6666666666666666, 0.5798006410031983, 0.5657135400622476, 0.0, 1.0, 0.18497371492045803], 
reward next is 0.8150, 
noisyNet noise sample is [array([-0.40226266], dtype=float32), 1.3388016]. 
=============================================
[2019-04-03 21:58:58,999] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4500, global step 70426: loss 0.7972
[2019-04-03 21:58:59,000] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4500, global step 70426: learning rate 0.0005
[2019-04-03 21:58:59,563] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4500, global step 70538: loss 0.4579
[2019-04-03 21:58:59,577] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4500, global step 70538: learning rate 0.0005
[2019-04-03 21:59:00,352] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4500, global step 70658: loss 0.4322
[2019-04-03 21:59:00,375] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4500, global step 70658: learning rate 0.0005
[2019-04-03 21:59:01,995] A3C_AGENT_WORKER-Thread-11 INFO:Local step 4500, global step 70940: loss 0.3301
[2019-04-03 21:59:01,997] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 4500, global step 70940: learning rate 0.0005
[2019-04-03 21:59:03,076] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4500, global step 71125: loss 0.5006
[2019-04-03 21:59:03,085] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4500, global step 71125: learning rate 0.0005
[2019-04-03 21:59:03,189] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.3107066e-22 5.5139081e-11 3.4648036e-09 1.8495858e-11 8.4370635e-09
 1.0000000e+00 2.1219399e-15], sum to 1.0000
[2019-04-03 21:59:03,189] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7947
[2019-04-03 21:59:03,450] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.833333333333334, 64.0, 113.6666666666667, 745.3333333333334, 26.0, 26.07071143929954, 0.4774139342445959, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2718600.0000, 
sim time next is 2719200.0000, 
raw observation next is [-8.666666666666668, 64.0, 112.8333333333333, 763.1666666666667, 26.0, 26.04552730193993, 0.4791306487238304, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.22253000923361033, 0.64, 0.376111111111111, 0.8432780847145489, 0.6666666666666666, 0.6704606084949942, 0.6597102162412768, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3890837], dtype=float32), -0.17461655]. 
=============================================
[2019-04-03 21:59:05,872] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4500, global step 71714: loss 0.3781
[2019-04-03 21:59:05,873] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4500, global step 71714: learning rate 0.0005
[2019-04-03 21:59:07,222] A3C_AGENT_WORKER-Thread-4 INFO:Local step 4500, global step 72009: loss 0.1144
[2019-04-03 21:59:07,251] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 4500, global step 72009: learning rate 0.0005
[2019-04-03 21:59:07,465] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4500, global step 72055: loss 0.0533
[2019-04-03 21:59:07,465] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4500, global step 72055: learning rate 0.0005
[2019-04-03 21:59:07,621] A3C_AGENT_WORKER-Thread-6 INFO:Local step 4500, global step 72078: loss 0.0624
[2019-04-03 21:59:07,624] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 4500, global step 72078: learning rate 0.0005
[2019-04-03 21:59:08,715] A3C_AGENT_WORKER-Thread-5 INFO:Local step 4500, global step 72290: loss 0.0069
[2019-04-03 21:59:08,749] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 4500, global step 72290: learning rate 0.0005
[2019-04-03 21:59:09,333] A3C_AGENT_WORKER-Thread-10 INFO:Local step 4500, global step 72425: loss 0.0348
[2019-04-03 21:59:09,335] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 4500, global step 72425: learning rate 0.0005
[2019-04-03 21:59:10,459] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4500, global step 72647: loss 0.0124
[2019-04-03 21:59:10,460] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4500, global step 72647: learning rate 0.0005
[2019-04-03 21:59:10,551] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4500, global step 72675: loss 0.0526
[2019-04-03 21:59:10,625] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4500, global step 72675: learning rate 0.0005
[2019-04-03 21:59:11,102] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4500, global step 72779: loss 0.0698
[2019-04-03 21:59:11,127] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4500, global step 72779: learning rate 0.0005
[2019-04-03 21:59:12,479] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4500, global step 73018: loss 0.1600
[2019-04-03 21:59:12,480] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4500, global step 73018: learning rate 0.0005
[2019-04-03 21:59:12,563] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4500, global step 73027: loss 0.2195
[2019-04-03 21:59:12,571] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4500, global step 73027: learning rate 0.0005
[2019-04-03 21:59:17,310] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.3332730e-18 7.2916131e-09 1.9913937e-06 1.5990533e-09 2.5837424e-05
 9.9997211e-01 7.3495723e-13], sum to 1.0000
[2019-04-03 21:59:17,310] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0840
[2019-04-03 21:59:17,348] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.5, 61.5, 0.0, 0.0, 26.0, 25.20103169256716, 0.441489642560388, 0.0, 1.0, 112343.5338559846], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2752200.0000, 
sim time next is 2752800.0000, 
raw observation next is [-5.666666666666667, 62.33333333333333, 0.0, 0.0, 26.0, 25.26604262793023, 0.454393174125205, 0.0, 1.0, 74458.3736824409], 
processed observation next is [1.0, 0.8695652173913043, 0.30563250230840255, 0.6233333333333333, 0.0, 0.0, 0.6666666666666666, 0.6055035523275191, 0.6514643913750683, 0.0, 1.0, 0.3545636842020995], 
reward next is 0.6454, 
noisyNet noise sample is [array([-0.5559447], dtype=float32), 2.2298746]. 
=============================================
[2019-04-03 21:59:19,018] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.12262729e-18 3.28671246e-09 3.77461911e-05 7.84255718e-08
 4.93669359e-05 9.99912858e-01 1.05081195e-11], sum to 1.0000
[2019-04-03 21:59:19,018] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1928
[2019-04-03 21:59:19,078] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.1434050e-17 4.6850936e-09 2.1669462e-06 4.7281752e-09 2.7970227e-06
 9.9999511e-01 4.4500133e-12], sum to 1.0000
[2019-04-03 21:59:19,079] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5907
[2019-04-03 21:59:19,104] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.0, 64.0, 0.0, 0.0, 26.0, 24.32543854232649, 0.1143977375789343, 0.0, 1.0, 41105.57512124209], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2783400.0000, 
sim time next is 2784000.0000, 
raw observation next is [-7.0, 64.0, 0.0, 0.0, 26.0, 24.31845036407891, 0.111353418757714, 0.0, 1.0, 41157.98634539922], 
processed observation next is [1.0, 0.21739130434782608, 0.2686980609418283, 0.64, 0.0, 0.0, 0.6666666666666666, 0.5265375303399091, 0.5371178062525713, 0.0, 1.0, 0.19599041116856772], 
reward next is 0.8040, 
noisyNet noise sample is [array([-0.17025498], dtype=float32), -1.7094152]. 
=============================================
[2019-04-03 21:59:19,161] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 44.00000000000001, 0.0, 0.0, 26.0, 25.02078361069814, 0.3171562581787732, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2837400.0000, 
sim time next is 2838000.0000, 
raw observation next is [2.0, 44.0, 0.0, 0.0, 26.0, 24.94933142044419, 0.3173390701900599, 0.0, 1.0, 198420.1055810666], 
processed observation next is [1.0, 0.8695652173913043, 0.518005540166205, 0.44, 0.0, 0.0, 0.6666666666666666, 0.5791109517036824, 0.6057796900633533, 0.0, 1.0, 0.9448576456241267], 
reward next is 0.0551, 
noisyNet noise sample is [array([-0.40411085], dtype=float32), -1.5498414]. 
=============================================
[2019-04-03 21:59:19,199] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[69.593994]
 [69.70538 ]
 [69.81824 ]
 [69.90932 ]
 [69.983894]], R is [[69.5994873 ]
 [69.70775604]
 [69.81534576]
 [69.9223175 ]
 [70.02858734]].
[2019-04-03 21:59:19,265] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[65.318245]
 [66.03692 ]
 [66.82341 ]
 [68.40486 ]
 [69.73078 ]], R is [[65.38555145]
 [65.73169708]
 [66.07437897]
 [66.41363525]
 [66.74949646]].
[2019-04-03 21:59:25,219] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.2038281e-19 2.1482165e-09 7.6690770e-07 7.8983570e-10 7.0223659e-06
 9.9999225e-01 6.4139978e-14], sum to 1.0000
[2019-04-03 21:59:25,229] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4895
[2019-04-03 21:59:25,245] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.866666666666667, 24.33333333333333, 98.83333333333333, 0.0, 26.0, 25.46238541603329, 0.3684546280589625, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2820000.0000, 
sim time next is 2820600.0000, 
raw observation next is [6.8, 24.5, 95.0, 0.0, 26.0, 25.66029375194089, 0.3819403165734907, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.6509695290858727, 0.245, 0.31666666666666665, 0.0, 0.6666666666666666, 0.638357812661741, 0.6273134388578302, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.6286335], dtype=float32), 0.4093768]. 
=============================================
[2019-04-03 21:59:26,772] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.4480696e-21 7.4380502e-11 8.0385362e-08 9.8941182e-11 2.2401444e-07
 9.9999964e-01 5.9223642e-15], sum to 1.0000
[2019-04-03 21:59:26,775] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6918
[2019-04-03 21:59:26,796] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.3, 26.5, 71.0, 76.0, 26.0, 24.8075375082347, 0.3045047193670034, 1.0, 1.0, 177996.0085079917], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2824200.0000, 
sim time next is 2824800.0000, 
raw observation next is [6.2, 27.0, 60.00000000000001, 71.0, 26.0, 25.09708216911681, 0.3468933364565815, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.6343490304709142, 0.27, 0.2, 0.07845303867403315, 0.6666666666666666, 0.5914235140930675, 0.6156311121521938, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.491964], dtype=float32), -0.10566163]. 
=============================================
[2019-04-03 21:59:28,451] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.79592599e-21 2.08051160e-10 1.14003626e-06 4.39833142e-10
 1.06938344e-07 9.99998689e-01 6.72556154e-14], sum to 1.0000
[2019-04-03 21:59:28,451] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9570
[2019-04-03 21:59:28,474] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 44.0, 0.0, 0.0, 26.0, 24.94182164581336, 0.3770510129656441, 0.0, 1.0, 113291.3175873807], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2839200.0000, 
sim time next is 2839800.0000, 
raw observation next is [2.0, 44.0, 0.0, 0.0, 26.0, 25.09097901432423, 0.3975566317161953, 0.0, 1.0, 71025.6547611692], 
processed observation next is [1.0, 0.8695652173913043, 0.518005540166205, 0.44, 0.0, 0.0, 0.6666666666666666, 0.5909149178603524, 0.6325188772387318, 0.0, 1.0, 0.3382174036246152], 
reward next is 0.6618, 
noisyNet noise sample is [array([-1.3159634], dtype=float32), -0.48610723]. 
=============================================
[2019-04-03 21:59:34,989] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1543833e-19 4.8676996e-10 1.1349547e-04 4.8387722e-10 2.6193868e-06
 9.9988389e-01 1.6830028e-13], sum to 1.0000
[2019-04-03 21:59:34,989] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3357
[2019-04-03 21:59:35,002] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 96.5, 78.0, 0.0, 26.0, 25.42653144950308, 0.3102101797759229, 1.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2889000.0000, 
sim time next is 2889600.0000, 
raw observation next is [0.6666666666666666, 95.33333333333334, 79.5, 0.0, 26.0, 25.42665634380329, 0.313184858875852, 1.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.43478260869565216, 0.4810710987996307, 0.9533333333333335, 0.265, 0.0, 0.6666666666666666, 0.6188880286502743, 0.6043949529586173, 1.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.62397987], dtype=float32), -1.7549498]. 
=============================================
[2019-04-03 21:59:37,040] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5000, global step 78689: loss 0.5647
[2019-04-03 21:59:37,040] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5000, global step 78689: learning rate 0.0005
[2019-04-03 21:59:37,547] A3C_AGENT_WORKER-Thread-12 INFO:Local step 5000, global step 78779: loss 0.6371
[2019-04-03 21:59:37,549] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 5000, global step 78780: learning rate 0.0005
[2019-04-03 21:59:37,883] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5000, global step 78843: loss 0.5307
[2019-04-03 21:59:37,884] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5000, global step 78843: learning rate 0.0005
[2019-04-03 21:59:39,087] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5000, global step 79080: loss 0.5820
[2019-04-03 21:59:39,088] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5000, global step 79080: learning rate 0.0005
[2019-04-03 21:59:40,819] A3C_AGENT_WORKER-Thread-11 INFO:Local step 5000, global step 79427: loss 0.8343
[2019-04-03 21:59:40,820] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 5000, global step 79427: learning rate 0.0005
[2019-04-03 21:59:42,691] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5000, global step 79859: loss 0.7041
[2019-04-03 21:59:42,692] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5000, global step 79859: learning rate 0.0005
[2019-04-03 21:59:43,701] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5000, global step 80045: loss 0.8095
[2019-04-03 21:59:43,701] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5000, global step 80045: learning rate 0.0005
[2019-04-03 21:59:44,007] A3C_AGENT_WORKER-Thread-4 INFO:Local step 5000, global step 80070: loss 1.0196
[2019-04-03 21:59:44,012] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 5000, global step 80070: learning rate 0.0005
[2019-04-03 21:59:44,525] A3C_AGENT_WORKER-Thread-6 INFO:Local step 5000, global step 80128: loss 0.9652
[2019-04-03 21:59:44,525] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 5000, global step 80128: learning rate 0.0005
[2019-04-03 21:59:45,716] A3C_AGENT_WORKER-Thread-5 INFO:Local step 5000, global step 80332: loss 1.1091
[2019-04-03 21:59:45,716] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 5000, global step 80332: learning rate 0.0005
[2019-04-03 21:59:46,840] A3C_AGENT_WORKER-Thread-10 INFO:Local step 5000, global step 80519: loss 1.7574
[2019-04-03 21:59:46,843] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 5000, global step 80519: learning rate 0.0005
[2019-04-03 21:59:48,568] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5000, global step 80890: loss 1.2951
[2019-04-03 21:59:48,595] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5000, global step 80890: learning rate 0.0005
[2019-04-03 21:59:49,006] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5000, global step 80948: loss 1.2778
[2019-04-03 21:59:49,018] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5000, global step 80948: learning rate 0.0005
[2019-04-03 21:59:49,392] A3C_AGENT_WORKER-Thread-13 INFO:Local step 5000, global step 80994: loss 1.2983
[2019-04-03 21:59:49,392] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 5000, global step 80994: learning rate 0.0005
[2019-04-03 21:59:49,585] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5000, global step 81007: loss 1.2568
[2019-04-03 21:59:49,598] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5000, global step 81007: learning rate 0.0005
[2019-04-03 21:59:50,017] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5000, global step 81064: loss 0.8593
[2019-04-03 21:59:50,017] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5000, global step 81064: learning rate 0.0005
[2019-04-03 21:59:53,762] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.8761320e-20 7.1098474e-09 1.3801590e-05 1.2554606e-09 9.0305798e-07
 9.9998522e-01 6.1221541e-13], sum to 1.0000
[2019-04-03 21:59:53,765] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1128
[2019-04-03 21:59:53,825] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.3333333333333334, 40.0, 96.5, 758.0, 26.0, 25.11646513623975, 0.3630527174207967, 0.0, 1.0, 18703.9667681089], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3076800.0000, 
sim time next is 3077400.0000, 
raw observation next is [-0.1666666666666666, 39.5, 94.0, 741.0, 26.0, 25.11740121887014, 0.3638207082200542, 0.0, 1.0, 18702.78602316953], 
processed observation next is [0.0, 0.6086956521739131, 0.4579870729455217, 0.395, 0.31333333333333335, 0.8187845303867404, 0.6666666666666666, 0.5931167682391782, 0.6212735694066848, 0.0, 1.0, 0.08906088582461681], 
reward next is 0.9109, 
noisyNet noise sample is [array([-2.4083679], dtype=float32), 0.27679676]. 
=============================================
[2019-04-03 22:00:09,137] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.7847590e-18 1.3680005e-07 1.6754062e-05 1.0077200e-08 2.0953426e-05
 9.9996209e-01 1.4714683e-12], sum to 1.0000
[2019-04-03 22:00:09,144] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4825
[2019-04-03 22:00:09,183] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 80.33333333333333, 114.3333333333333, 821.1666666666667, 26.0, 26.59048569934307, 0.7463084053744781, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3242400.0000, 
sim time next is 3243000.0000, 
raw observation next is [-2.0, 82.66666666666667, 113.6666666666667, 819.3333333333334, 26.0, 26.53444377149907, 0.621362246945949, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.40720221606648205, 0.8266666666666667, 0.378888888888889, 0.905340699815838, 0.6666666666666666, 0.7112036476249225, 0.707120748981983, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.07524352], dtype=float32), 0.89579415]. 
=============================================
[2019-04-03 22:00:09,233] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[73.2088  ]
 [73.30702 ]
 [73.4276  ]
 [73.56279 ]
 [73.719574]], R is [[73.32977295]
 [73.59647369]
 [73.86051178]
 [74.1219101 ]
 [74.38069153]].
[2019-04-03 22:00:10,469] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5500, global step 86355: loss 0.0091
[2019-04-03 22:00:10,471] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5500, global step 86355: learning rate 0.0005
[2019-04-03 22:00:10,769] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.5884167e-19 4.5636619e-09 3.0187119e-04 7.9093709e-09 7.8880316e-07
 9.9969733e-01 1.3294451e-12], sum to 1.0000
[2019-04-03 22:00:10,774] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7705
[2019-04-03 22:00:10,810] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 93.0, 0.0, 0.0, 26.0, 25.54491094203361, 0.5940747115436943, 0.0, 1.0, 41280.66613665034], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3196800.0000, 
sim time next is 3197400.0000, 
raw observation next is [1.833333333333333, 94.16666666666666, 0.0, 0.0, 26.0, 25.50400213273945, 0.5894911390414177, 0.0, 1.0, 58319.58770387038], 
processed observation next is [1.0, 0.0, 0.5133887349953832, 0.9416666666666665, 0.0, 0.0, 0.6666666666666666, 0.6253335110616209, 0.6964970463471393, 0.0, 1.0, 0.27771232239938276], 
reward next is 0.7223, 
noisyNet noise sample is [array([-0.27666453], dtype=float32), 0.28490734]. 
=============================================
[2019-04-03 22:00:11,136] A3C_AGENT_WORKER-Thread-12 INFO:Local step 5500, global step 86555: loss -0.1472
[2019-04-03 22:00:11,137] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 5500, global step 86555: learning rate 0.0005
[2019-04-03 22:00:12,437] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5500, global step 86844: loss -0.0020
[2019-04-03 22:00:12,444] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5500, global step 86845: learning rate 0.0005
[2019-04-03 22:00:12,640] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5500, global step 86896: loss 0.0365
[2019-04-03 22:00:12,667] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5500, global step 86896: learning rate 0.0005
[2019-04-03 22:00:14,763] A3C_AGENT_WORKER-Thread-11 INFO:Local step 5500, global step 87361: loss 0.0262
[2019-04-03 22:00:14,764] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 5500, global step 87361: learning rate 0.0005
[2019-04-03 22:00:15,086] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5500, global step 87460: loss -0.0561
[2019-04-03 22:00:15,088] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5500, global step 87460: learning rate 0.0005
[2019-04-03 22:00:15,938] A3C_AGENT_WORKER-Thread-6 INFO:Local step 5500, global step 87716: loss -0.0007
[2019-04-03 22:00:15,955] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 5500, global step 87716: learning rate 0.0005
[2019-04-03 22:00:16,325] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5500, global step 87828: loss 0.0054
[2019-04-03 22:00:16,325] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5500, global step 87828: learning rate 0.0005
[2019-04-03 22:00:17,634] A3C_AGENT_WORKER-Thread-4 INFO:Local step 5500, global step 88151: loss 0.2312
[2019-04-03 22:00:17,643] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 5500, global step 88154: learning rate 0.0005
[2019-04-03 22:00:18,855] A3C_AGENT_WORKER-Thread-10 INFO:Local step 5500, global step 88445: loss -0.8786
[2019-04-03 22:00:18,904] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 5500, global step 88445: learning rate 0.0005
[2019-04-03 22:00:19,052] A3C_AGENT_WORKER-Thread-5 INFO:Local step 5500, global step 88502: loss -0.0943
[2019-04-03 22:00:19,053] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 5500, global step 88502: learning rate 0.0005
[2019-04-03 22:00:20,602] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5500, global step 88899: loss 0.0177
[2019-04-03 22:00:20,603] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5500, global step 88899: learning rate 0.0005
[2019-04-03 22:00:21,045] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5500, global step 89018: loss 0.0065
[2019-04-03 22:00:21,046] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5500, global step 89018: learning rate 0.0005
[2019-04-03 22:00:21,050] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5500, global step 89019: loss 0.0255
[2019-04-03 22:00:21,052] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5500, global step 89019: learning rate 0.0005
[2019-04-03 22:00:21,104] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5500, global step 89046: loss 0.0841
[2019-04-03 22:00:21,105] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5500, global step 89046: learning rate 0.0005
[2019-04-03 22:00:21,185] A3C_AGENT_WORKER-Thread-13 INFO:Local step 5500, global step 89079: loss 0.0425
[2019-04-03 22:00:21,246] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 5500, global step 89099: learning rate 0.0005
[2019-04-03 22:00:27,844] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.8815663e-20 6.6182984e-09 9.5694895e-06 1.1970865e-09 1.1599441e-07
 9.9999034e-01 3.9787246e-13], sum to 1.0000
[2019-04-03 22:00:27,848] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9010
[2019-04-03 22:00:27,878] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.5099978071045, 0.5363975356470367, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3358800.0000, 
sim time next is 3359400.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.5937996787222, 0.5329860277394621, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6328166398935166, 0.6776620092464873, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2852211], dtype=float32), -0.047753602]. 
=============================================
[2019-04-03 22:00:39,033] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6000, global step 94269: loss 0.0316
[2019-04-03 22:00:39,033] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6000, global step 94269: learning rate 0.0005
[2019-04-03 22:00:39,883] A3C_AGENT_WORKER-Thread-12 INFO:Local step 6000, global step 94561: loss 0.0162
[2019-04-03 22:00:39,887] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 6000, global step 94561: learning rate 0.0005
[2019-04-03 22:00:40,628] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6000, global step 94806: loss 0.0162
[2019-04-03 22:00:40,630] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6000, global step 94806: learning rate 0.0005
[2019-04-03 22:00:40,934] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.2931717e-17 1.4990064e-08 5.1713143e-02 2.0439644e-05 2.2940552e-05
 9.4824344e-01 1.5126388e-10], sum to 1.0000
[2019-04-03 22:00:40,935] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7975
[2019-04-03 22:00:41,106] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 68.0, 0.0, 0.0, 26.0, 24.95228934502558, 0.3382701830010058, 0.0, 1.0, 41003.08071104908], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3558600.0000, 
sim time next is 3559200.0000, 
raw observation next is [-4.666666666666666, 67.0, 0.0, 0.0, 26.0, 24.94728333469578, 0.3311399763821865, 0.0, 1.0, 40948.34480770417], 
processed observation next is [0.0, 0.17391304347826086, 0.33333333333333337, 0.67, 0.0, 0.0, 0.6666666666666666, 0.578940277891315, 0.6103799921273955, 0.0, 1.0, 0.1949921181319246], 
reward next is 0.8050, 
noisyNet noise sample is [array([1.2590172], dtype=float32), -0.007872718]. 
=============================================
[2019-04-03 22:00:41,780] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6000, global step 95166: loss -0.0114
[2019-04-03 22:00:41,783] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6000, global step 95167: learning rate 0.0005
[2019-04-03 22:00:42,063] A3C_AGENT_WORKER-Thread-11 INFO:Local step 6000, global step 95256: loss -0.5059
[2019-04-03 22:00:42,064] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 6000, global step 95256: learning rate 0.0005
[2019-04-03 22:00:43,027] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6000, global step 95556: loss 0.0124
[2019-04-03 22:00:43,029] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6000, global step 95559: learning rate 0.0005
[2019-04-03 22:00:43,256] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.3674336e-17 1.0425049e-06 5.7745539e-03 7.6517435e-06 3.2941455e-06
 9.9421340e-01 5.7912408e-11], sum to 1.0000
[2019-04-03 22:00:43,259] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8804
[2019-04-03 22:00:43,310] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.666666666666667, 54.33333333333334, 113.5, 806.3333333333333, 26.0, 25.24407800834071, 0.4343784704363613, 0.0, 1.0, 18709.55539026789], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3583200.0000, 
sim time next is 3583800.0000, 
raw observation next is [-3.5, 54.5, 114.0, 816.0, 26.0, 25.1866242110592, 0.4319047297631146, 0.0, 1.0, 33688.4761138659], 
processed observation next is [0.0, 0.4782608695652174, 0.36565096952908593, 0.545, 0.38, 0.901657458563536, 0.6666666666666666, 0.5988853509216, 0.6439682432543715, 0.0, 1.0, 0.16042131482793287], 
reward next is 0.8396, 
noisyNet noise sample is [array([-0.8127595], dtype=float32), 0.48175284]. 
=============================================
[2019-04-03 22:00:43,506] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.5586366e-17 1.2882740e-06 2.2553323e-02 2.6749264e-04 9.7763550e-05
 9.7708011e-01 5.1113702e-10], sum to 1.0000
[2019-04-03 22:00:43,508] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2577
[2019-04-03 22:00:43,519] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.833333333333332, 27.33333333333333, 0.0, 0.0, 26.0, 25.47495353517648, 0.3644652545696934, 0.0, 1.0, 22431.68340246523], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3646200.0000, 
sim time next is 3646800.0000, 
raw observation next is [9.0, 27.0, 0.0, 0.0, 26.0, 25.51528887004728, 0.3639663083015345, 0.0, 1.0, 18751.02241748978], 
processed observation next is [0.0, 0.21739130434782608, 0.7119113573407203, 0.27, 0.0, 0.0, 0.6666666666666666, 0.6262740725039398, 0.6213221027671781, 0.0, 1.0, 0.08929058294042753], 
reward next is 0.9107, 
noisyNet noise sample is [array([0.2678631], dtype=float32), -0.5141407]. 
=============================================
[2019-04-03 22:00:44,242] A3C_AGENT_WORKER-Thread-6 INFO:Local step 6000, global step 95958: loss 0.1125
[2019-04-03 22:00:44,245] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 6000, global step 95960: learning rate 0.0005
[2019-04-03 22:00:44,701] A3C_AGENT_WORKER-Thread-4 INFO:Local step 6000, global step 96103: loss 0.0403
[2019-04-03 22:00:44,704] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 6000, global step 96103: learning rate 0.0005
[2019-04-03 22:00:44,801] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6000, global step 96153: loss 0.0109
[2019-04-03 22:00:44,803] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6000, global step 96154: learning rate 0.0005
[2019-04-03 22:00:45,545] A3C_AGENT_WORKER-Thread-10 INFO:Local step 6000, global step 96389: loss 0.0012
[2019-04-03 22:00:45,546] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 6000, global step 96389: learning rate 0.0005
[2019-04-03 22:00:45,963] A3C_AGENT_WORKER-Thread-5 INFO:Local step 6000, global step 96506: loss 0.0024
[2019-04-03 22:00:45,965] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 6000, global step 96506: learning rate 0.0005
[2019-04-03 22:00:47,684] A3C_AGENT_WORKER-Thread-13 INFO:Local step 6000, global step 96999: loss 0.0035
[2019-04-03 22:00:47,685] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 6000, global step 97000: learning rate 0.0005
[2019-04-03 22:00:48,265] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6000, global step 97201: loss 0.0056
[2019-04-03 22:00:48,267] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6000, global step 97202: learning rate 0.0005
[2019-04-03 22:00:48,597] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6000, global step 97324: loss -0.0005
[2019-04-03 22:00:48,597] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6000, global step 97324: learning rate 0.0005
[2019-04-03 22:00:48,714] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6000, global step 97362: loss 0.0034
[2019-04-03 22:00:48,716] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6000, global step 97362: learning rate 0.0005
[2019-04-03 22:00:49,299] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6000, global step 97539: loss 0.0074
[2019-04-03 22:00:49,299] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6000, global step 97539: learning rate 0.0005
[2019-04-03 22:00:51,387] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.8509976e-19 3.9554942e-07 9.1921883e-03 1.1370680e-05 7.4393024e-06
 9.9078864e-01 7.1337880e-11], sum to 1.0000
[2019-04-03 22:00:51,388] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4652
[2019-04-03 22:00:51,418] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.0, 25.0, 0.0, 0.0, 26.0, 25.49825325382211, 0.3709744615595101, 0.0, 1.0, 27958.96205084966], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3635400.0000, 
sim time next is 3636000.0000, 
raw observation next is [9.0, 25.0, 0.0, 0.0, 26.0, 25.50871244719762, 0.3698028217957272, 0.0, 1.0, 23400.98856363521], 
processed observation next is [0.0, 0.08695652173913043, 0.7119113573407203, 0.25, 0.0, 0.0, 0.6666666666666666, 0.6257260372664684, 0.6232676072652424, 0.0, 1.0, 0.11143327887445337], 
reward next is 0.8886, 
noisyNet noise sample is [array([-0.37977505], dtype=float32), -1.5174121]. 
=============================================
[2019-04-03 22:00:51,437] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[89.462036]
 [89.46572 ]
 [89.33744 ]
 [88.95572 ]
 [88.47038 ]], R is [[89.39442444]
 [89.36734009]
 [89.29366302]
 [89.16086578]
 [88.98986816]].
[2019-04-03 22:00:56,780] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-03 22:00:56,781] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-03 22:00:56,781] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:00:56,781] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-03 22:00:56,783] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run2
[2019-04-03 22:00:56,784] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-03 22:00:56,802] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:00:56,807] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run2
[2019-04-03 22:00:56,808] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:00:56,841] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run2
[2019-04-03 22:01:17,701] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.23437133], dtype=float32), 0.16893505]
[2019-04-03 22:01:17,701] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-9.133605770666668, 66.05329349333334, 0.0, 0.0, 26.0, 22.78152247362507, -0.2344497773705189, 0.0, 1.0, 47366.67109174636]
[2019-04-03 22:01:17,701] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-03 22:01:17,702] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [9.2313886e-16 2.2168706e-07 7.4986013e-04 6.0207600e-05 6.1458736e-06
 9.9918348e-01 8.2894741e-10], sampled 0.20616447260962367
[2019-04-03 22:01:58,651] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.23437133], dtype=float32), 0.16893505]
[2019-04-03 22:01:58,651] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [3.9, 88.33333333333334, 0.0, 0.0, 26.0, 25.44363675698062, 0.5058181522224473, 0.0, 1.0, 52058.56017621368]
[2019-04-03 22:01:58,651] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-03 22:01:58,652] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.12268305e-20 1.34928446e-09 2.11054685e-05 4.22177123e-07
 5.46203651e-08 9.99978423e-01 3.75501361e-13], sampled 0.04769601166774706
[2019-04-03 22:02:55,798] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.5638 239911596.9837 1605.3029
[2019-04-03 22:02:57,310] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.23437133], dtype=float32), 0.16893505]
[2019-04-03 22:02:57,310] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-2.737735662, 65.76812495666667, 0.0, 0.0, 26.0, 25.11012478139116, 0.3269459359285679, 0.0, 1.0, 39754.27929915791]
[2019-04-03 22:02:57,310] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-03 22:02:57,311] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.8514263e-18 1.6635429e-08 1.4463524e-04 8.6777327e-06 6.0772129e-07
 9.9984610e-01 1.4989987e-11], sampled 0.8826365902296978
[2019-04-03 22:03:17,761] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.23437133], dtype=float32), 0.16893505]
[2019-04-03 22:03:17,761] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [1.0, 40.0, 0.0, 0.0, 26.0, 25.55733216885408, 0.4395876615075293, 0.0, 1.0, 37456.04408780335]
[2019-04-03 22:03:17,761] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-03 22:03:17,762] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [3.4793665e-18 2.2403832e-08 6.9604350e-05 3.1213372e-06 8.4121564e-07
 9.9992645e-01 1.1649215e-11], sampled 0.843924806155416
[2019-04-03 22:03:19,103] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.8201 263377775.6296 1552.0399
[2019-04-03 22:03:20,350] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.6735 275798561.6764 1233.0993
[2019-04-03 22:03:21,372] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 100000, evaluation results [100000.0, 7241.820116049566, 263377775.62958866, 1552.0399168798824, 7353.563823886941, 239911596.98373976, 1605.3028739965662, 7182.673515826508, 275798561.67643094, 1233.0993326628943]
[2019-04-03 22:03:21,837] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.2540924e-19 1.3769066e-09 1.6308206e-05 2.0607149e-07 1.8139940e-06
 9.9998164e-01 2.0834087e-12], sum to 1.0000
[2019-04-03 22:03:21,838] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2285
[2019-04-03 22:03:21,859] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 75.0, 0.0, 0.0, 26.0, 25.45771792892574, 0.4581543926351443, 0.0, 1.0, 40031.39358479137], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3795600.0000, 
sim time next is 3796200.0000, 
raw observation next is [-3.0, 74.0, 0.0, 0.0, 26.0, 25.39941856820069, 0.4507390249385974, 0.0, 1.0, 68850.0281906832], 
processed observation next is [1.0, 0.9565217391304348, 0.3795013850415513, 0.74, 0.0, 0.0, 0.6666666666666666, 0.6166182140167242, 0.6502463416461991, 0.0, 1.0, 0.3278572770984914], 
reward next is 0.6721, 
noisyNet noise sample is [array([0.447345], dtype=float32), 0.883619]. 
=============================================
[2019-04-03 22:03:25,850] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6500, global step 102282: loss 1.0072
[2019-04-03 22:03:25,851] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6500, global step 102282: learning rate 0.0005
[2019-04-03 22:03:25,950] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6500, global step 102328: loss 1.0131
[2019-04-03 22:03:25,951] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6500, global step 102328: learning rate 0.0005
[2019-04-03 22:03:26,347] A3C_AGENT_WORKER-Thread-12 INFO:Local step 6500, global step 102525: loss 0.7514
[2019-04-03 22:03:26,348] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 6500, global step 102525: learning rate 0.0005
[2019-04-03 22:03:27,082] A3C_AGENT_WORKER-Thread-11 INFO:Local step 6500, global step 102866: loss 0.4378
[2019-04-03 22:03:27,086] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 6500, global step 102867: learning rate 0.0005
[2019-04-03 22:03:27,598] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6500, global step 103097: loss 0.2209
[2019-04-03 22:03:27,599] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6500, global step 103097: learning rate 0.0005
[2019-04-03 22:03:27,840] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6500, global step 103217: loss 0.3008
[2019-04-03 22:03:27,843] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6500, global step 103220: learning rate 0.0005
[2019-04-03 22:03:29,177] A3C_AGENT_WORKER-Thread-6 INFO:Local step 6500, global step 103822: loss 0.2964
[2019-04-03 22:03:29,178] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 6500, global step 103822: learning rate 0.0005
[2019-04-03 22:03:29,452] A3C_AGENT_WORKER-Thread-4 INFO:Local step 6500, global step 103961: loss 0.1444
[2019-04-03 22:03:29,453] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 6500, global step 103961: learning rate 0.0005
[2019-04-03 22:03:29,785] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6500, global step 104138: loss 0.1026
[2019-04-03 22:03:29,787] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6500, global step 104138: learning rate 0.0005
[2019-04-03 22:03:30,222] A3C_AGENT_WORKER-Thread-5 INFO:Local step 6500, global step 104337: loss 0.1000
[2019-04-03 22:03:30,226] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 6500, global step 104338: learning rate 0.0005
[2019-04-03 22:03:30,447] A3C_AGENT_WORKER-Thread-10 INFO:Local step 6500, global step 104448: loss 0.0786
[2019-04-03 22:03:30,450] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 6500, global step 104449: learning rate 0.0005
[2019-04-03 22:03:30,980] A3C_AGENT_WORKER-Thread-13 INFO:Local step 6500, global step 104691: loss 0.0291
[2019-04-03 22:03:30,980] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 6500, global step 104691: learning rate 0.0005
[2019-04-03 22:03:31,422] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6500, global step 104893: loss 0.0379
[2019-04-03 22:03:31,429] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6500, global step 104893: learning rate 0.0005
[2019-04-03 22:03:31,445] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6500, global step 104904: loss 0.0263
[2019-04-03 22:03:31,450] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6500, global step 104906: learning rate 0.0005
[2019-04-03 22:03:31,674] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6500, global step 105004: loss 0.0616
[2019-04-03 22:03:31,675] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6500, global step 105004: learning rate 0.0005
[2019-04-03 22:03:32,412] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6500, global step 105321: loss 0.0633
[2019-04-03 22:03:32,413] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6500, global step 105321: learning rate 0.0005
[2019-04-03 22:03:32,926] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.1881791e-19 9.7764907e-10 5.1184156e-06 3.1433765e-07 4.5929863e-08
 9.9999452e-01 3.8159035e-13], sum to 1.0000
[2019-04-03 22:03:32,933] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1152
[2019-04-03 22:03:33,007] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.33333333333333, 50.0, 99.66666666666667, 655.6666666666667, 26.0, 26.41705053019068, 0.5311809055695283, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4008000.0000, 
sim time next is 4008600.0000, 
raw observation next is [-10.0, 48.5, 101.0, 698.0, 26.0, 26.47915302306455, 0.5431928216890557, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.18559556786703602, 0.485, 0.33666666666666667, 0.7712707182320442, 0.6666666666666666, 0.7065960852553793, 0.6810642738963519, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3020251], dtype=float32), 0.2796537]. 
=============================================
[2019-04-03 22:03:40,054] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.0131891e-21 4.6758103e-10 4.8534639e-07 3.4890757e-08 4.5916643e-10
 9.9999952e-01 8.9614807e-15], sum to 1.0000
[2019-04-03 22:03:40,055] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3451
[2019-04-03 22:03:40,074] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 20.0, 96.5, 753.0, 26.0, 26.64135761758542, 0.6757873138430988, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4028400.0000, 
sim time next is 4029000.0000, 
raw observation next is [-1.833333333333333, 20.33333333333334, 94.0, 739.3333333333334, 26.0, 26.70846346888795, 0.6851850542329982, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.41181902123730385, 0.2033333333333334, 0.31333333333333335, 0.8169429097605894, 0.6666666666666666, 0.7257052890739958, 0.728395018077666, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.8355649], dtype=float32), -0.65646803]. 
=============================================
[2019-04-03 22:03:40,090] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[84.3758 ]
 [84.52496]
 [84.6224 ]
 [84.78403]
 [84.92206]], R is [[84.34172058]
 [84.49830627]
 [84.65332031]
 [84.80678558]
 [84.95871735]].
[2019-04-03 22:03:44,600] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7000, global step 110216: loss 4.7388
[2019-04-03 22:03:44,602] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7000, global step 110217: learning rate 0.0005
[2019-04-03 22:03:44,710] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7000, global step 110268: loss 5.0367
[2019-04-03 22:03:44,712] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7000, global step 110268: learning rate 0.0005
[2019-04-03 22:03:45,232] A3C_AGENT_WORKER-Thread-12 INFO:Local step 7000, global step 110447: loss 5.1873
[2019-04-03 22:03:45,247] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 7000, global step 110449: learning rate 0.0005
[2019-04-03 22:03:46,906] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7000, global step 111045: loss 7.4308
[2019-04-03 22:03:46,935] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7000, global step 111048: learning rate 0.0005
[2019-04-03 22:03:47,756] A3C_AGENT_WORKER-Thread-11 INFO:Local step 7000, global step 111326: loss 7.3951
[2019-04-03 22:03:47,761] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 7000, global step 111327: learning rate 0.0005
[2019-04-03 22:03:48,629] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7000, global step 111649: loss 6.1655
[2019-04-03 22:03:48,631] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7000, global step 111649: learning rate 0.0005
[2019-04-03 22:03:49,916] A3C_AGENT_WORKER-Thread-6 INFO:Local step 7000, global step 112038: loss 6.4707
[2019-04-03 22:03:49,921] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 7000, global step 112039: learning rate 0.0005
[2019-04-03 22:03:50,165] A3C_AGENT_WORKER-Thread-4 INFO:Local step 7000, global step 112104: loss 8.3464
[2019-04-03 22:03:50,165] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 7000, global step 112104: learning rate 0.0005
[2019-04-03 22:03:50,488] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7000, global step 112202: loss 7.1430
[2019-04-03 22:03:50,491] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7000, global step 112202: learning rate 0.0005
[2019-04-03 22:03:51,248] A3C_AGENT_WORKER-Thread-10 INFO:Local step 7000, global step 112478: loss 6.3414
[2019-04-03 22:03:51,250] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 7000, global step 112478: learning rate 0.0005
[2019-04-03 22:03:51,644] A3C_AGENT_WORKER-Thread-5 INFO:Local step 7000, global step 112598: loss 7.3814
[2019-04-03 22:03:51,646] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 7000, global step 112598: learning rate 0.0005
[2019-04-03 22:03:52,593] A3C_AGENT_WORKER-Thread-13 INFO:Local step 7000, global step 112918: loss 5.9029
[2019-04-03 22:03:52,594] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 7000, global step 112918: learning rate 0.0005
[2019-04-03 22:03:52,738] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.7069705e-19 6.4359579e-10 2.3688777e-05 1.4766194e-03 3.1559498e-08
 9.9849975e-01 3.1502361e-12], sum to 1.0000
[2019-04-03 22:03:52,738] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6389
[2019-04-03 22:03:52,758] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.066666666666667, 42.83333333333334, 0.0, 0.0, 26.0, 25.21623109008554, 0.3866894477922274, 0.0, 1.0, 60250.94501150013], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4218600.0000, 
sim time next is 4219200.0000, 
raw observation next is [1.0, 43.0, 0.0, 0.0, 26.0, 25.33347756506318, 0.407018178689407, 0.0, 1.0, 46677.25638396776], 
processed observation next is [0.0, 0.8695652173913043, 0.4903047091412743, 0.43, 0.0, 0.0, 0.6666666666666666, 0.6111231304219317, 0.6356727262298023, 0.0, 1.0, 0.22227264944746553], 
reward next is 0.7777, 
noisyNet noise sample is [array([-1.3301735], dtype=float32), -1.5834994]. 
=============================================
[2019-04-03 22:03:53,350] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7000, global step 113186: loss 7.5858
[2019-04-03 22:03:53,352] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7000, global step 113186: learning rate 0.0005
[2019-04-03 22:03:53,479] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7000, global step 113235: loss 7.2382
[2019-04-03 22:03:53,497] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7000, global step 113235: learning rate 0.0005
[2019-04-03 22:03:53,921] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7000, global step 113376: loss 7.1628
[2019-04-03 22:03:53,923] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7000, global step 113376: learning rate 0.0005
[2019-04-03 22:03:55,027] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7000, global step 113709: loss 5.2627
[2019-04-03 22:03:55,028] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7000, global step 113709: learning rate 0.0005
[2019-04-03 22:03:57,659] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.2990108e-21 9.7748543e-10 1.6467036e-05 2.1558462e-04 1.3442565e-09
 9.9976796e-01 1.5011459e-13], sum to 1.0000
[2019-04-03 22:03:57,659] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2685
[2019-04-03 22:03:57,678] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.8, 76.0, 0.0, 0.0, 26.0, 25.5057640817145, 0.4144368531666691, 0.0, 1.0, 62519.91348185801], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4312800.0000, 
sim time next is 4313400.0000, 
raw observation next is [4.733333333333333, 75.83333333333334, 0.0, 0.0, 26.0, 25.49983686827767, 0.4176000178538983, 0.0, 1.0, 48105.00621079143], 
processed observation next is [0.0, 0.9565217391304348, 0.5937211449676825, 0.7583333333333334, 0.0, 0.0, 0.6666666666666666, 0.6249864056898057, 0.6392000059512994, 0.0, 1.0, 0.22907145814662586], 
reward next is 0.7709, 
noisyNet noise sample is [array([-0.34627175], dtype=float32), 0.40890238]. 
=============================================
[2019-04-03 22:04:04,583] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.3891019e-22 2.6916449e-09 4.3992245e-07 4.4642920e-06 1.9303248e-09
 9.9999511e-01 5.7164826e-15], sum to 1.0000
[2019-04-03 22:04:04,616] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0037
[2019-04-03 22:04:04,645] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 86.0, 160.1666666666667, 24.33333333333333, 26.0, 26.50199900973207, 0.6428191096252512, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4447200.0000, 
sim time next is 4447800.0000, 
raw observation next is [1.0, 86.0, 142.0, 0.0, 26.0, 26.47499823988807, 0.6339084623803816, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4903047091412743, 0.86, 0.47333333333333333, 0.0, 0.6666666666666666, 0.7062498533240058, 0.7113028207934605, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2957335], dtype=float32), -0.520198]. 
=============================================
[2019-04-03 22:04:04,925] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.38771863e-21 2.15758567e-09 4.07548043e-07 4.50529427e-07
 1.37414515e-08 9.99999166e-01 3.14952227e-14], sum to 1.0000
[2019-04-03 22:04:04,933] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1670
[2019-04-03 22:04:04,952] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 86.0, 160.1666666666667, 24.33333333333333, 26.0, 26.5270171482848, 0.6507608175596741, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4447200.0000, 
sim time next is 4447800.0000, 
raw observation next is [1.0, 86.0, 142.0, 0.0, 26.0, 26.49766434310361, 0.6415855174691892, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4903047091412743, 0.86, 0.47333333333333333, 0.0, 0.6666666666666666, 0.7081386952586343, 0.7138618391563964, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5225362], dtype=float32), -0.13326964]. 
=============================================
[2019-04-03 22:04:08,374] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7500, global step 117999: loss 0.1592
[2019-04-03 22:04:08,375] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7500, global step 118000: learning rate 0.0005
[2019-04-03 22:04:09,452] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7500, global step 118342: loss 0.2667
[2019-04-03 22:04:09,454] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7500, global step 118342: learning rate 0.0005
[2019-04-03 22:04:09,686] A3C_AGENT_WORKER-Thread-12 INFO:Local step 7500, global step 118436: loss 0.3600
[2019-04-03 22:04:09,708] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 7500, global step 118439: learning rate 0.0005
[2019-04-03 22:04:11,877] A3C_AGENT_WORKER-Thread-11 INFO:Local step 7500, global step 119115: loss 0.3316
[2019-04-03 22:04:11,888] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 7500, global step 119115: learning rate 0.0005
[2019-04-03 22:04:11,964] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7500, global step 119144: loss 0.3354
[2019-04-03 22:04:11,965] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7500, global step 119144: learning rate 0.0005
[2019-04-03 22:04:14,055] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7500, global step 119810: loss 0.0052
[2019-04-03 22:04:14,060] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7500, global step 119812: learning rate 0.0005
[2019-04-03 22:04:14,714] A3C_AGENT_WORKER-Thread-4 INFO:Local step 7500, global step 120050: loss 0.0180
[2019-04-03 22:04:14,715] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 7500, global step 120050: learning rate 0.0005
[2019-04-03 22:04:15,018] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7500, global step 120155: loss 0.0111
[2019-04-03 22:04:15,020] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7500, global step 120155: learning rate 0.0005
[2019-04-03 22:04:15,109] A3C_AGENT_WORKER-Thread-6 INFO:Local step 7500, global step 120185: loss 0.2859
[2019-04-03 22:04:15,115] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 7500, global step 120185: learning rate 0.0005
[2019-04-03 22:04:15,480] A3C_AGENT_WORKER-Thread-5 INFO:Local step 7500, global step 120307: loss 0.0033
[2019-04-03 22:04:15,480] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 7500, global step 120307: learning rate 0.0005
[2019-04-03 22:04:15,483] A3C_AGENT_WORKER-Thread-10 INFO:Local step 7500, global step 120309: loss 0.0081
[2019-04-03 22:04:15,485] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 7500, global step 120311: learning rate 0.0005
[2019-04-03 22:04:15,830] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.7406250e-21 4.9468246e-10 2.3107084e-05 3.6563090e-06 4.5585349e-09
 9.9997318e-01 7.9569787e-14], sum to 1.0000
[2019-04-03 22:04:15,855] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1111
[2019-04-03 22:04:15,903] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 46.33333333333334, 245.5, 96.16666666666667, 26.0, 26.33292630173455, 0.6198442537408139, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4545600.0000, 
sim time next is 4546200.0000, 
raw observation next is [3.0, 45.66666666666666, 227.0, 79.33333333333334, 26.0, 26.41954360645968, 0.5198971660566319, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.45666666666666655, 0.7566666666666667, 0.08766114180478822, 0.6666666666666666, 0.70162863387164, 0.6732990553522106, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4361941], dtype=float32), 0.72280043]. 
=============================================
[2019-04-03 22:04:17,237] A3C_AGENT_WORKER-Thread-13 INFO:Local step 7500, global step 120826: loss 0.0078
[2019-04-03 22:04:17,238] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 7500, global step 120826: learning rate 0.0005
[2019-04-03 22:04:17,713] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7500, global step 120946: loss -0.1141
[2019-04-03 22:04:17,714] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7500, global step 120946: learning rate 0.0005
[2019-04-03 22:04:19,023] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7500, global step 121337: loss 0.0581
[2019-04-03 22:04:19,024] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7500, global step 121337: learning rate 0.0005
[2019-04-03 22:04:19,144] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7500, global step 121387: loss 0.0658
[2019-04-03 22:04:19,149] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7500, global step 121389: learning rate 0.0005
[2019-04-03 22:04:19,584] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7500, global step 121539: loss 0.0158
[2019-04-03 22:04:19,587] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7500, global step 121540: learning rate 0.0005
[2019-04-03 22:04:21,495] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.7665301e-21 4.6129270e-07 1.7326140e-04 6.6253692e-03 5.8579595e-08
 9.9320096e-01 2.1567198e-13], sum to 1.0000
[2019-04-03 22:04:21,506] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9587
[2019-04-03 22:04:21,543] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6666666666666667, 63.66666666666667, 158.1666666666667, 552.0, 26.0, 26.24522543855575, 0.5852027362027761, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4614000.0000, 
sim time next is 4614600.0000, 
raw observation next is [-0.3333333333333333, 61.83333333333333, 152.3333333333333, 595.0, 26.0, 26.36582308364526, 0.6098804684347624, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4533702677747, 0.6183333333333333, 0.5077777777777777, 0.6574585635359116, 0.6666666666666666, 0.697151923637105, 0.7032934894782542, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9408237], dtype=float32), -1.513208]. 
=============================================
[2019-04-03 22:04:33,542] A3C_AGENT_WORKER-Thread-2 INFO:Local step 8000, global step 126394: loss 0.1254
[2019-04-03 22:04:33,561] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 8000, global step 126394: learning rate 0.0005
[2019-04-03 22:04:34,057] A3C_AGENT_WORKER-Thread-19 INFO:Local step 8000, global step 126553: loss 0.1236
[2019-04-03 22:04:34,058] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 8000, global step 126553: learning rate 0.0005
[2019-04-03 22:04:34,354] A3C_AGENT_WORKER-Thread-12 INFO:Local step 8000, global step 126658: loss -1.0918
[2019-04-03 22:04:34,357] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 8000, global step 126658: learning rate 0.0005
[2019-04-03 22:04:35,672] A3C_AGENT_WORKER-Thread-11 INFO:Local step 8000, global step 127159: loss 0.0756
[2019-04-03 22:04:35,672] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 8000, global step 127159: learning rate 0.0005
[2019-04-03 22:04:35,941] A3C_AGENT_WORKER-Thread-3 INFO:Local step 8000, global step 127244: loss 0.1082
[2019-04-03 22:04:35,941] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 8000, global step 127244: learning rate 0.0005
[2019-04-03 22:04:37,746] A3C_AGENT_WORKER-Thread-20 INFO:Local step 8000, global step 127840: loss 0.0267
[2019-04-03 22:04:37,747] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 8000, global step 127840: learning rate 0.0005
[2019-04-03 22:04:38,563] A3C_AGENT_WORKER-Thread-17 INFO:Local step 8000, global step 128120: loss 0.0167
[2019-04-03 22:04:38,564] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 8000, global step 128120: learning rate 0.0005
[2019-04-03 22:04:38,619] A3C_AGENT_WORKER-Thread-4 INFO:Local step 8000, global step 128142: loss -0.0664
[2019-04-03 22:04:38,620] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 8000, global step 128142: learning rate 0.0005
[2019-04-03 22:04:38,643] A3C_AGENT_WORKER-Thread-5 INFO:Local step 8000, global step 128151: loss 0.0570
[2019-04-03 22:04:38,652] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 8000, global step 128153: learning rate 0.0005
[2019-04-03 22:04:38,756] A3C_AGENT_WORKER-Thread-6 INFO:Local step 8000, global step 128198: loss 0.0374
[2019-04-03 22:04:38,756] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 8000, global step 128198: learning rate 0.0005
[2019-04-03 22:04:40,071] A3C_AGENT_WORKER-Thread-10 INFO:Local step 8000, global step 128616: loss 0.0147
[2019-04-03 22:04:40,071] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 8000, global step 128616: learning rate 0.0005
[2019-04-03 22:04:41,683] A3C_AGENT_WORKER-Thread-16 INFO:Local step 8000, global step 129076: loss 0.0668
[2019-04-03 22:04:41,699] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 8000, global step 129076: learning rate 0.0005
[2019-04-03 22:04:42,063] A3C_AGENT_WORKER-Thread-13 INFO:Local step 8000, global step 129189: loss 0.0952
[2019-04-03 22:04:42,064] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 8000, global step 129189: learning rate 0.0005
[2019-04-03 22:04:42,464] A3C_AGENT_WORKER-Thread-14 INFO:Local step 8000, global step 129321: loss 0.0187
[2019-04-03 22:04:42,465] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 8000, global step 129321: learning rate 0.0005
[2019-04-03 22:04:43,362] A3C_AGENT_WORKER-Thread-18 INFO:Local step 8000, global step 129583: loss 0.0172
[2019-04-03 22:04:43,364] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 8000, global step 129583: learning rate 0.0005
[2019-04-03 22:04:43,448] A3C_AGENT_WORKER-Thread-15 INFO:Local step 8000, global step 129611: loss 0.0573
[2019-04-03 22:04:43,448] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 8000, global step 129611: learning rate 0.0005
[2019-04-03 22:04:46,436] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.2817198e-21 4.4723718e-09 1.1960034e-07 3.1723728e-05 1.9806587e-09
 9.9996817e-01 5.5509518e-14], sum to 1.0000
[2019-04-03 22:04:46,437] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5607
[2019-04-03 22:04:46,458] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.166666666666667, 44.16666666666667, 248.0, 378.6666666666667, 26.0, 25.09399801558412, 0.3656426637357439, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4889400.0000, 
sim time next is 4890000.0000, 
raw observation next is [2.333333333333333, 44.33333333333334, 242.0, 376.3333333333334, 26.0, 25.08128743586594, 0.365187305304503, 0.0, 1.0, 18689.71272695318], 
processed observation next is [0.0, 0.6086956521739131, 0.5272391505078486, 0.4433333333333334, 0.8066666666666666, 0.4158379373848988, 0.6666666666666666, 0.5901072863221616, 0.6217291017681676, 0.0, 1.0, 0.08899863203311038], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.01116925], dtype=float32), -1.5936224]. 
=============================================
[2019-04-03 22:04:46,486] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[87.66283 ]
 [87.77362 ]
 [87.90889 ]
 [88.01283 ]
 [88.079895]], R is [[87.72002411]
 [87.84282684]
 [87.96440125]
 [88.08475494]
 [88.20391083]].
[2019-04-03 22:04:53,665] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.00837222e-24 1.03935636e-11 3.54724357e-08 1.27594685e-05
 1.51099244e-10 9.99987245e-01 3.32635053e-16], sum to 1.0000
[2019-04-03 22:04:53,665] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5390
[2019-04-03 22:04:53,711] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.5, 25.5, 72.0, 641.0, 26.0, 28.014928885422, 0.8217866186817794, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4984200.0000, 
sim time next is 4984800.0000, 
raw observation next is [8.333333333333334, 25.66666666666666, 65.66666666666667, 584.8333333333334, 26.0, 27.42774709324719, 0.8676958094481827, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.6934441366574331, 0.2566666666666666, 0.2188888888888889, 0.6462246777163905, 0.6666666666666666, 0.7856455911039326, 0.7892319364827275, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4594525], dtype=float32), -1.2565961]. 
=============================================
[2019-04-03 22:04:57,057] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:04:57,057] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:04:57,061] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run2
[2019-04-03 22:04:57,353] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:04:57,353] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:04:57,355] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run2
[2019-04-03 22:04:57,720] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:04:57,721] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:04:57,724] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run2
[2019-04-03 22:04:58,553] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.3805837e-22 1.2967388e-08 6.5887880e-06 5.3814685e-01 8.9277101e-09
 4.6184656e-01 3.0907113e-13], sum to 1.0000
[2019-04-03 22:04:58,587] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3471
[2019-04-03 22:04:58,630] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [12.0, 19.0, 101.0, 769.6666666666667, 26.0, 28.64061787051327, 1.126175329473696, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5065800.0000, 
sim time next is 5066400.0000, 
raw observation next is [12.0, 19.0, 98.5, 757.3333333333334, 26.0, 28.70039509672981, 1.020146829304957, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.7950138504155125, 0.19, 0.3283333333333333, 0.8368324125230203, 0.6666666666666666, 0.8916995913941509, 0.8400489431016522, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.2677875], dtype=float32), 0.10129533]. 
=============================================
[2019-04-03 22:04:59,497] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:04:59,497] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:04:59,499] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run2
[2019-04-03 22:05:00,233] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:00,233] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:00,235] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run2
[2019-04-03 22:05:02,101] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:02,101] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:02,123] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run2
[2019-04-03 22:05:02,989] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:02,989] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:02,991] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run2
[2019-04-03 22:05:03,261] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:03,261] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:03,263] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run2
[2019-04-03 22:05:03,419] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:03,420] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:03,430] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run2
[2019-04-03 22:05:03,690] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:03,690] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:03,706] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run2
[2019-04-03 22:05:05,478] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:05,479] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:05,492] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run2
[2019-04-03 22:05:06,796] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:06,796] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:06,798] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run2
[2019-04-03 22:05:06,859] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:06,859] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:06,861] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run2
[2019-04-03 22:05:07,053] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:07,053] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:07,055] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run2
[2019-04-03 22:05:08,097] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:08,097] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:08,099] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run2
[2019-04-03 22:05:08,221] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:08,221] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:08,223] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run2
[2019-04-03 22:05:19,716] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0482324e-19 4.5353063e-10 1.9751492e-07 4.5779845e-04 9.2020905e-11
 9.9954200e-01 1.3017777e-11], sum to 1.0000
[2019-04-03 22:05:19,716] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4830
[2019-04-03 22:05:19,792] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.233333333333333, 83.33333333333334, 23.33333333333333, 0.0, 26.0, 24.5217261893856, 0.1771732158551255, 0.0, 1.0, 50744.26970951291], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 58800.0000, 
sim time next is 59400.0000, 
raw observation next is [6.05, 84.0, 18.0, 0.0, 26.0, 24.50386184605057, 0.1859069886179371, 0.0, 1.0, 59015.47869182152], 
processed observation next is [0.0, 0.6956521739130435, 0.6301939058171746, 0.84, 0.06, 0.0, 0.6666666666666666, 0.5419884871708808, 0.5619689962059791, 0.0, 1.0, 0.2810260890086739], 
reward next is 0.7190, 
noisyNet noise sample is [array([-0.16963407], dtype=float32), 0.34855208]. 
=============================================
[2019-04-03 22:05:41,964] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.2583983e-18 1.2466339e-08 1.1253108e-05 2.6351583e-04 5.5906471e-09
 9.9972516e-01 3.4788710e-11], sum to 1.0000
[2019-04-03 22:05:41,964] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1213
[2019-04-03 22:05:42,125] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.316666666666666, 64.5, 142.3333333333333, 0.0, 26.0, 25.18582318401195, 0.1458756657854181, 1.0, 1.0, 18721.76290850715], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 220200.0000, 
sim time next is 220800.0000, 
raw observation next is [-4.133333333333334, 64.0, 145.6666666666667, 0.0, 26.0, 24.72218954334707, 0.2360052485151558, 1.0, 1.0, 199538.1360595971], 
processed observation next is [1.0, 0.5652173913043478, 0.34810710987996313, 0.64, 0.48555555555555574, 0.0, 0.6666666666666666, 0.5601824619455892, 0.5786684161717186, 1.0, 1.0, 0.9501816002837957], 
reward next is 0.0498, 
noisyNet noise sample is [array([-1.0756253], dtype=float32), -0.5200883]. 
=============================================
[2019-04-03 22:05:42,559] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0051987e-20 9.6138708e-10 1.7358157e-07 2.0823977e-08 1.5290998e-10
 9.9999976e-01 1.3036165e-11], sum to 1.0000
[2019-04-03 22:05:42,585] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4135
[2019-04-03 22:05:42,599] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 76.16666666666667, 0.0, 0.0, 26.0, 24.49206839131318, 0.1664085478418742, 0.0, 1.0, 44191.58824757619], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 252600.0000, 
sim time next is 253200.0000, 
raw observation next is [-3.9, 77.33333333333334, 0.0, 0.0, 26.0, 24.46075229543123, 0.1591776734807486, 0.0, 1.0, 44179.26249534143], 
processed observation next is [1.0, 0.9565217391304348, 0.3545706371191136, 0.7733333333333334, 0.0, 0.0, 0.6666666666666666, 0.5383960246192693, 0.5530592244935829, 0.0, 1.0, 0.2103774404540068], 
reward next is 0.7896, 
noisyNet noise sample is [array([-0.9996735], dtype=float32), -1.1405826]. 
=============================================
[2019-04-03 22:05:48,387] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.0237232e-17 1.2047220e-08 4.4196091e-05 1.7572993e-02 5.1642601e-08
 9.8238277e-01 1.0056248e-09], sum to 1.0000
[2019-04-03 22:05:48,387] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8040
[2019-04-03 22:05:48,453] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.1, 68.0, 0.0, 0.0, 26.0, 22.54314000165216, -0.2726453480366316, 0.0, 1.0, 47882.97179625639], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 283200.0000, 
sim time next is 283800.0000, 
raw observation next is [-12.2, 67.5, 0.0, 0.0, 26.0, 22.49414729299266, -0.2715521864936798, 0.0, 1.0, 47923.56784467668], 
processed observation next is [1.0, 0.2608695652173913, 0.12465373961218838, 0.675, 0.0, 0.0, 0.6666666666666666, 0.3745122744160551, 0.4094826045021067, 0.0, 1.0, 0.22820746592703184], 
reward next is 0.7718, 
noisyNet noise sample is [array([0.5388698], dtype=float32), 1.2071283]. 
=============================================
[2019-04-03 22:06:19,021] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.8274726e-21 1.1303834e-10 5.4758038e-09 1.6787566e-02 2.4845356e-10
 9.8321241e-01 1.5357678e-12], sum to 1.0000
[2019-04-03 22:06:19,022] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1929
[2019-04-03 22:06:19,078] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.75, 41.0, 0.0, 0.0, 26.0, 25.08810795553453, 0.2949925818132944, 1.0, 1.0, 87610.09038443935], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 415800.0000, 
sim time next is 416400.0000, 
raw observation next is [-9.833333333333332, 41.33333333333334, 0.0, 0.0, 26.0, 25.08472063433556, 0.2978821066783279, 1.0, 1.0, 73203.56090163538], 
processed observation next is [1.0, 0.8260869565217391, 0.19021237303785785, 0.41333333333333344, 0.0, 0.0, 0.6666666666666666, 0.5903933861946301, 0.5992940355594426, 1.0, 1.0, 0.34858838524588276], 
reward next is 0.6514, 
noisyNet noise sample is [array([0.14180383], dtype=float32), -0.781098]. 
=============================================
[2019-04-03 22:06:34,088] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.2995859e-19 6.2365211e-09 3.5891027e-08 1.6102443e-02 4.1812279e-11
 9.8389751e-01 1.5508582e-11], sum to 1.0000
[2019-04-03 22:06:34,089] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3725
[2019-04-03 22:06:34,169] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 80.5, 0.0, 0.0, 26.0, 24.22112828104014, 0.1032846593110688, 0.0, 1.0, 42430.91032704792], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 613800.0000, 
sim time next is 614400.0000, 
raw observation next is [-3.899999999999999, 78.66666666666667, 0.0, 0.0, 26.0, 24.21714705622947, 0.09725527744671521, 0.0, 1.0, 42541.77143152586], 
processed observation next is [0.0, 0.08695652173913043, 0.35457063711911363, 0.7866666666666667, 0.0, 0.0, 0.6666666666666666, 0.5180955880191226, 0.5324184258155717, 0.0, 1.0, 0.20257986395964697], 
reward next is 0.7974, 
noisyNet noise sample is [array([1.8372937], dtype=float32), -1.9515916]. 
=============================================
[2019-04-03 22:06:34,639] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.3984434e-20 4.3187592e-10 9.4861239e-09 9.9998343e-01 1.7318452e-11
 1.6598864e-05 4.2146577e-12], sum to 1.0000
[2019-04-03 22:06:34,640] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7570
[2019-04-03 22:06:34,699] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 66.0, 0.0, 0.0, 26.0, 23.73668359418617, -0.01137927217754517, 0.0, 1.0, 44067.00476043121], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 625200.0000, 
sim time next is 625800.0000, 
raw observation next is [-4.5, 65.5, 0.0, 0.0, 26.0, 23.7123152832534, -0.01238591243125258, 0.0, 1.0, 43985.4052556622], 
processed observation next is [0.0, 0.21739130434782608, 0.3379501385041552, 0.655, 0.0, 0.0, 0.6666666666666666, 0.47602627360445, 0.4958713625229158, 0.0, 1.0, 0.20945431074124857], 
reward next is 0.7905, 
noisyNet noise sample is [array([0.4255928], dtype=float32), 0.5859358]. 
=============================================
[2019-04-03 22:06:34,865] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.2754967e-18 5.1569721e-08 9.0651639e-07 7.5657469e-01 1.1000454e-08
 2.4342436e-01 3.6480036e-10], sum to 1.0000
[2019-04-03 22:06:34,865] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8029
[2019-04-03 22:06:34,934] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.8, 87.0, 0.0, 0.0, 26.0, 24.9834378117394, 0.2928523028375311, 0.0, 1.0, 32506.05895351528], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 580200.0000, 
sim time next is 580800.0000, 
raw observation next is [-1.9, 87.0, 0.0, 0.0, 26.0, 24.9636030592504, 0.2877425200533035, 0.0, 1.0, 47948.17820900137], 
processed observation next is [0.0, 0.7391304347826086, 0.4099722991689751, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5803002549375332, 0.5959141733511012, 0.0, 1.0, 0.22832465813810177], 
reward next is 0.7717, 
noisyNet noise sample is [array([0.40531102], dtype=float32), -0.8448017]. 
=============================================
[2019-04-03 22:06:47,507] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.2214620e-20 1.0911148e-09 4.6871658e-09 9.5414704e-01 8.1716350e-10
 4.5852900e-02 1.1253650e-11], sum to 1.0000
[2019-04-03 22:06:47,507] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3102
[2019-04-03 22:06:47,600] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 66.33333333333334, 0.0, 0.0, 26.0, 25.02410300615295, 0.2221303497555626, 0.0, 1.0, 43265.86210195989], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 678000.0000, 
sim time next is 678600.0000, 
raw observation next is [-3.1, 67.0, 0.0, 0.0, 26.0, 25.02531936966371, 0.2196593467886331, 0.0, 1.0, 42740.31806773723], 
processed observation next is [0.0, 0.8695652173913043, 0.37673130193905824, 0.67, 0.0, 0.0, 0.6666666666666666, 0.585443280805309, 0.5732197822628777, 0.0, 1.0, 0.20352532413208205], 
reward next is 0.7965, 
noisyNet noise sample is [array([-0.63596004], dtype=float32), 0.22017439]. 
=============================================
[2019-04-03 22:06:49,582] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2086220e-21 3.8917316e-12 8.4036195e-11 9.9998116e-01 5.2311168e-12
 1.8799410e-05 2.2759087e-13], sum to 1.0000
[2019-04-03 22:06:49,653] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0474
[2019-04-03 22:06:49,686] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 69.83333333333334, 0.0, 0.0, 26.0, 23.57621379442949, -0.05482083146737604, 0.0, 1.0, 43850.81213819271], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 630600.0000, 
sim time next is 631200.0000, 
raw observation next is [-4.5, 71.66666666666667, 0.0, 0.0, 26.0, 23.54574507010143, -0.05888595018967408, 0.0, 1.0, 43892.65273252814], 
processed observation next is [0.0, 0.30434782608695654, 0.3379501385041552, 0.7166666666666667, 0.0, 0.0, 0.6666666666666666, 0.4621454225084524, 0.48037134993677527, 0.0, 1.0, 0.2090126320596578], 
reward next is 0.7910, 
noisyNet noise sample is [array([-1.0367107], dtype=float32), 0.001881684]. 
=============================================
[2019-04-03 22:06:55,196] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.1220148e-20 2.4784205e-08 1.2784105e-07 3.7938345e-02 1.6128281e-08
 9.6206152e-01 4.1217561e-11], sum to 1.0000
[2019-04-03 22:06:55,196] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7585
[2019-04-03 22:06:55,245] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.7, 49.5, 68.0, 3.0, 26.0, 25.22742281717996, 0.3958715926577653, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 750600.0000, 
sim time next is 751200.0000, 
raw observation next is [-2.066666666666666, 51.0, 56.66666666666667, 2.833333333333333, 26.0, 25.67409943608536, 0.4271760999592983, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.40535549399815335, 0.51, 0.1888888888888889, 0.0031307550644567215, 0.6666666666666666, 0.6395082863404467, 0.6423920333197661, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8944669], dtype=float32), 0.9218042]. 
=============================================
[2019-04-03 22:07:07,890] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.9440540e-22 1.2037872e-10 7.6414436e-10 9.9999940e-01 7.4129192e-12
 6.0981358e-07 8.8654798e-14], sum to 1.0000
[2019-04-03 22:07:07,891] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4252
[2019-04-03 22:07:07,957] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 83.0, 0.0, 0.0, 26.0, 25.0814771004057, 0.3103114309494024, 0.0, 1.0, 66294.35121485805], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 851400.0000, 
sim time next is 852000.0000, 
raw observation next is [-3.4, 83.0, 0.0, 0.0, 26.0, 25.06402759586089, 0.3060660049250876, 0.0, 1.0, 50999.98890726877], 
processed observation next is [1.0, 0.8695652173913043, 0.368421052631579, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5886689663217407, 0.6020220016416958, 0.0, 1.0, 0.2428570900346132], 
reward next is 0.7571, 
noisyNet noise sample is [array([-1.8964162], dtype=float32), 0.21807194]. 
=============================================
[2019-04-03 22:07:07,970] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[80.88705 ]
 [81.55181 ]
 [82.22501 ]
 [85.13517 ]
 [87.664085]], R is [[80.87262726]
 [80.74821472]
 [80.51134491]
 [80.70623016]
 [80.73989868]].
[2019-04-03 22:07:22,846] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.1891126e-27 6.7928404e-14 6.7314669e-12 9.9999988e-01 2.0202722e-14
 6.2732092e-08 2.6577636e-16], sum to 1.0000
[2019-04-03 22:07:22,846] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9285
[2019-04-03 22:07:22,858] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.6, 82.0, 0.0, 0.0, 26.0, 25.55978459567886, 0.456711086641393, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 957600.0000, 
sim time next is 958200.0000, 
raw observation next is [6.783333333333333, 81.66666666666667, 0.0, 0.0, 26.0, 25.56930436978565, 0.4482434457319038, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.6505078485687905, 0.8166666666666668, 0.0, 0.0, 0.6666666666666666, 0.6307753641488043, 0.6494144819106346, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.9893894], dtype=float32), -0.7406092]. 
=============================================
[2019-04-03 22:07:30,299] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0294784e-24 3.0461272e-11 1.1231728e-08 9.9445772e-01 7.7719255e-12
 5.5422429e-03 4.0139878e-13], sum to 1.0000
[2019-04-03 22:07:30,301] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6852
[2019-04-03 22:07:30,316] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [14.76666666666667, 80.0, 0.0, 0.0, 26.0, 25.86486237009361, 0.5870049208842719, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1014000.0000, 
sim time next is 1014600.0000, 
raw observation next is [14.58333333333333, 80.5, 0.0, 0.0, 26.0, 25.99987546805733, 0.5776842513325583, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.8665743305632503, 0.805, 0.0, 0.0, 0.6666666666666666, 0.6666562890047775, 0.6925614171108528, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5684999], dtype=float32), 0.7941704]. 
=============================================
[2019-04-03 22:07:42,311] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.1779992e-30 3.0609468e-16 1.3849731e-13 1.0000000e+00 8.2184636e-17
 2.9362430e-12 7.2904029e-19], sum to 1.0000
[2019-04-03 22:07:42,311] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2066
[2019-04-03 22:07:42,368] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [12.61666666666667, 86.0, 125.0, 0.0, 26.0, 25.6469106409182, 0.5560839064335747, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 996600.0000, 
sim time next is 997200.0000, 
raw observation next is [12.7, 86.0, 123.5, 0.0, 26.0, 25.95070354710873, 0.5811338451229963, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.8144044321329641, 0.86, 0.4116666666666667, 0.0, 0.6666666666666666, 0.6625586289257276, 0.6937112817076655, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.247134], dtype=float32), -1.1168836]. 
=============================================
[2019-04-03 22:07:52,640] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.2253734e-27 3.1445549e-14 3.7893351e-13 1.0000000e+00 1.0677236e-15
 6.3082634e-10 1.5561495e-16], sum to 1.0000
[2019-04-03 22:07:52,687] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3144
[2019-04-03 22:07:52,728] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [17.7, 67.0, 0.0, 0.0, 26.0, 24.59668832600839, 0.3842976190811451, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1195200.0000, 
sim time next is 1195800.0000, 
raw observation next is [17.7, 67.0, 0.0, 0.0, 26.0, 24.59686958586542, 0.3810769865533581, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.9529085872576178, 0.67, 0.0, 0.0, 0.6666666666666666, 0.5497391321554517, 0.6270256621844527, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3796501], dtype=float32), 1.00237]. 
=============================================
[2019-04-03 22:07:57,493] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.3245427e-29 1.0944416e-15 2.0656193e-14 1.0000000e+00 1.3615493e-17
 2.5705124e-11 4.3300969e-18], sum to 1.0000
[2019-04-03 22:07:57,518] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6779
[2019-04-03 22:07:57,544] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.5, 100.0, 0.0, 0.0, 26.0, 25.5020058270154, 0.5909868028155305, 0.0, 1.0, 18749.97769297432], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1291200.0000, 
sim time next is 1291800.0000, 
raw observation next is [5.5, 100.0, 0.0, 0.0, 26.0, 25.48288061093533, 0.5892334736185498, 0.0, 1.0, 35418.31171603059], 
processed observation next is [0.0, 0.9565217391304348, 0.6149584487534627, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6235733842446107, 0.6964111578728499, 0.0, 1.0, 0.1686586272191933], 
reward next is 0.8313, 
noisyNet noise sample is [array([0.14292102], dtype=float32), -0.673809]. 
=============================================
[2019-04-03 22:08:02,669] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.2382467e-21 8.6152568e-10 9.2155936e-09 9.9998868e-01 2.5726085e-10
 1.1287355e-05 1.5659337e-12], sum to 1.0000
[2019-04-03 22:08:02,669] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2434
[2019-04-03 22:08:02,692] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.366666666666667, 92.0, 0.0, 0.0, 26.0, 25.44896967154099, 0.5545390752306684, 0.0, 1.0, 41243.8790289588], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1309200.0000, 
sim time next is 1309800.0000, 
raw observation next is [2.283333333333333, 92.0, 0.0, 0.0, 26.0, 25.43766344701285, 0.5481153366633863, 0.0, 1.0, 42224.56229516613], 
processed observation next is [1.0, 0.13043478260869565, 0.5258541089566021, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6198052872510708, 0.6827051122211287, 0.0, 1.0, 0.20106934426269585], 
reward next is 0.7989, 
noisyNet noise sample is [array([1.3634658], dtype=float32), 0.45315248]. 
=============================================
[2019-04-03 22:08:14,320] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.3142694e-25 4.0788675e-14 3.7062974e-12 1.0000000e+00 9.5962794e-15
 4.7944443e-10 1.7848431e-15], sum to 1.0000
[2019-04-03 22:08:14,322] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7579
[2019-04-03 22:08:14,351] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.28962217131632, 0.4597507503624739, 0.0, 1.0, 38995.64302750208], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1398600.0000, 
sim time next is 1399200.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.29583365187685, 0.461015291734918, 0.0, 1.0, 38789.67651029267], 
processed observation next is [1.0, 0.17391304347826086, 0.44598337950138506, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6079861376564043, 0.6536717639116393, 0.0, 1.0, 0.18471274528710796], 
reward next is 0.8153, 
noisyNet noise sample is [array([-0.3980503], dtype=float32), -0.10083048]. 
=============================================
[2019-04-03 22:08:17,228] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.6851850e-26 5.4614608e-14 4.8178773e-13 1.0000000e+00 2.2569157e-15
 8.6826393e-11 3.8386578e-16], sum to 1.0000
[2019-04-03 22:08:17,229] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7611
[2019-04-03 22:08:17,240] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.5, 84.83333333333334, 0.0, 0.0, 26.0, 25.7016986789647, 0.5338645116602434, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1565400.0000, 
sim time next is 1566000.0000, 
raw observation next is [4.4, 86.0, 0.0, 0.0, 26.0, 25.60156706412787, 0.5257040620735105, 0.0, 1.0, 18736.71446041186], 
processed observation next is [1.0, 0.13043478260869565, 0.5844875346260389, 0.86, 0.0, 0.0, 0.6666666666666666, 0.6334639220106558, 0.6752346873578369, 0.0, 1.0, 0.08922244981148504], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.37337062], dtype=float32), 0.6509109]. 
=============================================
[2019-04-03 22:08:17,247] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[78.86988]
 [78.88402]
 [78.8966 ]
 [78.99444]
 [79.07888]], R is [[78.85839081]
 [79.06980896]
 [79.27911377]
 [79.44168091]
 [79.54399109]].
[2019-04-03 22:08:26,097] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.3251382e-23 1.1824949e-13 1.4508882e-10 1.0000000e+00 2.9652824e-13
 4.1254655e-08 3.8910563e-15], sum to 1.0000
[2019-04-03 22:08:26,097] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0565
[2019-04-03 22:08:26,118] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.616666666666667, 73.83333333333334, 0.0, 0.0, 26.0, 25.30940493897585, 0.6307638643150252, 0.0, 1.0, 173753.9425738378], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1543800.0000, 
sim time next is 1544400.0000, 
raw observation next is [7.7, 74.0, 0.0, 0.0, 26.0, 25.46048683418699, 0.6887371586476867, 0.0, 1.0, 42066.26284043533], 
processed observation next is [1.0, 0.9130434782608695, 0.6759002770083103, 0.74, 0.0, 0.0, 0.6666666666666666, 0.6217072361822492, 0.7295790528825622, 0.0, 1.0, 0.20031553733540633], 
reward next is 0.7997, 
noisyNet noise sample is [array([1.8707387], dtype=float32), -0.112503566]. 
=============================================
[2019-04-03 22:08:27,019] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.4694201e-24 4.5248977e-13 1.0802664e-11 1.0000000e+00 1.6510202e-13
 5.8982828e-09 4.0664523e-15], sum to 1.0000
[2019-04-03 22:08:27,019] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4200
[2019-04-03 22:08:27,040] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.416666666666667, 79.50000000000001, 0.0, 0.0, 26.0, 25.45817698002547, 0.4907562761383306, 0.0, 1.0, 39311.98276358261], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1581000.0000, 
sim time next is 1581600.0000, 
raw observation next is [5.333333333333334, 80.0, 0.0, 0.0, 26.0, 25.44032852103544, 0.4990318731000995, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.6103416435826409, 0.8, 0.0, 0.0, 0.6666666666666666, 0.6200273767529533, 0.6663439577000332, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.79332644], dtype=float32), -0.1853996]. 
=============================================
[2019-04-03 22:08:30,356] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.4469462e-23 5.0064297e-12 6.8826733e-10 9.9999988e-01 2.7387329e-12
 9.1074668e-08 1.9454355e-14], sum to 1.0000
[2019-04-03 22:08:30,357] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2006
[2019-04-03 22:08:30,380] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 88.0, 0.0, 0.0, 26.0, 25.26561614666427, 0.4539233782520944, 1.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1707600.0000, 
sim time next is 1708200.0000, 
raw observation next is [1.1, 88.0, 0.0, 0.0, 26.0, 25.20181073256224, 0.3301090328970632, 1.0, 1.0, 22333.3191628539], 
processed observation next is [1.0, 0.782608695652174, 0.49307479224376743, 0.88, 0.0, 0.0, 0.6666666666666666, 0.6001508943801867, 0.6100363442990211, 1.0, 1.0, 0.10634913887073286], 
reward next is 0.8937, 
noisyNet noise sample is [array([0.9542977], dtype=float32), -1.2662407]. 
=============================================
[2019-04-03 22:08:35,767] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.9666290e-23 5.9079343e-12 2.3069421e-10 1.0000000e+00 6.8093838e-13
 3.5467746e-08 2.7111326e-14], sum to 1.0000
[2019-04-03 22:08:35,767] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8702
[2019-04-03 22:08:35,812] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.7, 83.0, 45.5, 0.0, 26.0, 25.18859009468912, 0.3570922796135538, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1760400.0000, 
sim time next is 1761000.0000, 
raw observation next is [-1.8, 83.66666666666667, 52.0, 0.0, 26.0, 25.12097042545417, 0.3442666230397556, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.41274238227146814, 0.8366666666666667, 0.17333333333333334, 0.0, 0.6666666666666666, 0.5934142021211809, 0.6147555410132518, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.46261817], dtype=float32), -1.2053255]. 
=============================================
[2019-04-03 22:08:35,815] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[73.78351]
 [73.91905]
 [74.05206]
 [74.14143]
 [74.07455]], R is [[73.94097137]
 [74.20156097]
 [74.45954895]
 [74.71495056]
 [74.96780396]].
[2019-04-03 22:08:46,213] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.8484197e-21 1.2433790e-11 2.1459798e-10 1.0000000e+00 2.1129572e-12
 1.1999562e-08 1.9862641e-13], sum to 1.0000
[2019-04-03 22:08:46,213] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3419
[2019-04-03 22:08:46,295] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.783333333333333, 78.0, 138.6666666666667, 79.66666666666667, 26.0, 25.093145237173, 0.2335842265182289, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1849800.0000, 
sim time next is 1850400.0000, 
raw observation next is [-5.6, 78.0, 134.0, 72.5, 26.0, 25.01960917117569, 0.2180462253221129, 0.0, 1.0, 18757.33677820371], 
processed observation next is [0.0, 0.43478260869565216, 0.30747922437673136, 0.78, 0.44666666666666666, 0.08011049723756906, 0.6666666666666666, 0.5849674309313077, 0.572682075107371, 0.0, 1.0, 0.08932065132477957], 
reward next is 0.9107, 
noisyNet noise sample is [array([0.45641094], dtype=float32), 0.1466791]. 
=============================================
[2019-04-03 22:08:49,678] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.0392321e-20 2.9007982e-10 2.1525604e-09 1.0000000e+00 2.7588147e-11
 7.6717699e-09 2.2816161e-12], sum to 1.0000
[2019-04-03 22:08:49,679] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4342
[2019-04-03 22:08:49,704] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-9.100000000000001, 85.0, 0.0, 0.0, 26.0, 23.25883988553956, -0.1686694708306329, 0.0, 1.0, 44687.21542290307], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1923600.0000, 
sim time next is 1924200.0000, 
raw observation next is [-9.2, 86.5, 0.0, 0.0, 26.0, 23.19586501485474, -0.1795779042534386, 0.0, 1.0, 44627.96568197844], 
processed observation next is [1.0, 0.2608695652173913, 0.20775623268698065, 0.865, 0.0, 0.0, 0.6666666666666666, 0.43298875123789493, 0.44014069858218713, 0.0, 1.0, 0.21251412229513544], 
reward next is 0.7875, 
noisyNet noise sample is [array([-0.14663932], dtype=float32), 0.70116514]. 
=============================================
[2019-04-03 22:09:07,213] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.98755900e-26 1.40506105e-14 2.28748687e-11 1.00000000e+00
 6.57476189e-14 5.47131914e-12 4.96893773e-16], sum to 1.0000
[2019-04-03 22:09:07,213] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6094
[2019-04-03 22:09:07,343] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.1, 79.0, 140.6666666666667, 0.0, 26.0, 25.81266566273765, 0.4526215261815746, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2036400.0000, 
sim time next is 2037000.0000, 
raw observation next is [-4.0, 79.0, 133.3333333333333, 0.0, 26.0, 26.13050848470301, 0.4760679568449861, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.3518005540166205, 0.79, 0.4444444444444443, 0.0, 0.6666666666666666, 0.677542373725251, 0.6586893189483287, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.38119915], dtype=float32), -0.8181434]. 
=============================================
[2019-04-03 22:09:07,387] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[86.14248]
 [86.35628]
 [86.34464]
 [85.72248]
 [85.31757]], R is [[86.09253693]
 [86.23161316]
 [86.36930084]
 [85.55178833]
 [84.75310516]].
[2019-04-03 22:09:44,692] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.9935191e-25 1.1193415e-13 4.5958739e-13 1.0000000e+00 3.1560797e-14
 5.6300489e-09 8.3212781e-16], sum to 1.0000
[2019-04-03 22:09:44,725] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0349
[2019-04-03 22:09:44,773] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.2, 52.0, 0.0, 0.0, 26.0, 25.50686074055875, 0.4156808413630275, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2311200.0000, 
sim time next is 2311800.0000, 
raw observation next is [-1.2, 52.33333333333334, 0.0, 0.0, 26.0, 25.59389466084579, 0.418846898110051, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.42936288088642666, 0.5233333333333334, 0.0, 0.0, 0.6666666666666666, 0.6328245550704826, 0.6396156327033503, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.8466387], dtype=float32), 0.5380558]. 
=============================================
[2019-04-03 22:09:57,939] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.5548745e-21 2.0556589e-11 5.4754795e-10 1.0000000e+00 2.6308719e-12
 2.5086475e-09 5.5626201e-13], sum to 1.0000
[2019-04-03 22:09:57,939] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2701
[2019-04-03 22:09:57,996] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.9666666666666667, 44.66666666666667, 18.16666666666666, 23.0, 26.0, 24.96083074995219, 0.2726831361285817, 0.0, 1.0, 40076.67721661839], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2395200.0000, 
sim time next is 2395800.0000, 
raw observation next is [-1.15, 44.5, 0.0, 0.0, 26.0, 24.95557783111534, 0.2654938209473628, 0.0, 1.0, 44726.35813851634], 
processed observation next is [0.0, 0.7391304347826086, 0.4307479224376732, 0.445, 0.0, 0.0, 0.6666666666666666, 0.5796314859262782, 0.5884979403157876, 0.0, 1.0, 0.21298265780245876], 
reward next is 0.7870, 
noisyNet noise sample is [array([0.97243285], dtype=float32), 0.3673849]. 
=============================================
[2019-04-03 22:09:59,608] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.6527284e-19 4.7517362e-10 5.0253406e-09 9.9999964e-01 5.6612905e-11
 3.7213050e-07 1.4104274e-11], sum to 1.0000
[2019-04-03 22:09:59,608] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2867
[2019-04-03 22:09:59,623] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.3, 42.0, 0.0, 0.0, 26.0, 24.65143696616505, 0.1709530897751696, 0.0, 1.0, 43107.33324174085], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2417400.0000, 
sim time next is 2418000.0000, 
raw observation next is [-5.4, 42.33333333333333, 0.0, 0.0, 26.0, 24.65102450313191, 0.1622502105850496, 0.0, 1.0, 43105.29747559249], 
processed observation next is [0.0, 1.0, 0.31301939058171746, 0.4233333333333333, 0.0, 0.0, 0.6666666666666666, 0.5542520419276592, 0.5540834035283498, 0.0, 1.0, 0.20526332131234518], 
reward next is 0.7947, 
noisyNet noise sample is [array([0.5230466], dtype=float32), -0.5509812]. 
=============================================
[2019-04-03 22:09:59,626] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[69.3189  ]
 [69.489075]
 [69.6729  ]
 [69.87319 ]
 [70.087555]], R is [[69.2562561 ]
 [69.35842133]
 [69.45957184]
 [69.55979156]
 [69.65913391]].
[2019-04-03 22:10:01,153] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-03 22:10:01,206] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-03 22:10:01,207] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:10:01,208] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run3
[2019-04-03 22:10:01,285] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-03 22:10:01,285] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:10:01,287] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run3
[2019-04-03 22:10:01,339] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-03 22:10:01,339] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:10:01,341] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run3
[2019-04-03 22:10:43,313] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.4411699], dtype=float32), 0.33004472]
[2019-04-03 22:10:43,314] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [11.88571411666667, 87.24346477666667, 84.631277725, 0.0, 20.0, 21.11275124841271, -0.6877300651133843, 1.0, 1.0, 0.0]
[2019-04-03 22:10:43,314] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-03 22:10:43,315] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [5.9329364e-19 1.7081232e-10 3.1591527e-09 9.9999988e-01 3.5881048e-11
 1.1306648e-07 4.4157746e-12], sampled 0.3266407123791709
[2019-04-03 22:11:30,873] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5789.3442 154058886.3106 -1773.4456
[2019-04-03 22:11:37,693] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5043.9723 174442790.2437 -2782.7422
[2019-04-03 22:11:50,163] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5275.4799 196153257.0920 -2270.6413
[2019-04-03 22:11:51,186] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 200000, evaluation results [200000.0, 5043.972297248031, 174442790.24373633, -2782.7422032994377, 5789.344163732218, 154058886.31058824, -1773.4456022536383, 5275.479945300525, 196153257.0919658, -2270.6413487550612]
[2019-04-03 22:12:01,884] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.3840776e-31 3.8248116e-18 3.3778869e-16 1.0000000e+00 1.4949352e-18
 2.1470582e-16 3.9004687e-20], sum to 1.0000
[2019-04-03 22:12:01,884] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5983
[2019-04-03 22:12:01,926] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.416666666666667, 73.33333333333333, 87.66666666666667, 60.66666666666667, 26.0, 25.8365895151169, 0.3531739161155074, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2625000.0000, 
sim time next is 2625600.0000, 
raw observation next is [-6.133333333333335, 71.66666666666667, 90.33333333333333, 75.83333333333334, 26.0, 25.82779172399819, 0.3593648272531154, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.29270544783010155, 0.7166666666666667, 0.3011111111111111, 0.0837937384898711, 0.6666666666666666, 0.6523159769998491, 0.6197882757510385, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.91660273], dtype=float32), -1.4623982]. 
=============================================
[2019-04-03 22:12:05,879] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.5068660e-26 2.8871457e-15 9.5161997e-13 1.0000000e+00 2.6410085e-15
 1.1647723e-10 2.0333134e-16], sum to 1.0000
[2019-04-03 22:12:05,879] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0732
[2019-04-03 22:12:05,890] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.733333333333333, 28.66666666666667, 178.8333333333333, 405.3333333333334, 26.0, 25.30308667457226, 0.314852574383751, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2553600.0000, 
sim time next is 2554200.0000, 
raw observation next is [3.0, 28.0, 171.0, 482.0, 26.0, 25.41834293453547, 0.3428142724455684, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5457063711911359, 0.28, 0.57, 0.532596685082873, 0.6666666666666666, 0.6181952445446225, 0.6142714241485229, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.069424], dtype=float32), 1.0356063]. 
=============================================
[2019-04-03 22:12:06,262] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.6002250e-28 4.8731433e-17 2.0951248e-14 1.0000000e+00 7.2912480e-16
 5.3799767e-11 3.2071687e-18], sum to 1.0000
[2019-04-03 22:12:06,263] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3530
[2019-04-03 22:12:06,293] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.1, 58.0, 224.0, 171.0, 26.0, 25.73362999716765, 0.3870396064142303, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2633400.0000, 
sim time next is 2634000.0000, 
raw observation next is [-2.833333333333333, 56.66666666666667, 227.5, 167.0, 26.0, 25.71648444177423, 0.3868027801010965, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.3841181902123731, 0.5666666666666668, 0.7583333333333333, 0.18453038674033148, 0.6666666666666666, 0.6430403701478525, 0.6289342600336988, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.19621745], dtype=float32), 1.3771554]. 
=============================================
[2019-04-03 22:12:06,302] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[93.743195]
 [93.774925]
 [93.44449 ]
 [92.837456]
 [92.850426]], R is [[93.68241119]
 [93.74559021]
 [93.57091522]
 [93.2488327 ]
 [93.31634521]].
[2019-04-03 22:12:12,822] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.5672329e-20 8.5197613e-12 3.2645243e-08 9.9999559e-01 3.1913874e-10
 4.4512894e-06 8.5385189e-12], sum to 1.0000
[2019-04-03 22:12:12,824] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7633
[2019-04-03 22:12:12,882] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.4166666666666667, 46.33333333333334, 191.0, 189.6666666666667, 26.0, 25.46031412401701, 0.4455401773113885, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2639400.0000, 
sim time next is 2640000.0000, 
raw observation next is [-0.2333333333333334, 45.66666666666667, 177.5, 200.3333333333333, 26.0, 25.84649216947739, 0.4813745730640873, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.456140350877193, 0.4566666666666667, 0.5916666666666667, 0.2213627992633517, 0.6666666666666666, 0.6538743474564491, 0.6604581910213624, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.69397724], dtype=float32), -0.899698]. 
=============================================
[2019-04-03 22:12:12,906] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[71.34145 ]
 [71.97003 ]
 [72.228424]
 [71.74907 ]
 [71.54306 ]], R is [[71.04421997]
 [71.33377838]
 [71.30812073]
 [70.65331268]
 [70.31493378]].
[2019-04-03 22:12:14,617] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.1827137e-22 5.6670638e-14 2.5831845e-10 1.0000000e+00 5.9616998e-13
 7.2180312e-10 5.7584925e-14], sum to 1.0000
[2019-04-03 22:12:14,618] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7346
[2019-04-03 22:12:14,648] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-16.0, 83.0, 0.0, 0.0, 26.0, 23.08824129612724, -0.1239193550621719, 0.0, 1.0, 43343.74044588086], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2700000.0000, 
sim time next is 2700600.0000, 
raw observation next is [-15.83333333333333, 83.0, 0.0, 0.0, 26.0, 23.00802433545124, -0.1373052174927247, 0.0, 1.0, 43285.8750069052], 
processed observation next is [1.0, 0.2608695652173913, 0.02400738688827338, 0.83, 0.0, 0.0, 0.6666666666666666, 0.41733536128760323, 0.45423159416909176, 0.0, 1.0, 0.20612321431859618], 
reward next is 0.7939, 
noisyNet noise sample is [array([-0.9625291], dtype=float32), 1.6316282]. 
=============================================
[2019-04-03 22:12:16,142] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.7645061e-23 1.0915491e-13 2.1490865e-12 1.0000000e+00 2.4660719e-14
 1.5856061e-10 3.0789129e-15], sum to 1.0000
[2019-04-03 22:12:16,142] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5595
[2019-04-03 22:12:16,173] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.2, 60.5, 0.0, 0.0, 26.0, 25.19194566223795, 0.3902333788554735, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2661000.0000, 
sim time next is 2661600.0000, 
raw observation next is [-1.2, 61.00000000000001, 0.0, 0.0, 26.0, 25.19958971886931, 0.3729138759017004, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.42936288088642666, 0.6100000000000001, 0.0, 0.0, 0.6666666666666666, 0.5999658099057757, 0.6243046253005667, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.17238948], dtype=float32), -0.76615626]. 
=============================================
[2019-04-03 22:12:22,042] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.646614e-25 2.628034e-15 6.284883e-13 1.000000e+00 9.003322e-15
 9.496083e-11 1.537615e-16], sum to 1.0000
[2019-04-03 22:12:22,048] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7324
[2019-04-03 22:12:22,061] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.166666666666666, 59.83333333333334, 0.0, 0.0, 26.0, 24.44306858244278, 0.1547975692137588, 0.0, 1.0, 40765.97050748167], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2779800.0000, 
sim time next is 2780400.0000, 
raw observation next is [-6.333333333333333, 60.66666666666667, 0.0, 0.0, 26.0, 24.47681007647444, 0.1529900940123659, 0.0, 1.0, 40769.99870622924], 
processed observation next is [1.0, 0.17391304347826086, 0.28716528162511545, 0.6066666666666667, 0.0, 0.0, 0.6666666666666666, 0.5397341730395366, 0.550996698004122, 0.0, 1.0, 0.194142850982044], 
reward next is 0.8059, 
noisyNet noise sample is [array([0.5461247], dtype=float32), -0.08636715]. 
=============================================
[2019-04-03 22:12:24,574] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.08044787e-25 1.14247275e-13 1.05349245e-11 1.00000000e+00
 6.63931741e-15 4.62871625e-12 1.47342686e-14], sum to 1.0000
[2019-04-03 22:12:24,574] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0885
[2019-04-03 22:12:24,590] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 81.33333333333334, 0.0, 0.0, 26.0, 25.11922476812327, 0.2957754292357351, 0.0, 1.0, 56223.8232665979], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2859600.0000, 
sim time next is 2860200.0000, 
raw observation next is [1.0, 82.5, 0.0, 0.0, 26.0, 25.10678755959061, 0.2943149028761669, 0.0, 1.0, 56190.29491634563], 
processed observation next is [1.0, 0.08695652173913043, 0.4903047091412743, 0.825, 0.0, 0.0, 0.6666666666666666, 0.592232296632551, 0.598104967625389, 0.0, 1.0, 0.2675728329349792], 
reward next is 0.7324, 
noisyNet noise sample is [array([1.1938579], dtype=float32), 0.7002565]. 
=============================================
[2019-04-03 22:12:28,459] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.8331151e-24 8.5236467e-13 6.8508442e-11 1.0000000e+00 7.0911809e-13
 3.7904342e-09 2.5074547e-14], sum to 1.0000
[2019-04-03 22:12:28,460] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9190
[2019-04-03 22:12:28,542] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 93.0, 0.0, 0.0, 26.0, 24.80962299489654, 0.2404127293227629, 0.0, 1.0, 55777.89861489003], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2869200.0000, 
sim time next is 2869800.0000, 
raw observation next is [1.0, 94.16666666666666, 0.0, 0.0, 26.0, 24.83654257675716, 0.238330936419893, 0.0, 1.0, 55764.72135766777], 
processed observation next is [1.0, 0.21739130434782608, 0.4903047091412743, 0.9416666666666665, 0.0, 0.0, 0.6666666666666666, 0.56971188139643, 0.5794436454732977, 0.0, 1.0, 0.2655462921793703], 
reward next is 0.7345, 
noisyNet noise sample is [array([-0.9816289], dtype=float32), -0.5544955]. 
=============================================
[2019-04-03 22:12:29,394] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2655283e-21 1.7411413e-11 1.1556072e-09 1.0000000e+00 2.6848459e-11
 1.5665670e-08 1.1628593e-13], sum to 1.0000
[2019-04-03 22:12:29,394] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5838
[2019-04-03 22:12:29,444] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 81.5, 0.0, 0.0, 26.0, 25.00047585508761, 0.4418587343767859, 0.0, 1.0, 101749.5652524668], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2925000.0000, 
sim time next is 2925600.0000, 
raw observation next is [-1.0, 82.66666666666667, 0.0, 0.0, 26.0, 25.09548248273574, 0.4687732792137925, 0.0, 1.0, 64644.21166589476], 
processed observation next is [1.0, 0.8695652173913043, 0.4349030470914128, 0.8266666666666667, 0.0, 0.0, 0.6666666666666666, 0.591290206894645, 0.6562577597379309, 0.0, 1.0, 0.30782957936140365], 
reward next is 0.6922, 
noisyNet noise sample is [array([0.43607858], dtype=float32), 1.853726]. 
=============================================
[2019-04-03 22:12:35,151] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.4786087e-21 2.5259478e-12 4.8995552e-10 1.0000000e+00 7.8035137e-13
 7.5759105e-10 2.8662766e-13], sum to 1.0000
[2019-04-03 22:12:35,151] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5660
[2019-04-03 22:12:35,171] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 72.33333333333334, 0.0, 0.0, 26.0, 23.73310335852472, -0.0154404130890531, 0.0, 1.0, 40257.14743058592], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3046800.0000, 
sim time next is 3047400.0000, 
raw observation next is [-6.0, 73.5, 0.0, 0.0, 26.0, 23.70673305169077, -0.02006278648256676, 0.0, 1.0, 40309.20008162671], 
processed observation next is [0.0, 0.2608695652173913, 0.296398891966759, 0.735, 0.0, 0.0, 0.6666666666666666, 0.4755610876408974, 0.4933124045058111, 0.0, 1.0, 0.19194857181727004], 
reward next is 0.8081, 
noisyNet noise sample is [array([-1.4823942], dtype=float32), -1.5007519]. 
=============================================
[2019-04-03 22:12:36,247] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.5694237e-21 3.9941093e-12 7.7868691e-11 1.0000000e+00 1.5791216e-12
 1.2229570e-10 1.9854071e-14], sum to 1.0000
[2019-04-03 22:12:36,247] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8219
[2019-04-03 22:12:36,295] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.00604255882617, 0.3209373738062801, 0.0, 1.0, 48801.29409907426], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3004200.0000, 
sim time next is 3004800.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.0039578251085, 0.3207204469599723, 0.0, 1.0, 40921.83565136779], 
processed observation next is [0.0, 0.782608695652174, 0.40720221606648205, 0.6, 0.0, 0.0, 0.6666666666666666, 0.5836631520923751, 0.606906815653324, 0.0, 1.0, 0.19486588405413235], 
reward next is 0.8051, 
noisyNet noise sample is [array([-1.6336944], dtype=float32), 0.75405043]. 
=============================================
[2019-04-03 22:12:38,079] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.5435194e-21 1.6027842e-12 9.7107683e-11 1.0000000e+00 2.5202413e-11
 3.7987213e-09 2.3253516e-13], sum to 1.0000
[2019-04-03 22:12:38,079] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9960
[2019-04-03 22:12:38,145] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.01216597481954, 0.3165066491434173, 0.0, 1.0, 31285.3274087641], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3006000.0000, 
sim time next is 3006600.0000, 
raw observation next is [-2.166666666666667, 60.83333333333333, 0.0, 0.0, 26.0, 25.01217155059833, 0.3131507246121719, 0.0, 1.0, 36561.31839217097], 
processed observation next is [0.0, 0.8260869565217391, 0.4025854108956602, 0.6083333333333333, 0.0, 0.0, 0.6666666666666666, 0.5843476292165276, 0.6043835748707239, 0.0, 1.0, 0.1741015161531951], 
reward next is 0.8259, 
noisyNet noise sample is [array([1.4285723], dtype=float32), -0.36809105]. 
=============================================
[2019-04-03 22:12:45,820] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3721922e-31 1.1511636e-16 2.3629458e-15 1.0000000e+00 6.3204533e-17
 1.2309523e-12 3.0793630e-20], sum to 1.0000
[2019-04-03 22:12:45,820] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0769
[2019-04-03 22:12:45,853] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.833333333333334, 100.0, 88.33333333333334, 477.0, 26.0, 26.07159067414852, 0.5026342534109856, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3142200.0000, 
sim time next is 3142800.0000, 
raw observation next is [7.0, 100.0, 91.0, 519.5, 26.0, 26.21135069374869, 0.5173583551914502, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.6565096952908588, 1.0, 0.30333333333333334, 0.5740331491712707, 0.6666666666666666, 0.6842792244790576, 0.6724527850638168, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.9915074], dtype=float32), -0.2795945]. 
=============================================
[2019-04-03 22:12:49,951] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.1319060e-26 1.6488470e-15 3.2567693e-13 1.0000000e+00 1.9011236e-14
 4.0769409e-13 1.6800332e-16], sum to 1.0000
[2019-04-03 22:12:49,958] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0059
[2019-04-03 22:12:49,990] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.733333333333333, 100.0, 0.0, 0.0, 26.0, 25.19729730994136, 0.29251287761979, 0.0, 1.0, 53979.67027896643], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3126000.0000, 
sim time next is 3126600.0000, 
raw observation next is [2.8, 100.0, 0.0, 0.0, 26.0, 25.24632109332966, 0.2965888649438422, 0.0, 1.0, 53901.9814110275], 
processed observation next is [1.0, 0.17391304347826086, 0.5401662049861496, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6038600911108049, 0.5988629549812807, 0.0, 1.0, 0.2566761019572738], 
reward next is 0.7433, 
noisyNet noise sample is [array([0.95081425], dtype=float32), 1.3231145]. 
=============================================
[2019-04-03 22:12:50,833] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.8641156e-24 2.4209754e-15 7.8284804e-14 1.0000000e+00 6.5461444e-14
 4.3458058e-12 5.9597774e-17], sum to 1.0000
[2019-04-03 22:12:50,835] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1083
[2019-04-03 22:12:50,854] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.7, 86.0, 109.0, 752.0, 26.0, 26.41776429113253, 0.6985187319810303, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3234600.0000, 
sim time next is 3235200.0000, 
raw observation next is [-2.6, 84.0, 109.6666666666667, 761.8333333333334, 26.0, 26.41044701211077, 0.7061939709825796, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.3905817174515236, 0.84, 0.3655555555555557, 0.841804788213628, 0.6666666666666666, 0.7008705843425641, 0.7353979903275265, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.70396686], dtype=float32), 0.3440396]. 
=============================================
[2019-04-03 22:12:52,709] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.8582501e-25 2.0621678e-13 2.7882206e-12 1.0000000e+00 2.3500125e-13
 1.2395389e-10 1.2015682e-16], sum to 1.0000
[2019-04-03 22:12:52,710] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8968
[2019-04-03 22:12:52,729] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 93.0, 0.0, 0.0, 26.0, 25.4210052140895, 0.631533918731311, 0.0, 1.0, 43686.15855874274], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3193200.0000, 
sim time next is 3193800.0000, 
raw observation next is [2.0, 93.0, 0.0, 0.0, 26.0, 25.51459271548904, 0.6373342712827814, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.518005540166205, 0.93, 0.0, 0.0, 0.6666666666666666, 0.6262160596240868, 0.7124447570942604, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5548072], dtype=float32), 1.4533598]. 
=============================================
[2019-04-03 22:12:53,507] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.0112520e-22 1.2017037e-12 1.1493236e-11 1.0000000e+00 1.9456528e-12
 1.8147919e-10 8.5095217e-15], sum to 1.0000
[2019-04-03 22:12:53,510] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3541
[2019-04-03 22:12:53,526] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.166666666666667, 71.16666666666667, 0.0, 0.0, 26.0, 25.04233650639868, 0.3863407445507412, 0.0, 1.0, 43749.63653430805], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3287400.0000, 
sim time next is 3288000.0000, 
raw observation next is [-7.333333333333334, 72.33333333333334, 0.0, 0.0, 26.0, 25.04907506087838, 0.3805829478910958, 0.0, 1.0, 43761.26229218441], 
processed observation next is [1.0, 0.043478260869565216, 0.2594644506001847, 0.7233333333333334, 0.0, 0.0, 0.6666666666666666, 0.587422921739865, 0.6268609826303653, 0.0, 1.0, 0.20838696329611622], 
reward next is 0.7916, 
noisyNet noise sample is [array([1.5019102], dtype=float32), 0.3542031]. 
=============================================
[2019-04-03 22:12:53,536] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[66.71344]
 [67.0926 ]
 [67.6082 ]
 [68.07981]
 [68.57674]], R is [[66.56488037]
 [66.69090271]
 [66.8157959 ]
 [66.93959808]
 [67.06224823]].
[2019-04-03 22:12:54,889] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.85559033e-25 1.36099314e-14 2.53499240e-12 1.00000000e+00
 8.62526806e-14 1.08690487e-11 7.79763920e-16], sum to 1.0000
[2019-04-03 22:12:54,889] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1908
[2019-04-03 22:12:54,910] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 93.0, 0.0, 0.0, 26.0, 25.59432046310412, 0.6257093286045742, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3195000.0000, 
sim time next is 3195600.0000, 
raw observation next is [2.0, 93.0, 0.0, 0.0, 26.0, 25.60528811520976, 0.6151634927576999, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.518005540166205, 0.93, 0.0, 0.0, 0.6666666666666666, 0.6337740096008133, 0.7050544975859, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1872635], dtype=float32), 0.5158603]. 
=============================================
[2019-04-03 22:13:07,615] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.08358636e-23 1.00449195e-12 1.31112189e-12 1.00000000e+00
 3.09455478e-12 1.42747436e-09 7.76605440e-16], sum to 1.0000
[2019-04-03 22:13:07,616] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9816
[2019-04-03 22:13:07,627] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 49.0, 99.66666666666666, 765.3333333333334, 26.0, 26.70104312391956, 0.719860404447319, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3508800.0000, 
sim time next is 3509400.0000, 
raw observation next is [3.0, 49.0, 97.33333333333334, 749.6666666666667, 26.0, 26.74344988119462, 0.7223304576243632, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.49, 0.3244444444444445, 0.8283609576427257, 0.6666666666666666, 0.7286208234328851, 0.7407768192081211, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8416052], dtype=float32), -0.33868337]. 
=============================================
[2019-04-03 22:13:12,194] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.05258466e-26 3.14924711e-15 1.51512483e-13 1.00000000e+00
 2.39997485e-14 6.64924713e-13 1.09389789e-17], sum to 1.0000
[2019-04-03 22:13:12,195] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5791
[2019-04-03 22:13:12,209] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.3333333333333334, 63.66666666666667, 100.6666666666667, 686.6666666666667, 26.0, 25.85387625279866, 0.5189949064705648, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3490800.0000, 
sim time next is 3491400.0000, 
raw observation next is [-0.1666666666666666, 61.83333333333333, 102.3333333333333, 703.3333333333334, 26.0, 26.01452750198677, 0.5421911438660643, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4579870729455217, 0.6183333333333333, 0.341111111111111, 0.7771639042357275, 0.6666666666666666, 0.6678772918322308, 0.6807303812886881, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4542322], dtype=float32), -0.4002936]. 
=============================================
[2019-04-03 22:13:15,307] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.6453743e-26 2.6329123e-14 1.9503928e-13 1.0000000e+00 2.6535018e-14
 3.3444945e-11 2.3229053e-16], sum to 1.0000
[2019-04-03 22:13:15,309] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2885
[2019-04-03 22:13:15,315] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [12.0, 24.0, 113.5, 789.5, 26.0, 25.68766442348429, 0.4991903413894219, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3668400.0000, 
sim time next is 3669000.0000, 
raw observation next is [10.66666666666667, 27.5, 114.3333333333333, 798.3333333333334, 26.0, 25.69809642866755, 0.4997874376128952, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.7580794090489382, 0.275, 0.381111111111111, 0.8821362799263353, 0.6666666666666666, 0.6415080357222959, 0.6665958125376318, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.09115051], dtype=float32), 0.3123463]. 
=============================================
[2019-04-03 22:13:15,330] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[93.169754]
 [93.4117  ]
 [93.68625 ]
 [93.930115]
 [94.13797 ]], R is [[92.99434662]
 [93.06440735]
 [93.13376617]
 [93.20243073]
 [93.27040863]].
[2019-04-03 22:13:16,810] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.8278196e-25 4.6853667e-13 5.3437273e-13 1.0000000e+00 1.6943884e-13
 3.1047959e-11 2.5189325e-16], sum to 1.0000
[2019-04-03 22:13:16,811] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9063
[2019-04-03 22:13:16,854] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.0, 65.0, 105.5, 717.0, 26.0, 25.76153513195192, 0.507373816237126, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3578400.0000, 
sim time next is 3579000.0000, 
raw observation next is [-4.833333333333334, 63.16666666666667, 107.3333333333333, 730.6666666666666, 26.0, 25.70160110788565, 0.4986732650702106, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.32871652816251157, 0.6316666666666667, 0.3577777777777777, 0.807366482504604, 0.6666666666666666, 0.6418000923238042, 0.6662244216900702, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7643], dtype=float32), 0.5605714]. 
=============================================
[2019-04-03 22:13:16,858] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[88.70369 ]
 [89.25915 ]
 [89.67025 ]
 [90.064674]
 [90.25451 ]], R is [[88.27754974]
 [88.39477539]
 [88.51082611]
 [88.62571716]
 [88.73946381]].
[2019-04-03 22:13:22,411] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.9801696e-21 1.2824836e-11 8.4766916e-10 1.0000000e+00 6.2725871e-12
 1.4091253e-09 3.5709920e-13], sum to 1.0000
[2019-04-03 22:13:22,414] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3708
[2019-04-03 22:13:22,429] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.0, 77.0, 0.0, 0.0, 26.0, 24.70954607277753, 0.2303202758449619, 0.0, 1.0, 42838.82894860673], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3826800.0000, 
sim time next is 3827400.0000, 
raw observation next is [-5.0, 77.0, 0.0, 0.0, 26.0, 24.67823031375888, 0.2233559700656119, 0.0, 1.0, 42782.35399864827], 
processed observation next is [1.0, 0.30434782608695654, 0.32409972299168976, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5565191928132401, 0.5744519900218706, 0.0, 1.0, 0.20372549523165845], 
reward next is 0.7963, 
noisyNet noise sample is [array([-1.4737177], dtype=float32), 0.45710155]. 
=============================================
[2019-04-03 22:13:23,621] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.2868968e-29 3.0468258e-17 5.0880952e-15 1.0000000e+00 2.5751861e-17
 2.5589482e-11 6.7797894e-19], sum to 1.0000
[2019-04-03 22:13:23,621] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8099
[2019-04-03 22:13:23,634] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.333333333333333, 60.0, 111.1666666666667, 782.8333333333334, 26.0, 26.55612262888638, 0.6272319979571849, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3840000.0000, 
sim time next is 3840600.0000, 
raw observation next is [-1.166666666666667, 60.0, 112.3333333333333, 790.6666666666667, 26.0, 26.57259700018373, 0.6328546667760958, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.43028624192059095, 0.6, 0.37444444444444436, 0.8736648250460406, 0.6666666666666666, 0.7143830833486442, 0.7109515555920319, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.0305244], dtype=float32), 0.061332397]. 
=============================================
[2019-04-03 22:13:33,036] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.2167184e-21 2.3513153e-11 1.7661070e-10 1.0000000e+00 1.2992901e-11
 4.5080912e-09 5.5136185e-13], sum to 1.0000
[2019-04-03 22:13:33,065] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5875
[2019-04-03 22:13:33,120] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-12.0, 63.00000000000001, 0.0, 0.0, 26.0, 24.38801183790561, 0.1936584808212278, 0.0, 1.0, 43777.23037785777], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3982800.0000, 
sim time next is 3983400.0000, 
raw observation next is [-12.0, 63.0, 0.0, 0.0, 26.0, 24.38413364002111, 0.1806322529518629, 0.0, 1.0, 43765.62954486269], 
processed observation next is [1.0, 0.08695652173913043, 0.13019390581717452, 0.63, 0.0, 0.0, 0.6666666666666666, 0.5320111366684257, 0.5602107509839543, 0.0, 1.0, 0.20840775973744138], 
reward next is 0.7916, 
noisyNet noise sample is [array([0.0381131], dtype=float32), 0.18817963]. 
=============================================
[2019-04-03 22:13:46,850] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.9960721e-26 3.0434460e-13 1.2099280e-12 1.0000000e+00 1.6357949e-14
 3.1922947e-11 7.7115288e-16], sum to 1.0000
[2019-04-03 22:13:46,850] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4183
[2019-04-03 22:13:46,886] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.5, 42.5, 0.0, 0.0, 26.0, 25.3391475820235, 0.411579838436313, 0.0, 1.0, 46773.3717495204], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4152600.0000, 
sim time next is 4153200.0000, 
raw observation next is [-1.666666666666667, 43.66666666666666, 0.0, 0.0, 26.0, 25.34193089866002, 0.4081655314764128, 0.0, 1.0, 41711.45392747389], 
processed observation next is [0.0, 0.043478260869565216, 0.4164358264081256, 0.4366666666666666, 0.0, 0.0, 0.6666666666666666, 0.6118275748883351, 0.6360551771588042, 0.0, 1.0, 0.198625971083209], 
reward next is 0.8014, 
noisyNet noise sample is [array([1.1846365], dtype=float32), -0.66937554]. 
=============================================
[2019-04-03 22:14:27,090] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.8824673e-23 5.2627976e-13 8.0025611e-11 1.0000000e+00 7.6062464e-13
 1.3774520e-09 7.4353467e-14], sum to 1.0000
[2019-04-03 22:14:27,090] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6402
[2019-04-03 22:14:27,152] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.9333333333333333, 73.0, 0.0, 0.0, 26.0, 25.34747231279461, 0.4472415114791808, 0.0, 1.0, 42507.43949570083], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4503000.0000, 
sim time next is 4503600.0000, 
raw observation next is [-1.0, 73.0, 0.0, 0.0, 26.0, 25.37820716584761, 0.4536743103513864, 0.0, 1.0, 38845.08339354803], 
processed observation next is [1.0, 0.13043478260869565, 0.4349030470914128, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6148505971539674, 0.6512247701171288, 0.0, 1.0, 0.18497658758832397], 
reward next is 0.8150, 
noisyNet noise sample is [array([1.3138825], dtype=float32), -1.5913926]. 
=============================================
[2019-04-03 22:14:27,517] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.1832447e-23 2.2259035e-12 6.0982198e-11 1.0000000e+00 4.3334376e-13
 1.0885864e-09 3.4637640e-14], sum to 1.0000
[2019-04-03 22:14:27,517] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7732
[2019-04-03 22:14:27,564] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.9166666666666666, 73.0, 0.0, 0.0, 26.0, 25.37348421044976, 0.4342007444057552, 0.0, 1.0, 50378.31392951412], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4506600.0000, 
sim time next is 4507200.0000, 
raw observation next is [-0.9, 73.0, 0.0, 0.0, 26.0, 25.37416586175607, 0.4260779547760612, 0.0, 1.0, 45315.33373559439], 
processed observation next is [1.0, 0.17391304347826086, 0.43767313019390586, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6145138218130057, 0.6420259849253537, 0.0, 1.0, 0.21578730350283043], 
reward next is 0.7842, 
noisyNet noise sample is [array([-1.8708025], dtype=float32), 0.5021942]. 
=============================================
[2019-04-03 22:14:28,384] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.3222573e-26 3.9059578e-15 1.2542512e-13 1.0000000e+00 4.0200734e-15
 5.4367972e-14 5.3456534e-17], sum to 1.0000
[2019-04-03 22:14:28,390] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9380
[2019-04-03 22:14:28,400] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.5, 69.0, 0.0, 0.0, 26.0, 25.41576766497441, 0.4035105577577499, 0.0, 1.0, 51858.28422197361], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4593600.0000, 
sim time next is 4594200.0000, 
raw observation next is [-1.583333333333333, 69.33333333333333, 0.0, 0.0, 26.0, 25.34856445953804, 0.3996179270284876, 0.0, 1.0, 67012.6934579657], 
processed observation next is [1.0, 0.17391304347826086, 0.4187442289935365, 0.6933333333333332, 0.0, 0.0, 0.6666666666666666, 0.6123803716281699, 0.6332059756761625, 0.0, 1.0, 0.3191080640855509], 
reward next is 0.6809, 
noisyNet noise sample is [array([0.22177179], dtype=float32), -0.35976982]. 
=============================================
[2019-04-03 22:14:28,974] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.9006522e-25 7.1001771e-14 1.4989263e-11 1.0000000e+00 1.1422505e-13
 5.7753524e-11 3.5801131e-15], sum to 1.0000
[2019-04-03 22:14:28,974] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9037
[2019-04-03 22:14:29,010] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 80.0, 0.0, 0.0, 26.0, 25.61494848365923, 0.5138844977000021, 0.0, 1.0, 18735.6487910902], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4431600.0000, 
sim time next is 4432200.0000, 
raw observation next is [2.0, 80.0, 0.0, 0.0, 26.0, 25.58359069485099, 0.5047793785115325, 0.0, 1.0, 30181.95782463135], 
processed observation next is [1.0, 0.30434782608695654, 0.518005540166205, 0.8, 0.0, 0.0, 0.6666666666666666, 0.6319658912375825, 0.6682597928371775, 0.0, 1.0, 0.14372360868872072], 
reward next is 0.8563, 
noisyNet noise sample is [array([1.1061444], dtype=float32), -0.124763586]. 
=============================================
[2019-04-03 22:14:36,087] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.1381373e-27 2.3930727e-14 4.1651453e-13 1.0000000e+00 4.4075095e-13
 1.2168253e-11 9.2632072e-17], sum to 1.0000
[2019-04-03 22:14:36,087] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2735
[2019-04-03 22:14:36,148] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.95, 49.83333333333334, 200.3333333333333, 442.3333333333334, 26.0, 27.2949911806831, 0.8653183832388179, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4632600.0000, 
sim time next is 4633200.0000, 
raw observation next is [5.0, 50.0, 199.0, 364.0, 26.0, 27.39803003979415, 0.8812215734781818, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.6011080332409973, 0.5, 0.6633333333333333, 0.4022099447513812, 0.6666666666666666, 0.7831691699828459, 0.7937405244927273, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.14100182], dtype=float32), 0.4352073]. 
=============================================
[2019-04-03 22:14:49,924] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.6461845e-24 1.9024767e-14 2.2068296e-12 1.0000000e+00 4.6896108e-14
 1.2951680e-11 9.2485939e-16], sum to 1.0000
[2019-04-03 22:14:49,924] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1734
[2019-04-03 22:14:49,943] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 46.33333333333334, 0.0, 0.0, 26.0, 25.21446660026619, 0.4183232410269039, 0.0, 1.0, 56619.91032928967], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4823400.0000, 
sim time next is 4824000.0000, 
raw observation next is [1.0, 47.0, 0.0, 0.0, 26.0, 25.33007870057573, 0.431761796121312, 0.0, 1.0, 45613.18788537791], 
processed observation next is [0.0, 0.8695652173913043, 0.4903047091412743, 0.47, 0.0, 0.0, 0.6666666666666666, 0.6108398917146441, 0.643920598707104, 0.0, 1.0, 0.21720565659703767], 
reward next is 0.7828, 
noisyNet noise sample is [array([0.07333731], dtype=float32), -1.0486108]. 
=============================================
[2019-04-03 22:14:49,946] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[77.20432]
 [76.78217]
 [76.0422 ]
 [75.02413]
 [73.92376]], R is [[77.45253754]
 [77.40840149]
 [77.20786285]
 [76.65290833]
 [75.93959045]].
[2019-04-03 22:14:57,435] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.3498928e-24 8.3535008e-15 2.2408939e-12 1.0000000e+00 4.2288611e-15
 9.5521424e-11 2.3005516e-15], sum to 1.0000
[2019-04-03 22:14:57,438] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1641
[2019-04-03 22:14:57,456] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.833333333333333, 69.16666666666667, 0.0, 0.0, 26.0, 24.78060795889647, 0.2227844876729617, 0.0, 1.0, 39435.71227414166], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4857000.0000, 
sim time next is 4857600.0000, 
raw observation next is [-3.666666666666667, 67.33333333333334, 0.0, 0.0, 26.0, 24.7790003560034, 0.217262363353719, 0.0, 1.0, 39495.63758729546], 
processed observation next is [0.0, 0.21739130434782608, 0.3610341643582641, 0.6733333333333335, 0.0, 0.0, 0.6666666666666666, 0.5649166963336167, 0.572420787784573, 0.0, 1.0, 0.18807446470140696], 
reward next is 0.8119, 
noisyNet noise sample is [array([1.8941076], dtype=float32), -1.9866402]. 
=============================================
[2019-04-03 22:15:02,901] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.8851243e-27 5.6104362e-15 2.4949140e-13 1.0000000e+00 8.3685188e-16
 1.2755091e-11 7.2388375e-17], sum to 1.0000
[2019-04-03 22:15:02,904] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8782
[2019-04-03 22:15:02,958] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 46.0, 46.5, 280.0, 26.0, 25.22759297626612, 0.2751161859718199, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4953600.0000, 
sim time next is 4954200.0000, 
raw observation next is [-1.833333333333333, 44.83333333333334, 62.00000000000001, 373.3333333333334, 26.0, 25.18668349071388, 0.2710698352613869, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.41181902123730385, 0.4483333333333334, 0.2066666666666667, 0.412523020257827, 0.6666666666666666, 0.5988902908928232, 0.5903566117537956, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5893333], dtype=float32), -0.6015037]. 
=============================================
[2019-04-03 22:15:11,996] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:15:11,997] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:15:12,022] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run3
[2019-04-03 22:15:14,953] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:15:14,953] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:15:14,955] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run3
[2019-04-03 22:15:15,181] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.3823685e-23 1.3660679e-13 1.0723469e-10 1.0000000e+00 2.1629684e-13
 1.8910618e-11 1.0732164e-14], sum to 1.0000
[2019-04-03 22:15:15,181] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9758
[2019-04-03 22:15:15,214] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 50.0, 0.0, 0.0, 26.0, 25.03037806962126, 0.2197346755060496, 0.0, 1.0, 38671.46466260744], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4948800.0000, 
sim time next is 4949400.0000, 
raw observation next is [-3.0, 50.0, 0.0, 0.0, 26.0, 24.97386009954149, 0.2106167006281967, 0.0, 1.0, 38699.36080466043], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.5, 0.0, 0.0, 0.6666666666666666, 0.5811550082951241, 0.5702055668760656, 0.0, 1.0, 0.18428267049838298], 
reward next is 0.8157, 
noisyNet noise sample is [array([-0.19874902], dtype=float32), 0.050735425]. 
=============================================
[2019-04-03 22:15:15,407] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.3591912e-29 2.4959001e-16 1.7750256e-14 1.0000000e+00 6.8642724e-16
 8.4905455e-13 1.4945132e-18], sum to 1.0000
[2019-04-03 22:15:15,412] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6680
[2019-04-03 22:15:15,439] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [8.5, 25.5, 124.0, 865.0, 26.0, 27.44755140689433, 0.8705603298880525, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5056200.0000, 
sim time next is 5056800.0000, 
raw observation next is [8.666666666666668, 25.33333333333333, 123.0, 864.1666666666667, 26.0, 27.2090781293771, 0.8491683161207071, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.7026777469990768, 0.2533333333333333, 0.41, 0.9548802946593002, 0.6666666666666666, 0.7674231774480917, 0.7830561053735691, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.8966634], dtype=float32), 0.26424402]. 
=============================================
[2019-04-03 22:15:15,714] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:15:15,714] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:15:15,717] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run3
[2019-04-03 22:15:17,099] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:15:17,100] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:15:17,131] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run3
[2019-04-03 22:15:17,765] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:15:17,765] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:15:17,767] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run3
[2019-04-03 22:15:18,006] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:15:18,006] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:15:18,032] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run3
[2019-04-03 22:15:18,345] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:15:18,345] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:15:18,347] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run3
[2019-04-03 22:15:19,165] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:15:19,165] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:15:19,167] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run3
[2019-04-03 22:15:21,221] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:15:21,221] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:15:21,223] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run3
[2019-04-03 22:15:21,465] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:15:21,465] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:15:21,467] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run3
[2019-04-03 22:15:23,073] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:15:23,073] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:15:23,075] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run3
[2019-04-03 22:15:23,653] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:15:23,653] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:15:23,655] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run3
[2019-04-03 22:15:24,927] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.4170259e-30 1.7321915e-18 3.2412531e-16 1.0000000e+00 1.5655442e-17
 3.5497451e-12 3.8768676e-20], sum to 1.0000
[2019-04-03 22:15:24,928] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4666
[2019-04-03 22:15:24,935] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [11.33333333333333, 19.66666666666667, 112.1666666666667, 825.8333333333333, 26.0, 28.10828845635121, 1.027266345899885, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5062800.0000, 
sim time next is 5063400.0000, 
raw observation next is [11.5, 19.5, 111.0, 819.0, 26.0, 28.29822797726312, 1.057243774114189, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.7811634349030472, 0.195, 0.37, 0.9049723756906077, 0.6666666666666666, 0.8581856647719267, 0.8524145913713963, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.47693363], dtype=float32), -0.39884573]. 
=============================================
[2019-04-03 22:15:25,745] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:15:25,757] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:15:25,759] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run3
[2019-04-03 22:15:25,997] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:15:25,997] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:15:26,005] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run3
[2019-04-03 22:15:27,361] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:15:27,361] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:15:27,363] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run3
[2019-04-03 22:15:28,169] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:15:28,170] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:15:28,171] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run3
[2019-04-03 22:15:30,669] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.4361108e-26 1.4911442e-15 7.9157042e-13 1.0000000e+00 7.2443346e-17
 7.1613063e-12 9.1028414e-17], sum to 1.0000
[2019-04-03 22:15:30,687] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6107
[2019-04-03 22:15:30,746] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.8, 86.0, 0.0, 0.0, 20.0, 19.05927140476307, -1.007600876341694, 0.0, 1.0, 24371.50048321574], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 68400.0000, 
sim time next is 69000.0000, 
raw observation next is [3.616666666666667, 86.5, 0.0, 0.0, 20.0, 19.06104134726149, -1.005430618297602, 0.0, 1.0, 27207.00570289432], 
processed observation next is [0.0, 0.8260869565217391, 0.5627885503231764, 0.865, 0.0, 0.0, 0.16666666666666666, 0.08842011227179085, 0.16485646056746597, 0.0, 1.0, 0.12955717001378247], 
reward next is 0.8704, 
noisyNet noise sample is [array([0.53853977], dtype=float32), -0.5122813]. 
=============================================
[2019-04-03 22:15:30,789] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[84.42512 ]
 [84.409546]
 [84.40371 ]
 [84.38676 ]
 [84.3735  ]], R is [[84.48339844]
 [84.52250671]
 [84.56503296]
 [84.6091156 ]
 [84.65559387]].
[2019-04-03 22:15:33,102] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.9059023e-14 9.3583337e-09 1.2670124e-06 9.9998951e-01 2.6851188e-08
 9.1932934e-06 1.3933023e-09], sum to 1.0000
[2019-04-03 22:15:33,139] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2055
[2019-04-03 22:15:33,197] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.8, 80.0, 190.0, 36.0, 19.0, 19.43254462424228, -1.056666354230989, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 124200.0000, 
sim time next is 124800.0000, 
raw observation next is [-7.799999999999999, 82.0, 189.0, 32.16666666666666, 19.0, 19.45672691733969, -1.081814955335701, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.24653739612188372, 0.82, 0.63, 0.03554327808471454, 0.08333333333333333, 0.12139390977830751, 0.13939501488809966, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.24623089], dtype=float32), 1.5012543]. 
=============================================
[2019-04-03 22:15:38,083] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.5362236e-20 9.7341059e-13 6.0846522e-10 1.0000000e+00 6.1917743e-14
 6.2222220e-11 6.8268843e-14], sum to 1.0000
[2019-04-03 22:15:38,084] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0149
[2019-04-03 22:15:38,097] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.9, 85.66666666666667, 0.0, 0.0, 19.0, 18.65668316138253, -1.134381419650754, 0.0, 1.0, 102875.3299029185], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 97800.0000, 
sim time next is 98400.0000, 
raw observation next is [-3.0, 84.33333333333334, 0.0, 0.0, 19.0, 18.68027390928736, -1.127987612531068, 0.0, 1.0, 58024.80217010051], 
processed observation next is [1.0, 0.13043478260869565, 0.3795013850415513, 0.8433333333333334, 0.0, 0.0, 0.08333333333333333, 0.05668949244061346, 0.12400412915631069, 0.0, 1.0, 0.2763085817623834], 
reward next is 0.7237, 
noisyNet noise sample is [array([-0.44014844], dtype=float32), -0.22053841]. 
=============================================
[2019-04-03 22:15:47,407] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.2933840e-23 1.4355621e-13 3.0134573e-09 9.9999845e-01 9.8494990e-15
 1.6073343e-06 2.8029178e-15], sum to 1.0000
[2019-04-03 22:15:47,407] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1971
[2019-04-03 22:15:47,424] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 23.0, 20.41049604780098, -0.8268500482111346, 0.0, 1.0, 46587.65766679077], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 196800.0000, 
sim time next is 197400.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 23.0, 20.40964781201227, -0.8251587866057556, 0.0, 1.0, 46584.8980447763], 
processed observation next is [1.0, 0.2608695652173913, 0.21606648199445982, 0.78, 0.0, 0.0, 0.4166666666666667, 0.20080398433435587, 0.2249470711314148, 0.0, 1.0, 0.22183284783226812], 
reward next is 0.7782, 
noisyNet noise sample is [array([-1.6129504], dtype=float32), 0.5817387]. 
=============================================
[2019-04-03 22:16:02,337] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.9403431e-20 3.7509839e-13 1.2392080e-09 9.9955326e-01 9.1837614e-12
 4.4665986e-04 2.0717892e-14], sum to 1.0000
[2019-04-03 22:16:02,338] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1083
[2019-04-03 22:16:02,423] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-10.5, 46.0, 51.5, 859.5, 26.0, 25.91010263821143, 0.2660620358370725, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 396000.0000, 
sim time next is 396600.0000, 
raw observation next is [-10.33333333333333, 45.0, 50.33333333333333, 850.3333333333334, 26.0, 25.38019348392116, 0.3107684025922671, 1.0, 1.0, 127983.7299234879], 
processed observation next is [1.0, 0.6086956521739131, 0.17636195752539252, 0.45, 0.16777777777777778, 0.9395948434622469, 0.6666666666666666, 0.6150161236600967, 0.6035894675307557, 1.0, 1.0, 0.60944633296899], 
reward next is 0.3906, 
noisyNet noise sample is [array([-0.8266647], dtype=float32), -0.43361422]. 
=============================================
[2019-04-03 22:16:09,449] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.4240603e-21 2.3094578e-11 6.0545178e-09 9.9996889e-01 2.5018091e-12
 3.1124793e-05 1.4185890e-14], sum to 1.0000
[2019-04-03 22:16:09,450] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6038
[2019-04-03 22:16:09,496] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.9, 36.33333333333333, 14.0, 274.3333333333334, 26.0, 25.99426684350549, 0.2538370231186862, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 406200.0000, 
sim time next is 406800.0000, 
raw observation next is [-8.9, 36.0, 10.5, 210.0, 26.0, 25.93168332221417, 0.3416757941852078, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.21606648199445982, 0.36, 0.035, 0.23204419889502761, 0.6666666666666666, 0.6609736101845142, 0.6138919313950693, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.18346977], dtype=float32), -0.67582136]. 
=============================================
[2019-04-03 22:16:11,531] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.3680145e-21 1.0381709e-11 7.9566703e-08 9.9973875e-01 7.4799325e-12
 2.6116017e-04 7.6625514e-14], sum to 1.0000
[2019-04-03 22:16:11,531] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3859
[2019-04-03 22:16:11,623] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-13.95, 63.0, 71.0, 729.0, 26.0, 25.58592601688154, 0.275071620490227, 1.0, 1.0, 49182.82244568649], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 383400.0000, 
sim time next is 384000.0000, 
raw observation next is [-13.76666666666667, 62.0, 68.83333333333334, 734.8333333333333, 26.0, 25.66429106449905, 0.2929477894255819, 1.0, 1.0, 48562.38125566705], 
processed observation next is [1.0, 0.43478260869565216, 0.08125577100646345, 0.62, 0.22944444444444448, 0.8119705340699815, 0.6666666666666666, 0.6386909220415875, 0.5976492631418606, 1.0, 1.0, 0.2312494345507955], 
reward next is 0.7688, 
noisyNet noise sample is [array([0.69266105], dtype=float32), -1.5776407]. 
=============================================
[2019-04-03 22:16:11,626] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[80.531784]
 [80.76628 ]
 [80.928635]
 [81.28952 ]
 [81.75765 ]], R is [[80.04890442]
 [80.01420593]
 [79.97950745]
 [79.95323944]
 [79.95785522]].
[2019-04-03 22:16:11,970] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.9452146e-21 2.0000406e-11 5.8299062e-08 9.9977559e-01 6.1425425e-13
 2.2440075e-04 5.0516642e-14], sum to 1.0000
[2019-04-03 22:16:11,971] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4961
[2019-04-03 22:16:11,996] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-10.6, 48.66666666666666, 0.0, 0.0, 26.0, 24.18301105731646, 0.05797876128959101, 0.0, 1.0, 44953.85630018304], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 424200.0000, 
sim time next is 424800.0000, 
raw observation next is [-10.6, 49.0, 0.0, 0.0, 26.0, 24.12602029763864, 0.04486284260288812, 0.0, 1.0, 44742.92017621888], 
processed observation next is [1.0, 0.9565217391304348, 0.1689750692520776, 0.49, 0.0, 0.0, 0.6666666666666666, 0.5105016914698867, 0.5149542808676294, 0.0, 1.0, 0.21306152464866135], 
reward next is 0.7869, 
noisyNet noise sample is [array([0.00520158], dtype=float32), 0.099479735]. 
=============================================
[2019-04-03 22:16:14,118] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.9192955e-20 1.1450057e-11 1.5821210e-07 9.9973792e-01 1.8956213e-11
 2.6191145e-04 2.3056391e-13], sum to 1.0000
[2019-04-03 22:16:14,119] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6153
[2019-04-03 22:16:14,219] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-9.666666666666668, 40.66666666666667, 0.0, 0.0, 26.0, 25.04542249145425, 0.2361770351010515, 1.0, 1.0, 122380.7200873134], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 415200.0000, 
sim time next is 415800.0000, 
raw observation next is [-9.75, 41.0, 0.0, 0.0, 26.0, 25.00067968177372, 0.2505547425914198, 1.0, 1.0, 119126.4449882472], 
processed observation next is [1.0, 0.8260869565217391, 0.19252077562326872, 0.41, 0.0, 0.0, 0.6666666666666666, 0.5833899734811432, 0.5835182475304733, 1.0, 1.0, 0.56726878565832], 
reward next is 0.4327, 
noisyNet noise sample is [array([0.9042378], dtype=float32), -1.1020639]. 
=============================================
[2019-04-03 22:16:17,941] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.6840602e-22 4.5699603e-12 9.7708906e-08 9.9999845e-01 9.0103093e-13
 1.4730316e-06 5.0701776e-14], sum to 1.0000
[2019-04-03 22:16:17,941] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0608
[2019-04-03 22:16:17,980] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-11.7, 54.0, 0.0, 0.0, 26.0, 23.76937678750893, -0.03343747451878076, 0.0, 1.0, 44860.31850849077], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 429000.0000, 
sim time next is 429600.0000, 
raw observation next is [-11.7, 54.00000000000001, 0.0, 0.0, 26.0, 23.70752265049268, -0.04653859858804465, 0.0, 1.0, 44921.52853886635], 
processed observation next is [1.0, 1.0, 0.13850415512465375, 0.54, 0.0, 0.0, 0.6666666666666666, 0.47562688754105675, 0.4844871338039851, 0.0, 1.0, 0.21391204066126832], 
reward next is 0.7861, 
noisyNet noise sample is [array([-0.7261264], dtype=float32), 0.84283566]. 
=============================================
[2019-04-03 22:16:22,230] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.76583581e-27 6.94648147e-17 3.86595297e-13 1.00000000e+00
 1.13383044e-17 1.08016451e-09 8.37157486e-18], sum to 1.0000
[2019-04-03 22:16:22,234] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6260
[2019-04-03 22:16:22,248] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-11.7, 54.0, 0.0, 0.0, 26.0, 23.68612328502263, -0.03489743001663027, 0.0, 1.0, 45028.88711352285], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 430800.0000, 
sim time next is 431400.0000, 
raw observation next is [-11.7, 54.0, 0.0, 0.0, 26.0, 23.67303296167939, -0.04589443114744567, 0.0, 1.0, 45086.27141969712], 
processed observation next is [1.0, 1.0, 0.13850415512465375, 0.54, 0.0, 0.0, 0.6666666666666666, 0.4727527468066158, 0.4847018562841848, 0.0, 1.0, 0.21469653056998628], 
reward next is 0.7853, 
noisyNet noise sample is [array([0.04274149], dtype=float32), -0.034139834]. 
=============================================
[2019-04-03 22:16:24,521] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.1245836e-38 2.6596193e-24 6.3945601e-19 1.0000000e+00 5.3520361e-26
 5.6413341e-17 3.0253512e-26], sum to 1.0000
[2019-04-03 22:16:24,521] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0736
[2019-04-03 22:16:24,602] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.8, 86.0, 0.0, 0.0, 26.0, 24.68974726963645, 0.1975613004210561, 0.0, 1.0, 39887.96828596338], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 529200.0000, 
sim time next is 529800.0000, 
raw observation next is [3.616666666666667, 85.33333333333334, 0.0, 0.0, 26.0, 24.67057545210193, 0.1942317676544762, 0.0, 1.0, 39936.0801322546], 
processed observation next is [0.0, 0.13043478260869565, 0.5627885503231764, 0.8533333333333334, 0.0, 0.0, 0.6666666666666666, 0.5558812876751608, 0.5647439225514921, 0.0, 1.0, 0.19017181015359336], 
reward next is 0.8098, 
noisyNet noise sample is [array([0.60630476], dtype=float32), -3.1274295]. 
=============================================
[2019-04-03 22:16:26,341] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.4624314e-31 8.8775993e-18 1.0234186e-13 1.0000000e+00 1.3299208e-20
 2.8392724e-09 1.0867797e-20], sum to 1.0000
[2019-04-03 22:16:26,341] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6271
[2019-04-03 22:16:26,389] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.633333333333333, 87.0, 0.0, 0.0, 26.0, 24.94290251263538, 0.264474894879541, 0.0, 1.0, 51720.93042635026], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 585600.0000, 
sim time next is 586200.0000, 
raw observation next is [-2.716666666666667, 87.0, 0.0, 0.0, 26.0, 24.92443697512989, 0.2646405982040636, 0.0, 1.0, 59047.6467075807], 
processed observation next is [0.0, 0.782608695652174, 0.3873499538319483, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5770364145941574, 0.5882135327346879, 0.0, 1.0, 0.2811792700360986], 
reward next is 0.7188, 
noisyNet noise sample is [array([0.9505388], dtype=float32), -0.12761484]. 
=============================================
[2019-04-03 22:16:30,862] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.16877624e-26 2.15309582e-13 3.17686033e-09 9.99991298e-01
 2.57502886e-15 8.67301969e-06 3.37739353e-16], sum to 1.0000
[2019-04-03 22:16:30,862] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2497
[2019-04-03 22:16:30,901] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.3, 96.0, 0.0, 0.0, 26.0, 24.81700295900406, 0.2224968480667339, 0.0, 1.0, 40371.80208759144], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 514800.0000, 
sim time next is 515400.0000, 
raw observation next is [3.383333333333333, 96.16666666666666, 0.0, 0.0, 26.0, 24.81828103885892, 0.222276513231635, 0.0, 1.0, 40231.70906538489], 
processed observation next is [1.0, 1.0, 0.5563250230840259, 0.9616666666666666, 0.0, 0.0, 0.6666666666666666, 0.5681900865715767, 0.5740921710772117, 0.0, 1.0, 0.1915795669780233], 
reward next is 0.8084, 
noisyNet noise sample is [array([-0.40705037], dtype=float32), -0.52178377]. 
=============================================
[2019-04-03 22:16:38,947] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.2799732e-25 1.8177203e-14 3.2719532e-12 9.9999893e-01 1.3584579e-14
 1.0859575e-06 5.0091411e-18], sum to 1.0000
[2019-04-03 22:16:38,947] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0695
[2019-04-03 22:16:38,967] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.5, 45.83333333333333, 86.0, 753.3333333333333, 26.0, 25.61056745034281, 0.3953140876461953, 1.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 741000.0000, 
sim time next is 741600.0000, 
raw observation next is [0.5, 45.0, 84.5, 743.5, 26.0, 25.65844049976323, 0.4028530785094862, 1.0, 1.0, 18680.53207983417], 
processed observation next is [1.0, 0.6086956521739131, 0.4764542936288089, 0.45, 0.2816666666666667, 0.8215469613259668, 0.6666666666666666, 0.6382033749802692, 0.6342843595031621, 1.0, 1.0, 0.088954914665877], 
reward next is 0.9110, 
noisyNet noise sample is [array([-1.2359022], dtype=float32), -0.3229327]. 
=============================================
[2019-04-03 22:16:40,802] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2809313e-18 1.8455982e-09 1.3637282e-06 9.9423343e-01 5.1544286e-11
 5.7651508e-03 3.6856157e-11], sum to 1.0000
[2019-04-03 22:16:40,802] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7484
[2019-04-03 22:16:40,828] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.733333333333333, 86.33333333333333, 0.0, 0.0, 26.0, 24.48517100765816, 0.1564467438511813, 0.0, 1.0, 42245.87461447751], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 607200.0000, 
sim time next is 607800.0000, 
raw observation next is [-3.816666666666666, 86.16666666666667, 0.0, 0.0, 26.0, 24.4570267670837, 0.1487692150708797, 0.0, 1.0, 42214.6955861192], 
processed observation next is [0.0, 0.0, 0.3568790397045245, 0.8616666666666667, 0.0, 0.0, 0.6666666666666666, 0.5380855639236417, 0.5495897383569599, 0.0, 1.0, 0.20102235993390094], 
reward next is 0.7990, 
noisyNet noise sample is [array([-0.7960378], dtype=float32), 0.035600465]. 
=============================================
[2019-04-03 22:16:44,797] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.2898448e-25 4.7104268e-14 2.6802272e-10 1.0000000e+00 6.4760952e-16
 7.4691089e-09 1.6315461e-16], sum to 1.0000
[2019-04-03 22:16:44,800] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4181
[2019-04-03 22:16:44,894] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.566666666666667, 60.33333333333334, 108.1666666666667, 89.66666666666667, 26.0, 24.87857647065, 0.2197897303132198, 0.0, 1.0, 38456.47899494076], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 649200.0000, 
sim time next is 649800.0000, 
raw observation next is [-2.5, 60.0, 112.0, 100.0, 26.0, 24.87843191401977, 0.2221090729233982, 0.0, 1.0, 40993.5420309503], 
processed observation next is [0.0, 0.5217391304347826, 0.39335180055401664, 0.6, 0.37333333333333335, 0.11049723756906077, 0.6666666666666666, 0.5732026595016476, 0.5740363576411327, 0.0, 1.0, 0.19520734300452525], 
reward next is 0.8048, 
noisyNet noise sample is [array([-1.6885734], dtype=float32), 0.43034628]. 
=============================================
[2019-04-03 22:16:49,736] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0287986e-22 6.3714762e-13 1.7022181e-10 9.9999869e-01 4.6823096e-15
 1.2620507e-06 2.5210551e-15], sum to 1.0000
[2019-04-03 22:16:49,736] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5997
[2019-04-03 22:16:49,762] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.566666666666666, 71.66666666666667, 0.0, 0.0, 26.0, 24.34695011375173, 0.08876294628961734, 0.0, 1.0, 40965.24069361052], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 693600.0000, 
sim time next is 694200.0000, 
raw observation next is [-3.483333333333333, 71.83333333333333, 0.0, 0.0, 26.0, 24.38514630873762, 0.08613502556598851, 0.0, 1.0, 40952.07469223085], 
processed observation next is [1.0, 0.0, 0.3661126500461681, 0.7183333333333333, 0.0, 0.0, 0.6666666666666666, 0.5320955257281351, 0.5287116751886628, 0.0, 1.0, 0.1950098794868136], 
reward next is 0.8050, 
noisyNet noise sample is [array([0.06884918], dtype=float32), 1.30004]. 
=============================================
[2019-04-03 22:16:51,622] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.9904814e-24 1.5597042e-13 2.0737743e-09 9.9999523e-01 2.3297429e-14
 4.8090992e-06 1.8949880e-15], sum to 1.0000
[2019-04-03 22:16:51,622] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4018
[2019-04-03 22:16:51,670] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.2, 75.0, 46.5, 0.0, 26.0, 25.75443385416356, 0.2973692307839985, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 810000.0000, 
sim time next is 810600.0000, 
raw observation next is [-6.2, 75.0, 51.33333333333334, 0.0, 26.0, 25.68545752777084, 0.2943336030284032, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.2908587257617729, 0.75, 0.17111111111111113, 0.0, 0.6666666666666666, 0.6404547939809033, 0.5981112010094677, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4974309], dtype=float32), -0.8151575]. 
=============================================
[2019-04-03 22:17:04,058] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.7996550e-28 2.3096492e-18 2.1260085e-14 1.0000000e+00 1.7969538e-17
 4.9546225e-11 2.2483009e-19], sum to 1.0000
[2019-04-03 22:17:04,059] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3166
[2019-04-03 22:17:04,131] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.199999999999999, 75.0, 65.33333333333333, 0.0, 26.0, 25.71326417793728, 0.3012067277155048, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 812400.0000, 
sim time next is 813000.0000, 
raw observation next is [-6.2, 75.0, 69.66666666666666, 0.0, 26.0, 25.72017678865073, 0.3000307164153063, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.2908587257617729, 0.75, 0.2322222222222222, 0.0, 0.6666666666666666, 0.6433480657208941, 0.6000102388051021, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7832957], dtype=float32), -0.039762788]. 
=============================================
[2019-04-03 22:17:04,137] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[87.12052 ]
 [86.92408 ]
 [86.67802 ]
 [86.480774]
 [86.406075]], R is [[87.36864471]
 [87.49495697]
 [87.62001038]
 [87.74381256]
 [87.86637878]].
[2019-04-03 22:17:04,646] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.1506474e-25 3.4404367e-13 6.6079381e-10 9.9999976e-01 1.7050393e-15
 2.3922956e-07 6.9621349e-16], sum to 1.0000
[2019-04-03 22:17:04,648] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4571
[2019-04-03 22:17:04,683] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.2, 79.83333333333334, 0.0, 0.0, 26.0, 24.74225773459167, 0.2112167281581119, 0.0, 1.0, 39517.026670036], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 868200.0000, 
sim time next is 868800.0000, 
raw observation next is [-2.1, 79.66666666666667, 0.0, 0.0, 26.0, 24.73596133760008, 0.2051175810114657, 0.0, 1.0, 39486.99546173165], 
processed observation next is [1.0, 0.043478260869565216, 0.404432132963989, 0.7966666666666667, 0.0, 0.0, 0.6666666666666666, 0.5613301114666734, 0.5683725270038219, 0.0, 1.0, 0.18803331172253165], 
reward next is 0.8120, 
noisyNet noise sample is [array([-1.2237152], dtype=float32), 0.76394653]. 
=============================================
[2019-04-03 22:17:08,717] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3405858e-24 8.5893629e-13 4.0996520e-10 9.9997461e-01 2.8991258e-14
 2.5347992e-05 2.2603652e-15], sum to 1.0000
[2019-04-03 22:17:08,743] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5161
[2019-04-03 22:17:08,792] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.816666666666666, 85.5, 0.0, 0.0, 26.0, 25.30559909798084, 0.3643785075882966, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 846600.0000, 
sim time next is 847200.0000, 
raw observation next is [-3.733333333333333, 85.0, 0.0, 0.0, 26.0, 25.43659273337963, 0.3549650302180143, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.35918744228993543, 0.85, 0.0, 0.0, 0.6666666666666666, 0.6197160611149691, 0.6183216767393381, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1606513], dtype=float32), -0.40842864]. 
=============================================
[2019-04-03 22:17:12,811] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.63850676e-31 2.58300378e-20 1.15467879e-14 1.00000000e+00
 1.29020869e-20 1.20353533e-10 1.08407585e-20], sum to 1.0000
[2019-04-03 22:17:12,872] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7782
[2019-04-03 22:17:12,888] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [10.68333333333333, 76.0, 0.0, 0.0, 26.0, 25.7274029831801, 0.6525049118480025, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1126200.0000, 
sim time next is 1126800.0000, 
raw observation next is [10.5, 77.0, 0.0, 0.0, 26.0, 25.67769361429676, 0.6479754679928201, 0.0, 1.0, 67970.44993549299], 
processed observation next is [0.0, 0.043478260869565216, 0.7534626038781165, 0.77, 0.0, 0.0, 0.6666666666666666, 0.6398078011913967, 0.7159918226642734, 0.0, 1.0, 0.32366880921663327], 
reward next is 0.6763, 
noisyNet noise sample is [array([2.1426666], dtype=float32), 0.41401905]. 
=============================================
[2019-04-03 22:17:17,632] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-03 22:17:17,637] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-03 22:17:17,637] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:17:17,637] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-03 22:17:17,637] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-03 22:17:17,638] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:17:17,638] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:17:17,642] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run4
[2019-04-03 22:17:17,675] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run4
[2019-04-03 22:17:17,694] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run4
[2019-04-03 22:17:52,380] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.54348725], dtype=float32), 0.41790733]
[2019-04-03 22:17:52,381] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-10.16666666666667, 53.33333333333333, 172.8333333333333, 379.0, 23.0, 22.22642249275395, -0.4510259707987962, 0.0, 1.0, 38227.44632065635]
[2019-04-03 22:17:52,381] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-03 22:17:52,382] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [5.1669586e-15 1.1159206e-08 2.1298765e-06 9.9900085e-01 6.5590076e-09
 9.9705008e-04 4.9926929e-10], sampled 0.61271754442895
[2019-04-03 22:18:18,848] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.54348725], dtype=float32), 0.41790733]
[2019-04-03 22:18:18,848] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [14.4, 76.33333333333334, 0.0, 0.0, 25.0, 25.04586668148599, 0.4167187513553286, 0.0, 1.0, 0.0]
[2019-04-03 22:18:18,848] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-03 22:18:18,849] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.9597152e-28 2.4198569e-16 2.1880028e-12 1.0000000e+00 2.6800168e-18
 2.4735742e-08 1.5823488e-18], sampled 0.11482232297714012
[2019-04-03 22:18:30,022] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.54348725], dtype=float32), 0.41790733]
[2019-04-03 22:18:30,022] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [3.366666666666667, 76.66666666666667, 0.0, 0.0, 24.0, 23.1932538557667, 0.01725746420882547, 0.0, 1.0, 23875.77563522308]
[2019-04-03 22:18:30,022] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-03 22:18:30,027] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.8155863e-21 2.9536751e-12 2.3022944e-09 9.9998748e-01 1.8961034e-13
 1.2536199e-05 2.0868668e-14], sampled 0.6683177126599659
[2019-04-03 22:19:54,242] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7424.8829 218147676.0016 677.4138
[2019-04-03 22:19:59,777] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.54348725], dtype=float32), 0.41790733]
[2019-04-03 22:19:59,777] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-5.018722428000001, 77.88229066666668, 0.0, 0.0, 26.0, 25.19804215822753, 0.3370629751400994, 0.0, 1.0, 40812.88606177432]
[2019-04-03 22:19:59,777] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-03 22:19:59,778] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.2618236e-25 1.4089550e-14 2.6387481e-11 1.0000000e+00 4.1880660e-16
 3.8664641e-08 7.7771084e-17], sampled 0.6337944894959395
[2019-04-03 22:20:06,886] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.54348725], dtype=float32), 0.41790733]
[2019-04-03 22:20:06,887] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-0.585523663, 21.65124907, 92.70500256, 733.91434355, 26.0, 27.05068343441062, 0.4538748360923122, 1.0, 1.0, 0.0]
[2019-04-03 22:20:06,887] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-03 22:20:06,888] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.87398132e-17 5.47338730e-10 1.15189394e-07 9.95192885e-01
 1.00535524e-09 4.80697490e-03 5.97654435e-12], sampled 0.5809684586131342
[2019-04-03 22:20:18,088] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7259.8962 251731141.4290 953.2711
[2019-04-03 22:20:26,363] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7160.8238 273251237.2589 1158.7003
[2019-04-03 22:20:27,384] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 300000, evaluation results [300000.0, 7259.896238201633, 251731141.42904967, 953.271083148114, 7424.882935288908, 218147676.0015941, 677.4137627110454, 7160.8238393580905, 273251237.2588752, 1158.7003355112215]
[2019-04-03 22:20:28,390] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.4435694e-30 7.0599038e-19 1.0268045e-14 1.0000000e+00 5.1584188e-20
 7.0970063e-11 7.3158584e-21], sum to 1.0000
[2019-04-03 22:20:28,393] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9371
[2019-04-03 22:20:28,415] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [10.91666666666667, 77.33333333333334, 0.0, 0.0, 26.0, 25.63991022405189, 0.6312112614816175, 0.0, 1.0, 26274.72273056874], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1133400.0000, 
sim time next is 1134000.0000, 
raw observation next is [11.1, 77.0, 0.0, 0.0, 26.0, 25.64434941561353, 0.6308576074107844, 0.0, 1.0, 22598.60353281325], 
processed observation next is [0.0, 0.13043478260869565, 0.7700831024930749, 0.77, 0.0, 0.0, 0.6666666666666666, 0.6370291179677942, 0.7102858691369281, 0.0, 1.0, 0.10761239777530118], 
reward next is 0.8924, 
noisyNet noise sample is [array([0.03446808], dtype=float32), -0.27545753]. 
=============================================
[2019-04-03 22:20:28,435] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[95.69215]
 [95.88763]
 [96.11046]
 [96.36439]
 [96.74011]], R is [[95.40644073]
 [95.32726288]
 [95.22982025]
 [95.12133026]
 [95.0210495 ]].
[2019-04-03 22:20:30,428] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.2445791e-32 3.3442641e-19 7.7373749e-15 1.0000000e+00 2.8549518e-19
 1.7786353e-10 4.6355068e-21], sum to 1.0000
[2019-04-03 22:20:30,441] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4957
[2019-04-03 22:20:30,480] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [9.5, 92.83333333333333, 18.0, 0.0, 26.0, 25.40220278208122, 0.4226676693418523, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 979800.0000, 
sim time next is 980400.0000, 
raw observation next is [9.600000000000001, 92.66666666666667, 22.5, 0.0, 26.0, 25.33119803865171, 0.4847949739308919, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.7285318559556788, 0.9266666666666667, 0.075, 0.0, 0.6666666666666666, 0.6109331698876425, 0.6615983246436307, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0631709], dtype=float32), -1.464973]. 
=============================================
[2019-04-03 22:20:30,648] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.2092222e-29 2.5227028e-15 5.2235393e-11 9.9999976e-01 5.5289917e-17
 2.9618118e-07 2.1073286e-17], sum to 1.0000
[2019-04-03 22:20:30,652] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0199
[2019-04-03 22:20:30,658] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [16.06666666666667, 72.33333333333334, 74.33333333333334, 0.0, 26.0, 25.71542981289831, 0.6167583295330389, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1156800.0000, 
sim time next is 1157400.0000, 
raw observation next is [16.35, 71.0, 83.0, 0.0, 26.0, 25.72409331151405, 0.596403231240188, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.9155124653739612, 0.71, 0.27666666666666667, 0.0, 0.6666666666666666, 0.6436744426261708, 0.6988010770800627, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.02947584], dtype=float32), -1.8765999]. 
=============================================
[2019-04-03 22:20:35,555] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.1194400e-30 5.2369423e-17 1.6968141e-13 1.0000000e+00 1.2473312e-19
 1.1200993e-10 1.3463683e-19], sum to 1.0000
[2019-04-03 22:20:35,555] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8526
[2019-04-03 22:20:35,571] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [14.0, 59.5, 0.0, 0.0, 26.0, 25.8394410856923, 0.6691128330902664, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1108200.0000, 
sim time next is 1108800.0000, 
raw observation next is [13.8, 60.0, 0.0, 0.0, 26.0, 25.78541751704458, 0.6558936188490198, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.844875346260388, 0.6, 0.0, 0.0, 0.6666666666666666, 0.6487847930870482, 0.7186312062830066, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.30600098], dtype=float32), 0.6322775]. 
=============================================
[2019-04-03 22:20:37,961] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.9414912e-30 1.0539434e-17 7.1673387e-14 1.0000000e+00 2.4749231e-20
 6.3294966e-11 8.0608447e-20], sum to 1.0000
[2019-04-03 22:20:37,967] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0574
[2019-04-03 22:20:37,980] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [11.6, 82.0, 0.0, 0.0, 26.0, 25.66460151724338, 0.6086740949482573, 0.0, 1.0, 38387.56249333372], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1144200.0000, 
sim time next is 1144800.0000, 
raw observation next is [11.6, 83.0, 0.0, 0.0, 26.0, 25.64111268022508, 0.6089412737128503, 0.0, 1.0, 42518.35617403167], 
processed observation next is [0.0, 0.2608695652173913, 0.7839335180055402, 0.83, 0.0, 0.0, 0.6666666666666666, 0.6367593900187568, 0.7029804245709501, 0.0, 1.0, 0.20246836273348415], 
reward next is 0.7975, 
noisyNet noise sample is [array([-1.0524641], dtype=float32), 1.0800871]. 
=============================================
[2019-04-03 22:20:42,423] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.2415902e-28 7.6779196e-16 2.5069374e-11 1.0000000e+00 2.7543832e-18
 1.8672604e-10 4.0669117e-18], sum to 1.0000
[2019-04-03 22:20:42,423] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5756
[2019-04-03 22:20:42,497] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 92.0, 0.0, 0.0, 26.0, 25.38417861753091, 0.5551270432705414, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1322400.0000, 
sim time next is 1323000.0000, 
raw observation next is [1.1, 92.0, 0.0, 0.0, 26.0, 25.62404610312917, 0.5844389785400343, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.49307479224376743, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6353371752607643, 0.6948129928466781, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.84835106], dtype=float32), -1.3891525]. 
=============================================
[2019-04-03 22:20:42,515] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[83.34056 ]
 [80.69607 ]
 [80.743484]
 [80.89929 ]
 [81.02605 ]], R is [[85.34168243]
 [85.48826599]
 [85.36221313]
 [85.373703  ]
 [85.4200058 ]].
[2019-04-03 22:20:46,590] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.3609217e-29 1.1147245e-17 1.4984809e-14 1.0000000e+00 2.8239196e-19
 3.4709497e-12 2.0058228e-19], sum to 1.0000
[2019-04-03 22:20:46,594] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5286
[2019-04-03 22:20:46,635] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.2, 94.0, 0.0, 0.0, 26.0, 25.38777654639577, 0.4532130883135812, 0.0, 1.0, 63617.75973189619], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1479600.0000, 
sim time next is 1480200.0000, 
raw observation next is [2.2, 94.33333333333334, 0.0, 0.0, 26.0, 25.32698521203524, 0.4494903281876955, 0.0, 1.0, 57514.96604075958], 
processed observation next is [1.0, 0.13043478260869565, 0.5235457063711911, 0.9433333333333335, 0.0, 0.0, 0.6666666666666666, 0.6105821010029366, 0.6498301093958986, 0.0, 1.0, 0.2738807906702837], 
reward next is 0.7261, 
noisyNet noise sample is [array([-0.5511986], dtype=float32), 0.66954476]. 
=============================================
[2019-04-03 22:20:47,733] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.2937180e-27 1.6669901e-17 9.9775860e-14 1.0000000e+00 4.8995909e-18
 1.0003345e-10 2.9662125e-19], sum to 1.0000
[2019-04-03 22:20:47,734] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1518
[2019-04-03 22:20:47,752] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.08333333333333333, 94.5, 94.0, 0.0, 26.0, 25.68167910411987, 0.4800532192026494, 1.0, 1.0, 18681.88250266992], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1426200.0000, 
sim time next is 1426800.0000, 
raw observation next is [0.1666666666666667, 94.0, 95.0, 0.0, 26.0, 25.70004459795155, 0.4718279441764028, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.4672206832871654, 0.94, 0.31666666666666665, 0.0, 0.6666666666666666, 0.6416703831626291, 0.6572759813921343, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.0455797], dtype=float32), 0.76717645]. 
=============================================
[2019-04-03 22:20:51,061] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.0134703e-31 8.0763919e-19 2.0167841e-14 1.0000000e+00 5.6661119e-18
 9.1463788e-11 1.2649491e-21], sum to 1.0000
[2019-04-03 22:20:51,062] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7963
[2019-04-03 22:20:51,069] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [10.5, 58.0, 44.5, 17.0, 26.0, 26.46707234463368, 0.7102408105034711, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1530000.0000, 
sim time next is 1530600.0000, 
raw observation next is [10.41666666666667, 58.33333333333334, 30.33333333333333, 13.33333333333333, 26.0, 26.75724291778535, 0.7293998778251948, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.7511542012927056, 0.5833333333333335, 0.1011111111111111, 0.0147329650092081, 0.6666666666666666, 0.7297702431487793, 0.7431332926083982, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.20428471], dtype=float32), 0.10775268]. 
=============================================
[2019-04-03 22:20:57,870] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.9990975e-32 3.2929026e-21 6.7217001e-17 1.0000000e+00 4.3603740e-21
 6.9280465e-14 3.1251097e-22], sum to 1.0000
[2019-04-03 22:20:57,878] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3801
[2019-04-03 22:20:57,906] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.083333333333334, 81.5, 13.0, 15.0, 26.0, 25.47043129174267, 0.4726860147856939, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1583400.0000, 
sim time next is 1584000.0000, 
raw observation next is [5.0, 82.0, 19.0, 20.0, 26.0, 25.40772047358766, 0.458382580079626, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.6011080332409973, 0.82, 0.06333333333333334, 0.022099447513812154, 0.6666666666666666, 0.6173100394656382, 0.6527941933598753, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8574866], dtype=float32), -0.25961852]. 
=============================================
[2019-04-03 22:20:57,914] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[91.450836]
 [90.974525]
 [89.861885]
 [88.07895 ]
 [85.240585]], R is [[91.54641724]
 [91.63095093]
 [91.71464539]
 [91.79750061]
 [91.87952423]].
[2019-04-03 22:20:58,538] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.9300183e-27 7.6158413e-17 2.2792705e-13 1.0000000e+00 2.5099533e-17
 2.8320990e-10 1.6299271e-18], sum to 1.0000
[2019-04-03 22:20:58,541] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7249
[2019-04-03 22:20:58,561] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [9.116666666666667, 62.16666666666666, 205.3333333333333, 141.6666666666667, 26.0, 26.83539158987032, 0.7473501309004229, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1594200.0000, 
sim time next is 1594800.0000, 
raw observation next is [9.4, 61.0, 208.0, 168.5, 26.0, 26.82755223753324, 0.7600501140400552, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.7229916897506927, 0.61, 0.6933333333333334, 0.1861878453038674, 0.6666666666666666, 0.73562935312777, 0.7533500380133518, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.11342], dtype=float32), -0.0152848065]. 
=============================================
[2019-04-03 22:20:59,349] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.4717524e-26 3.1336667e-16 1.5818128e-13 1.0000000e+00 2.7420558e-17
 2.6841560e-10 1.1988748e-18], sum to 1.0000
[2019-04-03 22:20:59,349] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6938
[2019-04-03 22:20:59,377] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 87.33333333333334, 104.6666666666667, 0.0, 26.0, 25.41226560175919, 0.4574972546341039, 1.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1687800.0000, 
sim time next is 1688400.0000, 
raw observation next is [1.1, 88.0, 103.5, 0.0, 26.0, 25.38365776171205, 0.4591118778428754, 1.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.5652173913043478, 0.49307479224376743, 0.88, 0.345, 0.0, 0.6666666666666666, 0.6153048134760043, 0.6530372926142918, 1.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.4967275], dtype=float32), 0.47590578]. 
=============================================
[2019-04-03 22:21:02,424] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.1504861e-22 6.8264323e-13 6.7995887e-12 1.0000000e+00 2.4693289e-14
 1.7557774e-08 3.0174615e-15], sum to 1.0000
[2019-04-03 22:21:02,424] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1546
[2019-04-03 22:21:02,469] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.3, 87.0, 53.33333333333334, 0.0, 26.0, 25.02576579451155, 0.332379098737996, 0.0, 1.0, 44761.61356151287], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1785000.0000, 
sim time next is 1785600.0000, 
raw observation next is [-3.4, 87.0, 47.0, 0.0, 26.0, 24.99753993508586, 0.3295337962093232, 0.0, 1.0, 61522.18678487505], 
processed observation next is [0.0, 0.6956521739130435, 0.368421052631579, 0.87, 0.15666666666666668, 0.0, 0.6666666666666666, 0.5831283279238217, 0.609844598736441, 0.0, 1.0, 0.2929627942136907], 
reward next is 0.7070, 
noisyNet noise sample is [array([-1.9567055], dtype=float32), -0.6184525]. 
=============================================
[2019-04-03 22:21:02,796] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.65731456e-24 1.00047745e-14 3.53897966e-10 1.00000000e+00
 5.55451469e-15 1.66596141e-08 2.00276745e-16], sum to 1.0000
[2019-04-03 22:21:02,796] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9114
[2019-04-03 22:21:02,807] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 92.0, 74.5, 0.0, 26.0, 25.72573492507746, 0.5037387735408272, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1681200.0000, 
sim time next is 1681800.0000, 
raw observation next is [1.1, 90.66666666666667, 77.33333333333334, 0.0, 26.0, 25.73444344026926, 0.4980592411199457, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.49307479224376743, 0.9066666666666667, 0.25777777777777783, 0.0, 0.6666666666666666, 0.6445369533557717, 0.6660197470399819, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.20470361], dtype=float32), 0.93366563]. 
=============================================
[2019-04-03 22:21:11,824] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.1654102e-23 3.7289985e-14 7.5866979e-10 1.0000000e+00 9.0733830e-15
 1.1922263e-09 3.9530858e-15], sum to 1.0000
[2019-04-03 22:21:11,824] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7430
[2019-04-03 22:21:11,839] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.2, 79.0, 0.0, 0.0, 26.0, 23.63951670093263, -0.02195029784625067, 0.0, 1.0, 47079.56348664278], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1835400.0000, 
sim time next is 1836000.0000, 
raw observation next is [-6.2, 79.0, 0.0, 0.0, 26.0, 23.60217305157551, -0.02958515412175255, 0.0, 1.0, 47076.67131546295], 
processed observation next is [0.0, 0.2608695652173913, 0.2908587257617729, 0.79, 0.0, 0.0, 0.6666666666666666, 0.46684775429795905, 0.49013828195941583, 0.0, 1.0, 0.22417462531172833], 
reward next is 0.7758, 
noisyNet noise sample is [array([0.04495563], dtype=float32), -1.0966325]. 
=============================================
[2019-04-03 22:21:11,908] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[68.11649]
 [68.18373]
 [68.24748]
 [68.31494]
 [68.39797]], R is [[68.14784241]
 [68.24217224]
 [68.33558655]
 [68.4280014 ]
 [68.51939392]].
[2019-04-03 22:21:27,293] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.8796005e-22 2.2517067e-13 8.0726828e-11 1.0000000e+00 2.4549681e-14
 7.1471664e-09 1.0041782e-15], sum to 1.0000
[2019-04-03 22:21:27,293] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7890
[2019-04-03 22:21:27,325] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.2, 75.0, 0.0, 0.0, 26.0, 24.64587652648017, 0.159077284919021, 0.0, 1.0, 44815.24171330706], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1893600.0000, 
sim time next is 1894200.0000, 
raw observation next is [-6.383333333333334, 75.66666666666667, 0.0, 0.0, 26.0, 24.60780213357963, 0.1509926407095598, 0.0, 1.0, 44842.53067229597], 
processed observation next is [0.0, 0.9565217391304348, 0.28578024007386893, 0.7566666666666667, 0.0, 0.0, 0.6666666666666666, 0.5506501777983024, 0.55033088023652, 0.0, 1.0, 0.21353586034426653], 
reward next is 0.7865, 
noisyNet noise sample is [array([0.24620545], dtype=float32), 0.09420632]. 
=============================================
[2019-04-03 22:21:56,175] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.3295354e-27 6.6265836e-17 1.0448770e-12 1.0000000e+00 1.4581515e-17
 1.8892990e-11 1.0552141e-18], sum to 1.0000
[2019-04-03 22:21:56,178] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8356
[2019-04-03 22:21:56,206] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.3, 61.0, 0.0, 0.0, 26.0, 25.02454089909659, 0.3142103091947624, 0.0, 1.0, 38590.85011670386], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2330400.0000, 
sim time next is 2331000.0000, 
raw observation next is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.96794114898626, 0.3067616322479531, 0.0, 1.0, 38543.06911340538], 
processed observation next is [1.0, 1.0, 0.3988919667590028, 0.62, 0.0, 0.0, 0.6666666666666666, 0.5806617624155216, 0.6022538774159844, 0.0, 1.0, 0.18353842434954942], 
reward next is 0.8165, 
noisyNet noise sample is [array([1.3051946], dtype=float32), 0.5427359]. 
=============================================
[2019-04-03 22:21:56,248] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[82.561035]
 [82.55725 ]
 [82.240616]
 [82.377144]
 [82.501526]], R is [[82.82115173]
 [82.80917358]
 [82.79708862]
 [82.78516388]
 [82.7730484 ]].
[2019-04-03 22:21:57,306] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.2216242e-26 1.8392034e-16 2.7871624e-12 1.0000000e+00 6.4147473e-17
 6.0411194e-12 1.8071069e-17], sum to 1.0000
[2019-04-03 22:21:57,306] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1658
[2019-04-03 22:21:57,322] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.9, 91.0, 0.0, 0.0, 26.0, 23.58402140984873, -0.03032781081132568, 0.0, 1.0, 43230.6737921388], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2267400.0000, 
sim time next is 2268000.0000, 
raw observation next is [-8.9, 91.0, 0.0, 0.0, 26.0, 23.59982724784562, -0.03527179495094892, 0.0, 1.0, 43205.47408206232], 
processed observation next is [1.0, 0.2608695652173913, 0.21606648199445982, 0.91, 0.0, 0.0, 0.6666666666666666, 0.4666522706538017, 0.48824273501635035, 0.0, 1.0, 0.20574035277172534], 
reward next is 0.7943, 
noisyNet noise sample is [array([0.72634935], dtype=float32), 2.04019]. 
=============================================
[2019-04-03 22:21:57,329] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[76.862335]
 [76.92553 ]
 [76.98861 ]
 [77.053474]
 [77.12313 ]], R is [[76.82196045]
 [76.8478775 ]
 [76.87348175]
 [76.89868927]
 [76.92340851]].
[2019-04-03 22:21:57,841] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.4468468e-24 3.7142198e-14 1.5942206e-11 1.0000000e+00 4.7295975e-16
 1.7581214e-10 5.1003935e-16], sum to 1.0000
[2019-04-03 22:21:57,842] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9173
[2019-04-03 22:21:57,875] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 69.0, 7.833333333333332, 0.0, 26.0, 23.97068898303435, 0.05384602451171985, 0.0, 1.0, 41698.10390135674], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2360400.0000, 
sim time next is 2361000.0000, 
raw observation next is [-3.4, 69.0, 13.66666666666666, 0.0, 26.0, 23.93886963541086, 0.04869084017571013, 0.0, 1.0, 41788.31698511787], 
processed observation next is [0.0, 0.30434782608695654, 0.368421052631579, 0.69, 0.04555555555555554, 0.0, 0.6666666666666666, 0.494905802950905, 0.5162302800585701, 0.0, 1.0, 0.1989919856434184], 
reward next is 0.8010, 
noisyNet noise sample is [array([-1.1953832], dtype=float32), 0.48229215]. 
=============================================
[2019-04-03 22:21:57,888] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[69.828636]
 [69.73816 ]
 [69.77195 ]
 [69.79737 ]
 [69.82664 ]], R is [[70.03886414]
 [70.13991547]
 [70.24037933]
 [70.34026337]
 [70.43952942]].
[2019-04-03 22:21:58,130] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.9429519e-29 2.5307877e-17 5.3981725e-14 1.0000000e+00 1.0131567e-19
 2.2894704e-13 1.2569367e-19], sum to 1.0000
[2019-04-03 22:21:58,130] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2658
[2019-04-03 22:21:58,205] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-9.5, 91.0, 24.0, 18.0, 26.0, 25.05128418999058, 0.2442015924792174, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2275200.0000, 
sim time next is 2275800.0000, 
raw observation next is [-9.316666666666666, 90.33333333333334, 31.0, 17.33333333333333, 26.0, 25.13682705899164, 0.2465606549482225, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.20452446906740537, 0.9033333333333334, 0.10333333333333333, 0.01915285451197053, 0.6666666666666666, 0.5947355882493032, 0.5821868849827408, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.77536905], dtype=float32), -1.5651101]. 
=============================================
[2019-04-03 22:22:00,449] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.8087964e-24 3.5754048e-15 2.9955548e-12 1.0000000e+00 7.1337125e-16
 9.0906593e-10 1.7001288e-16], sum to 1.0000
[2019-04-03 22:22:00,449] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1368
[2019-04-03 22:22:00,489] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.9, 42.5, 0.0, 0.0, 26.0, 24.96447788899392, 0.2773627122773961, 0.0, 1.0, 86666.31466195334], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2403000.0000, 
sim time next is 2403600.0000, 
raw observation next is [-3.066666666666666, 42.33333333333334, 0.0, 0.0, 26.0, 25.02037178341049, 0.2837608017957658, 0.0, 1.0, 58112.46398412666], 
processed observation next is [0.0, 0.8260869565217391, 0.3776546629732226, 0.42333333333333345, 0.0, 0.0, 0.6666666666666666, 0.5850309819508741, 0.594586933931922, 0.0, 1.0, 0.2767260189720317], 
reward next is 0.7233, 
noisyNet noise sample is [array([1.0304434], dtype=float32), -0.43263054]. 
=============================================
[2019-04-03 22:22:06,115] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.7325923e-24 2.5187910e-14 1.9907720e-11 1.0000000e+00 1.1069751e-15
 2.0920314e-09 3.8659470e-16], sum to 1.0000
[2019-04-03 22:22:06,124] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0869
[2019-04-03 22:22:06,139] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 42.0, 0.0, 0.0, 26.0, 25.10634198163038, 0.2785274025836397, 0.0, 1.0, 43114.55555594756], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2408400.0000, 
sim time next is 2409000.0000, 
raw observation next is [-3.583333333333333, 42.33333333333334, 0.0, 0.0, 26.0, 25.15338423156319, 0.2738766417485896, 0.0, 1.0, 43056.80064673977], 
processed observation next is [0.0, 0.9130434782608695, 0.36334256694367506, 0.42333333333333345, 0.0, 0.0, 0.6666666666666666, 0.5961153526302659, 0.5912922139161966, 0.0, 1.0, 0.20503238403209414], 
reward next is 0.7950, 
noisyNet noise sample is [array([-1.6776102], dtype=float32), 1.002671]. 
=============================================
[2019-04-03 22:22:06,154] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[75.967224]
 [76.194   ]
 [76.332214]
 [76.4483  ]
 [76.59618 ]], R is [[75.81467438]
 [75.85121918]
 [75.88720703]
 [75.92282104]
 [75.95829773]].
[2019-04-03 22:22:15,427] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.6345135e-25 3.9175247e-14 3.6457690e-12 1.0000000e+00 1.9236490e-16
 1.3916510e-10 5.9443608e-17], sum to 1.0000
[2019-04-03 22:22:15,428] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0626
[2019-04-03 22:22:15,441] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.2, 35.66666666666667, 0.0, 0.0, 26.0, 25.28140541650853, 0.2746904531245379, 0.0, 1.0, 40010.71337667303], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2496000.0000, 
sim time next is 2496600.0000, 
raw observation next is [-1.2, 35.0, 0.0, 0.0, 26.0, 25.26141562246207, 0.2773260448066114, 0.0, 1.0, 40052.73218226815], 
processed observation next is [0.0, 0.9130434782608695, 0.42936288088642666, 0.35, 0.0, 0.0, 0.6666666666666666, 0.6051179685385059, 0.592442014935537, 0.0, 1.0, 0.1907272961060388], 
reward next is 0.8093, 
noisyNet noise sample is [array([-2.582822], dtype=float32), 0.016694287]. 
=============================================
[2019-04-03 22:22:20,104] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.6662877e-28 7.0343952e-18 5.0473081e-14 1.0000000e+00 4.1271130e-18
 2.4455361e-11 2.6438559e-19], sum to 1.0000
[2019-04-03 22:22:20,104] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5613
[2019-04-03 22:22:20,128] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 62.0, 0.0, 0.0, 26.0, 24.80632016257758, 0.2561584659181442, 0.0, 1.0, 41898.11738892963], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2592000.0000, 
sim time next is 2592600.0000, 
raw observation next is [-4.583333333333333, 63.0, 0.0, 0.0, 26.0, 24.76994392610926, 0.2475436148764931, 0.0, 1.0, 41906.38832614882], 
processed observation next is [1.0, 0.0, 0.3356417359187443, 0.63, 0.0, 0.0, 0.6666666666666666, 0.5641619938424384, 0.5825145382921644, 0.0, 1.0, 0.1995542301245182], 
reward next is 0.8004, 
noisyNet noise sample is [array([-0.692053], dtype=float32), 0.3164852]. 
=============================================
[2019-04-03 22:22:30,103] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.7176366e-24 1.6802006e-14 9.5865837e-10 9.9999940e-01 1.5833987e-14
 5.7285206e-07 2.2834138e-15], sum to 1.0000
[2019-04-03 22:22:30,104] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4578
[2019-04-03 22:22:30,113] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.833333333333333, 28.33333333333334, 26.99999999999999, 56.0, 26.0, 25.75964523307044, 0.3971659804958405, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2826600.0000, 
sim time next is 2827200.0000, 
raw observation next is [5.666666666666666, 28.66666666666667, 16.0, 51.0, 26.0, 25.82046626200415, 0.3767225540135963, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.6195752539242845, 0.28666666666666674, 0.05333333333333334, 0.056353591160221, 0.6666666666666666, 0.6517055218336791, 0.6255741846711987, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.43434], dtype=float32), -0.48729134]. 
=============================================
[2019-04-03 22:22:31,047] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.6075720e-23 4.6030676e-13 6.0067940e-10 1.0000000e+00 3.2541037e-14
 3.8002503e-09 4.2287480e-15], sum to 1.0000
[2019-04-03 22:22:31,048] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4052
[2019-04-03 22:22:31,068] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.5, 61.5, 0.0, 0.0, 26.0, 24.47655265074362, 0.149315234138898, 0.0, 1.0, 40796.05793726338], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2781000.0000, 
sim time next is 2781600.0000, 
raw observation next is [-6.666666666666666, 62.33333333333333, 0.0, 0.0, 26.0, 24.48117584916071, 0.1409724573615548, 0.0, 1.0, 40847.37888138447], 
processed observation next is [1.0, 0.17391304347826086, 0.2779316712834719, 0.6233333333333333, 0.0, 0.0, 0.6666666666666666, 0.5400979874300592, 0.5469908191205183, 0.0, 1.0, 0.19451132800659274], 
reward next is 0.8055, 
noisyNet noise sample is [array([0.7599484], dtype=float32), -1.0661882]. 
=============================================
[2019-04-03 22:22:39,039] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.8092054e-24 1.5994965e-14 6.8941533e-11 1.0000000e+00 3.1773614e-16
 1.2198622e-08 1.8053991e-16], sum to 1.0000
[2019-04-03 22:22:39,041] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4245
[2019-04-03 22:22:39,091] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 72.0, 0.0, 0.0, 26.0, 24.93948616950559, 0.3063349771050075, 0.0, 1.0, 55413.06276508685], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2852400.0000, 
sim time next is 2853000.0000, 
raw observation next is [1.0, 72.0, 0.0, 0.0, 26.0, 24.96307443809051, 0.3107566846702643, 0.0, 1.0, 54863.71436822393], 
processed observation next is [1.0, 0.0, 0.4903047091412743, 0.72, 0.0, 0.0, 0.6666666666666666, 0.580256203174209, 0.6035855615567548, 0.0, 1.0, 0.2612557827058283], 
reward next is 0.7387, 
noisyNet noise sample is [array([-1.4005916], dtype=float32), -0.4736322]. 
=============================================
[2019-04-03 22:22:39,129] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[76.30037 ]
 [75.878105]
 [75.35632 ]
 [74.561455]
 [74.563705]], R is [[76.68943024]
 [76.65866852]
 [76.63072968]
 [76.61586761]
 [76.62800598]].
[2019-04-03 22:23:04,928] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.2086226e-24 2.5029080e-16 6.5101136e-13 1.0000000e+00 1.5073788e-14
 3.7802974e-09 1.5616255e-17], sum to 1.0000
[2019-04-03 22:23:04,928] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4040
[2019-04-03 22:23:04,955] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 73.33333333333334, 114.3333333333333, 819.0, 26.0, 26.66893931695246, 0.7655215966764467, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3240600.0000, 
sim time next is 3241200.0000, 
raw observation next is [-2.0, 75.66666666666667, 114.6666666666667, 821.0, 26.0, 26.6957184173755, 0.7607620507813907, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.40720221606648205, 0.7566666666666667, 0.38222222222222235, 0.907182320441989, 0.6666666666666666, 0.7246432014479582, 0.7535873502604636, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.59046274], dtype=float32), 0.23597004]. 
=============================================
[2019-04-03 22:23:11,179] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4593379e-23 9.4266276e-14 2.7053873e-11 1.0000000e+00 2.7530455e-13
 1.8401302e-08 3.6732693e-16], sum to 1.0000
[2019-04-03 22:23:11,179] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9208
[2019-04-03 22:23:11,200] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.666666666666667, 53.33333333333333, 9.166666666666668, 110.8333333333333, 26.0, 25.51386480720796, 0.4922352817145849, 1.0, 1.0, 123347.7630408472], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3346800.0000, 
sim time next is 3347400.0000, 
raw observation next is [-2.833333333333333, 54.16666666666667, 0.0, 0.0, 26.0, 25.42584275866513, 0.5135991016301592, 1.0, 1.0, 74180.45497006456], 
processed observation next is [1.0, 0.7391304347826086, 0.3841181902123731, 0.5416666666666667, 0.0, 0.0, 0.6666666666666666, 0.6188202298887608, 0.6711997005433864, 1.0, 1.0, 0.3532402617622122], 
reward next is 0.6468, 
noisyNet noise sample is [array([1.0882928], dtype=float32), 1.5337119]. 
=============================================
[2019-04-03 22:23:14,731] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.2577139e-24 3.5173711e-14 1.3066875e-09 1.0000000e+00 6.0778167e-15
 1.7337001e-09 4.3682265e-16], sum to 1.0000
[2019-04-03 22:23:14,731] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9680
[2019-04-03 22:23:14,778] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.0, 77.0, 0.0, 0.0, 26.0, 25.05023902424551, 0.4001330880597533, 0.0, 1.0, 43632.3611321646], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3285000.0000, 
sim time next is 3285600.0000, 
raw observation next is [-7.0, 74.66666666666667, 0.0, 0.0, 26.0, 25.0485995889534, 0.4026679784151452, 0.0, 1.0, 43670.74878321346], 
processed observation next is [1.0, 0.0, 0.2686980609418283, 0.7466666666666667, 0.0, 0.0, 0.6666666666666666, 0.5873832990794501, 0.6342226594717151, 0.0, 1.0, 0.20795594658673075], 
reward next is 0.7920, 
noisyNet noise sample is [array([-1.5459788], dtype=float32), -1.5793445]. 
=============================================
[2019-04-03 22:23:15,252] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4160151e-23 5.0796715e-14 1.3845269e-09 1.0000000e+00 3.5246356e-15
 2.6868108e-09 9.7392536e-16], sum to 1.0000
[2019-04-03 22:23:15,253] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6770
[2019-04-03 22:23:15,292] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.60621044045154, 0.5234755889091764, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3360000.0000, 
sim time next is 3360600.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.5647155376764, 0.5089443941607962, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6303929614730333, 0.669648131386932, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.152766], dtype=float32), -0.08104691]. 
=============================================
[2019-04-03 22:23:18,477] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.1884543e-30 3.0833377e-19 4.3456395e-16 1.0000000e+00 1.6889038e-20
 3.4136034e-14 1.3695583e-21], sum to 1.0000
[2019-04-03 22:23:18,477] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2079
[2019-04-03 22:23:18,504] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.333333333333334, 67.0, 0.0, 0.0, 26.0, 25.44244056177675, 0.4496200503034353, 0.0, 1.0, 34380.183109109], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3363600.0000, 
sim time next is 3364200.0000, 
raw observation next is [-4.5, 68.0, 0.0, 0.0, 26.0, 25.40864610147992, 0.4361427112349051, 0.0, 1.0, 53575.49267001326], 
processed observation next is [1.0, 0.9565217391304348, 0.3379501385041552, 0.68, 0.0, 0.0, 0.6666666666666666, 0.6173871751233267, 0.6453809037449684, 0.0, 1.0, 0.2551213936667298], 
reward next is 0.7449, 
noisyNet noise sample is [array([0.60840386], dtype=float32), -1.4664947]. 
=============================================
[2019-04-03 22:23:20,432] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3723638e-25 2.8124614e-14 1.6120993e-11 1.0000000e+00 2.6647077e-16
 1.8455282e-09 2.6981234e-17], sum to 1.0000
[2019-04-03 22:23:20,433] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3914
[2019-04-03 22:23:20,484] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.666666666666667, 61.66666666666666, 0.0, 0.0, 26.0, 25.17119637731125, 0.5090473396755306, 0.0, 1.0, 87138.34461717264], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3357600.0000, 
sim time next is 3358200.0000, 
raw observation next is [-3.833333333333333, 63.33333333333334, 0.0, 0.0, 26.0, 25.33053906300839, 0.5262858806604943, 0.0, 1.0, 56898.24793763848], 
processed observation next is [1.0, 0.8695652173913043, 0.3564173591874424, 0.6333333333333334, 0.0, 0.0, 0.6666666666666666, 0.6108782552506993, 0.6754286268868315, 0.0, 1.0, 0.27094403779827847], 
reward next is 0.7291, 
noisyNet noise sample is [array([-1.6661594], dtype=float32), -2.5129507]. 
=============================================
[2019-04-03 22:23:21,044] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.3835646e-29 4.5442598e-18 2.0552979e-15 1.0000000e+00 7.7218276e-17
 5.8660549e-11 4.5045251e-20], sum to 1.0000
[2019-04-03 22:23:21,045] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1470
[2019-04-03 22:23:21,064] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 46.33333333333334, 116.6666666666667, 814.8333333333334, 26.0, 25.91291518814006, 0.5296354332092695, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3414000.0000, 
sim time next is 3414600.0000, 
raw observation next is [3.0, 47.0, 117.0, 817.0, 26.0, 25.10513408543662, 0.46239303777929, 1.0, 1.0, 90562.32981011267], 
processed observation next is [1.0, 0.5217391304347826, 0.5457063711911359, 0.47, 0.39, 0.9027624309392265, 0.6666666666666666, 0.5920945071197184, 0.6541310125930967, 1.0, 1.0, 0.4312491895719651], 
reward next is 0.5688, 
noisyNet noise sample is [array([1.4997256], dtype=float32), -0.28297594]. 
=============================================
[2019-04-03 22:23:27,967] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.5440905e-26 2.4693346e-17 1.8312558e-14 1.0000000e+00 8.8874350e-16
 8.2295941e-11 5.3211117e-19], sum to 1.0000
[2019-04-03 22:23:27,968] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2161
[2019-04-03 22:23:27,999] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 53.5, 103.0, 775.0, 26.0, 25.74927616152893, 0.570733956393311, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3421800.0000, 
sim time next is 3422400.0000, 
raw observation next is [3.0, 55.0, 99.83333333333334, 763.1666666666667, 26.0, 26.01085954151717, 0.5940060838659225, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.55, 0.3327777777777778, 0.8432780847145489, 0.6666666666666666, 0.6675716284597643, 0.6980020279553075, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.37818804], dtype=float32), 0.079408035]. 
=============================================
[2019-04-03 22:23:28,973] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.2500130e-30 3.2695477e-20 3.3691377e-15 1.0000000e+00 1.4213270e-19
 1.3559749e-13 2.6122142e-21], sum to 1.0000
[2019-04-03 22:23:28,975] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4890
[2019-04-03 22:23:29,012] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.5, 65.5, 99.0, 670.0, 26.0, 25.72908050099876, 0.4989751044678837, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3490200.0000, 
sim time next is 3490800.0000, 
raw observation next is [-0.3333333333333334, 63.66666666666667, 100.6666666666667, 686.6666666666667, 26.0, 25.8537690686422, 0.5189891438583886, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4533702677747, 0.6366666666666667, 0.33555555555555566, 0.7587476979742174, 0.6666666666666666, 0.6544807557201834, 0.6729963812861296, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1385005], dtype=float32), 0.03193012]. 
=============================================
[2019-04-03 22:23:32,459] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.6375691e-24 6.5225540e-17 1.3436435e-12 1.0000000e+00 1.3488521e-14
 3.0060951e-10 1.5931116e-18], sum to 1.0000
[2019-04-03 22:23:32,459] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1123
[2019-04-03 22:23:32,479] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 49.0, 108.5, 793.5, 26.0, 26.31150183787612, 0.6666827170926121, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3506400.0000, 
sim time next is 3507000.0000, 
raw observation next is [3.0, 49.0, 106.3333333333333, 789.3333333333334, 26.0, 26.47723743744925, 0.6855678367280172, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.49, 0.35444444444444434, 0.8721915285451197, 0.6666666666666666, 0.7064364531207709, 0.7285226122426725, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5897498], dtype=float32), -1.1537302]. 
=============================================
[2019-04-03 22:23:32,491] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[78.7315  ]
 [78.80624 ]
 [78.952225]
 [79.02507 ]
 [79.148926]], R is [[78.83018494]
 [79.04188538]
 [79.25146484]
 [79.37000275]
 [79.57630157]].
[2019-04-03 22:23:34,674] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.8413357e-26 1.2545005e-16 1.4413067e-13 1.0000000e+00 5.3135714e-17
 7.0822126e-11 1.5996450e-19], sum to 1.0000
[2019-04-03 22:23:34,677] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3886
[2019-04-03 22:23:34,719] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 53.5, 103.0, 775.0, 26.0, 25.7493404132532, 0.570739505297469, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3421800.0000, 
sim time next is 3422400.0000, 
raw observation next is [3.0, 55.0, 99.83333333333334, 763.1666666666667, 26.0, 26.01091902988952, 0.5939010912568217, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.55, 0.3327777777777778, 0.8432780847145489, 0.6666666666666666, 0.6675765858241268, 0.6979670304189405, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7759206], dtype=float32), -0.9418285]. 
=============================================
[2019-04-03 22:23:35,092] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.1618350e-32 1.1513144e-21 5.5923107e-17 1.0000000e+00 8.5000463e-23
 6.1954154e-17 1.6211434e-22], sum to 1.0000
[2019-04-03 22:23:35,092] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4679
[2019-04-03 22:23:35,165] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 71.0, 73.83333333333334, 350.3333333333333, 26.0, 25.2370960995254, 0.3929386226952383, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3486000.0000, 
sim time next is 3486600.0000, 
raw observation next is [-1.0, 71.0, 88.0, 399.0, 26.0, 25.3076095543194, 0.4171800040036407, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.4349030470914128, 0.71, 0.29333333333333333, 0.4408839779005525, 0.6666666666666666, 0.6089674628599498, 0.6390600013345469, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.736072], dtype=float32), -1.3199667]. 
=============================================
[2019-04-03 22:23:35,935] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.02953257e-25 1.54453427e-16 1.62828125e-13 1.00000000e+00
 1.48673669e-15 1.78608274e-11 3.83786327e-18], sum to 1.0000
[2019-04-03 22:23:35,937] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0682
[2019-04-03 22:23:35,985] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.5, 54.5, 114.0, 816.0, 26.0, 25.18603551615465, 0.4333428904984992, 0.0, 1.0, 34002.25222819425], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3583800.0000, 
sim time next is 3584400.0000, 
raw observation next is [-3.333333333333333, 54.66666666666667, 114.6666666666667, 817.1666666666666, 26.0, 25.14818615435576, 0.4418685641870687, 0.0, 1.0, 45346.34529588869], 
processed observation next is [0.0, 0.4782608695652174, 0.37026777469990774, 0.5466666666666667, 0.38222222222222235, 0.9029465930018415, 0.6666666666666666, 0.5956821795296466, 0.6472895213956896, 0.0, 1.0, 0.21593497759946997], 
reward next is 0.7841, 
noisyNet noise sample is [array([0.07106902], dtype=float32), -0.58130133]. 
=============================================
[2019-04-03 22:23:36,422] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.3089309e-27 4.0081235e-18 6.3208607e-14 1.0000000e+00 1.8696789e-15
 6.6280197e-11 7.0986019e-19], sum to 1.0000
[2019-04-03 22:23:36,433] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4976
[2019-04-03 22:23:36,511] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 52.0, 115.5, 814.5, 26.0, 26.19184308558067, 0.6377387187600917, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3502800.0000, 
sim time next is 3503400.0000, 
raw observation next is [2.166666666666667, 51.5, 115.3333333333333, 811.6666666666666, 26.0, 26.24514487756601, 0.6549033506676769, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5226223453370269, 0.515, 0.3844444444444443, 0.8968692449355432, 0.6666666666666666, 0.687095406463834, 0.7183011168892257, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.44169578], dtype=float32), -0.027731245]. 
=============================================
[2019-04-03 22:23:46,837] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.0558971e-31 1.1388360e-19 6.8319352e-15 1.0000000e+00 3.4486144e-22
 1.5207865e-15 5.3763771e-22], sum to 1.0000
[2019-04-03 22:23:46,837] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7704
[2019-04-03 22:23:46,885] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.666666666666667, 60.33333333333334, 0.0, 0.0, 26.0, 24.94223773968323, 0.325713435541265, 0.0, 1.0, 197695.6176672984], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3698400.0000, 
sim time next is 3699000.0000, 
raw observation next is [3.5, 61.0, 0.0, 0.0, 26.0, 24.94296760785531, 0.3535212930814133, 0.0, 1.0, 199161.4493692914], 
processed observation next is [0.0, 0.8260869565217391, 0.5595567867036012, 0.61, 0.0, 0.0, 0.6666666666666666, 0.5785806339879423, 0.6178404310271378, 0.0, 1.0, 0.9483878541394828], 
reward next is 0.0516, 
noisyNet noise sample is [array([-1.3597137], dtype=float32), 0.22846404]. 
=============================================
[2019-04-03 22:23:46,967] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[87.537544]
 [85.387344]
 [84.90781 ]
 [84.513725]
 [84.07433 ]], R is [[88.49073792]
 [87.66442871]
 [87.62088776]
 [87.61937714]
 [87.62026978]].
[2019-04-03 22:24:17,719] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.1916962e-30 2.6653137e-19 5.0600457e-16 1.0000000e+00 2.2199967e-20
 3.9539451e-15 3.8511296e-21], sum to 1.0000
[2019-04-03 22:24:17,719] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2217
[2019-04-03 22:24:17,779] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.0, 58.0, 48.5, 314.5, 26.0, 25.7076781674931, 0.4398746924156852, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3916800.0000, 
sim time next is 3917400.0000, 
raw observation next is [-8.0, 57.16666666666667, 62.66666666666667, 365.0, 26.0, 25.70754652071324, 0.4375751035120896, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.24099722991689754, 0.5716666666666668, 0.2088888888888889, 0.40331491712707185, 0.6666666666666666, 0.6422955433927701, 0.6458583678373632, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.58896446], dtype=float32), -1.8561255]. 
=============================================
[2019-04-03 22:24:19,236] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1689564e-25 3.2577487e-18 1.0128121e-13 1.0000000e+00 8.2768301e-16
 5.0876564e-12 5.5074947e-19], sum to 1.0000
[2019-04-03 22:24:19,236] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7823
[2019-04-03 22:24:19,304] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.333333333333334, 38.0, 102.1666666666667, 777.3333333333334, 26.0, 26.9806856553853, 0.7798254943983539, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3940800.0000, 
sim time next is 3941400.0000, 
raw observation next is [-4.166666666666667, 38.0, 99.33333333333334, 766.6666666666666, 26.0, 27.0237053493827, 0.6664172585015516, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3471837488457987, 0.38, 0.33111111111111113, 0.8471454880294659, 0.6666666666666666, 0.7519754457818916, 0.7221390861671839, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.08902755], dtype=float32), 0.30045754]. 
=============================================
[2019-04-03 22:24:36,077] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.51710877e-25 3.11653745e-15 1.28245175e-11 1.00000000e+00
 2.06731189e-16 9.08271201e-12 4.48902481e-17], sum to 1.0000
[2019-04-03 22:24:36,082] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9750
[2019-04-03 22:24:36,167] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 37.0, 0.0, 0.0, 26.0, 25.12638579136174, 0.3561274278764796, 0.0, 1.0, 40694.27736043111], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4060800.0000, 
sim time next is 4061400.0000, 
raw observation next is [-6.0, 37.66666666666667, 0.0, 0.0, 26.0, 25.10205897275176, 0.348879054926468, 0.0, 1.0, 40675.99590506363], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.3766666666666667, 0.0, 0.0, 0.6666666666666666, 0.5918382477293133, 0.6162930183088227, 0.0, 1.0, 0.1936952185955411], 
reward next is 0.8063, 
noisyNet noise sample is [array([0.21300977], dtype=float32), -1.1800363]. 
=============================================
[2019-04-03 22:24:38,237] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.1650201e-28 2.8070804e-18 7.8071383e-15 1.0000000e+00 6.5178715e-19
 4.6001099e-13 6.3728626e-19], sum to 1.0000
[2019-04-03 22:24:38,237] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8051
[2019-04-03 22:24:38,260] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 52.66666666666667, 0.0, 0.0, 26.0, 24.87979331111254, 0.2795376068223611, 0.0, 1.0, 39449.83630951471], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4166400.0000, 
sim time next is 4167000.0000, 
raw observation next is [-4.0, 52.0, 0.0, 0.0, 26.0, 24.84597849581558, 0.2707737679421314, 0.0, 1.0, 39472.92782680141], 
processed observation next is [0.0, 0.21739130434782608, 0.3518005540166205, 0.52, 0.0, 0.0, 0.6666666666666666, 0.5704982079846316, 0.5902579226473771, 0.0, 1.0, 0.18796632298476862], 
reward next is 0.8120, 
noisyNet noise sample is [array([0.64124894], dtype=float32), -0.8312708]. 
=============================================
[2019-04-03 22:24:38,269] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[79.57078]
 [79.67304]
 [79.77791]
 [79.8799 ]
 [79.95898]], R is [[79.47662354]
 [79.49399567]
 [79.51122284]
 [79.52825165]
 [79.5450592 ]].
[2019-04-03 22:24:55,614] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.2192205e-25 3.8163480e-16 2.7728353e-12 1.0000000e+00 5.9133140e-15
 9.4219146e-11 6.4760182e-17], sum to 1.0000
[2019-04-03 22:24:55,615] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5568
[2019-04-03 22:24:55,678] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 39.0, 205.0, 475.6666666666667, 26.0, 25.1542343301244, 0.3957627008014177, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4197000.0000, 
sim time next is 4197600.0000, 
raw observation next is [2.0, 40.0, 200.5, 379.0, 26.0, 25.14204096190622, 0.3956149336930013, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.518005540166205, 0.4, 0.6683333333333333, 0.41878453038674035, 0.6666666666666666, 0.5951700801588515, 0.6318716445643338, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.22535658], dtype=float32), -0.7091451]. 
=============================================
[2019-04-03 22:25:05,402] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.1354992e-26 7.5650803e-14 2.7837264e-11 1.0000000e+00 3.8879792e-18
 9.9636230e-11 1.3779201e-16], sum to 1.0000
[2019-04-03 22:25:05,402] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9514
[2019-04-03 22:25:05,415] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 26.0, 25.38175734819262, 0.3409059507161259, 0.0, 1.0, 51986.05808991961], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4244400.0000, 
sim time next is 4245000.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 26.0, 25.39925348783603, 0.3431008303098439, 0.0, 1.0, 36476.18453081213], 
processed observation next is [0.0, 0.13043478260869565, 0.5457063711911359, 0.45, 0.0, 0.0, 0.6666666666666666, 0.6166044573196693, 0.6143669434366147, 0.0, 1.0, 0.1736961168133911], 
reward next is 0.8263, 
noisyNet noise sample is [array([0.54420274], dtype=float32), 1.0166105]. 
=============================================
[2019-04-03 22:25:05,537] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[83.01037 ]
 [82.9     ]
 [82.916   ]
 [82.92794 ]
 [82.953186]], R is [[83.02872467]
 [82.95088959]
 [82.93514252]
 [82.92488861]
 [82.91999817]].
[2019-04-03 22:25:22,080] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.60930048e-30 6.81117835e-20 1.33580755e-14 1.00000000e+00
 1.25222204e-20 3.18535007e-14 7.81518950e-21], sum to 1.0000
[2019-04-03 22:25:22,080] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2841
[2019-04-03 22:25:22,155] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.8666666666666667, 72.33333333333333, 18.5, 11.0, 26.0, 25.62215852705079, 0.4289362006725356, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4520400.0000, 
sim time next is 4521000.0000, 
raw observation next is [-0.8333333333333334, 72.66666666666667, 36.99999999999999, 22.0, 26.0, 25.53268853766414, 0.4240989308236863, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.43951985226223456, 0.7266666666666667, 0.12333333333333331, 0.02430939226519337, 0.6666666666666666, 0.6277240448053449, 0.6413663102745621, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1199983], dtype=float32), 0.33493567]. 
=============================================
[2019-04-03 22:25:22,182] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[86.80967 ]
 [84.782394]
 [81.99932 ]
 [78.447495]
 [78.47149 ]], R is [[88.90126801]
 [89.01225281]
 [89.12213135]
 [89.23091125]
 [89.14604187]].
[2019-04-03 22:25:43,708] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.3394997e-23 1.5225497e-13 4.7977351e-11 1.0000000e+00 2.8408823e-12
 1.8587438e-09 6.0043704e-16], sum to 1.0000
[2019-04-03 22:25:43,708] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4540
[2019-04-03 22:25:43,741] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 36.5, 87.0, 608.3333333333334, 26.0, 25.1983843241401, 0.4340979641895544, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4810200.0000, 
sim time next is 4810800.0000, 
raw observation next is [3.0, 36.0, 84.5, 578.6666666666666, 26.0, 25.19917244079073, 0.4331050301870354, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.5457063711911359, 0.36, 0.2816666666666667, 0.6394106813996316, 0.6666666666666666, 0.5999310367325608, 0.6443683433956785, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.16969447], dtype=float32), -0.55672264]. 
=============================================
[2019-04-03 22:25:51,327] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-03 22:25:51,328] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-03 22:25:51,328] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:25:51,330] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run5
[2019-04-03 22:25:51,343] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-03 22:25:51,345] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:25:51,348] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run5
[2019-04-03 22:25:51,377] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-03 22:25:51,377] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:25:51,379] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run5
[2019-04-03 22:26:45,316] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.39035508], dtype=float32), 0.44681135]
[2019-04-03 22:26:45,316] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [2.533333333333333, 80.0, 0.0, 0.0, 20.0, 19.67284561818493, -0.8923305062064376, 0.0, 1.0, 91285.99712942846]
[2019-04-03 22:26:45,316] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-03 22:26:45,317] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [4.7193699e-15 6.6082682e-09 1.0024543e-06 9.9999762e-01 1.6933854e-10
 1.4837173e-06 5.0913190e-10], sampled 0.7409389515350201
[2019-04-03 22:27:46,181] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5795.4745 154867945.9689 -1751.8056
[2019-04-03 22:27:55,305] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5310.2295 184149682.1556 -2309.9646
[2019-04-03 22:28:14,058] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 6845.3892 222958959.3179 -1140.6564
[2019-04-03 22:28:15,083] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 400000, evaluation results [400000.0, 5310.229506067209, 184149682.15557247, -2309.9646318309824, 5795.474499695362, 154867945.96888202, -1751.80563718481, 6845.389207628345, 222958959.31789902, -1140.656370484203]
[2019-04-03 22:28:16,886] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.53123131e-28 1.32110943e-17 2.18349373e-14 1.00000000e+00
 3.30744627e-19 1.27516717e-13 1.05783734e-19], sum to 1.0000
[2019-04-03 22:28:16,887] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8864
[2019-04-03 22:28:16,901] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.5, 53.0, 0.0, 0.0, 26.0, 25.40866496456461, 0.4061598700275006, 0.0, 1.0, 46261.04657581358], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4829400.0000, 
sim time next is 4830000.0000, 
raw observation next is [-0.6666666666666666, 53.66666666666667, 0.0, 0.0, 26.0, 25.40624482902701, 0.4036904433851708, 0.0, 1.0, 41882.57046507694], 
processed observation next is [0.0, 0.9130434782608695, 0.44413665743305636, 0.5366666666666667, 0.0, 0.0, 0.6666666666666666, 0.6171870690855842, 0.6345634811283903, 0.0, 1.0, 0.19944081173846162], 
reward next is 0.8006, 
noisyNet noise sample is [array([0.44694757], dtype=float32), 0.060925603]. 
=============================================
[2019-04-03 22:28:16,913] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[82.76278 ]
 [82.82402 ]
 [82.7512  ]
 [82.656555]
 [82.681595]], R is [[82.55651093]
 [82.51065826]
 [82.39040375]
 [82.2689743 ]
 [82.20160675]].
[2019-04-03 22:28:20,361] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.2180284e-24 1.9006069e-16 1.9771973e-12 1.0000000e+00 9.9178707e-15
 1.3133915e-09 4.6740836e-17], sum to 1.0000
[2019-04-03 22:28:20,361] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5255
[2019-04-03 22:28:20,375] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.166666666666667, 44.16666666666667, 248.0, 378.6666666666667, 26.0, 25.09597704164484, 0.3659527310015996, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4889400.0000, 
sim time next is 4890000.0000, 
raw observation next is [2.333333333333333, 44.33333333333334, 242.0, 376.3333333333334, 26.0, 25.08327575056166, 0.3655929162925274, 0.0, 1.0, 18689.71169411487], 
processed observation next is [0.0, 0.6086956521739131, 0.5272391505078486, 0.4433333333333334, 0.8066666666666666, 0.4158379373848988, 0.6666666666666666, 0.5902729792134718, 0.6218643054308425, 0.0, 1.0, 0.08899862711483272], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.09190313], dtype=float32), -0.7221807]. 
=============================================
[2019-04-03 22:28:20,407] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[83.776535]
 [83.94701 ]
 [84.1472  ]
 [84.317825]
 [84.48388 ]], R is [[83.80570221]
 [83.96764374]
 [84.12796783]
 [84.28668976]
 [84.44382477]].
[2019-04-03 22:28:22,029] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.0778273e-28 1.1895815e-18 2.9032498e-14 1.0000000e+00 9.0554874e-19
 2.0624934e-14 5.6627357e-20], sum to 1.0000
[2019-04-03 22:28:22,032] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1096
[2019-04-03 22:28:22,043] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 50.0, 0.0, 0.0, 26.0, 25.38557119612973, 0.3938228770780937, 0.0, 1.0, 45958.91665859851], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5029200.0000, 
sim time next is 5029800.0000, 
raw observation next is [-1.0, 49.33333333333334, 0.0, 0.0, 26.0, 25.44044932560495, 0.4044385959465776, 0.0, 1.0, 18762.28679629367], 
processed observation next is [1.0, 0.21739130434782608, 0.4349030470914128, 0.4933333333333334, 0.0, 0.0, 0.6666666666666666, 0.6200374438004124, 0.6348128653155259, 0.0, 1.0, 0.08934422283949366], 
reward next is 0.9107, 
noisyNet noise sample is [array([0.6546336], dtype=float32), -0.28268373]. 
=============================================
[2019-04-03 22:28:26,247] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.9677655e-25 1.9841388e-14 9.9872680e-12 1.0000000e+00 2.0214652e-16
 1.5049229e-10 9.3302697e-17], sum to 1.0000
[2019-04-03 22:28:26,249] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5603
[2019-04-03 22:28:26,256] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.0, 26.0, 0.0, 0.0, 26.0, 25.75267808635432, 0.527574423667359, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4998600.0000, 
sim time next is 4999200.0000, 
raw observation next is [4.666666666666666, 27.0, 0.0, 0.0, 26.0, 25.6889120945305, 0.527327680027165, 0.0, 1.0, 196217.9094192961], 
processed observation next is [1.0, 0.8695652173913043, 0.5918744228993538, 0.27, 0.0, 0.0, 0.6666666666666666, 0.6407426745442084, 0.6757758933423883, 0.0, 1.0, 0.9343709972347434], 
reward next is 0.0656, 
noisyNet noise sample is [array([0.01901794], dtype=float32), -0.7610117]. 
=============================================
[2019-04-03 22:28:26,700] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:28:26,700] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:28:26,705] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run4
[2019-04-03 22:28:26,897] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:28:26,897] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:28:26,900] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run4
[2019-04-03 22:28:27,002] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:28:27,003] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:28:27,009] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run4
[2019-04-03 22:28:30,538] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:28:30,538] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:28:30,540] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run4
[2019-04-03 22:28:30,589] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:28:30,590] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:28:30,591] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run4
[2019-04-03 22:28:31,325] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:28:31,325] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:28:31,327] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run4
[2019-04-03 22:28:31,582] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:28:31,583] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:28:31,584] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run4
[2019-04-03 22:28:32,848] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:28:32,848] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:28:32,850] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run4
[2019-04-03 22:28:33,253] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:28:33,253] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:28:33,255] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run4
[2019-04-03 22:28:33,368] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:28:33,369] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:28:33,374] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run4
[2019-04-03 22:28:33,985] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:28:33,985] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:28:33,987] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run4
[2019-04-03 22:28:34,037] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:28:34,037] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:28:34,068] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run4
[2019-04-03 22:28:34,801] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:28:34,801] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:28:34,803] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run4
[2019-04-03 22:28:36,325] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:28:36,333] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:28:36,342] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run4
[2019-04-03 22:28:37,012] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.04035208e-21 3.26978407e-13 1.05900245e-11 1.00000000e+00
 1.98970011e-16 3.29919356e-11 5.21991465e-15], sum to 1.0000
[2019-04-03 22:28:37,013] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0023
[2019-04-03 22:28:37,049] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.8, 86.0, 0.0, 0.0, 19.0, 18.12393092355549, -1.214291001420683, 0.0, 1.0, 22972.33916739317], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 68400.0000, 
sim time next is 69000.0000, 
raw observation next is [3.616666666666667, 86.5, 0.0, 0.0, 19.0, 18.12486053898423, -1.212126460703748, 0.0, 1.0, 25808.24657558489], 
processed observation next is [0.0, 0.8260869565217391, 0.5627885503231764, 0.865, 0.0, 0.0, 0.08333333333333333, 0.010405044915352471, 0.09595784643208398, 0.0, 1.0, 0.12289641226468995], 
reward next is 0.8771, 
noisyNet noise sample is [array([-0.89855987], dtype=float32), -0.4316919]. 
=============================================
[2019-04-03 22:28:37,052] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[57.4596  ]
 [57.457443]
 [57.4747  ]
 [57.504147]
 [57.545547]], R is [[57.78136826]
 [58.0941658 ]
 [58.41316223]
 [58.73855591]
 [59.06221771]].
[2019-04-03 22:28:38,321] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:28:38,327] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:28:38,329] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run4
[2019-04-03 22:28:41,493] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:28:41,493] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:28:41,495] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run4
[2019-04-03 22:28:47,608] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.8581859e-22 3.5040859e-14 3.2379848e-12 1.0000000e+00 3.3643437e-17
 7.0291164e-09 3.5324408e-16], sum to 1.0000
[2019-04-03 22:28:47,608] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6010
[2019-04-03 22:28:47,652] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.8666666666666667, 92.33333333333334, 0.0, 0.0, 19.0, 18.98676887906039, -1.074153081835211, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 78000.0000, 
sim time next is 78600.0000, 
raw observation next is [0.6833333333333333, 94.16666666666666, 0.0, 0.0, 19.0, 18.91939404249011, -1.084178795726999, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.9130434782608695, 0.4815327793167129, 0.9416666666666665, 0.0, 0.0, 0.08333333333333333, 0.07661617020750928, 0.13860706809100032, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.43265143], dtype=float32), -1.1522077]. 
=============================================
[2019-04-03 22:28:50,578] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.1070419e-27 1.5926142e-17 3.6180629e-14 1.0000000e+00 9.2943765e-20
 2.1605968e-10 1.6256239e-18], sum to 1.0000
[2019-04-03 22:28:50,613] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5817
[2019-04-03 22:28:50,639] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [8.200000000000001, 86.0, 90.0, 0.0, 19.0, 18.18461116060688, -1.200961154891405, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 47400.0000, 
sim time next is 48000.0000, 
raw observation next is [8.100000000000001, 86.0, 88.5, 0.0, 19.0, 18.16280310554908, -1.205606516617211, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.6869806094182827, 0.86, 0.295, 0.0, 0.08333333333333333, 0.013566925462423404, 0.09813116112759634, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.70013225], dtype=float32), 0.39867616]. 
=============================================
[2019-04-03 22:28:50,698] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[84.31885 ]
 [84.60121 ]
 [84.89179 ]
 [85.181885]
 [85.41047 ]], R is [[84.21690369]
 [84.37473297]
 [84.53098297]
 [84.68567657]
 [84.83882141]].
[2019-04-03 22:28:55,276] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.7778259e-21 5.0921141e-14 3.5344577e-10 9.9986780e-01 7.3266046e-14
 1.3214764e-04 3.8571157e-15], sum to 1.0000
[2019-04-03 22:28:55,276] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6422
[2019-04-03 22:28:55,310] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 62.5, 30.66666666666666, 0.0, 23.0, 23.10618207209236, -0.321537456251576, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 231000.0000, 
sim time next is 231600.0000, 
raw observation next is [-3.4, 63.0, 24.33333333333333, 0.0, 23.0, 23.08960388061696, -0.3331904963862678, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.368421052631579, 0.63, 0.08111111111111109, 0.0, 0.4166666666666667, 0.42413365671807995, 0.3889365012045774, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([3.6684456], dtype=float32), 0.5401706]. 
=============================================
[2019-04-03 22:28:56,090] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.2388456e-26 3.3558946e-15 2.7254392e-13 9.9999452e-01 2.4667426e-16
 5.4874581e-06 2.9036157e-18], sum to 1.0000
[2019-04-03 22:28:56,090] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5058
[2019-04-03 22:28:56,216] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.2, 72.0, 138.5, 0.0, 26.0, 25.11406145158627, 0.1529614264404865, 1.0, 1.0, 30827.39434494522], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 212400.0000, 
sim time next is 213000.0000, 
raw observation next is [-6.0, 70.83333333333333, 143.3333333333333, 0.0, 26.0, 25.15356411518865, 0.1577944808357121, 1.0, 1.0, 18739.45656092354], 
processed observation next is [1.0, 0.4782608695652174, 0.296398891966759, 0.7083333333333333, 0.47777777777777763, 0.0, 0.6666666666666666, 0.5961303429323875, 0.5525981602785707, 1.0, 1.0, 0.08923550743296924], 
reward next is 0.9108, 
noisyNet noise sample is [array([-1.8935812], dtype=float32), 0.2681245]. 
=============================================
[2019-04-03 22:28:56,225] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[95.23894 ]
 [95.038475]
 [94.64849 ]
 [94.30491 ]
 [94.0902  ]], R is [[95.44830322]
 [95.34702301]
 [95.10689545]
 [94.82894897]
 [94.74761963]].
[2019-04-03 22:29:12,189] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.1721981e-17 2.7444713e-10 9.5705090e-08 2.7168435e-01 3.4482459e-10
 7.2831559e-01 1.2396324e-11], sum to 1.0000
[2019-04-03 22:29:12,189] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9313
[2019-04-03 22:29:12,203] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.6, 49.0, 0.0, 0.0, 26.0, 22.83771486320975, -0.2682948777040744, 0.0, 1.0, 46224.64971022945], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 442800.0000, 
sim time next is 443400.0000, 
raw observation next is [-10.7, 49.5, 0.0, 0.0, 26.0, 22.7869602785884, -0.2672976221706915, 0.0, 1.0, 46249.60993094477], 
processed observation next is [1.0, 0.13043478260869565, 0.1662049861495845, 0.495, 0.0, 0.0, 0.6666666666666666, 0.39891335654903326, 0.41090079260976947, 0.0, 1.0, 0.22023623776640366], 
reward next is 0.7798, 
noisyNet noise sample is [array([-2.0855246], dtype=float32), 0.59472585]. 
=============================================
[2019-04-03 22:29:26,614] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.7195389e-36 4.1396671e-24 8.0751289e-19 1.0000000e+00 4.6135498e-26
 4.4567135e-11 5.4900861e-26], sum to 1.0000
[2019-04-03 22:29:26,614] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4300
[2019-04-03 22:29:26,633] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.066666666666667, 83.33333333333334, 0.0, 0.0, 26.0, 24.56207025186659, 0.1739911786926399, 0.0, 1.0, 40137.23372205723], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 531600.0000, 
sim time next is 532200.0000, 
raw observation next is [2.883333333333334, 82.66666666666667, 0.0, 0.0, 26.0, 24.5407346215195, 0.1704966594157321, 0.0, 1.0, 40193.09349028937], 
processed observation next is [0.0, 0.13043478260869565, 0.5424746075715605, 0.8266666666666667, 0.0, 0.0, 0.6666666666666666, 0.5450612184599585, 0.5568322198052441, 0.0, 1.0, 0.19139568328709225], 
reward next is 0.8086, 
noisyNet noise sample is [array([-0.5147678], dtype=float32), -0.19665802]. 
=============================================
[2019-04-03 22:29:29,090] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.2873297e-32 3.9036669e-20 4.0250701e-17 9.9999952e-01 2.8113170e-22
 4.9542143e-07 4.2233928e-23], sum to 1.0000
[2019-04-03 22:29:29,090] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7290
[2019-04-03 22:29:29,103] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.7, 92.0, 0.0, 0.0, 26.0, 24.83659968624704, 0.2285418245747937, 0.0, 1.0, 41104.8447968999], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 511200.0000, 
sim time next is 511800.0000, 
raw observation next is [2.8, 92.66666666666667, 0.0, 0.0, 26.0, 24.83073820618, 0.2277855912903854, 0.0, 1.0, 41010.26811306912], 
processed observation next is [1.0, 0.9565217391304348, 0.5401662049861496, 0.9266666666666667, 0.0, 0.0, 0.6666666666666666, 0.5692281838483334, 0.5759285304301285, 0.0, 1.0, 0.19528699101461486], 
reward next is 0.8047, 
noisyNet noise sample is [array([1.3282341], dtype=float32), 0.7544789]. 
=============================================
[2019-04-03 22:29:31,588] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.2919588e-26 4.0166481e-17 1.1012220e-13 9.9999535e-01 1.7666450e-17
 4.6013729e-06 6.4851564e-18], sum to 1.0000
[2019-04-03 22:29:31,589] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1984
[2019-04-03 22:29:31,639] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.2, 80.0, 134.0, 495.5, 26.0, 24.93980681265154, 0.3151259953827499, 0.0, 1.0, 36788.83979588761], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 565200.0000, 
sim time next is 565800.0000, 
raw observation next is [-1.2, 80.0, 135.3333333333333, 528.6666666666666, 26.0, 24.92342467017881, 0.319988354335182, 0.0, 1.0, 45226.86652978959], 
processed observation next is [0.0, 0.5652173913043478, 0.42936288088642666, 0.8, 0.45111111111111096, 0.5841620626151013, 0.6666666666666666, 0.5769520558482343, 0.606662784778394, 0.0, 1.0, 0.21536603109423616], 
reward next is 0.7846, 
noisyNet noise sample is [array([0.92881113], dtype=float32), -0.38821134]. 
=============================================
[2019-04-03 22:29:32,707] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2754764e-23 1.5701841e-14 1.5896342e-12 9.9989653e-01 5.0675216e-15
 1.0342016e-04 9.0886463e-17], sum to 1.0000
[2019-04-03 22:29:32,708] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8712
[2019-04-03 22:29:32,790] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.2, 83.0, 90.16666666666667, 68.33333333333333, 26.0, 24.98477379240468, 0.3143910640620838, 0.0, 1.0, 22625.88456096275], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 574800.0000, 
sim time next is 575400.0000, 
raw observation next is [-1.2, 83.0, 80.33333333333334, 63.66666666666667, 26.0, 24.98236118159492, 0.3073885763134878, 0.0, 1.0, 28107.59455801492], 
processed observation next is [0.0, 0.6521739130434783, 0.42936288088642666, 0.83, 0.26777777777777784, 0.0703499079189687, 0.6666666666666666, 0.5818634317995768, 0.6024628587711626, 0.0, 1.0, 0.13384568837149963], 
reward next is 0.8662, 
noisyNet noise sample is [array([-0.4132936], dtype=float32), -0.6279057]. 
=============================================
[2019-04-03 22:29:34,815] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.5023830e-19 2.7966052e-10 2.2696605e-07 8.6950582e-01 5.2584820e-12
 1.3049400e-01 8.7640371e-13], sum to 1.0000
[2019-04-03 22:29:34,815] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9439
[2019-04-03 22:29:34,844] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 66.33333333333334, 0.0, 0.0, 26.0, 24.98887693532407, 0.2103445832099669, 0.0, 1.0, 43229.9591361296], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 678000.0000, 
sim time next is 678600.0000, 
raw observation next is [-3.1, 67.0, 0.0, 0.0, 26.0, 24.98746140330588, 0.2075246625001305, 0.0, 1.0, 42713.18789571621], 
processed observation next is [0.0, 0.8695652173913043, 0.37673130193905824, 0.67, 0.0, 0.0, 0.6666666666666666, 0.5822884502754899, 0.5691748875000435, 0.0, 1.0, 0.20339613283674388], 
reward next is 0.7966, 
noisyNet noise sample is [array([0.45396233], dtype=float32), 0.39045265]. 
=============================================
[2019-04-03 22:29:38,787] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.0011276e-23 8.7333197e-15 2.7272778e-12 9.9995124e-01 1.8578145e-15
 4.8773152e-05 3.7765252e-16], sum to 1.0000
[2019-04-03 22:29:38,788] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0603
[2019-04-03 22:29:38,829] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.7, 55.0, 81.66666666666667, 50.0, 26.0, 24.86794872226801, 0.2196914535584718, 0.0, 1.0, 40612.78649519024], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 658200.0000, 
sim time next is 658800.0000, 
raw observation next is [-0.6, 54.0, 82.0, 47.0, 26.0, 24.87168584245802, 0.2227802971812309, 0.0, 1.0, 35540.29504358783], 
processed observation next is [0.0, 0.6521739130434783, 0.44598337950138506, 0.54, 0.2733333333333333, 0.051933701657458566, 0.6666666666666666, 0.5726404868715017, 0.5742600990604103, 0.0, 1.0, 0.1692395002075611], 
reward next is 0.8308, 
noisyNet noise sample is [array([-0.92863446], dtype=float32), 0.086311646]. 
=============================================
[2019-04-03 22:29:48,682] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.2308920e-25 1.2661782e-16 4.1057405e-14 9.9999964e-01 1.2348712e-17
 3.7618074e-07 4.2734629e-19], sum to 1.0000
[2019-04-03 22:29:48,682] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7126
[2019-04-03 22:29:48,710] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.45, 65.5, 0.0, 0.0, 26.0, 24.62340605518425, 0.2198831239251062, 0.0, 1.0, 43080.74319212529], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 772200.0000, 
sim time next is 772800.0000, 
raw observation next is [-6.533333333333333, 66.0, 0.0, 0.0, 26.0, 24.59054814711147, 0.2125927464113868, 0.0, 1.0, 42944.27926626257], 
processed observation next is [1.0, 0.9565217391304348, 0.2816251154201293, 0.66, 0.0, 0.0, 0.6666666666666666, 0.5492123455926224, 0.5708642488037956, 0.0, 1.0, 0.20449656793458368], 
reward next is 0.7955, 
noisyNet noise sample is [array([0.9936708], dtype=float32), 1.3309615]. 
=============================================
[2019-04-03 22:29:51,907] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.8851359e-28 6.8833773e-19 1.3532606e-15 1.0000000e+00 1.6627087e-18
 3.2176117e-09 5.0881766e-21], sum to 1.0000
[2019-04-03 22:29:51,907] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4089
[2019-04-03 22:29:51,955] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.783333333333333, 71.66666666666667, 94.66666666666666, 0.0, 26.0, 25.57946267480467, 0.2953929992877982, 1.0, 1.0, 18715.5676860229], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 816600.0000, 
sim time next is 817200.0000, 
raw observation next is [-4.5, 71.0, 98.5, 0.0, 26.0, 25.60057484447367, 0.2971204353587585, 1.0, 1.0, 18714.20428912349], 
processed observation next is [1.0, 0.4782608695652174, 0.3379501385041552, 0.71, 0.3283333333333333, 0.0, 0.6666666666666666, 0.6333812370394725, 0.5990401451195861, 1.0, 1.0, 0.08911525851963566], 
reward next is 0.9109, 
noisyNet noise sample is [array([-0.66096115], dtype=float32), 0.81682074]. 
=============================================
[2019-04-03 22:29:54,337] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.4486646e-26 5.1817906e-17 3.6278863e-13 9.9999869e-01 1.3207291e-18
 1.2982401e-06 4.1181257e-19], sum to 1.0000
[2019-04-03 22:29:54,338] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1926
[2019-04-03 22:29:54,386] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.1, 79.0, 85.66666666666667, 0.0, 26.0, 25.98883048636159, 0.439404624537714, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 826800.0000, 
sim time next is 827400.0000, 
raw observation next is [-4.0, 79.0, 80.33333333333333, 0.0, 26.0, 26.17778740169233, 0.4578085843004602, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.3518005540166205, 0.79, 0.2677777777777778, 0.0, 0.6666666666666666, 0.6814822834743609, 0.6526028614334868, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.33041674], dtype=float32), -1.3001467]. 
=============================================
[2019-04-03 22:30:04,682] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.0869892e-32 4.3833888e-21 6.8826231e-17 1.0000000e+00 1.7566292e-22
 1.1740117e-12 9.3507287e-23], sum to 1.0000
[2019-04-03 22:30:04,682] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1623
[2019-04-03 22:30:04,703] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.7, 81.0, 0.0, 0.0, 26.0, 25.39292103864041, 0.4455296939743209, 0.0, 1.0, 32733.64881514197], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 962400.0000, 
sim time next is 963000.0000, 
raw observation next is [7.7, 81.5, 0.0, 0.0, 26.0, 25.47874902162784, 0.4433266401313378, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.6759002770083103, 0.815, 0.0, 0.0, 0.6666666666666666, 0.6232290851356534, 0.6477755467104459, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8634207], dtype=float32), 0.7237519]. 
=============================================
[2019-04-03 22:30:04,710] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[92.24605 ]
 [92.15973 ]
 [92.027565]
 [91.83147 ]
 [91.67821 ]], R is [[92.25544739]
 [92.17701721]
 [92.05458069]
 [91.83678436]
 [91.62509155]].
[2019-04-03 22:30:08,743] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.5553408e-32 2.5321539e-21 9.8079938e-18 1.0000000e+00 3.1088675e-22
 1.0229064e-11 9.2668191e-23], sum to 1.0000
[2019-04-03 22:30:08,748] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5249
[2019-04-03 22:30:08,765] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [8.8, 83.0, 0.0, 0.0, 26.0, 25.57103484556265, 0.4777880969151747, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 972000.0000, 
sim time next is 972600.0000, 
raw observation next is [9.0, 83.0, 0.0, 0.0, 26.0, 25.69084169001898, 0.4776686597490849, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.7119113573407203, 0.83, 0.0, 0.0, 0.6666666666666666, 0.6409034741682484, 0.6592228865830283, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7866774], dtype=float32), -0.39465535]. 
=============================================
[2019-04-03 22:30:10,633] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.5368044e-36 1.1290227e-24 4.0876248e-20 1.0000000e+00 1.9439696e-26
 1.1251632e-14 2.4362573e-26], sum to 1.0000
[2019-04-03 22:30:10,633] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0816
[2019-04-03 22:30:10,661] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [14.3, 96.66666666666666, 99.0, 0.0, 26.0, 25.07439048837099, 0.4861729428038233, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1253400.0000, 
sim time next is 1254000.0000, 
raw observation next is [14.2, 97.33333333333333, 100.0, 0.0, 26.0, 25.05668777786639, 0.4837660689816374, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.8559556786703602, 0.9733333333333333, 0.3333333333333333, 0.0, 0.6666666666666666, 0.5880573148221991, 0.6612553563272124, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5286756], dtype=float32), -0.24525362]. 
=============================================
[2019-04-03 22:30:10,677] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[105.497604]
 [105.808754]
 [105.92503 ]
 [106.18693 ]
 [106.33752 ]], R is [[105.19297791]
 [105.14105225]
 [105.08964539]
 [105.03874969]
 [104.98836517]].
[2019-04-03 22:30:16,254] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.8601273e-35 1.0059871e-24 8.0533101e-21 1.0000000e+00 2.2439403e-26
 6.2292592e-16 3.6031243e-26], sum to 1.0000
[2019-04-03 22:30:16,259] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5755
[2019-04-03 22:30:16,274] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [14.2, 97.33333333333333, 100.0, 0.0, 26.0, 25.06039465577191, 0.484722894330939, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1254000.0000, 
sim time next is 1254600.0000, 
raw observation next is [14.1, 98.0, 101.0, 0.0, 26.0, 25.03081697377322, 0.4813410102064959, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.8531855955678671, 0.98, 0.33666666666666667, 0.0, 0.6666666666666666, 0.5859014144811017, 0.6604470034021653, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4017652], dtype=float32), -0.6872821]. 
=============================================
[2019-04-03 22:30:20,887] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.8778876e-27 3.8491255e-17 1.1255152e-14 1.0000000e+00 6.3999533e-19
 3.1513476e-09 6.2202948e-20], sum to 1.0000
[2019-04-03 22:30:20,888] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1809
[2019-04-03 22:30:20,899] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.5465243e-24 2.7085304e-15 1.5176819e-12 9.9998820e-01 2.6932188e-16
 1.1789825e-05 4.9103321e-17], sum to 1.0000
[2019-04-03 22:30:20,902] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.133333333333334, 98.66666666666667, 0.0, 0.0, 26.0, 25.47071303379795, 0.5881729647628896, 0.0, 1.0, 34838.73956509513], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1293600.0000, 
sim time next is 1294200.0000, 
raw observation next is [4.95, 98.0, 0.0, 0.0, 26.0, 25.45711836731868, 0.5855540021614762, 0.0, 1.0, 40976.07881020577], 
processed observation next is [0.0, 1.0, 0.5997229916897507, 0.98, 0.0, 0.0, 0.6666666666666666, 0.6214265306098902, 0.6951846673871588, 0.0, 1.0, 0.19512418481050367], 
reward next is 0.8049, 
noisyNet noise sample is [array([0.90060115], dtype=float32), -0.06113463]. 
=============================================
[2019-04-03 22:30:20,904] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4637
[2019-04-03 22:30:20,919] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.7000000000000001, 92.0, 92.5, 0.0, 26.0, 26.10108044943661, 0.5906751919332902, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1333200.0000, 
sim time next is 1333800.0000, 
raw observation next is [0.8, 92.0, 102.0, 0.0, 26.0, 26.10362989854378, 0.5900971540596633, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4847645429362882, 0.92, 0.34, 0.0, 0.6666666666666666, 0.675302491545315, 0.6966990513532211, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0133985], dtype=float32), 0.99656904]. 
=============================================
[2019-04-03 22:30:38,003] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.9416635e-30 2.0253359e-20 4.0615031e-17 1.0000000e+00 5.9177866e-21
 1.0241455e-12 5.7774577e-22], sum to 1.0000
[2019-04-03 22:30:38,005] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9227
[2019-04-03 22:30:38,027] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.5, 85.5, 0.0, 0.0, 26.0, 25.54555888129156, 0.5077558380549925, 0.0, 1.0, 49638.32485043934], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1567800.0000, 
sim time next is 1568400.0000, 
raw observation next is [4.533333333333333, 85.33333333333334, 0.0, 0.0, 26.0, 25.48343453758703, 0.5143981216712735, 0.0, 1.0, 72494.15544986009], 
processed observation next is [1.0, 0.13043478260869565, 0.5881809787626964, 0.8533333333333334, 0.0, 0.0, 0.6666666666666666, 0.6236195447989191, 0.6714660405570911, 0.0, 1.0, 0.3452102640469528], 
reward next is 0.6548, 
noisyNet noise sample is [array([-1.1970588], dtype=float32), 0.028004995]. 
=============================================
[2019-04-03 22:30:39,329] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0095020e-29 1.9708787e-20 1.0608137e-15 1.0000000e+00 1.7578643e-20
 9.9518805e-12 5.4093153e-21], sum to 1.0000
[2019-04-03 22:30:39,331] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3974
[2019-04-03 22:30:39,343] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.2, 96.0, 0.0, 0.0, 26.0, 25.36681314474454, 0.447521448571136, 0.0, 1.0, 57872.017503806], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1483200.0000, 
sim time next is 1483800.0000, 
raw observation next is [2.2, 96.0, 0.0, 0.0, 26.0, 25.31759645452876, 0.45426112824074, 0.0, 1.0, 47955.27094420572], 
processed observation next is [1.0, 0.17391304347826086, 0.5235457063711911, 0.96, 0.0, 0.0, 0.6666666666666666, 0.6097997045440634, 0.6514203760802467, 0.0, 1.0, 0.22835843306764628], 
reward next is 0.7716, 
noisyNet noise sample is [array([0.7512652], dtype=float32), -0.19794019]. 
=============================================
[2019-04-03 22:30:42,375] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2631363e-28 1.4703834e-18 1.0148032e-14 1.0000000e+00 3.1345809e-20
 1.1500786e-10 2.7209515e-20], sum to 1.0000
[2019-04-03 22:30:42,377] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4985
[2019-04-03 22:30:42,389] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.1, 87.16666666666667, 0.0, 0.0, 26.0, 25.62142952373982, 0.5828437039600413, 0.0, 1.0, 38167.10953910777], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1642200.0000, 
sim time next is 1642800.0000, 
raw observation next is [7.0, 88.33333333333334, 0.0, 0.0, 26.0, 25.57693931377625, 0.591247395804213, 0.0, 1.0, 56596.53492632153], 
processed observation next is [1.0, 0.0, 0.6565096952908588, 0.8833333333333334, 0.0, 0.0, 0.6666666666666666, 0.6314116094813542, 0.697082465268071, 0.0, 1.0, 0.2695073091729597], 
reward next is 0.7305, 
noisyNet noise sample is [array([-0.28624108], dtype=float32), -0.44893008]. 
=============================================
[2019-04-03 22:30:43,522] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6206482e-30 2.4216897e-20 3.6186095e-17 1.0000000e+00 2.6464565e-21
 9.0136069e-13 1.3045604e-21], sum to 1.0000
[2019-04-03 22:30:43,523] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3182
[2019-04-03 22:30:43,544] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.716666666666667, 92.0, 0.0, 0.0, 26.0, 25.5837766651116, 0.5350303444755304, 0.0, 1.0, 30685.46959574369], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1667400.0000, 
sim time next is 1668000.0000, 
raw observation next is [4.433333333333334, 92.0, 0.0, 0.0, 26.0, 25.57269605113063, 0.5275693418723318, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.5854108956602032, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6310580042608859, 0.6758564472907773, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5906485], dtype=float32), 1.0981245]. 
=============================================
[2019-04-03 22:30:43,557] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[81.99994]
 [82.18237]
 [82.39488]
 [82.62107]
 [82.82843]], R is [[84.61148071]
 [84.61924744]
 [84.63998413]
 [84.70437622]
 [84.76810455]].
[2019-04-03 22:30:43,576] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.9889455e-31 2.9987953e-20 1.5633727e-16 1.0000000e+00 7.8818267e-23
 8.1754541e-13 2.6479685e-23], sum to 1.0000
[2019-04-03 22:30:43,578] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5749
[2019-04-03 22:30:43,588] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [11.06666666666667, 58.66666666666667, 0.0, 0.0, 26.0, 26.77670627186249, 0.752904965319309, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1618800.0000, 
sim time next is 1619400.0000, 
raw observation next is [10.78333333333333, 59.83333333333334, 0.0, 0.0, 26.0, 26.6390926676611, 0.745921382537278, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.7613111726685133, 0.5983333333333334, 0.0, 0.0, 0.6666666666666666, 0.7199243889717583, 0.7486404608457593, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3825027], dtype=float32), -0.10702468]. 
=============================================
[2019-04-03 22:30:53,126] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.0118625e-29 2.8873506e-18 7.5465246e-16 1.0000000e+00 4.1856573e-22
 1.4405728e-13 2.8180519e-21], sum to 1.0000
[2019-04-03 22:30:53,128] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9759
[2019-04-03 22:30:53,141] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.1666666666666667, 91.00000000000001, 0.0, 0.0, 26.0, 25.28781885436402, 0.4401674144668438, 0.0, 1.0, 42923.43741880801], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1735800.0000, 
sim time next is 1736400.0000, 
raw observation next is [0.1333333333333334, 91.0, 0.0, 0.0, 26.0, 25.27343632816496, 0.4382061886542994, 0.0, 1.0, 42936.07068210789], 
processed observation next is [0.0, 0.08695652173913043, 0.46629732225300097, 0.91, 0.0, 0.0, 0.6666666666666666, 0.6061196940137465, 0.6460687295514331, 0.0, 1.0, 0.204457479438609], 
reward next is 0.7955, 
noisyNet noise sample is [array([0.14277382], dtype=float32), 0.56379265]. 
=============================================
[2019-04-03 22:31:01,576] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3354051e-29 8.8807310e-22 9.3289835e-18 1.0000000e+00 1.1237320e-19
 3.1270336e-12 4.2713044e-23], sum to 1.0000
[2019-04-03 22:31:01,577] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2206
[2019-04-03 22:31:01,617] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.166666666666666, 76.33333333333334, 181.1666666666667, 198.3333333333333, 26.0, 25.80767532803569, 0.3218809059230635, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1939200.0000, 
sim time next is 1939800.0000, 
raw observation next is [-5.883333333333333, 75.66666666666666, 191.3333333333333, 160.6666666666667, 26.0, 25.7468756547589, 0.3170652345587175, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.2996306555863343, 0.7566666666666666, 0.6377777777777777, 0.1775322283609577, 0.6666666666666666, 0.6455729712299082, 0.6056884115195725, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.62909365], dtype=float32), 0.8680393]. 
=============================================
[2019-04-03 22:31:06,775] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.9419739e-28 7.8167517e-18 2.4143891e-16 1.0000000e+00 4.2701565e-19
 2.0411144e-10 7.4268760e-20], sum to 1.0000
[2019-04-03 22:31:06,775] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6361
[2019-04-03 22:31:06,818] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.9, 84.0, 0.0, 0.0, 26.0, 25.52610492924003, 0.3802881758082912, 0.0, 1.0, 9355.897741457735], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2057400.0000, 
sim time next is 2058000.0000, 
raw observation next is [-3.899999999999999, 84.66666666666666, 0.0, 0.0, 26.0, 25.24162614444192, 0.3444135820326544, 0.0, 1.0, 18709.84912982773], 
processed observation next is [1.0, 0.8260869565217391, 0.35457063711911363, 0.8466666666666666, 0.0, 0.0, 0.6666666666666666, 0.60346884537016, 0.6148045273442181, 0.0, 1.0, 0.08909451966584633], 
reward next is 0.9109, 
noisyNet noise sample is [array([-0.0828574], dtype=float32), -1.0612997]. 
=============================================
[2019-04-03 22:31:06,822] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[83.980316]
 [86.92338 ]
 [87.01022 ]
 [87.04581 ]
 [87.08727 ]], R is [[82.50930023]
 [82.63965607]
 [82.81326294]
 [82.98513031]
 [83.15528107]].
[2019-04-03 22:31:18,824] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.2998959e-28 3.9941276e-18 3.2901735e-16 1.0000000e+00 6.0476202e-20
 2.6369757e-12 1.5977659e-20], sum to 1.0000
[2019-04-03 22:31:18,825] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8708
[2019-04-03 22:31:18,931] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.8, 84.5, 69.0, 0.0, 26.0, 25.50784610324806, 0.2841818679353589, 1.0, 1.0, 18724.46380219951], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2021400.0000, 
sim time next is 2022000.0000, 
raw observation next is [-5.733333333333333, 84.0, 74.5, 0.0, 26.0, 25.53167680850036, 0.2910859133443859, 1.0, 1.0, 18721.93189313847], 
processed observation next is [1.0, 0.391304347826087, 0.30378578024007385, 0.84, 0.24833333333333332, 0.0, 0.6666666666666666, 0.6276397340416967, 0.597028637781462, 1.0, 1.0, 0.08915205663399271], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.11029071], dtype=float32), 0.6429042]. 
=============================================
[2019-04-03 22:31:18,961] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[79.77821]
 [80.04651]
 [80.36167]
 [80.6557 ]
 [81.0729 ]], R is [[79.79393768]
 [79.90683746]
 [80.10777283]
 [80.30669403]
 [80.50363159]].
[2019-04-03 22:31:28,373] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.5571025e-25 3.2541980e-17 2.5051375e-13 1.0000000e+00 6.5670406e-18
 1.8403152e-09 1.0985684e-18], sum to 1.0000
[2019-04-03 22:31:28,374] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0714
[2019-04-03 22:31:28,439] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 66.0, 36.0, 0.0, 26.0, 25.83405164096588, 0.4531578940153474, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2132400.0000, 
sim time next is 2133000.0000, 
raw observation next is [-4.5, 66.5, 26.0, 0.0, 26.0, 25.98552293966285, 0.4584943154444107, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.3379501385041552, 0.665, 0.08666666666666667, 0.0, 0.6666666666666666, 0.6654602449719041, 0.6528314384814703, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.1110376], dtype=float32), -0.25968692]. 
=============================================
[2019-04-03 22:31:28,443] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.5751802e-26 3.4550472e-18 7.0112095e-14 1.0000000e+00 1.6864397e-18
 6.8947092e-10 2.0870867e-19], sum to 1.0000
[2019-04-03 22:31:28,459] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[73.58643 ]
 [73.940994]
 [74.24183 ]
 [73.72351 ]
 [73.71253 ]], R is [[73.67917633]
 [73.94238281]
 [74.20295715]
 [73.51599121]
 [73.420578  ]].
[2019-04-03 22:31:28,461] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5567
[2019-04-03 22:31:28,502] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 86.0, 87.5, 0.0, 26.0, 26.24713443844342, 0.4537235158962847, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2041200.0000, 
sim time next is 2041800.0000, 
raw observation next is [-4.4, 85.33333333333334, 82.0, 0.0, 26.0, 26.19597098016751, 0.4446634559273726, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.3407202216066482, 0.8533333333333334, 0.2733333333333333, 0.0, 0.6666666666666666, 0.6829975816806257, 0.6482211519757909, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1134672], dtype=float32), -1.8302103]. 
=============================================
[2019-04-03 22:31:29,124] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3126132e-27 1.4639256e-17 1.4979058e-15 1.0000000e+00 7.5529355e-19
 1.1638006e-12 1.7537326e-19], sum to 1.0000
[2019-04-03 22:31:29,125] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7404
[2019-04-03 22:31:29,202] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 90.16666666666667, 0.0, 0.0, 26.0, 24.66098857306477, 0.233944312941347, 0.0, 1.0, 42695.72949621734], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2077800.0000, 
sim time next is 2078400.0000, 
raw observation next is [-4.5, 89.33333333333334, 0.0, 0.0, 26.0, 24.67087845879976, 0.2254727660941713, 0.0, 1.0, 42696.79055778226], 
processed observation next is [1.0, 0.043478260869565216, 0.3379501385041552, 0.8933333333333334, 0.0, 0.0, 0.6666666666666666, 0.5559065382333133, 0.5751575886980571, 0.0, 1.0, 0.20331805027515362], 
reward next is 0.7967, 
noisyNet noise sample is [array([0.49312466], dtype=float32), -0.83198494]. 
=============================================
[2019-04-03 22:31:51,102] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.84227058e-29 7.28431080e-20 7.49093969e-16 1.00000000e+00
 4.36728454e-20 1.36330942e-12 1.00212834e-20], sum to 1.0000
[2019-04-03 22:31:51,103] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5248
[2019-04-03 22:31:51,218] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.583333333333334, 87.66666666666666, 63.66666666666666, 23.66666666666666, 26.0, 25.62655415564215, 0.3039962482337284, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2278200.0000, 
sim time next is 2278800.0000, 
raw observation next is [-8.4, 87.0, 73.0, 27.5, 26.0, 25.62537242936445, 0.3043227667640716, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.2299168975069252, 0.87, 0.24333333333333335, 0.03038674033149171, 0.6666666666666666, 0.6354477024470375, 0.6014409222546905, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.21383928], dtype=float32), -0.7230983]. 
=============================================
[2019-04-03 22:32:00,115] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0256837e-24 1.2855539e-16 9.4197436e-14 1.0000000e+00 1.2439066e-17
 4.6115238e-12 2.0011923e-18], sum to 1.0000
[2019-04-03 22:32:00,130] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4069
[2019-04-03 22:32:00,226] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.4, 61.0, 0.0, 0.0, 26.0, 23.43285882666422, -0.1008303509853151, 0.0, 1.0, 44416.0790300315], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2436000.0000, 
sim time next is 2436600.0000, 
raw observation next is [-8.4, 61.0, 0.0, 0.0, 26.0, 23.39689345731361, -0.1087012121410302, 0.0, 1.0, 44403.88808588979], 
processed observation next is [0.0, 0.17391304347826086, 0.2299168975069252, 0.61, 0.0, 0.0, 0.6666666666666666, 0.4497411214428008, 0.4637662626196566, 0.0, 1.0, 0.21144708612328472], 
reward next is 0.7886, 
noisyNet noise sample is [array([-1.3405062], dtype=float32), -0.054966718]. 
=============================================
[2019-04-03 22:32:01,387] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.6711341e-28 3.9013338e-19 3.3917671e-16 1.0000000e+00 6.8451998e-19
 8.8402793e-13 1.3688277e-20], sum to 1.0000
[2019-04-03 22:32:01,387] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3622
[2019-04-03 22:32:01,481] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.816666666666666, 90.33333333333334, 0.0, 0.0, 26.0, 23.87036965869764, 0.01830697170904154, 0.0, 1.0, 43468.44719606221], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2263800.0000, 
sim time next is 2264400.0000, 
raw observation next is [-8.9, 91.0, 0.0, 0.0, 26.0, 23.81049915970418, 0.008865452582490102, 0.0, 1.0, 43421.80087154032], 
processed observation next is [1.0, 0.21739130434782608, 0.21606648199445982, 0.91, 0.0, 0.0, 0.6666666666666666, 0.48420826330868155, 0.50295515086083, 0.0, 1.0, 0.2067704803406682], 
reward next is 0.7932, 
noisyNet noise sample is [array([3.0625925], dtype=float32), -0.29344496]. 
=============================================
[2019-04-03 22:32:01,584] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.9721497e-28 4.4327291e-19 6.6118153e-16 1.0000000e+00 1.6112698e-18
 2.2261880e-12 7.2787798e-21], sum to 1.0000
[2019-04-03 22:32:01,584] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0296
[2019-04-03 22:32:01,607] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.9, 91.00000000000001, 0.0, 0.0, 26.0, 23.76265836962004, -0.003636141143075084, 0.0, 1.0, 43366.43729055744], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2265000.0000, 
sim time next is 2265600.0000, 
raw observation next is [-8.900000000000002, 91.0, 0.0, 0.0, 26.0, 23.69374840195481, -0.008760625414634432, 0.0, 1.0, 43320.40252292853], 
processed observation next is [1.0, 0.21739130434782608, 0.2160664819944598, 0.91, 0.0, 0.0, 0.6666666666666666, 0.47447903349623416, 0.4970797915284552, 0.0, 1.0, 0.20628763106156445], 
reward next is 0.7937, 
noisyNet noise sample is [array([-0.28337705], dtype=float32), -0.8793433]. 
=============================================
[2019-04-03 22:32:12,871] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.4221935e-25 9.0812685e-17 3.6022187e-14 1.0000000e+00 3.5000977e-18
 1.7824006e-11 3.8394010e-18], sum to 1.0000
[2019-04-03 22:32:12,872] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0587
[2019-04-03 22:32:12,921] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 42.0, 0.0, 0.0, 26.0, 25.15743960709527, 0.2868123765157258, 0.0, 1.0, 43326.75035819096], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2406000.0000, 
sim time next is 2406600.0000, 
raw observation next is [-3.4, 42.0, 0.0, 0.0, 26.0, 25.15860901874729, 0.2810689106720466, 0.0, 1.0, 43113.824395203], 
processed observation next is [0.0, 0.8695652173913043, 0.368421052631579, 0.42, 0.0, 0.0, 0.6666666666666666, 0.596550751562274, 0.5936896368906822, 0.0, 1.0, 0.20530392569144287], 
reward next is 0.7947, 
noisyNet noise sample is [array([0.06327444], dtype=float32), 1.1942933]. 
=============================================
[2019-04-03 22:32:18,188] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2154138e-28 3.6101538e-19 9.1441184e-17 1.0000000e+00 2.1711696e-20
 4.4681823e-13 1.8295849e-20], sum to 1.0000
[2019-04-03 22:32:18,188] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4449
[2019-04-03 22:32:18,231] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 69.0, 0.0, 0.0, 26.0, 24.00566339439197, 0.05943498292303753, 0.0, 1.0, 41607.94252565796], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2359800.0000, 
sim time next is 2360400.0000, 
raw observation next is [-3.4, 69.0, 7.833333333333332, 0.0, 26.0, 23.97057340625974, 0.05382990174113351, 0.0, 1.0, 41698.12880672357], 
processed observation next is [0.0, 0.30434782608695654, 0.368421052631579, 0.69, 0.026111111111111106, 0.0, 0.6666666666666666, 0.4975477838549782, 0.5179433005803779, 0.0, 1.0, 0.19856251812725512], 
reward next is 0.8014, 
noisyNet noise sample is [array([-0.786593], dtype=float32), -0.20239143]. 
=============================================
[2019-04-03 22:32:23,025] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.26865235e-26 5.22772092e-17 6.57463653e-14 1.00000000e+00
 5.47231757e-17 5.15877951e-10 3.11955219e-19], sum to 1.0000
[2019-04-03 22:32:23,025] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3064
[2019-04-03 22:32:23,038] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.35, 57.5, 0.0, 0.0, 26.0, 25.33661159781706, 0.3449537641752138, 0.0, 1.0, 53831.95408900568], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2586600.0000, 
sim time next is 2587200.0000, 
raw observation next is [-3.533333333333333, 58.0, 0.0, 0.0, 26.0, 25.21961152602672, 0.3322293445587113, 0.0, 1.0, 48843.12742881214], 
processed observation next is [1.0, 0.9565217391304348, 0.36472760849492153, 0.58, 0.0, 0.0, 0.6666666666666666, 0.6016342938355601, 0.6107431148529038, 0.0, 1.0, 0.23258632108958163], 
reward next is 0.7674, 
noisyNet noise sample is [array([-0.21899551], dtype=float32), -0.21231778]. 
=============================================
[2019-04-03 22:32:47,077] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.7835227e-30 3.0605405e-22 1.1383110e-18 1.0000000e+00 6.4129939e-22
 1.4388304e-15 6.6991785e-23], sum to 1.0000
[2019-04-03 22:32:47,111] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2027
[2019-04-03 22:32:47,130] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.1, 78.66666666666667, 0.0, 0.0, 26.0, 23.83789298488022, 0.01046738961455313, 0.0, 1.0, 44623.55142461543], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2616000.0000, 
sim time next is 2616600.0000, 
raw observation next is [-7.199999999999999, 78.83333333333334, 0.0, 0.0, 26.0, 23.78980662573852, 0.01311993383323187, 0.0, 1.0, 44741.33137886503], 
processed observation next is [1.0, 0.2608695652173913, 0.26315789473684215, 0.7883333333333334, 0.0, 0.0, 0.6666666666666666, 0.4824838854782101, 0.504373311277744, 0.0, 1.0, 0.21305395894697632], 
reward next is 0.7869, 
noisyNet noise sample is [array([-2.4864063], dtype=float32), 0.8426616]. 
=============================================
[2019-04-03 22:32:53,824] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.6031226e-27 9.9898859e-18 1.9518134e-14 1.0000000e+00 3.5023948e-18
 5.6282611e-11 1.0640741e-19], sum to 1.0000
[2019-04-03 22:32:53,824] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3268
[2019-04-03 22:32:53,908] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.8, 79.66666666666667, 0.0, 0.0, 26.0, 24.36647836804698, 0.130942942934135, 0.0, 1.0, 42645.56232276826], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2607600.0000, 
sim time next is 2608200.0000, 
raw observation next is [-5.9, 80.5, 0.0, 0.0, 26.0, 24.36927679623818, 0.132622972426569, 0.0, 1.0, 42725.50414343813], 
processed observation next is [1.0, 0.17391304347826086, 0.2991689750692521, 0.805, 0.0, 0.0, 0.6666666666666666, 0.5307730663531816, 0.544207657475523, 0.0, 1.0, 0.20345478163541966], 
reward next is 0.7965, 
noisyNet noise sample is [array([2.137209], dtype=float32), 0.21983807]. 
=============================================
[2019-04-03 22:33:13,634] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.5115696e-27 7.8849699e-19 9.8922098e-16 1.0000000e+00 3.1690633e-20
 6.7097115e-13 6.2692006e-20], sum to 1.0000
[2019-04-03 22:33:13,635] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9413
[2019-04-03 22:33:13,673] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.166666666666667, 82.83333333333333, 0.0, 0.0, 26.0, 24.30756462897351, 0.1971941858782554, 0.0, 1.0, 42805.47883383481], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2956200.0000, 
sim time next is 2956800.0000, 
raw observation next is [-3.333333333333333, 81.66666666666667, 0.0, 0.0, 26.0, 24.28846898285393, 0.1902497868380746, 0.0, 1.0, 42742.97934097329], 
processed observation next is [0.0, 0.21739130434782608, 0.37026777469990774, 0.8166666666666668, 0.0, 0.0, 0.6666666666666666, 0.5240390819044943, 0.5634165956126915, 0.0, 1.0, 0.20353799686177756], 
reward next is 0.7965, 
noisyNet noise sample is [array([-1.2740859], dtype=float32), 0.5026602]. 
=============================================
[2019-04-03 22:33:19,236] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1667747e-23 1.2228495e-14 6.0912966e-13 1.0000000e+00 2.6859073e-14
 1.7410906e-08 3.3443806e-18], sum to 1.0000
[2019-04-03 22:33:19,236] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8240
[2019-04-03 22:33:19,267] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.666666666666667, 51.66666666666666, 165.8333333333333, 550.5, 26.0, 26.0286942750993, 0.4558908694970039, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2803200.0000, 
sim time next is 2803800.0000, 
raw observation next is [-1.333333333333333, 50.83333333333334, 157.6666666666667, 593.0, 26.0, 26.02323793888207, 0.4633068999606353, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.42566943674976926, 0.5083333333333334, 0.5255555555555557, 0.6552486187845303, 0.6666666666666666, 0.6686031615735057, 0.6544356333202118, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5839209], dtype=float32), 1.1252546]. 
=============================================
[2019-04-03 22:33:24,271] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.04429376e-26 2.29071396e-18 8.15874671e-15 1.00000000e+00
 1.22580748e-19 7.97783335e-13 2.45486811e-19], sum to 1.0000
[2019-04-03 22:33:24,271] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4153
[2019-04-03 22:33:24,284] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.79758930385015, 0.3081028442276084, 0.0, 1.0, 43140.4072917513], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2944800.0000, 
sim time next is 2945400.0000, 
raw observation next is [-2.166666666666667, 84.83333333333334, 0.0, 0.0, 26.0, 24.76082735178473, 0.3048323513806284, 0.0, 1.0, 43100.42036472868], 
processed observation next is [0.0, 0.08695652173913043, 0.4025854108956602, 0.8483333333333334, 0.0, 0.0, 0.6666666666666666, 0.5634022793153942, 0.6016107837935428, 0.0, 1.0, 0.20524009697489848], 
reward next is 0.7948, 
noisyNet noise sample is [array([0.33677346], dtype=float32), 1.5817033]. 
=============================================
[2019-04-03 22:33:41,194] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.8242191e-27 2.3809130e-17 7.8924794e-15 1.0000000e+00 6.1499167e-19
 8.4172523e-11 4.0790732e-20], sum to 1.0000
[2019-04-03 22:33:41,195] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8288
[2019-04-03 22:33:41,258] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.4, 78.66666666666667, 0.0, 0.0, 26.0, 25.03082763993773, 0.2856719377429218, 0.0, 1.0, 48696.21634903156], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3087600.0000, 
sim time next is 3088200.0000, 
raw observation next is [-0.5, 80.33333333333334, 0.0, 0.0, 26.0, 25.00510342157401, 0.283941093206694, 0.0, 1.0, 53608.2186960215], 
processed observation next is [0.0, 0.7391304347826086, 0.44875346260387816, 0.8033333333333335, 0.0, 0.0, 0.6666666666666666, 0.5837586184645008, 0.5946470310688979, 0.0, 1.0, 0.25527723188581664], 
reward next is 0.7447, 
noisyNet noise sample is [array([2.0056362], dtype=float32), 0.9637852]. 
=============================================
[2019-04-03 22:33:48,302] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.1794589e-27 3.6809402e-17 2.4238771e-15 1.0000000e+00 1.8174839e-17
 1.7424652e-09 5.9338848e-20], sum to 1.0000
[2019-04-03 22:33:48,303] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8699
[2019-04-03 22:33:48,336] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.0, 100.0, 24.66666666666666, 243.6666666666666, 26.0, 27.23751715913389, 0.9315683675329908, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3172200.0000, 
sim time next is 3172800.0000, 
raw observation next is [6.0, 100.0, 16.33333333333333, 179.8333333333333, 26.0, 27.44162832480716, 0.686143755516969, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.6288088642659281, 1.0, 0.05444444444444443, 0.19871086556169423, 0.6666666666666666, 0.7868023604005966, 0.728714585172323, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0078111], dtype=float32), 0.21715908]. 
=============================================
[2019-04-03 22:33:52,725] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.7860806e-26 9.6279495e-17 2.1363629e-13 1.0000000e+00 6.0740554e-17
 9.9962878e-11 2.6098065e-19], sum to 1.0000
[2019-04-03 22:33:52,725] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9599
[2019-04-03 22:33:52,768] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 26.0, 25.29469152192078, 0.4759387271112726, 0.0, 1.0, 50863.16512205518], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3279000.0000, 
sim time next is 3279600.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 26.0, 25.31607494448173, 0.4712733330345962, 0.0, 1.0, 45716.19565910571], 
processed observation next is [1.0, 1.0, 0.296398891966759, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6096729120401442, 0.6570911110115321, 0.0, 1.0, 0.2176961698052653], 
reward next is 0.7823, 
noisyNet noise sample is [array([-1.3077669], dtype=float32), 0.6705577]. 
=============================================
[2019-04-03 22:33:55,637] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.3803356e-22 3.8950078e-14 9.6563816e-13 9.9999976e-01 2.2903741e-13
 2.7644137e-07 2.7488208e-17], sum to 1.0000
[2019-04-03 22:33:55,637] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6518
[2019-04-03 22:33:55,697] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.333333333333333, 73.0, 63.33333333333334, 540.1666666666667, 26.0, 27.08307794843409, 0.5240276842594996, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3255600.0000, 
sim time next is 3256200.0000, 
raw observation next is [-3.5, 74.0, 59.0, 511.0, 26.0, 26.92044941831657, 0.7846024476287273, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.36565096952908593, 0.74, 0.19666666666666666, 0.5646408839779006, 0.6666666666666666, 0.7433707848597141, 0.7615341492095759, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1990135], dtype=float32), -0.041916005]. 
=============================================
[2019-04-03 22:34:03,239] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.2301419e-30 9.6206656e-20 4.8721957e-17 1.0000000e+00 3.0066056e-21
 1.4996369e-13 4.1794189e-22], sum to 1.0000
[2019-04-03 22:34:03,239] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7487
[2019-04-03 22:34:03,276] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.22227620931559, 0.3830099670167806, 0.0, 1.0, 41941.61244485113], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3474000.0000, 
sim time next is 3474600.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.19712566773391, 0.3818579412783036, 0.0, 1.0, 41909.55255039756], 
processed observation next is [1.0, 0.21739130434782608, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5997604723111593, 0.6272859804261012, 0.0, 1.0, 0.199569297859036], 
reward next is 0.8004, 
noisyNet noise sample is [array([2.4748955], dtype=float32), 0.030311871]. 
=============================================
[2019-04-03 22:34:07,467] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.3596822e-28 4.6040899e-19 4.1578844e-16 1.0000000e+00 8.4081804e-20
 1.6616984e-13 1.9142094e-20], sum to 1.0000
[2019-04-03 22:34:07,474] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1958
[2019-04-03 22:34:07,517] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.666666666666667, 69.16666666666667, 0.0, 0.0, 26.0, 24.80925681766695, 0.2319095762458566, 0.0, 1.0, 42730.63027412078], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3388200.0000, 
sim time next is 3388800.0000, 
raw observation next is [-4.333333333333334, 67.33333333333334, 0.0, 0.0, 26.0, 24.7213697716537, 0.2231204989818958, 0.0, 1.0, 42664.45301560876], 
processed observation next is [1.0, 0.21739130434782608, 0.3425669436749769, 0.6733333333333335, 0.0, 0.0, 0.6666666666666666, 0.5601141476378082, 0.5743734996606319, 0.0, 1.0, 0.20316406197908932], 
reward next is 0.7968, 
noisyNet noise sample is [array([1.9676731], dtype=float32), 0.63041013]. 
=============================================
[2019-04-03 22:34:09,971] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-03 22:34:09,971] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-03 22:34:09,971] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:34:09,973] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run6
[2019-04-03 22:34:09,994] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-03 22:34:09,995] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:34:09,996] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-03 22:34:09,997] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:34:09,999] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run6
[2019-04-03 22:34:10,022] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run6
[2019-04-03 22:36:40,753] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7098.8260 178876465.7181 -817.5205
[2019-04-03 22:36:56,594] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 6569.6489 208183432.0125 -1203.7559
[2019-04-03 22:37:02,386] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 6935.8559 223694268.1349 -1082.0515
[2019-04-03 22:37:03,421] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 500000, evaluation results [500000.0, 6569.648938209325, 208183432.01248318, -1203.7558623349244, 7098.826049830529, 178876465.7180862, -817.5204724140117, 6935.855901593388, 223694268.13492185, -1082.0514637920874]
[2019-04-03 22:37:09,130] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4355611e-25 1.1745089e-16 1.3334865e-13 1.0000000e+00 1.1120710e-15
 2.8565350e-09 2.5819229e-18], sum to 1.0000
[2019-04-03 22:37:09,134] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2214
[2019-04-03 22:37:09,145] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 41.66666666666667, 66.83333333333333, 545.6666666666666, 26.0, 25.30735822507274, 0.4475595360129611, 0.0, 1.0, 9342.17152307084], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3601200.0000, 
sim time next is 3601800.0000, 
raw observation next is [0.0, 41.0, 63.0, 515.0, 26.0, 25.28263011593106, 0.4403150708072341, 0.0, 1.0, 18684.24474005441], 
processed observation next is [0.0, 0.6956521739130435, 0.46260387811634357, 0.41, 0.21, 0.569060773480663, 0.6666666666666666, 0.606885842994255, 0.646771690269078, 0.0, 1.0, 0.0889725940002591], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.98830783], dtype=float32), -1.9676818]. 
=============================================
[2019-04-03 22:37:15,545] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3746066e-32 1.1735423e-22 2.0993366e-18 1.0000000e+00 3.3936578e-23
 4.3849409e-15 6.7838078e-24], sum to 1.0000
[2019-04-03 22:37:15,545] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0356
[2019-04-03 22:37:15,564] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [12.0, 24.0, 113.5, 789.5, 26.0, 25.69011580173639, 0.499441110201415, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3668400.0000, 
sim time next is 3669000.0000, 
raw observation next is [10.66666666666667, 27.5, 114.3333333333333, 798.3333333333334, 26.0, 25.6998467129139, 0.4999729014682073, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.7580794090489382, 0.275, 0.381111111111111, 0.8821362799263353, 0.6666666666666666, 0.6416538927428249, 0.6666576338227358, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.55640006], dtype=float32), 1.4139748]. 
=============================================
[2019-04-03 22:37:15,585] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[99.24862 ]
 [99.259514]
 [99.2417  ]
 [99.11554 ]
 [98.90261 ]], R is [[99.0916748 ]
 [99.10076141]
 [99.10975647]
 [99.11865997]
 [99.12747192]].
[2019-04-03 22:37:17,972] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.2033045e-27 4.7625048e-19 1.9014200e-15 1.0000000e+00 1.4768078e-19
 5.3831429e-13 2.6149174e-20], sum to 1.0000
[2019-04-03 22:37:17,973] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9395
[2019-04-03 22:37:17,998] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 26.0, 25.06086851525421, 0.3240253606944634, 0.0, 1.0, 43615.46490208627], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3814800.0000, 
sim time next is 3815400.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 26.0, 25.01709305017878, 0.3187075242061106, 0.0, 1.0, 43675.38840927117], 
processed observation next is [1.0, 0.13043478260869565, 0.3518005540166205, 0.71, 0.0, 0.0, 0.6666666666666666, 0.584757754181565, 0.6062358414020369, 0.0, 1.0, 0.2079780400441484], 
reward next is 0.7920, 
noisyNet noise sample is [array([0.05698205], dtype=float32), -0.6322422]. 
=============================================
[2019-04-03 22:37:21,004] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.1477879e-26 3.3945745e-17 4.8533931e-14 1.0000000e+00 7.1499151e-18
 8.3114765e-12 1.0877928e-18], sum to 1.0000
[2019-04-03 22:37:21,006] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8525
[2019-04-03 22:37:21,027] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.666666666666667, 73.0, 0.0, 0.0, 26.0, 25.09291090813137, 0.2654621140308234, 0.0, 1.0, 42293.90543760363], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3739200.0000, 
sim time next is 3739800.0000, 
raw observation next is [-3.833333333333333, 75.0, 0.0, 0.0, 26.0, 25.05436497412045, 0.2548464068040417, 0.0, 1.0, 42197.95534893483], 
processed observation next is [1.0, 0.2608695652173913, 0.3564173591874424, 0.75, 0.0, 0.0, 0.6666666666666666, 0.5878637478433708, 0.5849488022680139, 0.0, 1.0, 0.2009426445187373], 
reward next is 0.7991, 
noisyNet noise sample is [array([-0.08722121], dtype=float32), 1.0308955]. 
=============================================
[2019-04-03 22:37:21,360] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.6931144e-26 3.1928149e-17 3.0454843e-13 1.0000000e+00 3.5822016e-16
 2.9900564e-09 1.6623907e-19], sum to 1.0000
[2019-04-03 22:37:21,361] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3514
[2019-04-03 22:37:21,368] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 60.0, 75.5, 625.0, 26.0, 26.87165023588729, 0.7120412876341571, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3772800.0000, 
sim time next is 3773400.0000, 
raw observation next is [0.0, 60.0, 71.66666666666666, 596.3333333333333, 26.0, 26.9077387714812, 0.7153324793165563, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.46260387811634357, 0.6, 0.23888888888888885, 0.6589318600368324, 0.6666666666666666, 0.7423115642900999, 0.7384441597721855, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5317768], dtype=float32), 0.037251417]. 
=============================================
[2019-04-03 22:37:41,216] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.6527740e-31 5.1917367e-22 1.0134670e-17 1.0000000e+00 7.0995626e-23
 4.4577650e-16 3.8827967e-23], sum to 1.0000
[2019-04-03 22:37:41,216] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2099
[2019-04-03 22:37:41,243] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.333333333333334, 36.33333333333333, 0.0, 0.0, 26.0, 24.76299860689167, 0.2120322687508842, 0.0, 1.0, 40423.77094260397], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4077600.0000, 
sim time next is 4078200.0000, 
raw observation next is [-4.166666666666667, 35.16666666666667, 0.0, 0.0, 26.0, 24.70766150910635, 0.215251483835028, 0.0, 1.0, 40401.16543666989], 
processed observation next is [1.0, 0.17391304347826086, 0.3471837488457987, 0.35166666666666674, 0.0, 0.0, 0.6666666666666666, 0.5589717924255293, 0.5717504946116759, 0.0, 1.0, 0.19238650207938043], 
reward next is 0.8076, 
noisyNet noise sample is [array([0.83125865], dtype=float32), -1.0835823]. 
=============================================
[2019-04-03 22:37:49,207] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.4997747e-32 3.4170141e-23 2.2791386e-18 1.0000000e+00 1.5315989e-21
 6.2189965e-14 7.9847115e-25], sum to 1.0000
[2019-04-03 22:37:49,207] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7012
[2019-04-03 22:37:49,233] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [15.0, 28.0, 119.0, 840.5, 26.0, 28.17332425980817, 1.016928573208307, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4363200.0000, 
sim time next is 4363800.0000, 
raw observation next is [14.93333333333333, 28.16666666666667, 118.6666666666667, 844.6666666666667, 26.0, 28.25284425835605, 1.036944396855141, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.8762696214219761, 0.28166666666666673, 0.39555555555555566, 0.9333333333333335, 0.6666666666666666, 0.8544036881963374, 0.845648132285047, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2946268], dtype=float32), 0.1268159]. 
=============================================
[2019-04-03 22:37:53,916] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.06193925e-30 1.47759047e-20 9.99011966e-17 1.00000000e+00
 1.04342485e-20 1.74500266e-13 9.60283157e-23], sum to 1.0000
[2019-04-03 22:37:53,918] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1404
[2019-04-03 22:37:53,928] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.65, 82.0, 120.0, 232.0, 26.0, 25.62360857760529, 0.560972513787685, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4437000.0000, 
sim time next is 4437600.0000, 
raw observation next is [1.533333333333333, 82.66666666666667, 127.5, 198.5, 26.0, 25.88798166413678, 0.5788749133425245, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.505078485687904, 0.8266666666666667, 0.425, 0.21933701657458562, 0.6666666666666666, 0.6573318053447318, 0.6929583044475082, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4201021], dtype=float32), -1.4145024]. 
=============================================
[2019-04-03 22:37:57,199] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1117280e-29 1.0000409e-17 1.6598152e-15 1.0000000e+00 2.1113653e-20
 5.5039258e-11 1.3358785e-22], sum to 1.0000
[2019-04-03 22:37:57,204] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7613
[2019-04-03 22:37:57,218] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [13.3, 34.0, 92.33333333333334, 0.0, 26.0, 28.80972130494817, 1.136568116867391, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4376400.0000, 
sim time next is 4377000.0000, 
raw observation next is [13.15, 34.5, 81.66666666666667, 0.0, 26.0, 28.44827313836435, 1.11098993546738, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.826869806094183, 0.345, 0.27222222222222225, 0.0, 0.6666666666666666, 0.8706894281970291, 0.8703299784891266, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([3.1076226], dtype=float32), 0.8975513]. 
=============================================
[2019-04-03 22:37:57,249] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[90.61393 ]
 [90.44236 ]
 [91.255936]
 [92.17009 ]
 [93.44037 ]], R is [[91.14767456]
 [91.23619843]
 [91.32383728]
 [91.41059875]
 [91.49649048]].
[2019-04-03 22:37:59,247] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.8637610e-33 1.0040722e-20 2.8542237e-18 1.0000000e+00 7.2386700e-25
 1.6224987e-14 6.7027279e-24], sum to 1.0000
[2019-04-03 22:37:59,247] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0507
[2019-04-03 22:37:59,279] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [12.6, 42.0, 7.5, 0.0, 26.0, 27.44652993431989, 0.9951267682328072, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4383600.0000, 
sim time next is 4384200.0000, 
raw observation next is [12.5, 43.0, 6.000000000000001, 0.0, 26.0, 27.54888427844538, 1.006463445694857, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.808864265927978, 0.43, 0.020000000000000004, 0.0, 0.6666666666666666, 0.7957403565371152, 0.8354878152316191, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.20691845], dtype=float32), -0.39522567]. 
=============================================
[2019-04-03 22:38:01,126] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.1655322e-25 2.1012628e-15 7.1548448e-13 1.0000000e+00 2.5569303e-17
 7.4859635e-10 5.5833124e-18], sum to 1.0000
[2019-04-03 22:38:01,130] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0030
[2019-04-03 22:38:01,153] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.16437137599721, 0.5461793801326474, 0.0, 1.0, 88739.53701545706], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4480800.0000, 
sim time next is 4481400.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.39614245142772, 0.5701762527816734, 0.0, 1.0, 45124.80553104326], 
processed observation next is [1.0, 0.8695652173913043, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6163452042856434, 0.6900587509272245, 0.0, 1.0, 0.21488002633830125], 
reward next is 0.7851, 
noisyNet noise sample is [array([0.17235279], dtype=float32), 0.22909407]. 
=============================================
[2019-04-03 22:38:03,526] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.2173190e-31 1.8975378e-21 5.3490190e-17 1.0000000e+00 3.9593478e-23
 1.1668073e-13 8.6459234e-25], sum to 1.0000
[2019-04-03 22:38:03,526] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8751
[2019-04-03 22:38:03,576] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 51.33333333333334, 167.0, 16.0, 26.0, 25.82538354377208, 0.468205885853114, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4539000.0000, 
sim time next is 4539600.0000, 
raw observation next is [2.0, 52.0, 187.0, 24.0, 26.0, 25.7410262196968, 0.3668182378091009, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.518005540166205, 0.52, 0.6233333333333333, 0.026519337016574586, 0.6666666666666666, 0.6450855183080666, 0.6222727459363669, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.7954625], dtype=float32), 0.18767627]. 
=============================================
[2019-04-03 22:38:04,353] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.2793415e-30 1.0572887e-19 7.9814202e-18 1.0000000e+00 1.8392258e-22
 1.9652776e-13 4.2101560e-23], sum to 1.0000
[2019-04-03 22:38:04,354] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0586
[2019-04-03 22:38:04,364] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 49.33333333333333, 125.5, 0.0, 26.0, 26.22298779505073, 0.5089233451664487, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4537200.0000, 
sim time next is 4537800.0000, 
raw observation next is [2.0, 50.0, 127.0, 0.0, 26.0, 26.08129888971366, 0.4946196116921664, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.518005540166205, 0.5, 0.42333333333333334, 0.0, 0.6666666666666666, 0.6734415741428051, 0.6648732038973888, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1599367], dtype=float32), 0.84571856]. 
=============================================
[2019-04-03 22:38:16,150] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.35477105e-29 1.88942812e-19 2.09793078e-15 1.00000000e+00
 1.34306925e-20 2.03141913e-11 1.05042527e-21], sum to 1.0000
[2019-04-03 22:38:16,151] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4838
[2019-04-03 22:38:16,163] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 92.0, 209.6666666666667, 6.0, 26.0, 26.47809227176143, 0.597559943903479, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4704600.0000, 
sim time next is 4705200.0000, 
raw observation next is [0.0, 92.0, 210.5, 6.0, 26.0, 26.47504338842134, 0.592220573802546, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.46260387811634357, 0.92, 0.7016666666666667, 0.0066298342541436465, 0.6666666666666666, 0.7062536157017784, 0.697406857934182, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9889201], dtype=float32), 1.4975439]. 
=============================================
[2019-04-03 22:38:16,953] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1594237e-29 8.0204182e-20 9.5524048e-17 1.0000000e+00 1.5952985e-21
 1.8991366e-15 1.3424693e-21], sum to 1.0000
[2019-04-03 22:38:16,953] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4205
[2019-04-03 22:38:16,968] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.033333333333333, 92.16666666666667, 0.0, 0.0, 26.0, 24.13420438404821, 0.1495293375841316, 0.0, 1.0, 41588.25712345779], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4774200.0000, 
sim time next is 4774800.0000, 
raw observation next is [-6.066666666666666, 92.33333333333334, 0.0, 0.0, 26.0, 24.09165591819002, 0.1410376505286061, 0.0, 1.0, 41648.2063664497], 
processed observation next is [0.0, 0.2608695652173913, 0.2945521698984303, 0.9233333333333335, 0.0, 0.0, 0.6666666666666666, 0.5076379931825018, 0.5470125501762021, 0.0, 1.0, 0.19832479222118907], 
reward next is 0.8017, 
noisyNet noise sample is [array([0.29520214], dtype=float32), 0.39022964]. 
=============================================
[2019-04-03 22:38:21,764] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0751486e-25 5.7737240e-18 2.5140873e-14 1.0000000e+00 1.7297425e-17
 1.6897243e-10 6.4595547e-19], sum to 1.0000
[2019-04-03 22:38:21,765] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6247
[2019-04-03 22:38:21,782] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.166666666666667, 44.16666666666667, 248.0, 378.6666666666667, 26.0, 25.09706233311238, 0.3658624461021128, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4889400.0000, 
sim time next is 4890000.0000, 
raw observation next is [2.333333333333333, 44.33333333333334, 242.0, 376.3333333333334, 26.0, 25.08358657550237, 0.3652380375677605, 0.0, 1.0, 18689.59051107672], 
processed observation next is [0.0, 0.6086956521739131, 0.5272391505078486, 0.4433333333333334, 0.8066666666666666, 0.4158379373848988, 0.6666666666666666, 0.5902988812918641, 0.6217460125225869, 0.0, 1.0, 0.08899805005274629], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.54041606], dtype=float32), 1.1504625]. 
=============================================
[2019-04-03 22:38:21,802] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[80.04872 ]
 [80.206406]
 [80.429016]
 [80.65709 ]
 [80.89655 ]], R is [[80.11327362]
 [80.31214142]
 [80.50901794]
 [80.70392609]
 [80.89688873]].
[2019-04-03 22:38:23,373] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.7302072e-26 6.0419538e-18 2.2239744e-14 1.0000000e+00 4.3938397e-19
 1.2139317e-12 6.0245053e-19], sum to 1.0000
[2019-04-03 22:38:23,374] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0807
[2019-04-03 22:38:23,405] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.166666666666667, 61.83333333333333, 0.0, 0.0, 26.0, 24.84263967778729, 0.2422687500188045, 0.0, 1.0, 39191.33812314594], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4853400.0000, 
sim time next is 4854000.0000, 
raw observation next is [-3.333333333333333, 63.66666666666667, 0.0, 0.0, 26.0, 24.81479232726386, 0.2357222423419324, 0.0, 1.0, 39200.44592258374], 
processed observation next is [0.0, 0.17391304347826086, 0.37026777469990774, 0.6366666666666667, 0.0, 0.0, 0.6666666666666666, 0.5678993606053216, 0.5785740807806441, 0.0, 1.0, 0.18666879010754162], 
reward next is 0.8133, 
noisyNet noise sample is [array([1.0565486], dtype=float32), 0.44571403]. 
=============================================
[2019-04-03 22:38:23,430] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[77.383766]
 [77.41931 ]
 [77.47543 ]
 [77.5289  ]
 [77.582924]], R is [[77.39366913]
 [77.4331131 ]
 [77.47213745]
 [77.51074219]
 [77.54892731]].
[2019-04-03 22:38:26,459] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.8094676e-30 1.5374778e-19 1.7263495e-15 1.0000000e+00 5.5949451e-21
 4.2382805e-13 3.5566484e-21], sum to 1.0000
[2019-04-03 22:38:26,460] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5291
[2019-04-03 22:38:26,472] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 35.0, 0.0, 0.0, 26.0, 25.48764462230172, 0.4747296274114809, 0.0, 1.0, 138133.5886816417], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5006400.0000, 
sim time next is 5007000.0000, 
raw observation next is [3.0, 34.5, 0.0, 0.0, 26.0, 25.45188560470893, 0.4861284426841192, 0.0, 1.0, 98192.50172383156], 
processed observation next is [1.0, 0.9565217391304348, 0.5457063711911359, 0.345, 0.0, 0.0, 0.6666666666666666, 0.6209904670590776, 0.6620428142280398, 0.0, 1.0, 0.46758334154205505], 
reward next is 0.5324, 
noisyNet noise sample is [array([1.0498997], dtype=float32), -1.5760239]. 
=============================================
[2019-04-03 22:38:26,490] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[83.72601 ]
 [83.24056 ]
 [83.030235]
 [83.30806 ]
 [83.354706]], R is [[83.64674377]
 [83.15250397]
 [82.75954437]
 [82.93195343]
 [83.10263824]].
[2019-04-03 22:38:26,963] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.5200020e-29 2.2144747e-19 6.1105227e-16 1.0000000e+00 4.0081174e-21
 1.6701506e-13 6.2200068e-21], sum to 1.0000
[2019-04-03 22:38:26,963] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7897
[2019-04-03 22:38:27,013] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.166666666666667, 46.5, 0.0, 0.0, 26.0, 24.96852274129503, 0.2926737340903257, 0.0, 1.0, 35701.47584930951], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4906200.0000, 
sim time next is 4906800.0000, 
raw observation next is [1.0, 47.0, 0.0, 0.0, 26.0, 24.97074361792596, 0.2911618238960345, 0.0, 1.0, 33056.75133544044], 
processed observation next is [0.0, 0.8260869565217391, 0.4903047091412743, 0.47, 0.0, 0.0, 0.6666666666666666, 0.5808953014938302, 0.5970539412986782, 0.0, 1.0, 0.1574131015973354], 
reward next is 0.8426, 
noisyNet noise sample is [array([0.50533026], dtype=float32), -1.0137049]. 
=============================================
[2019-04-03 22:38:31,457] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:38:31,457] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:38:31,480] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run5
[2019-04-03 22:38:31,930] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:38:31,931] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:38:31,941] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run5
[2019-04-03 22:38:32,154] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:38:32,154] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:38:32,158] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run5
[2019-04-03 22:38:33,968] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:38:33,969] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:38:33,989] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run5
[2019-04-03 22:38:36,485] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:38:36,485] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:38:36,487] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run5
[2019-04-03 22:38:36,781] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:38:36,781] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:38:36,796] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run5
[2019-04-03 22:38:36,885] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:38:36,885] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:38:36,890] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run5
[2019-04-03 22:38:38,893] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:38:38,894] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:38:38,896] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run5
[2019-04-03 22:38:39,033] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:38:39,033] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:38:39,035] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run5
[2019-04-03 22:38:39,561] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:38:39,561] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:38:39,563] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run5
[2019-04-03 22:38:40,597] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:38:40,597] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:38:40,599] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run5
[2019-04-03 22:38:40,761] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:38:40,761] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:38:40,763] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run5
[2019-04-03 22:38:40,957] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:38:40,957] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:38:40,959] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run5
[2019-04-03 22:38:41,045] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:38:41,045] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:38:41,059] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run5
[2019-04-03 22:38:43,065] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:38:43,065] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:38:43,067] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run5
[2019-04-03 22:38:43,542] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0519635e-24 3.3820128e-16 1.3340564e-13 1.0000000e+00 1.4464163e-19
 7.1203471e-13 3.0252828e-18], sum to 1.0000
[2019-04-03 22:38:43,542] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0825
[2019-04-03 22:38:43,550] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [8.100000000000001, 86.0, 88.5, 0.0, 19.0, 18.16280310554908, -1.205606516617211, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 48000.0000, 
sim time next is 48600.0000, 
raw observation next is [8.0, 86.0, 87.0, 0.0, 19.0, 18.13969536199948, -1.209009933556422, 0.0, 1.0, 18680.41167023054], 
processed observation next is [0.0, 0.5652173913043478, 0.6842105263157896, 0.86, 0.29, 0.0, 0.08333333333333333, 0.011641280166623247, 0.09699668881452601, 0.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.9457743], dtype=float32), 0.53449154]. 
=============================================
[2019-04-03 22:38:45,329] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:38:45,329] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:38:45,331] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run5
[2019-04-03 22:38:48,478] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.2029991e-11 5.6234364e-09 4.7167514e-08 9.9999976e-01 1.6394375e-09
 2.1481345e-07 3.4316812e-09], sum to 1.0000
[2019-04-03 22:38:48,479] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5473
[2019-04-03 22:38:48,491] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.4, 95.33333333333334, 0.0, 0.0, 19.0, 18.69959041274062, -1.106104896984767, 0.0, 1.0, 90374.5423478948], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1200.0000, 
sim time next is 1800.0000, 
raw observation next is [3.6, 95.5, 0.0, 0.0, 19.0, 18.69357862774905, -1.093296861457242, 0.0, 1.0, 64739.41657741964], 
processed observation next is [0.0, 0.0, 0.5623268698060943, 0.955, 0.0, 0.0, 0.08333333333333333, 0.05779821897908762, 0.13556771284758598, 0.0, 1.0, 0.30828293608295065], 
reward next is 0.6917, 
noisyNet noise sample is [array([-0.40651512], dtype=float32), 0.33687654]. 
=============================================
[2019-04-03 22:38:49,216] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.5604352e-28 6.8341890e-19 1.4692139e-15 1.0000000e+00 3.6921391e-22
 3.5222317e-14 2.2010909e-20], sum to 1.0000
[2019-04-03 22:38:49,216] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9615
[2019-04-03 22:38:49,234] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.233333333333333, 83.33333333333334, 23.33333333333333, 0.0, 19.0, 18.10743107192922, -1.215390845914321, 0.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 58800.0000, 
sim time next is 59400.0000, 
raw observation next is [6.05, 84.0, 18.0, 0.0, 19.0, 18.09589111889358, -1.212823360565247, 0.0, 1.0, 24505.09229659983], 
processed observation next is [0.0, 0.6956521739130435, 0.6301939058171746, 0.84, 0.06, 0.0, 0.08333333333333333, 0.007990926574464948, 0.095725546478251, 0.0, 1.0, 0.11669091569809442], 
reward next is 0.8833, 
noisyNet noise sample is [array([0.26545215], dtype=float32), -0.19908729]. 
=============================================
[2019-04-03 22:38:51,293] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.1918167e-19 3.5488410e-14 4.1694814e-11 1.0000000e+00 2.0904744e-14
 7.0329387e-09 1.8415150e-14], sum to 1.0000
[2019-04-03 22:38:51,305] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9377
[2019-04-03 22:38:51,318] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.466666666666667, 75.66666666666666, 0.0, 0.0, 19.0, 18.70553870019928, -1.157581621611581, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 103200.0000, 
sim time next is 103800.0000, 
raw observation next is [-4.733333333333333, 74.83333333333334, 0.0, 0.0, 19.0, 18.61253706030474, -1.163457496398047, 0.0, 1.0, 105218.0500970827], 
processed observation next is [1.0, 0.17391304347826086, 0.3314866112650046, 0.7483333333333334, 0.0, 0.0, 0.08333333333333333, 0.051044755025394885, 0.11218083453398431, 0.0, 1.0, 0.5010383337956319], 
reward next is 0.4990, 
noisyNet noise sample is [array([0.6440058], dtype=float32), 1.2369432]. 
=============================================
[2019-04-03 22:38:56,019] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.5286529e-19 9.4170062e-12 3.4040923e-10 2.9055548e-01 1.7766478e-11
 7.0944452e-01 7.3647055e-14], sum to 1.0000
[2019-04-03 22:38:56,019] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4579
[2019-04-03 22:38:56,112] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.700000000000001, 61.0, 146.5, 169.0, 26.0, 24.53107968086193, 0.06075003949760924, 1.0, 1.0, 83310.5665070343], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 138000.0000, 
sim time next is 138600.0000, 
raw observation next is [-6.7, 61.0, 148.0, 106.0, 26.0, 24.78676601621358, 0.0911991689912332, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.2770083102493075, 0.61, 0.49333333333333335, 0.11712707182320442, 0.6666666666666666, 0.565563834684465, 0.5303997229970777, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4815004], dtype=float32), 0.2688832]. 
=============================================
[2019-04-03 22:38:56,361] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.5988989e-22 8.9957633e-12 2.9198075e-11 9.9396402e-01 4.4779621e-15
 6.0359621e-03 2.0611377e-16], sum to 1.0000
[2019-04-03 22:38:56,361] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6560
[2019-04-03 22:38:56,423] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.7, 63.5, 49.66666666666667, 31.0, 25.0, 24.00169828362197, -0.02964809047561351, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 143400.0000, 
sim time next is 144000.0000, 
raw observation next is [-6.7, 64.0, 44.0, 24.0, 25.0, 24.33907164804834, -0.0006436305895128706, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.2770083102493075, 0.64, 0.14666666666666667, 0.026519337016574586, 0.5833333333333334, 0.528255970670695, 0.4997854564701624, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.290521], dtype=float32), -1.9576154]. 
=============================================
[2019-04-03 22:38:56,427] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[80.188835]
 [78.76201 ]
 [76.14372 ]
 [73.912224]
 [71.58141 ]], R is [[81.64266205]
 [81.82623291]
 [81.03973389]
 [80.26817322]
 [79.51072693]].
[2019-04-03 22:38:58,459] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.5321910e-27 6.1274839e-18 3.7078018e-16 9.9999964e-01 8.0899562e-19
 3.5984738e-07 6.3173082e-20], sum to 1.0000
[2019-04-03 22:38:58,459] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7898
[2019-04-03 22:38:58,488] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.3946273436661, -0.3717219154498438, 0.0, 1.0, 44791.59532069331], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 185400.0000, 
sim time next is 186000.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.32130141683499, -0.3855446037210558, 0.0, 1.0, 44839.13826061658], 
processed observation next is [1.0, 0.13043478260869565, 0.21606648199445982, 0.78, 0.0, 0.0, 0.6666666666666666, 0.360108451402916, 0.37148513209298145, 0.0, 1.0, 0.2135197060029361], 
reward next is 0.7865, 
noisyNet noise sample is [array([-0.03085002], dtype=float32), -0.3601681]. 
=============================================
[2019-04-03 22:38:58,524] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[90.88775 ]
 [90.82867 ]
 [90.743065]
 [90.421295]
 [90.12301 ]], R is [[90.80846405]
 [90.68708801]
 [90.56708527]
 [90.44850159]
 [90.33139038]].
[2019-04-03 22:39:04,190] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0568748e-21 2.7494191e-14 3.5920834e-12 9.5579362e-01 1.1142270e-14
 4.4206362e-02 1.0171382e-15], sum to 1.0000
[2019-04-03 22:39:04,190] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0827
[2019-04-03 22:39:04,278] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.116666666666666, 74.5, 109.0, 0.0, 26.0, 25.23195931975694, 0.1384664807783725, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 209400.0000, 
sim time next is 210000.0000, 
raw observation next is [-6.933333333333334, 74.0, 116.5, 0.0, 26.0, 25.21473017345436, 0.1282427953108679, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.270544783010157, 0.74, 0.3883333333333333, 0.0, 0.6666666666666666, 0.6012275144545299, 0.542747598436956, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.3776963], dtype=float32), 0.9631029]. 
=============================================
[2019-04-03 22:39:04,282] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[79.4594 ]
 [79.60277]
 [79.86413]
 [80.05373]
 [80.47092]], R is [[79.54271698]
 [79.74729156]
 [79.94982147]
 [80.15032196]
 [80.34881592]].
[2019-04-03 22:39:08,144] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.4520392e-23 9.4367141e-14 2.5762167e-13 9.9990594e-01 3.6994299e-16
 9.4065763e-05 1.8145014e-16], sum to 1.0000
[2019-04-03 22:39:08,145] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0681
[2019-04-03 22:39:08,220] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 26.0, 24.67823501694714, 0.2480833469084845, 1.0, 1.0, 174162.1214051498], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 237600.0000, 
sim time next is 238200.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 26.0, 25.06294208885637, 0.3098030514399574, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.368421052631579, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5885785074046975, 0.6032676838133192, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.36852694], dtype=float32), 1.1770533]. 
=============================================
[2019-04-03 22:39:11,590] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.6186241e-20 1.6166488e-13 8.9820047e-11 9.9998868e-01 6.7960393e-15
 1.1306737e-05 2.3110102e-15], sum to 1.0000
[2019-04-03 22:39:11,590] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1727
[2019-04-03 22:39:11,623] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.9, 68.33333333333334, 0.0, 0.0, 26.0, 23.81504663752546, -0.02389029999115069, 0.0, 1.0, 45373.36173240308], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 264000.0000, 
sim time next is 264600.0000, 
raw observation next is [-7.0, 69.0, 0.0, 0.0, 26.0, 23.74867681074454, -0.0291598069048134, 0.0, 1.0, 45516.06909140416], 
processed observation next is [1.0, 0.043478260869565216, 0.2686980609418283, 0.69, 0.0, 0.0, 0.6666666666666666, 0.47905640089537843, 0.4902800643650622, 0.0, 1.0, 0.21674318614954363], 
reward next is 0.7833, 
noisyNet noise sample is [array([-0.10045591], dtype=float32), -0.5069225]. 
=============================================
[2019-04-03 22:39:17,728] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.6013212e-22 1.4777223e-14 5.3224430e-12 9.9999475e-01 2.5592810e-15
 5.2465221e-06 3.9753075e-16], sum to 1.0000
[2019-04-03 22:39:17,743] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5144
[2019-04-03 22:39:17,775] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-14.3, 68.0, 0.0, 0.0, 26.0, 23.22099284053636, -0.1310931956878553, 0.0, 1.0, 47649.00693669276], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 348000.0000, 
sim time next is 348600.0000, 
raw observation next is [-14.4, 68.5, 0.0, 0.0, 26.0, 23.19825233626228, -0.1401318659724934, 0.0, 1.0, 47699.44528340721], 
processed observation next is [1.0, 0.0, 0.0637119113573407, 0.685, 0.0, 0.0, 0.6666666666666666, 0.4331876946885232, 0.45328937800916885, 0.0, 1.0, 0.22714021563527242], 
reward next is 0.7729, 
noisyNet noise sample is [array([0.73290205], dtype=float32), 0.10590577]. 
=============================================
[2019-04-03 22:39:34,279] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.4582779e-33 1.0199302e-21 7.1776494e-19 1.0000000e+00 4.4404497e-25
 1.2125364e-12 3.5446696e-24], sum to 1.0000
[2019-04-03 22:39:34,286] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1506
[2019-04-03 22:39:34,310] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.783333333333333, 95.33333333333333, 0.0, 0.0, 26.0, 24.83877007456969, 0.2242059232675625, 0.0, 1.0, 42150.74192791605], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 508200.0000, 
sim time next is 508800.0000, 
raw observation next is [1.966666666666667, 94.66666666666666, 0.0, 0.0, 26.0, 24.8305119814289, 0.2225151841040347, 0.0, 1.0, 41591.94716732633], 
processed observation next is [1.0, 0.9130434782608695, 0.5170821791320407, 0.9466666666666665, 0.0, 0.0, 0.6666666666666666, 0.5692093317857415, 0.5741717280346782, 0.0, 1.0, 0.19805689127298254], 
reward next is 0.8019, 
noisyNet noise sample is [array([-0.28406724], dtype=float32), 0.87665284]. 
=============================================
[2019-04-03 22:39:43,607] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.7758487e-28 4.7265985e-18 1.1332176e-15 1.0000000e+00 3.5487493e-20
 5.2923367e-11 1.8134297e-20], sum to 1.0000
[2019-04-03 22:39:43,610] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5633
[2019-04-03 22:39:43,664] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.8, 87.0, 0.0, 0.0, 26.0, 24.93692411629354, 0.279626652908787, 0.0, 1.0, 46124.75625581931], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 587400.0000, 
sim time next is 588000.0000, 
raw observation next is [-2.8, 87.0, 0.0, 0.0, 26.0, 24.9646483493171, 0.2870491075064719, 0.0, 1.0, 124012.5975553091], 
processed observation next is [0.0, 0.8260869565217391, 0.38504155124653744, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5803873624430915, 0.5956830358354906, 0.0, 1.0, 0.5905361788348052], 
reward next is 0.4095, 
noisyNet noise sample is [array([-1.1283033], dtype=float32), 1.1364727]. 
=============================================
[2019-04-03 22:39:43,667] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[78.30784 ]
 [78.3611  ]
 [78.415794]
 [78.45112 ]
 [78.55515 ]], R is [[78.09964752]
 [78.09900665]
 [78.05374908]
 [77.99729156]
 [77.97625732]].
[2019-04-03 22:39:43,983] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.0490798e-28 1.6446663e-19 1.5842531e-16 1.0000000e+00 5.1911650e-21
 3.2442211e-09 1.2143198e-21], sum to 1.0000
[2019-04-03 22:39:43,984] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2523
[2019-04-03 22:39:44,025] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.516666666666667, 67.66666666666667, 126.3333333333333, 61.66666666666666, 26.0, 25.91267634812948, 0.3235465780802169, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 727800.0000, 
sim time next is 728400.0000, 
raw observation next is [-1.333333333333333, 67.33333333333334, 132.6666666666667, 64.83333333333334, 26.0, 25.85759925325688, 0.3228313743149845, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.42566943674976926, 0.6733333333333335, 0.4422222222222224, 0.07163904235727442, 0.6666666666666666, 0.6547999377714065, 0.6076104581049948, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.0111196], dtype=float32), -0.1359199]. 
=============================================
[2019-04-03 22:39:44,518] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5803775e-24 1.8569618e-16 5.3704108e-15 1.0000000e+00 3.5319234e-19
 4.4290247e-09 1.9194542e-18], sum to 1.0000
[2019-04-03 22:39:44,519] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8463
[2019-04-03 22:39:44,541] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.65, 70.0, 0.0, 0.0, 26.0, 24.67206221725558, 0.1495234284465078, 0.0, 1.0, 41795.74568487392], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 685800.0000, 
sim time next is 686400.0000, 
raw observation next is [-3.733333333333333, 70.33333333333333, 0.0, 0.0, 26.0, 24.67863897517698, 0.1443381193880831, 0.0, 1.0, 41715.57567237774], 
processed observation next is [0.0, 0.9565217391304348, 0.35918744228993543, 0.7033333333333333, 0.0, 0.0, 0.6666666666666666, 0.556553247931415, 0.5481127064626944, 0.0, 1.0, 0.198645598439894], 
reward next is 0.8014, 
noisyNet noise sample is [array([-1.4946232], dtype=float32), 1.2297708]. 
=============================================
[2019-04-03 22:39:57,219] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.1139065e-27 7.8959609e-18 3.0629457e-15 1.0000000e+00 3.2560433e-20
 2.6905095e-08 4.8517309e-20], sum to 1.0000
[2019-04-03 22:39:57,221] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5440
[2019-04-03 22:39:57,316] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.916666666666667, 74.33333333333333, 78.33333333333334, 0.0, 26.0, 25.7005611851027, 0.2898529093313969, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 814200.0000, 
sim time next is 814800.0000, 
raw observation next is [-5.633333333333334, 73.66666666666667, 82.66666666666666, 0.0, 26.0, 25.64976262520116, 0.2879480753041654, 1.0, 1.0, 44107.12059276935], 
processed observation next is [1.0, 0.43478260869565216, 0.30655586334256696, 0.7366666666666667, 0.2755555555555555, 0.0, 0.6666666666666666, 0.6374802187667633, 0.5959826917680552, 1.0, 1.0, 0.21003390758461596], 
reward next is 0.7900, 
noisyNet noise sample is [array([0.62461996], dtype=float32), -1.3960743]. 
=============================================
[2019-04-03 22:40:02,443] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.0285771e-26 4.5257665e-18 3.6508221e-15 1.0000000e+00 5.1009504e-19
 3.8718664e-08 1.9409533e-19], sum to 1.0000
[2019-04-03 22:40:02,443] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0487
[2019-04-03 22:40:02,514] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 75.0, 99.0, 0.0, 26.0, 25.50687355590981, 0.2900225707319784, 1.0, 1.0, 27790.7972225566], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 822600.0000, 
sim time next is 823200.0000, 
raw observation next is [-4.5, 76.33333333333333, 97.66666666666666, 0.0, 26.0, 25.428720771584, 0.2867311260333665, 1.0, 1.0, 27881.13490143905], 
processed observation next is [1.0, 0.5217391304347826, 0.3379501385041552, 0.7633333333333333, 0.32555555555555554, 0.0, 0.6666666666666666, 0.6190600642986667, 0.5955770420111222, 1.0, 1.0, 0.13276730905447168], 
reward next is 0.8672, 
noisyNet noise sample is [array([1.353993], dtype=float32), -1.2341284]. 
=============================================
[2019-04-03 22:40:03,420] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.1922424e-32 1.8244275e-20 1.2856850e-18 1.0000000e+00 1.1842448e-23
 1.9440745e-10 3.7359849e-24], sum to 1.0000
[2019-04-03 22:40:03,422] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6318
[2019-04-03 22:40:03,470] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [12.28333333333333, 86.0, 125.3333333333333, 0.0, 26.0, 26.73732321615047, 0.5714372229190067, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 994200.0000, 
sim time next is 994800.0000, 
raw observation next is [12.36666666666667, 86.0, 126.6666666666667, 0.0, 26.0, 25.77773553062474, 0.5784693669379505, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.8051708217913206, 0.86, 0.42222222222222233, 0.0, 0.6666666666666666, 0.6481446275520616, 0.6928231223126501, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6066203], dtype=float32), -1.5133151]. 
=============================================
[2019-04-03 22:40:14,177] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.7452840e-26 1.8073483e-17 1.8208652e-15 1.0000000e+00 1.7408476e-20
 1.1704139e-09 3.1014452e-19], sum to 1.0000
[2019-04-03 22:40:14,177] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5116
[2019-04-03 22:40:14,182] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [15.18333333333333, 56.33333333333333, 0.0, 0.0, 26.0, 26.48958882970687, 0.7672880619246326, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1104600.0000, 
sim time next is 1105200.0000, 
raw observation next is [15.0, 57.0, 0.0, 0.0, 26.0, 26.39774794761123, 0.7642225186729116, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.8781163434903049, 0.57, 0.0, 0.0, 0.6666666666666666, 0.6998123289676025, 0.7547408395576372, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.19066842], dtype=float32), -2.174967]. 
=============================================
[2019-04-03 22:40:17,230] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.8109912e-33 4.3106855e-24 9.9116801e-20 1.0000000e+00 3.7824860e-26
 1.1013400e-14 4.4012241e-25], sum to 1.0000
[2019-04-03 22:40:17,232] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2422
[2019-04-03 22:40:17,257] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [13.63333333333333, 81.0, 26.0, 0.1666666666666666, 26.0, 25.66046962917044, 0.6096030320953489, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1153200.0000, 
sim time next is 1153800.0000, 
raw observation next is [14.1, 79.5, 31.0, 0.0, 26.0, 25.65482285862309, 0.6262480164015342, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.8531855955678671, 0.795, 0.10333333333333333, 0.0, 0.6666666666666666, 0.6379019048852573, 0.7087493388005114, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0283725], dtype=float32), 0.57396036]. 
=============================================
[2019-04-03 22:40:23,117] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.7418597e-31 2.3842891e-21 7.8125146e-19 1.0000000e+00 8.6147792e-25
 2.4803264e-11 9.7795331e-24], sum to 1.0000
[2019-04-03 22:40:23,119] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0272
[2019-04-03 22:40:23,129] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [14.0, 98.66666666666666, 100.0, 0.0, 26.0, 25.00157805506298, 0.4785204946282536, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1255200.0000, 
sim time next is 1255800.0000, 
raw observation next is [13.9, 99.33333333333334, 99.0, 0.0, 26.0, 24.96958191606826, 0.4740753900667249, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.847645429362881, 0.9933333333333334, 0.33, 0.0, 0.6666666666666666, 0.5807984930056884, 0.6580251300222416, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5978577], dtype=float32), 1.3314947]. 
=============================================
[2019-04-03 22:40:26,800] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4686396e-26 4.4985381e-17 4.5172695e-15 9.9999988e-01 2.5639193e-20
 7.5753107e-08 5.3272419e-20], sum to 1.0000
[2019-04-03 22:40:26,800] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6523
[2019-04-03 22:40:26,814] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 95.0, 67.66666666666667, 0.0, 26.0, 25.99214656936732, 0.5070150491534328, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1419600.0000, 
sim time next is 1420200.0000, 
raw observation next is [0.0, 95.0, 72.0, 0.0, 26.0, 25.94461186296463, 0.5007392618595353, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.46260387811634357, 0.95, 0.24, 0.0, 0.6666666666666666, 0.6620509885803859, 0.6669130872865118, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.40135777], dtype=float32), 0.64321893]. 
=============================================
[2019-04-03 22:40:39,537] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1623704e-26 5.5960821e-19 9.7882323e-16 1.0000000e+00 8.1906623e-19
 3.5420233e-11 6.1740813e-19], sum to 1.0000
[2019-04-03 22:40:39,537] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2955
[2019-04-03 22:40:39,549] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.2, 94.66666666666667, 0.0, 0.0, 26.0, 25.28076843055315, 0.4677264320497178, 0.0, 1.0, 44039.32004191025], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1480800.0000, 
sim time next is 1481400.0000, 
raw observation next is [2.2, 95.0, 0.0, 0.0, 26.0, 25.39706354767142, 0.4675321647058615, 0.0, 1.0, 25372.60414044195], 
processed observation next is [1.0, 0.13043478260869565, 0.5235457063711911, 0.95, 0.0, 0.0, 0.6666666666666666, 0.6164219623059516, 0.6558440549019539, 0.0, 1.0, 0.120821924478295], 
reward next is 0.8792, 
noisyNet noise sample is [array([-0.35090217], dtype=float32), -0.03752158]. 
=============================================
[2019-04-03 22:40:54,898] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.9089982e-27 1.2579591e-18 8.2799879e-16 1.0000000e+00 2.8969681e-20
 1.0760191e-09 1.2096053e-19], sum to 1.0000
[2019-04-03 22:40:54,899] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0988
[2019-04-03 22:40:54,939] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 88.0, 0.0, 0.0, 26.0, 25.01811397073084, 0.4381441953792087, 0.0, 1.0, 26699.47565576638], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1713600.0000, 
sim time next is 1714200.0000, 
raw observation next is [1.0, 88.66666666666667, 0.0, 0.0, 26.0, 24.94740115376582, 0.4343009308013339, 0.0, 1.0, 69324.8448212488], 
processed observation next is [1.0, 0.8695652173913043, 0.4903047091412743, 0.8866666666666667, 0.0, 0.0, 0.6666666666666666, 0.5789500961471518, 0.6447669769337779, 0.0, 1.0, 0.3301183086726133], 
reward next is 0.6699, 
noisyNet noise sample is [array([-0.10680385], dtype=float32), 0.36165965]. 
=============================================
[2019-04-03 22:41:01,149] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6699827e-23 7.1609809e-15 2.0834201e-11 1.0000000e+00 1.1732991e-17
 1.9832784e-08 2.3276586e-17], sum to 1.0000
[2019-04-03 22:41:01,149] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2851
[2019-04-03 22:41:01,194] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 88.0, 0.0, 0.0, 26.0, 25.08335039608588, 0.4773127051318155, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1711200.0000, 
sim time next is 1711800.0000, 
raw observation next is [1.1, 88.0, 0.0, 0.0, 26.0, 25.13621639966397, 0.47223781713526, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.49307479224376743, 0.88, 0.0, 0.0, 0.6666666666666666, 0.5946846999719974, 0.6574126057117533, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.00750216], dtype=float32), 0.45779625]. 
=============================================
[2019-04-03 22:41:12,050] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.9864233e-24 5.3990807e-17 8.1226658e-14 1.0000000e+00 2.8014736e-19
 4.9111193e-10 1.8120278e-18], sum to 1.0000
[2019-04-03 22:41:12,051] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0469
[2019-04-03 22:41:12,083] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.100000000000001, 78.83333333333334, 0.0, 0.0, 26.0, 24.50234516792595, 0.1857213187043273, 0.0, 1.0, 45590.39129024601], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1815000.0000, 
sim time next is 1815600.0000, 
raw observation next is [-5.2, 78.66666666666667, 0.0, 0.0, 26.0, 24.47233176743466, 0.1788063224992393, 0.0, 1.0, 45647.24732417997], 
processed observation next is [0.0, 0.0, 0.31855955678670367, 0.7866666666666667, 0.0, 0.0, 0.6666666666666666, 0.5393609806195551, 0.5596021074997465, 0.0, 1.0, 0.217367844400857], 
reward next is 0.7826, 
noisyNet noise sample is [array([-0.768207], dtype=float32), 1.419728]. 
=============================================
[2019-04-03 22:41:19,425] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1937383e-22 2.0685434e-15 3.7117567e-13 9.9999988e-01 2.3730073e-16
 1.7193013e-07 6.0280741e-17], sum to 1.0000
[2019-04-03 22:41:19,425] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8301
[2019-04-03 22:41:19,524] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 77.0, 186.0, 84.0, 26.0, 25.02390830763958, 0.2848511118981354, 0.0, 1.0, 45305.79505239567], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1866600.0000, 
sim time next is 1867200.0000, 
raw observation next is [-4.5, 79.0, 167.0, 70.0, 26.0, 25.02573709589707, 0.2847454880482947, 0.0, 1.0, 43143.58934885474], 
processed observation next is [0.0, 0.6086956521739131, 0.3379501385041552, 0.79, 0.5566666666666666, 0.07734806629834254, 0.6666666666666666, 0.5854780913247559, 0.5949151626827649, 0.0, 1.0, 0.20544566356597493], 
reward next is 0.7946, 
noisyNet noise sample is [array([-0.54003066], dtype=float32), -0.2472174]. 
=============================================
[2019-04-03 22:41:25,803] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.3138450e-27 1.2577672e-18 5.5544905e-16 1.0000000e+00 6.2727618e-21
 2.6306000e-09 1.6764894e-20], sum to 1.0000
[2019-04-03 22:41:25,804] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3349
[2019-04-03 22:41:25,906] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.96404485164885, 0.3667516443947216, 0.0, 1.0, 55084.00507041119], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1972800.0000, 
sim time next is 1973400.0000, 
raw observation next is [-5.600000000000001, 82.16666666666667, 0.0, 0.0, 26.0, 25.16429748862451, 0.381251409777945, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.3074792243767313, 0.8216666666666668, 0.0, 0.0, 0.6666666666666666, 0.5970247907187091, 0.627083803259315, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2542285], dtype=float32), -1.2103591]. 
=============================================
[2019-04-03 22:41:42,635] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-03 22:41:42,636] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-03 22:41:42,636] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:41:42,636] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-03 22:41:42,637] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:41:42,645] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-03 22:41:42,646] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:41:42,647] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run7
[2019-04-03 22:41:42,681] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run7
[2019-04-03 22:41:42,742] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run7
[2019-04-03 22:44:17,534] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7267.6968 188917510.1569 -443.5460
[2019-04-03 22:44:27,599] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.73323405], dtype=float32), 0.25206852]
[2019-04-03 22:44:27,599] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [0.79469208, 71.93855661, 0.0, 0.0, 25.0, 24.13833095524847, 0.1879957358938009, 0.0, 1.0, 0.0]
[2019-04-03 22:44:27,600] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-03 22:44:27,600] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [4.9445825e-24 4.4659198e-15 1.7273986e-13 1.0000000e+00 4.5355466e-19
 6.9893171e-09 4.8811411e-18], sampled 0.5303608550016032
[2019-04-03 22:44:44,610] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7407.7463 223362470.3656 -399.9828
[2019-04-03 22:45:17,797] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7227.1198 261048080.5577 579.7060
[2019-04-03 22:45:18,846] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 600000, evaluation results [600000.0, 7407.746345491542, 223362470.3655589, -399.9828152270937, 7267.696820125497, 188917510.1569302, -443.5459585406526, 7227.119769459546, 261048080.55773956, 579.706023077094]
[2019-04-03 22:45:20,532] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.8853568e-30 1.3261256e-21 9.9133543e-18 1.0000000e+00 2.0127487e-22
 1.8290087e-13 4.7262623e-23], sum to 1.0000
[2019-04-03 22:45:20,533] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7482
[2019-04-03 22:45:20,637] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.6, 75.0, 15.0, 87.33333333333331, 26.0, 25.22017748895677, 0.3099482685266184, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2188200.0000, 
sim time next is 2188800.0000, 
raw observation next is [-5.6, 75.0, 21.5, 131.0, 26.0, 25.49588567166465, 0.3471356930734757, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.30747922437673136, 0.75, 0.07166666666666667, 0.14475138121546963, 0.6666666666666666, 0.6246571393053874, 0.6157118976911585, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6288556], dtype=float32), -0.37251762]. 
=============================================
[2019-04-03 22:45:23,562] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.1808866e-27 2.9075316e-19 2.3832405e-16 1.0000000e+00 2.5646631e-20
 3.3116829e-11 2.9838243e-20], sum to 1.0000
[2019-04-03 22:45:23,562] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0220
[2019-04-03 22:45:23,613] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.8, 82.0, 123.0, 77.5, 26.0, 25.48138615200819, 0.3384670860147466, 1.0, 1.0, 18741.53088383785], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2106000.0000, 
sim time next is 2106600.0000, 
raw observation next is [-7.8, 82.00000000000001, 140.0, 91.0, 26.0, 25.56952479364689, 0.3455196559705467, 1.0, 1.0, 18737.94707287288], 
processed observation next is [1.0, 0.391304347826087, 0.24653739612188366, 0.8200000000000002, 0.4666666666666667, 0.1005524861878453, 0.6666666666666666, 0.6307937328039076, 0.6151732186568489, 1.0, 1.0, 0.08922831939463277], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.85034204], dtype=float32), -0.6720641]. 
=============================================
[2019-04-03 22:45:28,686] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.8146343e-27 2.2585797e-21 2.2207550e-16 1.0000000e+00 3.0147153e-20
 2.8278659e-11 7.1168423e-21], sum to 1.0000
[2019-04-03 22:45:28,687] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3530
[2019-04-03 22:45:28,744] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.199999999999999, 77.66666666666667, 0.0, 0.0, 26.0, 23.87998084915323, 0.02471747262777029, 0.0, 1.0, 41905.91843185117], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2180400.0000, 
sim time next is 2181000.0000, 
raw observation next is [-6.2, 78.33333333333334, 0.0, 0.0, 26.0, 23.83145146150664, 0.01227527021832714, 0.0, 1.0, 41896.12780058199], 
processed observation next is [1.0, 0.21739130434782608, 0.2908587257617729, 0.7833333333333334, 0.0, 0.0, 0.6666666666666666, 0.48595428845888655, 0.5040917567394424, 0.0, 1.0, 0.19950537047896186], 
reward next is 0.8005, 
noisyNet noise sample is [array([0.74393755], dtype=float32), -0.8833738]. 
=============================================
[2019-04-03 22:45:28,762] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[82.49252 ]
 [82.438965]
 [82.40408 ]
 [82.398926]
 [82.39921 ]], R is [[82.50507355]
 [82.48046875]
 [82.45594788]
 [82.43160248]
 [82.4074707 ]].
[2019-04-03 22:45:29,205] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.8728137e-27 2.1548006e-19 2.7077449e-16 1.0000000e+00 5.0600572e-19
 4.1591085e-11 1.7896518e-20], sum to 1.0000
[2019-04-03 22:45:29,209] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2284
[2019-04-03 22:45:29,254] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.833333333333334, 71.0, 114.5, 75.16666666666664, 26.0, 25.99377385663495, 0.3981764884641075, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2197200.0000, 
sim time next is 2197800.0000, 
raw observation next is [-4.75, 71.0, 117.0, 0.0, 26.0, 25.96452967655491, 0.3853483414297085, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.3310249307479225, 0.71, 0.39, 0.0, 0.6666666666666666, 0.6637108063795759, 0.6284494471432361, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.52713597], dtype=float32), 0.094755985]. 
=============================================
[2019-04-03 22:45:39,941] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.7620702e-24 6.5920704e-16 1.0958907e-12 9.9999988e-01 2.3178850e-17
 7.3999701e-08 1.7753953e-17], sum to 1.0000
[2019-04-03 22:45:39,941] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6889
[2019-04-03 22:45:39,954] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.3, 46.5, 41.0, 0.0, 26.0, 25.88776368137875, 0.3947350912910308, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2305800.0000, 
sim time next is 2306400.0000, 
raw observation next is [-0.4, 47.33333333333333, 35.00000000000001, 0.0, 26.0, 25.81877906675232, 0.3865886356230021, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.45152354570637127, 0.4733333333333333, 0.1166666666666667, 0.0, 0.6666666666666666, 0.65156492222936, 0.6288628785410008, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1876587], dtype=float32), 0.9357427]. 
=============================================
[2019-04-03 22:45:46,692] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.6421429e-26 7.0073721e-18 4.7836537e-15 1.0000000e+00 6.1874939e-20
 3.6246837e-12 1.8592555e-19], sum to 1.0000
[2019-04-03 22:45:46,692] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3941
[2019-04-03 22:45:46,715] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 42.0, 0.0, 0.0, 26.0, 25.15855606131137, 0.2810488123752466, 0.0, 1.0, 43113.64613146947], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2406600.0000, 
sim time next is 2407200.0000, 
raw observation next is [-3.4, 42.0, 0.0, 0.0, 26.0, 25.13559495472015, 0.2756497768725827, 0.0, 1.0, 43156.72813957219], 
processed observation next is [0.0, 0.8695652173913043, 0.368421052631579, 0.42, 0.0, 0.0, 0.6666666666666666, 0.594632912893346, 0.5918832589575276, 0.0, 1.0, 0.20550822923605805], 
reward next is 0.7945, 
noisyNet noise sample is [array([0.44962212], dtype=float32), -0.5034757]. 
=============================================
[2019-04-03 22:45:58,584] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.9342215e-31 3.8832429e-22 2.3450175e-19 1.0000000e+00 2.2310395e-22
 2.0627697e-13 6.7168862e-23], sum to 1.0000
[2019-04-03 22:45:58,585] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9391
[2019-04-03 22:45:58,689] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.1, 59.0, 0.0, 0.0, 26.0, 24.99521964086517, 0.3759343569657551, 1.0, 1.0, 91820.73423881375], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2659800.0000, 
sim time next is 2660400.0000, 
raw observation next is [-1.2, 60.0, 0.0, 0.0, 26.0, 25.06536919033127, 0.3934296300442569, 1.0, 1.0, 20917.24031758263], 
processed observation next is [1.0, 0.8260869565217391, 0.42936288088642666, 0.6, 0.0, 0.0, 0.6666666666666666, 0.5887807658609393, 0.6311432100147524, 1.0, 1.0, 0.099605906274203], 
reward next is 0.9004, 
noisyNet noise sample is [array([0.48277196], dtype=float32), 0.96200264]. 
=============================================
[2019-04-03 22:46:01,670] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.0547892e-27 5.7907464e-20 5.6154464e-15 1.0000000e+00 3.4135393e-21
 3.7805583e-12 5.1913831e-21], sum to 1.0000
[2019-04-03 22:46:01,670] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9175
[2019-04-03 22:46:01,693] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.8, 56.0, 0.0, 0.0, 26.0, 25.39079342547323, 0.4117009993184244, 0.0, 1.0, 52407.87137001048], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2584200.0000, 
sim time next is 2584800.0000, 
raw observation next is [-2.8, 56.0, 0.0, 0.0, 26.0, 25.37306867689582, 0.4120620938140023, 0.0, 1.0, 55626.4299018372], 
processed observation next is [1.0, 0.9565217391304348, 0.38504155124653744, 0.56, 0.0, 0.0, 0.6666666666666666, 0.6144223897413182, 0.637354031271334, 0.0, 1.0, 0.26488776143731996], 
reward next is 0.7351, 
noisyNet noise sample is [array([1.8427126], dtype=float32), -1.0012085]. 
=============================================
[2019-04-03 22:46:07,221] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.4176070e-27 1.1007114e-19 6.4669859e-16 1.0000000e+00 5.4608816e-20
 2.2327504e-11 1.5610990e-20], sum to 1.0000
[2019-04-03 22:46:07,223] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9572
[2019-04-03 22:46:07,302] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.0, 77.0, 77.0, 0.0, 26.0, 25.49822152855774, 0.3345487626891415, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2622600.0000, 
sim time next is 2623200.0000, 
raw observation next is [-6.9, 76.33333333333334, 79.66666666666667, 15.16666666666666, 26.0, 25.70660453319153, 0.3578746380684418, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.27146814404432135, 0.7633333333333334, 0.26555555555555554, 0.01675874769797421, 0.6666666666666666, 0.6422170444326275, 0.619291546022814, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.19647858], dtype=float32), -0.5796803]. 
=============================================
[2019-04-03 22:46:23,064] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.4191358e-30 7.8119107e-21 6.1850961e-17 1.0000000e+00 2.4442960e-22
 2.7165548e-12 1.5726212e-22], sum to 1.0000
[2019-04-03 22:46:23,069] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4771
[2019-04-03 22:46:23,081] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.8333333333333334, 94.16666666666666, 81.0, 0.0, 26.0, 25.44226683960881, 0.3163067964939031, 1.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2890200.0000, 
sim time next is 2890800.0000, 
raw observation next is [1.0, 93.0, 82.5, 0.0, 26.0, 25.45217930769072, 0.3188777745786944, 1.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.4782608695652174, 0.4903047091412743, 0.93, 0.275, 0.0, 0.6666666666666666, 0.62101494230756, 0.6062925915262315, 1.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.56728226], dtype=float32), -0.39153913]. 
=============================================
[2019-04-03 22:46:42,297] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.3198282e-24 1.7970702e-17 7.3557900e-14 1.0000000e+00 1.2320390e-17
 1.8015622e-10 1.7789004e-17], sum to 1.0000
[2019-04-03 22:46:42,307] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2229
[2019-04-03 22:46:42,322] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.0, 72.33333333333333, 0.0, 0.0, 26.0, 25.08108932034083, 0.3948317804955857, 0.0, 1.0, 43687.83562084782], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3286200.0000, 
sim time next is 3286800.0000, 
raw observation next is [-7.0, 70.0, 0.0, 0.0, 26.0, 25.05068873116273, 0.3885367546865912, 0.0, 1.0, 43721.30709618969], 
processed observation next is [1.0, 0.043478260869565216, 0.2686980609418283, 0.7, 0.0, 0.0, 0.6666666666666666, 0.5875573942635608, 0.629512251562197, 0.0, 1.0, 0.20819670045804614], 
reward next is 0.7918, 
noisyNet noise sample is [array([-0.01279185], dtype=float32), -0.08994262]. 
=============================================
[2019-04-03 22:46:43,856] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.4493820e-27 3.5832176e-21 1.4560020e-15 1.0000000e+00 6.3112370e-21
 2.3473753e-12 2.0698408e-20], sum to 1.0000
[2019-04-03 22:46:43,859] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1617
[2019-04-03 22:46:43,886] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.52485698697152, 0.511575992777305, 0.0, 1.0, 39671.75423189538], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3210600.0000, 
sim time next is 3211200.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.40552046114943, 0.5056376420516978, 0.0, 1.0, 102146.7241687679], 
processed observation next is [1.0, 0.17391304347826086, 0.4349030470914128, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6171267050957857, 0.6685458806838992, 0.0, 1.0, 0.4864129722322281], 
reward next is 0.5136, 
noisyNet noise sample is [array([-0.08582986], dtype=float32), -0.76192933]. 
=============================================
[2019-04-03 22:46:45,265] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.3229021e-25 1.4067873e-15 3.1295143e-13 1.0000000e+00 1.3325527e-18
 5.0902877e-08 2.3589524e-18], sum to 1.0000
[2019-04-03 22:46:45,265] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9462
[2019-04-03 22:46:45,281] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.0, 100.0, 33.0, 307.5, 26.0, 27.61510623711728, 0.9759589058796937, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3171600.0000, 
sim time next is 3172200.0000, 
raw observation next is [6.0, 100.0, 24.66666666666666, 243.6666666666666, 26.0, 27.23700807280459, 0.9312907123461408, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.6288088642659281, 1.0, 0.0822222222222222, 0.269244935543278, 0.6666666666666666, 0.7697506727337157, 0.8104302374487137, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7802497], dtype=float32), 0.036244713]. 
=============================================
[2019-04-03 22:46:50,695] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.0476153e-28 1.1991298e-20 1.7335740e-17 1.0000000e+00 3.2218310e-20
 1.0007729e-11 2.1802649e-21], sum to 1.0000
[2019-04-03 22:46:50,697] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4344
[2019-04-03 22:46:50,742] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 92.0, 93.0, 511.5, 26.0, 26.00458857135601, 0.6064690587409097, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3229200.0000, 
sim time next is 3229800.0000, 
raw observation next is [-3.0, 92.0, 95.66666666666667, 558.6666666666666, 26.0, 26.0602539483048, 0.615299687859521, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.3795013850415513, 0.92, 0.3188888888888889, 0.6173112338858194, 0.6666666666666666, 0.6716878290254001, 0.7050998959531737, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6433049], dtype=float32), 1.0725662]. 
=============================================
[2019-04-03 22:46:52,567] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.1560178e-27 6.8427683e-20 2.2667928e-17 1.0000000e+00 3.7550975e-19
 6.9597640e-11 1.0509709e-20], sum to 1.0000
[2019-04-03 22:46:52,570] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3097
[2019-04-03 22:46:52,583] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.4, 80.0, 111.0, 781.5, 26.0, 26.46413542365845, 0.7231975736840099, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3236400.0000, 
sim time next is 3237000.0000, 
raw observation next is [-2.333333333333333, 78.5, 111.6666666666667, 791.3333333333334, 26.0, 26.51008983049948, 0.7325899588568014, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.3979686057248385, 0.785, 0.37222222222222234, 0.874401473296501, 0.6666666666666666, 0.7091741525416234, 0.7441966529522671, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.95921946], dtype=float32), 0.81755924]. 
=============================================
[2019-04-03 22:46:52,602] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[87.843636]
 [88.11407 ]
 [88.40664 ]
 [88.641014]
 [88.75425 ]], R is [[87.73435974]
 [87.85701752]
 [87.97844696]
 [88.09866333]
 [88.21767426]].
[2019-04-03 22:46:53,379] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.5315818e-28 3.5740455e-20 7.4034094e-17 1.0000000e+00 1.7194629e-21
 2.2944301e-11 1.4765247e-21], sum to 1.0000
[2019-04-03 22:46:53,380] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3867
[2019-04-03 22:46:53,413] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 46.66666666666667, 69.0, 558.6666666666667, 26.0, 26.39038120561288, 0.6772835608730391, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3341400.0000, 
sim time next is 3342000.0000, 
raw observation next is [-2.0, 47.33333333333334, 64.5, 529.8333333333333, 26.0, 26.57303489238265, 0.6803929001776355, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.40720221606648205, 0.47333333333333344, 0.215, 0.5854511970534069, 0.6666666666666666, 0.714419574365221, 0.7267976333925451, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.802803], dtype=float32), -0.43859947]. 
=============================================
[2019-04-03 22:46:53,430] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[85.69073]
 [85.89184]
 [86.13454]
 [86.04868]
 [86.2834 ]], R is [[85.58183289]
 [85.72601318]
 [85.86875153]
 [85.71302032]
 [85.85588837]].
[2019-04-03 22:46:57,336] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0531515e-25 1.7358618e-19 2.5359895e-15 1.0000000e+00 9.2406807e-19
 1.9658963e-09 6.0353825e-20], sum to 1.0000
[2019-04-03 22:46:57,336] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1079
[2019-04-03 22:46:57,353] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.833333333333334, 53.33333333333333, 115.3333333333333, 803.6666666666666, 26.0, 25.95105753958691, 0.5577634341109804, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3330600.0000, 
sim time next is 3331200.0000, 
raw observation next is [-4.666666666666667, 52.66666666666667, 114.6666666666667, 801.8333333333334, 26.0, 25.93826874990581, 0.5668962932203664, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.3333333333333333, 0.5266666666666667, 0.38222222222222235, 0.8860036832412523, 0.6666666666666666, 0.6615223958254841, 0.6889654310734555, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.19695556], dtype=float32), -0.94567406]. 
=============================================
[2019-04-03 22:46:59,045] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.2602359e-29 7.5358577e-22 2.9771888e-17 1.0000000e+00 3.5407830e-21
 1.4990056e-12 1.1982878e-21], sum to 1.0000
[2019-04-03 22:46:59,047] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3727
[2019-04-03 22:46:59,061] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.79735208205544, 0.2879446100298693, 0.0, 1.0, 41126.37977965021], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3379200.0000, 
sim time next is 3379800.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.83073489058001, 0.2871847174080107, 0.0, 1.0, 41104.55747408896], 
processed observation next is [1.0, 0.08695652173913043, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5692279075483343, 0.5957282391360036, 0.0, 1.0, 0.19573598797185218], 
reward next is 0.8043, 
noisyNet noise sample is [array([0.38404942], dtype=float32), 0.056607053]. 
=============================================
[2019-04-03 22:46:59,881] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.8951127e-29 5.3153447e-21 1.7151100e-17 1.0000000e+00 3.8452432e-21
 7.7833018e-12 5.8306393e-22], sum to 1.0000
[2019-04-03 22:46:59,882] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4336
[2019-04-03 22:46:59,896] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 47.66666666666667, 114.0, 797.3333333333333, 26.0, 26.54393829801234, 0.6409253148768234, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3410400.0000, 
sim time next is 3411000.0000, 
raw observation next is [3.0, 47.0, 115.0, 804.0, 26.0, 26.61155079242861, 0.4194839980416654, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.5457063711911359, 0.47, 0.38333333333333336, 0.8883977900552487, 0.6666666666666666, 0.7176292327023841, 0.6398279993472218, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.26110396], dtype=float32), -0.9703345]. 
=============================================
[2019-04-03 22:46:59,913] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[92.13017 ]
 [92.33829 ]
 [92.53203 ]
 [92.698845]
 [92.89644 ]], R is [[91.82082367]
 [91.90261841]
 [91.9835968 ]
 [92.06375885]
 [92.14311981]].
[2019-04-03 22:47:00,047] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.0019848e-30 8.6358837e-22 1.9211756e-18 1.0000000e+00 5.3811345e-23
 2.1780128e-13 5.5373555e-23], sum to 1.0000
[2019-04-03 22:47:00,048] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6318
[2019-04-03 22:47:00,062] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 86.0, 0.0, 0.0, 26.0, 25.52000856507815, 0.4558465173934301, 0.0, 1.0, 88745.55680993859], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3450000.0000, 
sim time next is 3450600.0000, 
raw observation next is [1.0, 86.0, 0.0, 0.0, 26.0, 25.36002072042785, 0.4598887306008939, 0.0, 1.0, 139792.6841965072], 
processed observation next is [1.0, 0.9565217391304348, 0.4903047091412743, 0.86, 0.0, 0.0, 0.6666666666666666, 0.6133350600356543, 0.6532962435336312, 0.0, 1.0, 0.6656794485547962], 
reward next is 0.3343, 
noisyNet noise sample is [array([0.13799806], dtype=float32), 1.4196988]. 
=============================================
[2019-04-03 22:47:00,079] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.7634728e-28 6.2974088e-21 2.7323452e-17 1.0000000e+00 1.6068234e-21
 3.7035520e-14 4.8015738e-21], sum to 1.0000
[2019-04-03 22:47:00,079] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4046
[2019-04-03 22:47:00,091] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.833333333333333, 70.0, 0.0, 0.0, 26.0, 25.05334986657005, 0.3588632552341469, 0.0, 1.0, 41183.05061220359], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3556200.0000, 
sim time next is 3556800.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 26.0, 25.01539758832228, 0.3513036169196463, 0.0, 1.0, 41135.55939415097], 
processed observation next is [0.0, 0.17391304347826086, 0.3518005540166205, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5846164656935233, 0.617101205639882, 0.0, 1.0, 0.19588361616262367], 
reward next is 0.8041, 
noisyNet noise sample is [array([-0.9805546], dtype=float32), 1.6750883]. 
=============================================
[2019-04-03 22:47:06,092] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0525845e-26 2.7061102e-18 7.1719152e-15 9.9999988e-01 4.2418115e-18
 1.5418655e-07 4.3632898e-19], sum to 1.0000
[2019-04-03 22:47:06,095] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6483
[2019-04-03 22:47:06,102] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 49.0, 106.3333333333333, 789.3333333333334, 26.0, 26.47720224891708, 0.6855083039393324, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3507000.0000, 
sim time next is 3507600.0000, 
raw observation next is [3.0, 49.0, 104.1666666666667, 785.1666666666667, 26.0, 26.54188478367927, 0.7003240676544662, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.49, 0.3472222222222223, 0.8675874769797423, 0.6666666666666666, 0.7118237319732724, 0.7334413558848221, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.36943406], dtype=float32), -1.4668963]. 
=============================================
[2019-04-03 22:47:06,827] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.6173467e-27 8.9235624e-20 3.6619929e-16 1.0000000e+00 1.0544008e-19
 7.9779372e-10 7.1421626e-21], sum to 1.0000
[2019-04-03 22:47:06,829] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1108
[2019-04-03 22:47:06,854] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.5, 48.5, 109.0, 764.0, 26.0, 26.54259305832477, 0.6209859722848771, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3407400.0000, 
sim time next is 3408000.0000, 
raw observation next is [2.666666666666667, 48.66666666666666, 110.0, 770.6666666666667, 26.0, 26.56195010049274, 0.6346940740963135, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.5364727608494922, 0.4866666666666666, 0.36666666666666664, 0.8515653775322285, 0.6666666666666666, 0.7134958417077284, 0.7115646913654379, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.36776033], dtype=float32), 0.49606556]. 
=============================================
[2019-04-03 22:47:06,858] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[86.971886]
 [86.92336 ]
 [86.940926]
 [86.914375]
 [86.87405 ]], R is [[87.1450882 ]
 [87.27363586]
 [87.40090179]
 [87.52689362]
 [87.65162659]].
[2019-04-03 22:47:10,676] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.9577088e-31 6.0918351e-23 5.9232806e-19 1.0000000e+00 4.1506705e-25
 1.4454265e-15 2.4022446e-23], sum to 1.0000
[2019-04-03 22:47:10,678] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0167
[2019-04-03 22:47:10,695] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [8.0, 32.0, 46.5, 262.0, 26.0, 25.48153866479425, 0.3969326913496676, 0.0, 1.0, 18753.19856365772], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3657600.0000, 
sim time next is 3658200.0000, 
raw observation next is [8.5, 31.0, 60.66666666666668, 309.0, 26.0, 25.53201670787552, 0.4071451360303238, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.698060941828255, 0.31, 0.20222222222222228, 0.3414364640883978, 0.6666666666666666, 0.6276680589896267, 0.6357150453434413, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8745487], dtype=float32), 1.680525]. 
=============================================
[2019-04-03 22:47:15,408] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.26591797e-29 5.97385767e-20 1.42163760e-17 1.00000000e+00
 3.05858130e-21 5.08981399e-11 1.02584116e-22], sum to 1.0000
[2019-04-03 22:47:15,408] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8438
[2019-04-03 22:47:15,415] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.5, 46.5, 87.0, 717.0, 26.0, 26.39762404645396, 0.7014159332297697, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3857400.0000, 
sim time next is 3858000.0000, 
raw observation next is [2.666666666666667, 46.0, 83.16666666666666, 689.3333333333333, 26.0, 26.64321004369985, 0.7218369631781534, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5364727608494922, 0.46, 0.2772222222222222, 0.7616942909760589, 0.6666666666666666, 0.720267503641654, 0.7406123210593845, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.33132434], dtype=float32), 0.289573]. 
=============================================
[2019-04-03 22:47:15,428] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[89.580215]
 [89.63958 ]
 [89.79994 ]
 [90.13812 ]
 [90.4664  ]], R is [[89.54551697]
 [89.65006256]
 [89.75356293]
 [89.8560257 ]
 [89.95746613]].
[2019-04-03 22:47:24,221] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.5608218e-27 2.1029263e-19 1.0235230e-15 1.0000000e+00 4.3689176e-20
 1.2652517e-12 2.7713488e-20], sum to 1.0000
[2019-04-03 22:47:24,221] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5543
[2019-04-03 22:47:24,234] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.92335166139877, 0.2943697779020555, 0.0, 1.0, 41754.31976849194], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3909600.0000, 
sim time next is 3910200.0000, 
raw observation next is [-6.166666666666666, 59.83333333333334, 0.0, 0.0, 26.0, 24.90871117040771, 0.2959814924181288, 0.0, 1.0, 41901.90103060278], 
processed observation next is [1.0, 0.2608695652173913, 0.29178208679593726, 0.5983333333333334, 0.0, 0.0, 0.6666666666666666, 0.5757259308673092, 0.5986604974727096, 0.0, 1.0, 0.19953286205048942], 
reward next is 0.8005, 
noisyNet noise sample is [array([0.04950932], dtype=float32), -0.3782431]. 
=============================================
[2019-04-03 22:47:33,072] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.1145348e-25 5.4387464e-18 8.3432417e-16 1.0000000e+00 2.4753046e-17
 2.7919287e-09 2.5603926e-19], sum to 1.0000
[2019-04-03 22:47:33,073] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3611
[2019-04-03 22:47:33,132] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.833333333333334, 43.33333333333334, 108.3333333333333, 753.3333333333334, 26.0, 26.56375944067353, 0.569395295845259, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4011000.0000, 
sim time next is 4011600.0000, 
raw observation next is [-8.666666666666668, 42.66666666666667, 110.1666666666667, 767.1666666666667, 26.0, 26.56717045989281, 0.5761159257847565, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.22253000923361033, 0.4266666666666667, 0.36722222222222234, 0.8476979742173113, 0.6666666666666666, 0.7139308716577343, 0.6920386419282522, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2039181], dtype=float32), 0.15344545]. 
=============================================
[2019-04-03 22:47:33,332] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.9294461e-25 2.4313676e-16 3.1636735e-14 1.0000000e+00 2.4636716e-16
 6.1702274e-09 8.5875068e-19], sum to 1.0000
[2019-04-03 22:47:33,333] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1171
[2019-04-03 22:47:33,356] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.666666666666666, 36.66666666666666, 58.16666666666666, 474.8333333333334, 26.0, 26.19853513634177, 0.6575483594982499, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3948000.0000, 
sim time next is 3948600.0000, 
raw observation next is [-4.833333333333334, 37.33333333333334, 50.33333333333334, 413.6666666666667, 26.0, 26.44129396299094, 0.5142567745649544, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.32871652816251157, 0.3733333333333334, 0.1677777777777778, 0.45709023941068144, 0.6666666666666666, 0.7034411635825784, 0.6714189248549848, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7515093], dtype=float32), 0.69109607]. 
=============================================
[2019-04-03 22:47:48,464] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.3908893e-30 1.8944628e-22 4.9866278e-19 1.0000000e+00 1.0005034e-23
 3.3310692e-14 2.2668377e-23], sum to 1.0000
[2019-04-03 22:47:48,464] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1086
[2019-04-03 22:47:48,476] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.166666666666667, 47.16666666666667, 0.0, 0.0, 26.0, 25.40222412128963, 0.3683196659860857, 0.0, 1.0, 45648.57616510626], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4230600.0000, 
sim time next is 4231200.0000, 
raw observation next is [1.333333333333333, 47.33333333333334, 0.0, 0.0, 26.0, 25.39882636539883, 0.3675465335547927, 0.0, 1.0, 43142.92566509189], 
processed observation next is [0.0, 1.0, 0.4995383194829178, 0.47333333333333344, 0.0, 0.0, 0.6666666666666666, 0.6165688637832357, 0.6225155111849309, 0.0, 1.0, 0.20544250316710425], 
reward next is 0.7946, 
noisyNet noise sample is [array([-0.29213655], dtype=float32), 0.74023104]. 
=============================================
[2019-04-03 22:47:52,284] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.0130880e-28 4.2533486e-18 1.7588481e-15 1.0000000e+00 8.0203183e-22
 5.5339716e-10 3.4405087e-21], sum to 1.0000
[2019-04-03 22:47:52,288] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0363
[2019-04-03 22:47:52,311] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [13.45, 33.5, 103.0, 0.0, 26.0, 28.81714655956952, 0.9759773014797107, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4375800.0000, 
sim time next is 4376400.0000, 
raw observation next is [13.3, 34.0, 92.33333333333334, 0.0, 26.0, 28.80843794269763, 1.136363661731128, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.8310249307479226, 0.34, 0.3077777777777778, 0.0, 0.6666666666666666, 0.9007031618914692, 0.8787878872437093, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.35615817], dtype=float32), 0.3291904]. 
=============================================
[2019-04-03 22:47:53,318] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.3616582e-27 3.6456116e-18 1.0016799e-15 1.0000000e+00 7.1500410e-21
 3.9111620e-11 1.7070268e-20], sum to 1.0000
[2019-04-03 22:47:53,319] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3566
[2019-04-03 22:47:53,349] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [9.533333333333333, 60.66666666666666, 0.0, 0.0, 26.0, 26.50098006093678, 0.7655443814371848, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4398600.0000, 
sim time next is 4399200.0000, 
raw observation next is [9.4, 61.0, 0.0, 0.0, 26.0, 26.44582776708836, 0.7546785429672052, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.7229916897506927, 0.61, 0.0, 0.0, 0.6666666666666666, 0.7038189805906967, 0.7515595143224018, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.2143986], dtype=float32), 0.07865667]. 
=============================================
[2019-04-03 22:47:55,682] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.2759184e-29 1.6095549e-19 1.2049471e-15 1.0000000e+00 5.2904602e-22
 7.6339342e-13 3.9339838e-21], sum to 1.0000
[2019-04-03 22:47:55,684] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5616
[2019-04-03 22:47:55,694] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.683333333333334, 65.16666666666667, 0.0, 0.0, 26.0, 25.9587542845359, 0.6149999996393654, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4410600.0000, 
sim time next is 4411200.0000, 
raw observation next is [6.566666666666667, 65.33333333333334, 0.0, 0.0, 26.0, 25.90543263854812, 0.6091123856441146, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.6445060018467221, 0.6533333333333334, 0.0, 0.0, 0.6666666666666666, 0.6587860532123434, 0.7030374618813715, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5877851], dtype=float32), -0.63254976]. 
=============================================
[2019-04-03 22:47:58,101] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.2999215e-27 4.1666742e-19 8.8502576e-15 1.0000000e+00 3.2094908e-20
 3.8106279e-10 1.6530625e-20], sum to 1.0000
[2019-04-03 22:47:58,108] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9804
[2019-04-03 22:47:58,117] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 86.0, 135.3333333333333, 0.0, 26.0, 26.48298115886468, 0.6399834174562596, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4448400.0000, 
sim time next is 4449000.0000, 
raw observation next is [1.0, 86.0, 128.6666666666667, 0.0, 26.0, 26.47861085375026, 0.6338137127714025, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4903047091412743, 0.86, 0.42888888888888904, 0.0, 0.6666666666666666, 0.7065509044791884, 0.7112712375904676, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3053637], dtype=float32), -0.39171034]. 
=============================================
[2019-04-03 22:47:58,146] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[84.2736  ]
 [84.35364 ]
 [84.87588 ]
 [85.428154]
 [86.15421 ]], R is [[84.47122955]
 [84.62651825]
 [84.78025055]
 [84.93244934]
 [85.08312225]].
[2019-04-03 22:48:04,801] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.9552929e-30 3.6868933e-20 1.3071882e-17 1.0000000e+00 3.6369489e-23
 9.7354104e-12 2.2243505e-23], sum to 1.0000
[2019-04-03 22:48:04,802] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4468
[2019-04-03 22:48:04,818] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 72.0, 131.3333333333333, 3.666666666666666, 26.0, 26.12759580038214, 0.5474688088907605, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4722600.0000, 
sim time next is 4723200.0000, 
raw observation next is [1.0, 72.0, 123.5, 5.5, 26.0, 26.16213662228925, 0.5469566953146596, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.4903047091412743, 0.72, 0.4116666666666667, 0.0060773480662983425, 0.6666666666666666, 0.6801780518574375, 0.6823188984382199, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4001718], dtype=float32), 0.25831154]. 
=============================================
[2019-04-03 22:48:06,033] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.1804285e-31 4.1268684e-22 1.7727561e-17 1.0000000e+00 5.8640057e-23
 1.0300633e-13 6.5618403e-23], sum to 1.0000
[2019-04-03 22:48:06,036] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2517
[2019-04-03 22:48:06,094] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.5333333333333334, 72.66666666666667, 92.5, 55.0, 26.0, 25.33979577713733, 0.4542847176896315, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4522800.0000, 
sim time next is 4523400.0000, 
raw observation next is [-0.4, 72.5, 111.0, 66.0, 26.0, 25.64496051691852, 0.4763799491520874, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.45152354570637127, 0.725, 0.37, 0.07292817679558011, 0.6666666666666666, 0.6370800430765433, 0.6587933163840292, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.22022001], dtype=float32), 0.66144544]. 
=============================================
[2019-04-03 22:48:06,694] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.9799755e-29 3.7627117e-19 1.4286958e-15 1.0000000e+00 6.0651060e-21
 2.5692612e-12 2.7911237e-21], sum to 1.0000
[2019-04-03 22:48:06,695] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6111
[2019-04-03 22:48:06,704] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 25.76965277066809, 0.6587859717502574, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4654800.0000, 
sim time next is 4655400.0000, 
raw observation next is [2.0, 52.83333333333334, 0.0, 0.0, 26.0, 25.91556136980817, 0.6625089888042616, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.518005540166205, 0.5283333333333334, 0.0, 0.0, 0.6666666666666666, 0.6596301141506808, 0.7208363296014205, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.14570162], dtype=float32), -0.5235756]. 
=============================================
[2019-04-03 22:48:12,852] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2482001e-30 1.8944441e-19 1.2736819e-17 1.0000000e+00 2.5831759e-24
 4.2284479e-12 3.1032751e-22], sum to 1.0000
[2019-04-03 22:48:12,855] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7995
[2019-04-03 22:48:12,874] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.5, 51.0, 0.0, 0.0, 26.0, 26.40418501965249, 0.7037280729508852, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4645800.0000, 
sim time next is 4646400.0000, 
raw observation next is [3.333333333333333, 51.66666666666666, 0.0, 0.0, 26.0, 26.37009979586732, 0.6966108477688007, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.5549399815327793, 0.5166666666666666, 0.0, 0.0, 0.6666666666666666, 0.6975083163222765, 0.7322036159229336, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5815567], dtype=float32), 0.37266785]. 
=============================================
[2019-04-03 22:48:20,076] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.4472702e-31 1.2613737e-21 3.2526450e-18 1.0000000e+00 6.9260031e-24
 8.2460878e-12 2.6270913e-23], sum to 1.0000
[2019-04-03 22:48:20,078] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3118
[2019-04-03 22:48:20,135] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.666666666666667, 77.33333333333334, 145.1666666666667, 0.9999999999999998, 26.0, 25.09686180367783, 0.5007567551164308, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4714800.0000, 
sim time next is 4715400.0000, 
raw observation next is [1.833333333333333, 75.16666666666667, 155.3333333333333, 2.0, 26.0, 25.73437803310134, 0.5481753015031112, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5133887349953832, 0.7516666666666667, 0.5177777777777777, 0.0022099447513812156, 0.6666666666666666, 0.644531502758445, 0.682725100501037, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.102523], dtype=float32), 0.7493421]. 
=============================================
[2019-04-03 22:48:22,754] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.5228795e-26 2.9623919e-18 4.9945552e-16 1.0000000e+00 1.4569205e-20
 2.4243847e-12 6.9025165e-20], sum to 1.0000
[2019-04-03 22:48:22,754] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0101
[2019-04-03 22:48:22,769] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.833333333333333, 59.16666666666666, 0.0, 0.0, 26.0, 25.3871841692686, 0.3837062120680867, 0.0, 1.0, 50640.58391476031], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4837800.0000, 
sim time next is 4838400.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.44202913166353, 0.3800408167978088, 0.0, 1.0, 18761.14879910263], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.6, 0.0, 0.0, 0.6666666666666666, 0.6201690943052943, 0.6266802722659363, 0.0, 1.0, 0.08933880380525062], 
reward next is 0.9107, 
noisyNet noise sample is [array([0.5324773], dtype=float32), 1.45315]. 
=============================================
[2019-04-03 22:48:24,349] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.7586468e-30 2.5701269e-21 6.1228364e-18 1.0000000e+00 2.5885053e-22
 7.6795239e-14 2.5227906e-22], sum to 1.0000
[2019-04-03 22:48:24,349] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4434
[2019-04-03 22:48:24,368] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 40.0, 0.0, 0.0, 26.0, 25.39902093099394, 0.346920414211557, 0.0, 1.0, 30943.50577399054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4924800.0000, 
sim time next is 4925400.0000, 
raw observation next is [0.8333333333333334, 40.5, 0.0, 0.0, 26.0, 25.41086913312982, 0.3426194820848405, 0.0, 1.0, 27877.48183478014], 
processed observation next is [1.0, 0.0, 0.4856879039704525, 0.405, 0.0, 0.0, 0.6666666666666666, 0.6175724277608184, 0.6142064940282802, 0.0, 1.0, 0.13274991349895304], 
reward next is 0.8673, 
noisyNet noise sample is [array([-0.10376899], dtype=float32), -0.4139211]. 
=============================================
[2019-04-03 22:48:24,669] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:48:24,669] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:48:24,694] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run6
[2019-04-03 22:48:26,688] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:48:26,689] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:48:26,699] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run6
[2019-04-03 22:48:28,997] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.4915680e-30 2.6210680e-21 1.3172296e-17 1.0000000e+00 2.9740061e-20
 2.6059012e-12 6.1452845e-23], sum to 1.0000
[2019-04-03 22:48:28,998] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2105
[2019-04-03 22:48:29,028] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 44.0, 112.0, 698.0, 26.0, 26.61349840790092, 0.6505278719372418, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5045400.0000, 
sim time next is 5046000.0000, 
raw observation next is [2.333333333333333, 43.0, 112.6666666666667, 716.5, 26.0, 26.72797930776827, 0.6791550728382197, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.5272391505078486, 0.43, 0.37555555555555564, 0.7917127071823205, 0.6666666666666666, 0.727331608980689, 0.7263850242794065, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.4554446], dtype=float32), -2.3606145]. 
=============================================
[2019-04-03 22:48:29,028] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:48:29,028] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:48:29,034] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[93.35214]
 [93.18469]
 [92.97298]
 [92.87981]
 [92.72763]], R is [[93.48842621]
 [93.55354309]
 [93.61801147]
 [93.68183136]
 [93.74501038]].
[2019-04-03 22:48:29,042] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run6
[2019-04-03 22:48:29,289] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:48:29,289] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:48:29,292] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run6
[2019-04-03 22:48:31,250] A3C_AGENT_WORKER-Thread-2 INFO:Local step 42500, global step 678048: loss 2.0475
[2019-04-03 22:48:31,259] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 42500, global step 678048: learning rate 0.0005
[2019-04-03 22:48:32,529] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:48:32,530] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:48:32,542] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run6
[2019-04-03 22:48:32,958] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:48:32,958] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:48:32,962] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run6
[2019-04-03 22:48:33,096] A3C_AGENT_WORKER-Thread-19 INFO:Local step 42500, global step 678796: loss 0.8094
[2019-04-03 22:48:33,096] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 42500, global step 678796: learning rate 0.0005
[2019-04-03 22:48:33,785] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:48:33,785] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:48:33,789] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run6
[2019-04-03 22:48:34,359] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:48:34,360] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:48:34,381] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run6
[2019-04-03 22:48:35,590] A3C_AGENT_WORKER-Thread-3 INFO:Local step 42500, global step 679638: loss 0.2780
[2019-04-03 22:48:35,591] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 42500, global step 679638: learning rate 0.0005
[2019-04-03 22:48:35,629] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:48:35,629] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:48:35,633] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run6
[2019-04-03 22:48:35,893] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:48:35,893] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:48:35,905] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run6
[2019-04-03 22:48:36,046] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:48:36,047] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:48:36,057] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run6
[2019-04-03 22:48:36,203] A3C_AGENT_WORKER-Thread-17 INFO:Local step 42500, global step 679827: loss 0.2621
[2019-04-03 22:48:36,212] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 42500, global step 679827: learning rate 0.0005
[2019-04-03 22:48:36,405] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:48:36,405] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:48:36,409] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run6
[2019-04-03 22:48:37,833] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:48:37,833] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:48:37,837] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run6
[2019-04-03 22:48:39,226] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:48:39,226] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:48:39,230] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run6
[2019-04-03 22:48:39,409] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:48:39,409] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:48:39,413] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run6
[2019-04-03 22:48:39,473] A3C_AGENT_WORKER-Thread-12 INFO:Local step 42500, global step 680431: loss 1.4908
[2019-04-03 22:48:39,473] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 42500, global step 680431: learning rate 0.0005
[2019-04-03 22:48:40,214] A3C_AGENT_WORKER-Thread-6 INFO:Local step 42500, global step 680548: loss 1.8630
[2019-04-03 22:48:40,223] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 42500, global step 680548: learning rate 0.0005
[2019-04-03 22:48:41,078] A3C_AGENT_WORKER-Thread-18 INFO:Local step 42500, global step 680709: loss 1.9723
[2019-04-03 22:48:41,090] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 42500, global step 680712: learning rate 0.0005
[2019-04-03 22:48:41,735] A3C_AGENT_WORKER-Thread-5 INFO:Local step 42500, global step 680881: loss 1.9239
[2019-04-03 22:48:41,738] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 42500, global step 680881: learning rate 0.0005
[2019-04-03 22:48:42,791] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:48:42,791] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:48:42,798] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run6
[2019-04-03 22:48:42,972] A3C_AGENT_WORKER-Thread-20 INFO:Local step 42500, global step 681185: loss 3.2251
[2019-04-03 22:48:42,973] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 42500, global step 681185: learning rate 0.0005
[2019-04-03 22:48:43,432] A3C_AGENT_WORKER-Thread-13 INFO:Local step 42500, global step 681334: loss 3.8805
[2019-04-03 22:48:43,435] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 42500, global step 681334: learning rate 0.0005
[2019-04-03 22:48:44,135] A3C_AGENT_WORKER-Thread-10 INFO:Local step 42500, global step 681557: loss 3.5258
[2019-04-03 22:48:44,138] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 42500, global step 681557: learning rate 0.0005
[2019-04-03 22:48:44,299] A3C_AGENT_WORKER-Thread-11 INFO:Local step 42500, global step 681617: loss 3.3440
[2019-04-03 22:48:44,300] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 42500, global step 681617: learning rate 0.0005
[2019-04-03 22:48:45,199] A3C_AGENT_WORKER-Thread-4 INFO:Local step 42500, global step 681974: loss 2.4321
[2019-04-03 22:48:45,202] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 42500, global step 681975: learning rate 0.0005
[2019-04-03 22:48:46,456] A3C_AGENT_WORKER-Thread-14 INFO:Local step 42500, global step 682554: loss 1.8999
[2019-04-03 22:48:46,457] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 42500, global step 682555: learning rate 0.0005
[2019-04-03 22:48:46,787] A3C_AGENT_WORKER-Thread-16 INFO:Local step 42500, global step 682714: loss 1.2089
[2019-04-03 22:48:46,790] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 42500, global step 682715: learning rate 0.0005
[2019-04-03 22:48:47,260] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.5971754e-18 1.7402762e-12 2.1691762e-10 9.9999952e-01 6.8061214e-13
 4.6498212e-07 5.0321108e-13], sum to 1.0000
[2019-04-03 22:48:47,262] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6002
[2019-04-03 22:48:47,287] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.566666666666667, 74.33333333333334, 0.0, 0.0, 19.0, 18.51268945259458, -1.150476123356727, 0.0, 1.0, 58486.1750753485], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 105600.0000, 
sim time next is 106200.0000, 
raw observation next is [-5.85, 74.5, 0.0, 0.0, 19.0, 18.53630267287648, -1.14400948416602, 0.0, 1.0, 34865.20602194814], 
processed observation next is [1.0, 0.21739130434782608, 0.30055401662049863, 0.745, 0.0, 0.0, 0.08333333333333333, 0.0446918894063734, 0.11866350527799334, 0.0, 1.0, 0.16602479058070543], 
reward next is 0.8340, 
noisyNet noise sample is [array([0.40854487], dtype=float32), -0.1232391]. 
=============================================
[2019-04-03 22:48:49,623] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.27359416e-30 2.56580931e-19 8.02397196e-19 1.00000000e+00
 1.03727245e-22 1.91669250e-10 1.50254741e-22], sum to 1.0000
[2019-04-03 22:48:49,623] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1041
[2019-04-03 22:48:49,666] A3C_AGENT_WORKER-Thread-15 INFO:Local step 42500, global step 683722: loss 0.7211
[2019-04-03 22:48:49,666] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 42500, global step 683722: learning rate 0.0005
[2019-04-03 22:48:49,717] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.3, 67.33333333333334, 0.0, 0.0, 26.0, 24.84631053669217, 0.143488324315925, 1.0, 1.0, 93007.84997016021], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 157800.0000, 
sim time next is 158400.0000, 
raw observation next is [-8.4, 68.0, 0.0, 0.0, 26.0, 24.76406865566251, 0.1485911201101787, 0.0, 1.0, 132057.7413489649], 
processed observation next is [1.0, 0.8695652173913043, 0.2299168975069252, 0.68, 0.0, 0.0, 0.6666666666666666, 0.5636723879718758, 0.5495303733700595, 0.0, 1.0, 0.6288463873760233], 
reward next is 0.3712, 
noisyNet noise sample is [array([0.17615739], dtype=float32), -0.45625725]. 
=============================================
[2019-04-03 22:48:51,326] A3C_AGENT_WORKER-Thread-2 INFO:Local step 43000, global step 684196: loss 0.9990
[2019-04-03 22:48:51,328] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 43000, global step 684196: learning rate 0.0005
[2019-04-03 22:48:52,217] A3C_AGENT_WORKER-Thread-19 INFO:Local step 43000, global step 684459: loss 1.3514
[2019-04-03 22:48:52,219] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 43000, global step 684459: learning rate 0.0005
[2019-04-03 22:48:57,715] A3C_AGENT_WORKER-Thread-3 INFO:Local step 43000, global step 685995: loss 7.8967
[2019-04-03 22:48:57,716] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 43000, global step 685995: learning rate 0.0005
[2019-04-03 22:48:58,220] A3C_AGENT_WORKER-Thread-17 INFO:Local step 43000, global step 686117: loss 3.6806
[2019-04-03 22:48:58,221] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 43000, global step 686117: learning rate 0.0005
[2019-04-03 22:48:59,782] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.7049769e-26 1.3874840e-16 5.6397659e-14 9.9999702e-01 6.6364010e-18
 2.9296982e-06 1.9324809e-19], sum to 1.0000
[2019-04-03 22:48:59,782] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3078
[2019-04-03 22:48:59,863] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.566666666666667, 68.33333333333334, 0.0, 0.0, 26.0, 24.48484330228547, 0.1373410692544113, 0.0, 1.0, 44556.43707678143], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 249600.0000, 
sim time next is 250200.0000, 
raw observation next is [-3.65, 70.0, 0.0, 0.0, 26.0, 24.42564679182316, 0.1265402075793367, 0.0, 1.0, 44575.43915166227], 
processed observation next is [1.0, 0.9130434782608695, 0.3614958448753463, 0.7, 0.0, 0.0, 0.6666666666666666, 0.5354705659852632, 0.5421800691931122, 0.0, 1.0, 0.21226399596029652], 
reward next is 0.7877, 
noisyNet noise sample is [array([0.29132208], dtype=float32), 0.79532164]. 
=============================================
[2019-04-03 22:49:00,703] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.9667283e-22 6.3469832e-14 7.8448104e-13 9.9927408e-01 2.9439621e-15
 7.2590407e-04 1.3894513e-16], sum to 1.0000
[2019-04-03 22:49:00,703] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8714
[2019-04-03 22:49:00,819] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-12.8, 73.5, 0.0, 0.0, 26.0, 25.04187633573795, 0.1801115319893339, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 333000.0000, 
sim time next is 333600.0000, 
raw observation next is [-12.8, 74.66666666666666, 0.0, 0.0, 26.0, 24.82869042165639, 0.1360191046121784, 0.0, 1.0, 9387.206634917296], 
processed observation next is [1.0, 0.8695652173913043, 0.1080332409972299, 0.7466666666666666, 0.0, 0.0, 0.6666666666666666, 0.5690575351380325, 0.5453397015373928, 0.0, 1.0, 0.04470098397579665], 
reward next is 0.9553, 
noisyNet noise sample is [array([0.29537967], dtype=float32), 0.28211096]. 
=============================================
[2019-04-03 22:49:02,165] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.00979765e-23 6.06007282e-18 5.75653417e-15 9.99999881e-01
 5.59312372e-16 1.22627199e-07 1.42920101e-17], sum to 1.0000
[2019-04-03 22:49:02,165] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0548
[2019-04-03 22:49:02,208] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-11.33333333333333, 69.0, 0.0, 0.0, 26.0, 22.54621426111853, -0.2959728006214342, 0.0, 1.0, 48085.29504961417], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 279600.0000, 
sim time next is 280200.0000, 
raw observation next is [-11.51666666666667, 69.5, 0.0, 0.0, 26.0, 22.48058928891469, -0.3089142339507363, 0.0, 1.0, 48104.4623867064], 
processed observation next is [1.0, 0.21739130434782608, 0.14358264081255764, 0.695, 0.0, 0.0, 0.6666666666666666, 0.3733824407428908, 0.39702858868308794, 0.0, 1.0, 0.22906886850812572], 
reward next is 0.7709, 
noisyNet noise sample is [array([0.98426116], dtype=float32), -0.4297848]. 
=============================================
[2019-04-03 22:49:02,848] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.8251521e-24 3.9406172e-16 3.8817462e-15 1.0000000e+00 1.6926149e-17
 1.1989404e-08 1.3472526e-18], sum to 1.0000
[2019-04-03 22:49:02,848] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9844
[2019-04-03 22:49:02,944] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 26.0, 24.88408110735118, 0.2087421569123128, 0.0, 1.0, 39774.81469016543], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 246600.0000, 
sim time next is 247200.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 26.0, 24.81398434301379, 0.1937544995272362, 0.0, 1.0, 43040.14458216838], 
processed observation next is [1.0, 0.8695652173913043, 0.368421052631579, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5678320285844825, 0.5645848331757454, 0.0, 1.0, 0.20495306943889705], 
reward next is 0.7950, 
noisyNet noise sample is [array([-0.8788248], dtype=float32), 0.23128362]. 
=============================================
[2019-04-03 22:49:05,836] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.7266428e-21 7.5490727e-14 1.5181181e-12 9.9557424e-01 1.5320682e-13
 4.4257403e-03 1.6686529e-16], sum to 1.0000
[2019-04-03 22:49:05,836] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7242
[2019-04-03 22:49:06,059] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-10.78333333333333, 60.5, 93.33333333333334, 560.6666666666666, 26.0, 25.81407011248203, 0.3592548206242209, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 298200.0000, 
sim time next is 298800.0000, 
raw observation next is [-10.6, 60.0, 96.5, 585.0, 26.0, 25.84441617991237, 0.3620854603481223, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.1689750692520776, 0.6, 0.32166666666666666, 0.6464088397790055, 0.6666666666666666, 0.6537013483260307, 0.6206951534493741, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6533801], dtype=float32), 1.0600296]. 
=============================================
[2019-04-03 22:49:06,605] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.7769537e-20 3.1169967e-13 2.7631406e-12 9.6513921e-01 9.5760747e-14
 3.4860794e-02 2.8380764e-15], sum to 1.0000
[2019-04-03 22:49:06,606] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3738
[2019-04-03 22:49:06,695] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-9.5, 43.0, 82.0, 623.0, 26.0, 26.25234330820973, 0.4994327011702998, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 311400.0000, 
sim time next is 312000.0000, 
raw observation next is [-9.5, 42.66666666666667, 80.0, 598.6666666666667, 26.0, 26.28809163997177, 0.4878235914223382, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.1994459833795014, 0.4266666666666667, 0.26666666666666666, 0.661510128913444, 0.6666666666666666, 0.6906743033309809, 0.6626078638074461, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2286248], dtype=float32), -2.2406697]. 
=============================================
[2019-04-03 22:49:06,744] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.0285492e-23 8.7073240e-17 4.1110134e-15 1.0000000e+00 6.3213215e-17
 7.7569773e-09 8.4400348e-18], sum to 1.0000
[2019-04-03 22:49:06,744] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9501
[2019-04-03 22:49:06,798] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[73.11803]
 [73.18676]
 [73.32445]
 [73.57199]
 [73.42457]], R is [[73.08469391]
 [73.35385132]
 [73.62031555]
 [73.8841095 ]
 [73.64911652]].
[2019-04-03 22:49:06,798] A3C_AGENT_WORKER-Thread-12 INFO:Local step 43000, global step 687668: loss 0.1200
[2019-04-03 22:49:06,798] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 43000, global step 687668: learning rate 0.0005
[2019-04-03 22:49:06,806] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-16.2, 78.0, 0.0, 0.0, 26.0, 20.96896860225718, -0.7016626239388737, 0.0, 1.0, 49729.75839748718], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 369600.0000, 
sim time next is 370200.0000, 
raw observation next is [-16.2, 78.0, 0.0, 0.0, 26.0, 20.88846669944504, -0.7046224063891976, 0.0, 1.0, 49863.23684105206], 
processed observation next is [1.0, 0.2608695652173913, 0.013850415512465375, 0.78, 0.0, 0.0, 0.6666666666666666, 0.24070555828708665, 0.26512586453693415, 0.0, 1.0, 0.23744398495739075], 
reward next is 0.7626, 
noisyNet noise sample is [array([0.59417814], dtype=float32), -0.6400698]. 
=============================================
[2019-04-03 22:49:08,578] A3C_AGENT_WORKER-Thread-6 INFO:Local step 43000, global step 687948: loss 0.1382
[2019-04-03 22:49:08,595] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 43000, global step 687948: learning rate 0.0005
[2019-04-03 22:49:08,667] A3C_AGENT_WORKER-Thread-18 INFO:Local step 43000, global step 687961: loss 0.1191
[2019-04-03 22:49:08,667] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 43000, global step 687961: learning rate 0.0005
[2019-04-03 22:49:10,903] A3C_AGENT_WORKER-Thread-5 INFO:Local step 43000, global step 688341: loss 0.0375
[2019-04-03 22:49:10,906] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 43000, global step 688341: learning rate 0.0005
[2019-04-03 22:49:12,212] A3C_AGENT_WORKER-Thread-20 INFO:Local step 43000, global step 688559: loss 3.3504
[2019-04-03 22:49:12,212] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 43000, global step 688559: learning rate 0.0005
[2019-04-03 22:49:12,477] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.4669668e-21 1.5453503e-14 6.3771427e-13 9.9992836e-01 1.3508003e-13
 7.1596238e-05 8.3675680e-16], sum to 1.0000
[2019-04-03 22:49:12,477] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1064
[2019-04-03 22:49:12,603] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-14.1, 67.0, 0.0, 0.0, 26.0, 23.27076740632431, -0.1262496141310428, 0.0, 1.0, 47560.48653707137], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 346800.0000, 
sim time next is 347400.0000, 
raw observation next is [-14.2, 67.5, 0.0, 0.0, 26.0, 23.2014064774052, -0.1414868302738085, 0.0, 1.0, 47634.18938667587], 
processed observation next is [1.0, 0.0, 0.06925207756232687, 0.675, 0.0, 0.0, 0.6666666666666666, 0.4334505397837667, 0.4528377232420638, 0.0, 1.0, 0.2268294732698851], 
reward next is 0.7732, 
noisyNet noise sample is [array([0.20155314], dtype=float32), -1.1801914]. 
=============================================
[2019-04-03 22:49:13,260] A3C_AGENT_WORKER-Thread-13 INFO:Local step 43000, global step 688734: loss 0.0771
[2019-04-03 22:49:13,261] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 43000, global step 688734: learning rate 0.0005
[2019-04-03 22:49:14,717] A3C_AGENT_WORKER-Thread-11 INFO:Local step 43000, global step 688968: loss 0.0794
[2019-04-03 22:49:14,717] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 43000, global step 688968: learning rate 0.0005
[2019-04-03 22:49:14,837] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.1138916e-24 7.1941279e-18 4.5657786e-15 1.0000000e+00 1.7659804e-16
 3.5114007e-09 1.2585063e-18], sum to 1.0000
[2019-04-03 22:49:14,837] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3189
[2019-04-03 22:49:15,029] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-12.55, 68.5, 0.0, 0.0, 26.0, 22.37974554265816, -0.2331574819297942, 1.0, 1.0, 203411.9187802927], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 286200.0000, 
sim time next is 286800.0000, 
raw observation next is [-12.63333333333333, 69.0, 0.0, 0.0, 26.0, 22.9684748612696, -0.1125124423457806, 1.0, 1.0, 165512.0206147112], 
processed observation next is [1.0, 0.30434782608695654, 0.11265004616805181, 0.69, 0.0, 0.0, 0.6666666666666666, 0.41403957177246653, 0.46249585255140646, 1.0, 1.0, 0.7881524791176723], 
reward next is 0.2118, 
noisyNet noise sample is [array([-0.59616196], dtype=float32), -0.118466355]. 
=============================================
[2019-04-03 22:49:15,790] A3C_AGENT_WORKER-Thread-10 INFO:Local step 43000, global step 689137: loss 0.1570
[2019-04-03 22:49:15,792] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 43000, global step 689137: learning rate 0.0005
[2019-04-03 22:49:16,848] A3C_AGENT_WORKER-Thread-4 INFO:Local step 43000, global step 689313: loss 1.3047
[2019-04-03 22:49:16,849] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 43000, global step 689313: learning rate 0.0005
[2019-04-03 22:49:18,208] A3C_AGENT_WORKER-Thread-16 INFO:Local step 43000, global step 689548: loss 0.0239
[2019-04-03 22:49:18,225] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 43000, global step 689548: learning rate 0.0005
[2019-04-03 22:49:19,448] A3C_AGENT_WORKER-Thread-14 INFO:Local step 43000, global step 689758: loss 0.0263
[2019-04-03 22:49:19,449] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 43000, global step 689758: learning rate 0.0005
[2019-04-03 22:49:26,376] A3C_AGENT_WORKER-Thread-15 INFO:Local step 43000, global step 690955: loss 0.0395
[2019-04-03 22:49:26,377] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 43000, global step 690955: learning rate 0.0005
[2019-04-03 22:49:33,095] A3C_AGENT_WORKER-Thread-2 INFO:Local step 43500, global step 692089: loss 0.0086
[2019-04-03 22:49:33,134] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 43500, global step 692089: learning rate 0.0005
[2019-04-03 22:49:34,640] A3C_AGENT_WORKER-Thread-19 INFO:Local step 43500, global step 692368: loss 0.0379
[2019-04-03 22:49:34,689] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 43500, global step 692368: learning rate 0.0005
[2019-04-03 22:49:42,399] A3C_AGENT_WORKER-Thread-3 INFO:Local step 43500, global step 693836: loss 0.4222
[2019-04-03 22:49:42,402] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 43500, global step 693836: learning rate 0.0005
[2019-04-03 22:49:42,962] A3C_AGENT_WORKER-Thread-17 INFO:Local step 43500, global step 693936: loss 0.5547
[2019-04-03 22:49:42,982] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 43500, global step 693936: learning rate 0.0005
[2019-04-03 22:49:50,115] A3C_AGENT_WORKER-Thread-12 INFO:Local step 43500, global step 695383: loss 2.7174
[2019-04-03 22:49:50,125] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 43500, global step 695383: learning rate 0.0005
[2019-04-03 22:49:50,712] A3C_AGENT_WORKER-Thread-18 INFO:Local step 43500, global step 695527: loss 1.6619
[2019-04-03 22:49:50,767] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 43500, global step 695527: learning rate 0.0005
[2019-04-03 22:49:52,597] A3C_AGENT_WORKER-Thread-6 INFO:Local step 43500, global step 695890: loss 2.3693
[2019-04-03 22:49:52,668] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 43500, global step 695906: learning rate 0.0005
[2019-04-03 22:49:54,482] A3C_AGENT_WORKER-Thread-5 INFO:Local step 43500, global step 696277: loss 1.6824
[2019-04-03 22:49:54,496] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 43500, global step 696280: learning rate 0.0005
[2019-04-03 22:49:55,333] A3C_AGENT_WORKER-Thread-13 INFO:Local step 43500, global step 696489: loss 1.5434
[2019-04-03 22:49:55,341] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 43500, global step 696489: learning rate 0.0005
[2019-04-03 22:49:56,583] A3C_AGENT_WORKER-Thread-20 INFO:Local step 43500, global step 696763: loss 2.4315
[2019-04-03 22:49:56,583] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 43500, global step 696763: learning rate 0.0005
[2019-04-03 22:49:59,237] A3C_AGENT_WORKER-Thread-11 INFO:Local step 43500, global step 697263: loss 2.0062
[2019-04-03 22:49:59,238] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 43500, global step 697263: learning rate 0.0005
[2019-04-03 22:49:59,897] A3C_AGENT_WORKER-Thread-10 INFO:Local step 43500, global step 697423: loss 2.9760
[2019-04-03 22:49:59,898] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 43500, global step 697423: learning rate 0.0005
[2019-04-03 22:50:00,553] A3C_AGENT_WORKER-Thread-16 INFO:Local step 43500, global step 697584: loss 3.0300
[2019-04-03 22:50:00,555] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 43500, global step 697585: learning rate 0.0005
[2019-04-03 22:50:01,480] A3C_AGENT_WORKER-Thread-4 INFO:Local step 43500, global step 697785: loss 1.7934
[2019-04-03 22:50:01,480] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 43500, global step 697785: learning rate 0.0005
[2019-04-03 22:50:01,659] A3C_AGENT_WORKER-Thread-14 INFO:Local step 43500, global step 697822: loss 1.4625
[2019-04-03 22:50:01,673] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 43500, global step 697822: learning rate 0.0005
[2019-04-03 22:50:06,121] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.4902614e-25 6.6133727e-18 3.9917571e-15 1.0000000e+00 1.0817234e-16
 7.0027411e-09 1.6765615e-18], sum to 1.0000
[2019-04-03 22:50:06,121] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9283
[2019-04-03 22:50:06,138] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 74.0, 0.0, 0.0, 26.0, 24.38281392330465, 0.07407070882936144, 0.0, 1.0, 41095.68782117176], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 697200.0000, 
sim time next is 697800.0000, 
raw observation next is [-3.4, 74.5, 0.0, 0.0, 26.0, 24.3430528569674, 0.07150186830395582, 0.0, 1.0, 41120.65659199796], 
processed observation next is [1.0, 0.043478260869565216, 0.368421052631579, 0.745, 0.0, 0.0, 0.6666666666666666, 0.5285877380806167, 0.5238339561013187, 0.0, 1.0, 0.19581265043808554], 
reward next is 0.8042, 
noisyNet noise sample is [array([-1.1785184], dtype=float32), 0.26931232]. 
=============================================
[2019-04-03 22:50:08,884] A3C_AGENT_WORKER-Thread-15 INFO:Local step 43500, global step 699392: loss 2.0451
[2019-04-03 22:50:08,890] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 43500, global step 699393: learning rate 0.0005
[2019-04-03 22:50:09,904] A3C_AGENT_WORKER-Thread-2 INFO:Local step 44000, global step 699644: loss 0.2152
[2019-04-03 22:50:09,937] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 44000, global step 699644: learning rate 0.0005
[2019-04-03 22:50:11,239] A3C_AGENT_WORKER-Thread-6 INFO:Evaluating...
[2019-04-03 22:50:11,257] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-03 22:50:11,258] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:50:11,260] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run8
[2019-04-03 22:50:11,278] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-03 22:50:11,279] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:50:11,280] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-03 22:50:11,281] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:50:11,284] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run8
[2019-04-03 22:50:11,303] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run8
[2019-04-03 22:50:41,391] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.51720667], dtype=float32), 0.20429952]
[2019-04-03 22:50:41,391] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-11.15, 60.16666666666666, 0.0, 0.0, 26.0, 25.14533252342734, 0.2744103501022547, 1.0, 1.0, 37718.2827324575]
[2019-04-03 22:50:41,392] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-03 22:50:41,393] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [6.6210906e-22 7.0581378e-14 3.0667518e-13 9.9999726e-01 3.0565512e-16
 2.7281999e-06 8.0094324e-17], sampled 0.6123942212954773
[2019-04-03 22:52:27,035] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.51720667], dtype=float32), 0.20429952]
[2019-04-03 22:52:27,035] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-8.731064217, 50.13483159, 0.0, 0.0, 26.0, 23.10114077252241, -0.150775827110701, 0.0, 1.0, 65004.64622992831]
[2019-04-03 22:52:27,035] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-03 22:52:27,036] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [6.5906611e-24 3.0603615e-17 2.0174296e-15 1.0000000e+00 2.4961656e-17
 9.5871566e-10 1.9709936e-18], sampled 0.9582121797431064
[2019-04-03 22:53:09,280] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.51720667], dtype=float32), 0.20429952]
[2019-04-03 22:53:09,280] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [0.6, 40.0, 0.0, 0.0, 26.0, 25.45969620052046, 0.4390874388651647, 0.0, 1.0, 40015.31869524262]
[2019-04-03 22:53:09,280] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-03 22:53:09,281] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [7.1272912e-24 6.6934550e-16 5.7634121e-15 1.0000000e+00 6.9129845e-18
 5.8470935e-09 2.4271766e-18], sampled 0.7644519251155337
[2019-04-03 22:53:14,539] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7359.5593 235359619.6358 1409.3041
[2019-04-03 22:53:23,302] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.51720667], dtype=float32), 0.20429952]
[2019-04-03 22:53:23,302] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [7.0, 54.5, 188.0, 717.0, 26.0, 25.36697929645097, 0.4401558196351656, 0.0, 1.0, 0.0]
[2019-04-03 22:53:23,302] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-03 22:53:23,303] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [8.8731781e-23 2.3699787e-15 3.3255533e-14 9.9999642e-01 3.9866887e-16
 3.5772466e-06 2.0841853e-17], sampled 0.6196794822399755
[2019-04-03 22:53:49,857] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7228.9719 260970436.2331 1495.2230
[2019-04-03 22:53:58,302] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7169.4638 273387153.0191 1172.3264
[2019-04-03 22:53:59,338] A3C_AGENT_WORKER-Thread-6 INFO:Global step: 700000, evaluation results [700000.0, 7228.971908494348, 260970436.23305014, 1495.2230207212087, 7359.559339848403, 235359619.63576445, 1409.3041125254701, 7169.46382758481, 273387153.0191391, 1172.3264023636013]
[2019-04-03 22:54:01,380] A3C_AGENT_WORKER-Thread-19 INFO:Local step 44000, global step 700418: loss 0.1163
[2019-04-03 22:54:01,381] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 44000, global step 700418: learning rate 0.0005
[2019-04-03 22:54:06,446] A3C_AGENT_WORKER-Thread-3 INFO:Local step 44000, global step 701893: loss 0.1651
[2019-04-03 22:54:06,448] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 44000, global step 701894: learning rate 0.0005
[2019-04-03 22:54:06,737] A3C_AGENT_WORKER-Thread-17 INFO:Local step 44000, global step 701976: loss 0.1681
[2019-04-03 22:54:06,739] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 44000, global step 701976: learning rate 0.0005
[2019-04-03 22:54:07,591] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.5908969e-23 1.1776925e-13 1.4350955e-12 9.9507684e-01 2.4585102e-16
 4.9231458e-03 9.9126492e-17], sum to 1.0000
[2019-04-03 22:54:07,591] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5299
[2019-04-03 22:54:07,655] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.899999999999999, 83.66666666666666, 57.33333333333334, 0.0, 26.0, 26.22302609389384, 0.4364208125237283, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 830400.0000, 
sim time next is 831000.0000, 
raw observation next is [-3.9, 84.83333333333334, 55.66666666666666, 0.0, 26.0, 26.194874862947, 0.3280999609066415, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3545706371191136, 0.8483333333333334, 0.18555555555555553, 0.0, 0.6666666666666666, 0.6829062385789166, 0.6093666536355472, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5134886], dtype=float32), 0.57105136]. 
=============================================
[2019-04-03 22:54:07,674] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[78.01895 ]
 [77.9551  ]
 [78.065155]
 [78.22033 ]
 [78.418915]], R is [[78.35555267]
 [78.5719986 ]
 [78.78627777]
 [78.99841309]
 [79.20842743]].
[2019-04-03 22:54:11,519] A3C_AGENT_WORKER-Thread-18 INFO:Local step 44000, global step 703539: loss 0.2514
[2019-04-03 22:54:11,524] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 44000, global step 703540: learning rate 0.0005
[2019-04-03 22:54:11,831] A3C_AGENT_WORKER-Thread-12 INFO:Local step 44000, global step 703649: loss 0.1482
[2019-04-03 22:54:11,832] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 44000, global step 703651: learning rate 0.0005
[2019-04-03 22:54:12,308] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.3156936e-26 3.6066322e-17 9.4145393e-16 9.9999952e-01 2.0623398e-17
 4.4777363e-07 9.7086273e-20], sum to 1.0000
[2019-04-03 22:54:12,311] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3087
[2019-04-03 22:54:12,335] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 83.0, 0.0, 0.0, 26.0, 24.86984010509781, 0.2585787491703808, 0.0, 1.0, 41533.97690843936], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 856200.0000, 
sim time next is 856800.0000, 
raw observation next is [-3.4, 83.0, 0.0, 0.0, 26.0, 24.84835818484279, 0.259232677848956, 0.0, 1.0, 41530.86709108786], 
processed observation next is [1.0, 0.9565217391304348, 0.368421052631579, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5706965154035659, 0.5864108926163186, 0.0, 1.0, 0.19776603376708504], 
reward next is 0.8022, 
noisyNet noise sample is [array([0.9040935], dtype=float32), 2.0831547]. 
=============================================
[2019-04-03 22:54:13,576] A3C_AGENT_WORKER-Thread-6 INFO:Local step 44000, global step 704246: loss 0.1728
[2019-04-03 22:54:13,587] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 44000, global step 704247: learning rate 0.0005
[2019-04-03 22:54:14,304] A3C_AGENT_WORKER-Thread-13 INFO:Local step 44000, global step 704539: loss 0.1082
[2019-04-03 22:54:14,305] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 44000, global step 704540: learning rate 0.0005
[2019-04-03 22:54:14,343] A3C_AGENT_WORKER-Thread-5 INFO:Local step 44000, global step 704558: loss 0.1238
[2019-04-03 22:54:14,343] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 44000, global step 704558: learning rate 0.0005
[2019-04-03 22:54:15,880] A3C_AGENT_WORKER-Thread-20 INFO:Local step 44000, global step 705300: loss 0.1502
[2019-04-03 22:54:15,880] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 44000, global step 705300: learning rate 0.0005
[2019-04-03 22:54:16,284] A3C_AGENT_WORKER-Thread-2 INFO:Local step 44500, global step 705488: loss 2.3495
[2019-04-03 22:54:16,287] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 44500, global step 705489: learning rate 0.0005
[2019-04-03 22:54:16,846] A3C_AGENT_WORKER-Thread-11 INFO:Local step 44000, global step 705760: loss 0.1728
[2019-04-03 22:54:16,848] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 44000, global step 705760: learning rate 0.0005
[2019-04-03 22:54:17,353] A3C_AGENT_WORKER-Thread-10 INFO:Local step 44000, global step 706000: loss 0.1381
[2019-04-03 22:54:17,354] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 44000, global step 706000: learning rate 0.0005
[2019-04-03 22:54:17,832] A3C_AGENT_WORKER-Thread-16 INFO:Local step 44000, global step 706221: loss 0.1860
[2019-04-03 22:54:17,849] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 44000, global step 706221: learning rate 0.0005
[2019-04-03 22:54:17,914] A3C_AGENT_WORKER-Thread-4 INFO:Local step 44000, global step 706256: loss 0.1371
[2019-04-03 22:54:17,916] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 44000, global step 706256: learning rate 0.0005
[2019-04-03 22:54:17,991] A3C_AGENT_WORKER-Thread-19 INFO:Local step 44500, global step 706292: loss 0.8051
[2019-04-03 22:54:17,992] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 44500, global step 706292: learning rate 0.0005
[2019-04-03 22:54:18,364] A3C_AGENT_WORKER-Thread-14 INFO:Local step 44000, global step 706498: loss 0.1966
[2019-04-03 22:54:18,368] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 44000, global step 706499: learning rate 0.0005
[2019-04-03 22:54:20,662] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.1101783e-29 2.7311985e-17 2.6918401e-18 9.9999988e-01 6.2919783e-22
 6.4283221e-08 4.0250326e-21], sum to 1.0000
[2019-04-03 22:54:20,672] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1790
[2019-04-03 22:54:20,685] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [15.68333333333333, 69.16666666666667, 207.3333333333333, 143.3333333333333, 26.0, 27.3716601937648, 0.9457053945890803, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1077000.0000, 
sim time next is 1077600.0000, 
raw observation next is [15.86666666666667, 68.33333333333334, 230.6666666666667, 179.1666666666667, 26.0, 27.45162807719935, 0.9743414184308768, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.9021237303785783, 0.6833333333333335, 0.7688888888888891, 0.19797421731123394, 0.6666666666666666, 0.7876356730999458, 0.8247804728102923, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7086121], dtype=float32), -0.92682606]. 
=============================================
[2019-04-03 22:54:22,012] A3C_AGENT_WORKER-Thread-3 INFO:Local step 44500, global step 708454: loss 0.3008
[2019-04-03 22:54:22,013] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 44500, global step 708454: learning rate 0.0005
[2019-04-03 22:54:22,461] A3C_AGENT_WORKER-Thread-17 INFO:Local step 44500, global step 708732: loss 0.0256
[2019-04-03 22:54:22,463] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 44500, global step 708733: learning rate 0.0005
[2019-04-03 22:54:22,654] A3C_AGENT_WORKER-Thread-15 INFO:Local step 44000, global step 708852: loss 0.0974
[2019-04-03 22:54:22,657] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 44000, global step 708853: learning rate 0.0005
[2019-04-03 22:54:25,795] A3C_AGENT_WORKER-Thread-12 INFO:Local step 44500, global step 710704: loss 0.0034
[2019-04-03 22:54:25,796] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 44500, global step 710704: learning rate 0.0005
[2019-04-03 22:54:25,852] A3C_AGENT_WORKER-Thread-18 INFO:Local step 44500, global step 710750: loss 0.0255
[2019-04-03 22:54:25,852] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 44500, global step 710750: learning rate 0.0005
[2019-04-03 22:54:27,498] A3C_AGENT_WORKER-Thread-6 INFO:Local step 44500, global step 711728: loss 0.0137
[2019-04-03 22:54:27,499] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 44500, global step 711728: learning rate 0.0005
[2019-04-03 22:54:28,041] A3C_AGENT_WORKER-Thread-5 INFO:Local step 44500, global step 712059: loss 0.0017
[2019-04-03 22:54:28,046] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 44500, global step 712059: learning rate 0.0005
[2019-04-03 22:54:28,432] A3C_AGENT_WORKER-Thread-13 INFO:Local step 44500, global step 712275: loss 0.0235
[2019-04-03 22:54:28,437] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 44500, global step 712280: learning rate 0.0005
[2019-04-03 22:54:28,703] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.6774035e-31 1.1542437e-20 1.5715382e-19 1.0000000e+00 4.8895896e-23
 4.2866372e-13 2.1028820e-23], sum to 1.0000
[2019-04-03 22:54:28,706] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3774
[2019-04-03 22:54:28,721] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [10.41666666666667, 77.33333333333334, 0.0, 0.0, 26.0, 25.65470152192082, 0.640819746084961, 0.0, 1.0, 22425.79362573681], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1127400.0000, 
sim time next is 1128000.0000, 
raw observation next is [10.33333333333333, 77.66666666666667, 0.0, 0.0, 26.0, 25.65589841161444, 0.6395954932398198, 0.0, 1.0, 22292.2641217096], 
processed observation next is [0.0, 0.043478260869565216, 0.7488457987072946, 0.7766666666666667, 0.0, 0.0, 0.6666666666666666, 0.6379915343012034, 0.7131984977466065, 0.0, 1.0, 0.10615363867480761], 
reward next is 0.8938, 
noisyNet noise sample is [array([0.6437151], dtype=float32), 0.81825906]. 
=============================================
[2019-04-03 22:54:28,738] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[97.66417 ]
 [97.58255 ]
 [97.368614]
 [97.13891 ]
 [96.85935 ]], R is [[97.55406189]
 [97.47173309]
 [97.37818146]
 [97.25400543]
 [97.08268738]].
[2019-04-03 22:54:29,890] A3C_AGENT_WORKER-Thread-20 INFO:Local step 44500, global step 713045: loss 0.0126
[2019-04-03 22:54:29,899] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 44500, global step 713045: learning rate 0.0005
[2019-04-03 22:54:30,702] A3C_AGENT_WORKER-Thread-11 INFO:Local step 44500, global step 713500: loss 0.1860
[2019-04-03 22:54:30,704] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 44500, global step 713501: learning rate 0.0005
[2019-04-03 22:54:31,787] A3C_AGENT_WORKER-Thread-10 INFO:Local step 44500, global step 714083: loss 0.0218
[2019-04-03 22:54:31,788] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 44500, global step 714084: learning rate 0.0005
[2019-04-03 22:54:31,866] A3C_AGENT_WORKER-Thread-16 INFO:Local step 44500, global step 714121: loss 0.0218
[2019-04-03 22:54:31,872] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 44500, global step 714124: learning rate 0.0005
[2019-04-03 22:54:32,188] A3C_AGENT_WORKER-Thread-4 INFO:Local step 44500, global step 714307: loss 0.0155
[2019-04-03 22:54:32,193] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 44500, global step 714313: learning rate 0.0005
[2019-04-03 22:54:32,689] A3C_AGENT_WORKER-Thread-14 INFO:Local step 44500, global step 714613: loss 0.1088
[2019-04-03 22:54:32,690] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 44500, global step 714614: learning rate 0.0005
[2019-04-03 22:54:32,960] A3C_AGENT_WORKER-Thread-2 INFO:Local step 45000, global step 714765: loss 3.2140
[2019-04-03 22:54:32,962] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 45000, global step 714766: learning rate 0.0005
[2019-04-03 22:54:35,437] A3C_AGENT_WORKER-Thread-19 INFO:Local step 45000, global step 715922: loss 2.7611
[2019-04-03 22:54:35,449] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 45000, global step 715922: learning rate 0.0005
[2019-04-03 22:54:36,817] A3C_AGENT_WORKER-Thread-15 INFO:Local step 44500, global step 716543: loss 0.6194
[2019-04-03 22:54:36,819] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 44500, global step 716546: learning rate 0.0005
[2019-04-03 22:54:39,341] A3C_AGENT_WORKER-Thread-3 INFO:Local step 45000, global step 717715: loss 1.7247
[2019-04-03 22:54:39,342] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 45000, global step 717715: learning rate 0.0005
[2019-04-03 22:54:39,683] A3C_AGENT_WORKER-Thread-17 INFO:Local step 45000, global step 717866: loss 1.9986
[2019-04-03 22:54:39,701] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 45000, global step 717868: learning rate 0.0005
[2019-04-03 22:54:40,712] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.4308138e-26 2.7464100e-17 1.4242395e-14 9.9999976e-01 3.2482177e-18
 2.1609024e-07 2.9434090e-19], sum to 1.0000
[2019-04-03 22:54:40,712] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1657
[2019-04-03 22:54:40,730] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 26.0, 25.33671525315955, 0.4937699568897219, 0.0, 1.0, 41260.86719922496], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1465200.0000, 
sim time next is 1465800.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 26.0, 25.38029791727462, 0.495481327903819, 0.0, 1.0, 34757.22208407836], 
processed observation next is [1.0, 1.0, 0.5069252077562327, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6150248264395518, 0.6651604426346064, 0.0, 1.0, 0.16551058135275412], 
reward next is 0.8345, 
noisyNet noise sample is [array([0.8431131], dtype=float32), 1.8505427]. 
=============================================
[2019-04-03 22:54:41,796] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.4281359e-28 8.7245292e-21 2.9706997e-17 1.0000000e+00 7.0539475e-21
 3.1527650e-10 7.3008004e-22], sum to 1.0000
[2019-04-03 22:54:41,799] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6034
[2019-04-03 22:54:41,819] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 100.0, 0.0, 0.0, 26.0, 25.4262089179365, 0.4147129642691805, 0.0, 1.0, 65040.17510693166], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1494000.0000, 
sim time next is 1494600.0000, 
raw observation next is [1.1, 100.0, 0.0, 0.0, 26.0, 25.25296378331603, 0.4115832144545852, 0.0, 1.0, 80798.36831577943], 
processed observation next is [1.0, 0.30434782608695654, 0.49307479224376743, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6044136486096692, 0.6371944048181951, 0.0, 1.0, 0.3847541348370449], 
reward next is 0.6152, 
noisyNet noise sample is [array([-1.1683034], dtype=float32), 0.5147298]. 
=============================================
[2019-04-03 22:54:43,244] A3C_AGENT_WORKER-Thread-12 INFO:Local step 45000, global step 719401: loss 0.7679
[2019-04-03 22:54:43,256] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 45000, global step 719401: learning rate 0.0005
[2019-04-03 22:54:43,550] A3C_AGENT_WORKER-Thread-18 INFO:Local step 45000, global step 719532: loss 1.0293
[2019-04-03 22:54:43,554] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 45000, global step 719532: learning rate 0.0005
[2019-04-03 22:54:44,438] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.0102305e-28 9.8591127e-20 2.1630383e-17 1.0000000e+00 5.0566200e-20
 2.9576952e-10 1.1292656e-21], sum to 1.0000
[2019-04-03 22:54:44,439] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0898
[2019-04-03 22:54:44,456] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.2, 96.0, 0.0, 0.0, 26.0, 25.37595913423902, 0.4502814894865206, 0.0, 1.0, 34695.61534494258], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1490400.0000, 
sim time next is 1491000.0000, 
raw observation next is [2.016666666666667, 96.66666666666666, 0.0, 0.0, 26.0, 25.37830974284343, 0.4579517465279218, 0.0, 1.0, 34540.65191090133], 
processed observation next is [1.0, 0.2608695652173913, 0.5184672206832872, 0.9666666666666666, 0.0, 0.0, 0.6666666666666666, 0.6148591452369526, 0.6526505821759739, 0.0, 1.0, 0.16447929481381587], 
reward next is 0.8355, 
noisyNet noise sample is [array([2.010204], dtype=float32), -1.3887109]. 
=============================================
[2019-04-03 22:54:44,477] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[84.35371 ]
 [84.376656]
 [84.35769 ]
 [84.34248 ]
 [84.31559 ]], R is [[84.33520508]
 [84.32663727]
 [84.30715942]
 [84.28825378]
 [84.26965332]].
[2019-04-03 22:54:45,166] A3C_AGENT_WORKER-Thread-6 INFO:Local step 45000, global step 720213: loss 0.1459
[2019-04-03 22:54:45,170] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 45000, global step 720214: learning rate 0.0005
[2019-04-03 22:54:45,947] A3C_AGENT_WORKER-Thread-13 INFO:Local step 45000, global step 720564: loss 0.0931
[2019-04-03 22:54:45,948] A3C_AGENT_WORKER-Thread-5 INFO:Local step 45000, global step 720564: loss 0.1178
[2019-04-03 22:54:45,949] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 45000, global step 720564: learning rate 0.0005
[2019-04-03 22:54:45,964] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 45000, global step 720564: learning rate 0.0005
[2019-04-03 22:54:47,840] A3C_AGENT_WORKER-Thread-20 INFO:Local step 45000, global step 721443: loss 0.1026
[2019-04-03 22:54:47,843] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 45000, global step 721444: learning rate 0.0005
[2019-04-03 22:54:48,379] A3C_AGENT_WORKER-Thread-11 INFO:Local step 45000, global step 721708: loss 0.0799
[2019-04-03 22:54:48,380] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 45000, global step 721708: learning rate 0.0005
[2019-04-03 22:54:49,938] A3C_AGENT_WORKER-Thread-4 INFO:Local step 45000, global step 722357: loss 0.1566
[2019-04-03 22:54:49,940] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 45000, global step 722357: learning rate 0.0005
[2019-04-03 22:54:50,138] A3C_AGENT_WORKER-Thread-10 INFO:Local step 45000, global step 722444: loss 0.0547
[2019-04-03 22:54:50,138] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 45000, global step 722444: learning rate 0.0005
[2019-04-03 22:54:50,154] A3C_AGENT_WORKER-Thread-16 INFO:Local step 45000, global step 722452: loss 0.0667
[2019-04-03 22:54:50,157] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 45000, global step 722452: learning rate 0.0005
[2019-04-03 22:54:50,812] A3C_AGENT_WORKER-Thread-14 INFO:Local step 45000, global step 722775: loss 0.3881
[2019-04-03 22:54:50,817] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 45000, global step 722775: learning rate 0.0005
[2019-04-03 22:54:51,261] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [9.9201231e-26 3.6369477e-16 1.0288429e-15 1.0000000e+00 1.4459199e-19
 3.5528480e-08 8.3054045e-20], sum to 1.0000
[2019-04-03 22:54:51,264] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6197
[2019-04-03 22:54:51,274] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.566666666666666, 71.0, 0.0, 0.0, 26.0, 25.60672281151183, 0.5632872270737693, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1540200.0000, 
sim time next is 1540800.0000, 
raw observation next is [7.2, 73.0, 0.0, 0.0, 26.0, 25.51563521704564, 0.5509638458890117, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.662049861495845, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6263029347538035, 0.6836546152963372, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.10303487], dtype=float32), 1.0826452]. 
=============================================
[2019-04-03 22:54:51,342] A3C_AGENT_WORKER-Thread-2 INFO:Local step 45500, global step 723055: loss 0.0192
[2019-04-03 22:54:51,347] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 45500, global step 723060: learning rate 0.0005
[2019-04-03 22:54:53,457] A3C_AGENT_WORKER-Thread-19 INFO:Local step 45500, global step 724100: loss 0.0017
[2019-04-03 22:54:53,461] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 45500, global step 724102: learning rate 0.0005
[2019-04-03 22:54:54,201] A3C_AGENT_WORKER-Thread-15 INFO:Local step 45000, global step 724393: loss 0.2700
[2019-04-03 22:54:54,203] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 45000, global step 724394: learning rate 0.0005
[2019-04-03 22:54:57,857] A3C_AGENT_WORKER-Thread-3 INFO:Local step 45500, global step 725912: loss 0.0424
[2019-04-03 22:54:57,858] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 45500, global step 725912: learning rate 0.0005
[2019-04-03 22:54:58,290] A3C_AGENT_WORKER-Thread-17 INFO:Local step 45500, global step 726092: loss 0.0087
[2019-04-03 22:54:58,291] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 45500, global step 726092: learning rate 0.0005
[2019-04-03 22:55:00,031] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0384694e-27 1.2607317e-17 5.7691255e-17 1.0000000e+00 5.0943309e-22
 7.2107555e-09 7.0911245e-21], sum to 1.0000
[2019-04-03 22:55:00,031] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2911
[2019-04-03 22:55:00,048] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [9.583333333333334, 65.16666666666666, 0.0, 0.0, 26.0, 26.24623792537372, 0.6894572160291667, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1623000.0000, 
sim time next is 1623600.0000, 
raw observation next is [9.4, 66.0, 0.0, 0.0, 26.0, 26.18610212556902, 0.6744621547608571, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.7229916897506927, 0.66, 0.0, 0.0, 0.6666666666666666, 0.6821751771307515, 0.724820718253619, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.1611226], dtype=float32), -0.9369088]. 
=============================================
[2019-04-03 22:55:01,581] A3C_AGENT_WORKER-Thread-12 INFO:Local step 45500, global step 727193: loss 0.0015
[2019-04-03 22:55:01,582] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 45500, global step 727194: learning rate 0.0005
[2019-04-03 22:55:02,185] A3C_AGENT_WORKER-Thread-18 INFO:Local step 45500, global step 727413: loss 0.0201
[2019-04-03 22:55:02,185] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 45500, global step 727413: learning rate 0.0005
[2019-04-03 22:55:04,803] A3C_AGENT_WORKER-Thread-6 INFO:Local step 45500, global step 728187: loss 0.0084
[2019-04-03 22:55:04,804] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 45500, global step 728188: learning rate 0.0005
[2019-04-03 22:55:05,212] A3C_AGENT_WORKER-Thread-13 INFO:Local step 45500, global step 728306: loss 0.0065
[2019-04-03 22:55:05,213] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 45500, global step 728306: learning rate 0.0005
[2019-04-03 22:55:05,937] A3C_AGENT_WORKER-Thread-5 INFO:Local step 45500, global step 728532: loss 0.0192
[2019-04-03 22:55:05,939] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 45500, global step 728532: learning rate 0.0005
[2019-04-03 22:55:07,296] A3C_AGENT_WORKER-Thread-20 INFO:Local step 45500, global step 728964: loss 0.0074
[2019-04-03 22:55:07,301] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 45500, global step 728967: learning rate 0.0005
[2019-04-03 22:55:07,772] A3C_AGENT_WORKER-Thread-11 INFO:Local step 45500, global step 729114: loss 0.0008
[2019-04-03 22:55:07,775] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 45500, global step 729116: learning rate 0.0005
[2019-04-03 22:55:08,621] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.7797804e-27 3.7592664e-20 6.5343803e-18 1.0000000e+00 1.6642816e-19
 1.8542362e-09 1.3195001e-21], sum to 1.0000
[2019-04-03 22:55:08,622] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4990
[2019-04-03 22:55:08,671] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.449999999999999, 77.0, 171.0, 236.0, 26.0, 25.83928132780191, 0.3359332508889716, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1938600.0000, 
sim time next is 1939200.0000, 
raw observation next is [-6.166666666666666, 76.33333333333334, 181.1666666666667, 198.3333333333333, 26.0, 25.81098786971707, 0.3239268626084162, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.29178208679593726, 0.7633333333333334, 0.603888888888889, 0.21915285451197047, 0.6666666666666666, 0.6509156558097559, 0.6079756208694721, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.14582844], dtype=float32), -0.84810376]. 
=============================================
[2019-04-03 22:55:09,107] A3C_AGENT_WORKER-Thread-4 INFO:Local step 45500, global step 729497: loss 0.0009
[2019-04-03 22:55:09,108] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 45500, global step 729497: learning rate 0.0005
[2019-04-03 22:55:09,632] A3C_AGENT_WORKER-Thread-16 INFO:Local step 45500, global step 729645: loss 0.0027
[2019-04-03 22:55:09,640] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 45500, global step 729645: learning rate 0.0005
[2019-04-03 22:55:10,197] A3C_AGENT_WORKER-Thread-14 INFO:Local step 45500, global step 729821: loss 0.0258
[2019-04-03 22:55:10,198] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 45500, global step 729821: learning rate 0.0005
[2019-04-03 22:55:10,390] A3C_AGENT_WORKER-Thread-10 INFO:Local step 45500, global step 729878: loss 0.0213
[2019-04-03 22:55:10,392] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 45500, global step 729878: learning rate 0.0005
[2019-04-03 22:55:14,704] A3C_AGENT_WORKER-Thread-15 INFO:Local step 45500, global step 731067: loss 0.0024
[2019-04-03 22:55:14,706] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 45500, global step 731067: learning rate 0.0005
[2019-04-03 22:55:18,331] A3C_AGENT_WORKER-Thread-2 INFO:Local step 46000, global step 732149: loss 0.2972
[2019-04-03 22:55:18,334] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 46000, global step 732150: learning rate 0.0005
[2019-04-03 22:55:19,417] A3C_AGENT_WORKER-Thread-19 INFO:Local step 46000, global step 732478: loss 0.2298
[2019-04-03 22:55:19,418] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 46000, global step 732478: learning rate 0.0005
[2019-04-03 22:55:20,029] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.1212901e-21 1.7527666e-13 5.5743973e-12 9.9923456e-01 2.8347877e-14
 7.6549221e-04 7.0070123e-16], sum to 1.0000
[2019-04-03 22:55:20,029] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1993
[2019-04-03 22:55:20,110] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.533333333333334, 64.0, 174.6666666666667, 128.5, 26.0, 25.87216148522423, 0.4060145721990606, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2118000.0000, 
sim time next is 2118600.0000, 
raw observation next is [-6.45, 64.0, 151.0, 134.0, 26.0, 25.74971491078458, 0.3664315245042631, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.28393351800554023, 0.64, 0.5033333333333333, 0.14806629834254142, 0.6666666666666666, 0.6458095758987149, 0.622143841501421, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9807215], dtype=float32), -0.7977593]. 
=============================================
[2019-04-03 22:55:22,486] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.0162073e-25 2.0753477e-16 3.1517605e-14 1.0000000e+00 4.1106641e-17
 3.7054569e-08 8.0804601e-19], sum to 1.0000
[2019-04-03 22:55:22,490] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2901
[2019-04-03 22:55:22,503] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 90.16666666666667, 0.0, 0.0, 26.0, 24.75038363389555, 0.261454514086353, 0.0, 1.0, 42669.89255888834], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2073000.0000, 
sim time next is 2073600.0000, 
raw observation next is [-4.5, 91.0, 0.0, 0.0, 26.0, 24.7126019570163, 0.2535785455148261, 0.0, 1.0, 42695.30203603007], 
processed observation next is [1.0, 0.0, 0.3379501385041552, 0.91, 0.0, 0.0, 0.6666666666666666, 0.559383496418025, 0.5845261818382753, 0.0, 1.0, 0.20331096207633367], 
reward next is 0.7967, 
noisyNet noise sample is [array([-0.5928057], dtype=float32), -0.7427895]. 
=============================================
[2019-04-03 22:55:24,469] A3C_AGENT_WORKER-Thread-3 INFO:Local step 46000, global step 733947: loss 0.4513
[2019-04-03 22:55:24,481] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 46000, global step 733952: learning rate 0.0005
[2019-04-03 22:55:25,525] A3C_AGENT_WORKER-Thread-17 INFO:Local step 46000, global step 734228: loss 0.5634
[2019-04-03 22:55:25,527] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 46000, global step 734228: learning rate 0.0005
[2019-04-03 22:55:28,750] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.3219951e-27 5.2305029e-21 6.0724309e-18 1.0000000e+00 3.2072633e-20
 1.0263279e-12 1.6811307e-22], sum to 1.0000
[2019-04-03 22:55:28,750] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2307
[2019-04-03 22:55:28,946] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.2, 87.0, 0.0, 0.0, 26.0, 24.05286590026068, 0.093496119109347, 1.0, 1.0, 202437.4169944721], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2013600.0000, 
sim time next is 2014200.0000, 
raw observation next is [-6.2, 87.0, 0.0, 0.0, 26.0, 24.30111870917892, 0.1887293359753923, 1.0, 1.0, 202283.7020673573], 
processed observation next is [1.0, 0.30434782608695654, 0.2908587257617729, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5250932257649099, 0.5629097786584641, 1.0, 1.0, 0.9632557241302729], 
reward next is 0.0367, 
noisyNet noise sample is [array([-0.8451737], dtype=float32), -0.8483055]. 
=============================================
[2019-04-03 22:55:29,729] A3C_AGENT_WORKER-Thread-12 INFO:Local step 46000, global step 735359: loss 0.7796
[2019-04-03 22:55:29,730] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 46000, global step 735359: learning rate 0.0005
[2019-04-03 22:55:30,595] A3C_AGENT_WORKER-Thread-18 INFO:Local step 46000, global step 735606: loss 1.1412
[2019-04-03 22:55:30,599] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 46000, global step 735606: learning rate 0.0005
[2019-04-03 22:55:32,118] A3C_AGENT_WORKER-Thread-6 INFO:Local step 46000, global step 736023: loss 1.0646
[2019-04-03 22:55:32,118] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 46000, global step 736023: learning rate 0.0005
[2019-04-03 22:55:32,144] A3C_AGENT_WORKER-Thread-13 INFO:Local step 46000, global step 736030: loss 1.0929
[2019-04-03 22:55:32,144] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 46000, global step 736030: learning rate 0.0005
[2019-04-03 22:55:33,537] A3C_AGENT_WORKER-Thread-5 INFO:Local step 46000, global step 736426: loss 1.1128
[2019-04-03 22:55:33,580] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 46000, global step 736426: learning rate 0.0005
[2019-04-03 22:55:35,394] A3C_AGENT_WORKER-Thread-20 INFO:Local step 46000, global step 736919: loss 1.1110
[2019-04-03 22:55:35,395] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 46000, global step 736919: learning rate 0.0005
[2019-04-03 22:55:35,439] A3C_AGENT_WORKER-Thread-11 INFO:Local step 46000, global step 736925: loss 1.2814
[2019-04-03 22:55:35,440] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 46000, global step 736925: learning rate 0.0005
[2019-04-03 22:55:36,172] A3C_AGENT_WORKER-Thread-4 INFO:Local step 46000, global step 737160: loss 1.0203
[2019-04-03 22:55:36,173] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 46000, global step 737160: learning rate 0.0005
[2019-04-03 22:55:37,778] A3C_AGENT_WORKER-Thread-14 INFO:Local step 46000, global step 737592: loss 0.5699
[2019-04-03 22:55:37,778] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 46000, global step 737592: learning rate 0.0005
[2019-04-03 22:55:37,864] A3C_AGENT_WORKER-Thread-16 INFO:Local step 46000, global step 737610: loss 0.4565
[2019-04-03 22:55:37,864] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 46000, global step 737610: learning rate 0.0005
[2019-04-03 22:55:38,511] A3C_AGENT_WORKER-Thread-10 INFO:Local step 46000, global step 737814: loss 0.3739
[2019-04-03 22:55:38,512] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 46000, global step 737814: learning rate 0.0005
[2019-04-03 22:55:42,531] A3C_AGENT_WORKER-Thread-15 INFO:Local step 46000, global step 738939: loss 0.1120
[2019-04-03 22:55:42,532] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 46000, global step 738939: learning rate 0.0005
[2019-04-03 22:55:46,991] A3C_AGENT_WORKER-Thread-2 INFO:Local step 46500, global step 740211: loss 0.0061
[2019-04-03 22:55:46,996] A3C_AGENT_WORKER-Thread-19 INFO:Local step 46500, global step 740214: loss 0.0103
[2019-04-03 22:55:46,996] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 46500, global step 740214: learning rate 0.0005
[2019-04-03 22:55:46,999] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 46500, global step 740211: learning rate 0.0005
[2019-04-03 22:55:51,945] A3C_AGENT_WORKER-Thread-3 INFO:Local step 46500, global step 741651: loss 0.0203
[2019-04-03 22:55:51,948] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 46500, global step 741652: learning rate 0.0005
[2019-04-03 22:55:52,804] A3C_AGENT_WORKER-Thread-17 INFO:Local step 46500, global step 741965: loss 0.0335
[2019-04-03 22:55:52,813] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 46500, global step 741965: learning rate 0.0005
[2019-04-03 22:55:53,058] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.0414833e-25 1.2408503e-16 1.1474245e-15 1.0000000e+00 1.3951781e-17
 1.4006275e-10 4.6920302e-20], sum to 1.0000
[2019-04-03 22:55:53,059] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4277
[2019-04-03 22:55:53,085] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.7, 54.66666666666667, 0.0, 0.0, 26.0, 25.45095838241011, 0.4287860241380402, 0.0, 1.0, 18759.94410681529], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2323200.0000, 
sim time next is 2323800.0000, 
raw observation next is [-1.7, 55.0, 0.0, 0.0, 26.0, 25.43324509670265, 0.4261532597734727, 0.0, 1.0, 32378.80125053494], 
processed observation next is [1.0, 0.9130434782608695, 0.4155124653739613, 0.55, 0.0, 0.0, 0.6666666666666666, 0.6194370913918874, 0.6420510865911576, 0.0, 1.0, 0.15418476785969018], 
reward next is 0.8458, 
noisyNet noise sample is [array([-0.73108596], dtype=float32), 0.7235791]. 
=============================================
[2019-04-03 22:55:58,011] A3C_AGENT_WORKER-Thread-12 INFO:Local step 46500, global step 743633: loss 0.0496
[2019-04-03 22:55:58,018] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 46500, global step 743634: learning rate 0.0005
[2019-04-03 22:55:58,634] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.81541762e-26 1.10865165e-17 6.94923763e-17 1.00000000e+00
 2.54114472e-20 7.94741280e-13 8.97685401e-21], sum to 1.0000
[2019-04-03 22:55:58,634] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9513
[2019-04-03 22:55:58,635] A3C_AGENT_WORKER-Thread-18 INFO:Local step 46500, global step 743832: loss 0.0948
[2019-04-03 22:55:58,638] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 46500, global step 743833: learning rate 0.0005
[2019-04-03 22:55:58,705] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.7333333333333335, 28.66666666666667, 0.0, 0.0, 26.0, 24.91526233761005, 0.2153368896514626, 0.0, 1.0, 32968.582386511], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2485200.0000, 
sim time next is 2485800.0000, 
raw observation next is [0.55, 29.0, 0.0, 0.0, 26.0, 24.9117031541434, 0.213407718819245, 0.0, 1.0, 38752.04568804013], 
processed observation next is [0.0, 0.782608695652174, 0.4778393351800555, 0.29, 0.0, 0.0, 0.6666666666666666, 0.5759752628452833, 0.5711359062730816, 0.0, 1.0, 0.1845335508954292], 
reward next is 0.8155, 
noisyNet noise sample is [array([-0.4117507], dtype=float32), -0.024395652]. 
=============================================
[2019-04-03 22:56:00,195] A3C_AGENT_WORKER-Thread-13 INFO:Local step 46500, global step 744359: loss 0.0846
[2019-04-03 22:56:00,201] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 46500, global step 744361: learning rate 0.0005
[2019-04-03 22:56:00,748] A3C_AGENT_WORKER-Thread-6 INFO:Local step 46500, global step 744569: loss 0.0410
[2019-04-03 22:56:00,749] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 46500, global step 744569: learning rate 0.0005
[2019-04-03 22:56:00,848] A3C_AGENT_WORKER-Thread-5 INFO:Local step 46500, global step 744605: loss 0.0586
[2019-04-03 22:56:00,848] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 46500, global step 744605: learning rate 0.0005
[2019-04-03 22:56:02,557] A3C_AGENT_WORKER-Thread-11 INFO:Local step 46500, global step 745125: loss 0.1094
[2019-04-03 22:56:02,560] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 46500, global step 745125: learning rate 0.0005
[2019-04-03 22:56:02,643] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.9159977e-25 2.3084853e-19 1.4730508e-17 1.0000000e+00 4.0849322e-18
 5.7226473e-13 1.0267549e-19], sum to 1.0000
[2019-04-03 22:56:02,644] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6086
[2019-04-03 22:56:02,656] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-9.100000000000001, 60.33333333333334, 0.0, 0.0, 26.0, 23.15253598678981, -0.1725266463584578, 0.0, 1.0, 44042.2259947432], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2442000.0000, 
sim time next is 2442600.0000, 
raw observation next is [-9.2, 60.5, 0.0, 0.0, 26.0, 23.11108005430267, -0.1809694816249503, 0.0, 1.0, 44035.50381956014], 
processed observation next is [0.0, 0.2608695652173913, 0.20775623268698065, 0.605, 0.0, 0.0, 0.6666666666666666, 0.4259233378585557, 0.4396768394583499, 0.0, 1.0, 0.20969287533123876], 
reward next is 0.7903, 
noisyNet noise sample is [array([0.19957288], dtype=float32), 1.2522901]. 
=============================================
[2019-04-03 22:56:03,329] A3C_AGENT_WORKER-Thread-20 INFO:Local step 46500, global step 745366: loss 0.1556
[2019-04-03 22:56:03,329] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 46500, global step 745366: learning rate 0.0005
[2019-04-03 22:56:03,710] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.1645259e-28 2.3420483e-20 1.0048625e-18 1.0000000e+00 2.7145651e-20
 5.0158847e-15 3.5869629e-22], sum to 1.0000
[2019-04-03 22:56:03,710] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2092
[2019-04-03 22:56:03,730] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.9, 65.66666666666667, 0.0, 0.0, 26.0, 24.44316325324081, 0.166942955948844, 0.0, 1.0, 40315.05458562005], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2347800.0000, 
sim time next is 2348400.0000, 
raw observation next is [-3.0, 66.33333333333334, 0.0, 0.0, 26.0, 24.41813296025172, 0.1585319609674286, 0.0, 1.0, 40425.84924445701], 
processed observation next is [0.0, 0.17391304347826086, 0.3795013850415513, 0.6633333333333334, 0.0, 0.0, 0.6666666666666666, 0.5348444133543101, 0.5528439869891429, 0.0, 1.0, 0.19250404402122384], 
reward next is 0.8075, 
noisyNet noise sample is [array([-0.4298216], dtype=float32), 1.1416506]. 
=============================================
[2019-04-03 22:56:03,736] A3C_AGENT_WORKER-Thread-4 INFO:Local step 46500, global step 745509: loss 0.1018
[2019-04-03 22:56:03,738] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 46500, global step 745509: learning rate 0.0005
[2019-04-03 22:56:04,839] A3C_AGENT_WORKER-Thread-16 INFO:Local step 46500, global step 745851: loss 0.0729
[2019-04-03 22:56:04,840] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 46500, global step 745851: learning rate 0.0005
[2019-04-03 22:56:05,536] A3C_AGENT_WORKER-Thread-14 INFO:Local step 46500, global step 746123: loss 0.1002
[2019-04-03 22:56:05,539] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 46500, global step 746123: learning rate 0.0005
[2019-04-03 22:56:05,750] A3C_AGENT_WORKER-Thread-10 INFO:Local step 46500, global step 746216: loss 0.1014
[2019-04-03 22:56:05,753] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 46500, global step 746217: learning rate 0.0005
[2019-04-03 22:56:09,203] A3C_AGENT_WORKER-Thread-15 INFO:Local step 46500, global step 747446: loss 0.0510
[2019-04-03 22:56:09,204] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 46500, global step 747446: learning rate 0.0005
[2019-04-03 22:56:10,347] A3C_AGENT_WORKER-Thread-19 INFO:Local step 47000, global step 747780: loss 0.0505
[2019-04-03 22:56:10,348] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 47000, global step 747780: learning rate 0.0005
[2019-04-03 22:56:10,510] A3C_AGENT_WORKER-Thread-2 INFO:Local step 47000, global step 747844: loss 0.0734
[2019-04-03 22:56:10,511] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 47000, global step 747844: learning rate 0.0005
[2019-04-03 22:56:15,521] A3C_AGENT_WORKER-Thread-3 INFO:Local step 47000, global step 749748: loss 0.0362
[2019-04-03 22:56:15,522] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 47000, global step 749748: learning rate 0.0005
[2019-04-03 22:56:15,921] A3C_AGENT_WORKER-Thread-17 INFO:Local step 47000, global step 749896: loss 0.0202
[2019-04-03 22:56:15,922] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 47000, global step 749896: learning rate 0.0005
[2019-04-03 22:56:16,225] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1832399e-27 9.8133360e-20 1.1016240e-17 1.0000000e+00 1.1267796e-19
 8.7504500e-12 1.6505978e-21], sum to 1.0000
[2019-04-03 22:56:16,227] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5106
[2019-04-03 22:56:16,255] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.533333333333333, 58.0, 0.0, 0.0, 26.0, 25.21889126767941, 0.3322949960773215, 0.0, 1.0, 48795.46530949503], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2587200.0000, 
sim time next is 2587800.0000, 
raw observation next is [-3.716666666666666, 58.5, 0.0, 0.0, 26.0, 25.1474720140686, 0.3210569876976701, 0.0, 1.0, 44451.82146726354], 
processed observation next is [1.0, 0.9565217391304348, 0.3596491228070176, 0.585, 0.0, 0.0, 0.6666666666666666, 0.59562266783905, 0.6070189958992234, 0.0, 1.0, 0.21167534032030255], 
reward next is 0.7883, 
noisyNet noise sample is [array([1.4528673], dtype=float32), -0.56995016]. 
=============================================
[2019-04-03 22:56:18,522] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.8681719e-26 1.4234066e-17 2.7456263e-16 1.0000000e+00 1.0540797e-18
 8.3921767e-12 3.6292071e-20], sum to 1.0000
[2019-04-03 22:56:18,522] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9454
[2019-04-03 22:56:18,563] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 62.0, 0.0, 0.0, 26.0, 24.80764554861884, 0.2567176544113166, 0.0, 1.0, 41900.12414415574], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2592000.0000, 
sim time next is 2592600.0000, 
raw observation next is [-4.583333333333333, 63.0, 0.0, 0.0, 26.0, 24.77133209608879, 0.248105161860391, 0.0, 1.0, 41908.12939594275], 
processed observation next is [1.0, 0.0, 0.3356417359187443, 0.63, 0.0, 0.0, 0.6666666666666666, 0.564277674674066, 0.5827017206201304, 0.0, 1.0, 0.1995625209330607], 
reward next is 0.8004, 
noisyNet noise sample is [array([0.14602274], dtype=float32), -0.8992844]. 
=============================================
[2019-04-03 22:56:20,876] A3C_AGENT_WORKER-Thread-12 INFO:Local step 47000, global step 751635: loss 0.3390
[2019-04-03 22:56:20,877] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 47000, global step 751635: learning rate 0.0005
[2019-04-03 22:56:21,148] A3C_AGENT_WORKER-Thread-18 INFO:Local step 47000, global step 751737: loss 0.3695
[2019-04-03 22:56:21,149] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 47000, global step 751738: learning rate 0.0005
[2019-04-03 22:56:22,508] A3C_AGENT_WORKER-Thread-6 INFO:Local step 47000, global step 752177: loss 0.2771
[2019-04-03 22:56:22,508] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 47000, global step 752177: learning rate 0.0005
[2019-04-03 22:56:22,678] A3C_AGENT_WORKER-Thread-5 INFO:Local step 47000, global step 752234: loss 0.2468
[2019-04-03 22:56:22,679] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 47000, global step 752234: learning rate 0.0005
[2019-04-03 22:56:22,852] A3C_AGENT_WORKER-Thread-13 INFO:Local step 47000, global step 752299: loss 0.2826
[2019-04-03 22:56:22,864] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 47000, global step 752299: learning rate 0.0005
[2019-04-03 22:56:24,403] A3C_AGENT_WORKER-Thread-11 INFO:Local step 47000, global step 752836: loss 0.4719
[2019-04-03 22:56:24,403] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 47000, global step 752836: learning rate 0.0005
[2019-04-03 22:56:25,675] A3C_AGENT_WORKER-Thread-4 INFO:Local step 47000, global step 753253: loss 0.1679
[2019-04-03 22:56:25,676] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 47000, global step 753253: learning rate 0.0005
[2019-04-03 22:56:25,681] A3C_AGENT_WORKER-Thread-20 INFO:Local step 47000, global step 753253: loss 0.1730
[2019-04-03 22:56:25,681] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 47000, global step 753253: learning rate 0.0005
[2019-04-03 22:56:26,498] A3C_AGENT_WORKER-Thread-16 INFO:Local step 47000, global step 753530: loss 0.0655
[2019-04-03 22:56:26,502] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 47000, global step 753530: learning rate 0.0005
[2019-04-03 22:56:26,999] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.5837179e-24 2.7596024e-16 2.0252020e-15 1.0000000e+00 3.5142272e-17
 2.0633907e-10 5.9951378e-19], sum to 1.0000
[2019-04-03 22:56:27,000] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8040
[2019-04-03 22:56:27,045] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 25.34743593533036, 0.4393966039963999, 0.0, 1.0, 48972.43941769113], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2757600.0000, 
sim time next is 2758200.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 25.34191298315631, 0.3884565503683787, 0.0, 1.0, 48232.50064192586], 
processed observation next is [1.0, 0.9565217391304348, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.6118260819296925, 0.6294855167894595, 0.0, 1.0, 0.22967857448536122], 
reward next is 0.7703, 
noisyNet noise sample is [array([-0.42543426], dtype=float32), 1.1117215]. 
=============================================
[2019-04-03 22:56:27,817] A3C_AGENT_WORKER-Thread-10 INFO:Local step 47000, global step 753964: loss 0.0137
[2019-04-03 22:56:27,817] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 47000, global step 753964: learning rate 0.0005
[2019-04-03 22:56:28,134] A3C_AGENT_WORKER-Thread-14 INFO:Local step 47000, global step 754087: loss 0.0077
[2019-04-03 22:56:28,136] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 47000, global step 754087: learning rate 0.0005
[2019-04-03 22:56:31,954] A3C_AGENT_WORKER-Thread-15 INFO:Local step 47000, global step 755395: loss 0.1275
[2019-04-03 22:56:31,954] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 47000, global step 755395: learning rate 0.0005
[2019-04-03 22:56:33,820] A3C_AGENT_WORKER-Thread-2 INFO:Local step 47500, global step 756054: loss 0.0952
[2019-04-03 22:56:33,821] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 47500, global step 756054: learning rate 0.0005
[2019-04-03 22:56:33,974] A3C_AGENT_WORKER-Thread-19 INFO:Local step 47500, global step 756115: loss 0.1249
[2019-04-03 22:56:33,975] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 47500, global step 756116: learning rate 0.0005
[2019-04-03 22:56:36,303] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.6729387e-26 2.4712287e-17 3.0264863e-16 1.0000000e+00 1.1026992e-19
 2.9446685e-11 1.0064882e-20], sum to 1.0000
[2019-04-03 22:56:36,303] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9994
[2019-04-03 22:56:36,358] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 37.0, 0.0, 0.0, 26.0, 24.55321864216611, 0.3453586265639562, 1.0, 1.0, 87657.80989389439], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2833200.0000, 
sim time next is 2833800.0000, 
raw observation next is [2.833333333333333, 38.16666666666667, 0.0, 0.0, 26.0, 24.96554934042344, 0.3923605142906356, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.541089566020314, 0.3816666666666667, 0.0, 0.0, 0.6666666666666666, 0.5804624450352867, 0.6307868380968785, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5810232], dtype=float32), 0.35282487]. 
=============================================
[2019-04-03 22:56:36,849] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.6151993e-28 5.2755200e-17 2.7788814e-16 1.0000000e+00 5.3420956e-21
 3.0123841e-11 2.0475523e-21], sum to 1.0000
[2019-04-03 22:56:36,850] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3880
[2019-04-03 22:56:36,861] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.666666666666667, 31.16666666666666, 0.0, 0.0, 26.0, 25.50534489025992, 0.3254469943276991, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2830200.0000, 
sim time next is 2830800.0000, 
raw observation next is [4.333333333333334, 32.33333333333334, 0.0, 0.0, 26.0, 25.31933782187528, 0.287464119002619, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.58264081255771, 0.3233333333333334, 0.0, 0.0, 0.6666666666666666, 0.6099448184896067, 0.595821373000873, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.574339], dtype=float32), -1.9937183]. 
=============================================
[2019-04-03 22:56:38,846] A3C_AGENT_WORKER-Thread-17 INFO:Local step 47500, global step 757854: loss 0.0848
[2019-04-03 22:56:38,846] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 47500, global step 757854: learning rate 0.0005
[2019-04-03 22:56:38,974] A3C_AGENT_WORKER-Thread-3 INFO:Local step 47500, global step 757914: loss 0.0900
[2019-04-03 22:56:38,974] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 47500, global step 757914: learning rate 0.0005
[2019-04-03 22:56:43,759] A3C_AGENT_WORKER-Thread-12 INFO:Local step 47500, global step 759615: loss 0.1950
[2019-04-03 22:56:43,762] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 47500, global step 759615: learning rate 0.0005
[2019-04-03 22:56:44,280] A3C_AGENT_WORKER-Thread-18 INFO:Local step 47500, global step 759824: loss 0.0538
[2019-04-03 22:56:44,287] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 47500, global step 759824: learning rate 0.0005
[2019-04-03 22:56:45,324] A3C_AGENT_WORKER-Thread-5 INFO:Local step 47500, global step 760261: loss 0.2085
[2019-04-03 22:56:45,325] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 47500, global step 760261: learning rate 0.0005
[2019-04-03 22:56:46,191] A3C_AGENT_WORKER-Thread-6 INFO:Local step 47500, global step 760608: loss 0.0407
[2019-04-03 22:56:46,206] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 47500, global step 760612: learning rate 0.0005
[2019-04-03 22:56:46,293] A3C_AGENT_WORKER-Thread-13 INFO:Local step 47500, global step 760650: loss 0.0756
[2019-04-03 22:56:46,301] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 47500, global step 760650: learning rate 0.0005
[2019-04-03 22:56:47,083] A3C_AGENT_WORKER-Thread-11 INFO:Local step 47500, global step 760910: loss 0.0791
[2019-04-03 22:56:47,083] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 47500, global step 760910: learning rate 0.0005
[2019-04-03 22:56:48,459] A3C_AGENT_WORKER-Thread-20 INFO:Local step 47500, global step 761394: loss 0.1267
[2019-04-03 22:56:48,459] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 47500, global step 761394: learning rate 0.0005
[2019-04-03 22:56:49,577] A3C_AGENT_WORKER-Thread-4 INFO:Local step 47500, global step 761803: loss 0.2117
[2019-04-03 22:56:49,578] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 47500, global step 761803: learning rate 0.0005
[2019-04-03 22:56:49,714] A3C_AGENT_WORKER-Thread-16 INFO:Local step 47500, global step 761842: loss 0.1599
[2019-04-03 22:56:49,715] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 47500, global step 761842: learning rate 0.0005
[2019-04-03 22:56:50,667] A3C_AGENT_WORKER-Thread-10 INFO:Local step 47500, global step 762234: loss 0.2079
[2019-04-03 22:56:50,668] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 47500, global step 762234: learning rate 0.0005
[2019-04-03 22:56:51,510] A3C_AGENT_WORKER-Thread-14 INFO:Local step 47500, global step 762604: loss 0.2623
[2019-04-03 22:56:51,511] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 47500, global step 762604: learning rate 0.0005
[2019-04-03 22:56:51,840] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.70107619e-28 1.13177988e-21 1.14097158e-18 1.00000000e+00
 9.96231854e-21 1.00920935e-14 1.92786713e-22], sum to 1.0000
[2019-04-03 22:56:51,841] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8437
[2019-04-03 22:56:51,857] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.36670218595718, 0.3480575752609659, 0.0, 1.0, 48956.21108721245], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3100200.0000, 
sim time next is 3100800.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.39003839911619, 0.3485164691714762, 0.0, 1.0, 34245.66181769907], 
processed observation next is [0.0, 0.9130434782608695, 0.4349030470914128, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6158365332596825, 0.616172156390492, 0.0, 1.0, 0.16307458008428127], 
reward next is 0.8369, 
noisyNet noise sample is [array([0.04204731], dtype=float32), 0.17837566]. 
=============================================
[2019-04-03 22:56:52,718] A3C_AGENT_WORKER-Thread-2 INFO:Local step 48000, global step 763061: loss 4.0867
[2019-04-03 22:56:52,718] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 48000, global step 763061: learning rate 0.0005
[2019-04-03 22:56:52,978] A3C_AGENT_WORKER-Thread-19 INFO:Local step 48000, global step 763144: loss 4.2522
[2019-04-03 22:56:52,978] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 48000, global step 763144: learning rate 0.0005
[2019-04-03 22:56:55,330] A3C_AGENT_WORKER-Thread-15 INFO:Local step 47500, global step 764135: loss 0.4888
[2019-04-03 22:56:55,331] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 47500, global step 764135: learning rate 0.0005
[2019-04-03 22:56:57,168] A3C_AGENT_WORKER-Thread-3 INFO:Local step 48000, global step 765068: loss 5.9169
[2019-04-03 22:56:57,168] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 48000, global step 765068: learning rate 0.0005
[2019-04-03 22:56:57,942] A3C_AGENT_WORKER-Thread-17 INFO:Local step 48000, global step 765439: loss 6.7135
[2019-04-03 22:56:57,943] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 48000, global step 765439: learning rate 0.0005
[2019-04-03 22:57:02,116] A3C_AGENT_WORKER-Thread-18 INFO:Local step 48000, global step 767381: loss 7.2284
[2019-04-03 22:57:02,117] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 48000, global step 767381: learning rate 0.0005
[2019-04-03 22:57:02,166] A3C_AGENT_WORKER-Thread-12 INFO:Local step 48000, global step 767399: loss 6.7222
[2019-04-03 22:57:02,167] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 48000, global step 767399: learning rate 0.0005
[2019-04-03 22:57:03,698] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.7356438e-26 2.5506234e-20 2.2839092e-17 1.0000000e+00 1.3228637e-18
 3.8894374e-12 5.5731112e-21], sum to 1.0000
[2019-04-03 22:57:03,699] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3375
[2019-04-03 22:57:03,714] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.5, 65.5, 99.0, 670.0, 26.0, 25.72885990402821, 0.4989136201036552, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3490200.0000, 
sim time next is 3490800.0000, 
raw observation next is [-0.3333333333333334, 63.66666666666667, 100.6666666666667, 686.6666666666667, 26.0, 25.8535564286286, 0.5189151029978557, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4533702677747, 0.6366666666666667, 0.33555555555555566, 0.7587476979742174, 0.6666666666666666, 0.6544630357190501, 0.6729717009992853, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2122697], dtype=float32), 0.12483807]. 
=============================================
[2019-04-03 22:57:04,183] A3C_AGENT_WORKER-Thread-6 INFO:Local step 48000, global step 768290: loss 8.7617
[2019-04-03 22:57:04,183] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 48000, global step 768290: learning rate 0.0005
[2019-04-03 22:57:04,265] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.7826887e-27 2.5339150e-19 8.4555077e-17 1.0000000e+00 1.3636368e-20
 2.2550174e-12 3.7557813e-21], sum to 1.0000
[2019-04-03 22:57:04,266] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4270
[2019-04-03 22:57:04,306] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 67.0, 0.0, 0.0, 26.0, 25.95134669317657, 0.6289514285448968, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3266400.0000, 
sim time next is 3267000.0000, 
raw observation next is [-4.0, 68.0, 0.0, 0.0, 26.0, 25.91476753665826, 0.614971040289605, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.3518005540166205, 0.68, 0.0, 0.0, 0.6666666666666666, 0.6595639613881884, 0.7049903467632017, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5592659], dtype=float32), -0.9700341]. 
=============================================
[2019-04-03 22:57:04,317] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[83.43887 ]
 [84.0933  ]
 [83.98819 ]
 [83.91457 ]
 [83.969826]], R is [[84.00352478]
 [84.1634903 ]
 [84.32185364]
 [84.4786377 ]
 [84.6338501 ]].
[2019-04-03 22:57:04,376] A3C_AGENT_WORKER-Thread-5 INFO:Local step 48000, global step 768364: loss 9.4466
[2019-04-03 22:57:04,379] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 48000, global step 768365: learning rate 0.0005
[2019-04-03 22:57:04,616] A3C_AGENT_WORKER-Thread-13 INFO:Local step 48000, global step 768458: loss 8.9066
[2019-04-03 22:57:04,617] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 48000, global step 768458: learning rate 0.0005
[2019-04-03 22:57:04,866] A3C_AGENT_WORKER-Thread-11 INFO:Local step 48000, global step 768552: loss 9.0341
[2019-04-03 22:57:04,868] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 48000, global step 768552: learning rate 0.0005
[2019-04-03 22:57:05,657] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.1047211e-25 7.2085771e-18 1.3125247e-17 1.0000000e+00 2.2206342e-18
 9.7557562e-10 9.3893465e-20], sum to 1.0000
[2019-04-03 22:57:05,661] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9678
[2019-04-03 22:57:05,669] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 52.0, 114.0, 800.0, 26.0, 26.00731204516251, 0.5800940228917711, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3331800.0000, 
sim time next is 3332400.0000, 
raw observation next is [-4.333333333333334, 51.33333333333333, 112.6666666666667, 792.0, 26.0, 26.08312623881513, 0.590154928700187, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.3425669436749769, 0.5133333333333333, 0.37555555555555564, 0.8751381215469614, 0.6666666666666666, 0.6735938532345941, 0.696718309566729, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.08051741], dtype=float32), 0.95353127]. 
=============================================
[2019-04-03 22:57:07,229] A3C_AGENT_WORKER-Thread-20 INFO:Local step 48000, global step 769649: loss 10.5451
[2019-04-03 22:57:07,230] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 48000, global step 769649: learning rate 0.0005
[2019-04-03 22:57:07,607] A3C_AGENT_WORKER-Thread-4 INFO:Local step 48000, global step 769787: loss 10.0258
[2019-04-03 22:57:07,609] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 48000, global step 769787: learning rate 0.0005
[2019-04-03 22:57:07,610] A3C_AGENT_WORKER-Thread-16 INFO:Local step 48000, global step 769787: loss 9.8892
[2019-04-03 22:57:07,611] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 48000, global step 769788: learning rate 0.0005
[2019-04-03 22:57:08,183] A3C_AGENT_WORKER-Thread-10 INFO:Local step 48000, global step 769986: loss 10.4543
[2019-04-03 22:57:08,183] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 48000, global step 769986: learning rate 0.0005
[2019-04-03 22:57:09,054] A3C_AGENT_WORKER-Thread-14 INFO:Local step 48000, global step 770356: loss 12.4884
[2019-04-03 22:57:09,055] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 48000, global step 770356: learning rate 0.0005
[2019-04-03 22:57:09,728] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0254160e-27 3.0402747e-19 1.2953832e-18 1.0000000e+00 9.9930010e-20
 2.0282107e-10 1.1339493e-21], sum to 1.0000
[2019-04-03 22:57:09,731] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3838
[2019-04-03 22:57:09,780] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.166666666666667, 50.66666666666667, 111.3333333333333, 784.0, 26.0, 26.15877037233783, 0.4927555264195094, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3333000.0000, 
sim time next is 3333600.0000, 
raw observation next is [-4.0, 50.0, 110.0, 776.0, 26.0, 25.7305586714503, 0.5233010737950768, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3518005540166205, 0.5, 0.36666666666666664, 0.8574585635359117, 0.6666666666666666, 0.6442132226208583, 0.6744336912650256, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.67519915], dtype=float32), -0.83024865]. 
=============================================
[2019-04-03 22:57:11,051] A3C_AGENT_WORKER-Thread-2 INFO:Local step 48500, global step 771202: loss 0.8514
[2019-04-03 22:57:11,052] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 48500, global step 771202: learning rate 0.0005
[2019-04-03 22:57:11,093] A3C_AGENT_WORKER-Thread-19 INFO:Local step 48500, global step 771219: loss 0.5480
[2019-04-03 22:57:11,094] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 48500, global step 771220: learning rate 0.0005
[2019-04-03 22:57:13,178] A3C_AGENT_WORKER-Thread-15 INFO:Local step 48000, global step 772226: loss 10.0386
[2019-04-03 22:57:13,178] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 48000, global step 772226: learning rate 0.0005
[2019-04-03 22:57:14,984] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.1532458e-28 3.0870316e-20 1.0565877e-18 1.0000000e+00 2.6281364e-21
 2.5400766e-14 6.8339699e-22], sum to 1.0000
[2019-04-03 22:57:14,984] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9176
[2019-04-03 22:57:15,016] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.5, 40.5, 14.0, 142.0, 26.0, 25.10458275603628, 0.3792704649334275, 0.0, 1.0, 49637.38112478337], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3605400.0000, 
sim time next is 3606000.0000, 
raw observation next is [-0.6666666666666666, 41.0, 11.66666666666667, 118.3333333333333, 26.0, 25.10097488788444, 0.3765450366601972, 0.0, 1.0, 39768.1762208002], 
processed observation next is [0.0, 0.7391304347826086, 0.44413665743305636, 0.41, 0.038888888888888896, 0.1307550644567219, 0.6666666666666666, 0.5917479073237034, 0.6255150122200658, 0.0, 1.0, 0.18937226771809618], 
reward next is 0.8106, 
noisyNet noise sample is [array([-0.60847634], dtype=float32), 0.6588446]. 
=============================================
[2019-04-03 22:57:15,027] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[85.24973 ]
 [85.374504]
 [85.4325  ]
 [85.588234]
 [85.923004]], R is [[85.34835052]
 [85.25849915]
 [85.14929199]
 [85.09799194]
 [85.15344238]].
[2019-04-03 22:57:15,503] A3C_AGENT_WORKER-Thread-3 INFO:Local step 48500, global step 773373: loss 0.0850
[2019-04-03 22:57:15,504] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 48500, global step 773374: learning rate 0.0005
[2019-04-03 22:57:15,902] A3C_AGENT_WORKER-Thread-17 INFO:Local step 48500, global step 773588: loss 0.0195
[2019-04-03 22:57:15,904] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 48500, global step 773588: learning rate 0.0005
[2019-04-03 22:57:20,074] A3C_AGENT_WORKER-Thread-18 INFO:Local step 48500, global step 775686: loss 0.0829
[2019-04-03 22:57:20,075] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 48500, global step 775686: learning rate 0.0005
[2019-04-03 22:57:20,137] A3C_AGENT_WORKER-Thread-12 INFO:Local step 48500, global step 775711: loss 0.0297
[2019-04-03 22:57:20,138] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 48500, global step 775711: learning rate 0.0005
[2019-04-03 22:57:21,629] A3C_AGENT_WORKER-Thread-13 INFO:Local step 48500, global step 776494: loss 0.0038
[2019-04-03 22:57:21,630] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 48500, global step 776494: learning rate 0.0005
[2019-04-03 22:57:21,691] A3C_AGENT_WORKER-Thread-5 INFO:Local step 48500, global step 776525: loss 0.0310
[2019-04-03 22:57:21,692] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 48500, global step 776525: learning rate 0.0005
[2019-04-03 22:57:22,007] A3C_AGENT_WORKER-Thread-6 INFO:Local step 48500, global step 776659: loss 0.0034
[2019-04-03 22:57:22,020] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 48500, global step 776665: learning rate 0.0005
[2019-04-03 22:57:22,412] A3C_AGENT_WORKER-Thread-11 INFO:Local step 48500, global step 776835: loss 0.0031
[2019-04-03 22:57:22,414] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 48500, global step 776835: learning rate 0.0005
[2019-04-03 22:57:24,116] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.3886689e-28 1.1027918e-19 1.3239585e-19 1.0000000e+00 3.7616496e-19
 3.0647487e-13 1.9411439e-21], sum to 1.0000
[2019-04-03 22:57:24,116] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7113
[2019-04-03 22:57:24,133] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 43.0, 74.5, 607.0, 26.0, 25.33864374351925, 0.4605405376960127, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3600000.0000, 
sim time next is 3600600.0000, 
raw observation next is [0.0, 42.33333333333334, 70.66666666666667, 576.3333333333333, 26.0, 25.32320203718857, 0.4531633754611172, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.46260387811634357, 0.42333333333333345, 0.23555555555555557, 0.6368324125230201, 0.6666666666666666, 0.6102668364323808, 0.6510544584870391, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1582605], dtype=float32), -1.0399128]. 
=============================================
[2019-04-03 22:57:24,444] A3C_AGENT_WORKER-Thread-20 INFO:Local step 48500, global step 777921: loss 0.1757
[2019-04-03 22:57:24,445] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 48500, global step 777922: learning rate 0.0005
[2019-04-03 22:57:24,883] A3C_AGENT_WORKER-Thread-4 INFO:Local step 48500, global step 778160: loss 0.2990
[2019-04-03 22:57:24,884] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 48500, global step 778160: learning rate 0.0005
[2019-04-03 22:57:24,932] A3C_AGENT_WORKER-Thread-16 INFO:Local step 48500, global step 778186: loss 0.1506
[2019-04-03 22:57:24,933] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 48500, global step 778186: learning rate 0.0005
[2019-04-03 22:57:25,581] A3C_AGENT_WORKER-Thread-10 INFO:Local step 48500, global step 778555: loss 0.3044
[2019-04-03 22:57:25,583] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 48500, global step 778557: learning rate 0.0005
[2019-04-03 22:57:25,762] A3C_AGENT_WORKER-Thread-2 INFO:Local step 49000, global step 778658: loss 0.8236
[2019-04-03 22:57:25,763] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 49000, global step 778658: learning rate 0.0005
[2019-04-03 22:57:26,127] A3C_AGENT_WORKER-Thread-19 INFO:Local step 49000, global step 778853: loss 0.3507
[2019-04-03 22:57:26,127] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 49000, global step 778853: learning rate 0.0005
[2019-04-03 22:57:26,336] A3C_AGENT_WORKER-Thread-14 INFO:Local step 48500, global step 778947: loss 0.3643
[2019-04-03 22:57:26,337] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 48500, global step 778947: learning rate 0.0005
[2019-04-03 22:57:30,065] A3C_AGENT_WORKER-Thread-3 INFO:Local step 49000, global step 780954: loss 0.4321
[2019-04-03 22:57:30,067] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 49000, global step 780956: learning rate 0.0005
[2019-04-03 22:57:30,270] A3C_AGENT_WORKER-Thread-15 INFO:Local step 48500, global step 781058: loss 0.3940
[2019-04-03 22:57:30,272] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 48500, global step 781059: learning rate 0.0005
[2019-04-03 22:57:30,729] A3C_AGENT_WORKER-Thread-17 INFO:Local step 49000, global step 781252: loss 0.6247
[2019-04-03 22:57:30,730] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 49000, global step 781252: learning rate 0.0005
[2019-04-03 22:57:34,758] A3C_AGENT_WORKER-Thread-18 INFO:Local step 49000, global step 783228: loss 1.1258
[2019-04-03 22:57:34,759] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 49000, global step 783229: learning rate 0.0005
[2019-04-03 22:57:35,131] A3C_AGENT_WORKER-Thread-12 INFO:Local step 49000, global step 783416: loss 0.8515
[2019-04-03 22:57:35,132] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 49000, global step 783416: learning rate 0.0005
[2019-04-03 22:57:36,238] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [9.9649102e-27 9.8402511e-20 1.2706821e-18 1.0000000e+00 1.0087762e-18
 3.6151969e-12 3.5235762e-21], sum to 1.0000
[2019-04-03 22:57:36,238] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7802
[2019-04-03 22:57:36,262] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 60.0, 105.5, 727.5, 26.0, 26.42944267183988, 0.5905577107490517, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3837600.0000, 
sim time next is 3838200.0000, 
raw observation next is [-1.833333333333333, 60.0, 107.0, 743.3333333333334, 26.0, 26.46872246411919, 0.6025009803710657, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.41181902123730385, 0.6, 0.3566666666666667, 0.8213627992633518, 0.6666666666666666, 0.7057268720099324, 0.7008336601236885, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.07742754], dtype=float32), -0.2072429]. 
=============================================
[2019-04-03 22:57:36,326] A3C_AGENT_WORKER-Thread-13 INFO:Local step 49000, global step 783970: loss 0.5625
[2019-04-03 22:57:36,327] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 49000, global step 783970: learning rate 0.0005
[2019-04-03 22:57:36,572] A3C_AGENT_WORKER-Thread-5 INFO:Local step 49000, global step 784079: loss 0.7849
[2019-04-03 22:57:36,573] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 49000, global step 784080: learning rate 0.0005
[2019-04-03 22:57:36,855] A3C_AGENT_WORKER-Thread-11 INFO:Local step 49000, global step 784191: loss 0.8560
[2019-04-03 22:57:36,859] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 49000, global step 784192: learning rate 0.0005
[2019-04-03 22:57:37,234] A3C_AGENT_WORKER-Thread-6 INFO:Local step 49000, global step 784322: loss 0.7044
[2019-04-03 22:57:37,236] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 49000, global step 784322: learning rate 0.0005
[2019-04-03 22:57:37,867] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.1368085e-29 5.1393274e-20 5.3657924e-19 1.0000000e+00 2.9054664e-21
 5.6105945e-12 6.2060912e-22], sum to 1.0000
[2019-04-03 22:57:37,867] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9737
[2019-04-03 22:57:37,878] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.833333333333333, 45.5, 79.33333333333334, 661.6666666666667, 26.0, 26.78203515236208, 0.7386266030426003, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3858600.0000, 
sim time next is 3859200.0000, 
raw observation next is [3.0, 45.0, 75.5, 634.0, 26.0, 26.88199264882173, 0.7516221461508409, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.5457063711911359, 0.45, 0.25166666666666665, 0.7005524861878453, 0.6666666666666666, 0.7401660540684775, 0.7505407153836137, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7855049], dtype=float32), 0.0284884]. 
=============================================
[2019-04-03 22:57:38,628] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.8269980e-29 9.0007241e-20 6.4621707e-18 1.0000000e+00 1.0833307e-22
 6.4235353e-14 6.7219873e-23], sum to 1.0000
[2019-04-03 22:57:38,630] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6786
[2019-04-03 22:57:38,639] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 51.0, 0.0, 0.0, 26.0, 25.2831277321565, 0.4555308898888602, 0.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3873000.0000, 
sim time next is 3873600.0000, 
raw observation next is [1.0, 51.0, 0.0, 0.0, 26.0, 25.1979581204959, 0.4429932410235324, 0.0, 1.0, 23973.79659860912], 
processed observation next is [1.0, 0.8695652173913043, 0.4903047091412743, 0.51, 0.0, 0.0, 0.6666666666666666, 0.5998298433746584, 0.6476644136745108, 0.0, 1.0, 0.11416093618385295], 
reward next is 0.8858, 
noisyNet noise sample is [array([0.19949146], dtype=float32), 1.3681006]. 
=============================================
[2019-04-03 22:57:39,015] A3C_AGENT_WORKER-Thread-20 INFO:Local step 49000, global step 785173: loss 0.5510
[2019-04-03 22:57:39,016] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 49000, global step 785173: learning rate 0.0005
[2019-04-03 22:57:39,406] A3C_AGENT_WORKER-Thread-16 INFO:Local step 49000, global step 785362: loss 0.4875
[2019-04-03 22:57:39,407] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 49000, global step 785362: learning rate 0.0005
[2019-04-03 22:57:39,894] A3C_AGENT_WORKER-Thread-4 INFO:Local step 49000, global step 785587: loss 0.7835
[2019-04-03 22:57:39,894] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 49000, global step 785587: learning rate 0.0005
[2019-04-03 22:57:40,570] A3C_AGENT_WORKER-Thread-10 INFO:Local step 49000, global step 785899: loss 0.6519
[2019-04-03 22:57:40,572] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 49000, global step 785900: learning rate 0.0005
[2019-04-03 22:57:41,286] A3C_AGENT_WORKER-Thread-14 INFO:Local step 49000, global step 786240: loss 0.4934
[2019-04-03 22:57:41,288] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 49000, global step 786240: learning rate 0.0005
[2019-04-03 22:57:43,413] A3C_AGENT_WORKER-Thread-19 INFO:Local step 49500, global step 787157: loss 0.1433
[2019-04-03 22:57:43,414] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 49500, global step 787158: learning rate 0.0005
[2019-04-03 22:57:43,525] A3C_AGENT_WORKER-Thread-2 INFO:Local step 49500, global step 787209: loss 0.1070
[2019-04-03 22:57:43,526] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 49500, global step 787209: learning rate 0.0005
[2019-04-03 22:57:44,484] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.5666109e-28 2.5006482e-22 9.4324620e-20 1.0000000e+00 2.9811490e-21
 2.6812481e-15 1.6722786e-22], sum to 1.0000
[2019-04-03 22:57:44,485] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6504
[2019-04-03 22:57:44,504] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 46.0, 0.0, 0.0, 26.0, 25.34541635651365, 0.3996379031691915, 0.0, 1.0, 39390.8635816282], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4154400.0000, 
sim time next is 4155000.0000, 
raw observation next is [-2.166666666666667, 46.66666666666667, 0.0, 0.0, 26.0, 25.33302192933593, 0.3945189323199019, 0.0, 1.0, 39282.69007175465], 
processed observation next is [0.0, 0.08695652173913043, 0.4025854108956602, 0.46666666666666673, 0.0, 0.0, 0.6666666666666666, 0.6110851607779942, 0.6315063107733007, 0.0, 1.0, 0.1870604289131174], 
reward next is 0.8129, 
noisyNet noise sample is [array([0.84508073], dtype=float32), 0.13264202]. 
=============================================
[2019-04-03 22:57:44,508] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[86.0492 ]
 [86.0069 ]
 [85.88069]
 [85.86188]
 [85.8281 ]], R is [[86.01438141]
 [85.96665955]
 [85.91690063]
 [85.85909271]
 [85.77774811]].
[2019-04-03 22:57:45,146] A3C_AGENT_WORKER-Thread-15 INFO:Local step 49000, global step 788071: loss 0.6160
[2019-04-03 22:57:45,147] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 49000, global step 788071: learning rate 0.0005
[2019-04-03 22:57:47,827] A3C_AGENT_WORKER-Thread-3 INFO:Local step 49500, global step 789181: loss 0.1057
[2019-04-03 22:57:47,832] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 49500, global step 789181: learning rate 0.0005
[2019-04-03 22:57:48,785] A3C_AGENT_WORKER-Thread-17 INFO:Local step 49500, global step 789407: loss 0.1147
[2019-04-03 22:57:48,786] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 49500, global step 789407: learning rate 0.0005
[2019-04-03 22:57:57,263] A3C_AGENT_WORKER-Thread-18 INFO:Local step 49500, global step 791484: loss 0.3867
[2019-04-03 22:57:57,263] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 49500, global step 791484: learning rate 0.0005
[2019-04-03 22:57:57,720] A3C_AGENT_WORKER-Thread-12 INFO:Local step 49500, global step 791599: loss 0.3862
[2019-04-03 22:57:57,720] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 49500, global step 791599: learning rate 0.0005
[2019-04-03 22:58:00,723] A3C_AGENT_WORKER-Thread-5 INFO:Local step 49500, global step 792359: loss 0.3395
[2019-04-03 22:58:00,724] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 49500, global step 792359: learning rate 0.0005
[2019-04-03 22:58:00,784] A3C_AGENT_WORKER-Thread-13 INFO:Local step 49500, global step 792374: loss 0.3511
[2019-04-03 22:58:00,785] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 49500, global step 792374: learning rate 0.0005
[2019-04-03 22:58:01,279] A3C_AGENT_WORKER-Thread-11 INFO:Local step 49500, global step 792505: loss 0.1594
[2019-04-03 22:58:01,280] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 49500, global step 792505: learning rate 0.0005
[2019-04-03 22:58:02,159] A3C_AGENT_WORKER-Thread-6 INFO:Local step 49500, global step 792747: loss 0.2355
[2019-04-03 22:58:02,175] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 49500, global step 792754: learning rate 0.0005
[2019-04-03 22:58:04,142] A3C_AGENT_WORKER-Thread-16 INFO:Local step 49500, global step 793334: loss 0.2881
[2019-04-03 22:58:04,147] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 49500, global step 793334: learning rate 0.0005
[2019-04-03 22:58:04,483] A3C_AGENT_WORKER-Thread-20 INFO:Local step 49500, global step 793427: loss 0.2599
[2019-04-03 22:58:04,501] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 49500, global step 793429: learning rate 0.0005
[2019-04-03 22:58:06,782] A3C_AGENT_WORKER-Thread-4 INFO:Local step 49500, global step 794168: loss 0.9464
[2019-04-03 22:58:06,785] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 49500, global step 794168: learning rate 0.0005
[2019-04-03 22:58:07,654] A3C_AGENT_WORKER-Thread-10 INFO:Local step 49500, global step 794475: loss 0.3824
[2019-04-03 22:58:07,656] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 49500, global step 794475: learning rate 0.0005
[2019-04-03 22:58:08,545] A3C_AGENT_WORKER-Thread-14 INFO:Local step 49500, global step 794783: loss 0.3189
[2019-04-03 22:58:08,546] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 49500, global step 794783: learning rate 0.0005
[2019-04-03 22:58:09,208] A3C_AGENT_WORKER-Thread-2 INFO:Local step 50000, global step 795038: loss 0.0017
[2019-04-03 22:58:09,240] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 50000, global step 795043: learning rate 0.0005
[2019-04-03 22:58:09,265] A3C_AGENT_WORKER-Thread-19 INFO:Local step 50000, global step 795051: loss 0.0023
[2019-04-03 22:58:09,266] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 50000, global step 795051: learning rate 0.0005
[2019-04-03 22:58:13,936] A3C_AGENT_WORKER-Thread-15 INFO:Local step 49500, global step 796930: loss 0.2066
[2019-04-03 22:58:13,957] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 49500, global step 796930: learning rate 0.0005
[2019-04-03 22:58:14,393] A3C_AGENT_WORKER-Thread-3 INFO:Local step 50000, global step 797118: loss 0.0089
[2019-04-03 22:58:14,394] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 50000, global step 797118: learning rate 0.0005
[2019-04-03 22:58:14,984] A3C_AGENT_WORKER-Thread-17 INFO:Local step 50000, global step 797399: loss 0.0072
[2019-04-03 22:58:14,986] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 50000, global step 797399: learning rate 0.0005
[2019-04-03 22:58:19,055] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.5400563e-30 9.5486382e-20 6.0341419e-19 1.0000000e+00 2.4832508e-23
 1.7247805e-14 5.3290230e-23], sum to 1.0000
[2019-04-03 22:58:19,056] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1646
[2019-04-03 22:58:19,072] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.066666666666666, 64.33333333333333, 0.0, 0.0, 26.0, 25.85538967610545, 0.6385826736591409, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4408800.0000, 
sim time next is 4409400.0000, 
raw observation next is [6.933333333333334, 64.66666666666667, 0.0, 0.0, 26.0, 25.91611135331454, 0.6324677099559919, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.6546629732225301, 0.6466666666666667, 0.0, 0.0, 0.6666666666666666, 0.659675946109545, 0.7108225699853307, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5575352], dtype=float32), 0.15924269]. 
=============================================
[2019-04-03 22:58:19,630] A3C_AGENT_WORKER-Thread-18 INFO:Local step 50000, global step 799460: loss 0.0108
[2019-04-03 22:58:19,631] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 50000, global step 799460: learning rate 0.0005
[2019-04-03 22:58:19,701] A3C_AGENT_WORKER-Thread-12 INFO:Local step 50000, global step 799491: loss 0.0048
[2019-04-03 22:58:19,703] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 50000, global step 799491: learning rate 0.0005
[2019-04-03 22:58:20,888] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-03 22:58:20,888] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-03 22:58:20,889] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-03 22:58:20,889] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:58:20,889] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:58:20,889] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-03 22:58:20,890] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:58:20,892] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run9
[2019-04-03 22:58:20,938] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run9
[2019-04-03 22:58:20,953] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run9
[2019-04-03 22:59:16,287] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.28002673], dtype=float32), 0.30232474]
[2019-04-03 22:59:16,287] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [14.75, 79.0, 199.0, 241.0, 21.0, 22.08324581094454, -0.3653160712098409, 1.0, 1.0, 0.0]
[2019-04-03 22:59:16,287] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-03 22:59:16,288] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.8259187e-18 3.3589750e-10 1.2695725e-10 9.9999583e-01 1.5616623e-15
 4.2259881e-06 4.2357231e-14], sampled 0.0660516253910084
[2019-04-03 22:59:19,657] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.28002673], dtype=float32), 0.30232474]
[2019-04-03 22:59:19,658] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [0.5590822086666669, 83.97810307666667, 0.0, 0.0, 23.0, 22.56588180772466, -0.09071985490877066, 0.0, 1.0, 53813.18715867736]
[2019-04-03 22:59:19,658] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-03 22:59:19,659] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.5872626e-21 2.9981968e-15 8.5848754e-14 1.0000000e+00 2.8080151e-16
 4.7688780e-11 1.3867527e-16], sampled 0.8345617471745429
[2019-04-03 22:59:27,200] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.28002673], dtype=float32), 0.30232474]
[2019-04-03 22:59:27,201] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [6.95, 80.5, 118.0, 129.0, 21.0, 21.01543191606948, -0.5083350143574279, 1.0, 1.0, 0.0]
[2019-04-03 22:59:27,201] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-03 22:59:27,202] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [5.6670604e-19 2.4264666e-12 8.3247402e-12 1.0000000e+00 2.5339975e-15
 1.0473153e-08 1.1745100e-14], sampled 0.3304020192604685
[2019-04-03 23:00:46,395] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6510.4705 169251062.9958 -1226.7725
[2019-04-03 23:01:13,935] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7112.9325 211016951.5158 -1043.6062
[2019-04-03 23:01:31,416] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7327.1006 236908090.8879 -507.6366
[2019-04-03 23:01:32,449] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 800000, evaluation results [800000.0, 7112.932537655771, 211016951.5157941, -1043.6062490436727, 6510.470470541113, 169251062.99576482, -1226.7725274704069, 7327.100625568107, 236908090.88787648, -507.63660435551765]
[2019-04-03 23:01:32,873] A3C_AGENT_WORKER-Thread-5 INFO:Local step 50000, global step 800119: loss 0.0027
[2019-04-03 23:01:32,874] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 50000, global step 800119: learning rate 0.0005
[2019-04-03 23:01:33,357] A3C_AGENT_WORKER-Thread-11 INFO:Local step 50000, global step 800267: loss 0.0043
[2019-04-03 23:01:33,358] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 50000, global step 800267: learning rate 0.0005
[2019-04-03 23:01:33,408] A3C_AGENT_WORKER-Thread-13 INFO:Local step 50000, global step 800286: loss 0.0066
[2019-04-03 23:01:33,409] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 50000, global step 800286: learning rate 0.0005
[2019-04-03 23:01:33,788] A3C_AGENT_WORKER-Thread-6 INFO:Local step 50000, global step 800390: loss 0.0008
[2019-04-03 23:01:33,792] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 50000, global step 800392: learning rate 0.0005
[2019-04-03 23:01:34,208] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.4584048e-28 4.4627751e-20 5.9522540e-18 1.0000000e+00 8.0727759e-22
 5.7075364e-14 6.2754872e-22], sum to 1.0000
[2019-04-03 23:01:34,208] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7020
[2019-04-03 23:01:34,256] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.50651641730828, 0.5399567429496556, 0.0, 1.0, 57733.41067442153], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4485600.0000, 
sim time next is 4486200.0000, 
raw observation next is [-0.05, 72.0, 0.0, 0.0, 26.0, 25.46482812868489, 0.4939577663897255, 0.0, 1.0, 71029.81838568079], 
processed observation next is [1.0, 0.9565217391304348, 0.461218836565097, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6220690107237408, 0.6646525887965752, 0.0, 1.0, 0.33823723040800374], 
reward next is 0.6618, 
noisyNet noise sample is [array([0.7404657], dtype=float32), 1.6116118]. 
=============================================
[2019-04-03 23:01:35,682] A3C_AGENT_WORKER-Thread-16 INFO:Local step 50000, global step 800938: loss 0.0461
[2019-04-03 23:01:35,683] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 50000, global step 800938: learning rate 0.0005
[2019-04-03 23:01:36,474] A3C_AGENT_WORKER-Thread-20 INFO:Local step 50000, global step 801191: loss 0.0007
[2019-04-03 23:01:36,474] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 50000, global step 801191: learning rate 0.0005
[2019-04-03 23:01:38,783] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.0808625e-26 5.9139847e-17 2.7023377e-16 1.0000000e+00 1.7877617e-20
 2.3818560e-11 2.4369154e-20], sum to 1.0000
[2019-04-03 23:01:38,783] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6695
[2019-04-03 23:01:38,794] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [9.25, 61.16666666666667, 0.0, 0.0, 26.0, 26.46563550036304, 0.713958128419176, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4399800.0000, 
sim time next is 4400400.0000, 
raw observation next is [9.100000000000001, 61.33333333333334, 0.0, 0.0, 26.0, 26.38835660910224, 0.6857329586658599, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.7146814404432135, 0.6133333333333334, 0.0, 0.0, 0.6666666666666666, 0.6990297174251866, 0.72857765288862, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6372158], dtype=float32), -0.7993374]. 
=============================================
[2019-04-03 23:01:38,871] A3C_AGENT_WORKER-Thread-4 INFO:Local step 50000, global step 801910: loss 0.0062
[2019-04-03 23:01:38,873] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 50000, global step 801910: learning rate 0.0005
[2019-04-03 23:01:40,873] A3C_AGENT_WORKER-Thread-14 INFO:Local step 50000, global step 802568: loss 0.0014
[2019-04-03 23:01:40,874] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 50000, global step 802568: learning rate 0.0005
[2019-04-03 23:01:40,966] A3C_AGENT_WORKER-Thread-10 INFO:Local step 50000, global step 802599: loss 0.0016
[2019-04-03 23:01:40,973] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 50000, global step 802599: learning rate 0.0005
[2019-04-03 23:01:43,539] A3C_AGENT_WORKER-Thread-2 INFO:Local step 50500, global step 803356: loss 0.0750
[2019-04-03 23:01:43,541] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 50500, global step 803356: learning rate 0.0005
[2019-04-03 23:01:44,260] A3C_AGENT_WORKER-Thread-19 INFO:Local step 50500, global step 803573: loss 0.0285
[2019-04-03 23:01:44,264] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 50500, global step 803573: learning rate 0.0005
[2019-04-03 23:01:44,419] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3131741e-27 1.7385806e-17 3.9724894e-18 1.0000000e+00 1.6746606e-21
 9.1489660e-10 3.0368275e-22], sum to 1.0000
[2019-04-03 23:01:44,446] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3240
[2019-04-03 23:01:44,490] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 45.66666666666666, 227.0, 79.33333333333334, 26.0, 25.7875087907377, 0.5513497903786471, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4546200.0000, 
sim time next is 4546800.0000, 
raw observation next is [3.0, 45.0, 208.5, 62.5, 26.0, 26.09368686547731, 0.5745135348272981, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5457063711911359, 0.45, 0.695, 0.06906077348066299, 0.6666666666666666, 0.6744739054564425, 0.6915045116090993, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5895364], dtype=float32), -0.9936455]. 
=============================================
[2019-04-03 23:01:47,555] A3C_AGENT_WORKER-Thread-15 INFO:Local step 50000, global step 804738: loss 0.0146
[2019-04-03 23:01:47,556] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 50000, global step 804738: learning rate 0.0005
[2019-04-03 23:01:49,953] A3C_AGENT_WORKER-Thread-17 INFO:Local step 50500, global step 805410: loss 0.0331
[2019-04-03 23:01:49,957] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 50500, global step 805410: learning rate 0.0005
[2019-04-03 23:01:50,071] A3C_AGENT_WORKER-Thread-3 INFO:Local step 50500, global step 805455: loss 0.0548
[2019-04-03 23:01:50,073] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 50500, global step 805457: learning rate 0.0005
[2019-04-03 23:01:55,322] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.1481232e-30 3.6298838e-21 1.3575908e-19 1.0000000e+00 1.5469596e-21
 1.7151684e-14 6.0697754e-23], sum to 1.0000
[2019-04-03 23:01:55,322] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9982
[2019-04-03 23:01:55,436] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.5, 48.0, 0.0, 0.0, 26.0, 25.15232040329835, 0.2651870237055191, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4951800.0000, 
sim time next is 4952400.0000, 
raw observation next is [-2.333333333333333, 47.33333333333333, 15.5, 93.33333333333331, 26.0, 25.34936202382598, 0.267117904247595, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.3979686057248385, 0.4733333333333333, 0.051666666666666666, 0.1031307550644567, 0.6666666666666666, 0.6124468353188316, 0.589039301415865, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.00797432], dtype=float32), -0.5414616]. 
=============================================
[2019-04-03 23:01:55,591] A3C_AGENT_WORKER-Thread-18 INFO:Local step 50500, global step 807327: loss 0.0640
[2019-04-03 23:01:55,592] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 50500, global step 807327: learning rate 0.0005
[2019-04-03 23:01:56,775] A3C_AGENT_WORKER-Thread-12 INFO:Local step 50500, global step 807683: loss 0.0346
[2019-04-03 23:01:56,775] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 50500, global step 807683: learning rate 0.0005
[2019-04-03 23:01:57,862] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.29434819e-28 1.24450745e-20 2.00245227e-18 1.00000000e+00
 7.57710719e-22 1.88280538e-15 1.44692728e-22], sum to 1.0000
[2019-04-03 23:01:57,862] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9726
[2019-04-03 23:01:57,907] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 41.16666666666666, 0.0, 0.0, 26.0, 25.27871108055382, 0.3857572302721464, 0.0, 1.0, 53624.95091012948], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4909800.0000, 
sim time next is 4910400.0000, 
raw observation next is [1.0, 40.0, 0.0, 0.0, 26.0, 25.38131387520803, 0.398311076198298, 0.0, 1.0, 38890.34052124318], 
processed observation next is [0.0, 0.8695652173913043, 0.4903047091412743, 0.4, 0.0, 0.0, 0.6666666666666666, 0.6151094896006691, 0.632770358732766, 0.0, 1.0, 0.18519209772020565], 
reward next is 0.8148, 
noisyNet noise sample is [array([-0.05625343], dtype=float32), -1.7333796]. 
=============================================
[2019-04-03 23:01:58,532] A3C_AGENT_WORKER-Thread-5 INFO:Local step 50500, global step 808287: loss 0.0315
[2019-04-03 23:01:58,534] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 50500, global step 808288: learning rate 0.0005
[2019-04-03 23:01:58,827] A3C_AGENT_WORKER-Thread-13 INFO:Local step 50500, global step 808410: loss 0.0479
[2019-04-03 23:01:58,828] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 50500, global step 808410: learning rate 0.0005
[2019-04-03 23:01:59,037] A3C_AGENT_WORKER-Thread-6 INFO:Local step 50500, global step 808483: loss 0.0366
[2019-04-03 23:01:59,038] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 50500, global step 808483: learning rate 0.0005
[2019-04-03 23:01:59,072] A3C_AGENT_WORKER-Thread-11 INFO:Local step 50500, global step 808497: loss 0.0392
[2019-04-03 23:01:59,102] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 50500, global step 808498: learning rate 0.0005
[2019-04-03 23:02:00,592] A3C_AGENT_WORKER-Thread-16 INFO:Local step 50500, global step 808999: loss 0.0337
[2019-04-03 23:02:00,601] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 50500, global step 809006: learning rate 0.0005
[2019-04-03 23:02:01,596] A3C_AGENT_WORKER-Thread-20 INFO:Local step 50500, global step 809353: loss 0.0304
[2019-04-03 23:02:01,596] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 50500, global step 809353: learning rate 0.0005
[2019-04-03 23:02:03,311] A3C_AGENT_WORKER-Thread-4 INFO:Local step 50500, global step 809889: loss 0.0620
[2019-04-03 23:02:03,322] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 50500, global step 809890: learning rate 0.0005
[2019-04-03 23:02:05,628] A3C_AGENT_WORKER-Thread-14 INFO:Local step 50500, global step 810721: loss 0.0512
[2019-04-03 23:02:05,629] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 50500, global step 810721: learning rate 0.0005
[2019-04-03 23:02:05,634] A3C_AGENT_WORKER-Thread-10 INFO:Local step 50500, global step 810723: loss 0.0492
[2019-04-03 23:02:05,635] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 50500, global step 810723: learning rate 0.0005
[2019-04-03 23:02:06,922] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:02:06,922] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:02:06,989] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run7
[2019-04-03 23:02:07,071] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:02:07,071] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:02:07,076] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run7
[2019-04-03 23:02:11,450] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:02:11,450] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:02:11,454] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run7
[2019-04-03 23:02:11,706] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0285658e-27 1.1589039e-18 1.1930176e-17 1.0000000e+00 6.9699046e-21
 1.7096013e-12 1.1065622e-21], sum to 1.0000
[2019-04-03 23:02:11,706] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4699
[2019-04-03 23:02:11,714] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 45.0, 175.1666666666667, 414.0, 26.0, 25.09314565611526, 0.3761313852596898, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4893600.0000, 
sim time next is 4894200.0000, 
raw observation next is [3.0, 45.0, 163.0, 422.0, 26.0, 25.10473499169836, 0.3770541051040698, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.5457063711911359, 0.45, 0.5433333333333333, 0.4662983425414365, 0.6666666666666666, 0.5920612493081968, 0.6256847017013566, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7179668], dtype=float32), -0.48557997]. 
=============================================
[2019-04-03 23:02:11,872] A3C_AGENT_WORKER-Thread-15 INFO:Local step 50500, global step 812631: loss 0.0308
[2019-04-03 23:02:11,883] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 50500, global step 812631: learning rate 0.0005
[2019-04-03 23:02:12,402] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:02:12,403] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:02:12,424] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run7
[2019-04-03 23:02:18,289] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:02:18,305] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:02:18,337] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run7
[2019-04-03 23:02:18,944] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:02:18,944] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:02:18,958] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run7
[2019-04-03 23:02:19,386] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.1201355e-34 1.8225387e-24 1.6091641e-24 1.0000000e+00 6.5955625e-25
 2.2942808e-19 3.4603996e-27], sum to 1.0000
[2019-04-03 23:02:19,389] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2676
[2019-04-03 23:02:19,398] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 36.0, 0.0, 0.0, 26.0, 25.44823625777292, 0.3734183195636852, 0.0, 1.0, 19071.90873533876], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4917000.0000, 
sim time next is 4917600.0000, 
raw observation next is [1.0, 36.0, 0.0, 0.0, 26.0, 25.45210356099629, 0.3668119391939079, 0.0, 1.0, 24369.87237228764], 
processed observation next is [0.0, 0.9565217391304348, 0.4903047091412743, 0.36, 0.0, 0.0, 0.6666666666666666, 0.6210086300830241, 0.6222706463979694, 0.0, 1.0, 0.1160470112966078], 
reward next is 0.8840, 
noisyNet noise sample is [array([-0.33424917], dtype=float32), -1.3456372]. 
=============================================
[2019-04-03 23:02:20,512] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:02:20,512] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:02:20,541] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run7
[2019-04-03 23:02:20,569] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:02:20,569] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:02:20,604] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run7
[2019-04-03 23:02:20,949] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:02:20,949] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:02:20,955] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run7
[2019-04-03 23:02:21,403] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:02:21,404] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:02:21,407] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run7
[2019-04-03 23:02:22,673] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:02:22,673] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:02:22,703] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run7
[2019-04-03 23:02:23,298] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:02:23,299] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:02:23,304] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run7
[2019-04-03 23:02:24,836] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:02:24,836] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:02:24,841] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run7
[2019-04-03 23:02:26,570] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:02:26,571] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:02:26,574] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run7
[2019-04-03 23:02:27,601] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:02:27,601] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:02:27,604] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run7
[2019-04-03 23:02:32,154] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:02:32,154] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:02:32,159] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run7
[2019-04-03 23:02:36,536] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.7126241e-17 2.3008152e-10 4.6513504e-09 3.3938232e-01 4.9138993e-10
 6.6061771e-01 2.0919592e-12], sum to 1.0000
[2019-04-03 23:02:36,536] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3731
[2019-04-03 23:02:36,629] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.2, 69.33333333333334, 175.0, 111.3333333333333, 26.0, 22.66002891954412, -0.2860546393903094, 1.0, 1.0, 113760.7614418031], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 128400.0000, 
sim time next is 129000.0000, 
raw observation next is [-8.3, 65.16666666666667, 166.0, 209.6666666666666, 26.0, 23.18757747752932, -0.2065013552297967, 1.0, 1.0, 94414.68036365953], 
processed observation next is [1.0, 0.4782608695652174, 0.23268698060941828, 0.6516666666666667, 0.5533333333333333, 0.23167587476979734, 0.6666666666666666, 0.43229812312744337, 0.43116621492340107, 1.0, 1.0, 0.4495937160174263], 
reward next is 0.5504, 
noisyNet noise sample is [array([0.90487665], dtype=float32), 0.28049648]. 
=============================================
[2019-04-03 23:02:36,638] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[67.93798 ]
 [65.11127 ]
 [62.07942 ]
 [59.123623]
 [56.350697]], R is [[70.21140289]
 [69.96757507]
 [69.49253845]
 [68.82875824]
 [68.14047241]].
[2019-04-03 23:02:48,842] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.8585068e-20 8.0249965e-12 3.4698664e-11 6.9192320e-01 1.5292226e-14
 3.0807680e-01 1.5273359e-15], sum to 1.0000
[2019-04-03 23:02:48,842] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0293
[2019-04-03 23:02:48,884] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 63.0, 24.33333333333333, 0.0, 26.0, 25.13134813417199, 0.2395590269357991, 1.0, 1.0, 18721.44115827666], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 231600.0000, 
sim time next is 232200.0000, 
raw observation next is [-3.4, 63.5, 18.0, 0.0, 26.0, 25.30562995365146, 0.2555267107082456, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.368421052631579, 0.635, 0.06, 0.0, 0.6666666666666666, 0.6088024961376218, 0.5851755702360819, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.3945963], dtype=float32), 0.627068]. 
=============================================
[2019-04-03 23:02:50,726] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4217339e-21 4.6202030e-15 6.0535805e-13 9.9995542e-01 2.5989501e-12
 4.4568769e-05 2.4908809e-16], sum to 1.0000
[2019-04-03 23:02:50,726] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8347
[2019-04-03 23:02:50,754] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-16.2, 78.0, 0.0, 0.0, 26.0, 21.47578363851948, -0.5711320028604007, 0.0, 1.0, 49086.68933249181], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 367200.0000, 
sim time next is 367800.0000, 
raw observation next is [-16.2, 78.0, 0.0, 0.0, 26.0, 21.37163774903912, -0.57666989706223, 0.0, 1.0, 49199.86184992387], 
processed observation next is [1.0, 0.2608695652173913, 0.013850415512465375, 0.78, 0.0, 0.0, 0.6666666666666666, 0.2809698124199267, 0.30777670097925663, 0.0, 1.0, 0.2342850564282089], 
reward next is 0.7657, 
noisyNet noise sample is [array([0.19043994], dtype=float32), 0.8560391]. 
=============================================
[2019-04-03 23:03:08,079] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.2186340e-21 6.3826132e-12 3.4936498e-12 8.7336361e-01 1.8855169e-12
 1.2663639e-01 6.7260853e-16], sum to 1.0000
[2019-04-03 23:03:08,079] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1311
[2019-04-03 23:03:08,118] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-12.8, 75.83333333333334, 0.0, 0.0, 26.0, 24.84008593162134, 0.2150241400134489, 0.0, 1.0, 44002.14951243679], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 334200.0000, 
sim time next is 334800.0000, 
raw observation next is [-12.8, 77.0, 0.0, 0.0, 26.0, 24.68855919279858, 0.1875489625684036, 0.0, 1.0, 46444.05746328485], 
processed observation next is [1.0, 0.9130434782608695, 0.1080332409972299, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5573799327332148, 0.5625163208561346, 0.0, 1.0, 0.22116217839659452], 
reward next is 0.7788, 
noisyNet noise sample is [array([-0.48938867], dtype=float32), -1.0922885]. 
=============================================
[2019-04-03 23:03:11,410] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.0207366e-21 3.7912750e-12 5.9310360e-12 1.0946271e-01 1.5875931e-12
 8.9053726e-01 1.5865917e-15], sum to 1.0000
[2019-04-03 23:03:11,410] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0612
[2019-04-03 23:03:11,488] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-9.2, 39.0, 37.0, 707.0, 26.0, 25.95561113769354, 0.4499794666408139, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 401400.0000, 
sim time next is 402000.0000, 
raw observation next is [-9.100000000000001, 38.66666666666667, 34.33333333333334, 656.3333333333334, 26.0, 26.03931130127854, 0.3534722810438115, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.21052631578947364, 0.3866666666666667, 0.11444444444444447, 0.7252302025782689, 0.6666666666666666, 0.6699426084398784, 0.6178240936812706, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.25074407], dtype=float32), -0.6705679]. 
=============================================
[2019-04-03 23:03:11,492] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[82.07569]
 [82.1839 ]
 [82.0878 ]
 [81.54532]
 [81.41673]], R is [[81.83633423]
 [82.01797485]
 [82.19779968]
 [81.47003937]
 [81.6553421 ]].
[2019-04-03 23:03:21,371] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.9613878e-22 3.0216891e-11 9.0142087e-13 9.7208989e-01 9.3122692e-15
 2.7910069e-02 7.4888645e-17], sum to 1.0000
[2019-04-03 23:03:21,372] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8361
[2019-04-03 23:03:21,443] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.333333333333333, 48.0, 70.83333333333334, 7.666666666666665, 26.0, 24.64668298222186, 0.3268690389462519, 1.0, 1.0, 197262.1719716093], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 750000.0000, 
sim time next is 750600.0000, 
raw observation next is [-1.7, 49.5, 68.0, 3.0, 26.0, 25.20639596338492, 0.3890360210891015, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.4155124653739613, 0.495, 0.22666666666666666, 0.0033149171270718232, 0.6666666666666666, 0.6005329969487434, 0.6296786736963672, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.527582], dtype=float32), 0.49275854]. 
=============================================
[2019-04-03 23:03:36,253] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.0774025e-24 2.9247644e-17 1.2233941e-15 9.9999940e-01 3.4128783e-18
 5.4675064e-07 1.9473005e-19], sum to 1.0000
[2019-04-03 23:03:36,255] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2250
[2019-04-03 23:03:36,317] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.333333333333333, 67.33333333333334, 132.6666666666667, 64.83333333333334, 26.0, 25.88219269183076, 0.3294769815208176, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 728400.0000, 
sim time next is 729000.0000, 
raw observation next is [-1.15, 67.0, 139.0, 68.0, 26.0, 25.87235172820242, 0.3316563072316568, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4307479224376732, 0.67, 0.4633333333333333, 0.07513812154696133, 0.6666666666666666, 0.6560293106835351, 0.6105521024105522, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.02263874], dtype=float32), -0.50526345]. 
=============================================
[2019-04-03 23:03:36,322] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[84.865616]
 [85.097755]
 [85.35607 ]
 [85.709785]
 [86.12738 ]], R is [[84.59274292]
 [84.74681854]
 [84.89935303]
 [85.05036163]
 [85.19985962]].
[2019-04-03 23:03:40,338] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.8742370e-22 2.8259955e-14 1.6492982e-13 9.9992168e-01 1.2332234e-14
 7.8311954e-05 1.2929136e-16], sum to 1.0000
[2019-04-03 23:03:40,339] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4247
[2019-04-03 23:03:40,352] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.466666666666667, 71.0, 0.0, 0.0, 26.0, 24.20410696919251, 0.1129430252829097, 0.0, 1.0, 41588.20960277166], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 782400.0000, 
sim time next is 783000.0000, 
raw observation next is [-7.55, 71.0, 0.0, 0.0, 26.0, 24.19163687165048, 0.1088642035714441, 0.0, 1.0, 41574.33194133288], 
processed observation next is [1.0, 0.043478260869565216, 0.25346260387811637, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5159697393042068, 0.536288067857148, 0.0, 1.0, 0.19797300924444228], 
reward next is 0.8020, 
noisyNet noise sample is [array([-0.31874517], dtype=float32), -0.6400406]. 
=============================================
[2019-04-03 23:03:40,379] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[81.841965]
 [81.49866 ]
 [81.119385]
 [80.64253 ]
 [80.1655  ]], R is [[82.19881439]
 [82.17878723]
 [82.15895081]
 [82.13928223]
 [82.11973572]].
[2019-04-03 23:03:46,177] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.4868785e-23 1.4676155e-13 5.9878704e-14 9.9958581e-01 1.3133990e-16
 4.1423852e-04 3.6523561e-17], sum to 1.0000
[2019-04-03 23:03:46,178] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2922
[2019-04-03 23:03:46,188] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.366666666666667, 86.16666666666666, 90.33333333333333, 0.0, 26.0, 25.41873923668885, 0.2823881345509838, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 904200.0000, 
sim time next is 904800.0000, 
raw observation next is [1.633333333333334, 88.33333333333334, 93.66666666666667, 0.0, 26.0, 25.38849867325454, 0.28398022736036, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.5078485687903971, 0.8833333333333334, 0.31222222222222223, 0.0, 0.6666666666666666, 0.6157082227712115, 0.5946600757867867, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.1125824], dtype=float32), -0.48548135]. 
=============================================
[2019-04-03 23:04:02,771] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.7724549e-33 8.9572789e-23 2.7847077e-23 1.0000000e+00 5.2221563e-26
 1.8892311e-14 8.0294816e-27], sum to 1.0000
[2019-04-03 23:04:02,772] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6853
[2019-04-03 23:04:02,816] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [14.4, 96.66666666666666, 97.0, 0.0, 26.0, 25.05507778991106, 0.4850657054180804, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1252200.0000, 
sim time next is 1252800.0000, 
raw observation next is [14.4, 96.0, 98.0, 0.0, 26.0, 25.06499444947828, 0.4852916880131342, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.8614958448753465, 0.96, 0.32666666666666666, 0.0, 0.6666666666666666, 0.5887495374565234, 0.661763896004378, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.54979706], dtype=float32), 0.17402245]. 
=============================================
[2019-04-03 23:04:03,866] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3602691e-29 6.8416005e-18 1.3731856e-18 1.0000000e+00 1.0807003e-24
 7.6991485e-10 7.6082100e-23], sum to 1.0000
[2019-04-03 23:04:03,868] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0675
[2019-04-03 23:04:03,881] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [19.4, 49.0, 92.0, 0.0, 26.0, 27.45302440823993, 0.9702596817447136, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1092600.0000, 
sim time next is 1093200.0000, 
raw observation next is [19.4, 49.0, 82.5, 0.0, 26.0, 26.83959649446679, 0.933613457887712, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 1.0, 0.49, 0.275, 0.0, 0.6666666666666666, 0.7366330412055658, 0.8112044859625707, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.36898997], dtype=float32), 0.041850056]. 
=============================================
[2019-04-03 23:04:09,629] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.1687094e-25 4.9223998e-15 2.2321465e-15 9.9999654e-01 1.5024708e-18
 3.4763812e-06 1.8626638e-19], sum to 1.0000
[2019-04-03 23:04:09,633] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1681
[2019-04-03 23:04:09,652] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 92.0, 9.0, 0.0, 26.0, 25.89482910516498, 0.5068754335660811, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1443600.0000, 
sim time next is 1444200.0000, 
raw observation next is [1.1, 91.33333333333334, 5.999999999999998, 0.0, 26.0, 25.93131874521583, 0.507316140521063, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.49307479224376743, 0.9133333333333334, 0.019999999999999993, 0.0, 0.6666666666666666, 0.6609432287679858, 0.6691053801736877, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3163401], dtype=float32), 0.30924293]. 
=============================================
[2019-04-03 23:04:16,502] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.6542650e-23 1.1343612e-13 8.7868183e-15 9.9993575e-01 4.5320432e-17
 6.4190797e-05 7.6351575e-18], sum to 1.0000
[2019-04-03 23:04:16,505] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0889
[2019-04-03 23:04:16,512] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 92.0, 127.0, 0.0, 26.0, 26.12276889596579, 0.5925651692776661, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1337400.0000, 
sim time next is 1338000.0000, 
raw observation next is [1.1, 92.0, 124.6666666666667, 0.0, 26.0, 26.12075734914822, 0.5832786330793865, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.49307479224376743, 0.92, 0.4155555555555557, 0.0, 0.6666666666666666, 0.6767297790956851, 0.6944262110264622, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.57577246], dtype=float32), 0.38142452]. 
=============================================
[2019-04-03 23:04:16,520] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[81.60145 ]
 [81.601105]
 [81.57621 ]
 [81.55137 ]
 [81.567024]], R is [[81.59908295]
 [81.78309631]
 [81.96526337]
 [82.14561462]
 [82.32415771]].
[2019-04-03 23:04:18,456] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4817493e-29 6.7795046e-19 1.5701188e-18 1.0000000e+00 4.4530239e-23
 4.6058259e-09 3.1089491e-23], sum to 1.0000
[2019-04-03 23:04:18,456] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0327
[2019-04-03 23:04:18,490] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 88.0, 96.66666666666667, 0.0, 26.0, 26.01969682286213, 0.60637221251311, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1690800.0000, 
sim time next is 1691400.0000, 
raw observation next is [1.1, 88.0, 93.33333333333333, 0.0, 26.0, 26.19061301240642, 0.6219344535678067, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.49307479224376743, 0.88, 0.3111111111111111, 0.0, 0.6666666666666666, 0.6825510843672017, 0.7073114845226023, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.02404358], dtype=float32), 0.115746625]. 
=============================================
[2019-04-03 23:04:19,136] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.3361133e-23 1.4618502e-15 9.3987582e-14 9.9999559e-01 3.1441421e-16
 4.4419980e-06 6.4159396e-18], sum to 1.0000
[2019-04-03 23:04:19,140] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5063
[2019-04-03 23:04:19,186] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 26.0, 25.28889827746431, 0.502513199474846, 0.0, 1.0, 41393.8810045139], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1380600.0000, 
sim time next is 1381200.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 26.0, 25.28016088751795, 0.4993839190268721, 0.0, 1.0, 41183.79367373745], 
processed observation next is [1.0, 1.0, 0.46260387811634357, 0.95, 0.0, 0.0, 0.6666666666666666, 0.6066800739598293, 0.6664613063422907, 0.0, 1.0, 0.19611330320827355], 
reward next is 0.8039, 
noisyNet noise sample is [array([0.26819828], dtype=float32), 1.1889466]. 
=============================================
[2019-04-03 23:04:23,075] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.6304691e-27 3.1569522e-18 3.3176980e-17 1.0000000e+00 3.2400491e-19
 4.6189932e-10 3.9444423e-21], sum to 1.0000
[2019-04-03 23:04:23,075] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1176
[2019-04-03 23:04:23,110] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 26.0, 25.34788748903586, 0.4812554850910458, 0.0, 1.0, 39199.02400919044], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1469400.0000, 
sim time next is 1470000.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 26.0, 25.33995680424516, 0.4787667079188123, 0.0, 1.0, 37806.43059319595], 
processed observation next is [1.0, 0.0, 0.5069252077562327, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6116630670204298, 0.6595889026396041, 0.0, 1.0, 0.18003062187236168], 
reward next is 0.8200, 
noisyNet noise sample is [array([-2.0649462], dtype=float32), -0.26888347]. 
=============================================
[2019-04-03 23:04:23,126] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[85.715836]
 [85.58381 ]
 [85.52039 ]
 [85.56147 ]
 [85.48666 ]], R is [[85.77594757]
 [85.73152924]
 [85.67144012]
 [85.59200287]
 [85.56804657]].
[2019-04-03 23:04:25,388] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.7319670e-25 1.1542064e-16 1.0979458e-15 1.0000000e+00 1.5686880e-18
 7.4081861e-09 1.1030190e-19], sum to 1.0000
[2019-04-03 23:04:25,389] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2458
[2019-04-03 23:04:25,433] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.266666666666667, 91.0, 0.0, 0.0, 26.0, 24.92184040814325, 0.4549535004317883, 0.0, 1.0, 199461.9625487929], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1455600.0000, 
sim time next is 1456200.0000, 
raw observation next is [1.35, 90.5, 0.0, 0.0, 26.0, 24.96575521960986, 0.4930494633049246, 0.0, 1.0, 133190.2020260743], 
processed observation next is [1.0, 0.8695652173913043, 0.5000000000000001, 0.905, 0.0, 0.0, 0.6666666666666666, 0.580479601634155, 0.6643498211016415, 0.0, 1.0, 0.6342390572670205], 
reward next is 0.3658, 
noisyNet noise sample is [array([1.4984174], dtype=float32), -0.57292676]. 
=============================================
[2019-04-03 23:04:30,318] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.9854789e-25 2.2704374e-16 4.0738875e-16 1.0000000e+00 8.1119002e-19
 8.5266677e-10 2.2948759e-19], sum to 1.0000
[2019-04-03 23:04:30,319] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7504
[2019-04-03 23:04:30,330] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.0, 79.5, 0.0, 0.0, 26.0, 25.57647129525812, 0.5125254303612148, 0.0, 1.0, 72022.29283260892], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1561800.0000, 
sim time next is 1562400.0000, 
raw observation next is [5.0, 79.0, 0.0, 0.0, 26.0, 25.43461923591735, 0.5187259234395559, 0.0, 1.0, 126996.1765271345], 
processed observation next is [1.0, 0.08695652173913043, 0.6011080332409973, 0.79, 0.0, 0.0, 0.6666666666666666, 0.6195516029931124, 0.6729086411465186, 0.0, 1.0, 0.6047436977482595], 
reward next is 0.3953, 
noisyNet noise sample is [array([-0.57069576], dtype=float32), -1.3473281]. 
=============================================
[2019-04-03 23:04:30,812] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.7693525e-25 1.2394403e-16 5.5618919e-15 9.9999976e-01 3.7948577e-18
 1.9370205e-07 8.2904306e-20], sum to 1.0000
[2019-04-03 23:04:30,812] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9643
[2019-04-03 23:04:30,833] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 92.0, 74.5, 0.0, 26.0, 25.8950827941324, 0.5373234246826651, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1681200.0000, 
sim time next is 1681800.0000, 
raw observation next is [1.1, 90.66666666666667, 77.33333333333334, 0.0, 26.0, 25.9072407098716, 0.5332570468299492, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.49307479224376743, 0.9066666666666667, 0.25777777777777783, 0.0, 0.6666666666666666, 0.6589367258226334, 0.6777523489433164, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.29036158], dtype=float32), 2.544653]. 
=============================================
[2019-04-03 23:04:31,493] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.1381809e-25 9.5660653e-20 5.3908280e-17 1.0000000e+00 2.0711708e-17
 1.5577740e-12 1.5101743e-20], sum to 1.0000
[2019-04-03 23:04:31,494] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8739
[2019-04-03 23:04:31,567] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.7, 85.66666666666667, 21.83333333333334, 0.0, 26.0, 24.65232045015436, 0.3370987649341314, 0.0, 1.0, 105404.1575488903], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1758000.0000, 
sim time next is 1758600.0000, 
raw observation next is [-1.7, 85.0, 26.0, 0.0, 26.0, 24.88229481919808, 0.369587206787597, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.4155124653739613, 0.85, 0.08666666666666667, 0.0, 0.6666666666666666, 0.5735245682665067, 0.6231957355958656, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.801207], dtype=float32), -1.003242]. 
=============================================
[2019-04-03 23:04:32,199] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [9.9825109e-30 5.8832387e-20 2.4560766e-19 1.0000000e+00 4.3660785e-23
 1.3393635e-11 2.1187031e-24], sum to 1.0000
[2019-04-03 23:04:32,201] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4215
[2019-04-03 23:04:32,220] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [8.833333333333334, 63.33333333333333, 202.6666666666667, 114.8333333333333, 26.0, 26.78533029859022, 0.7395151631314033, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1593600.0000, 
sim time next is 1594200.0000, 
raw observation next is [9.116666666666667, 62.16666666666666, 205.3333333333333, 141.6666666666667, 26.0, 26.83631534575593, 0.7475558630882748, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.7151431209602955, 0.6216666666666666, 0.6844444444444443, 0.15653775322283614, 0.6666666666666666, 0.7363596121463276, 0.7491852876960916, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8723727], dtype=float32), 1.6017092]. 
=============================================
[2019-04-03 23:04:46,765] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.2738837e-24 2.3957922e-16 2.8052863e-15 1.0000000e+00 3.3065288e-17
 1.0613264e-10 2.3616343e-19], sum to 1.0000
[2019-04-03 23:04:46,765] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4708
[2019-04-03 23:04:46,820] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 79.0, 29.0, 0.0, 26.0, 25.04283645342352, 0.2619471528780917, 0.0, 1.0, 26591.68902663136], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1873800.0000, 
sim time next is 1874400.0000, 
raw observation next is [-4.5, 80.33333333333333, 24.33333333333334, 0.0, 26.0, 25.04511945506579, 0.2554703233159497, 0.0, 1.0, 32611.70079668921], 
processed observation next is [0.0, 0.6956521739130435, 0.3379501385041552, 0.8033333333333332, 0.08111111111111113, 0.0, 0.6666666666666666, 0.5870932879221492, 0.5851567744386499, 0.0, 1.0, 0.15529381331756767], 
reward next is 0.8447, 
noisyNet noise sample is [array([0.8279882], dtype=float32), -0.011525351]. 
=============================================
[2019-04-03 23:04:52,221] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.3890522e-23 7.6050747e-16 1.7416757e-15 1.0000000e+00 1.7980860e-16
 8.1258549e-11 4.9894654e-18], sum to 1.0000
[2019-04-03 23:04:52,223] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3452
[2019-04-03 23:04:52,296] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 77.66666666666667, 36.16666666666666, 0.0, 26.0, 25.0087598318116, 0.2638978342345968, 0.0, 1.0, 46263.73820570636], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1873200.0000, 
sim time next is 1873800.0000, 
raw observation next is [-4.5, 79.0, 29.0, 0.0, 26.0, 25.04286095612391, 0.2619432878246468, 0.0, 1.0, 26584.19276798272], 
processed observation next is [0.0, 0.6956521739130435, 0.3379501385041552, 0.79, 0.09666666666666666, 0.0, 0.6666666666666666, 0.5869050796769925, 0.5873144292748823, 0.0, 1.0, 0.12659139413325105], 
reward next is 0.8734, 
noisyNet noise sample is [array([-0.05139273], dtype=float32), -0.3206949]. 
=============================================
[2019-04-03 23:04:54,699] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.9859853e-29 4.7484639e-19 9.7288839e-18 1.0000000e+00 3.4715014e-22
 8.0387372e-12 2.3755182e-22], sum to 1.0000
[2019-04-03 23:04:54,700] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8461
[2019-04-03 23:04:54,765] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.9, 82.0, 0.0, 0.0, 26.0, 26.07770643520589, 0.4962926709109675, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2055000.0000, 
sim time next is 2055600.0000, 
raw observation next is [-3.9, 82.0, 0.0, 0.0, 26.0, 25.95851201438883, 0.4684885421792311, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.3545706371191136, 0.82, 0.0, 0.0, 0.6666666666666666, 0.6632093345324025, 0.6561628473930771, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.16498312], dtype=float32), -1.0380982]. 
=============================================
[2019-04-03 23:05:03,811] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.9817213e-27 1.2235196e-17 1.6994025e-16 1.0000000e+00 8.7932874e-22
 2.9759150e-11 5.8631586e-22], sum to 1.0000
[2019-04-03 23:05:03,811] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4131
[2019-04-03 23:05:03,886] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.9, 82.0, 32.0, 0.0, 26.0, 25.81882760469335, 0.4026988774436374, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2046600.0000, 
sim time next is 2047200.0000, 
raw observation next is [-3.899999999999999, 82.0, 27.0, 0.0, 26.0, 25.83043159776891, 0.2542828581660886, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.35457063711911363, 0.82, 0.09, 0.0, 0.6666666666666666, 0.6525359664807425, 0.5847609527220295, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.09935752], dtype=float32), 1.3915571]. 
=============================================
[2019-04-03 23:05:14,316] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.6892549e-25 1.6075684e-17 1.0770177e-15 1.0000000e+00 2.2880415e-18
 2.0229977e-11 9.9181101e-20], sum to 1.0000
[2019-04-03 23:05:14,333] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6535
[2019-04-03 23:05:14,378] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.299999999999999, 79.5, 0.0, 0.0, 26.0, 24.46268610848117, 0.1904052176899245, 0.0, 1.0, 42453.94215550489], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2163000.0000, 
sim time next is 2163600.0000, 
raw observation next is [-7.3, 79.0, 0.0, 0.0, 26.0, 24.46136964264504, 0.1797500834745032, 0.0, 1.0, 42484.84569559179], 
processed observation next is [1.0, 0.043478260869565216, 0.26038781163434904, 0.79, 0.0, 0.0, 0.6666666666666666, 0.5384474702204199, 0.559916694491501, 0.0, 1.0, 0.20230878902662758], 
reward next is 0.7977, 
noisyNet noise sample is [array([0.39450786], dtype=float32), -0.44139293]. 
=============================================
[2019-04-03 23:05:30,814] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.6710913e-27 4.0301227e-19 3.5224395e-18 1.0000000e+00 3.1268539e-20
 5.7455933e-13 1.4874412e-21], sum to 1.0000
[2019-04-03 23:05:30,815] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0523
[2019-04-03 23:05:30,836] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.466666666666667, 83.33333333333334, 0.0, 0.0, 26.0, 24.16880473241445, 0.1361647137302809, 0.0, 1.0, 44292.354724068], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2254800.0000, 
sim time next is 2255400.0000, 
raw observation next is [-7.55, 84.0, 0.0, 0.0, 26.0, 24.28510707364731, 0.1386711377616019, 0.0, 1.0, 43990.59867415491], 
processed observation next is [1.0, 0.08695652173913043, 0.25346260387811637, 0.84, 0.0, 0.0, 0.6666666666666666, 0.5237589228039425, 0.5462237125872006, 0.0, 1.0, 0.20947904130549957], 
reward next is 0.7905, 
noisyNet noise sample is [array([-0.8541472], dtype=float32), 1.6669512]. 
=============================================
[2019-04-03 23:05:44,109] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.9701416e-28 9.2400719e-21 3.3138401e-17 1.0000000e+00 8.1747037e-21
 5.3464774e-14 2.7296400e-22], sum to 1.0000
[2019-04-03 23:05:44,109] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8021
[2019-04-03 23:05:44,192] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.8, 54.00000000000001, 0.0, 0.0, 26.0, 24.72477402088902, 0.1760830781870141, 1.0, 1.0, 88969.33052112804], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2532000.0000, 
sim time next is 2532600.0000, 
raw observation next is [-2.8, 54.0, 0.0, 0.0, 26.0, 25.01431545754158, 0.211218940783361, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.38504155124653744, 0.54, 0.0, 0.0, 0.6666666666666666, 0.5845262881284651, 0.5704063135944537, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.13113706], dtype=float32), -0.5547841]. 
=============================================
[2019-04-03 23:05:47,127] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.9544882e-28 2.0400852e-19 4.8837670e-18 1.0000000e+00 1.9913873e-22
 6.8136583e-13 4.7134633e-22], sum to 1.0000
[2019-04-03 23:05:47,129] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3857
[2019-04-03 23:05:47,152] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.1, 48.66666666666667, 134.0, 41.0, 26.0, 25.83837858483053, 0.3037763544332682, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2542200.0000, 
sim time next is 2542800.0000, 
raw observation next is [-1.0, 48.33333333333334, 133.5, 43.0, 26.0, 25.80809425392646, 0.300720705534155, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4349030470914128, 0.48333333333333345, 0.445, 0.04751381215469613, 0.6666666666666666, 0.6506745211605383, 0.6002402351780517, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3988284], dtype=float32), -0.56811154]. 
=============================================
[2019-04-03 23:05:47,351] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.5038025e-26 2.1050371e-19 8.0568829e-19 1.0000000e+00 3.0327440e-20
 2.0975278e-14 3.9124625e-21], sum to 1.0000
[2019-04-03 23:05:47,352] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3161
[2019-04-03 23:05:47,366] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.916666666666667, 41.5, 0.0, 0.0, 26.0, 24.79082844234032, 0.1936336159399697, 0.0, 1.0, 43038.90273039533], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2415000.0000, 
sim time next is 2415600.0000, 
raw observation next is [-5.0, 41.0, 0.0, 0.0, 26.0, 24.74321320031834, 0.1845268004385248, 0.0, 1.0, 43063.62175177031], 
processed observation next is [0.0, 1.0, 0.32409972299168976, 0.41, 0.0, 0.0, 0.6666666666666666, 0.5619344333598617, 0.5615089334795083, 0.0, 1.0, 0.20506486548462052], 
reward next is 0.7949, 
noisyNet noise sample is [array([-0.7063339], dtype=float32), -0.7395503]. 
=============================================
[2019-04-03 23:05:51,695] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.2417403e-27 1.4223746e-18 1.1890195e-17 1.0000000e+00 7.4909742e-22
 2.0368032e-12 4.3607900e-21], sum to 1.0000
[2019-04-03 23:05:51,701] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0401
[2019-04-03 23:05:51,754] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.733333333333333, 51.33333333333333, 135.5, 35.0, 26.0, 25.7513276207376, 0.2999528414906563, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2540400.0000, 
sim time next is 2541000.0000, 
raw observation next is [-1.466666666666667, 50.16666666666667, 135.0, 37.0, 26.0, 25.79396652833038, 0.3042459366839683, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.42197599261311175, 0.5016666666666667, 0.45, 0.04088397790055249, 0.6666666666666666, 0.6494972106941983, 0.6014153122279894, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0145302], dtype=float32), -0.7783736]. 
=============================================
[2019-04-03 23:05:51,766] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[82.25865 ]
 [82.57219 ]
 [82.61542 ]
 [82.725006]
 [82.69834 ]], R is [[82.11400604]
 [82.29286957]
 [82.46994019]
 [82.64524078]
 [82.81878662]].
[2019-04-03 23:05:59,569] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.5367350e-24 7.3145060e-17 3.0172642e-16 1.0000000e+00 7.0204486e-20
 2.8120073e-09 7.9830871e-20], sum to 1.0000
[2019-04-03 23:05:59,571] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3016
[2019-04-03 23:05:59,598] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.0, 64.0, 112.5, 790.0, 26.0, 25.94372252714177, 0.4710752667533145, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2721600.0000, 
sim time next is 2722200.0000, 
raw observation next is [-7.666666666666667, 63.16666666666667, 112.6666666666667, 793.0, 26.0, 25.9331791849024, 0.4757713746103021, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.2502308402585411, 0.6316666666666667, 0.37555555555555564, 0.876243093922652, 0.6666666666666666, 0.6610982654085333, 0.658590458203434, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.65638226], dtype=float32), -0.12430429]. 
=============================================
[2019-04-03 23:06:06,420] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.2216466e-29 3.2906047e-21 2.8163783e-19 1.0000000e+00 3.6051940e-22
 6.6299280e-15 1.8720918e-22], sum to 1.0000
[2019-04-03 23:06:06,421] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9750
[2019-04-03 23:06:06,440] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 72.0, 0.0, 0.0, 26.0, 24.99693747815742, 0.3147372689323232, 0.0, 1.0, 53857.49645767272], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2853600.0000, 
sim time next is 2854200.0000, 
raw observation next is [1.0, 72.0, 0.0, 0.0, 26.0, 25.03480216176905, 0.3185872155920618, 0.0, 1.0, 52810.85766268804], 
processed observation next is [1.0, 0.0, 0.4903047091412743, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5862335134807543, 0.6061957385306873, 0.0, 1.0, 0.2514802745842288], 
reward next is 0.7485, 
noisyNet noise sample is [array([0.07003536], dtype=float32), -0.7622644]. 
=============================================
[2019-04-03 23:06:08,907] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.4175715e-26 2.5770409e-19 1.2094358e-17 1.0000000e+00 3.7906898e-19
 1.1958183e-12 9.0759121e-22], sum to 1.0000
[2019-04-03 23:06:08,909] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5029
[2019-04-03 23:06:08,958] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 71.0, 162.0, 96.00000000000001, 26.0, 25.08430878158022, 0.2989279983693368, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2973000.0000, 
sim time next is 2973600.0000, 
raw observation next is [-4.0, 71.0, 166.0, 78.0, 26.0, 24.9776031464029, 0.2862457747686953, 0.0, 1.0, 66258.3059690585], 
processed observation next is [0.0, 0.43478260869565216, 0.3518005540166205, 0.71, 0.5533333333333333, 0.0861878453038674, 0.6666666666666666, 0.5814669288669082, 0.5954152582562318, 0.0, 1.0, 0.3155157427098024], 
reward next is 0.6845, 
noisyNet noise sample is [array([-0.521182], dtype=float32), -0.76694137]. 
=============================================
[2019-04-03 23:06:10,966] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.9473499e-29 2.7990073e-18 2.6725322e-19 1.0000000e+00 2.1446029e-23
 1.9938919e-11 3.3000940e-22], sum to 1.0000
[2019-04-03 23:06:10,966] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0359
[2019-04-03 23:06:11,024] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.666666666666666, 26.0, 114.1666666666667, 0.0, 26.0, 25.85770391192284, 0.3072978343081477, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2817600.0000, 
sim time next is 2818200.0000, 
raw observation next is [6.833333333333334, 25.0, 110.3333333333333, 0.0, 26.0, 25.38688186165499, 0.3278120216347027, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.651892890120037, 0.25, 0.36777777777777765, 0.0, 0.6666666666666666, 0.6155734884712493, 0.6092706738782342, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.19578688], dtype=float32), -1.3925241]. 
=============================================
[2019-04-03 23:06:20,827] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.4865994e-24 1.6542383e-17 1.0086734e-16 1.0000000e+00 1.0755099e-17
 4.3516097e-13 8.8199367e-20], sum to 1.0000
[2019-04-03 23:06:20,827] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0077
[2019-04-03 23:06:20,844] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.75, 65.0, 0.0, 0.0, 26.0, 25.22241748265968, 0.3371502101347335, 0.0, 1.0, 39332.85248307828], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3015000.0000, 
sim time next is 3015600.0000, 
raw observation next is [-3.833333333333333, 65.0, 0.0, 0.0, 26.0, 25.19162015253815, 0.3310829913629965, 0.0, 1.0, 39179.85267348841], 
processed observation next is [0.0, 0.9130434782608695, 0.3564173591874424, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5993016793781791, 0.6103609971209988, 0.0, 1.0, 0.18657072701661145], 
reward next is 0.8134, 
noisyNet noise sample is [array([0.1304355], dtype=float32), 0.8838992]. 
=============================================
[2019-04-03 23:06:25,006] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.5154717e-28 1.5373866e-23 2.8425403e-20 1.0000000e+00 5.2853772e-22
 2.0851879e-16 2.4310715e-23], sum to 1.0000
[2019-04-03 23:06:25,007] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9090
[2019-04-03 23:06:25,035] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.166666666666667, 82.83333333333333, 0.0, 0.0, 26.0, 24.28490928172898, 0.188777624832335, 0.0, 1.0, 42819.63626847925], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2956200.0000, 
sim time next is 2956800.0000, 
raw observation next is [-3.333333333333333, 81.66666666666667, 0.0, 0.0, 26.0, 24.26592229436589, 0.1818835877438231, 0.0, 1.0, 42757.28572563834], 
processed observation next is [0.0, 0.21739130434782608, 0.37026777469990774, 0.8166666666666668, 0.0, 0.0, 0.6666666666666666, 0.5221601911971575, 0.5606278625812744, 0.0, 1.0, 0.20360612250303972], 
reward next is 0.7964, 
noisyNet noise sample is [array([-1.664201], dtype=float32), 0.8898851]. 
=============================================
[2019-04-03 23:06:28,389] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.2075749e-29 1.0426042e-22 7.1162720e-21 1.0000000e+00 2.2030976e-21
 1.5980678e-16 2.0857055e-23], sum to 1.0000
[2019-04-03 23:06:28,393] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7543
[2019-04-03 23:06:28,408] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 66.16666666666667, 28.33333333333333, 166.3333333333333, 26.0, 23.5748409375169, -0.0375757204730501, 0.0, 1.0, 40758.49450771215], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3052200.0000, 
sim time next is 3052800.0000, 
raw observation next is [-6.0, 64.0, 42.0, 214.5, 26.0, 23.55992825961469, -0.02590651997215162, 0.0, 1.0, 40645.79382647915], 
processed observation next is [0.0, 0.34782608695652173, 0.296398891966759, 0.64, 0.14, 0.23701657458563535, 0.6666666666666666, 0.46332735496789085, 0.49136449334261617, 0.0, 1.0, 0.19355139917371025], 
reward next is 0.8064, 
noisyNet noise sample is [array([-0.4397185], dtype=float32), 1.633382]. 
=============================================
[2019-04-03 23:06:30,148] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.6499646e-26 9.7236989e-16 2.6342256e-16 1.0000000e+00 1.0534520e-19
 2.7116073e-11 1.5269860e-19], sum to 1.0000
[2019-04-03 23:06:30,153] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5481
[2019-04-03 23:06:30,191] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 26.0, 25.6886641773978, 0.5580494794312929, 0.0, 1.0, 43751.71763761465], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3268800.0000, 
sim time next is 3269400.0000, 
raw observation next is [-4.166666666666667, 72.66666666666667, 0.0, 0.0, 26.0, 25.58966935932538, 0.5461941784352993, 0.0, 1.0, 42676.75543626567], 
processed observation next is [1.0, 0.8695652173913043, 0.3471837488457987, 0.7266666666666667, 0.0, 0.0, 0.6666666666666666, 0.6324724466104484, 0.6820647261450997, 0.0, 1.0, 0.2032226449345984], 
reward next is 0.7968, 
noisyNet noise sample is [array([-1.0661678], dtype=float32), -0.10020851]. 
=============================================
[2019-04-03 23:06:31,604] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.7564120e-25 1.0039818e-17 4.4829520e-16 1.0000000e+00 1.2092224e-19
 5.9265115e-12 7.9462330e-20], sum to 1.0000
[2019-04-03 23:06:31,605] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2650
[2019-04-03 23:06:31,633] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.666666666666667, 75.0, 0.0, 0.0, 26.0, 25.08905315289621, 0.3781833430343822, 0.0, 1.0, 41630.79232329715], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3368400.0000, 
sim time next is 3369000.0000, 
raw observation next is [-5.833333333333333, 76.0, 0.0, 0.0, 26.0, 25.06012342563956, 0.3687623233973085, 0.0, 1.0, 41565.89288231793], 
processed observation next is [1.0, 1.0, 0.30101569713758086, 0.76, 0.0, 0.0, 0.6666666666666666, 0.5883436188032967, 0.6229207744657695, 0.0, 1.0, 0.197932823249133], 
reward next is 0.8021, 
noisyNet noise sample is [array([-0.51205134], dtype=float32), 1.5596296]. 
=============================================
[2019-04-03 23:06:31,644] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[76.612206]
 [76.617   ]
 [76.61254 ]
 [76.6439  ]
 [76.671036]], R is [[76.59622955]
 [76.63202667]
 [76.66687012]
 [76.69992065]
 [76.72833252]].
[2019-04-03 23:06:33,333] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-03 23:06:33,338] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-03 23:06:33,338] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:06:33,350] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run10
[2019-04-03 23:06:33,369] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-03 23:06:33,372] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:06:33,373] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-03 23:06:33,375] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:06:33,377] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run10
[2019-04-03 23:06:33,392] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run10
[2019-04-03 23:07:39,651] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.03478538], dtype=float32), 0.44125193]
[2019-04-03 23:07:39,652] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [14.97439596666667, 94.07054072666666, 0.0, 0.0, 24.0, 22.92564021678799, 0.01321655184970411, 0.0, 1.0, 0.0]
[2019-04-03 23:07:39,652] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-03 23:07:39,653] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [3.9795544e-27 3.5276012e-19 6.4556871e-19 1.0000000e+00 1.2887900e-23
 5.6686879e-17 1.8762894e-21], sampled 0.18153533867171068
[2019-04-03 23:08:14,336] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.03478538], dtype=float32), 0.44125193]
[2019-04-03 23:08:14,336] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-5.868482337166667, 85.79357646166667, 0.0, 0.0, 24.0, 22.71305564996617, -0.2861404838000577, 0.0, 1.0, 41681.75187151851]
[2019-04-03 23:08:14,336] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-03 23:08:14,337] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.3737795e-23 5.4195683e-18 2.7487180e-16 1.0000000e+00 2.8980005e-17
 7.7094673e-13 7.2853431e-19], sampled 0.517790180631986
[2019-04-03 23:08:57,382] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.03478538], dtype=float32), 0.44125193]
[2019-04-03 23:08:57,383] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [19.66666666666667, 56.66666666666667, 230.8333333333333, 465.5, 22.0, 25.89764774462038, 0.7041653364377659, 1.0, 0.0, 0.0]
[2019-04-03 23:08:57,383] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-03 23:08:57,384] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [5.69369798e-18 1.10351408e-08 1.16082526e-10 9.99996901e-01
 9.72644625e-17 3.07662208e-06 8.18172705e-14], sampled 0.7079854412477276
[2019-04-03 23:09:04,648] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7236.3086 181279742.7742 -723.0707
[2019-04-03 23:09:42,489] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7402.5665 223247897.6204 -405.4165
[2019-04-03 23:10:02,150] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7280.6466 249157780.0715 51.1463
[2019-04-03 23:10:03,187] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 900000, evaluation results [900000.0, 7402.566460521894, 223247897.62040925, -405.41648061752056, 7236.308639482133, 181279742.7742251, -723.0707354610348, 7280.646603917524, 249157780.07150912, 51.14630615652743]
[2019-04-03 23:10:07,603] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.1801857e-25 1.0516382e-17 5.5622727e-16 1.0000000e+00 3.6998163e-18
 4.0162997e-09 1.6910772e-19], sum to 1.0000
[2019-04-03 23:10:07,603] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9157
[2019-04-03 23:10:07,650] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.833333333333334, 69.0, 108.6666666666667, 698.3333333333333, 26.0, 26.36079346585047, 0.5659168155900963, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3319800.0000, 
sim time next is 3320400.0000, 
raw observation next is [-7.666666666666667, 68.0, 109.8333333333333, 719.1666666666667, 26.0, 26.31548797942843, 0.5701802239784548, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.2502308402585411, 0.68, 0.366111111111111, 0.7946593001841622, 0.6666666666666666, 0.6929573316190357, 0.690060074659485, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.1193453], dtype=float32), -0.024115738]. 
=============================================
[2019-04-03 23:10:10,161] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0176569e-28 9.5269437e-22 2.8959200e-19 1.0000000e+00 6.9988891e-22
 2.0298591e-14 7.8038608e-23], sum to 1.0000
[2019-04-03 23:10:10,164] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0078
[2019-04-03 23:10:10,193] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.1, 100.0, 0.0, 0.0, 26.0, 25.35700519730404, 0.2969079597490721, 0.0, 1.0, 100149.9128727108], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3121800.0000, 
sim time next is 3122400.0000, 
raw observation next is [2.2, 100.0, 0.0, 0.0, 26.0, 25.29347410614479, 0.2973206420011809, 0.0, 1.0, 78133.14196697842], 
processed observation next is [1.0, 0.13043478260869565, 0.5235457063711911, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6077895088453991, 0.5991068806670603, 0.0, 1.0, 0.3720625807951353], 
reward next is 0.6279, 
noisyNet noise sample is [array([-0.36682057], dtype=float32), -0.45888782]. 
=============================================
[2019-04-03 23:10:16,459] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.6569472e-25 6.6490877e-19 8.6694735e-17 1.0000000e+00 7.4609556e-19
 2.1226789e-13 5.7306414e-21], sum to 1.0000
[2019-04-03 23:10:16,459] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6322
[2019-04-03 23:10:16,485] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.40227916739757, 0.344040047009736, 0.0, 1.0, 33658.6794507426], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3102000.0000, 
sim time next is 3102600.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.37162348106064, 0.3437025333224153, 0.0, 1.0, 53074.93641596613], 
processed observation next is [0.0, 0.9130434782608695, 0.4349030470914128, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6143019567550535, 0.6145675111074718, 0.0, 1.0, 0.2527377924569816], 
reward next is 0.7473, 
noisyNet noise sample is [array([0.7154464], dtype=float32), 0.889733]. 
=============================================
[2019-04-03 23:10:46,302] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.9983034e-29 2.2903959e-21 1.1578958e-19 1.0000000e+00 3.0130829e-20
 3.9132079e-14 1.3068608e-22], sum to 1.0000
[2019-04-03 23:10:46,302] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9240
[2019-04-03 23:10:46,339] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.333333333333333, 51.66666666666666, 117.3333333333333, 821.1666666666667, 26.0, 25.18641627221034, 0.4511624314825706, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3588000.0000, 
sim time next is 3588600.0000, 
raw observation next is [-2.166666666666667, 50.83333333333334, 116.6666666666667, 819.3333333333334, 26.0, 25.19550142962539, 0.4505336089053638, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.4025854108956602, 0.5083333333333334, 0.388888888888889, 0.905340699815838, 0.6666666666666666, 0.5996251191354492, 0.6501778696351213, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.40890688], dtype=float32), -0.02633376]. 
=============================================
[2019-04-03 23:10:48,577] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.0506165e-29 1.0582485e-20 4.8688623e-20 1.0000000e+00 2.4160863e-22
 6.6872057e-15 6.7175526e-23], sum to 1.0000
[2019-04-03 23:10:48,577] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0695
[2019-04-03 23:10:48,627] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 74.0, 89.0, 429.0, 26.0, 25.33778055811674, 0.357657363716392, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3745800.0000, 
sim time next is 3746400.0000, 
raw observation next is [-4.0, 75.0, 90.83333333333334, 470.0, 26.0, 25.50319758351218, 0.3723131786452019, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.3518005540166205, 0.75, 0.3027777777777778, 0.5193370165745856, 0.6666666666666666, 0.6252664652926816, 0.6241043928817339, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6832184], dtype=float32), 0.51070833]. 
=============================================
[2019-04-03 23:11:25,251] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.3845580e-25 4.1159478e-19 3.3578431e-17 1.0000000e+00 2.0239930e-19
 5.3918263e-13 1.9719692e-20], sum to 1.0000
[2019-04-03 23:11:25,253] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5555
[2019-04-03 23:11:25,433] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.166666666666667, 63.0, 0.0, 0.0, 26.0, 24.64372806325769, 0.2369519410325157, 0.0, 1.0, 42752.80640826027], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3913800.0000, 
sim time next is 3914400.0000, 
raw observation next is [-7.333333333333334, 62.0, 5.0, 135.8333333333333, 26.0, 24.64212015815783, 0.3514771711560438, 1.0, 1.0, 202408.6299163178], 
processed observation next is [1.0, 0.30434782608695654, 0.2594644506001847, 0.62, 0.016666666666666666, 0.1500920810313075, 0.6666666666666666, 0.5535100131798192, 0.6171590570520146, 1.0, 1.0, 0.9638506186491324], 
reward next is 0.0361, 
noisyNet noise sample is [array([1.5618639], dtype=float32), -0.107222684]. 
=============================================
[2019-04-03 23:11:30,346] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.6794095e-26 1.6029733e-20 1.1746823e-18 1.0000000e+00 4.5904061e-21
 2.2544807e-15 1.6203404e-21], sum to 1.0000
[2019-04-03 23:11:30,346] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0894
[2019-04-03 23:11:30,380] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.5, 48.0, 0.0, 0.0, 26.0, 25.25877039144297, 0.3806462582648887, 0.0, 1.0, 39269.14704036416], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4156200.0000, 
sim time next is 4156800.0000, 
raw observation next is [-2.666666666666667, 48.66666666666666, 0.0, 0.0, 26.0, 25.23187963207639, 0.3737908884940189, 0.0, 1.0, 39321.84230291332], 
processed observation next is [0.0, 0.08695652173913043, 0.38873499538319484, 0.4866666666666666, 0.0, 0.0, 0.6666666666666666, 0.6026566360063658, 0.6245969628313396, 0.0, 1.0, 0.18724686810911104], 
reward next is 0.8128, 
noisyNet noise sample is [array([-0.41924527], dtype=float32), 0.37186062]. 
=============================================
[2019-04-03 23:11:34,694] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.7769728e-26 4.5067318e-17 1.3093899e-16 1.0000000e+00 1.0928743e-20
 4.6197577e-11 1.1291413e-20], sum to 1.0000
[2019-04-03 23:11:34,694] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3227
[2019-04-03 23:11:34,707] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.166666666666667, 26.83333333333334, 0.0, 0.0, 26.0, 25.67628106801733, 0.5057808008215531, 1.0, 1.0, 142044.8468018997], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4039800.0000, 
sim time next is 4040400.0000, 
raw observation next is [-3.333333333333333, 27.66666666666667, 0.0, 0.0, 26.0, 25.7064104087582, 0.4975168639272436, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.37026777469990774, 0.2766666666666667, 0.0, 0.0, 0.6666666666666666, 0.6422008673965166, 0.6658389546424145, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.02492003], dtype=float32), -1.0537009]. 
=============================================
[2019-04-03 23:11:42,644] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.5326349e-29 3.9460937e-23 1.2817612e-20 1.0000000e+00 9.5446540e-23
 2.8137396e-17 1.1036475e-24], sum to 1.0000
[2019-04-03 23:11:42,644] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0861
[2019-04-03 23:11:42,663] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 26.0, 25.41511229025836, 0.3407800379936567, 0.0, 1.0, 39613.23386857485], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4242000.0000, 
sim time next is 4242600.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 26.0, 25.41171658906955, 0.3396522916246081, 0.0, 1.0, 40481.08268282232], 
processed observation next is [0.0, 0.08695652173913043, 0.5457063711911359, 0.45, 0.0, 0.0, 0.6666666666666666, 0.617643049089129, 0.613217430541536, 0.0, 1.0, 0.192767060394392], 
reward next is 0.8072, 
noisyNet noise sample is [array([-0.31011024], dtype=float32), -0.82415706]. 
=============================================
[2019-04-03 23:11:47,272] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.6305501e-29 1.5637700e-19 5.1777019e-19 1.0000000e+00 2.1361546e-21
 7.2079907e-13 3.9216651e-22], sum to 1.0000
[2019-04-03 23:11:47,275] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2220
[2019-04-03 23:11:47,324] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 76.0, 29.0, 45.83333333333334, 26.0, 26.00927136261033, 0.5333859270702287, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4468800.0000, 
sim time next is 4469400.0000, 
raw observation next is [0.0, 75.0, 25.0, 55.0, 26.0, 25.89779344054402, 0.5010176604346838, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.46260387811634357, 0.75, 0.08333333333333333, 0.06077348066298342, 0.6666666666666666, 0.6581494533786684, 0.6670058868115613, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.35351884], dtype=float32), -1.4501344]. 
=============================================
[2019-04-03 23:11:48,011] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.0337317e-31 3.0906441e-23 3.1204298e-21 1.0000000e+00 5.5213044e-23
 7.2383684e-17 4.1933701e-25], sum to 1.0000
[2019-04-03 23:11:48,011] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3345
[2019-04-03 23:11:48,032] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.666666666666666, 54.66666666666667, 190.1666666666667, 640.3333333333334, 26.0, 25.34839662978964, 0.402278328636732, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4272000.0000, 
sim time next is 4272600.0000, 
raw observation next is [4.833333333333334, 54.83333333333334, 176.3333333333333, 676.6666666666666, 26.0, 25.29772581897496, 0.4012431091418545, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.5964912280701755, 0.5483333333333335, 0.5877777777777776, 0.7476979742173112, 0.6666666666666666, 0.6081438182479134, 0.6337477030472848, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.13656889], dtype=float32), 0.36457372]. 
=============================================
[2019-04-03 23:11:51,506] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.1741802e-28 2.7361362e-21 4.1891461e-19 1.0000000e+00 1.6486039e-20
 1.3040916e-15 4.4683409e-22], sum to 1.0000
[2019-04-03 23:11:51,510] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3600
[2019-04-03 23:11:51,563] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 48.33333333333334, 0.0, 0.0, 26.0, 25.38317126600808, 0.325340307276647, 0.0, 1.0, 38450.73807425366], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4254600.0000, 
sim time next is 4255200.0000, 
raw observation next is [3.0, 49.0, 0.0, 0.0, 26.0, 25.37656259346337, 0.3238287689423558, 0.0, 1.0, 41811.96127212344], 
processed observation next is [0.0, 0.2608695652173913, 0.5457063711911359, 0.49, 0.0, 0.0, 0.6666666666666666, 0.6147135494552808, 0.6079429229807852, 0.0, 1.0, 0.1991045774863021], 
reward next is 0.8009, 
noisyNet noise sample is [array([-0.02129601], dtype=float32), 0.73190653]. 
=============================================
[2019-04-03 23:11:58,229] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.6144187e-27 2.5118365e-20 6.0434028e-19 1.0000000e+00 3.1476432e-19
 1.2508605e-14 3.3943861e-21], sum to 1.0000
[2019-04-03 23:11:58,229] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7401
[2019-04-03 23:11:58,254] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.24786783002378, 0.3439189118892091, 0.0, 1.0, 40583.53728515151], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4841400.0000, 
sim time next is 4842000.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.22860776348985, 0.3396145208883706, 0.0, 1.0, 39678.79903031501], 
processed observation next is [0.0, 0.043478260869565216, 0.40720221606648205, 0.6, 0.0, 0.0, 0.6666666666666666, 0.602383980290821, 0.6132048402961235, 0.0, 1.0, 0.18894666204911909], 
reward next is 0.8111, 
noisyNet noise sample is [array([-0.44195464], dtype=float32), -1.1836715]. 
=============================================
[2019-04-03 23:11:58,262] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[80.74593 ]
 [80.69405 ]
 [80.572395]
 [80.40529 ]
 [80.31052 ]], R is [[80.83457947]
 [80.83298492]
 [80.81816864]
 [80.76881409]
 [80.66657257]].
[2019-04-03 23:12:01,169] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.1061509e-29 6.5517637e-22 1.5825533e-19 1.0000000e+00 1.7828777e-21
 3.2073080e-15 1.1257038e-22], sum to 1.0000
[2019-04-03 23:12:01,170] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9210
[2019-04-03 23:12:01,185] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 71.0, 0.0, 0.0, 26.0, 25.26277974446394, 0.3933256389235429, 0.0, 1.0, 40542.23791337594], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4517400.0000, 
sim time next is 4518000.0000, 
raw observation next is [-1.0, 71.0, 0.0, 0.0, 26.0, 25.2740260256585, 0.3834649970813834, 0.0, 1.0, 40475.50586811144], 
processed observation next is [1.0, 0.30434782608695654, 0.4349030470914128, 0.71, 0.0, 0.0, 0.6666666666666666, 0.6061688354715417, 0.6278216656937944, 0.0, 1.0, 0.19274050413386398], 
reward next is 0.8073, 
noisyNet noise sample is [array([0.7679294], dtype=float32), 0.6662169]. 
=============================================
[2019-04-03 23:12:01,196] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[86.39953 ]
 [86.36388 ]
 [86.37336 ]
 [86.395874]
 [86.40157 ]], R is [[86.3601532 ]
 [86.30348969]
 [86.24694824]
 [86.19059753]
 [86.13449097]].
[2019-04-03 23:12:03,029] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4877999e-27 9.9265026e-22 4.7648466e-20 1.0000000e+00 2.1726257e-22
 1.6185229e-15 1.2664210e-22], sum to 1.0000
[2019-04-03 23:12:03,030] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8356
[2019-04-03 23:12:03,048] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.5, 53.0, 0.0, 0.0, 26.0, 25.40853048152888, 0.4062003303708303, 0.0, 1.0, 45887.6489765901], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4829400.0000, 
sim time next is 4830000.0000, 
raw observation next is [-0.6666666666666666, 53.66666666666667, 0.0, 0.0, 26.0, 25.40634048552615, 0.4036832680250719, 0.0, 1.0, 41603.22831416314], 
processed observation next is [0.0, 0.9130434782608695, 0.44413665743305636, 0.5366666666666667, 0.0, 0.0, 0.6666666666666666, 0.6171950404605125, 0.6345610893416906, 0.0, 1.0, 0.1981106110198245], 
reward next is 0.8019, 
noisyNet noise sample is [array([0.5401007], dtype=float32), -0.7343663]. 
=============================================
[2019-04-03 23:12:03,058] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[83.722725]
 [83.74097 ]
 [83.72044 ]
 [83.63183 ]
 [83.598274]], R is [[83.54729462]
 [83.49330902]
 [83.36467743]
 [83.2349472 ]
 [83.15567017]].
[2019-04-03 23:12:05,197] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.9800137e-27 1.8331949e-22 2.9208145e-20 1.0000000e+00 1.7312204e-18
 1.6193822e-14 8.3016225e-23], sum to 1.0000
[2019-04-03 23:12:05,201] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3980
[2019-04-03 23:12:05,255] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.666666666666667, 69.0, 170.5, 472.5, 26.0, 25.58694250189128, 0.4669221060096467, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4786800.0000, 
sim time next is 4787400.0000, 
raw observation next is [-3.333333333333333, 67.0, 167.0, 524.0, 26.0, 25.61293516959248, 0.4664669827032955, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.37026777469990774, 0.67, 0.5566666666666666, 0.5790055248618785, 0.6666666666666666, 0.6344112641327065, 0.6554889942344319, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3337268], dtype=float32), 0.23292935]. 
=============================================
[2019-04-03 23:12:06,327] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.6613176e-26 7.4747110e-20 7.1044574e-18 1.0000000e+00 9.3094170e-21
 5.4046036e-13 4.4453571e-21], sum to 1.0000
[2019-04-03 23:12:06,328] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7877
[2019-04-03 23:12:06,341] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 37.0, 122.5, 734.5, 26.0, 25.13426992009683, 0.4437959342961335, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4806000.0000, 
sim time next is 4806600.0000, 
raw observation next is [3.0, 37.0, 114.0, 732.0, 26.0, 25.18444750238671, 0.4461989282438905, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.5457063711911359, 0.37, 0.38, 0.8088397790055248, 0.6666666666666666, 0.5987039585322259, 0.6487329760812969, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.02552288], dtype=float32), 0.26317644]. 
=============================================
[2019-04-03 23:12:07,316] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.5163730e-27 4.8732263e-21 4.6201000e-19 1.0000000e+00 1.3813600e-19
 8.5996214e-15 1.2535358e-21], sum to 1.0000
[2019-04-03 23:12:07,319] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9214
[2019-04-03 23:12:07,330] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 67.0, 0.0, 0.0, 26.0, 24.4465628158215, 0.1461817193488934, 0.0, 1.0, 39575.85365043813], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4864800.0000, 
sim time next is 4865400.0000, 
raw observation next is [-4.0, 68.0, 0.0, 0.0, 26.0, 24.41910690919077, 0.1398770416360725, 0.0, 1.0, 39593.50173807456], 
processed observation next is [0.0, 0.30434782608695654, 0.3518005540166205, 0.68, 0.0, 0.0, 0.6666666666666666, 0.5349255757658975, 0.5466256805453575, 0.0, 1.0, 0.18854048446702174], 
reward next is 0.8115, 
noisyNet noise sample is [array([-1.4993263], dtype=float32), -1.2409538]. 
=============================================
[2019-04-03 23:12:07,743] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.8972650e-26 2.2701244e-17 9.5375179e-16 1.0000000e+00 2.2103729e-19
 2.8139305e-11 6.8061358e-21], sum to 1.0000
[2019-04-03 23:12:07,743] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8997
[2019-04-03 23:12:07,790] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 78.0, 0.0, 0.0, 26.0, 25.29482155451537, 0.4572372071447143, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4735800.0000, 
sim time next is 4736400.0000, 
raw observation next is [-1.0, 78.0, 0.0, 0.0, 26.0, 25.22707578646086, 0.4408403490427193, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.4349030470914128, 0.78, 0.0, 0.0, 0.6666666666666666, 0.6022563155384049, 0.6469467830142398, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.86944085], dtype=float32), -0.4673176]. 
=============================================
[2019-04-03 23:12:13,609] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:12:13,609] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:12:13,630] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run8
[2019-04-03 23:12:17,000] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:12:17,001] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:12:17,014] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run8
[2019-04-03 23:12:18,571] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:12:18,571] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:12:18,576] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run8
[2019-04-03 23:12:21,790] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:12:21,790] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:12:21,798] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run8
[2019-04-03 23:12:22,324] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.0425144e-28 2.4724437e-18 1.1428845e-17 1.0000000e+00 4.8160842e-21
 8.6632810e-12 2.4459295e-21], sum to 1.0000
[2019-04-03 23:12:22,325] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5997
[2019-04-03 23:12:22,342] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.5, 25.66666666666667, 122.3333333333333, 851.6666666666666, 26.0, 26.98872315736744, 0.6890052106671045, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4967400.0000, 
sim time next is 4968000.0000, 
raw observation next is [6.0, 25.0, 122.5, 855.0, 26.0, 27.00233871257605, 0.7073873112896775, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.6288088642659281, 0.25, 0.4083333333333333, 0.9447513812154696, 0.6666666666666666, 0.750194892714671, 0.7357957704298924, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0668755], dtype=float32), 0.7990042]. 
=============================================
[2019-04-03 23:12:22,360] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[89.4262 ]
 [89.3797 ]
 [89.39821]
 [89.4133 ]
 [89.43355]], R is [[89.58566284]
 [89.68980408]
 [89.79290771]
 [89.89498138]
 [89.99603271]].
[2019-04-03 23:12:23,654] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:12:23,654] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:12:23,680] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run8
[2019-04-03 23:12:24,821] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:12:24,821] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:12:24,825] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run8
[2019-04-03 23:12:28,194] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:12:28,200] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:12:28,204] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run8
[2019-04-03 23:12:29,127] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:12:29,127] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:12:29,133] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run8
[2019-04-03 23:12:29,260] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.6575336e-29 7.3416738e-18 2.3185120e-18 1.0000000e+00 2.5049063e-22
 7.5161828e-13 9.8022089e-23], sum to 1.0000
[2019-04-03 23:12:29,262] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0748
[2019-04-03 23:12:29,304] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.0, 25.0, 17.0, 152.0, 26.0, 27.21157630143873, 0.6694268200584533, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4989600.0000, 
sim time next is 4990200.0000, 
raw observation next is [6.0, 24.66666666666667, 0.0, 0.0, 26.0, 26.44962399816563, 0.699896585696027, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.6288088642659281, 0.2466666666666667, 0.0, 0.0, 0.6666666666666666, 0.7041353331804693, 0.7332988618986757, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7059337], dtype=float32), 1.8019218]. 
=============================================
[2019-04-03 23:12:30,081] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:12:30,081] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:12:30,086] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run8
[2019-04-03 23:12:31,346] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:12:31,346] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:12:31,348] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run8
[2019-04-03 23:12:32,694] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:12:32,694] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:12:32,704] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run8
[2019-04-03 23:12:34,273] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:12:34,273] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:12:34,279] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run8
[2019-04-03 23:12:35,505] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:12:35,506] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:12:35,511] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run8
[2019-04-03 23:12:37,029] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:12:37,029] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:12:37,036] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run8
[2019-04-03 23:12:37,117] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:12:37,117] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:12:37,121] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run8
[2019-04-03 23:12:41,661] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:12:41,661] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:12:41,672] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run8
[2019-04-03 23:12:52,055] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.2842679e-20 6.9471772e-13 8.9794497e-13 9.9998748e-01 1.9055961e-12
 1.2523306e-05 4.6651748e-16], sum to 1.0000
[2019-04-03 23:12:52,112] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7366
[2019-04-03 23:12:52,188] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.4, 68.0, 0.0, 0.0, 25.0, 23.62734538097715, -0.1027873870944814, 0.0, 1.0, 44067.40676028961], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 162000.0000, 
sim time next is 162600.0000, 
raw observation next is [-8.4, 68.5, 0.0, 0.0, 25.0, 23.47692299593317, -0.1248208813692616, 0.0, 1.0, 45363.92636748948], 
processed observation next is [1.0, 0.9130434782608695, 0.2299168975069252, 0.685, 0.0, 0.0, 0.5833333333333334, 0.4564102496610974, 0.4583930395435795, 0.0, 1.0, 0.21601869698804516], 
reward next is 0.7840, 
noisyNet noise sample is [array([1.4407412], dtype=float32), -2.2200756]. 
=============================================
[2019-04-03 23:12:56,470] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.2462713e-20 2.4603889e-13 2.0602892e-13 9.9999642e-01 1.3643060e-12
 3.5416347e-06 1.7798665e-15], sum to 1.0000
[2019-04-03 23:12:56,471] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9482
[2019-04-03 23:12:56,532] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-10.6, 48.33333333333333, 0.0, 0.0, 26.0, 24.37882413812463, 0.09405492054686315, 0.0, 1.0, 45203.26333882517], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 423600.0000, 
sim time next is 424200.0000, 
raw observation next is [-10.6, 48.66666666666666, 0.0, 0.0, 26.0, 24.31114092497294, 0.08166418375392004, 0.0, 1.0, 44779.03654910265], 
processed observation next is [1.0, 0.9130434782608695, 0.1689750692520776, 0.4866666666666666, 0.0, 0.0, 0.6666666666666666, 0.5259284104144116, 0.52722139458464, 0.0, 1.0, 0.2132335073766793], 
reward next is 0.7868, 
noisyNet noise sample is [array([0.3419322], dtype=float32), -0.08389939]. 
=============================================
[2019-04-03 23:12:57,556] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1055782e-22 1.5736645e-15 1.7788268e-15 9.9999964e-01 4.9508154e-14
 3.8662102e-07 7.4177074e-18], sum to 1.0000
[2019-04-03 23:12:57,557] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1485
[2019-04-03 23:12:57,605] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.4, 70.0, 0.0, 0.0, 26.0, 23.94239194056714, 0.009342324100220845, 0.0, 1.0, 45376.3883788813], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 164400.0000, 
sim time next is 165000.0000, 
raw observation next is [-8.4, 70.5, 0.0, 0.0, 26.0, 23.87540548801675, -0.002328209215780799, 0.0, 1.0, 45051.0619202872], 
processed observation next is [1.0, 0.9130434782608695, 0.2299168975069252, 0.705, 0.0, 0.0, 0.6666666666666666, 0.4896171240013958, 0.4992239302614064, 0.0, 1.0, 0.2145288662870819], 
reward next is 0.7855, 
noisyNet noise sample is [array([2.0641909], dtype=float32), -0.16313036]. 
=============================================
[2019-04-03 23:12:57,628] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[85.4752 ]
 [85.10319]
 [84.66442]
 [83.95828]
 [83.38039]], R is [[85.73091125]
 [85.65752411]
 [85.58528137]
 [85.51506042]
 [85.44994354]].
[2019-04-03 23:13:02,880] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.3548792e-18 4.8256525e-12 2.7910483e-11 9.0138364e-01 2.3778429e-12
 9.8616324e-02 9.2841265e-15], sum to 1.0000
[2019-04-03 23:13:02,883] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1329
[2019-04-03 23:13:03,010] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-12.8, 51.0, 58.0, 834.5, 26.0, 25.70236312824944, 0.2920155315435997, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 388800.0000, 
sim time next is 389400.0000, 
raw observation next is [-12.61666666666667, 51.0, 58.0, 858.0, 26.0, 25.66412691636672, 0.2153739576516548, 1.0, 1.0, 164705.0403212411], 
processed observation next is [1.0, 0.5217391304347826, 0.11311172668513378, 0.51, 0.19333333333333333, 0.9480662983425414, 0.6666666666666666, 0.63867724303056, 0.5717913192172183, 1.0, 1.0, 0.7843097158154338], 
reward next is 0.2157, 
noisyNet noise sample is [array([-0.38531077], dtype=float32), 1.0680455]. 
=============================================
[2019-04-03 23:13:08,259] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.7152351e-24 4.7965284e-18 3.0371843e-16 1.0000000e+00 6.7077688e-16
 1.4675667e-08 3.1099177e-18], sum to 1.0000
[2019-04-03 23:13:08,274] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6975
[2019-04-03 23:13:08,354] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-12.55, 68.5, 0.0, 0.0, 26.0, 22.45336438780504, -0.2210189414547105, 1.0, 1.0, 203415.8259907421], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 286200.0000, 
sim time next is 286800.0000, 
raw observation next is [-12.63333333333333, 69.0, 0.0, 0.0, 26.0, 23.04111974653452, -0.1004805086245546, 1.0, 1.0, 165254.1093026278], 
processed observation next is [1.0, 0.30434782608695654, 0.11265004616805181, 0.69, 0.0, 0.0, 0.6666666666666666, 0.42009331221121, 0.46650649712514847, 1.0, 1.0, 0.7869243300125133], 
reward next is 0.2131, 
noisyNet noise sample is [array([0.3383046], dtype=float32), 2.0073197]. 
=============================================
[2019-04-03 23:13:33,789] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.1802221e-22 1.7675234e-16 1.8975197e-13 9.9999976e-01 1.6450143e-15
 1.8078448e-07 3.1424628e-17], sum to 1.0000
[2019-04-03 23:13:33,789] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5588
[2019-04-03 23:13:33,821] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.3, 71.0, 0.0, 0.0, 26.0, 24.22054656028456, 0.1325650780178794, 0.0, 1.0, 41672.63542430871], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 780000.0000, 
sim time next is 780600.0000, 
raw observation next is [-7.299999999999999, 71.0, 0.0, 0.0, 26.0, 24.25302272422917, 0.1264065309352622, 0.0, 1.0, 41622.83960031014], 
processed observation next is [1.0, 0.0, 0.2603878116343491, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5210852270190974, 0.5421355103117541, 0.0, 1.0, 0.19820399809671493], 
reward next is 0.8018, 
noisyNet noise sample is [array([0.13600862], dtype=float32), -0.23580356]. 
=============================================
[2019-04-03 23:13:33,898] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.6204782e-23 5.3682956e-17 6.3559987e-16 1.0000000e+00 5.6764591e-16
 5.1848809e-10 2.8443220e-18], sum to 1.0000
[2019-04-03 23:13:33,898] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0175
[2019-04-03 23:13:33,991] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.8, 87.0, 0.0, 0.0, 26.0, 24.97109424068528, 0.2959653278755663, 0.0, 1.0, 78578.15496263282], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 588600.0000, 
sim time next is 589200.0000, 
raw observation next is [-2.8, 87.0, 0.0, 0.0, 26.0, 25.01469693479531, 0.3001986225532348, 0.0, 1.0, 55482.97729239251], 
processed observation next is [0.0, 0.8260869565217391, 0.38504155124653744, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5845580778996092, 0.6000662075177449, 0.0, 1.0, 0.26420465377329766], 
reward next is 0.7358, 
noisyNet noise sample is [array([-0.9056079], dtype=float32), -1.1101084]. 
=============================================
[2019-04-03 23:13:45,066] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.1587677e-25 3.5158376e-16 5.4079903e-15 9.9999988e-01 2.9596127e-18
 8.5103309e-08 1.6012994e-19], sum to 1.0000
[2019-04-03 23:13:45,066] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7631
[2019-04-03 23:13:45,091] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 84.0, 87.0, 0.0, 26.0, 25.43296188036634, 0.2747149579088844, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 903600.0000, 
sim time next is 904200.0000, 
raw observation next is [1.366666666666667, 86.16666666666666, 90.33333333333333, 0.0, 26.0, 25.3756203682509, 0.2697857910238492, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.5004616805170823, 0.8616666666666666, 0.3011111111111111, 0.0, 0.6666666666666666, 0.614635030687575, 0.5899285970079498, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5362289], dtype=float32), -1.6226548]. 
=============================================
[2019-04-03 23:13:53,733] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.6398960e-27 1.5074715e-17 2.0306614e-17 1.0000000e+00 7.2631679e-20
 8.4666452e-09 1.9524107e-20], sum to 1.0000
[2019-04-03 23:13:53,736] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3948
[2019-04-03 23:13:53,815] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.2, 93.0, 84.0, 0.0, 26.0, 24.88470403889965, 0.2358132361492955, 1.0, 1.0, 139166.8478761795], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 916800.0000, 
sim time next is 917400.0000, 
raw observation next is [4.3, 93.0, 78.0, 0.0, 26.0, 24.19516745633791, 0.2602650395204149, 1.0, 1.0, 197131.094790273], 
processed observation next is [1.0, 0.6086956521739131, 0.5817174515235458, 0.93, 0.26, 0.0, 0.6666666666666666, 0.5162639546948258, 0.5867550131734717, 1.0, 1.0, 0.9387194990013], 
reward next is 0.0613, 
noisyNet noise sample is [array([1.9556526], dtype=float32), -0.21267055]. 
=============================================
[2019-04-03 23:13:57,071] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.7480256e-30 9.3388368e-20 1.2034144e-20 1.0000000e+00 2.3328950e-24
 1.8764625e-12 1.9701681e-24], sum to 1.0000
[2019-04-03 23:13:57,072] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6577
[2019-04-03 23:13:57,080] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [14.4, 79.66666666666667, 0.0, 0.0, 26.0, 25.48998963518814, 0.4776454496289291, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1020000.0000, 
sim time next is 1020600.0000, 
raw observation next is [14.4, 79.0, 0.0, 0.0, 26.0, 25.33788668865289, 0.4560790945900579, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.8614958448753465, 0.79, 0.0, 0.0, 0.6666666666666666, 0.6114905573877408, 0.6520263648633526, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.60537153], dtype=float32), -0.78813004]. 
=============================================
[2019-04-03 23:14:00,085] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.4151763e-28 7.0891339e-18 6.2758140e-17 1.0000000e+00 2.5130811e-21
 1.5008274e-09 3.1765278e-22], sum to 1.0000
[2019-04-03 23:14:00,085] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6654
[2019-04-03 23:14:00,100] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.8, 93.0, 100.0, 0.0, 26.0, 25.17928895510666, 0.2507723186549649, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 910800.0000, 
sim time next is 911400.0000, 
raw observation next is [3.8, 93.0, 98.66666666666666, 0.0, 26.0, 25.1424434249811, 0.2433540925838809, 1.0, 1.0, 21037.32552880497], 
processed observation next is [1.0, 0.5652173913043478, 0.5678670360110805, 0.93, 0.32888888888888884, 0.0, 0.6666666666666666, 0.595203618748425, 0.5811180308612937, 1.0, 1.0, 0.10017774061335699], 
reward next is 0.8998, 
noisyNet noise sample is [array([0.08502921], dtype=float32), -1.9907247]. 
=============================================
[2019-04-03 23:14:00,662] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.7987568e-25 7.7367502e-16 9.9371407e-16 9.9999988e-01 5.4913043e-17
 1.7659698e-07 1.7406823e-19], sum to 1.0000
[2019-04-03 23:14:00,662] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9267
[2019-04-03 23:14:00,707] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.9, 82.5, 59.0, 0.0, 26.0, 26.24974235033502, 0.4428121506772768, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 829800.0000, 
sim time next is 830400.0000, 
raw observation next is [-3.899999999999999, 83.66666666666666, 57.33333333333334, 0.0, 26.0, 26.22473625911972, 0.437007263362797, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.35457063711911363, 0.8366666666666666, 0.19111111111111115, 0.0, 0.6666666666666666, 0.6853946882599766, 0.645669087787599, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2176056], dtype=float32), -0.6657666]. 
=============================================
[2019-04-03 23:14:02,545] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.9497149e-29 1.4418344e-17 1.2816696e-18 1.0000000e+00 7.1928266e-21
 1.6973678e-09 2.2621916e-22], sum to 1.0000
[2019-04-03 23:14:02,549] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8414
[2019-04-03 23:14:02,560] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [12.98333333333333, 85.16666666666667, 122.0, 0.0, 26.0, 26.30181150823337, 0.6338479040489211, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 997800.0000, 
sim time next is 998400.0000, 
raw observation next is [13.26666666666667, 84.33333333333334, 120.5, 0.0, 26.0, 26.3600007763422, 0.6465266177692307, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.8301015697137583, 0.8433333333333334, 0.40166666666666667, 0.0, 0.6666666666666666, 0.6966667313618501, 0.7155088725897435, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.04595835], dtype=float32), 0.35135585]. 
=============================================
[2019-04-03 23:14:06,056] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.6907275e-26 2.0286782e-18 1.3946360e-16 1.0000000e+00 1.2206519e-18
 1.9120234e-09 9.8219497e-20], sum to 1.0000
[2019-04-03 23:14:06,057] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4023
[2019-04-03 23:14:06,069] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 26.0, 25.50388314067369, 0.5007445622631085, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1385400.0000, 
sim time next is 1386000.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 26.0, 25.410738522621, 0.4828958008192303, 0.0, 1.0, 59425.1422104944], 
processed observation next is [1.0, 0.043478260869565216, 0.46260387811634357, 0.95, 0.0, 0.0, 0.6666666666666666, 0.61756154355175, 0.6609652669397434, 0.0, 1.0, 0.28297686766902097], 
reward next is 0.7170, 
noisyNet noise sample is [array([0.41457185], dtype=float32), -0.012832326]. 
=============================================
[2019-04-03 23:14:06,093] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.2864898e-32 7.9584234e-21 8.4921515e-20 1.0000000e+00 1.7120177e-25
 1.3302382e-12 3.0671745e-25], sum to 1.0000
[2019-04-03 23:14:06,102] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9296
[2019-04-03 23:14:06,103] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[86.18857]
 [86.25232]
 [86.29344]
 [86.36435]
 [86.43745]], R is [[86.03014374]
 [86.16984558]
 [86.30815125]
 [86.31795502]
 [86.26081848]].
[2019-04-03 23:14:06,115] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [15.5, 76.0, 30.33333333333334, 0.0, 26.0, 24.56134606402571, 0.5307518872305835, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1009200.0000, 
sim time next is 1009800.0000, 
raw observation next is [15.5, 76.5, 25.0, 0.0, 26.0, 25.93589939100807, 0.6317632320901125, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.8919667590027703, 0.765, 0.08333333333333333, 0.0, 0.6666666666666666, 0.6613249492506726, 0.7105877440300375, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.011552], dtype=float32), 1.218806]. 
=============================================
[2019-04-03 23:14:06,357] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.8246620e-35 6.8961032e-25 3.0653046e-24 1.0000000e+00 9.8829787e-29
 1.1389669e-18 2.1055266e-27], sum to 1.0000
[2019-04-03 23:14:06,357] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8660
[2019-04-03 23:14:06,400] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [14.4, 100.0, 86.5, 0.0, 26.0, 24.70654604881751, 0.4488977997975718, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1249200.0000, 
sim time next is 1249800.0000, 
raw observation next is [14.4, 99.33333333333334, 89.33333333333334, 0.0, 26.0, 24.88838206761692, 0.4649776992774488, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.8614958448753465, 0.9933333333333334, 0.2977777777777778, 0.0, 0.6666666666666666, 0.5740318389680766, 0.6549925664258163, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.31574807], dtype=float32), 0.021409081]. 
=============================================
[2019-04-03 23:14:11,512] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.5014670e-30 3.8431501e-19 1.7347638e-18 1.0000000e+00 3.9982351e-22
 4.8837007e-11 1.4943099e-23], sum to 1.0000
[2019-04-03 23:14:11,515] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8009
[2019-04-03 23:14:11,526] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [12.45, 86.0, 128.0, 0.0, 26.0, 25.72521910055691, 0.5547758937898672, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 995400.0000, 
sim time next is 996000.0000, 
raw observation next is [12.53333333333333, 86.0, 126.5, 0.0, 26.0, 26.02035008604125, 0.6025932183352948, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.8097876269621421, 0.86, 0.4216666666666667, 0.0, 0.6666666666666666, 0.6683625071701043, 0.7008644061117649, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.29087317], dtype=float32), -1.3674773]. 
=============================================
[2019-04-03 23:14:11,535] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[102.44249]
 [102.27076]
 [102.20988]
 [102.21423]
 [102.24536]], R is [[102.556427  ]
 [102.5308609 ]
 [102.5055542 ]
 [102.48049927]
 [102.45569611]].
[2019-04-03 23:14:20,781] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.3947652e-34 1.2566836e-25 7.3335342e-24 1.0000000e+00 8.0692679e-29
 1.2829897e-19 3.7751328e-27], sum to 1.0000
[2019-04-03 23:14:20,784] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9238
[2019-04-03 23:14:20,788] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [15.0, 99.33333333333334, 43.33333333333333, 0.0, 26.0, 23.38010743093631, 0.1298513969542226, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1241400.0000, 
sim time next is 1242000.0000, 
raw observation next is [15.0, 100.0, 51.0, 0.0, 26.0, 23.36704124511965, 0.1297088681457992, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.8781163434903049, 1.0, 0.17, 0.0, 0.6666666666666666, 0.4472534370933043, 0.543236289381933, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5770832], dtype=float32), 1.4164783]. 
=============================================
[2019-04-03 23:14:20,798] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[103.37956 ]
 [103.20834 ]
 [103.083206]
 [103.04732 ]
 [103.01706 ]], R is [[103.50488281]
 [103.46983337]
 [103.43513489]
 [103.40078735]
 [103.36678314]].
[2019-04-03 23:14:26,951] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.7861799e-27 5.6862680e-19 1.8794596e-17 1.0000000e+00 4.6310731e-20
 1.6110911e-11 2.3751568e-21], sum to 1.0000
[2019-04-03 23:14:26,951] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7840
[2019-04-03 23:14:26,959] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 86.66666666666667, 87.0, 0.0, 26.0, 25.87073176400173, 0.5256426233081921, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1683600.0000, 
sim time next is 1684200.0000, 
raw observation next is [1.1, 85.33333333333333, 91.0, 0.0, 26.0, 25.86063993712067, 0.5240761987011773, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.49307479224376743, 0.8533333333333333, 0.30333333333333334, 0.0, 0.6666666666666666, 0.6550533280933891, 0.6746920662337258, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.12753986], dtype=float32), -1.1455643]. 
=============================================
[2019-04-03 23:14:32,406] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.95752749e-24 6.75801843e-20 1.66796967e-17 1.00000000e+00
 1.04974295e-16 6.44671220e-13 2.17375646e-19], sum to 1.0000
[2019-04-03 23:14:32,409] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4306
[2019-04-03 23:14:32,434] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.2, 79.0, 0.0, 0.0, 26.0, 23.6388413390288, -0.02210925670918804, 0.0, 1.0, 47080.00699343778], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1835400.0000, 
sim time next is 1836000.0000, 
raw observation next is [-6.2, 79.0, 0.0, 0.0, 26.0, 23.60150005741037, -0.02974377341809982, 0.0, 1.0, 47077.11456613654], 
processed observation next is [0.0, 0.2608695652173913, 0.2908587257617729, 0.79, 0.0, 0.0, 0.6666666666666666, 0.46679167145086414, 0.4900854088606334, 0.0, 1.0, 0.2241767360292216], 
reward next is 0.7758, 
noisyNet noise sample is [array([-1.9802098], dtype=float32), -1.110297]. 
=============================================
[2019-04-03 23:14:32,468] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[76.35097]
 [76.28864]
 [76.23631]
 [76.18752]
 [76.16236]], R is [[76.42526245]
 [76.43682098]
 [76.44828796]
 [76.45957947]
 [76.47065735]].
[2019-04-03 23:14:32,496] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.1759720e-26 1.6395064e-17 1.0199751e-17 1.0000000e+00 1.2539059e-18
 6.4879157e-12 4.7075560e-20], sum to 1.0000
[2019-04-03 23:14:32,497] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1299
[2019-04-03 23:14:32,514] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.45, 91.83333333333334, 0.0, 0.0, 26.0, 25.35830132493592, 0.4593849623677201, 0.0, 1.0, 43476.3486429094], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1732200.0000, 
sim time next is 1732800.0000, 
raw observation next is [0.4, 91.66666666666667, 0.0, 0.0, 26.0, 25.34255350101214, 0.455650681738862, 0.0, 1.0, 43137.17917774404], 
processed observation next is [0.0, 0.043478260869565216, 0.4736842105263158, 0.9166666666666667, 0.0, 0.0, 0.6666666666666666, 0.6118794584176784, 0.6518835605796206, 0.0, 1.0, 0.20541513894163826], 
reward next is 0.7946, 
noisyNet noise sample is [array([0.96815884], dtype=float32), 1.0174901]. 
=============================================
[2019-04-03 23:14:34,742] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.2085929e-30 4.6914171e-22 9.5678851e-21 1.0000000e+00 9.0387304e-23
 6.3209901e-15 6.7434291e-24], sum to 1.0000
[2019-04-03 23:14:34,745] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3677
[2019-04-03 23:14:34,762] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.5, 79.0, 0.0, 0.0, 26.0, 25.45259555318831, 0.4923466442408362, 0.0, 1.0, 55812.34169623457], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1580400.0000, 
sim time next is 1581000.0000, 
raw observation next is [5.416666666666667, 79.50000000000001, 0.0, 0.0, 26.0, 25.45811579656987, 0.4907552884962141, 0.0, 1.0, 39318.55831314804], 
processed observation next is [1.0, 0.30434782608695654, 0.6126500461680519, 0.7950000000000002, 0.0, 0.0, 0.6666666666666666, 0.6215096497141559, 0.6635850961654047, 0.0, 1.0, 0.1872312300626097], 
reward next is 0.8128, 
noisyNet noise sample is [array([-0.91616005], dtype=float32), -0.30133718]. 
=============================================
[2019-04-03 23:14:34,786] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[89.15722 ]
 [88.98991 ]
 [88.875496]
 [88.753975]
 [88.766396]], R is [[89.24402618]
 [89.08581543]
 [88.87612915]
 [88.65498352]
 [88.43066406]].
[2019-04-03 23:14:40,079] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.8994360e-23 5.0926698e-18 4.9694851e-17 1.0000000e+00 1.4941727e-16
 1.0875021e-11 5.7849869e-19], sum to 1.0000
[2019-04-03 23:14:40,080] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7493
[2019-04-03 23:14:40,158] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 87.0, 71.5, 0.0, 26.0, 24.99501535282628, 0.3528534236992689, 0.0, 1.0, 38351.49610800915], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1783200.0000, 
sim time next is 1783800.0000, 
raw observation next is [-3.1, 87.0, 66.0, 0.0, 26.0, 25.01343761567061, 0.3547328819542203, 0.0, 1.0, 33424.40844490723], 
processed observation next is [0.0, 0.6521739130434783, 0.37673130193905824, 0.87, 0.22, 0.0, 0.6666666666666666, 0.5844531346392176, 0.6182442939847401, 0.0, 1.0, 0.1591638497376535], 
reward next is 0.8408, 
noisyNet noise sample is [array([0.20689052], dtype=float32), -0.80098915]. 
=============================================
[2019-04-03 23:14:45,069] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.5832099e-28 3.0825701e-21 1.2062606e-18 1.0000000e+00 3.1429606e-21
 1.6235387e-14 1.2589839e-22], sum to 1.0000
[2019-04-03 23:14:45,076] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0509
[2019-04-03 23:14:45,104] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.45, 91.83333333333334, 0.0, 0.0, 26.0, 25.35768902161118, 0.4594094596173042, 0.0, 1.0, 43521.8738863632], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1732200.0000, 
sim time next is 1732800.0000, 
raw observation next is [0.4, 91.66666666666667, 0.0, 0.0, 26.0, 25.34224720075499, 0.455706836251118, 0.0, 1.0, 43144.30058334359], 
processed observation next is [0.0, 0.043478260869565216, 0.4736842105263158, 0.9166666666666667, 0.0, 0.0, 0.6666666666666666, 0.6118539333962492, 0.6519022787503727, 0.0, 1.0, 0.20544905039687422], 
reward next is 0.7946, 
noisyNet noise sample is [array([-2.051886], dtype=float32), 0.15160903]. 
=============================================
[2019-04-03 23:14:45,905] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.23546770e-26 2.85150741e-15 6.91222362e-17 1.00000000e+00
 2.22832117e-19 6.66717626e-10 1.03785536e-20], sum to 1.0000
[2019-04-03 23:14:45,905] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0230
[2019-04-03 23:14:45,911] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [9.583333333333334, 65.16666666666666, 0.0, 0.0, 26.0, 26.24605098037377, 0.6894288246932372, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1623000.0000, 
sim time next is 1623600.0000, 
raw observation next is [9.4, 66.0, 0.0, 0.0, 26.0, 26.18591742117476, 0.6744344341995351, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.7229916897506927, 0.66, 0.0, 0.0, 0.6666666666666666, 0.6821597850978968, 0.7248114780665117, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3959228], dtype=float32), 0.29571223]. 
=============================================
[2019-04-03 23:14:46,698] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-03 23:14:46,699] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-03 23:14:46,699] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:14:46,701] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-03 23:14:46,702] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:14:46,703] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-03 23:14:46,704] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:14:46,704] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run11
[2019-04-03 23:14:47,445] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run11
[2019-04-03 23:14:47,445] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run11
[2019-04-03 23:15:21,270] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.16951811], dtype=float32), 0.33035794]
[2019-04-03 23:15:21,270] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-4.598252135, 73.27777375, 0.0, 0.0, 25.0, 23.35870831887502, -0.1467159053735388, 0.0, 1.0, 42231.27522984919]
[2019-04-03 23:15:21,270] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-03 23:15:21,271] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.5792453e-23 8.7932329e-19 1.5061617e-16 1.0000000e+00 9.2796529e-17
 1.9921382e-12 7.0144823e-19], sampled 0.20009828526584372
[2019-04-03 23:17:17,830] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7282.0662 208296016.9223 316.5678
[2019-04-03 23:18:10,452] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7225.1813 261535066.1287 611.5549
[2019-04-03 23:18:15,396] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7233.3826 261147766.0421 1500.9732
[2019-04-03 23:18:16,428] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 1000000, evaluation results [1000000.0, 7233.3826307817735, 261147766.04213944, 1500.973204563118, 7282.0661980461955, 208296016.92233184, 316.5677595359409, 7225.181330856192, 261535066.12872317, 611.5549014906542]
[2019-04-03 23:18:22,169] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.8566435e-25 3.6777365e-19 1.2689105e-16 1.0000000e+00 6.7989614e-17
 5.9996431e-12 2.7700397e-19], sum to 1.0000
[2019-04-03 23:18:22,169] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8764
[2019-04-03 23:18:22,214] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.1, 86.33333333333333, 0.0, 0.0, 26.0, 24.94410434612033, 0.3499602449298326, 0.0, 1.0, 43752.31667738047], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1749000.0000, 
sim time next is 1749600.0000, 
raw observation next is [-1.2, 87.0, 0.0, 0.0, 26.0, 24.91515520801207, 0.3440576310718994, 0.0, 1.0, 43814.69128232976], 
processed observation next is [0.0, 0.2608695652173913, 0.42936288088642666, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5762629340010058, 0.6146858770239665, 0.0, 1.0, 0.20864138705871316], 
reward next is 0.7914, 
noisyNet noise sample is [array([-1.1072875], dtype=float32), 1.8722832]. 
=============================================
[2019-04-03 23:18:26,940] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.0228856e-24 6.1602477e-19 2.6749944e-16 1.0000000e+00 3.3536973e-16
 4.9408702e-12 5.0059921e-19], sum to 1.0000
[2019-04-03 23:18:26,940] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3034
[2019-04-03 23:18:27,036] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.7, 78.0, 87.5, 47.0, 26.0, 24.97888006364974, 0.2557485912828533, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1846800.0000, 
sim time next is 1847400.0000, 
raw observation next is [-6.516666666666667, 78.0, 107.6666666666667, 62.66666666666667, 26.0, 25.13099602426166, 0.2653088390244552, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.2820867959372115, 0.78, 0.358888888888889, 0.0692449355432781, 0.6666666666666666, 0.5942496686884716, 0.5884362796748184, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7863087], dtype=float32), -0.05023655]. 
=============================================
[2019-04-03 23:18:34,356] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.3250844e-25 2.6900236e-18 1.2832119e-16 1.0000000e+00 2.7123010e-18
 1.0629567e-11 8.5429682e-20], sum to 1.0000
[2019-04-03 23:18:34,356] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9319
[2019-04-03 23:18:34,369] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.3, 81.50000000000001, 0.0, 0.0, 26.0, 24.39390839140975, 0.1886179099443054, 0.0, 1.0, 42462.8367923837], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2160600.0000, 
sim time next is 2161200.0000, 
raw observation next is [-7.300000000000001, 81.0, 0.0, 0.0, 26.0, 24.34950893949468, 0.1924276254950162, 0.0, 1.0, 42484.45226184854], 
processed observation next is [1.0, 0.0, 0.26038781163434904, 0.81, 0.0, 0.0, 0.6666666666666666, 0.52912574495789, 0.564142541831672, 0.0, 1.0, 0.2023069155326121], 
reward next is 0.7977, 
noisyNet noise sample is [array([-0.00822131], dtype=float32), 1.1857079]. 
=============================================
[2019-04-03 23:18:34,625] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.8172440e-27 1.1490466e-20 6.7449124e-19 1.0000000e+00 5.0561186e-20
 3.7663275e-13 9.0676756e-22], sum to 1.0000
[2019-04-03 23:18:34,625] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1070
[2019-04-03 23:18:34,654] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.100000000000001, 78.66666666666667, 0.0, 0.0, 26.0, 24.33665728339722, 0.1591150552803682, 0.0, 1.0, 42566.88388950402], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2164800.0000, 
sim time next is 2165400.0000, 
raw observation next is [-7.0, 78.5, 0.0, 0.0, 26.0, 24.31473010268187, 0.1562289844228111, 0.0, 1.0, 42585.4333953871], 
processed observation next is [1.0, 0.043478260869565216, 0.2686980609418283, 0.785, 0.0, 0.0, 0.6666666666666666, 0.5262275085568225, 0.552076328140937, 0.0, 1.0, 0.20278777807327192], 
reward next is 0.7972, 
noisyNet noise sample is [array([-1.3807489], dtype=float32), 0.4627113]. 
=============================================
[2019-04-03 23:18:47,669] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.2511636e-25 4.4524535e-20 6.2010898e-17 1.0000000e+00 1.4099979e-18
 8.2033203e-13 3.2013470e-20], sum to 1.0000
[2019-04-03 23:18:47,669] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5162
[2019-04-03 23:18:47,697] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.700000000000001, 81.33333333333334, 0.0, 0.0, 26.0, 23.9850618604744, 0.04796448688163171, 0.0, 1.0, 43623.33262799682], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2096400.0000, 
sim time next is 2097000.0000, 
raw observation next is [-6.7, 80.5, 0.0, 0.0, 26.0, 23.91889351552689, 0.0400639154806099, 0.0, 1.0, 43631.15257518511], 
processed observation next is [1.0, 0.2608695652173913, 0.2770083102493075, 0.805, 0.0, 0.0, 0.6666666666666666, 0.49324112629390743, 0.5133546384935367, 0.0, 1.0, 0.20776739321516718], 
reward next is 0.7922, 
noisyNet noise sample is [array([-0.62489], dtype=float32), -0.36965123]. 
=============================================
[2019-04-03 23:18:47,719] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[83.530365]
 [83.570786]
 [83.64482 ]
 [83.732605]
 [83.830505]], R is [[83.46534729]
 [83.42295837]
 [83.3811264 ]
 [83.33987427]
 [83.29933167]].
[2019-04-03 23:18:57,978] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.5176768e-25 4.0552062e-15 1.8259769e-16 1.0000000e+00 8.6755519e-19
 4.8166443e-10 4.4483809e-19], sum to 1.0000
[2019-04-03 23:18:57,979] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1882
[2019-04-03 23:18:58,052] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.9, 82.0, 22.0, 0.0, 26.0, 25.76391343250082, 0.3642612920145071, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2047800.0000, 
sim time next is 2048400.0000, 
raw observation next is [-3.9, 82.0, 17.0, 0.0, 26.0, 25.36571977626763, 0.3422344299410687, 1.0, 1.0, 63747.95657185437], 
processed observation next is [1.0, 0.7391304347826086, 0.3545706371191136, 0.82, 0.056666666666666664, 0.0, 0.6666666666666666, 0.6138099813556357, 0.6140781433136896, 1.0, 1.0, 0.3035616979612113], 
reward next is 0.6964, 
noisyNet noise sample is [array([-1.6681032], dtype=float32), 1.9581854]. 
=============================================
[2019-04-03 23:19:00,698] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2321233e-25 1.7833956e-20 1.1210314e-17 1.0000000e+00 5.1258228e-18
 1.7265728e-12 1.9625062e-20], sum to 1.0000
[2019-04-03 23:19:00,698] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0628
[2019-04-03 23:19:00,748] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.9, 65.66666666666667, 127.0, 390.0, 26.0, 25.16583249631041, 0.3051158222882135, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2368200.0000, 
sim time next is 2368800.0000, 
raw observation next is [-2.8, 65.0, 130.0, 405.0, 26.0, 25.1087053286628, 0.2965062303193058, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.38504155124653744, 0.65, 0.43333333333333335, 0.44751381215469616, 0.6666666666666666, 0.5923921107219, 0.5988354101064353, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.91277426], dtype=float32), 0.9824219]. 
=============================================
[2019-04-03 23:19:18,758] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.2487118e-27 3.3752745e-22 3.3654972e-18 1.0000000e+00 7.0934785e-21
 3.3388136e-16 7.6049640e-22], sum to 1.0000
[2019-04-03 23:19:18,758] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5042
[2019-04-03 23:19:18,798] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 66.33333333333333, 0.0, 0.0, 26.0, 24.17243051698217, 0.1081109072220013, 0.0, 1.0, 41138.37982180741], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2353200.0000, 
sim time next is 2353800.0000, 
raw observation next is [-2.9, 65.66666666666667, 0.0, 0.0, 26.0, 24.14996806178299, 0.1031366649770448, 0.0, 1.0, 41174.36917079481], 
processed observation next is [0.0, 0.21739130434782608, 0.38227146814404434, 0.6566666666666667, 0.0, 0.0, 0.6666666666666666, 0.5124973384819157, 0.5343788883256816, 0.0, 1.0, 0.19606842462283242], 
reward next is 0.8039, 
noisyNet noise sample is [array([0.45413142], dtype=float32), -0.6670373]. 
=============================================
[2019-04-03 23:19:21,571] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.1443020e-25 2.5303411e-19 1.6623354e-17 1.0000000e+00 4.7933436e-19
 6.1036826e-13 4.0969735e-21], sum to 1.0000
[2019-04-03 23:19:21,571] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5773
[2019-04-03 23:19:21,673] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 47.0, 83.66666666666667, 246.6666666666667, 26.0, 25.02354908487911, 0.3083725152490506, 0.0, 1.0, 18714.57162688416], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2389800.0000, 
sim time next is 2390400.0000, 
raw observation next is [0.0, 47.0, 82.5, 199.5, 26.0, 25.01438450609742, 0.3002212491829522, 0.0, 1.0, 18713.40202648315], 
processed observation next is [0.0, 0.6956521739130435, 0.46260387811634357, 0.47, 0.275, 0.22044198895027625, 0.6666666666666666, 0.5845320421747852, 0.6000737497276507, 0.0, 1.0, 0.08911143822134833], 
reward next is 0.9109, 
noisyNet noise sample is [array([0.846846], dtype=float32), 0.053338595]. 
=============================================
[2019-04-03 23:19:37,436] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.8153507e-24 5.6197936e-16 1.5467815e-15 1.0000000e+00 8.2745797e-18
 2.3931758e-08 2.7282180e-19], sum to 1.0000
[2019-04-03 23:19:37,439] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2375
[2019-04-03 23:19:37,452] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.733333333333333, 28.66666666666667, 178.8333333333333, 405.3333333333334, 26.0, 25.30202180091295, 0.31509375042782, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2553600.0000, 
sim time next is 2554200.0000, 
raw observation next is [3.0, 28.0, 171.0, 482.0, 26.0, 25.41796687346369, 0.3430469376467851, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5457063711911359, 0.28, 0.57, 0.532596685082873, 0.6666666666666666, 0.6181639061219742, 0.6143489792155951, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6318917], dtype=float32), -0.12763976]. 
=============================================
[2019-04-03 23:19:45,187] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [9.2011812e-26 3.8002639e-17 7.2713332e-16 1.0000000e+00 5.9593197e-18
 1.5414966e-09 5.9718072e-20], sum to 1.0000
[2019-04-03 23:19:45,187] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3608
[2019-04-03 23:19:45,253] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.6, 47.0, 204.5, 179.0, 26.0, 24.9289290081535, 0.3890539550285317, 1.0, 1.0, 65733.5378336153], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2638800.0000, 
sim time next is 2639400.0000, 
raw observation next is [-0.4166666666666667, 46.33333333333334, 191.0, 189.6666666666667, 26.0, 25.46073340791816, 0.445743362862745, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.45106186518928904, 0.46333333333333343, 0.6366666666666667, 0.20957642725598533, 0.6666666666666666, 0.62172778399318, 0.6485811209542484, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0093327], dtype=float32), -0.6495812]. 
=============================================
[2019-04-03 23:20:03,075] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.07294187e-24 2.63362882e-19 1.80528198e-16 1.00000000e+00
 1.09830137e-16 7.07914699e-11 6.27181110e-19], sum to 1.0000
[2019-04-03 23:20:03,075] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2687
[2019-04-03 23:20:03,130] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-13.0, 83.5, 97.0, 612.0, 26.0, 25.94194489486375, 0.4320352790369451, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2712600.0000, 
sim time next is 2713200.0000, 
raw observation next is [-12.66666666666667, 81.0, 100.3333333333333, 622.3333333333333, 26.0, 25.98875729780227, 0.4371618033643581, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.11172668513388727, 0.81, 0.3344444444444443, 0.6876611418047881, 0.6666666666666666, 0.6657297748168558, 0.6457206011214527, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4347731], dtype=float32), -0.79935056]. 
=============================================
[2019-04-03 23:20:10,980] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0300388e-26 4.5503162e-17 8.2253007e-17 1.0000000e+00 2.4196663e-21
 3.8352446e-10 1.0175635e-21], sum to 1.0000
[2019-04-03 23:20:10,981] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6269
[2019-04-03 23:20:10,993] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.8, 99.5, 84.0, 679.0, 26.0, 26.80418251521471, 0.8944740340715676, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3166200.0000, 
sim time next is 3166800.0000, 
raw observation next is [6.733333333333333, 99.33333333333334, 79.66666666666666, 649.0, 26.0, 27.14266216272335, 0.9368077813859336, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.649122807017544, 0.9933333333333334, 0.26555555555555554, 0.7171270718232045, 0.6666666666666666, 0.7618885135602792, 0.8122692604619779, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.17909202], dtype=float32), -0.7357993]. 
=============================================
[2019-04-03 23:20:13,282] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.1570721e-25 8.7563078e-20 7.8581596e-18 1.0000000e+00 1.2384327e-18
 2.4799915e-14 4.5838092e-21], sum to 1.0000
[2019-04-03 23:20:13,283] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0393
[2019-04-03 23:20:13,306] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.666666666666667, 65.0, 0.0, 0.0, 26.0, 25.25520089926234, 0.3440987015905828, 0.0, 1.0, 39498.33338493641], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3014400.0000, 
sim time next is 3015000.0000, 
raw observation next is [-3.75, 65.0, 0.0, 0.0, 26.0, 25.22140898676739, 0.3371029422622779, 0.0, 1.0, 39334.22033951075], 
processed observation next is [0.0, 0.9130434782608695, 0.3587257617728532, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6017840822306159, 0.6123676474207592, 0.0, 1.0, 0.18730581114052738], 
reward next is 0.8127, 
noisyNet noise sample is [array([-0.826506], dtype=float32), 0.15519959]. 
=============================================
[2019-04-03 23:20:13,315] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[78.30141]
 [78.39467]
 [78.49182]
 [78.64049]
 [78.61067]], R is [[78.22286987]
 [78.25255585]
 [78.28094482]
 [78.30792236]
 [78.33342743]].
[2019-04-03 23:20:15,347] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.3045719e-26 7.9772947e-22 3.5687614e-19 1.0000000e+00 1.0871285e-19
 2.4971395e-15 4.9772916e-21], sum to 1.0000
[2019-04-03 23:20:15,348] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1556
[2019-04-03 23:20:15,361] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.5, 80.5, 0.0, 0.0, 26.0, 24.23877810983114, 0.1783315930724544, 0.0, 1.0, 42709.52907300323], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2957400.0000, 
sim time next is 2958000.0000, 
raw observation next is [-3.666666666666667, 79.33333333333333, 0.0, 0.0, 26.0, 24.23209757247461, 0.1724758238904855, 0.0, 1.0, 42665.86028435822], 
processed observation next is [0.0, 0.21739130434782608, 0.3610341643582641, 0.7933333333333333, 0.0, 0.0, 0.6666666666666666, 0.5193414643728843, 0.5574919412968286, 0.0, 1.0, 0.20317076325884867], 
reward next is 0.7968, 
noisyNet noise sample is [array([-0.8205862], dtype=float32), -0.59342945]. 
=============================================
[2019-04-03 23:20:15,364] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[79.46109]
 [79.51613]
 [79.51118]
 [79.45012]
 [79.32494]], R is [[79.36914825]
 [79.37207794]
 [79.37475586]
 [79.37710571]
 [79.37908936]].
[2019-04-03 23:20:24,948] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.6613638e-27 3.8865963e-20 1.6456901e-18 1.0000000e+00 5.3241545e-20
 5.9812902e-13 6.4703501e-21], sum to 1.0000
[2019-04-03 23:20:24,949] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6833
[2019-04-03 23:20:24,983] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 60.0, 105.5, 775.5, 26.0, 25.11532731746672, 0.4065961742966546, 0.0, 1.0, 18713.93162957756], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2988000.0000, 
sim time next is 2988600.0000, 
raw observation next is [-2.0, 60.0, 104.0, 767.3333333333334, 26.0, 25.11081716645801, 0.4129330551983731, 0.0, 1.0, 18744.97319734673], 
processed observation next is [0.0, 0.6086956521739131, 0.40720221606648205, 0.6, 0.3466666666666667, 0.8478821362799264, 0.6666666666666666, 0.592568097204834, 0.6376443517327911, 0.0, 1.0, 0.08926177713022253], 
reward next is 0.9107, 
noisyNet noise sample is [array([0.6796947], dtype=float32), 0.12940459]. 
=============================================
[2019-04-03 23:20:26,873] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.8152696e-29 1.6782777e-24 3.0874675e-20 1.0000000e+00 1.4622593e-21
 3.8701337e-18 2.6218652e-23], sum to 1.0000
[2019-04-03 23:20:26,876] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7738
[2019-04-03 23:20:26,910] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 77.0, 27.99999999999999, 22.33333333333333, 26.0, 23.55669705161704, 0.03318024676853068, 0.0, 1.0, 60770.29288841777], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2965800.0000, 
sim time next is 2966400.0000, 
raw observation next is [-4.0, 77.0, 42.0, 29.0, 26.0, 23.53051538269968, 0.04041620916914425, 0.0, 1.0, 60216.6492281961], 
processed observation next is [0.0, 0.34782608695652173, 0.3518005540166205, 0.77, 0.14, 0.032044198895027624, 0.6666666666666666, 0.4608762818916399, 0.513472069723048, 0.0, 1.0, 0.2867459487056957], 
reward next is 0.7133, 
noisyNet noise sample is [array([1.4352574], dtype=float32), -0.42963856]. 
=============================================
[2019-04-03 23:20:28,245] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.1984874e-22 1.0672843e-14 3.0035785e-14 1.0000000e+00 6.6365508e-17
 1.8135237e-08 4.3593851e-17], sum to 1.0000
[2019-04-03 23:20:28,261] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8936
[2019-04-03 23:20:28,286] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 71.0, 9.0, 104.0, 26.0, 26.12886367256177, 0.6050752404175113, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3259800.0000, 
sim time next is 3260400.0000, 
raw observation next is [-4.0, 69.0, 0.0, 0.0, 26.0, 25.96601704555025, 0.6042485475148143, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.3518005540166205, 0.69, 0.0, 0.0, 0.6666666666666666, 0.6638347537958541, 0.701416182504938, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.290851], dtype=float32), -1.0283858]. 
=============================================
[2019-04-03 23:20:35,715] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.0977538e-28 7.7150990e-21 3.1764372e-18 1.0000000e+00 1.1786125e-21
 2.3275701e-13 1.3826713e-21], sum to 1.0000
[2019-04-03 23:20:35,717] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6988
[2019-04-03 23:20:35,763] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 92.0, 85.0, 370.0, 26.0, 25.5989953707539, 0.5620203361449464, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3227400.0000, 
sim time next is 3228000.0000, 
raw observation next is [-3.0, 92.0, 87.66666666666667, 417.1666666666667, 26.0, 25.78202655279839, 0.5761751277223398, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.3795013850415513, 0.92, 0.2922222222222222, 0.4609576427255985, 0.6666666666666666, 0.6485022127331991, 0.6920583759074467, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7011521], dtype=float32), -1.3166096]. 
=============================================
[2019-04-03 23:20:35,775] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[88.702614]
 [88.252525]
 [87.920006]
 [87.587746]
 [87.261375]], R is [[89.08171082]
 [89.19089508]
 [89.29898834]
 [89.40599823]
 [89.51194   ]].
[2019-04-03 23:20:42,760] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.8570497e-31 1.5552427e-22 8.2891632e-21 1.0000000e+00 7.6473680e-25
 2.1930747e-16 1.0466878e-24], sum to 1.0000
[2019-04-03 23:20:42,774] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2765
[2019-04-03 23:20:42,825] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.8333333333333334, 71.16666666666667, 31.33333333333333, 204.3333333333333, 26.0, 25.39955510762009, 0.3933565690918095, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3484200.0000, 
sim time next is 3484800.0000, 
raw observation next is [-1.0, 71.0, 45.5, 253.0, 26.0, 25.34831914489965, 0.3836580135699812, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.4349030470914128, 0.71, 0.15166666666666667, 0.2795580110497238, 0.6666666666666666, 0.6123599287416376, 0.6278860045233271, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3765617], dtype=float32), -0.26935175]. 
=============================================
[2019-04-03 23:20:44,321] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.4903403e-28 6.5780625e-21 2.0710828e-18 1.0000000e+00 3.8738350e-21
 7.9209866e-14 4.0968778e-22], sum to 1.0000
[2019-04-03 23:20:44,325] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1182
[2019-04-03 23:20:44,346] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.45425562337196, 0.4931720261479837, 0.0, 1.0, 71290.37052379863], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3361800.0000, 
sim time next is 3362400.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.41006005098788, 0.5008071357426992, 0.0, 1.0, 77118.45518039155], 
processed observation next is [1.0, 0.9565217391304348, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.61750500424899, 0.6669357119142331, 0.0, 1.0, 0.3672307389542455], 
reward next is 0.6328, 
noisyNet noise sample is [array([0.40191266], dtype=float32), 0.64958817]. 
=============================================
[2019-04-03 23:20:58,056] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.1340059e-27 1.8824989e-20 1.4702495e-17 1.0000000e+00 1.3859467e-19
 5.5771430e-13 1.4323366e-21], sum to 1.0000
[2019-04-03 23:20:58,059] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0581
[2019-04-03 23:20:58,086] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 76.0, 0.0, 0.0, 26.0, 25.70732921150989, 0.5074065310830419, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3535800.0000, 
sim time next is 3536400.0000, 
raw observation next is [-1.0, 74.0, 0.0, 0.0, 26.0, 25.68254840596231, 0.4713894593298523, 0.0, 1.0, 21338.44240551276], 
processed observation next is [1.0, 0.9565217391304348, 0.4349030470914128, 0.74, 0.0, 0.0, 0.6666666666666666, 0.6402123671635259, 0.6571298197766174, 0.0, 1.0, 0.10161163050244172], 
reward next is 0.8984, 
noisyNet noise sample is [array([0.14833836], dtype=float32), -0.062985964]. 
=============================================
[2019-04-03 23:21:01,435] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.22017738e-25 1.09212062e-19 1.06393716e-17 1.00000000e+00
 1.06735953e-18 2.75696417e-14 4.07398795e-20], sum to 1.0000
[2019-04-03 23:21:01,437] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8585
[2019-04-03 23:21:01,463] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-14.0, 69.0, 0.0, 0.0, 26.0, 23.38221157147732, -0.06557961107734388, 0.0, 1.0, 42985.86264591621], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3999600.0000, 
sim time next is 4000200.0000, 
raw observation next is [-13.83333333333333, 68.0, 0.0, 0.0, 26.0, 23.35770982252125, -0.07525666355248942, 0.0, 1.0, 42886.35281033228], 
processed observation next is [1.0, 0.30434782608695654, 0.07940904893813489, 0.68, 0.0, 0.0, 0.6666666666666666, 0.4464758185434376, 0.47491444548250356, 0.0, 1.0, 0.20422072766824897], 
reward next is 0.7958, 
noisyNet noise sample is [array([0.75349104], dtype=float32), 0.12570034]. 
=============================================
[2019-04-03 23:21:08,354] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.6213748e-29 1.7318131e-20 2.3968226e-18 1.0000000e+00 5.4270437e-23
 1.2482020e-12 6.2097824e-23], sum to 1.0000
[2019-04-03 23:21:08,354] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4606
[2019-04-03 23:21:08,386] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.3333333333333333, 29.33333333333333, 120.8333333333333, 820.1666666666666, 26.0, 26.72294549091617, 0.6318514035992417, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4102800.0000, 
sim time next is 4103400.0000, 
raw observation next is [0.6666666666666667, 28.66666666666667, 120.6666666666667, 824.3333333333334, 26.0, 26.28478567293018, 0.6184652219520878, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4810710987996307, 0.28666666666666674, 0.4022222222222223, 0.910865561694291, 0.6666666666666666, 0.690398806077515, 0.7061550739840293, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.94038063], dtype=float32), 0.1691195]. 
=============================================
[2019-04-03 23:21:09,182] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.0735431e-27 1.7047297e-20 1.2530889e-17 1.0000000e+00 3.1446757e-21
 1.6209922e-14 6.3377755e-21], sum to 1.0000
[2019-04-03 23:21:09,184] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2019
[2019-04-03 23:21:09,213] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.97911923953637, 0.3092214658881238, 0.0, 1.0, 43818.42820757724], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3816600.0000, 
sim time next is 3817200.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.96291519329936, 0.3005506910249313, 0.0, 1.0, 43881.89026741477], 
processed observation next is [1.0, 0.17391304347826086, 0.3518005540166205, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5802429327749467, 0.6001835636749772, 0.0, 1.0, 0.20896138222578461], 
reward next is 0.7910, 
noisyNet noise sample is [array([-1.1214805], dtype=float32), -0.0642022]. 
=============================================
[2019-04-03 23:21:11,170] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.78593454e-27 3.94770194e-17 2.10259055e-18 1.00000000e+00
 1.98332859e-22 1.04740565e-11 8.09992129e-22], sum to 1.0000
[2019-04-03 23:21:11,173] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7335
[2019-04-03 23:21:11,191] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.3333333333333333, 29.33333333333333, 120.8333333333333, 820.1666666666666, 26.0, 26.74018227492791, 0.6361939520671203, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4102800.0000, 
sim time next is 4103400.0000, 
raw observation next is [0.6666666666666667, 28.66666666666667, 120.6666666666667, 824.3333333333334, 26.0, 26.30123559929691, 0.6214647229791601, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4810710987996307, 0.28666666666666674, 0.4022222222222223, 0.910865561694291, 0.6666666666666666, 0.6917696332747424, 0.70715490765972, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.85593927], dtype=float32), 0.580744]. 
=============================================
[2019-04-03 23:21:22,181] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.0849448e-27 7.6670999e-21 4.1772895e-19 1.0000000e+00 4.4320291e-22
 2.7729104e-15 6.1148413e-22], sum to 1.0000
[2019-04-03 23:21:22,181] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7163
[2019-04-03 23:21:22,193] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.95, 40.16666666666667, 38.66666666666666, 292.0, 26.0, 25.37573614031077, 0.3881904344784577, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4209000.0000, 
sim time next is 4209600.0000, 
raw observation next is [1.9, 40.33333333333334, 31.33333333333333, 227.5, 26.0, 25.29123829832747, 0.3644556423857637, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.7391304347826086, 0.515235457063712, 0.40333333333333343, 0.10444444444444442, 0.2513812154696133, 0.6666666666666666, 0.6076031915272893, 0.6214852141285879, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.33077574], dtype=float32), 0.59580964]. 
=============================================
[2019-04-03 23:21:24,357] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0002877e-27 2.8976838e-22 3.0550629e-19 1.0000000e+00 1.5790782e-21
 1.1740346e-15 4.8055846e-22], sum to 1.0000
[2019-04-03 23:21:24,357] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1859
[2019-04-03 23:21:24,371] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-12.16666666666667, 64.0, 0.0, 0.0, 26.0, 24.10064708499301, 0.102508096971645, 0.0, 1.0, 43691.59151745447], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3989400.0000, 
sim time next is 3990000.0000, 
raw observation next is [-12.33333333333333, 65.0, 0.0, 0.0, 26.0, 24.06643159969102, 0.09575549270319794, 0.0, 1.0, 43711.49335529501], 
processed observation next is [1.0, 0.17391304347826086, 0.12096029547553101, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5055359666409182, 0.5319184975677326, 0.0, 1.0, 0.20814996835854765], 
reward next is 0.7919, 
noisyNet noise sample is [array([-0.8255899], dtype=float32), -1.0449756]. 
=============================================
[2019-04-03 23:21:24,387] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[82.5661  ]
 [82.469604]
 [82.35359 ]
 [82.207016]
 [82.05267 ]], R is [[82.62229919]
 [82.58802032]
 [82.55429077]
 [82.52106476]
 [82.4881897 ]].
[2019-04-03 23:21:28,232] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.26136447e-28 5.10773521e-23 1.06351006e-19 1.00000000e+00
 1.90718752e-23 5.40124356e-17 1.90656195e-23], sum to 1.0000
[2019-04-03 23:21:28,239] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5174
[2019-04-03 23:21:28,279] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.133333333333333, 68.0, 0.0, 0.0, 26.0, 25.62543296826994, 0.5171145218103755, 0.0, 1.0, 35988.92118273592], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4427400.0000, 
sim time next is 4428000.0000, 
raw observation next is [3.0, 68.0, 0.0, 0.0, 26.0, 25.55838033907826, 0.5175472686588639, 0.0, 1.0, 69657.93847915684], 
processed observation next is [1.0, 0.2608695652173913, 0.5457063711911359, 0.68, 0.0, 0.0, 0.6666666666666666, 0.6298650282565216, 0.6725157562196213, 0.0, 1.0, 0.3317044689483659], 
reward next is 0.6683, 
noisyNet noise sample is [array([-1.0604318], dtype=float32), -0.3977831]. 
=============================================
[2019-04-03 23:21:28,319] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[84.23016 ]
 [84.2184  ]
 [84.344406]
 [84.388756]
 [84.53868 ]], R is [[84.12088776]
 [84.10829926]
 [84.26721954]
 [84.42454529]
 [84.58029938]].
[2019-04-03 23:21:28,480] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0951998e-26 5.5270444e-20 1.7575518e-17 1.0000000e+00 4.4571755e-21
 1.5188400e-13 3.9261877e-21], sum to 1.0000
[2019-04-03 23:21:28,481] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1662
[2019-04-03 23:21:28,517] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 30.66666666666667, 0.0, 0.0, 26.0, 25.36673895205021, 0.4529219825176288, 1.0, 1.0, 26924.90713642444], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4047000.0000, 
sim time next is 4047600.0000, 
raw observation next is [-4.0, 30.33333333333334, 0.0, 0.0, 26.0, 25.34063307379439, 0.4565445821037296, 0.0, 1.0, 198814.0385197999], 
processed observation next is [1.0, 0.8695652173913043, 0.3518005540166205, 0.3033333333333334, 0.0, 0.0, 0.6666666666666666, 0.6117194228161992, 0.6521815273679099, 0.0, 1.0, 0.946733516760952], 
reward next is 0.0533, 
noisyNet noise sample is [array([0.40043512], dtype=float32), 1.6303205]. 
=============================================
[2019-04-03 23:21:29,170] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.3956477e-30 7.2862412e-24 2.6714299e-20 1.0000000e+00 2.1849992e-24
 3.4924171e-17 3.3917163e-23], sum to 1.0000
[2019-04-03 23:21:29,170] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8333
[2019-04-03 23:21:29,234] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.983333333333333, 74.16666666666667, 30.66666666666666, 163.6666666666666, 26.0, 25.40580017583015, 0.3597395948318824, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4348200.0000, 
sim time next is 4348800.0000, 
raw observation next is [3.0, 74.0, 46.0, 245.5, 26.0, 25.3485441236861, 0.3813386765970108, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.5457063711911359, 0.74, 0.15333333333333332, 0.2712707182320442, 0.6666666666666666, 0.6123786769738416, 0.6271128921990036, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.39392528], dtype=float32), -0.96539176]. 
=============================================
[2019-04-03 23:21:35,736] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.8380669e-30 1.6335208e-23 2.3007313e-19 1.0000000e+00 8.2762951e-23
 1.5413316e-17 1.7548801e-23], sum to 1.0000
[2019-04-03 23:21:35,738] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0116
[2019-04-03 23:21:35,759] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.6, 73.0, 0.0, 0.0, 26.0, 25.27522374607248, 0.4398278891843828, 0.0, 1.0, 48211.01865072636], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4500000.0000, 
sim time next is 4500600.0000, 
raw observation next is [-0.6666666666666666, 73.0, 0.0, 0.0, 26.0, 25.28611502507178, 0.4464978396227894, 0.0, 1.0, 44375.55344081133], 
processed observation next is [1.0, 0.08695652173913043, 0.44413665743305636, 0.73, 0.0, 0.0, 0.6666666666666666, 0.607176252089315, 0.6488326132075964, 0.0, 1.0, 0.21131215924195873], 
reward next is 0.7887, 
noisyNet noise sample is [array([-0.422938], dtype=float32), 0.7598839]. 
=============================================
[2019-04-03 23:21:56,273] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1326630e-25 2.5688969e-17 5.0305604e-16 1.0000000e+00 3.4822614e-20
 4.9541664e-12 3.3403098e-20], sum to 1.0000
[2019-04-03 23:21:56,277] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5337
[2019-04-03 23:21:56,293] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.6, 47.5, 40.0, 145.0, 26.0, 27.31501088077335, 0.7744844145984734, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4642200.0000, 
sim time next is 4642800.0000, 
raw observation next is [4.4, 48.0, 33.33333333333334, 120.8333333333333, 26.0, 26.96407417017999, 0.6295863777486246, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.5844875346260389, 0.48, 0.11111111111111115, 0.1335174953959484, 0.6666666666666666, 0.7470061808483326, 0.7098621259162082, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.06620043], dtype=float32), -0.31266758]. 
=============================================
[2019-04-03 23:22:04,616] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.1010485e-28 5.7393238e-22 2.7555890e-20 1.0000000e+00 4.4683747e-22
 2.0031562e-17 3.0178174e-23], sum to 1.0000
[2019-04-03 23:22:04,620] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9464
[2019-04-03 23:22:04,632] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.666666666666667, 36.0, 41.16666666666666, 245.6666666666667, 26.0, 25.09807470869416, 0.3838754119783565, 0.0, 1.0, 42462.1794695506], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4814400.0000, 
sim time next is 4815000.0000, 
raw observation next is [2.5, 37.0, 33.0, 185.0, 26.0, 25.09896740172868, 0.375651349013014, 0.0, 1.0, 28310.19508633866], 
processed observation next is [0.0, 0.7391304347826086, 0.5318559556786704, 0.37, 0.11, 0.20441988950276244, 0.6666666666666666, 0.5915806168107233, 0.6252171163376713, 0.0, 1.0, 0.13481045279208886], 
reward next is 0.8652, 
noisyNet noise sample is [array([-0.5994877], dtype=float32), -0.06662639]. 
=============================================
[2019-04-03 23:22:04,640] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[81.10521 ]
 [81.07357 ]
 [81.09759 ]
 [81.268135]
 [81.581764]], R is [[81.15865326]
 [81.14486694]
 [81.21635437]
 [81.31523895]
 [81.50209045]].
[2019-04-03 23:22:04,809] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:22:04,809] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:22:04,812] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run9
[2019-04-03 23:22:05,049] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.8452720e-29 2.5282011e-25 9.5434570e-22 1.0000000e+00 1.9937523e-21
 1.2354599e-18 1.9138414e-23], sum to 1.0000
[2019-04-03 23:22:05,049] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8429
[2019-04-03 23:22:05,078] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.67616520056011, 0.2767256256202071, 0.0, 1.0, 40588.8015482038], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4763400.0000, 
sim time next is 4764000.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.61548703473772, 0.2677479331688166, 0.0, 1.0, 40634.91885537963], 
processed observation next is [0.0, 0.13043478260869565, 0.296398891966759, 0.92, 0.0, 0.0, 0.6666666666666666, 0.5512905862281432, 0.5892493110562722, 0.0, 1.0, 0.19349961359704584], 
reward next is 0.8065, 
noisyNet noise sample is [array([-0.56620777], dtype=float32), 0.92370564]. 
=============================================
[2019-04-03 23:22:05,097] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[83.79132]
 [83.86851]
 [83.93545]
 [83.99897]
 [84.08425]], R is [[83.78636169]
 [83.75521851]
 [83.72457886]
 [83.6943512 ]
 [83.66439056]].
[2019-04-03 23:22:05,332] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.9316682e-27 6.3275505e-23 1.7204424e-19 1.0000000e+00 5.6799778e-20
 2.8963442e-16 7.5730904e-22], sum to 1.0000
[2019-04-03 23:22:05,337] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7968
[2019-04-03 23:22:05,360] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.35760561506052, 0.2237545491553696, 0.0, 1.0, 41127.74534860066], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4768200.0000, 
sim time next is 4768800.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.40909871935231, 0.2211578026100542, 0.0, 1.0, 41156.67790753775], 
processed observation next is [0.0, 0.17391304347826086, 0.296398891966759, 0.92, 0.0, 0.0, 0.6666666666666666, 0.534091559946026, 0.5737192675366848, 0.0, 1.0, 0.19598418051208452], 
reward next is 0.8040, 
noisyNet noise sample is [array([-1.7433811], dtype=float32), 0.52011824]. 
=============================================
[2019-04-03 23:22:05,540] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0615389e-29 7.1563134e-22 4.6131790e-19 1.0000000e+00 1.0841616e-22
 6.9407071e-17 4.5540707e-23], sum to 1.0000
[2019-04-03 23:22:05,540] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8388
[2019-04-03 23:22:05,550] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [8.7, 19.0, 0.0, 0.0, 26.0, 26.77394110826894, 0.7796469575343078, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5090400.0000, 
sim time next is 5091000.0000, 
raw observation next is [8.65, 19.16666666666667, 0.0, 0.0, 26.0, 26.72135049072393, 0.7264334421905166, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.7022160664819946, 0.1916666666666667, 0.0, 0.0, 0.6666666666666666, 0.7267792075603273, 0.7421444807301723, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.40206113], dtype=float32), -0.95248663]. 
=============================================
[2019-04-03 23:22:05,582] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[83.94001 ]
 [83.71535 ]
 [83.44864 ]
 [83.281044]
 [83.05645 ]], R is [[84.26204681]
 [84.41942596]
 [84.57523346]
 [84.72948456]
 [84.88218689]].
[2019-04-03 23:22:06,797] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:22:06,797] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:22:06,802] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run9
[2019-04-03 23:22:07,724] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.8073845e-30 2.8683278e-24 4.2688634e-22 1.0000000e+00 2.6534780e-24
 1.8919255e-17 1.2303726e-23], sum to 1.0000
[2019-04-03 23:22:07,726] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7846
[2019-04-03 23:22:07,745] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.666666666666667, 41.0, 178.3333333333333, 750.5, 26.0, 25.10106845570872, 0.4435045561176152, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4797600.0000, 
sim time next is 4798200.0000, 
raw observation next is [1.833333333333333, 40.5, 186.6666666666667, 714.0, 26.0, 25.17871900877367, 0.4461770804031054, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.5133887349953832, 0.405, 0.6222222222222223, 0.7889502762430939, 0.6666666666666666, 0.5982265840644724, 0.6487256934677018, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.55173016], dtype=float32), 1.0472566]. 
=============================================
[2019-04-03 23:22:10,611] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.5638202e-28 1.7390972e-18 2.9502122e-18 1.0000000e+00 2.8971201e-22
 5.5771533e-13 3.9797467e-22], sum to 1.0000
[2019-04-03 23:22:10,611] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3691
[2019-04-03 23:22:10,645] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [10.33333333333333, 21.66666666666666, 116.8333333333333, 853.1666666666667, 26.0, 27.5558736702013, 0.9273350617123653, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5060400.0000, 
sim time next is 5061000.0000, 
raw observation next is [10.66666666666667, 20.83333333333334, 115.6666666666667, 846.3333333333333, 26.0, 26.95950805535361, 0.8919487530956798, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.7580794090489382, 0.2083333333333334, 0.38555555555555565, 0.9351749539594842, 0.6666666666666666, 0.7466256712794674, 0.7973162510318933, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.08548321], dtype=float32), -0.49336043]. 
=============================================
[2019-04-03 23:22:10,669] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[83.99405 ]
 [84.2109  ]
 [84.387665]
 [84.42512 ]
 [84.57886 ]], R is [[84.11542511]
 [84.2742691 ]
 [84.43152618]
 [84.58721161]
 [84.74134064]].
[2019-04-03 23:22:10,725] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:22:10,725] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:22:10,728] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run9
[2019-04-03 23:22:13,357] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:22:13,357] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:22:13,370] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run9
[2019-04-03 23:22:13,704] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.5905251e-30 2.4133320e-22 3.7351108e-19 1.0000000e+00 4.0202280e-22
 5.8464065e-15 1.4103547e-23], sum to 1.0000
[2019-04-03 23:22:13,704] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3404
[2019-04-03 23:22:13,723] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.333333333333333, 43.0, 112.6666666666667, 716.5, 26.0, 26.72820058517025, 0.6791936618355437, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5046000.0000, 
sim time next is 5046600.0000, 
raw observation next is [2.666666666666667, 42.0, 113.3333333333333, 735.0, 26.0, 26.91699537166572, 0.6925116667220214, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.5364727608494922, 0.42, 0.37777777777777766, 0.8121546961325967, 0.6666666666666666, 0.74308294763881, 0.7308372222406737, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6041805], dtype=float32), -0.13205531]. 
=============================================
[2019-04-03 23:22:15,885] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:22:15,885] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:22:15,897] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run9
[2019-04-03 23:22:17,330] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:22:17,330] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:22:17,334] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run9
[2019-04-03 23:22:20,361] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:22:20,361] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:22:20,364] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run9
[2019-04-03 23:22:21,185] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:22:21,185] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:22:21,191] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run9
[2019-04-03 23:22:21,636] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:22:21,638] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:22:21,656] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run9
[2019-04-03 23:22:24,354] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:22:24,354] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:22:24,358] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run9
[2019-04-03 23:22:24,404] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:22:24,405] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:22:24,411] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run9
[2019-04-03 23:22:26,005] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:22:26,005] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:22:26,024] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run9
[2019-04-03 23:22:29,632] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:22:29,632] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:22:29,654] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run9
[2019-04-03 23:22:29,700] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:22:29,701] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:22:29,706] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run9
[2019-04-03 23:22:30,378] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [9.5899505e-22 1.7025041e-16 2.3739401e-16 1.0000000e+00 1.2191635e-13
 8.8900467e-12 7.4050722e-18], sum to 1.0000
[2019-04-03 23:22:30,405] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9371
[2019-04-03 23:22:30,438] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.6, 82.0, 34.0, 0.0, 19.0, 18.1206848200649, -1.210229582005826, 0.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 57600.0000, 
sim time next is 58200.0000, 
raw observation next is [6.416666666666667, 82.66666666666667, 28.66666666666666, 0.0, 19.0, 18.11686224468156, -1.212872128365692, 0.0, 1.0, 18680.41167023054], 
processed observation next is [0.0, 0.6956521739130435, 0.6403508771929826, 0.8266666666666667, 0.09555555555555553, 0.0, 0.08333333333333333, 0.009738520390129976, 0.09570929054476933, 0.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.71309793], dtype=float32), 0.8828138]. 
=============================================
[2019-04-03 23:22:30,545] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:22:30,545] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:22:30,548] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run9
[2019-04-03 23:22:35,142] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:22:35,143] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:22:35,147] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run9
[2019-04-03 23:22:40,636] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.2955355e-27 4.0926021e-20 1.6989795e-18 1.0000000e+00 4.0048510e-19
 1.2519365e-11 3.5479218e-21], sum to 1.0000
[2019-04-03 23:22:40,639] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7443
[2019-04-03 23:22:40,664] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 25.0, 22.10948922915838, -0.4301912419645801, 0.0, 1.0, 44972.21224314787], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 183600.0000, 
sim time next is 184200.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 25.0, 22.06903276178767, -0.4394984913243754, 0.0, 1.0, 45027.22525456407], 
processed observation next is [1.0, 0.13043478260869565, 0.21606648199445982, 0.78, 0.0, 0.0, 0.5833333333333334, 0.3390860634823059, 0.3535005028918749, 0.0, 1.0, 0.214415358355067], 
reward next is 0.7856, 
noisyNet noise sample is [array([1.3814243], dtype=float32), -1.170986]. 
=============================================
[2019-04-03 23:22:45,131] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.5071938e-21 9.5851290e-15 8.3382947e-14 9.9985600e-01 1.9010748e-14
 1.4404352e-04 2.5229247e-17], sum to 1.0000
[2019-04-03 23:22:45,131] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3213
[2019-04-03 23:22:45,213] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-10.6, 60.0, 96.5, 585.0, 26.0, 24.90509651771147, 0.1034508665490762, 1.0, 1.0, 45846.3274968872], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 298800.0000, 
sim time next is 299400.0000, 
raw observation next is [-10.6, 58.16666666666666, 99.66666666666666, 609.3333333333334, 26.0, 25.12356580134641, 0.1288001455598976, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.1689750692520776, 0.5816666666666666, 0.3322222222222222, 0.6732965009208104, 0.6666666666666666, 0.593630483445534, 0.5429333818532992, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4253417], dtype=float32), 0.05767596]. 
=============================================
[2019-04-03 23:22:46,962] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.5107306e-21 3.6729588e-14 8.3995885e-13 7.7470160e-01 2.2503550e-13
 2.2529842e-01 1.9861326e-16], sum to 1.0000
[2019-04-03 23:22:46,965] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4060
[2019-04-03 23:22:47,022] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.433333333333334, 61.0, 137.5, 503.8333333333334, 26.0, 24.98902890438772, 0.1807989858424361, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 134400.0000, 
sim time next is 135000.0000, 
raw observation next is [-7.25, 61.0, 139.0, 484.0, 26.0, 25.09113772581734, 0.1916867847222787, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.26177285318559557, 0.61, 0.4633333333333333, 0.5348066298342542, 0.6666666666666666, 0.5909281438181117, 0.5638955949074262, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.173948], dtype=float32), 0.014052779]. 
=============================================
[2019-04-03 23:22:47,026] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[90.500725]
 [89.75594 ]
 [88.86068 ]
 [87.7654  ]
 [86.93148 ]], R is [[91.24581146]
 [91.33335114]
 [91.33055115]
 [91.1707077 ]
 [90.95075226]].
[2019-04-03 23:22:49,203] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.3573445e-26 2.2866594e-21 4.8913952e-17 1.0000000e+00 4.8940596e-19
 6.6662197e-11 2.7312677e-20], sum to 1.0000
[2019-04-03 23:22:49,204] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0755
[2019-04-03 23:22:49,256] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.766666666666666, 43.33333333333333, 0.0, 0.0, 26.0, 22.11530829668158, -0.4451812793154271, 0.0, 1.0, 46752.76822817305], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 456000.0000, 
sim time next is 456600.0000, 
raw observation next is [-8.583333333333334, 43.16666666666667, 0.0, 0.0, 26.0, 22.11838760798253, -0.4560776873079542, 0.0, 1.0, 46444.38121988855], 
processed observation next is [1.0, 0.2608695652173913, 0.22483841181902123, 0.4316666666666667, 0.0, 0.0, 0.6666666666666666, 0.34319896733187755, 0.3479741042306819, 0.0, 1.0, 0.2211637200947074], 
reward next is 0.7788, 
noisyNet noise sample is [array([-0.4617903], dtype=float32), 1.0453981]. 
=============================================
[2019-04-03 23:23:01,741] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.4725925e-18 7.9412431e-12 1.4510529e-10 6.5865856e-01 4.4250710e-12
 3.4134147e-01 1.9555539e-13], sum to 1.0000
[2019-04-03 23:23:01,742] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4957
[2019-04-03 23:23:01,833] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-12.38333333333333, 64.16666666666667, 0.0, 0.0, 26.0, 25.199458146787, 0.3111302347618491, 1.0, 1.0, 70768.11274419767], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 328200.0000, 
sim time next is 328800.0000, 
raw observation next is [-12.46666666666667, 65.33333333333334, 0.0, 0.0, 26.0, 25.19833237852966, 0.305910898399224, 1.0, 1.0, 66412.1143275225], 
processed observation next is [1.0, 0.8260869565217391, 0.11726685133887339, 0.6533333333333334, 0.0, 0.0, 0.6666666666666666, 0.5998610315441383, 0.601970299466408, 1.0, 1.0, 0.3162481634643929], 
reward next is 0.6838, 
noisyNet noise sample is [array([-0.7325203], dtype=float32), 0.83818525]. 
=============================================
[2019-04-03 23:23:10,013] A3C_AGENT_WORKER-Thread-4 INFO:Evaluating...
[2019-04-03 23:23:10,019] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-03 23:23:10,020] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:23:10,021] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run12
[2019-04-03 23:23:10,039] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-03 23:23:10,040] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-03 23:23:10,040] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:23:10,040] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:23:10,043] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run12
[2019-04-03 23:23:10,056] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run12
[2019-04-03 23:23:35,626] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.10120614], dtype=float32), 0.4620784]
[2019-04-03 23:23:35,627] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-5.979842137666666, 63.46015106166666, 179.6610555666667, 156.5916333, 26.0, 25.00635455668208, 0.2844502197652569, 0.0, 1.0, 18762.85842128588]
[2019-04-03 23:23:35,627] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-03 23:23:35,640] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [6.1525495e-25 2.0791482e-19 5.3518969e-17 1.0000000e+00 3.4335126e-18
 2.1797283e-10 1.1643093e-19], sampled 0.26694429169359746
[2019-04-03 23:24:23,975] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.10120614], dtype=float32), 0.4620784]
[2019-04-03 23:24:23,975] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-2.233333333333333, 68.33333333333334, 0.0, 0.0, 24.0, 23.2320635844458, -0.101744351762916, 0.0, 1.0, 45478.7833848907]
[2019-04-03 23:24:23,975] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-03 23:24:23,976] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.4328550e-22 2.4300670e-18 8.2888998e-16 1.0000000e+00 5.4586017e-16
 2.0956765e-10 6.8312471e-18], sampled 0.1927079754155926
[2019-04-03 23:25:41,487] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7338.4138 208724862.5432 343.7727
[2019-04-03 23:26:28,280] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7232.8104 261011585.2458 1500.7305
[2019-04-03 23:26:35,358] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7170.1357 273601524.3194 1180.6849
[2019-04-03 23:26:36,392] A3C_AGENT_WORKER-Thread-4 INFO:Global step: 1100000, evaluation results [1100000.0, 7232.810372331253, 261011585.24580604, 1500.730468505664, 7338.41381810364, 208724862.54321727, 343.77271126983953, 7170.135731141093, 273601524.3194218, 1180.6849227371808]
[2019-04-03 23:26:42,370] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.6313424e-21 2.4461764e-17 1.4262600e-13 9.9999976e-01 1.0775066e-15
 2.4774823e-07 1.2318661e-15], sum to 1.0000
[2019-04-03 23:26:42,371] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4707
[2019-04-03 23:26:42,385] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-15.0, 69.0, 0.0, 0.0, 26.0, 23.14876487520826, -0.1777713295097878, 0.0, 1.0, 48477.43165702767], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 352800.0000, 
sim time next is 353400.0000, 
raw observation next is [-15.0, 69.0, 0.0, 0.0, 26.0, 23.08590816794911, -0.2015905430058283, 0.0, 1.0, 48699.48674684575], 
processed observation next is [1.0, 0.08695652173913043, 0.04709141274238226, 0.69, 0.0, 0.0, 0.6666666666666666, 0.4238256806624259, 0.4328031523313906, 0.0, 1.0, 0.23190231784212262], 
reward next is 0.7681, 
noisyNet noise sample is [array([-0.23773138], dtype=float32), -0.005294968]. 
=============================================
[2019-04-03 23:26:58,935] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0376906e-22 2.6841905e-16 9.9099534e-15 9.9999869e-01 1.8453714e-16
 1.3382441e-06 2.8741817e-18], sum to 1.0000
[2019-04-03 23:26:58,935] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9020
[2019-04-03 23:26:59,014] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.9, 83.33333333333334, 0.0, 0.0, 26.0, 24.27022449853487, 0.2690714870769977, 1.0, 1.0, 201640.2991345395], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 843600.0000, 
sim time next is 844200.0000, 
raw observation next is [-3.9, 84.0, 0.0, 0.0, 26.0, 24.75317572111235, 0.3343455818091832, 1.0, 1.0, 67286.54901559428], 
processed observation next is [1.0, 0.782608695652174, 0.3545706371191136, 0.84, 0.0, 0.0, 0.6666666666666666, 0.562764643426029, 0.6114485272697278, 1.0, 1.0, 0.3204121381694966], 
reward next is 0.6796, 
noisyNet noise sample is [array([-0.8516274], dtype=float32), 1.3479563]. 
=============================================
[2019-04-03 23:27:15,303] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.3062975e-27 2.1326534e-21 1.0389378e-18 1.0000000e+00 7.8799107e-21
 1.0031288e-12 6.0888676e-21], sum to 1.0000
[2019-04-03 23:27:15,304] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4262
[2019-04-03 23:27:15,316] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.0, 96.0, 0.0, 0.0, 26.0, 25.21227206928739, 0.4010625950554335, 0.0, 1.0, 38379.64483643151], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 949800.0000, 
sim time next is 950400.0000, 
raw observation next is [5.0, 96.0, 0.0, 0.0, 26.0, 25.20411423264312, 0.3981525397672903, 0.0, 1.0, 38319.66483958188], 
processed observation next is [1.0, 0.0, 0.6011080332409973, 0.96, 0.0, 0.0, 0.6666666666666666, 0.6003428527202601, 0.6327175132557634, 0.0, 1.0, 0.18247459447419942], 
reward next is 0.8175, 
noisyNet noise sample is [array([0.37955448], dtype=float32), 0.2573272]. 
=============================================
[2019-04-03 23:27:28,916] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.4661033e-29 1.0147769e-20 2.6240305e-20 1.0000000e+00 4.9099091e-22
 5.8299017e-12 2.6122813e-23], sum to 1.0000
[2019-04-03 23:27:28,921] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8937
[2019-04-03 23:27:28,989] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.600000000000001, 94.66666666666667, 0.0, 0.0, 26.0, 24.97181905082783, 0.2644039596885656, 1.0, 1.0, 37899.62501828182], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 927600.0000, 
sim time next is 928200.0000, 
raw observation next is [4.5, 95.33333333333333, 0.0, 0.0, 26.0, 24.76960025028725, 0.2844366843985779, 1.0, 1.0, 144114.5065725927], 
processed observation next is [1.0, 0.7391304347826086, 0.5872576177285319, 0.9533333333333333, 0.0, 0.0, 0.6666666666666666, 0.5641333541906043, 0.5948122281328593, 1.0, 1.0, 0.6862595551075843], 
reward next is 0.3137, 
noisyNet noise sample is [array([0.08867473], dtype=float32), -1.8352456]. 
=============================================
[2019-04-03 23:27:35,497] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.4466529e-34 2.4791973e-25 1.6874312e-25 1.0000000e+00 3.3477700e-29
 7.6862228e-21 6.2682456e-27], sum to 1.0000
[2019-04-03 23:27:35,497] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7001
[2019-04-03 23:27:35,542] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [16.0, 84.66666666666667, 0.0, 0.0, 26.0, 23.98104560589657, 0.2574512213545475, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1217400.0000, 
sim time next is 1218000.0000, 
raw observation next is [15.9, 86.33333333333334, 0.0, 0.0, 26.0, 24.03709820921365, 0.2558578064648737, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.08695652173913043, 0.9030470914127425, 0.8633333333333334, 0.0, 0.0, 0.6666666666666666, 0.5030915174344708, 0.5852859354882912, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0964646], dtype=float32), 0.4472719]. 
=============================================
[2019-04-03 23:27:35,551] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[94.84708]
 [94.73574]
 [94.71568]
 [94.75321]
 [94.79856]], R is [[95.00836182]
 [95.05828094]
 [95.10769653]
 [95.15662384]
 [95.20505524]].
[2019-04-03 23:27:45,827] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.2424308e-29 3.6617516e-18 2.0700166e-18 1.0000000e+00 4.7519258e-26
 3.1100072e-14 3.2179817e-23], sum to 1.0000
[2019-04-03 23:27:45,827] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1137
[2019-04-03 23:27:46,012] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [18.55, 49.5, 35.0, 0.0, 26.0, 27.97572798378895, 1.04229963094413, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1096200.0000, 
sim time next is 1096800.0000, 
raw observation next is [18.26666666666667, 49.66666666666666, 29.33333333333334, 0.4999999999999999, 26.0, 28.02315424917211, 1.05006134241221, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.968605724838412, 0.4966666666666666, 0.0977777777777778, 0.0005524861878453037, 0.6666666666666666, 0.8352628540976758, 0.8500204474707367, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4479621], dtype=float32), -0.4127369]. 
=============================================
[2019-04-03 23:27:56,195] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.9557193e-29 3.4354481e-20 2.4450724e-20 1.0000000e+00 2.7498318e-22
 1.2665266e-12 3.9252551e-23], sum to 1.0000
[2019-04-03 23:27:56,195] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5579
[2019-04-03 23:27:56,261] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [14.4, 80.33333333333333, 0.0, 0.0, 26.0, 25.80609759886279, 0.546405546744331, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1019400.0000, 
sim time next is 1020000.0000, 
raw observation next is [14.4, 79.66666666666667, 0.0, 0.0, 26.0, 25.73869400770236, 0.5277563316408549, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.8614958448753465, 0.7966666666666667, 0.0, 0.0, 0.6666666666666666, 0.64489116730853, 0.6759187772136183, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.7006376], dtype=float32), 0.44511732]. 
=============================================
[2019-04-03 23:27:56,298] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[92.91234]
 [92.79849]
 [93.29585]
 [93.43616]
 [93.51051]], R is [[93.06091309]
 [93.13030243]
 [93.1989975 ]
 [93.26700592]
 [93.33433533]].
[2019-04-03 23:28:09,555] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.7290529e-36 4.6823356e-29 9.8777775e-29 1.0000000e+00 9.9411124e-33
 7.1420666e-25 3.2303555e-30], sum to 1.0000
[2019-04-03 23:28:09,565] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8732
[2019-04-03 23:28:09,572] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [15.0, 96.0, 0.0, 0.0, 26.0, 23.69363572651105, 0.1880141270596137, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1227600.0000, 
sim time next is 1228200.0000, 
raw observation next is [15.0, 96.0, 0.0, 0.0, 26.0, 23.66950996406156, 0.1848394522495108, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.21739130434782608, 0.8781163434903049, 0.96, 0.0, 0.0, 0.6666666666666666, 0.47245916367179674, 0.5616131507498369, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0403508], dtype=float32), 0.10921802]. 
=============================================
[2019-04-03 23:28:10,139] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.5342515e-28 3.2186338e-22 1.9171418e-18 1.0000000e+00 9.9153762e-22
 5.6601853e-13 1.5584623e-21], sum to 1.0000
[2019-04-03 23:28:10,139] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7080
[2019-04-03 23:28:10,245] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.8, 92.0, 0.0, 0.0, 26.0, 25.52131559037893, 0.4690687855691333, 0.0, 1.0, 21902.40800677666], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1473600.0000, 
sim time next is 1474200.0000, 
raw observation next is [1.9, 92.0, 0.0, 0.0, 26.0, 25.45677065445726, 0.4626546859091827, 0.0, 1.0, 61348.24407189861], 
processed observation next is [1.0, 0.043478260869565216, 0.515235457063712, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6213975545381049, 0.6542182286363942, 0.0, 1.0, 0.29213449558046956], 
reward next is 0.7079, 
noisyNet noise sample is [array([0.5762157], dtype=float32), 0.93285704]. 
=============================================
[2019-04-03 23:28:19,442] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.7819407e-27 7.4555100e-22 7.7034907e-19 1.0000000e+00 6.2552226e-21
 1.9331166e-12 5.4071080e-21], sum to 1.0000
[2019-04-03 23:28:19,464] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4284
[2019-04-03 23:28:19,549] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 92.0, 0.0, 0.0, 26.0, 25.47609062423321, 0.5780919888784597, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1305000.0000, 
sim time next is 1305600.0000, 
raw observation next is [2.9, 92.0, 0.0, 0.0, 26.0, 25.52089072674691, 0.5762767464286384, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.5429362880886427, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6267408938955757, 0.6920922488095461, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.04028907], dtype=float32), 0.0031748882]. 
=============================================
[2019-04-03 23:28:23,615] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.7305863e-25 3.1082453e-18 8.4057558e-17 1.0000000e+00 1.1405625e-18
 3.0345373e-09 4.7249713e-20], sum to 1.0000
[2019-04-03 23:28:23,615] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8898
[2019-04-03 23:28:23,671] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.3, 92.0, 66.0, 0.0, 26.0, 25.99050371796159, 0.5388245478619321, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1679400.0000, 
sim time next is 1680000.0000, 
raw observation next is [1.233333333333333, 92.0, 68.83333333333333, 0.0, 26.0, 25.90447171996067, 0.5360033246141992, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.49676823638042483, 0.92, 0.22944444444444442, 0.0, 0.6666666666666666, 0.6587059766633893, 0.6786677748713997, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3419613], dtype=float32), 0.21617731]. 
=============================================
[2019-04-03 23:28:23,764] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[85.504295]
 [85.69286 ]
 [85.913   ]
 [86.12078 ]
 [86.3693  ]], R is [[85.60314178]
 [85.74710846]
 [85.88964081]
 [86.03074646]
 [86.17044067]].
[2019-04-03 23:28:45,464] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.7234363e-28 1.8126680e-21 4.6908694e-19 1.0000000e+00 1.4221174e-21
 2.9283946e-13 1.0942797e-21], sum to 1.0000
[2019-04-03 23:28:45,468] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9652
[2019-04-03 23:28:45,494] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.6, 97.0, 0.0, 0.0, 26.0, 25.67671162085968, 0.5678495023547137, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1657200.0000, 
sim time next is 1657800.0000, 
raw observation next is [6.6, 97.0, 0.0, 0.0, 26.0, 25.70211997737071, 0.5583241883225069, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.6454293628808865, 0.97, 0.0, 0.0, 0.6666666666666666, 0.6418433314475592, 0.6861080627741689, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.67843777], dtype=float32), -0.2807706]. 
=============================================
[2019-04-03 23:29:02,665] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.3486854e-22 1.8225173e-17 5.1270820e-15 1.0000000e+00 5.7731791e-16
 1.6498556e-09 9.8630353e-18], sum to 1.0000
[2019-04-03 23:29:02,677] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2348
[2019-04-03 23:29:02,758] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 81.66666666666667, 110.0, 27.99999999999999, 26.0, 25.07965432458941, 0.2776599000677906, 0.0, 1.0, 21216.00567062446], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1869000.0000, 
sim time next is 1869600.0000, 
raw observation next is [-4.5, 80.33333333333334, 91.0, 14.0, 26.0, 25.06356295768926, 0.2678219013831455, 0.0, 1.0, 38958.61457449508], 
processed observation next is [0.0, 0.6521739130434783, 0.3379501385041552, 0.8033333333333335, 0.30333333333333334, 0.015469613259668509, 0.6666666666666666, 0.588630246474105, 0.5892739671277152, 0.0, 1.0, 0.18551721225950038], 
reward next is 0.8145, 
noisyNet noise sample is [array([-0.48367262], dtype=float32), -0.56018746]. 
=============================================
[2019-04-03 23:29:04,856] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.2092131e-25 6.8082167e-20 1.2933003e-17 1.0000000e+00 7.3335812e-19
 6.8320757e-12 5.5890817e-20], sum to 1.0000
[2019-04-03 23:29:04,857] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2853
[2019-04-03 23:29:04,918] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.8, 87.0, 82.5, 0.0, 26.0, 24.98263304571901, 0.3462391263661795, 0.0, 1.0, 33501.55448765568], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1782000.0000, 
sim time next is 1782600.0000, 
raw observation next is [-2.9, 87.0, 77.0, 0.0, 26.0, 24.99574473846824, 0.3438583556901009, 0.0, 1.0, 33145.61338840837], 
processed observation next is [0.0, 0.6521739130434783, 0.38227146814404434, 0.87, 0.25666666666666665, 0.0, 0.6666666666666666, 0.5829787282056866, 0.6146194518967003, 0.0, 1.0, 0.15783625423051603], 
reward next is 0.8422, 
noisyNet noise sample is [array([0.6677106], dtype=float32), -0.6206621]. 
=============================================
[2019-04-03 23:29:07,075] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.3860884e-28 4.3285522e-22 8.1048521e-18 1.0000000e+00 1.5142982e-21
 1.6751817e-13 1.0928863e-21], sum to 1.0000
[2019-04-03 23:29:07,075] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0250
[2019-04-03 23:29:07,090] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 91.0, 0.0, 0.0, 26.0, 24.6899081330006, 0.2353557569331132, 0.0, 1.0, 42687.24308721832], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2077200.0000, 
sim time next is 2077800.0000, 
raw observation next is [-4.5, 90.16666666666667, 0.0, 0.0, 26.0, 24.66077336246945, 0.233896817651548, 0.0, 1.0, 42695.75414435917], 
processed observation next is [1.0, 0.043478260869565216, 0.3379501385041552, 0.9016666666666667, 0.0, 0.0, 0.6666666666666666, 0.555064446872454, 0.5779656058838493, 0.0, 1.0, 0.2033131149731389], 
reward next is 0.7967, 
noisyNet noise sample is [array([0.73499036], dtype=float32), -1.0007112]. 
=============================================
[2019-04-03 23:29:07,209] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.3345660e-29 1.1369721e-22 6.1345224e-19 1.0000000e+00 3.5945514e-22
 1.8435814e-14 4.3960293e-22], sum to 1.0000
[2019-04-03 23:29:07,225] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3429
[2019-04-03 23:29:07,245] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.100000000000001, 86.33333333333333, 0.0, 0.0, 26.0, 24.31415746964681, 0.1557991874071051, 0.0, 1.0, 41916.55901493226], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1991400.0000, 
sim time next is 1992000.0000, 
raw observation next is [-6.0, 85.66666666666667, 0.0, 0.0, 26.0, 24.37326332616276, 0.1542567881175143, 0.0, 1.0, 41817.56437397633], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.8566666666666667, 0.0, 0.0, 0.6666666666666666, 0.53110527718023, 0.5514189293725048, 0.0, 1.0, 0.19913125892369682], 
reward next is 0.8009, 
noisyNet noise sample is [array([0.4614161], dtype=float32), 0.16844815]. 
=============================================
[2019-04-03 23:29:07,255] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[84.87323 ]
 [84.84673 ]
 [84.8837  ]
 [84.964066]
 [85.03334 ]], R is [[84.82108307]
 [84.77326965]
 [84.72550201]
 [84.67772675]
 [84.62992096]].
[2019-04-03 23:29:20,897] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.9140482e-27 1.1625197e-21 2.2287908e-17 1.0000000e+00 1.5388139e-21
 1.8154322e-14 3.3941643e-22], sum to 1.0000
[2019-04-03 23:29:20,897] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9347
[2019-04-03 23:29:20,922] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.7, 75.5, 0.0, 0.0, 26.0, 24.42506837402362, 0.1812401995380218, 0.0, 1.0, 44242.12166101408], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2249400.0000, 
sim time next is 2250000.0000, 
raw observation next is [-6.7, 75.0, 0.0, 0.0, 26.0, 24.4048296743112, 0.172674054394352, 0.0, 1.0, 44195.95519291264], 
processed observation next is [1.0, 0.043478260869565216, 0.2770083102493075, 0.75, 0.0, 0.0, 0.6666666666666666, 0.5337358061925999, 0.5575580181314507, 0.0, 1.0, 0.21045692949006017], 
reward next is 0.7895, 
noisyNet noise sample is [array([0.06190898], dtype=float32), -0.8400743]. 
=============================================
[2019-04-03 23:29:20,980] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[83.12944 ]
 [83.19702 ]
 [83.265434]
 [83.38649 ]
 [83.36599 ]], R is [[83.06882477]
 [83.02745819]
 [82.98632812]
 [82.94537354]
 [82.9047699 ]].
[2019-04-03 23:29:30,116] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.6807345e-28 6.4886593e-23 1.1464940e-20 1.0000000e+00 1.8484690e-21
 5.2481417e-17 4.3693608e-23], sum to 1.0000
[2019-04-03 23:29:30,116] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7571
[2019-04-03 23:29:30,153] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.3, 68.33333333333333, 0.0, 0.0, 26.0, 24.12308087796372, 0.08754236695118256, 0.0, 1.0, 41322.98552061567], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2357400.0000, 
sim time next is 2358000.0000, 
raw observation next is [-3.4, 69.0, 0.0, 0.0, 26.0, 24.10971211193639, 0.08099868373731593, 0.0, 1.0, 41374.52415742472], 
processed observation next is [0.0, 0.30434782608695654, 0.368421052631579, 0.69, 0.0, 0.0, 0.6666666666666666, 0.5091426759946991, 0.526999561245772, 0.0, 1.0, 0.19702154360678437], 
reward next is 0.8030, 
noisyNet noise sample is [array([1.0014151], dtype=float32), 1.2275536]. 
=============================================
[2019-04-03 23:29:30,165] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[83.443436]
 [83.40398 ]
 [83.35774 ]
 [83.29256 ]
 [83.20261 ]], R is [[83.45037842]
 [83.4190979 ]
 [83.38841248]
 [83.35832977]
 [83.32868958]].
[2019-04-03 23:29:30,471] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.0568357e-26 2.5443068e-20 7.4742907e-18 1.0000000e+00 1.0467623e-19
 1.8579647e-12 9.0820465e-21], sum to 1.0000
[2019-04-03 23:29:30,472] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5598
[2019-04-03 23:29:30,492] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 89.33333333333334, 0.0, 0.0, 26.0, 24.79079584847198, 0.2697287190021915, 0.0, 1.0, 42628.27947940329], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2072400.0000, 
sim time next is 2073000.0000, 
raw observation next is [-4.5, 90.16666666666667, 0.0, 0.0, 26.0, 24.75061112480772, 0.2615015155383134, 0.0, 1.0, 42669.93390484514], 
processed observation next is [1.0, 1.0, 0.3379501385041552, 0.9016666666666667, 0.0, 0.0, 0.6666666666666666, 0.5625509270673099, 0.5871671718461045, 0.0, 1.0, 0.2031901614516435], 
reward next is 0.7968, 
noisyNet noise sample is [array([0.5745786], dtype=float32), -0.49535877]. 
=============================================
[2019-04-03 23:29:30,496] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[83.477295]
 [83.348434]
 [83.27589 ]
 [83.17154 ]
 [83.17384 ]], R is [[83.59080505]
 [83.55190277]
 [83.51366425]
 [83.4761734 ]
 [83.43950653]].
[2019-04-03 23:29:31,945] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.4891772e-24 8.8941024e-19 5.8200126e-16 1.0000000e+00 1.4393669e-17
 2.7979739e-09 1.5729356e-19], sum to 1.0000
[2019-04-03 23:29:31,945] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4858
[2019-04-03 23:29:32,041] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.416666666666667, 76.33333333333333, 152.3333333333333, 46.33333333333333, 26.0, 25.66547952175959, 0.3358156392521963, 1.0, 1.0, 18727.81057978611], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2283000.0000, 
sim time next is 2283600.0000, 
raw observation next is [-6.133333333333335, 74.66666666666667, 165.1666666666667, 48.16666666666667, 26.0, 25.7020114725354, 0.3446120727408564, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.29270544783010155, 0.7466666666666667, 0.5505555555555557, 0.05322283609576428, 0.6666666666666666, 0.64183428937795, 0.6148706909136188, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.48852956], dtype=float32), 0.6729348]. 
=============================================
[2019-04-03 23:29:36,344] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.6774870e-26 4.5579554e-19 9.0969006e-18 1.0000000e+00 1.1499681e-19
 2.5614667e-12 3.8178380e-21], sum to 1.0000
[2019-04-03 23:29:36,344] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2125
[2019-04-03 23:29:36,430] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.8, 70.5, 0.0, 0.0, 26.0, 25.09574007960708, 0.3455524230734475, 1.0, 1.0, 110518.8329832218], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2230200.0000, 
sim time next is 2230800.0000, 
raw observation next is [-4.866666666666666, 70.66666666666666, 0.0, 0.0, 26.0, 24.99889110132304, 0.3613126886419682, 1.0, 1.0, 135668.7624835282], 
processed observation next is [1.0, 0.8260869565217391, 0.3277931671283472, 0.7066666666666666, 0.0, 0.0, 0.6666666666666666, 0.5832409251102533, 0.6204375628806561, 1.0, 1.0, 0.646041726112039], 
reward next is 0.3540, 
noisyNet noise sample is [array([1.356438], dtype=float32), -0.7670333]. 
=============================================
[2019-04-03 23:29:38,393] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.7179235e-27 3.8460627e-22 1.6430752e-17 1.0000000e+00 7.0659852e-21
 1.8611835e-14 1.4588384e-21], sum to 1.0000
[2019-04-03 23:29:38,393] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7324
[2019-04-03 23:29:38,407] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.633333333333333, 84.66666666666666, 0.0, 0.0, 26.0, 24.34777569785585, 0.118528322224859, 0.0, 1.0, 44097.57122549867], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2256000.0000, 
sim time next is 2256600.0000, 
raw observation next is [-7.716666666666667, 85.33333333333334, 0.0, 0.0, 26.0, 24.24490226909403, 0.1030215657901376, 0.0, 1.0, 43951.47609694441], 
processed observation next is [1.0, 0.08695652173913043, 0.24884579870729456, 0.8533333333333334, 0.0, 0.0, 0.6666666666666666, 0.5204085224245025, 0.5343405219300459, 0.0, 1.0, 0.20929274331878292], 
reward next is 0.7907, 
noisyNet noise sample is [array([0.8697879], dtype=float32), 0.077964745]. 
=============================================
[2019-04-03 23:29:43,553] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2972479e-25 1.3081105e-18 2.1717031e-17 1.0000000e+00 2.9502446e-19
 1.8220007e-12 4.0674351e-20], sum to 1.0000
[2019-04-03 23:29:43,554] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0316
[2019-04-03 23:29:43,607] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 50.33333333333334, 165.1666666666667, 0.0, 26.0, 24.98816082259778, 0.2975041821754021, 0.0, 1.0, 18718.78561044054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2384400.0000, 
sim time next is 2385000.0000, 
raw observation next is [0.0, 49.5, 160.0, 0.0, 26.0, 24.97501427707824, 0.2913810125791194, 0.0, 1.0, 24125.57435671971], 
processed observation next is [0.0, 0.6086956521739131, 0.46260387811634357, 0.495, 0.5333333333333333, 0.0, 0.6666666666666666, 0.5812511897565201, 0.5971270041930398, 0.0, 1.0, 0.114883687412951], 
reward next is 0.8851, 
noisyNet noise sample is [array([-0.82651526], dtype=float32), 0.2857958]. 
=============================================
[2019-04-03 23:29:43,612] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[78.53586 ]
 [78.638275]
 [78.85149 ]
 [79.08482 ]
 [79.201904]], R is [[78.62173462]
 [78.74638367]
 [78.86977386]
 [78.98625946]
 [79.04373169]].
[2019-04-03 23:29:47,847] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.2731374e-25 3.0950041e-19 6.4672296e-17 1.0000000e+00 3.9550837e-19
 2.8764689e-13 1.9304163e-19], sum to 1.0000
[2019-04-03 23:29:47,848] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1006
[2019-04-03 23:29:47,861] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.72465293337637, 0.2395252487235643, 0.0, 1.0, 38726.44254482866], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2338800.0000, 
sim time next is 2339400.0000, 
raw observation next is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.69691059560201, 0.2380118882279967, 0.0, 1.0, 38835.64992420549], 
processed observation next is [0.0, 0.043478260869565216, 0.3988919667590028, 0.62, 0.0, 0.0, 0.6666666666666666, 0.5580758829668341, 0.5793372960759989, 0.0, 1.0, 0.18493166630574043], 
reward next is 0.8151, 
noisyNet noise sample is [array([-0.16433789], dtype=float32), -0.7091789]. 
=============================================
[2019-04-03 23:29:49,982] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.36360984e-26 9.50875780e-21 1.01775575e-17 1.00000000e+00
 1.43113103e-19 4.30623094e-13 6.24556623e-21], sum to 1.0000
[2019-04-03 23:29:49,982] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3957
[2019-04-03 23:29:50,004] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.366666666666667, 81.33333333333334, 0.0, 0.0, 26.0, 24.13974988703854, 0.07102290722142342, 0.0, 1.0, 43267.74171542483], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2611200.0000, 
sim time next is 2611800.0000, 
raw observation next is [-6.45, 80.5, 0.0, 0.0, 26.0, 24.06182577940947, 0.05805441646899421, 0.0, 1.0, 43426.46874188166], 
processed observation next is [1.0, 0.21739130434782608, 0.28393351800554023, 0.805, 0.0, 0.0, 0.6666666666666666, 0.5051521482841226, 0.5193514721563314, 0.0, 1.0, 0.20679270829467458], 
reward next is 0.7932, 
noisyNet noise sample is [array([0.56836176], dtype=float32), 0.44875878]. 
=============================================
[2019-04-03 23:30:08,445] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.4169179e-28 4.4495261e-23 6.6994986e-19 1.0000000e+00 1.9056694e-21
 3.0237412e-15 1.3836572e-22], sum to 1.0000
[2019-04-03 23:30:08,446] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9797
[2019-04-03 23:30:08,554] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.300000000000001, 79.0, 0.0, 0.0, 26.0, 23.82093967309129, 0.06666057938122816, 1.0, 1.0, 202354.9988504075], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2618400.0000, 
sim time next is 2619000.0000, 
raw observation next is [-7.3, 79.0, 0.0, 0.0, 26.0, 24.08348325575867, 0.1731873556983389, 1.0, 1.0, 203353.9654536377], 
processed observation next is [1.0, 0.30434782608695654, 0.26038781163434904, 0.79, 0.0, 0.0, 0.6666666666666666, 0.506956937979889, 0.557729118566113, 1.0, 1.0, 0.9683522164458939], 
reward next is 0.0316, 
noisyNet noise sample is [array([-1.3349773], dtype=float32), 1.0356286]. 
=============================================
[2019-04-03 23:30:08,575] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[85.42104 ]
 [84.360146]
 [84.32663 ]
 [84.31775 ]
 [84.306755]], R is [[85.59798431]
 [84.77841187]
 [84.71690369]
 [84.65632629]
 [84.59671021]].
[2019-04-03 23:30:17,968] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.0450328e-27 6.9438702e-21 1.6117800e-18 1.0000000e+00 4.8793652e-21
 1.4143030e-13 2.0013419e-21], sum to 1.0000
[2019-04-03 23:30:17,968] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3833
[2019-04-03 23:30:17,988] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.86375247762316, 0.3370533203941505, 0.0, 1.0, 43350.8073908184], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2939400.0000, 
sim time next is 2940000.0000, 
raw observation next is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.8603544922453, 0.3312574395981034, 0.0, 1.0, 43336.792433244], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.85, 0.0, 0.0, 0.6666666666666666, 0.5716962076871083, 0.6104191465327011, 0.0, 1.0, 0.20636567825354288], 
reward next is 0.7936, 
noisyNet noise sample is [array([-0.87346166], dtype=float32), -1.759713]. 
=============================================
[2019-04-03 23:30:18,008] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[85.21552 ]
 [85.292046]
 [85.30752 ]
 [85.27112 ]
 [85.5366  ]], R is [[85.17299652]
 [85.11483002]
 [85.05722046]
 [85.00020599]
 [84.94379425]].
[2019-04-03 23:30:21,031] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.03269103e-25 5.80349008e-18 1.22762497e-16 1.00000000e+00
 1.48713064e-19 7.47240742e-11 7.23217451e-20], sum to 1.0000
[2019-04-03 23:30:21,035] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0235
[2019-04-03 23:30:21,049] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.1666666666666667, 98.83333333333334, 68.33333333333334, 0.0, 26.0, 25.43806736330703, 0.3152775183996538, 1.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2887800.0000, 
sim time next is 2888400.0000, 
raw observation next is [0.3333333333333333, 97.66666666666667, 73.16666666666666, 0.0, 26.0, 25.43414323175783, 0.3172770202408932, 1.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.43478260869565216, 0.4718374884579871, 0.9766666666666667, 0.24388888888888885, 0.0, 0.6666666666666666, 0.6195119359798191, 0.6057590067469644, 1.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.74599683], dtype=float32), 1.241806]. 
=============================================
[2019-04-03 23:30:32,317] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.0285498e-27 4.9830261e-22 2.6639109e-19 1.0000000e+00 1.7930206e-20
 4.2258593e-16 7.8014835e-22], sum to 1.0000
[2019-04-03 23:30:32,321] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9998
[2019-04-03 23:30:32,346] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 84.0, 0.0, 0.0, 26.0, 24.40196845795583, 0.2154486222920829, 0.0, 1.0, 42941.42964359853], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2953200.0000, 
sim time next is 2953800.0000, 
raw observation next is [-3.0, 84.0, 0.0, 0.0, 26.0, 24.3746604818869, 0.2083264479556619, 0.0, 1.0, 42967.04598875168], 
processed observation next is [0.0, 0.17391304347826086, 0.3795013850415513, 0.84, 0.0, 0.0, 0.6666666666666666, 0.5312217068239082, 0.569442149318554, 0.0, 1.0, 0.2046049808988175], 
reward next is 0.7954, 
noisyNet noise sample is [array([-0.55353105], dtype=float32), 1.197292]. 
=============================================
[2019-04-03 23:30:32,951] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.4923259e-25 2.8159580e-20 1.4661163e-17 1.0000000e+00 1.0435294e-18
 7.9553216e-15 4.0588793e-20], sum to 1.0000
[2019-04-03 23:30:32,952] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5480
[2019-04-03 23:30:32,980] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 77.0, 0.0, 0.0, 26.0, 23.75292476440601, 0.06473192150173566, 0.0, 1.0, 60639.12435677188], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2962800.0000, 
sim time next is 2963400.0000, 
raw observation next is [-4.0, 77.0, 0.0, 0.0, 26.0, 23.70137380102461, 0.05455096159458619, 0.0, 1.0, 60708.9249074775], 
processed observation next is [0.0, 0.30434782608695654, 0.3518005540166205, 0.77, 0.0, 0.0, 0.6666666666666666, 0.47511448341871737, 0.5181836538648621, 0.0, 1.0, 0.2890901186070357], 
reward next is 0.7109, 
noisyNet noise sample is [array([-1.2733705], dtype=float32), -0.17178097]. 
=============================================
[2019-04-03 23:30:33,874] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.0867260e-25 3.0098005e-18 2.0449736e-16 1.0000000e+00 2.3700197e-18
 7.9742931e-12 1.2588369e-19], sum to 1.0000
[2019-04-03 23:30:33,877] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4961
[2019-04-03 23:30:33,891] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 100.0, 0.0, 0.0, 26.0, 25.66248824734644, 0.7202073712898125, 0.0, 1.0, 18727.88522351269], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3186000.0000, 
sim time next is 3186600.0000, 
raw observation next is [2.833333333333333, 100.0, 0.0, 0.0, 26.0, 25.74567216378514, 0.7234496071534372, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.541089566020314, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6454726803154284, 0.7411498690511458, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9821601], dtype=float32), 0.8058827]. 
=============================================
[2019-04-03 23:30:41,647] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.6762205e-27 1.5566568e-20 2.7860657e-17 1.0000000e+00 7.8874327e-20
 7.3470120e-14 2.9451830e-20], sum to 1.0000
[2019-04-03 23:30:41,647] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0377
[2019-04-03 23:30:41,674] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 82.5, 0.0, 0.0, 26.0, 25.10628184965236, 0.2942187589722391, 0.0, 1.0, 56190.50290481545], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2860200.0000, 
sim time next is 2860800.0000, 
raw observation next is [1.0, 83.66666666666666, 0.0, 0.0, 26.0, 25.10336721559059, 0.2914538935849542, 0.0, 1.0, 55698.76554461501], 
processed observation next is [1.0, 0.08695652173913043, 0.4903047091412743, 0.8366666666666666, 0.0, 0.0, 0.6666666666666666, 0.5919472679658826, 0.5971512978616514, 0.0, 1.0, 0.2652322168791191], 
reward next is 0.7348, 
noisyNet noise sample is [array([0.331811], dtype=float32), -0.42123884]. 
=============================================
[2019-04-03 23:30:41,775] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.08988428e-29 9.08515643e-23 6.09456400e-20 1.00000000e+00
 4.40756073e-24 2.02868811e-16 1.05583155e-23], sum to 1.0000
[2019-04-03 23:30:41,775] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3182
[2019-04-03 23:30:41,825] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.666666666666667, 47.33333333333334, 109.8333333333333, 807.8333333333334, 26.0, 25.10459435919041, 0.3584679620840188, 0.0, 1.0, 18714.697419683], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3072000.0000, 
sim time next is 3072600.0000, 
raw observation next is [-1.5, 46.0, 109.0, 806.0, 26.0, 25.10573928574195, 0.3593909975597158, 0.0, 1.0, 18713.06844184645], 
processed observation next is [0.0, 0.5652173913043478, 0.4210526315789474, 0.46, 0.36333333333333334, 0.8906077348066298, 0.6666666666666666, 0.5921449404784959, 0.6197969991865719, 0.0, 1.0, 0.08910984972307834], 
reward next is 0.9109, 
noisyNet noise sample is [array([-2.339384], dtype=float32), 0.020950612]. 
=============================================
[2019-04-03 23:30:51,449] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.2735197e-30 3.9589723e-22 1.1918610e-19 1.0000000e+00 1.4691575e-23
 4.9138028e-16 5.6873192e-23], sum to 1.0000
[2019-04-03 23:30:51,449] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1731
[2019-04-03 23:30:51,462] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 84.83333333333334, 0.0, 0.0, 26.0, 25.38184194210428, 0.4623691283989158, 0.0, 1.0, 58562.74029144668], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3456600.0000, 
sim time next is 3457200.0000, 
raw observation next is [1.0, 83.66666666666667, 0.0, 0.0, 26.0, 25.41962255269212, 0.4687125584257121, 0.0, 1.0, 27505.95921270073], 
processed observation next is [1.0, 0.0, 0.4903047091412743, 0.8366666666666667, 0.0, 0.0, 0.6666666666666666, 0.6183018793910099, 0.6562375194752373, 0.0, 1.0, 0.13098075815571777], 
reward next is 0.8690, 
noisyNet noise sample is [array([0.9605357], dtype=float32), -1.2559537]. 
=============================================
[2019-04-03 23:30:54,579] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.4415286e-29 2.8488024e-21 1.2913319e-19 1.0000000e+00 3.5525770e-23
 2.4605442e-16 5.8997260e-23], sum to 1.0000
[2019-04-03 23:30:54,580] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4584
[2019-04-03 23:30:54,606] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.0, 100.0, 0.0, 0.0, 26.0, 25.3963919157761, 0.3114030114862389, 0.0, 1.0, 64021.71152226604], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3132000.0000, 
sim time next is 3132600.0000, 
raw observation next is [4.333333333333334, 100.0, 0.0, 0.0, 26.0, 25.38763780769077, 0.3086308345073978, 0.0, 1.0, 59932.45763261225], 
processed observation next is [1.0, 0.2608695652173913, 0.58264081255771, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6156364839742308, 0.6028769448357992, 0.0, 1.0, 0.28539265539339165], 
reward next is 0.7146, 
noisyNet noise sample is [array([0.75256395], dtype=float32), 0.9630733]. 
=============================================
[2019-04-03 23:31:02,094] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.3971736e-27 5.0203083e-22 3.5112757e-19 1.0000000e+00 9.7031840e-21
 6.3127698e-16 1.0165742e-21], sum to 1.0000
[2019-04-03 23:31:02,095] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0861
[2019-04-03 23:31:02,117] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.5, 55.0, 0.0, 0.0, 26.0, 25.36703424102302, 0.3775460379829774, 0.0, 1.0, 51014.23633066174], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3623400.0000, 
sim time next is 3624000.0000, 
raw observation next is [-2.666666666666667, 56.66666666666667, 0.0, 0.0, 26.0, 25.36119204656512, 0.3753597409774654, 0.0, 1.0, 43973.57277987416], 
processed observation next is [0.0, 0.9565217391304348, 0.38873499538319484, 0.5666666666666668, 0.0, 0.0, 0.6666666666666666, 0.6134326705470933, 0.6251199136591551, 0.0, 1.0, 0.20939796561844837], 
reward next is 0.7906, 
noisyNet noise sample is [array([-1.3825322], dtype=float32), 0.82484674]. 
=============================================
[2019-04-03 23:31:02,122] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[83.150986]
 [83.178505]
 [83.246704]
 [83.279495]
 [83.33215 ]], R is [[83.11979675]
 [83.04566956]
 [82.99591827]
 [82.99781036]
 [83.02696228]].
[2019-04-03 23:31:02,846] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.1763027e-26 1.9107396e-17 3.3634731e-16 1.0000000e+00 3.5207566e-20
 2.4561896e-11 4.5245685e-21], sum to 1.0000
[2019-04-03 23:31:02,850] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5104
[2019-04-03 23:31:02,868] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.333333333333333, 48.33333333333333, 107.3333333333333, 746.3333333333334, 26.0, 26.59643134957111, 0.6118508421635155, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3406800.0000, 
sim time next is 3407400.0000, 
raw observation next is [2.5, 48.5, 109.0, 764.0, 26.0, 26.54282098628178, 0.6210425295762616, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.5318559556786704, 0.485, 0.36333333333333334, 0.8441988950276244, 0.6666666666666666, 0.7119017488568149, 0.7070141765254205, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.18440028], dtype=float32), 0.731233]. 
=============================================
[2019-04-03 23:31:06,633] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.8459283e-26 2.2906964e-18 2.3393999e-17 1.0000000e+00 4.8812106e-20
 8.1829579e-13 1.1288053e-20], sum to 1.0000
[2019-04-03 23:31:06,633] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1891
[2019-04-03 23:31:06,667] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.333333333333333, 58.33333333333334, 0.0, 0.0, 26.0, 25.13012730632137, 0.4520348758038029, 0.0, 1.0, 198946.8401993331], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3356400.0000, 
sim time next is 3357000.0000, 
raw observation next is [-3.5, 60.0, 0.0, 0.0, 26.0, 25.11019855143876, 0.4746015889651054, 0.0, 1.0, 155454.4837661535], 
processed observation next is [1.0, 0.8695652173913043, 0.36565096952908593, 0.6, 0.0, 0.0, 0.6666666666666666, 0.59251654595323, 0.6582005296550352, 0.0, 1.0, 0.7402594465054929], 
reward next is 0.2597, 
noisyNet noise sample is [array([3.445716], dtype=float32), 0.62956053]. 
=============================================
[2019-04-03 23:31:06,694] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[83.58633 ]
 [83.31651 ]
 [83.633644]
 [83.37772 ]
 [82.97886 ]], R is [[83.32006836]
 [82.539505  ]
 [82.37754059]
 [82.32579041]
 [82.3824234 ]].
[2019-04-03 23:31:07,216] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.6257158e-27 1.4558116e-17 1.0294901e-16 1.0000000e+00 1.0524628e-21
 8.3709298e-12 2.5514103e-21], sum to 1.0000
[2019-04-03 23:31:07,219] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1977
[2019-04-03 23:31:07,233] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 49.0, 114.3333333333333, 809.6666666666666, 26.0, 25.85683621687829, 0.5256715666123077, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3417000.0000, 
sim time next is 3417600.0000, 
raw observation next is [3.0, 49.0, 113.6666666666667, 807.8333333333334, 26.0, 25.85976943757749, 0.5738217254076926, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5457063711911359, 0.49, 0.378888888888889, 0.892633517495396, 0.6666666666666666, 0.6549807864647909, 0.6912739084692309, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6058233], dtype=float32), 2.157269]. 
=============================================
[2019-04-03 23:31:09,170] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.1117243e-28 2.4024957e-20 2.1600595e-19 1.0000000e+00 1.4226911e-23
 9.0098549e-14 2.5063673e-23], sum to 1.0000
[2019-04-03 23:31:09,172] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5723
[2019-04-03 23:31:09,179] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 55.33333333333334, 115.8333333333333, 820.8333333333334, 26.0, 25.6249364369123, 0.550520663573202, 1.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3500400.0000, 
sim time next is 3501000.0000, 
raw observation next is [2.0, 54.5, 116.0, 823.0, 26.0, 25.82033764729356, 0.574775798814028, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.518005540166205, 0.545, 0.38666666666666666, 0.9093922651933701, 0.6666666666666666, 0.65169480394113, 0.6915919329380094, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.735381], dtype=float32), 0.59235823]. 
=============================================
[2019-04-03 23:31:09,195] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[89.08613]
 [89.05596]
 [88.89828]
 [88.95494]
 [89.07308]], R is [[89.08506775]
 [89.10527039]
 [88.89923096]
 [89.01023865]
 [89.12014008]].
[2019-04-03 23:31:16,417] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.6093249e-30 1.0108538e-23 3.9454055e-21 1.0000000e+00 2.2774805e-24
 3.2317080e-17 7.4949107e-24], sum to 1.0000
[2019-04-03 23:31:16,418] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8199
[2019-04-03 23:31:16,449] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.666666666666667, 69.0, 0.0, 0.0, 26.0, 25.39056494710602, 0.3925763417747267, 0.0, 1.0, 31422.34112739174], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3901200.0000, 
sim time next is 3901800.0000, 
raw observation next is [-2.833333333333333, 70.0, 0.0, 0.0, 26.0, 25.36419165465024, 0.3836524971903037, 0.0, 1.0, 45046.12844368221], 
processed observation next is [1.0, 0.13043478260869565, 0.3841181902123731, 0.7, 0.0, 0.0, 0.6666666666666666, 0.6136826378875201, 0.6278841657301012, 0.0, 1.0, 0.21450537354134386], 
reward next is 0.7855, 
noisyNet noise sample is [array([-1.833593], dtype=float32), -0.7412079]. 
=============================================
[2019-04-03 23:31:22,712] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.2770916e-27 2.8264335e-17 8.9476367e-18 1.0000000e+00 1.1963147e-21
 7.8458576e-12 1.0878041e-20], sum to 1.0000
[2019-04-03 23:31:22,715] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8779
[2019-04-03 23:31:22,740] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.333333333333333, 67.33333333333333, 14.16666666666667, 122.5, 26.0, 25.81197587144761, 0.3728437657015145, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3778800.0000, 
sim time next is 3779400.0000, 
raw observation next is [-1.666666666666667, 69.16666666666667, 0.0, 0.0, 26.0, 25.55653952820087, 0.4889865949596237, 1.0, 1.0, 187265.3969184458], 
processed observation next is [1.0, 0.7391304347826086, 0.4164358264081256, 0.6916666666666668, 0.0, 0.0, 0.6666666666666666, 0.6297116273500724, 0.6629955316532079, 1.0, 1.0, 0.8917399853259323], 
reward next is 0.1083, 
noisyNet noise sample is [array([-0.4283137], dtype=float32), 0.2861455]. 
=============================================
[2019-04-03 23:31:43,943] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.9407376e-27 2.7204560e-18 7.3620348e-18 1.0000000e+00 7.6445835e-21
 1.5887567e-11 9.3130755e-21], sum to 1.0000
[2019-04-03 23:31:43,944] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4607
[2019-04-03 23:31:43,951] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 49.0, 118.8333333333333, 804.1666666666666, 26.0, 26.32506512587115, 0.6071811611935864, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3928800.0000, 
sim time next is 3929400.0000, 
raw observation next is [-6.0, 49.0, 120.0, 810.0, 26.0, 26.35141211071539, 0.609986210698957, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.296398891966759, 0.49, 0.4, 0.8950276243093923, 0.6666666666666666, 0.6959510092262825, 0.7033287368996524, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.859073], dtype=float32), -0.9622663]. 
=============================================
[2019-04-03 23:31:47,303] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.0574842e-29 2.7522350e-22 7.2105773e-20 1.0000000e+00 3.8878953e-23
 1.9279171e-16 1.4388582e-22], sum to 1.0000
[2019-04-03 23:31:47,307] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5742
[2019-04-03 23:31:47,323] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.333333333333334, 36.33333333333333, 0.0, 0.0, 26.0, 24.84487250792186, 0.2286354378326336, 0.0, 1.0, 40387.5048161459], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4077600.0000, 
sim time next is 4078200.0000, 
raw observation next is [-4.166666666666667, 35.16666666666667, 0.0, 0.0, 26.0, 24.78900492128295, 0.2317388605702894, 0.0, 1.0, 40362.83536815546], 
processed observation next is [1.0, 0.17391304347826086, 0.3471837488457987, 0.35166666666666674, 0.0, 0.0, 0.6666666666666666, 0.5657504101069124, 0.5772462868567632, 0.0, 1.0, 0.19220397794359742], 
reward next is 0.8078, 
noisyNet noise sample is [array([0.8277948], dtype=float32), -1.2077771]. 
=============================================
[2019-04-03 23:31:49,714] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.15972445e-29 8.76022837e-25 5.17351148e-21 1.00000000e+00
 6.52168468e-24 1.78877994e-18 4.25664690e-24], sum to 1.0000
[2019-04-03 23:31:49,716] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7831
[2019-04-03 23:31:49,767] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 49.5, 92.0, 488.0, 26.0, 24.96254426047946, 0.3967672617401649, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4177800.0000, 
sim time next is 4178400.0000, 
raw observation next is [-4.333333333333334, 48.0, 94.66666666666667, 516.6666666666666, 26.0, 25.52989617133662, 0.4361266004414035, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.3425669436749769, 0.48, 0.3155555555555556, 0.570902394106814, 0.6666666666666666, 0.627491347611385, 0.6453755334804678, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.57512414], dtype=float32), -0.9772948]. 
=============================================
[2019-04-03 23:31:50,509] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-03 23:31:50,510] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-03 23:31:50,510] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:31:50,519] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-03 23:31:50,520] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-03 23:31:50,520] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:31:50,521] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:31:50,523] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run13
[2019-04-03 23:31:50,538] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run13
[2019-04-03 23:31:50,550] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run13
[2019-04-03 23:32:20,271] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.10102317], dtype=float32), 0.53555167]
[2019-04-03 23:32:20,271] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [11.01666666666667, 89.0, 0.0, 0.0, 22.0, 22.13607499851962, -0.3282674290353155, 0.0, 1.0, 0.0]
[2019-04-03 23:32:20,272] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-03 23:32:20,272] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.6957485e-18 1.4062629e-13 9.4784120e-13 1.0000000e+00 4.3353472e-13
 8.7853608e-10 5.9392667e-15], sampled 0.9376535758714538
[2019-04-03 23:33:17,318] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.10102317], dtype=float32), 0.53555167]
[2019-04-03 23:33:17,319] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [1.75, 39.0, 66.0, 292.0, 22.0, 21.5236562327551, -0.5847962036821307, 0.0, 1.0, 0.0]
[2019-04-03 23:33:17,319] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-03 23:33:17,320] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [3.5668424e-19 2.5115747e-15 6.1758399e-14 1.0000000e+00 6.4068583e-14
 4.2966280e-11 1.1882726e-15], sampled 0.05854629254470867
[2019-04-03 23:33:58,654] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.10102317], dtype=float32), 0.53555167]
[2019-04-03 23:33:58,654] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [7.883333333333334, 68.0, 260.0, 140.0, 23.0, 24.87584505482316, 0.5373310660672228, 0.0, 1.0, 0.0]
[2019-04-03 23:33:58,654] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-03 23:33:58,655] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [2.1739814e-18 2.3592418e-13 3.5211335e-12 1.0000000e+00 8.9647379e-14
 1.2145966e-08 8.0662482e-15], sampled 0.2683530434343093
[2019-04-03 23:34:05,533] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7194.2951 186725761.2883 -508.9979
[2019-04-03 23:34:48,905] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7335.1132 236975030.2814 -494.8509
[2019-04-03 23:34:51,913] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7346.1530 236556195.3515 249.1338
[2019-04-03 23:34:52,962] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 1200000, evaluation results [1200000.0, 7346.152998087057, 236556195.35151896, 249.13383176957768, 7194.295126521754, 186725761.28834048, -508.9978535182306, 7335.113157562538, 236975030.28140858, -494.8509154016365]
[2019-04-03 23:35:13,356] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.0548216e-29 2.9814417e-23 4.5035089e-21 1.0000000e+00 2.9423373e-23
 6.5195372e-19 2.2245032e-23], sum to 1.0000
[2019-04-03 23:35:13,356] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3973
[2019-04-03 23:35:13,369] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.0, 50.66666666666667, 0.0, 0.0, 26.0, 24.60821066784549, 0.2120370918579526, 0.0, 1.0, 40426.57690199183], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4173600.0000, 
sim time next is 4174200.0000, 
raw observation next is [-5.0, 51.5, 0.0, 0.0, 26.0, 24.61606881939739, 0.2018170465405777, 0.0, 1.0, 40279.50262367834], 
processed observation next is [0.0, 0.30434782608695654, 0.32409972299168976, 0.515, 0.0, 0.0, 0.6666666666666666, 0.551339068283116, 0.5672723488468593, 0.0, 1.0, 0.19180715535084922], 
reward next is 0.8082, 
noisyNet noise sample is [array([-0.13748486], dtype=float32), 0.5140818]. 
=============================================
[2019-04-03 23:35:28,591] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.77885661e-30 2.88182072e-22 2.50958568e-20 1.00000000e+00
 3.55119305e-24 1.52849693e-17 1.28519625e-23], sum to 1.0000
[2019-04-03 23:35:28,592] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4260
[2019-04-03 23:35:28,605] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.85, 69.83333333333334, 0.0, 0.0, 26.0, 25.57288313503273, 0.400472127527133, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4335000.0000, 
sim time next is 4335600.0000, 
raw observation next is [3.8, 69.66666666666667, 0.0, 0.0, 26.0, 25.63319969652255, 0.3890401517830347, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.5678670360110805, 0.6966666666666668, 0.0, 0.0, 0.6666666666666666, 0.6360999747102124, 0.6296800505943448, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9080287], dtype=float32), -0.34966996]. 
=============================================
[2019-04-03 23:35:47,935] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.6981714e-27 8.0865306e-20 3.2866830e-18 1.0000000e+00 2.0727542e-22
 3.3431645e-13 5.6749911e-22], sum to 1.0000
[2019-04-03 23:35:47,936] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6639
[2019-04-03 23:35:47,998] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 92.0, 130.5, 0.9999999999999998, 26.0, 26.37389739672905, 0.5680399609345529, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4700400.0000, 
sim time next is 4701000.0000, 
raw observation next is [0.0, 92.0, 146.0, 2.0, 26.0, 26.4003995056291, 0.5757197127225652, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.46260387811634357, 0.92, 0.4866666666666667, 0.0022099447513812156, 0.6666666666666666, 0.7000332921357583, 0.6919065709075217, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.89238846], dtype=float32), 0.13816893]. 
=============================================
[2019-04-03 23:35:48,027] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[85.46972 ]
 [85.53863 ]
 [85.73791 ]
 [85.88064 ]
 [85.899315]], R is [[85.61595917]
 [85.75980377]
 [85.90220642]
 [86.04318237]
 [86.18275452]].
[2019-04-03 23:35:49,632] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:35:49,632] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:35:49,653] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run10
[2019-04-03 23:35:50,598] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:35:50,599] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:35:50,610] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run10
[2019-04-03 23:35:55,420] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3636216e-27 3.1531424e-22 1.4886688e-19 1.0000000e+00 3.5798273e-22
 3.8531968e-16 4.9900645e-22], sum to 1.0000
[2019-04-03 23:35:55,420] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8900
[2019-04-03 23:35:55,433] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 55.0, 0.0, 0.0, 26.0, 25.3859514232419, 0.3879499850822041, 0.0, 1.0, 34642.02757333067], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4834800.0000, 
sim time next is 4835400.0000, 
raw observation next is [-1.166666666666667, 55.83333333333334, 0.0, 0.0, 26.0, 25.38765077117703, 0.3874791587509869, 0.0, 1.0, 35976.09495916951], 
processed observation next is [0.0, 1.0, 0.43028624192059095, 0.5583333333333335, 0.0, 0.0, 0.6666666666666666, 0.6156375642647524, 0.6291597195836623, 0.0, 1.0, 0.1713147379008072], 
reward next is 0.8287, 
noisyNet noise sample is [array([-1.4246999], dtype=float32), 1.2153258]. 
=============================================
[2019-04-03 23:35:57,414] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.6880558e-30 9.3598908e-25 1.9867058e-22 1.0000000e+00 4.2388412e-23
 1.0148437e-18 1.7465378e-24], sum to 1.0000
[2019-04-03 23:35:57,414] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0423
[2019-04-03 23:35:57,438] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.033333333333333, 92.16666666666667, 0.0, 0.0, 26.0, 24.13389306943979, 0.1494433381277502, 0.0, 1.0, 41588.48298543296], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4774200.0000, 
sim time next is 4774800.0000, 
raw observation next is [-6.066666666666666, 92.33333333333334, 0.0, 0.0, 26.0, 24.091346399351, 0.1409518623886327, 0.0, 1.0, 41648.43282943968], 
processed observation next is [0.0, 0.2608695652173913, 0.2945521698984303, 0.9233333333333335, 0.0, 0.0, 0.6666666666666666, 0.5076121999459167, 0.5469839541295443, 0.0, 1.0, 0.1983258706163794], 
reward next is 0.8017, 
noisyNet noise sample is [array([-0.99419314], dtype=float32), -0.90751797]. 
=============================================
[2019-04-03 23:35:59,444] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3557977e-30 2.9642592e-24 1.1600073e-22 1.0000000e+00 7.0803525e-25
 1.6235896e-20 5.4164901e-25], sum to 1.0000
[2019-04-03 23:35:59,444] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9270
[2019-04-03 23:35:59,487] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.8333333333333334, 54.33333333333334, 0.0, 0.0, 26.0, 25.39978004428338, 0.400468205107738, 0.0, 1.0, 42912.62584254154], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4830600.0000, 
sim time next is 4831200.0000, 
raw observation next is [-1.0, 55.0, 0.0, 0.0, 26.0, 25.38937727238975, 0.4003840308656265, 0.0, 1.0, 46049.27159113092], 
processed observation next is [0.0, 0.9565217391304348, 0.4349030470914128, 0.55, 0.0, 0.0, 0.6666666666666666, 0.6157814393658126, 0.6334613436218756, 0.0, 1.0, 0.219282245672052], 
reward next is 0.7807, 
noisyNet noise sample is [array([-0.28864512], dtype=float32), -0.48483402]. 
=============================================
[2019-04-03 23:36:00,722] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:36:00,722] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:36:00,725] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run10
[2019-04-03 23:36:04,341] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:36:04,341] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:36:04,357] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run10
[2019-04-03 23:36:05,805] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:36:05,805] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:36:05,810] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run10
[2019-04-03 23:36:11,671] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:36:11,671] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:36:11,673] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run10
[2019-04-03 23:36:12,789] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:36:12,789] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:36:12,798] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run10
[2019-04-03 23:36:13,179] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:36:13,179] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:36:13,182] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run10
[2019-04-03 23:36:14,597] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:36:14,597] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:36:14,600] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run10
[2019-04-03 23:36:17,568] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:36:17,568] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:36:17,619] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run10
[2019-04-03 23:36:19,009] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.7640112e-27 8.7108451e-17 8.1690465e-17 1.0000000e+00 4.1298269e-23
 7.0053316e-13 1.7357474e-21], sum to 1.0000
[2019-04-03 23:36:19,010] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8768
[2019-04-03 23:36:19,027] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [11.0, 17.0, 0.0, 0.0, 26.0, 28.15232015789783, 1.036060047558096, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5079000.0000, 
sim time next is 5079600.0000, 
raw observation next is [11.0, 17.0, 0.0, 0.0, 26.0, 28.0267117918844, 1.024563549604718, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.7673130193905818, 0.17, 0.0, 0.0, 0.6666666666666666, 0.8355593159903666, 0.8415211832015727, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.33169535], dtype=float32), -0.67109394]. 
=============================================
[2019-04-03 23:36:21,153] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1184992e-12 6.5902689e-10 1.7233489e-08 9.8965704e-01 6.1351550e-08
 1.0342921e-02 1.9583322e-10], sum to 1.0000
[2019-04-03 23:36:21,153] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3195
[2019-04-03 23:36:21,165] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.8, 63.16666666666667, 42.33333333333334, 2.999999999999999, 19.0, 19.24122116550117, -1.089097958319921, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 119400.0000, 
sim time next is 120000.0000, 
raw observation next is [-7.8, 65.33333333333334, 43.66666666666666, 1.5, 19.0, 19.29135315215712, -1.078791050134389, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.24653739612188366, 0.6533333333333334, 0.14555555555555552, 0.0016574585635359116, 0.08333333333333333, 0.10761276267976012, 0.14040298328853704, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.2079502], dtype=float32), 0.6759478]. 
=============================================
[2019-04-03 23:36:21,197] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[53.618046]
 [53.830437]
 [54.16476 ]
 [54.587498]
 [54.93549 ]], R is [[53.01517868]
 [52.48502731]
 [51.96017838]
 [51.44057846]
 [50.92617416]].
[2019-04-03 23:36:21,381] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:36:21,381] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:36:21,424] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run10
[2019-04-03 23:36:22,434] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.9564366e-15 2.3352456e-09 7.7677820e-10 5.7224101e-01 1.6239806e-08
 4.2775896e-01 1.4246000e-12], sum to 1.0000
[2019-04-03 23:36:22,434] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0143
[2019-04-03 23:36:22,534] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.4, 61.0, 157.0, 308.0, 26.0, 21.98848337131508, -0.4220502836308972, 1.0, 1.0, 168590.6706833684], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 129600.0000, 
sim time next is 130200.0000, 
raw observation next is [-8.3, 61.0, 148.0, 406.3333333333334, 26.0, 22.5732683263101, -0.3352436359329036, 1.0, 1.0, 117087.7032545746], 
processed observation next is [1.0, 0.5217391304347826, 0.23268698060941828, 0.61, 0.49333333333333335, 0.44898710865561703, 0.6666666666666666, 0.381105693859175, 0.3882521213556988, 1.0, 1.0, 0.5575604916884505], 
reward next is 0.4424, 
noisyNet noise sample is [array([0.24306197], dtype=float32), 0.038372315]. 
=============================================
[2019-04-03 23:36:23,523] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:36:23,524] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:36:23,597] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run10
[2019-04-03 23:36:23,721] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:36:23,721] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:36:23,727] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run10
[2019-04-03 23:36:25,702] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:36:25,702] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:36:25,706] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run10
[2019-04-03 23:36:29,605] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:36:29,605] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:36:29,643] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run10
[2019-04-03 23:36:34,155] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:36:34,155] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:36:34,159] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run10
[2019-04-03 23:36:41,600] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.0339511e-22 1.2189500e-16 1.9145584e-14 1.0000000e+00 2.1821093e-16
 7.7379889e-09 2.2952899e-17], sum to 1.0000
[2019-04-03 23:36:41,606] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7348
[2019-04-03 23:36:41,687] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-11.2, 55.0, 0.0, 0.0, 25.0, 22.4073567187797, -0.3691806115113992, 0.0, 1.0, 46434.89333140056], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 439200.0000, 
sim time next is 439800.0000, 
raw observation next is [-11.1, 54.0, 0.0, 0.0, 25.0, 22.39315319659993, -0.3796534882302882, 0.0, 1.0, 46480.93931284526], 
processed observation next is [1.0, 0.08695652173913043, 0.1551246537396122, 0.54, 0.0, 0.0, 0.5833333333333334, 0.36609609971666074, 0.3734488372565706, 0.0, 1.0, 0.2213378062516441], 
reward next is 0.7787, 
noisyNet noise sample is [array([-0.5263455], dtype=float32), 1.2520163]. 
=============================================
[2019-04-03 23:36:55,830] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.8944580e-20 1.4182670e-11 1.3647544e-10 9.9651730e-01 5.9593591e-16
 3.4826561e-03 1.0715655e-14], sum to 1.0000
[2019-04-03 23:36:55,830] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9311
[2019-04-03 23:36:55,991] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-12.43333333333333, 51.00000000000001, 58.0, 881.5, 26.0, 25.26953219009664, 0.2693653635518033, 1.0, 1.0, 184260.6095643086], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 390000.0000, 
sim time next is 390600.0000, 
raw observation next is [-12.25, 51.0, 58.0, 905.0, 26.0, 24.36025314229677, 0.2870373344441078, 1.0, 1.0, 201485.5670207418], 
processed observation next is [1.0, 0.5217391304347826, 0.12326869806094183, 0.51, 0.19333333333333333, 1.0, 0.6666666666666666, 0.5300210951913975, 0.5956791114813692, 1.0, 1.0, 0.9594550810511514], 
reward next is 0.0405, 
noisyNet noise sample is [array([-1.0739185], dtype=float32), 0.036180917]. 
=============================================
[2019-04-03 23:37:22,817] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.4167465e-27 1.4558997e-18 3.1777007e-16 1.0000000e+00 9.6447296e-21
 1.2983321e-10 1.2419708e-19], sum to 1.0000
[2019-04-03 23:37:22,817] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4600
[2019-04-03 23:37:22,851] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.55, 96.5, 0.0, 0.0, 26.0, 24.83132642518295, 0.2311828347556724, 0.0, 1.0, 40010.30814715641], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 516600.0000, 
sim time next is 517200.0000, 
raw observation next is [3.633333333333333, 96.66666666666666, 0.0, 0.0, 26.0, 24.8386682945801, 0.2369466774361079, 0.0, 1.0, 39923.54801791863], 
processed observation next is [1.0, 1.0, 0.5632502308402586, 0.9666666666666666, 0.0, 0.0, 0.6666666666666666, 0.5698890245483416, 0.578982225812036, 0.0, 1.0, 0.19011213341866012], 
reward next is 0.8099, 
noisyNet noise sample is [array([-0.18368517], dtype=float32), 0.19908002]. 
=============================================
[2019-04-03 23:37:24,496] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.8724196e-27 8.1107781e-21 1.0932882e-18 1.0000000e+00 5.6136943e-21
 2.0222473e-13 4.7699362e-21], sum to 1.0000
[2019-04-03 23:37:24,497] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2813
[2019-04-03 23:37:24,514] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 66.5, 0.0, 0.0, 26.0, 23.63758399445653, -0.03710110650440724, 0.0, 1.0, 43772.86514408274], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 628200.0000, 
sim time next is 628800.0000, 
raw observation next is [-4.5, 67.0, 0.0, 0.0, 26.0, 23.63243896786932, -0.04321825216393031, 0.0, 1.0, 43761.50203765366], 
processed observation next is [0.0, 0.2608695652173913, 0.3379501385041552, 0.67, 0.0, 0.0, 0.6666666666666666, 0.46936991398910993, 0.48559391594535656, 0.0, 1.0, 0.2083881049412079], 
reward next is 0.7916, 
noisyNet noise sample is [array([0.76711535], dtype=float32), -0.070194945]. 
=============================================
[2019-04-03 23:37:28,041] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.4466263e-26 2.3113185e-15 3.6437960e-15 1.0000000e+00 1.0021824e-19
 4.6668180e-08 1.7643289e-18], sum to 1.0000
[2019-04-03 23:37:28,042] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7951
[2019-04-03 23:37:28,111] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.1833333333333333, 36.5, 68.66666666666666, 0.0, 26.0, 25.6672955449935, 0.2860323803673541, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 486600.0000, 
sim time next is 487200.0000, 
raw observation next is [0.3666666666666667, 36.0, 62.33333333333334, 0.0, 26.0, 25.70990002850086, 0.285665523845252, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.4727608494921515, 0.36, 0.2077777777777778, 0.0, 0.6666666666666666, 0.6424916690417385, 0.5952218412817506, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4063768], dtype=float32), 1.0164379]. 
=============================================
[2019-04-03 23:37:30,731] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.1003705e-25 1.4423310e-15 9.1178248e-16 1.0000000e+00 3.9688504e-20
 8.2055589e-09 3.5930918e-18], sum to 1.0000
[2019-04-03 23:37:30,731] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7547
[2019-04-03 23:37:30,798] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 43.0, 10.0, 0.0, 26.0, 25.5489989465748, 0.2912221960533222, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 493200.0000, 
sim time next is 493800.0000, 
raw observation next is [1.0, 49.83333333333334, 0.0, 0.0, 26.0, 25.70509532492985, 0.2983058417515631, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.4903047091412743, 0.4983333333333334, 0.0, 0.0, 0.6666666666666666, 0.6420912770774875, 0.5994352805838544, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3511603], dtype=float32), 0.33167273]. 
=============================================
[2019-04-03 23:37:32,571] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.2801855e-28 5.0540142e-21 4.1932869e-19 1.0000000e+00 2.8471820e-22
 6.5181984e-14 1.0844972e-21], sum to 1.0000
[2019-04-03 23:37:32,571] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4529
[2019-04-03 23:37:32,584] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 75.0, 0.0, 0.0, 26.0, 24.31473363137608, 0.05868151283709189, 0.0, 1.0, 41148.94759548441], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 699000.0000, 
sim time next is 699600.0000, 
raw observation next is [-3.4, 75.0, 0.0, 0.0, 26.0, 24.26983831175276, 0.05749000833895618, 0.0, 1.0, 41191.52161554361], 
processed observation next is [1.0, 0.08695652173913043, 0.368421052631579, 0.75, 0.0, 0.0, 0.6666666666666666, 0.5224865259793967, 0.5191633361129854, 0.0, 1.0, 0.19615010293116003], 
reward next is 0.8038, 
noisyNet noise sample is [array([0.57880116], dtype=float32), 0.2060855]. 
=============================================
[2019-04-03 23:37:36,187] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.4469085e-25 1.5401604e-19 1.3630876e-17 1.0000000e+00 3.1994684e-19
 3.3901378e-12 1.5851574e-19], sum to 1.0000
[2019-04-03 23:37:36,187] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5545
[2019-04-03 23:37:36,201] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 83.0, 0.0, 0.0, 26.0, 24.776527569134, 0.2206045622824353, 0.0, 1.0, 42633.13604913382], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 601200.0000, 
sim time next is 601800.0000, 
raw observation next is [-3.4, 83.66666666666667, 0.0, 0.0, 26.0, 24.75859339720681, 0.2142604999064118, 0.0, 1.0, 42555.09823184121], 
processed observation next is [0.0, 1.0, 0.368421052631579, 0.8366666666666667, 0.0, 0.0, 0.6666666666666666, 0.5632161164339008, 0.5714201666354706, 0.0, 1.0, 0.2026433249135296], 
reward next is 0.7974, 
noisyNet noise sample is [array([1.9454068], dtype=float32), -1.8336638]. 
=============================================
[2019-04-03 23:37:45,858] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.1430636e-24 3.1068007e-17 2.8450858e-15 1.0000000e+00 9.4616260e-18
 4.4490402e-09 2.9254452e-18], sum to 1.0000
[2019-04-03 23:37:45,858] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4000
[2019-04-03 23:37:45,876] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.9, 62.5, 0.0, 0.0, 26.0, 24.88225634847922, 0.2721554648181533, 0.0, 1.0, 44069.39983090947], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 768600.0000, 
sim time next is 769200.0000, 
raw observation next is [-6.0, 63.0, 0.0, 0.0, 26.0, 24.82548112184043, 0.2627414296864105, 0.0, 1.0, 43647.7810268335], 
processed observation next is [1.0, 0.9130434782608695, 0.296398891966759, 0.63, 0.0, 0.0, 0.6666666666666666, 0.5687900934867024, 0.5875804765621369, 0.0, 1.0, 0.20784657631825476], 
reward next is 0.7922, 
noisyNet noise sample is [array([-0.43351853], dtype=float32), 0.0029509494]. 
=============================================
[2019-04-03 23:37:47,207] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.4406285e-28 3.9002603e-20 1.7053689e-18 1.0000000e+00 3.4198849e-22
 1.8490831e-13 3.6468840e-22], sum to 1.0000
[2019-04-03 23:37:47,210] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3684
[2019-04-03 23:37:47,226] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.300000000000001, 71.0, 0.0, 0.0, 26.0, 23.66853940351833, -0.03846674393447797, 0.0, 1.0, 41935.21156325127], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 796800.0000, 
sim time next is 797400.0000, 
raw observation next is [-7.3, 71.0, 0.0, 0.0, 26.0, 23.61345542120269, -0.04496447607368022, 0.0, 1.0, 42008.3242513244], 
processed observation next is [1.0, 0.21739130434782608, 0.26038781163434904, 0.71, 0.0, 0.0, 0.6666666666666666, 0.46778795176689086, 0.48501184130877323, 0.0, 1.0, 0.20003963929202093], 
reward next is 0.8000, 
noisyNet noise sample is [array([1.3940306], dtype=float32), -0.07953567]. 
=============================================
[2019-04-03 23:37:55,981] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.0341315e-30 7.2237101e-20 2.5753520e-18 1.0000000e+00 2.4225731e-23
 9.4751273e-12 2.0047630e-23], sum to 1.0000
[2019-04-03 23:37:55,983] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1725
[2019-04-03 23:37:56,008] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.433333333333334, 94.33333333333334, 102.6666666666667, 0.0, 26.0, 25.20884771938472, 0.2507799123702077, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 909600.0000, 
sim time next is 910200.0000, 
raw observation next is [3.616666666666666, 93.66666666666666, 101.3333333333333, 0.0, 26.0, 25.16862674035355, 0.2504597698911093, 1.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.5217391304347826, 0.5627885503231764, 0.9366666666666665, 0.3377777777777777, 0.0, 0.6666666666666666, 0.5973855616961291, 0.5834865899637031, 1.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.4757847], dtype=float32), -0.45810792]. 
=============================================
[2019-04-03 23:37:56,895] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.8972505e-26 7.0124598e-16 7.1837140e-16 1.0000000e+00 5.2667224e-21
 7.1851983e-11 4.1111284e-20], sum to 1.0000
[2019-04-03 23:37:56,896] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8890
[2019-04-03 23:37:56,916] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.433333333333334, 52.5, 45.33333333333334, 2.666666666666667, 26.0, 25.86983753212418, 0.4377793442620332, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 751800.0000, 
sim time next is 752400.0000, 
raw observation next is [-2.8, 54.0, 34.0, 2.5, 26.0, 25.97127802380311, 0.4381511993922712, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.38504155124653744, 0.54, 0.11333333333333333, 0.0027624309392265192, 0.6666666666666666, 0.6642731686502591, 0.6460503997974237, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.75345194], dtype=float32), 0.00019890066]. 
=============================================
[2019-04-03 23:37:59,948] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.0526880e-27 4.4320529e-19 4.1609921e-17 1.0000000e+00 1.7359462e-21
 2.0493746e-12 3.4530015e-20], sum to 1.0000
[2019-04-03 23:37:59,951] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9726
[2019-04-03 23:37:59,964] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.3, 71.0, 0.0, 0.0, 26.0, 23.5897979139903, -0.04802956259715182, 0.0, 1.0, 42049.63051567709], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 798000.0000, 
sim time next is 798600.0000, 
raw observation next is [-7.299999999999999, 71.0, 0.0, 0.0, 26.0, 23.57112175488462, -0.05334715016318756, 0.0, 1.0, 42079.88047120472], 
processed observation next is [1.0, 0.21739130434782608, 0.2603878116343491, 0.71, 0.0, 0.0, 0.6666666666666666, 0.46426014624038486, 0.48221761661227086, 0.0, 1.0, 0.20038038319621296], 
reward next is 0.7996, 
noisyNet noise sample is [array([1.5004925], dtype=float32), -1.2228384]. 
=============================================
[2019-04-03 23:38:04,868] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.3137229e-34 4.0746169e-24 6.4764673e-23 1.0000000e+00 8.4759349e-28
 7.7816148e-18 1.0770164e-25], sum to 1.0000
[2019-04-03 23:38:04,874] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5419
[2019-04-03 23:38:04,901] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [13.3, 80.0, 0.0, 0.0, 26.0, 25.79725071026207, 0.5832859713776324, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1059600.0000, 
sim time next is 1060200.0000, 
raw observation next is [13.3, 80.0, 0.0, 0.0, 26.0, 25.73284124201439, 0.5779092237838189, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.8310249307479226, 0.8, 0.0, 0.0, 0.6666666666666666, 0.6444034368345326, 0.6926364079279397, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.02876904], dtype=float32), -0.9499356]. 
=============================================
[2019-04-03 23:38:07,405] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.4589076e-35 4.6930923e-25 2.7832423e-23 1.0000000e+00 1.1787027e-28
 5.1643648e-20 5.1860625e-27], sum to 1.0000
[2019-04-03 23:38:07,408] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1848
[2019-04-03 23:38:07,420] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [10.33333333333333, 77.66666666666667, 0.0, 0.0, 26.0, 25.64784738543015, 0.6475613658401641, 0.0, 1.0, 18726.4881332096], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1128000.0000, 
sim time next is 1128600.0000, 
raw observation next is [10.25, 78.0, 0.0, 0.0, 26.0, 25.66685649561283, 0.6480301196408568, 0.0, 1.0, 18725.99046145284], 
processed observation next is [0.0, 0.043478260869565216, 0.7465373961218837, 0.78, 0.0, 0.0, 0.6666666666666666, 0.6389047079677358, 0.7160100398802856, 0.0, 1.0, 0.08917138314977542], 
reward next is 0.9108, 
noisyNet noise sample is [array([-0.7727346], dtype=float32), -1.6349283]. 
=============================================
[2019-04-03 23:38:10,903] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.8365336e-37 5.4476084e-27 7.5959278e-27 1.0000000e+00 1.4490742e-32
 1.2980844e-25 7.6668314e-30], sum to 1.0000
[2019-04-03 23:38:10,903] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5281
[2019-04-03 23:38:10,924] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [15.5, 93.0, 0.0, 0.0, 26.0, 23.91468553172772, 0.2302709931676303, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1221600.0000, 
sim time next is 1222200.0000, 
raw observation next is [15.5, 93.0, 0.0, 0.0, 26.0, 23.88997790607975, 0.2246237756207315, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.13043478260869565, 0.8919667590027703, 0.93, 0.0, 0.0, 0.6666666666666666, 0.4908314921733125, 0.5748745918735771, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.01463254], dtype=float32), -1.3541871]. 
=============================================
[2019-04-03 23:38:11,442] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3644012e-28 8.1559691e-17 3.7060884e-18 1.0000000e+00 1.1936383e-22
 3.3038974e-11 1.7409531e-21], sum to 1.0000
[2019-04-03 23:38:11,445] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7377
[2019-04-03 23:38:11,456] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 92.0, 106.0, 0.0, 26.0, 25.7120606411578, 0.5299714528938954, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1344600.0000, 
sim time next is 1345200.0000, 
raw observation next is [1.1, 92.0, 100.1666666666667, 0.0, 26.0, 25.69167042751869, 0.5344712768227294, 1.0, 1.0, 26883.26957439067], 
processed observation next is [1.0, 0.5652173913043478, 0.49307479224376743, 0.92, 0.333888888888889, 0.0, 0.6666666666666666, 0.6409725356265575, 0.6781570922742431, 1.0, 1.0, 0.12801556940186032], 
reward next is 0.8720, 
noisyNet noise sample is [array([-0.5685788], dtype=float32), -0.16751896]. 
=============================================
[2019-04-03 23:38:11,534] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.4902498e-28 1.0847460e-20 1.8283388e-17 1.0000000e+00 1.3072299e-22
 1.9625543e-13 3.1897284e-21], sum to 1.0000
[2019-04-03 23:38:11,535] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4785
[2019-04-03 23:38:11,550] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.4, 98.33333333333333, 0.0, 0.0, 26.0, 25.35479962290689, 0.4633987295622634, 0.0, 1.0, 72845.45740239203], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1395600.0000, 
sim time next is 1396200.0000, 
raw observation next is [-0.5, 99.16666666666667, 0.0, 0.0, 26.0, 25.29099636098744, 0.454239349023321, 0.0, 1.0, 57098.2401991657], 
processed observation next is [1.0, 0.13043478260869565, 0.44875346260387816, 0.9916666666666667, 0.0, 0.0, 0.6666666666666666, 0.6075830300822865, 0.651413116341107, 0.0, 1.0, 0.2718963819007891], 
reward next is 0.7281, 
noisyNet noise sample is [array([-0.7769293], dtype=float32), -1.290783]. 
=============================================
[2019-04-03 23:38:14,911] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.9443153e-28 3.2508554e-20 1.0889434e-17 1.0000000e+00 1.1708951e-22
 1.6729306e-13 7.2214433e-22], sum to 1.0000
[2019-04-03 23:38:14,914] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8263
[2019-04-03 23:38:15,002] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.1146852036274, 0.4180581041996279, 0.0, 1.0, 38639.71866833383], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1408200.0000, 
sim time next is 1408800.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.14260117852918, 0.4615724351700843, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.44598337950138506, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5952167648774317, 0.6538574783900281, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.86170006], dtype=float32), -1.6983633]. 
=============================================
[2019-04-03 23:38:24,567] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4538870e-38 9.2839868e-29 7.5869757e-28 1.0000000e+00 1.6157560e-34
 1.2062649e-26 2.7232014e-30], sum to 1.0000
[2019-04-03 23:38:24,574] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4340
[2019-04-03 23:38:24,639] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [15.0, 96.0, 0.0, 0.0, 26.0, 23.63139955165895, 0.1815052299010811, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1229400.0000, 
sim time next is 1230000.0000, 
raw observation next is [15.0, 96.0, 0.0, 0.0, 26.0, 23.64646603076586, 0.1783941863214454, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.21739130434782608, 0.8781163434903049, 0.96, 0.0, 0.0, 0.6666666666666666, 0.4705388358971551, 0.5594647287738151, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.0335457], dtype=float32), 0.8420038]. 
=============================================
[2019-04-03 23:38:24,646] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[101.233376]
 [101.223816]
 [101.23595 ]
 [101.264206]
 [101.29196 ]], R is [[101.23593903]
 [101.22357941]
 [101.21134186]
 [101.19922638]
 [101.18723297]].
[2019-04-03 23:38:28,642] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.6153485e-27 9.0758864e-20 9.7013900e-17 1.0000000e+00 2.1301654e-21
 5.2300655e-13 2.2215967e-21], sum to 1.0000
[2019-04-03 23:38:28,642] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0328
[2019-04-03 23:38:28,683] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.1, 92.0, 0.0, 0.0, 26.0, 25.4331027895702, 0.5775684295177798, 0.0, 1.0, 18763.89668928735], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1304400.0000, 
sim time next is 1305000.0000, 
raw observation next is [3.0, 92.0, 0.0, 0.0, 26.0, 25.47579451967882, 0.577949287482865, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.5457063711911359, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6229828766399018, 0.6926497624942883, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8448125], dtype=float32), -0.03343442]. 
=============================================
[2019-04-03 23:38:28,693] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[81.97324 ]
 [81.943855]
 [81.970665]
 [81.903694]
 [81.8532  ]], R is [[82.15093994]
 [82.24007416]
 [82.2008667 ]
 [82.10800934]
 [81.88707733]].
[2019-04-03 23:38:41,190] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.8883262e-26 1.2042542e-21 9.3963336e-20 1.0000000e+00 1.2232884e-21
 1.7803822e-15 7.4481192e-22], sum to 1.0000
[2019-04-03 23:38:41,191] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3662
[2019-04-03 23:38:41,207] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.7, 87.0, 0.0, 0.0, 26.0, 24.61061423588864, 0.2926723206637337, 0.0, 1.0, 44108.48411257214], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1754400.0000, 
sim time next is 1755000.0000, 
raw observation next is [-1.7, 87.0, 0.0, 0.0, 26.0, 24.58876886691899, 0.2957424002662712, 0.0, 1.0, 44124.30366686865], 
processed observation next is [0.0, 0.30434782608695654, 0.4155124653739613, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5490640722432492, 0.598580800088757, 0.0, 1.0, 0.21011573174699355], 
reward next is 0.7899, 
noisyNet noise sample is [array([1.4057951], dtype=float32), 1.2122781]. 
=============================================
[2019-04-03 23:38:41,236] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[80.53142 ]
 [80.528244]
 [80.547424]
 [80.57464 ]
 [80.58002 ]], R is [[80.54992676]
 [80.53439331]
 [80.51907349]
 [80.50393677]
 [80.4890213 ]].
[2019-04-03 23:38:43,281] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.5221200e-21 2.9569064e-13 1.1692784e-12 9.9999034e-01 6.2402347e-16
 9.6841668e-06 7.7511217e-16], sum to 1.0000
[2019-04-03 23:38:43,281] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9325
[2019-04-03 23:38:43,320] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.4, 85.33333333333334, 82.0, 0.0, 25.0, 24.92377089781151, 0.2163836450089096, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 2041800.0000, 
sim time next is 2042400.0000, 
raw observation next is [-4.300000000000001, 84.66666666666667, 76.5, 0.0, 25.0, 25.10680710345444, 0.2222273657317252, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.34349030470914127, 0.8466666666666667, 0.255, 0.0, 0.5833333333333334, 0.59223392528787, 0.5740757885772417, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8621097], dtype=float32), 0.90801376]. 
=============================================
[2019-04-03 23:38:46,789] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.6082200e-26 2.6213007e-19 2.7422232e-17 1.0000000e+00 1.7276702e-19
 1.6548317e-13 1.2951299e-20], sum to 1.0000
[2019-04-03 23:38:46,790] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5015
[2019-04-03 23:38:46,819] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.933333333333333, 82.16666666666667, 0.0, 0.0, 26.0, 24.21772161179666, 0.1190296061714081, 0.0, 1.0, 46302.5434024409], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1821000.0000, 
sim time next is 1821600.0000, 
raw observation next is [-6.0, 83.0, 0.0, 0.0, 26.0, 24.1830637504001, 0.1113678620420806, 0.0, 1.0, 46376.26315186571], 
processed observation next is [0.0, 0.08695652173913043, 0.296398891966759, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5152553125333416, 0.5371226206806935, 0.0, 1.0, 0.22083934834221766], 
reward next is 0.7792, 
noisyNet noise sample is [array([-0.09372857], dtype=float32), -0.3491172]. 
=============================================
[2019-04-03 23:38:51,617] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.5158167e-28 1.2381823e-18 1.7714311e-17 1.0000000e+00 2.0242524e-22
 6.4526821e-12 1.7715289e-21], sum to 1.0000
[2019-04-03 23:38:51,619] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2589
[2019-04-03 23:38:51,660] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.5, 92.0, 59.5, 0.0, 26.0, 26.00484364433503, 0.5608487103554478, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1677600.0000, 
sim time next is 1678200.0000, 
raw observation next is [1.433333333333333, 92.0, 61.66666666666667, 0.0, 26.0, 26.0245516839991, 0.5543114735326884, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.502308402585411, 0.92, 0.20555555555555557, 0.0, 0.6666666666666666, 0.6687126403332583, 0.6847704911775628, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.35135686], dtype=float32), 1.615136]. 
=============================================
[2019-04-03 23:38:57,692] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.9764816e-26 1.7924958e-18 5.2187745e-17 1.0000000e+00 5.5351676e-20
 1.1347961e-12 1.4701463e-20], sum to 1.0000
[2019-04-03 23:38:57,692] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5373
[2019-04-03 23:38:57,766] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 77.0, 186.0, 84.0, 26.0, 25.02390989796264, 0.2848516232121218, 0.0, 1.0, 45305.81671204297], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1866600.0000, 
sim time next is 1867200.0000, 
raw observation next is [-4.5, 79.0, 167.0, 70.0, 26.0, 25.02573872498164, 0.2847460031573132, 0.0, 1.0, 43143.52724107457], 
processed observation next is [0.0, 0.6086956521739131, 0.3379501385041552, 0.79, 0.5566666666666666, 0.07734806629834254, 0.6666666666666666, 0.5854782270818033, 0.594915334385771, 0.0, 1.0, 0.2054453678146408], 
reward next is 0.7946, 
noisyNet noise sample is [array([0.0527776], dtype=float32), -0.20873468]. 
=============================================
[2019-04-03 23:39:07,247] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.9450220e-25 9.8639909e-15 8.3480213e-15 1.0000000e+00 7.6709660e-20
 1.4269116e-08 6.0615501e-19], sum to 1.0000
[2019-04-03 23:39:07,248] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2240
[2019-04-03 23:39:07,338] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.300000000000001, 84.66666666666667, 76.5, 0.0, 26.0, 26.14695019589106, 0.3264756865519465, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2042400.0000, 
sim time next is 2043000.0000, 
raw observation next is [-4.2, 84.0, 71.0, 0.0, 26.0, 25.46191965623349, 0.3418937797944239, 1.0, 1.0, 66972.19804900249], 
processed observation next is [1.0, 0.6521739130434783, 0.34626038781163443, 0.84, 0.23666666666666666, 0.0, 0.6666666666666666, 0.6218266380194576, 0.613964593264808, 1.0, 1.0, 0.31891522880477374], 
reward next is 0.6811, 
noisyNet noise sample is [array([0.53452075], dtype=float32), -1.1402826]. 
=============================================
[2019-04-03 23:39:07,353] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[85.454254]
 [85.55112 ]
 [85.56105 ]
 [85.43528 ]
 [85.47714 ]], R is [[85.16384888]
 [85.31221008]
 [85.45909119]
 [85.60449982]
 [85.74845886]].
[2019-04-03 23:39:19,014] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1007022e-28 6.1984695e-23 1.1684256e-18 1.0000000e+00 2.3429830e-23
 4.5219595e-15 4.8942385e-22], sum to 1.0000
[2019-04-03 23:39:19,014] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0067
[2019-04-03 23:39:19,036] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.38262014642849, 0.1325843000035039, 0.0, 1.0, 41525.98911708251], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1997400.0000, 
sim time next is 1998000.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.38956800244009, 0.1217271574889456, 0.0, 1.0, 41468.16595311525], 
processed observation next is [1.0, 0.13043478260869565, 0.30747922437673136, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5324640002033408, 0.5405757191629819, 0.0, 1.0, 0.1974674569195964], 
reward next is 0.8025, 
noisyNet noise sample is [array([-0.1065879], dtype=float32), 1.2421516]. 
=============================================
[2019-04-03 23:39:19,061] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[83.59938 ]
 [83.54009 ]
 [83.47329 ]
 [83.4524  ]
 [83.431816]], R is [[83.67440033]
 [83.63990784]
 [83.6055069 ]
 [83.57116699]
 [83.53686523]].
[2019-04-03 23:39:24,310] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.2601606e-24 9.6202766e-13 5.7156516e-14 9.9999988e-01 6.3374219e-18
 6.8748733e-08 3.1177926e-18], sum to 1.0000
[2019-04-03 23:39:24,311] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9165
[2019-04-03 23:39:24,432] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 70.0, 8.333333333333332, 0.0, 26.0, 25.32354034871825, 0.2912241284107916, 1.0, 1.0, 88356.24763420332], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2222400.0000, 
sim time next is 2223000.0000, 
raw observation next is [-4.5, 69.5, 0.0, 0.0, 26.0, 24.4152432675118, 0.2352672703731374, 1.0, 1.0, 198264.3124049813], 
processed observation next is [1.0, 0.7391304347826086, 0.3379501385041552, 0.695, 0.0, 0.0, 0.6666666666666666, 0.5346036056259834, 0.5784224234577124, 1.0, 1.0, 0.9441157733570538], 
reward next is 0.0559, 
noisyNet noise sample is [array([0.8789285], dtype=float32), 2.0008426]. 
=============================================
[2019-04-03 23:39:24,467] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[81.99699 ]
 [82.342064]
 [82.86039 ]
 [83.333824]
 [83.77264 ]], R is [[80.95106506]
 [80.72080994]
 [80.91360474]
 [81.1044693 ]
 [81.29342651]].
[2019-04-03 23:39:25,502] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.3585359e-25 3.3555785e-16 3.2390176e-15 1.0000000e+00 2.3537378e-19
 7.0420593e-11 1.2453873e-18], sum to 1.0000
[2019-04-03 23:39:25,502] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6725
[2019-04-03 23:39:25,518] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.333333333333334, 83.0, 0.0, 0.0, 26.0, 25.29362625249329, 0.4103787314973886, 0.0, 1.0, 42239.41602859557], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2151600.0000, 
sim time next is 2152200.0000, 
raw observation next is [-6.516666666666667, 83.0, 0.0, 0.0, 26.0, 25.28213070821898, 0.4073991574532476, 0.0, 1.0, 42201.76117653507], 
processed observation next is [1.0, 0.9130434782608695, 0.2820867959372115, 0.83, 0.0, 0.0, 0.6666666666666666, 0.6068442256849149, 0.6357997191510826, 0.0, 1.0, 0.20096076750730985], 
reward next is 0.7990, 
noisyNet noise sample is [array([-1.8780752], dtype=float32), -0.52010363]. 
=============================================
[2019-04-03 23:39:26,795] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0848838e-25 1.1419617e-15 9.5935280e-16 9.9999988e-01 3.3489089e-20
 6.1297143e-08 7.1446181e-20], sum to 1.0000
[2019-04-03 23:39:26,799] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4285
[2019-04-03 23:39:26,833] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 67.5, 18.0, 0.0, 26.0, 26.12709274054956, 0.4649884048366008, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2134200.0000, 
sim time next is 2134800.0000, 
raw observation next is [-4.5, 68.0, 14.0, 0.0, 26.0, 26.15036108645772, 0.4603091345659149, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.3379501385041552, 0.68, 0.04666666666666667, 0.0, 0.6666666666666666, 0.6791967572048101, 0.6534363781886383, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1752369], dtype=float32), -0.75470114]. 
=============================================
[2019-04-03 23:39:47,686] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.2182566e-29 1.9801848e-21 5.8809052e-20 1.0000000e+00 1.5692884e-23
 3.3008736e-15 3.2538158e-23], sum to 1.0000
[2019-04-03 23:39:47,686] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6316
[2019-04-03 23:39:47,751] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 69.0, 79.0, 180.0, 26.0, 25.35017209965007, 0.3273469349659432, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2365200.0000, 
sim time next is 2365800.0000, 
raw observation next is [-3.3, 68.33333333333333, 93.0, 240.0, 26.0, 25.35006590821369, 0.3267446432543125, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.37119113573407203, 0.6833333333333332, 0.31, 0.26519337016574585, 0.6666666666666666, 0.6125054923511408, 0.6089148810847709, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2896914], dtype=float32), -0.5345321]. 
=============================================
[2019-04-03 23:39:53,344] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.1754623e-26 6.9733300e-15 3.6329820e-16 1.0000000e+00 1.3742735e-21
 1.4533524e-11 4.5513855e-20], sum to 1.0000
[2019-04-03 23:39:53,344] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8803
[2019-04-03 23:39:53,360] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.8, 29.0, 49.00000000000001, 109.6666666666667, 26.0, 25.66251142627504, 0.3885333900334692, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2566200.0000, 
sim time next is 2566800.0000, 
raw observation next is [2.7, 29.0, 38.5, 83.5, 26.0, 25.76047073457593, 0.3959092397428209, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.5373961218836566, 0.29, 0.12833333333333333, 0.09226519337016574, 0.6666666666666666, 0.6467058945479941, 0.6319697465809403, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8687518], dtype=float32), -2.1997888]. 
=============================================
[2019-04-03 23:40:03,987] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.7513407e-30 1.7708059e-21 1.1981702e-19 1.0000000e+00 1.9886828e-24
 4.6316302e-16 5.7528231e-23], sum to 1.0000
[2019-04-03 23:40:03,988] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7507
[2019-04-03 23:40:04,011] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.833333333333333, 63.66666666666667, 0.0, 0.0, 26.0, 25.21044489496309, 0.3587139480373176, 0.0, 1.0, 43811.45435860499], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2848200.0000, 
sim time next is 2848800.0000, 
raw observation next is [1.666666666666667, 65.33333333333334, 0.0, 0.0, 26.0, 25.20372797060103, 0.3557869074666231, 0.0, 1.0, 43044.2239848723], 
processed observation next is [1.0, 1.0, 0.5087719298245615, 0.6533333333333334, 0.0, 0.0, 0.6666666666666666, 0.6003106642167525, 0.6185956358222077, 0.0, 1.0, 0.20497249516605856], 
reward next is 0.7950, 
noisyNet noise sample is [array([0.14295274], dtype=float32), -0.14879571]. 
=============================================
[2019-04-03 23:40:07,180] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.94441334e-28 2.29045924e-19 1.08141645e-19 1.00000000e+00
 2.44968195e-22 1.58078781e-16 2.44347558e-22], sum to 1.0000
[2019-04-03 23:40:07,205] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6660
[2019-04-03 23:40:07,255] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.2, 75.33333333333334, 15.33333333333333, 154.3333333333333, 25.0, 24.11981133322289, 0.0961878601464638, 0.0, 1.0, 28854.96944990364], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 3086400.0000, 
sim time next is 3087000.0000, 
raw observation next is [-0.3, 77.0, 7.0, 88.0, 25.0, 24.11840216477647, 0.08683868808868722, 0.0, 1.0, 28121.26833776669], 
processed observation next is [0.0, 0.7391304347826086, 0.4542936288088643, 0.77, 0.023333333333333334, 0.09723756906077348, 0.5833333333333334, 0.5098668470647058, 0.5289462293628957, 0.0, 1.0, 0.13391080160841282], 
reward next is 0.8661, 
noisyNet noise sample is [array([-0.7003951], dtype=float32), -0.2594777]. 
=============================================
[2019-04-03 23:40:07,297] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[79.41821 ]
 [79.42948 ]
 [79.35519 ]
 [79.18471 ]
 [79.120865]], R is [[79.39690399]
 [79.4655304 ]
 [79.49967194]
 [79.4640274 ]
 [79.42475891]].
[2019-04-03 23:40:11,775] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.3881773e-28 6.3184908e-20 1.4102139e-17 1.0000000e+00 1.1689499e-21
 1.1158669e-13 2.5710486e-21], sum to 1.0000
[2019-04-03 23:40:11,776] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2807
[2019-04-03 23:40:11,801] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.683333333333334, 69.0, 0.0, 0.0, 26.0, 25.04146598236473, 0.3327562383444325, 0.0, 1.0, 45146.51795529344], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2674200.0000, 
sim time next is 2674800.0000, 
raw observation next is [-5.0, 69.0, 0.0, 0.0, 26.0, 24.98044713570679, 0.3211665096829521, 0.0, 1.0, 44649.95528983513], 
processed observation next is [1.0, 1.0, 0.32409972299168976, 0.69, 0.0, 0.0, 0.6666666666666666, 0.5817039279755658, 0.6070555032276507, 0.0, 1.0, 0.21261883471350063], 
reward next is 0.7874, 
noisyNet noise sample is [array([0.2089921], dtype=float32), -0.28470218]. 
=============================================
[2019-04-03 23:40:13,403] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.6890743e-24 4.0591216e-15 1.9370160e-14 1.0000000e+00 2.1683648e-19
 2.7964802e-09 1.1310694e-18], sum to 1.0000
[2019-04-03 23:40:13,403] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8303
[2019-04-03 23:40:13,451] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.0, 59.0, 0.0, 0.0, 26.0, 25.20854787630729, 0.4020417683602027, 1.0, 1.0, 59881.27469965156], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2746800.0000, 
sim time next is 2747400.0000, 
raw observation next is [-5.0, 59.0, 0.0, 0.0, 26.0, 25.19287874265737, 0.3937825790828155, 1.0, 1.0, 61389.56287151729], 
processed observation next is [1.0, 0.8260869565217391, 0.32409972299168976, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5994065618881143, 0.6312608596942718, 1.0, 1.0, 0.29233125176913], 
reward next is 0.7077, 
noisyNet noise sample is [array([-0.7942083], dtype=float32), -0.16237053]. 
=============================================
[2019-04-03 23:40:14,397] A3C_AGENT_WORKER-Thread-5 INFO:Evaluating...
[2019-04-03 23:40:14,400] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-03 23:40:14,400] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:40:14,402] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run14
[2019-04-03 23:40:14,418] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-03 23:40:14,420] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-03 23:40:14,422] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:40:14,423] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:40:14,428] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run14
[2019-04-03 23:40:14,442] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run14
[2019-04-03 23:40:19,132] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.03755743], dtype=float32), 0.70095533]
[2019-04-03 23:40:19,132] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [5.163008012666666, 100.0, 0.0, 0.0, 19.0, 18.79299026956776, -1.064080299270401, 0.0, 1.0, 0.0]
[2019-04-03 23:40:19,132] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-03 23:40:19,133] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.9361348e-13 5.1431698e-10 2.4590479e-09 9.9990308e-01 9.6214073e-08
 9.6824857e-05 5.4084060e-11], sampled 0.7752444686965784
[2019-04-03 23:40:41,272] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.03755743], dtype=float32), 0.70095533]
[2019-04-03 23:40:41,272] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [9.333333333333334, 84.0, 0.0, 0.0, 22.0, 21.04434303922772, -0.540931022337877, 0.0, 1.0, 52322.8541225104]
[2019-04-03 23:40:41,272] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-03 23:40:41,273] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [4.1367566e-19 8.5171616e-13 3.3772602e-13 9.9999988e-01 3.1604182e-13
 1.0073493e-07 9.6939956e-16], sampled 0.26621938275798784
[2019-04-03 23:42:16,356] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7322.1648 183056572.9776 -632.3427
[2019-04-03 23:42:46,766] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.03755743], dtype=float32), 0.70095533]
[2019-04-03 23:42:46,766] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [2.0, 80.0, 0.0, 0.0, 24.0, 23.75133851531784, 0.06179922532423789, 1.0, 1.0, 0.0]
[2019-04-03 23:42:46,766] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-03 23:42:46,767] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [8.7141668e-25 1.2200188e-18 7.0695587e-17 1.0000000e+00 2.2944382e-19
 1.2045417e-13 1.3174946e-19], sampled 0.6571825069248215
[2019-04-03 23:43:03,630] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7337.5894 237053426.4224 -494.4002
[2019-04-03 23:43:05,980] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7349.3376 236403122.2654 247.4047
[2019-04-03 23:43:07,015] A3C_AGENT_WORKER-Thread-5 INFO:Global step: 1300000, evaluation results [1300000.0, 7349.337552188603, 236403122.2654271, 247.40470312072432, 7322.164774291856, 183056572.97758985, -632.3426749271513, 7337.58941291947, 237053426.42244333, -494.40020939034747]
[2019-04-03 23:43:33,199] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.0144513e-22 2.3499345e-11 8.7392040e-13 1.0000000e+00 5.9759837e-18
 4.1272892e-09 2.1682181e-17], sum to 1.0000
[2019-04-03 23:43:33,200] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2066
[2019-04-03 23:43:33,206] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 25.0, 25.31466753221013, 0.4167969352689551, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 3520800.0000, 
sim time next is 3521400.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 25.0, 25.34857525914668, 0.4078414306281054, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.518005540166205, 0.52, 0.0, 0.0, 0.5833333333333334, 0.6123812715955568, 0.6359471435427019, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.23126085], dtype=float32), -0.34094983]. 
=============================================
[2019-04-03 23:43:40,370] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.6470973e-27 2.7511789e-19 1.3059721e-17 1.0000000e+00 7.7485417e-22
 6.9451667e-14 6.2983216e-21], sum to 1.0000
[2019-04-03 23:43:40,370] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8227
[2019-04-03 23:43:40,388] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 68.0, 0.0, 0.0, 26.0, 25.40871578432773, 0.4361904533701311, 0.0, 1.0, 53590.98495391024], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3364200.0000, 
sim time next is 3364800.0000, 
raw observation next is [-4.666666666666666, 69.0, 0.0, 0.0, 26.0, 25.3254019738519, 0.4242850161027826, 0.0, 1.0, 66719.98039512477], 
processed observation next is [1.0, 0.9565217391304348, 0.33333333333333337, 0.69, 0.0, 0.0, 0.6666666666666666, 0.6104501644876583, 0.6414283387009275, 0.0, 1.0, 0.317714192357737], 
reward next is 0.6823, 
noisyNet noise sample is [array([0.90614957], dtype=float32), 0.75582194]. 
=============================================
[2019-04-03 23:43:40,479] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.2282060e-26 2.6366673e-17 1.4292244e-15 1.0000000e+00 5.5465607e-20
 5.0884913e-12 2.1393196e-20], sum to 1.0000
[2019-04-03 23:43:40,485] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7915
[2019-04-03 23:43:40,517] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 69.0, 0.0, 0.0, 26.0, 25.87489597369127, 0.5971699874046511, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3267600.0000, 
sim time next is 3268200.0000, 
raw observation next is [-4.0, 70.0, 0.0, 0.0, 26.0, 25.79176318088486, 0.5753815733253046, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.3518005540166205, 0.7, 0.0, 0.0, 0.6666666666666666, 0.6493135984070717, 0.6917938577751016, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.20486642], dtype=float32), 0.18790577]. 
=============================================
[2019-04-03 23:43:49,838] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.1524784e-24 5.5497416e-14 3.3435646e-14 1.0000000e+00 1.2535849e-19
 3.2070855e-09 1.0317536e-18], sum to 1.0000
[2019-04-03 23:43:49,838] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7927
[2019-04-03 23:43:49,850] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 75.0, 25.66666666666666, 239.6666666666666, 26.0, 26.302499377437, 0.6891552557059679, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3258600.0000, 
sim time next is 3259200.0000, 
raw observation next is [-4.0, 73.0, 17.33333333333333, 171.8333333333333, 26.0, 26.45406332247495, 0.6223179820250059, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.3518005540166205, 0.73, 0.05777777777777776, 0.18987108655616938, 0.6666666666666666, 0.7045052768729126, 0.7074393273416687, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3488754], dtype=float32), -0.4356201]. 
=============================================
[2019-04-03 23:43:50,039] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.7687983e-25 9.4993559e-16 7.5982021e-16 1.0000000e+00 2.6910693e-19
 5.0205878e-10 1.6410878e-19], sum to 1.0000
[2019-04-03 23:43:50,039] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9367
[2019-04-03 23:43:50,079] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 77.0, 105.5, 722.0, 25.0, 25.46532620741938, 0.3364459634537178, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 3751200.0000, 
sim time next is 3751800.0000, 
raw observation next is [-3.0, 76.0, 107.3333333333333, 737.6666666666667, 25.0, 25.54242851659782, 0.3514836042119512, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.3795013850415513, 0.76, 0.3577777777777777, 0.8151012891344384, 0.5833333333333334, 0.628535709716485, 0.6171612014039837, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.096572], dtype=float32), -1.5197338]. 
=============================================
[2019-04-03 23:44:01,524] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.9420448e-28 2.0323120e-17 1.0369546e-17 1.0000000e+00 2.4396091e-23
 1.2932042e-13 1.5479329e-22], sum to 1.0000
[2019-04-03 23:44:01,524] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0071
[2019-04-03 23:44:01,567] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 25.60028959804449, 0.4980818037747591, 1.0, 1.0, 90358.60635358257], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3523800.0000, 
sim time next is 3524400.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 25.46833571748356, 0.4960293983667676, 1.0, 1.0, 51178.41918713024], 
processed observation next is [1.0, 0.8260869565217391, 0.518005540166205, 0.52, 0.0, 0.0, 0.6666666666666666, 0.6223613097902966, 0.6653431327889225, 1.0, 1.0, 0.24370675803395353], 
reward next is 0.7563, 
noisyNet noise sample is [array([0.9503536], dtype=float32), -1.6338743]. 
=============================================
[2019-04-03 23:44:09,151] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.8628751e-31 1.7095657e-23 4.6537565e-21 1.0000000e+00 2.1939684e-25
 4.6779165e-18 3.7279271e-24], sum to 1.0000
[2019-04-03 23:44:09,154] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0091
[2019-04-03 23:44:09,192] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 67.0, 0.0, 0.0, 26.0, 25.3308210889142, 0.3411290755928205, 0.0, 1.0, 85590.93612089676], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3721200.0000, 
sim time next is 3721800.0000, 
raw observation next is [-3.0, 66.0, 0.0, 0.0, 26.0, 25.24848402685197, 0.3437770636797852, 0.0, 1.0, 60879.35599856614], 
processed observation next is [1.0, 0.043478260869565216, 0.3795013850415513, 0.66, 0.0, 0.0, 0.6666666666666666, 0.6040403355709975, 0.6145923545599284, 0.0, 1.0, 0.28990169523126735], 
reward next is 0.7101, 
noisyNet noise sample is [array([0.3147178], dtype=float32), 0.30600935]. 
=============================================
[2019-04-03 23:44:12,123] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.5622711e-29 1.1122136e-18 4.0514583e-18 1.0000000e+00 1.1927546e-23
 6.1534524e-14 2.0957477e-22], sum to 1.0000
[2019-04-03 23:44:12,124] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4850
[2019-04-03 23:44:12,163] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.833333333333333, 54.16666666666667, 0.0, 0.0, 26.0, 25.42584076612959, 0.5135974210068944, 1.0, 1.0, 74180.27120579743], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3347400.0000, 
sim time next is 3348000.0000, 
raw observation next is [-3.0, 55.0, 0.0, 0.0, 26.0, 25.45967542495551, 0.5421439014606461, 1.0, 1.0, 46869.55423568316], 
processed observation next is [1.0, 0.782608695652174, 0.3795013850415513, 0.55, 0.0, 0.0, 0.6666666666666666, 0.6216396187462925, 0.6807146338202154, 1.0, 1.0, 0.22318835350325314], 
reward next is 0.7768, 
noisyNet noise sample is [array([0.30238232], dtype=float32), -0.901523]. 
=============================================
[2019-04-03 23:44:12,194] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[84.197876]
 [84.5128  ]
 [84.16318 ]
 [84.312225]
 [84.614494]], R is [[84.2490921 ]
 [84.05335999]
 [83.62545776]
 [83.78920746]
 [83.95131683]].
[2019-04-03 23:44:15,611] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.0304735e-29 2.7327059e-20 3.0172120e-20 1.0000000e+00 2.0233933e-24
 1.8018755e-17 2.5572638e-23], sum to 1.0000
[2019-04-03 23:44:15,622] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7359
[2019-04-03 23:44:15,630] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.0, 59.0, 0.0, 0.0, 26.0, 25.05717095224053, 0.3246198252193707, 0.0, 1.0, 50362.73738235263], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3694800.0000, 
sim time next is 3695400.0000, 
raw observation next is [4.0, 59.0, 0.0, 0.0, 26.0, 25.00201324214382, 0.3200146678537593, 0.0, 1.0, 58895.29670327033], 
processed observation next is [0.0, 0.782608695652174, 0.5734072022160666, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5835011035119851, 0.6066715559512531, 0.0, 1.0, 0.28045379382509683], 
reward next is 0.7195, 
noisyNet noise sample is [array([0.72716415], dtype=float32), -1.0351586]. 
=============================================
[2019-04-03 23:44:23,513] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.75099051e-28 6.32395835e-22 2.89458362e-19 1.00000000e+00
 1.05546566e-22 7.66468864e-17 4.00940007e-22], sum to 1.0000
[2019-04-03 23:44:23,517] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0032
[2019-04-03 23:44:23,558] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.5, 68.0, 0.0, 0.0, 26.0, 25.13541698569519, 0.374212488213386, 0.0, 1.0, 41174.80887011442], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3555000.0000, 
sim time next is 3555600.0000, 
raw observation next is [-3.666666666666667, 69.0, 0.0, 0.0, 26.0, 25.09450305201045, 0.3663855557437024, 0.0, 1.0, 41197.99315512183], 
processed observation next is [0.0, 0.13043478260869565, 0.3610341643582641, 0.69, 0.0, 0.0, 0.6666666666666666, 0.5912085876675374, 0.6221285185812341, 0.0, 1.0, 0.19618091978629443], 
reward next is 0.8038, 
noisyNet noise sample is [array([1.6140189], dtype=float32), 0.76458824]. 
=============================================
[2019-04-03 23:44:25,650] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.2652566e-30 1.6132670e-20 4.1423725e-22 1.0000000e+00 9.1622723e-25
 2.3132920e-17 4.0625268e-24], sum to 1.0000
[2019-04-03 23:44:25,651] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0063
[2019-04-03 23:44:25,753] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 39.0, 205.0, 475.6666666666667, 25.0, 24.20397299753913, 0.1837909348199425, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 4197000.0000, 
sim time next is 4197600.0000, 
raw observation next is [2.0, 40.0, 200.5, 379.0, 25.0, 24.19374173450657, 0.1839196591946488, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.518005540166205, 0.4, 0.6683333333333333, 0.41878453038674035, 0.5833333333333334, 0.5161451445422142, 0.561306553064883, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.361105], dtype=float32), 1.0314153]. 
=============================================
[2019-04-03 23:44:36,618] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.8057402e-27 1.4944854e-17 9.3878210e-17 1.0000000e+00 9.8126522e-22
 3.2919067e-12 8.9310454e-22], sum to 1.0000
[2019-04-03 23:44:36,618] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8044
[2019-04-03 23:44:36,661] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-9.666666666666668, 47.0, 102.8333333333333, 711.8333333333334, 26.0, 26.5331527836697, 0.5532085421662062, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4009200.0000, 
sim time next is 4009800.0000, 
raw observation next is [-9.333333333333332, 45.5, 104.6666666666667, 725.6666666666666, 26.0, 26.55606745461983, 0.5583526437929315, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.20406278855032323, 0.455, 0.348888888888889, 0.8018416206261509, 0.6666666666666666, 0.713005621218319, 0.6861175479309772, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.70154196], dtype=float32), -0.3166819]. 
=============================================
[2019-04-03 23:44:43,565] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.3979483e-29 2.6319490e-21 6.9370732e-19 1.0000000e+00 1.2914778e-22
 6.2401283e-16 9.8277440e-23], sum to 1.0000
[2019-04-03 23:44:43,592] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2181
[2019-04-03 23:44:43,663] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.0, 77.0, 0.0, 0.0, 26.0, 24.74232465583422, 0.2319235779627747, 0.0, 1.0, 43000.84912961742], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3825600.0000, 
sim time next is 3826200.0000, 
raw observation next is [-5.0, 77.0, 0.0, 0.0, 26.0, 24.66889766805754, 0.2345841680786889, 0.0, 1.0, 42926.30464491455], 
processed observation next is [1.0, 0.2608695652173913, 0.32409972299168976, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5557414723381283, 0.5781947226928963, 0.0, 1.0, 0.20441097449959308], 
reward next is 0.7956, 
noisyNet noise sample is [array([0.32429236], dtype=float32), 1.1711831]. 
=============================================
[2019-04-03 23:44:48,317] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.7587314e-30 9.2146880e-22 7.8965593e-19 1.0000000e+00 1.5197819e-24
 2.5519314e-17 2.1213578e-22], sum to 1.0000
[2019-04-03 23:44:48,318] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8824
[2019-04-03 23:44:48,382] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.683333333333334, 65.16666666666667, 0.0, 0.0, 25.0, 25.00344918535256, 0.4227720950578615, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 4410600.0000, 
sim time next is 4411200.0000, 
raw observation next is [6.566666666666667, 65.33333333333334, 0.0, 0.0, 25.0, 25.01043428108491, 0.4234061513798967, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.6445060018467221, 0.6533333333333334, 0.0, 0.0, 0.5833333333333334, 0.5842028567570757, 0.6411353837932988, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.48203963], dtype=float32), 0.8507211]. 
=============================================
[2019-04-03 23:44:49,143] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.3344273e-28 1.6783541e-17 2.7162474e-17 1.0000000e+00 1.8174921e-23
 1.9264497e-13 1.2587493e-21], sum to 1.0000
[2019-04-03 23:44:49,143] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0763
[2019-04-03 23:44:49,263] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 29.0, 116.0, 835.5, 26.0, 25.96759063451801, 0.5380387294483934, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4021200.0000, 
sim time next is 4021800.0000, 
raw observation next is [-3.833333333333333, 28.5, 115.3333333333333, 833.6666666666666, 26.0, 26.02123524065374, 0.5498943642889681, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.3564173591874424, 0.285, 0.3844444444444443, 0.9211786372007366, 0.6666666666666666, 0.6684362700544785, 0.683298121429656, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.07581208], dtype=float32), 1.2803622]. 
=============================================
[2019-04-03 23:44:54,645] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.3291656e-27 1.5252577e-18 1.0554960e-18 1.0000000e+00 7.6363566e-23
 1.7088125e-12 1.0071597e-21], sum to 1.0000
[2019-04-03 23:44:54,646] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6380
[2019-04-03 23:44:54,825] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.333333333333334, 41.33333333333333, 113.1666666666667, 786.8333333333334, 26.0, 26.60045153531421, 0.5840262612504302, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4012800.0000, 
sim time next is 4013400.0000, 
raw observation next is [-8.166666666666666, 40.66666666666667, 114.3333333333333, 792.6666666666667, 26.0, 26.56646618535979, 0.5868354130611092, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.23638042474607576, 0.40666666666666673, 0.381111111111111, 0.8758747697974218, 0.6666666666666666, 0.7138721821133158, 0.6956118043537031, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.1333944], dtype=float32), -0.9175215]. 
=============================================
[2019-04-03 23:44:55,398] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.0426093e-27 1.9544833e-19 1.0859401e-17 1.0000000e+00 1.1399127e-21
 3.0356468e-14 7.9271246e-21], sum to 1.0000
[2019-04-03 23:44:55,399] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9145
[2019-04-03 23:44:55,459] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.583333333333333, 67.0, 0.0, 0.0, 25.0, 24.93743028082644, 0.3957844974739875, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 4420200.0000, 
sim time next is 4420800.0000, 
raw observation next is [4.5, 67.0, 0.0, 0.0, 25.0, 25.04131208778422, 0.3750984354954026, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.5872576177285319, 0.67, 0.0, 0.0, 0.5833333333333334, 0.5867760073153517, 0.6250328118318008, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7397205], dtype=float32), 0.9097562]. 
=============================================
[2019-04-03 23:45:07,620] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.2452358e-25 3.6624119e-16 1.9778344e-16 1.0000000e+00 4.5901457e-20
 5.9749762e-12 1.9340723e-19], sum to 1.0000
[2019-04-03 23:45:07,624] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5630
[2019-04-03 23:45:07,690] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 25.0, 24.54898063956469, 0.2558704524024886, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 4564800.0000, 
sim time next is 4565400.0000, 
raw observation next is [2.0, 52.83333333333334, 0.0, 0.0, 25.0, 24.48154855562565, 0.2465223084155193, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.518005540166205, 0.5283333333333334, 0.0, 0.0, 0.5833333333333334, 0.5401290463021375, 0.5821741028051731, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.29095197], dtype=float32), -1.4749578]. 
=============================================
[2019-04-03 23:45:25,058] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0016613e-29 3.9239290e-20 2.3334706e-19 1.0000000e+00 4.8045715e-24
 7.6178501e-16 6.5362080e-23], sum to 1.0000
[2019-04-03 23:45:25,058] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7102
[2019-04-03 23:45:25,104] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [9.933333333333334, 59.66666666666667, 0.0, 0.0, 26.0, 26.66573245676805, 0.7994846195299155, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4396800.0000, 
sim time next is 4397400.0000, 
raw observation next is [9.8, 60.0, 0.0, 0.0, 26.0, 26.61519841589728, 0.7883407382100281, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.7340720221606649, 0.6, 0.0, 0.0, 0.6666666666666666, 0.7179332013247732, 0.7627802460700094, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.0740343], dtype=float32), 0.6540611]. 
=============================================
[2019-04-03 23:45:26,843] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.1272197e-30 6.5497247e-19 5.7032093e-20 1.0000000e+00 2.8231246e-25
 1.0839882e-14 1.2882782e-24], sum to 1.0000
[2019-04-03 23:45:26,872] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0671
[2019-04-03 23:45:26,944] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.5, 27.5, 114.0, 830.0, 26.0, 26.12653395030224, 0.5758460132508444, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4023000.0000, 
sim time next is 4023600.0000, 
raw observation next is [-3.333333333333333, 27.0, 112.3333333333333, 824.0, 26.0, 26.183317384097, 0.4866463923586039, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.37026777469990774, 0.27, 0.37444444444444436, 0.9104972375690608, 0.6666666666666666, 0.6819431153414165, 0.6622154641195347, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.5086052], dtype=float32), 0.3811262]. 
=============================================
[2019-04-03 23:45:30,508] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.7117062e-25 6.3943502e-16 5.4013904e-16 1.0000000e+00 4.8798703e-20
 1.0213584e-11 6.0975173e-20], sum to 1.0000
[2019-04-03 23:45:30,508] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8204
[2019-04-03 23:45:30,544] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 72.0, 100.0, 11.0, 25.0, 24.36389154565333, 0.244722844857054, 1.0, 1.0, 18680.41167023053], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 4725000.0000, 
sim time next is 4725600.0000, 
raw observation next is [1.0, 72.0, 88.16666666666667, 13.83333333333333, 25.0, 24.68206700423158, 0.2703909747106849, 1.0, 1.0, 9340.205835115268], 
processed observation next is [1.0, 0.6956521739130435, 0.4903047091412743, 0.72, 0.2938888888888889, 0.015285451197053403, 0.5833333333333334, 0.5568389170192983, 0.5901303249035617, 1.0, 1.0, 0.04447717064340604], 
reward next is 0.9555, 
noisyNet noise sample is [array([-0.5896889], dtype=float32), -1.6662391]. 
=============================================
[2019-04-03 23:45:35,963] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.7394408e-27 9.8799151e-16 7.9050362e-17 1.0000000e+00 4.4378443e-24
 1.2919919e-12 3.9627194e-22], sum to 1.0000
[2019-04-03 23:45:35,964] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8320
[2019-04-03 23:45:35,996] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [13.0, 33.0, 118.3333333333333, 812.0, 26.0, 27.74313928274142, 0.9006362325586398, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4360200.0000, 
sim time next is 4360800.0000, 
raw observation next is [13.4, 32.0, 119.1666666666667, 820.0, 26.0, 27.7519403680508, 0.9238157412672706, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.8337950138504157, 0.32, 0.3972222222222223, 0.9060773480662984, 0.6666666666666666, 0.8126616973375667, 0.8079385804224235, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.70442116], dtype=float32), -1.058891]. 
=============================================
[2019-04-03 23:45:47,819] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2588654e-27 1.6043171e-15 8.7175268e-17 1.0000000e+00 7.1472210e-24
 2.0030914e-13 2.8905752e-21], sum to 1.0000
[2019-04-03 23:45:47,819] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7565
[2019-04-03 23:45:47,869] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [13.15, 34.5, 81.66666666666667, 0.0, 26.0, 28.4477179290318, 1.110967956344521, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4377000.0000, 
sim time next is 4377600.0000, 
raw observation next is [13.0, 35.0, 71.0, 0.0, 26.0, 28.61404247966846, 0.9379515588376249, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.8227146814404434, 0.35, 0.23666666666666666, 0.0, 0.6666666666666666, 0.8845035399723716, 0.8126505196125416, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.28416938], dtype=float32), 1.1763533]. 
=============================================
[2019-04-03 23:45:48,726] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.9890397e-29 7.4427054e-19 6.9944159e-19 1.0000000e+00 8.0524073e-24
 1.8463650e-14 9.0434901e-23], sum to 1.0000
[2019-04-03 23:45:48,727] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9895
[2019-04-03 23:45:48,733] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 50.66666666666666, 147.0, 7.999999999999998, 26.0, 25.9502200701652, 0.478843327968203, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4538400.0000, 
sim time next is 4539000.0000, 
raw observation next is [2.0, 51.33333333333334, 167.0, 16.0, 26.0, 25.82529235440111, 0.4681982907660531, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.518005540166205, 0.5133333333333334, 0.5566666666666666, 0.017679558011049725, 0.6666666666666666, 0.6521076962000926, 0.6560660969220177, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.80330396], dtype=float32), -0.00017012362]. 
=============================================
[2019-04-03 23:45:48,794] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[85.22848]
 [84.7497 ]
 [84.80791]
 [84.97695]
 [85.22007]], R is [[85.79789734]
 [85.93991852]
 [86.08052063]
 [86.21971893]
 [86.35752106]].
[2019-04-03 23:46:04,166] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:46:04,166] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:46:04,172] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run11
[2019-04-03 23:46:08,921] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:46:08,921] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:46:08,925] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run11
[2019-04-03 23:46:09,590] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.2309717e-26 9.8886475e-19 2.5096757e-17 1.0000000e+00 3.1664757e-21
 2.2275630e-13 7.0625634e-21], sum to 1.0000
[2019-04-03 23:46:09,590] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6409
[2019-04-03 23:46:09,625] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.5, 84.5, 0.0, 0.0, 26.0, 25.49838964825872, 0.5145396171740134, 0.0, 1.0, 24915.07095570558], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4743000.0000, 
sim time next is 4743600.0000, 
raw observation next is [-2.666666666666667, 84.33333333333334, 0.0, 0.0, 26.0, 25.44759990400019, 0.5084741647771159, 0.0, 1.0, 59913.04912807896], 
processed observation next is [1.0, 0.9130434782608695, 0.38873499538319484, 0.8433333333333334, 0.0, 0.0, 0.6666666666666666, 0.620633325333349, 0.6694913882590386, 0.0, 1.0, 0.2853002339432331], 
reward next is 0.7147, 
noisyNet noise sample is [array([1.2618548], dtype=float32), -1.5808296]. 
=============================================
[2019-04-03 23:46:13,264] A3C_AGENT_WORKER-Thread-2 INFO:Local step 85000, global step 1350322: loss 3.1846
[2019-04-03 23:46:13,316] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 85000, global step 1350322: learning rate 0.0005
[2019-04-03 23:46:14,748] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.3035537e-31 1.0165692e-23 2.8598306e-22 1.0000000e+00 3.7802668e-25
 2.9622646e-20 1.4080694e-24], sum to 1.0000
[2019-04-03 23:46:14,749] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2818
[2019-04-03 23:46:14,791] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 40.0, 0.0, 0.0, 26.0, 25.37985595157443, 0.3983301498694833, 0.0, 1.0, 39701.31030939527], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4910400.0000, 
sim time next is 4911000.0000, 
raw observation next is [1.0, 39.33333333333334, 0.0, 0.0, 26.0, 25.47623180403738, 0.402704434998814, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.4903047091412743, 0.3933333333333334, 0.0, 0.0, 0.6666666666666666, 0.623019317003115, 0.6342348116662714, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.95269203], dtype=float32), 0.22035159]. 
=============================================
[2019-04-03 23:46:14,865] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[84.41149]
 [84.6284 ]
 [84.72181]
 [84.57919]
 [84.17254]], R is [[84.38669586]
 [84.35377502]
 [84.25402069]
 [84.01676178]
 [83.44892883]].
[2019-04-03 23:46:17,741] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.3332513e-30 2.4100925e-23 5.3255969e-20 1.0000000e+00 1.9595238e-25
 8.2819339e-19 4.1628032e-24], sum to 1.0000
[2019-04-03 23:46:17,741] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8736
[2019-04-03 23:46:17,829] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.666666666666667, 48.66666666666667, 0.0, 0.0, 26.0, 24.87436211249825, 0.2415024017067199, 1.0, 1.0, 8336.977784759561], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4951200.0000, 
sim time next is 4951800.0000, 
raw observation next is [-2.5, 48.0, 0.0, 0.0, 26.0, 25.15228228915963, 0.2651860008485245, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.39335180055401664, 0.48, 0.0, 0.0, 0.6666666666666666, 0.5960235240966357, 0.5883953336161748, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3708963], dtype=float32), -1.2619164]. 
=============================================
[2019-04-03 23:46:18,498] A3C_AGENT_WORKER-Thread-19 INFO:Local step 85000, global step 1351577: loss 2.5385
[2019-04-03 23:46:18,499] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 85000, global step 1351577: learning rate 0.0005
[2019-04-03 23:46:27,227] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.6038720e-32 2.4491189e-24 4.7174205e-22 1.0000000e+00 4.7777182e-26
 4.4677806e-21 3.1018026e-25], sum to 1.0000
[2019-04-03 23:46:27,228] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4692
[2019-04-03 23:46:27,242] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.2, 46.0, 281.0, 390.0, 26.0, 25.11422613461927, 0.360258691084393, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4883400.0000, 
sim time next is 4884000.0000, 
raw observation next is [1.266666666666667, 45.66666666666667, 279.5, 389.6666666666666, 26.0, 25.09541002026522, 0.3544727361558371, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.4976915974145891, 0.4566666666666667, 0.9316666666666666, 0.4305709023941067, 0.6666666666666666, 0.591284168355435, 0.6181575787186123, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.45330873], dtype=float32), -0.36968037]. 
=============================================
[2019-04-03 23:46:27,255] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[85.22774 ]
 [85.31383 ]
 [85.40637 ]
 [85.50506 ]
 [85.562645]], R is [[85.22858429]
 [85.376297  ]
 [85.52253723]
 [85.66731262]
 [85.81063843]].
[2019-04-03 23:46:29,373] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:46:29,373] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:46:29,376] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run11
[2019-04-03 23:46:29,809] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:46:29,809] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:46:29,815] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run11
[2019-04-03 23:46:31,687] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:46:31,688] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:46:31,711] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run11
[2019-04-03 23:46:36,655] A3C_AGENT_WORKER-Thread-18 INFO:Local step 85000, global step 1358031: loss 1.9494
[2019-04-03 23:46:36,656] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 85000, global step 1358031: learning rate 0.0005
[2019-04-03 23:46:37,044] A3C_AGENT_WORKER-Thread-3 INFO:Local step 85000, global step 1358209: loss 1.7339
[2019-04-03 23:46:37,045] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 85000, global step 1358209: learning rate 0.0005
[2019-04-03 23:46:37,127] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:46:37,127] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:46:37,133] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run11
[2019-04-03 23:46:38,266] A3C_AGENT_WORKER-Thread-2 INFO:Local step 85500, global step 1358717: loss 0.2541
[2019-04-03 23:46:38,267] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 85500, global step 1358717: learning rate 0.0005
[2019-04-03 23:46:38,478] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:46:38,478] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:46:38,483] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run11
[2019-04-03 23:46:38,656] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:46:38,656] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:46:38,665] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run11
[2019-04-03 23:46:39,080] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:46:39,081] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:46:39,087] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run11
[2019-04-03 23:46:39,120] A3C_AGENT_WORKER-Thread-17 INFO:Local step 85000, global step 1359043: loss 1.5558
[2019-04-03 23:46:39,126] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 85000, global step 1359045: learning rate 0.0005
[2019-04-03 23:46:39,474] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.5064489e-31 1.0674662e-24 4.5473719e-22 1.0000000e+00 1.2524100e-25
 7.6744195e-20 2.9803924e-25], sum to 1.0000
[2019-04-03 23:46:39,475] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0859
[2019-04-03 23:46:39,489] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.6666666666666667, 37.0, 0.0, 0.0, 26.0, 25.42795044329053, 0.3661867590234519, 0.0, 1.0, 43047.18244271174], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4918800.0000, 
sim time next is 4919400.0000, 
raw observation next is [0.5, 37.5, 0.0, 0.0, 26.0, 25.4610612761858, 0.3624150925701799, 0.0, 1.0, 21853.81174654678], 
processed observation next is [0.0, 0.9565217391304348, 0.4764542936288089, 0.375, 0.0, 0.0, 0.6666666666666666, 0.6217551063488166, 0.6208050308567267, 0.0, 1.0, 0.10406577022165134], 
reward next is 0.8959, 
noisyNet noise sample is [array([-0.8968797], dtype=float32), -0.79299694]. 
=============================================
[2019-04-03 23:46:40,606] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:46:40,606] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:46:40,612] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run11
[2019-04-03 23:46:41,830] A3C_AGENT_WORKER-Thread-19 INFO:Local step 85500, global step 1359808: loss 0.1491
[2019-04-03 23:46:41,830] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 85500, global step 1359808: learning rate 0.0005
[2019-04-03 23:46:44,650] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.6495549e-32 4.3467325e-24 5.6492633e-23 1.0000000e+00 5.3312951e-25
 1.5168269e-18 2.1109171e-25], sum to 1.0000
[2019-04-03 23:46:44,651] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9245
[2019-04-03 23:46:44,667] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 36.0, 0.0, 0.0, 26.0, 25.42013929993405, 0.3688057689560592, 0.0, 1.0, 59975.9999427247], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4915200.0000, 
sim time next is 4915800.0000, 
raw observation next is [1.0, 36.0, 0.0, 0.0, 26.0, 25.41025099218428, 0.3704134580371021, 0.0, 1.0, 51731.79671504904], 
processed observation next is [0.0, 0.9130434782608695, 0.4903047091412743, 0.36, 0.0, 0.0, 0.6666666666666666, 0.6175209160153567, 0.6234711526790341, 0.0, 1.0, 0.24634188911928112], 
reward next is 0.7537, 
noisyNet noise sample is [array([-0.46006092], dtype=float32), -0.69655466]. 
=============================================
[2019-04-03 23:46:45,149] A3C_AGENT_WORKER-Thread-5 INFO:Local step 85000, global step 1360480: loss 1.1853
[2019-04-03 23:46:45,150] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 85000, global step 1360480: learning rate 0.0005
[2019-04-03 23:46:45,621] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:46:45,621] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:46:45,630] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run11
[2019-04-03 23:46:46,287] A3C_AGENT_WORKER-Thread-6 INFO:Local step 85000, global step 1360852: loss 0.7734
[2019-04-03 23:46:46,290] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 85000, global step 1360852: learning rate 0.0005
[2019-04-03 23:46:46,467] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1828187e-17 1.1306883e-12 1.1165907e-12 9.9999750e-01 1.6989951e-10
 2.5038837e-06 2.1017796e-14], sum to 1.0000
[2019-04-03 23:46:46,489] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8069
[2019-04-03 23:46:46,532] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.7, 93.0, 0.0, 0.0, 19.0, 18.84983252241928, -1.09289030418532, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 21000.0000, 
sim time next is 21600.0000, 
raw observation next is [7.7, 93.0, 0.0, 0.0, 19.0, 18.83734255591628, -1.097273730849206, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.2608695652173913, 0.6759002770083103, 0.93, 0.0, 0.0, 0.08333333333333333, 0.06977854632635665, 0.13424208971693133, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6089841], dtype=float32), 1.4719994]. 
=============================================
[2019-04-03 23:46:46,683] A3C_AGENT_WORKER-Thread-13 INFO:Local step 85000, global step 1360987: loss 0.6735
[2019-04-03 23:46:46,685] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 85000, global step 1360987: learning rate 0.0005
[2019-04-03 23:46:46,846] A3C_AGENT_WORKER-Thread-12 INFO:Local step 85000, global step 1361043: loss 0.5851
[2019-04-03 23:46:46,849] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 85000, global step 1361043: learning rate 0.0005
[2019-04-03 23:46:47,677] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:46:47,677] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:46:47,699] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run11
[2019-04-03 23:46:48,272] A3C_AGENT_WORKER-Thread-16 INFO:Local step 85000, global step 1361610: loss 0.2851
[2019-04-03 23:46:48,274] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 85000, global step 1361610: learning rate 0.0005
[2019-04-03 23:46:48,501] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:46:48,501] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:46:48,504] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run11
[2019-04-03 23:46:48,621] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.9672777e-18 2.7967396e-13 5.7734156e-13 9.9997652e-01 9.5097437e-11
 2.3437538e-05 3.0541833e-14], sum to 1.0000
[2019-04-03 23:46:48,621] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6474
[2019-04-03 23:46:48,651] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.516666666666667, 88.33333333333334, 0.0, 0.0, 19.0, 19.06311616401108, -1.008884135590593, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 72600.0000, 
sim time next is 73200.0000, 
raw observation next is [2.333333333333333, 87.66666666666667, 0.0, 0.0, 19.0, 19.17600977017722, -1.005573889710252, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.5272391505078486, 0.8766666666666667, 0.0, 0.0, 0.08333333333333333, 0.09800081418143503, 0.16480870342991602, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.13995227], dtype=float32), 1.271728]. 
=============================================
[2019-04-03 23:46:52,753] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:46:52,753] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:46:52,756] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run11
[2019-04-03 23:46:53,479] A3C_AGENT_WORKER-Thread-4 INFO:Local step 85000, global step 1362990: loss 0.2761
[2019-04-03 23:46:53,482] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 85000, global step 1362990: learning rate 0.0005
[2019-04-03 23:46:55,837] A3C_AGENT_WORKER-Thread-11 INFO:Local step 85000, global step 1363527: loss 0.3664
[2019-04-03 23:46:55,863] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 85000, global step 1363531: learning rate 0.0005
[2019-04-03 23:46:56,556] A3C_AGENT_WORKER-Thread-14 INFO:Local step 85000, global step 1363766: loss 0.3307
[2019-04-03 23:46:56,559] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 85000, global step 1363766: learning rate 0.0005
[2019-04-03 23:46:56,865] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:46:56,866] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:46:56,873] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run11
[2019-04-03 23:46:57,527] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:46:57,527] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:46:57,531] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run11
[2019-04-03 23:46:57,577] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.2770725e-19 5.8031027e-13 2.4021478e-11 9.9988306e-01 9.6492150e-13
 1.1690162e-04 2.6131039e-14], sum to 1.0000
[2019-04-03 23:46:57,586] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3571
[2019-04-03 23:46:57,666] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.4, 68.0, 0.0, 0.0, 24.0, 23.09674213555319, -0.1759521471303129, 0.0, 1.0, 54759.55914326756], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 159600.0000, 
sim time next is 160200.0000, 
raw observation next is [-8.4, 68.0, 0.0, 0.0, 24.0, 23.06919818324467, -0.1878230952184015, 0.0, 1.0, 50023.70201903999], 
processed observation next is [1.0, 0.8695652173913043, 0.2299168975069252, 0.68, 0.0, 0.0, 0.5, 0.42243318193705576, 0.43739230159386616, 0.0, 1.0, 0.2382081048525714], 
reward next is 0.7618, 
noisyNet noise sample is [array([0.01681803], dtype=float32), 0.9007189]. 
=============================================
[2019-04-03 23:47:01,017] A3C_AGENT_WORKER-Thread-20 INFO:Local step 85000, global step 1364923: loss 0.5760
[2019-04-03 23:47:01,049] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 85000, global step 1364923: learning rate 0.0005
[2019-04-03 23:47:01,878] A3C_AGENT_WORKER-Thread-3 INFO:Local step 85500, global step 1365117: loss 1.0073
[2019-04-03 23:47:01,884] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 85500, global step 1365117: learning rate 0.0005
[2019-04-03 23:47:02,528] A3C_AGENT_WORKER-Thread-18 INFO:Local step 85500, global step 1365256: loss 0.1795
[2019-04-03 23:47:02,547] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 85500, global step 1365256: learning rate 0.0005
[2019-04-03 23:47:04,980] A3C_AGENT_WORKER-Thread-10 INFO:Local step 85000, global step 1365844: loss 1.5984
[2019-04-03 23:47:04,981] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 85000, global step 1365844: learning rate 0.0005
[2019-04-03 23:47:05,306] A3C_AGENT_WORKER-Thread-2 INFO:Local step 86000, global step 1365941: loss 0.1775
[2019-04-03 23:47:05,307] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 86000, global step 1365941: learning rate 0.0005
[2019-04-03 23:47:05,616] A3C_AGENT_WORKER-Thread-15 INFO:Local step 85000, global step 1366048: loss 0.4974
[2019-04-03 23:47:05,622] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 85000, global step 1366048: learning rate 0.0005
[2019-04-03 23:47:06,586] A3C_AGENT_WORKER-Thread-17 INFO:Local step 85500, global step 1366321: loss 0.1520
[2019-04-03 23:47:06,586] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 85500, global step 1366321: learning rate 0.0005
[2019-04-03 23:47:10,123] A3C_AGENT_WORKER-Thread-19 INFO:Local step 86000, global step 1367437: loss 0.2815
[2019-04-03 23:47:10,126] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 86000, global step 1367437: learning rate 0.0005
[2019-04-03 23:47:13,055] A3C_AGENT_WORKER-Thread-5 INFO:Local step 85500, global step 1368082: loss 0.5168
[2019-04-03 23:47:13,056] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 85500, global step 1368082: learning rate 0.0005
[2019-04-03 23:47:13,659] A3C_AGENT_WORKER-Thread-13 INFO:Local step 85500, global step 1368226: loss 0.1038
[2019-04-03 23:47:13,660] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 85500, global step 1368226: learning rate 0.0005
[2019-04-03 23:47:14,170] A3C_AGENT_WORKER-Thread-6 INFO:Local step 85500, global step 1368359: loss 0.3370
[2019-04-03 23:47:14,181] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 85500, global step 1368359: learning rate 0.0005
[2019-04-03 23:47:14,642] A3C_AGENT_WORKER-Thread-12 INFO:Local step 85500, global step 1368471: loss 0.8350
[2019-04-03 23:47:14,665] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 85500, global step 1368471: learning rate 0.0005
[2019-04-03 23:47:17,200] A3C_AGENT_WORKER-Thread-16 INFO:Local step 85500, global step 1369225: loss 0.0718
[2019-04-03 23:47:17,200] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 85500, global step 1369225: learning rate 0.0005
[2019-04-03 23:47:21,466] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.7193069e-27 2.6818549e-20 3.5435851e-18 1.0000000e+00 9.9456059e-22
 5.6652376e-14 3.9712864e-21], sum to 1.0000
[2019-04-03 23:47:21,467] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9869
[2019-04-03 23:47:21,549] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.8, 90.33333333333334, 0.0, 0.0, 24.0, 23.37755931447343, -0.1250139737792083, 0.0, 1.0, 38798.65114668009], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 521400.0000, 
sim time next is 522000.0000, 
raw observation next is [5.0, 89.0, 0.0, 0.0, 24.0, 23.39147997780358, -0.1046819947089737, 0.0, 1.0, 32087.90974100837], 
processed observation next is [0.0, 0.043478260869565216, 0.6011080332409973, 0.89, 0.0, 0.0, 0.5, 0.4492899981502984, 0.46510600176367545, 0.0, 1.0, 0.15279957019527796], 
reward next is 0.8472, 
noisyNet noise sample is [array([2.0678482], dtype=float32), 0.062029783]. 
=============================================
[2019-04-03 23:47:21,580] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[90.40471 ]
 [90.07939 ]
 [89.77976 ]
 [89.638664]
 [89.5042  ]], R is [[90.74971008]
 [90.65746307]
 [90.53918457]
 [90.39507294]
 [90.22797394]].
[2019-04-03 23:47:21,686] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0447993e-27 1.5881102e-20 1.2218253e-19 1.0000000e+00 9.6870170e-22
 1.4612365e-13 4.9538223e-21], sum to 1.0000
[2019-04-03 23:47:21,686] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1170
[2019-04-03 23:47:21,698] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.65, 88.5, 0.0, 0.0, 24.0, 23.53905653764948, -0.1238003170526592, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 523800.0000, 
sim time next is 524400.0000, 
raw observation next is [4.533333333333333, 88.33333333333334, 0.0, 0.0, 24.0, 23.46055525543458, -0.1365499966328143, 0.0, 1.0, 51431.561157407], 
processed observation next is [0.0, 0.043478260869565216, 0.5881809787626964, 0.8833333333333334, 0.0, 0.0, 0.5, 0.4550462712862151, 0.4544833344557286, 0.0, 1.0, 0.24491219598765238], 
reward next is 0.7551, 
noisyNet noise sample is [array([-0.975605], dtype=float32), 0.8113145]. 
=============================================
[2019-04-03 23:47:22,253] A3C_AGENT_WORKER-Thread-4 INFO:Local step 85500, global step 1370617: loss 0.5164
[2019-04-03 23:47:22,253] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 85500, global step 1370617: learning rate 0.0005
[2019-04-03 23:47:23,964] A3C_AGENT_WORKER-Thread-11 INFO:Local step 85500, global step 1371073: loss 0.2270
[2019-04-03 23:47:23,964] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 85500, global step 1371073: learning rate 0.0005
[2019-04-03 23:47:25,892] A3C_AGENT_WORKER-Thread-14 INFO:Local step 85500, global step 1371703: loss 0.4917
[2019-04-03 23:47:25,894] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 85500, global step 1371703: learning rate 0.0005
[2019-04-03 23:47:27,478] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.5306041e-22 1.3415095e-17 5.4159393e-15 1.0000000e+00 7.4592024e-16
 6.6719213e-10 3.2635591e-17], sum to 1.0000
[2019-04-03 23:47:27,478] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1591
[2019-04-03 23:47:27,625] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-12.46666666666667, 68.0, 0.0, 0.0, 23.0, 20.80226797786232, -0.6395093357414586, 1.0, 1.0, 202240.7622548749], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 285600.0000, 
sim time next is 286200.0000, 
raw observation next is [-12.55, 68.5, 0.0, 0.0, 23.0, 21.12676368423084, -0.5497968917743248, 1.0, 1.0, 203351.7346567631], 
processed observation next is [1.0, 0.30434782608695654, 0.11495844875346259, 0.685, 0.0, 0.0, 0.4166666666666667, 0.2605636403525701, 0.3167343694085584, 1.0, 1.0, 0.9683415936036337], 
reward next is 0.0000, 
noisyNet noise sample is [array([-2.182795], dtype=float32), 1.1923171]. 
=============================================
[2019-04-03 23:47:28,682] A3C_AGENT_WORKER-Thread-20 INFO:Local step 85500, global step 1372490: loss 0.7259
[2019-04-03 23:47:28,682] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 85500, global step 1372490: learning rate 0.0005
[2019-04-03 23:47:28,692] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.3499584e-26 1.3316895e-17 4.2340060e-17 1.0000000e+00 1.1603227e-20
 1.9378577e-11 2.4815785e-20], sum to 1.0000
[2019-04-03 23:47:28,692] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4061
[2019-04-03 23:47:28,740] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.1, 27.0, 118.0, 0.0, 25.0, 24.35213979076879, -0.06639404178767727, 1.0, 1.0, 18697.6937756574], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 472800.0000, 
sim time next is 473400.0000, 
raw observation next is [-2.0, 26.5, 122.0, 0.0, 25.0, 24.26258396367654, -0.06708513052940264, 1.0, 1.0, 18696.84030280436], 
processed observation next is [1.0, 0.4782608695652174, 0.40720221606648205, 0.265, 0.4066666666666667, 0.0, 0.5833333333333334, 0.521881996973045, 0.47763828982353246, 1.0, 1.0, 0.08903257287049696], 
reward next is 0.9110, 
noisyNet noise sample is [array([-1.9078335], dtype=float32), -0.5113003]. 
=============================================
[2019-04-03 23:47:29,997] A3C_AGENT_WORKER-Thread-2 INFO:Local step 86500, global step 1372872: loss 0.0644
[2019-04-03 23:47:29,997] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 86500, global step 1372872: learning rate 0.0005
[2019-04-03 23:47:30,263] A3C_AGENT_WORKER-Thread-18 INFO:Local step 86000, global step 1372941: loss 0.1544
[2019-04-03 23:47:30,264] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 86000, global step 1372941: learning rate 0.0005
[2019-04-03 23:47:30,425] A3C_AGENT_WORKER-Thread-3 INFO:Local step 86000, global step 1372987: loss 0.1928
[2019-04-03 23:47:30,426] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 86000, global step 1372987: learning rate 0.0005
[2019-04-03 23:47:32,063] A3C_AGENT_WORKER-Thread-10 INFO:Local step 85500, global step 1373439: loss 1.0969
[2019-04-03 23:47:32,063] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 85500, global step 1373439: learning rate 0.0005
[2019-04-03 23:47:32,104] A3C_AGENT_WORKER-Thread-15 INFO:Local step 85500, global step 1373448: loss 1.2700
[2019-04-03 23:47:32,109] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 85500, global step 1373448: learning rate 0.0005
[2019-04-03 23:47:33,624] A3C_AGENT_WORKER-Thread-17 INFO:Local step 86000, global step 1374000: loss 0.0241
[2019-04-03 23:47:33,633] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 86000, global step 1374002: learning rate 0.0005
[2019-04-03 23:47:34,756] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4150695e-24 3.5346982e-20 3.2905235e-17 1.0000000e+00 1.1415114e-18
 8.6209138e-13 9.4119389e-20], sum to 1.0000
[2019-04-03 23:47:34,757] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7520
[2019-04-03 23:47:34,843] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.383333333333333, 57.83333333333333, 0.0, 0.0, 24.0, 22.97492763896165, -0.2316573463195495, 0.0, 1.0, 49638.04513399176], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 670200.0000, 
sim time next is 670800.0000, 
raw observation next is [-1.566666666666667, 58.66666666666667, 0.0, 0.0, 24.0, 22.98210948489093, -0.2301280344905396, 0.0, 1.0, 44579.58696193579], 
processed observation next is [0.0, 0.782608695652174, 0.4192059095106187, 0.5866666666666667, 0.0, 0.0, 0.5, 0.4151757904075775, 0.4232906551698201, 0.0, 1.0, 0.2122837474377895], 
reward next is 0.7877, 
noisyNet noise sample is [array([0.22781664], dtype=float32), -0.18489742]. 
=============================================
[2019-04-03 23:47:35,334] A3C_AGENT_WORKER-Thread-19 INFO:Local step 86500, global step 1374581: loss 0.0735
[2019-04-03 23:47:35,343] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 86500, global step 1374583: learning rate 0.0005
[2019-04-03 23:47:40,003] A3C_AGENT_WORKER-Thread-5 INFO:Local step 86000, global step 1376157: loss 0.1559
[2019-04-03 23:47:40,005] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 86000, global step 1376158: learning rate 0.0005
[2019-04-03 23:47:40,399] A3C_AGENT_WORKER-Thread-6 INFO:Local step 86000, global step 1376305: loss 0.0585
[2019-04-03 23:47:40,400] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 86000, global step 1376305: learning rate 0.0005
[2019-04-03 23:47:40,805] A3C_AGENT_WORKER-Thread-12 INFO:Local step 86000, global step 1376456: loss 0.0131
[2019-04-03 23:47:40,806] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 86000, global step 1376456: learning rate 0.0005
[2019-04-03 23:47:41,128] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.6352187e-25 2.6480622e-21 2.4328217e-18 1.0000000e+00 7.3860501e-20
 3.3982349e-13 1.5244186e-20], sum to 1.0000
[2019-04-03 23:47:41,128] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2598
[2019-04-03 23:47:41,168] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 69.16666666666667, 0.0, 0.0, 25.0, 23.25723638393224, -0.1308342456969059, 0.0, 1.0, 44861.16165887333], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 622200.0000, 
sim time next is 622800.0000, 
raw observation next is [-4.5, 68.0, 0.0, 0.0, 25.0, 23.23115273094976, -0.1359528443653051, 0.0, 1.0, 44813.26800279077], 
processed observation next is [0.0, 0.21739130434782608, 0.3379501385041552, 0.68, 0.0, 0.0, 0.5833333333333334, 0.4359293942458133, 0.45468238521156495, 0.0, 1.0, 0.21339651429900366], 
reward next is 0.7866, 
noisyNet noise sample is [array([-0.980554], dtype=float32), -0.22749273]. 
=============================================
[2019-04-03 23:47:41,217] A3C_AGENT_WORKER-Thread-13 INFO:Local step 86000, global step 1376590: loss 0.0011
[2019-04-03 23:47:41,219] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 86000, global step 1376590: learning rate 0.0005
[2019-04-03 23:47:43,689] A3C_AGENT_WORKER-Thread-16 INFO:Local step 86000, global step 1377577: loss 0.1247
[2019-04-03 23:47:43,689] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 86000, global step 1377577: learning rate 0.0005
[2019-04-03 23:47:43,927] A3C_AGENT_WORKER-Thread-2 INFO:Local step 87000, global step 1377657: loss 3.0095
[2019-04-03 23:47:43,928] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 87000, global step 1377657: learning rate 0.0005
[2019-04-03 23:47:44,970] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.3980712e-24 7.2532933e-17 2.0700827e-15 9.9999988e-01 3.7025692e-18
 7.2352677e-08 1.6324711e-19], sum to 1.0000
[2019-04-03 23:47:44,970] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8706
[2019-04-03 23:47:45,012] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.633333333333334, 73.66666666666667, 82.66666666666666, 0.0, 24.0, 24.1649859637815, -0.07266516653233883, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 814800.0000, 
sim time next is 815400.0000, 
raw observation next is [-5.35, 73.0, 87.0, 0.0, 24.0, 24.13371967800822, -0.0822701738446206, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.31440443213296404, 0.73, 0.29, 0.0, 0.5, 0.511143306500685, 0.4725766087184598, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.42250636], dtype=float32), 0.4218976]. 
=============================================
[2019-04-03 23:47:48,855] A3C_AGENT_WORKER-Thread-4 INFO:Local step 86000, global step 1379446: loss 0.0189
[2019-04-03 23:47:48,858] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 86000, global step 1379447: learning rate 0.0005
[2019-04-03 23:47:49,254] A3C_AGENT_WORKER-Thread-19 INFO:Local step 87000, global step 1379627: loss 2.2526
[2019-04-03 23:47:49,255] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 87000, global step 1379627: learning rate 0.0005
[2019-04-03 23:47:49,813] A3C_AGENT_WORKER-Thread-11 INFO:Local step 86000, global step 1379885: loss 0.0004
[2019-04-03 23:47:49,823] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 86000, global step 1379885: learning rate 0.0005
[2019-04-03 23:47:52,287] A3C_AGENT_WORKER-Thread-14 INFO:Local step 86000, global step 1380741: loss 0.0590
[2019-04-03 23:47:52,288] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 86000, global step 1380741: learning rate 0.0005
[2019-04-03 23:47:52,736] A3C_AGENT_WORKER-Thread-20 INFO:Local step 86000, global step 1380929: loss 0.0049
[2019-04-03 23:47:52,736] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 86000, global step 1380929: learning rate 0.0005
[2019-04-03 23:47:52,905] A3C_AGENT_WORKER-Thread-3 INFO:Local step 86500, global step 1381007: loss 0.2874
[2019-04-03 23:47:52,905] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 86500, global step 1381007: learning rate 0.0005
[2019-04-03 23:47:53,101] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.2358003e-24 5.3950808e-19 5.7429930e-18 1.0000000e+00 4.8696634e-19
 7.5823279e-14 6.4496287e-20], sum to 1.0000
[2019-04-03 23:47:53,118] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6728
[2019-04-03 23:47:53,149] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.2, 82.0, 122.5, 401.3333333333334, 24.0, 23.15077219921674, -0.0884535693735255, 0.0, 1.0, 18704.35591212654], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 571200.0000, 
sim time next is 571800.0000, 
raw observation next is [-1.2, 82.5, 118.0, 335.6666666666667, 24.0, 23.14744579384954, -0.09754028035931123, 0.0, 1.0, 18703.57499275966], 
processed observation next is [0.0, 0.6086956521739131, 0.42936288088642666, 0.825, 0.3933333333333333, 0.370902394106814, 0.5, 0.4289538161541282, 0.4674865732135629, 0.0, 1.0, 0.08906464282266505], 
reward next is 0.9109, 
noisyNet noise sample is [array([0.84674203], dtype=float32), 1.4501812]. 
=============================================
[2019-04-03 23:47:54,136] A3C_AGENT_WORKER-Thread-18 INFO:Local step 86500, global step 1381441: loss 0.1969
[2019-04-03 23:47:54,138] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 86500, global step 1381442: learning rate 0.0005
[2019-04-03 23:47:54,292] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.7281186e-28 1.3586877e-21 9.6415339e-20 1.0000000e+00 4.1538606e-22
 4.2725419e-16 9.7465406e-23], sum to 1.0000
[2019-04-03 23:47:54,292] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4098
[2019-04-03 23:47:54,324] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.8, 65.0, 0.0, 0.0, 26.0, 24.96657013363106, 0.2180040587443489, 0.0, 1.0, 44472.8490338963], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 676800.0000, 
sim time next is 677400.0000, 
raw observation next is [-2.9, 65.66666666666667, 0.0, 0.0, 26.0, 24.96452355660902, 0.2245647782851536, 0.0, 1.0, 43617.25749553944], 
processed observation next is [0.0, 0.8695652173913043, 0.38227146814404434, 0.6566666666666667, 0.0, 0.0, 0.6666666666666666, 0.5803769630507517, 0.5748549260950512, 0.0, 1.0, 0.20770122616923542], 
reward next is 0.7923, 
noisyNet noise sample is [array([-1.738232], dtype=float32), 0.28759623]. 
=============================================
[2019-04-03 23:47:56,649] A3C_AGENT_WORKER-Thread-15 INFO:Local step 86000, global step 1382343: loss 0.0033
[2019-04-03 23:47:56,663] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 86000, global step 1382344: learning rate 0.0005
[2019-04-03 23:47:56,846] A3C_AGENT_WORKER-Thread-10 INFO:Local step 86000, global step 1382422: loss 0.0078
[2019-04-03 23:47:56,860] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 86000, global step 1382422: learning rate 0.0005
[2019-04-03 23:47:57,095] A3C_AGENT_WORKER-Thread-17 INFO:Local step 86500, global step 1382507: loss 0.1897
[2019-04-03 23:47:57,096] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 86500, global step 1382508: learning rate 0.0005
[2019-04-03 23:48:00,525] A3C_AGENT_WORKER-Thread-2 INFO:Local step 87500, global step 1383746: loss 0.0537
[2019-04-03 23:48:00,528] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 87500, global step 1383749: learning rate 0.0005
[2019-04-03 23:48:02,598] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.5503499e-25 2.7598368e-14 6.7643257e-16 1.0000000e+00 8.9523896e-24
 1.1374240e-13 5.0300729e-19], sum to 1.0000
[2019-04-03 23:48:02,599] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7950
[2019-04-03 23:48:02,609] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [19.3, 49.83333333333334, 124.0, 0.0, 24.0, 26.35736192209675, 0.6502421186667133, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1090200.0000, 
sim time next is 1090800.0000, 
raw observation next is [19.4, 49.0, 116.0, 0.0, 24.0, 26.40678342341261, 0.6490958832994852, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 1.0, 0.49, 0.38666666666666666, 0.0, 0.5, 0.7005652852843841, 0.7163652944331617, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3690308], dtype=float32), 1.881094]. 
=============================================
[2019-04-03 23:48:02,814] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.2496582e-25 3.9953181e-17 5.7243418e-16 1.0000000e+00 1.1085158e-18
 3.9149199e-09 8.1622476e-19], sum to 1.0000
[2019-04-03 23:48:02,814] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0414
[2019-04-03 23:48:02,841] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.366666666666667, 86.16666666666666, 90.33333333333333, 0.0, 24.0, 24.02834650151716, -0.05337516589942134, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 904200.0000, 
sim time next is 904800.0000, 
raw observation next is [1.633333333333334, 88.33333333333334, 93.66666666666667, 0.0, 24.0, 24.00493569222048, -0.0508665226943612, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.5078485687903971, 0.8833333333333334, 0.31222222222222223, 0.0, 0.5, 0.5004113076850402, 0.4830444924352129, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.37406608], dtype=float32), -0.1083426]. 
=============================================
[2019-04-03 23:48:02,999] A3C_AGENT_WORKER-Thread-6 INFO:Local step 86500, global step 1384903: loss 0.3545
[2019-04-03 23:48:03,003] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 86500, global step 1384903: learning rate 0.0005
[2019-04-03 23:48:03,650] A3C_AGENT_WORKER-Thread-5 INFO:Local step 86500, global step 1385203: loss 0.0520
[2019-04-03 23:48:03,652] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 86500, global step 1385205: learning rate 0.0005
[2019-04-03 23:48:03,716] A3C_AGENT_WORKER-Thread-12 INFO:Local step 86500, global step 1385236: loss 0.1719
[2019-04-03 23:48:03,724] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 86500, global step 1385236: learning rate 0.0005
[2019-04-03 23:48:04,456] A3C_AGENT_WORKER-Thread-13 INFO:Local step 86500, global step 1385555: loss 0.2812
[2019-04-03 23:48:04,457] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 86500, global step 1385555: learning rate 0.0005
[2019-04-03 23:48:04,538] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.9138813e-27 6.2066152e-21 4.2689675e-19 1.0000000e+00 1.6425140e-21
 6.6502528e-12 5.2068528e-21], sum to 1.0000
[2019-04-03 23:48:04,541] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4496
[2019-04-03 23:48:04,558] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.0, 96.0, 0.0, 0.0, 25.0, 24.39121360290167, 0.2054171447698104, 0.0, 1.0, 33981.23463568062], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 948600.0000, 
sim time next is 949200.0000, 
raw observation next is [5.0, 96.0, 0.0, 0.0, 25.0, 24.40958464827273, 0.210352153757714, 0.0, 1.0, 25720.01680453691], 
processed observation next is [1.0, 1.0, 0.6011080332409973, 0.96, 0.0, 0.0, 0.5833333333333334, 0.5341320540227276, 0.5701173845859047, 0.0, 1.0, 0.1224762704977948], 
reward next is 0.8775, 
noisyNet noise sample is [array([-1.5743878], dtype=float32), -0.5420094]. 
=============================================
[2019-04-03 23:48:04,572] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.7850870e-26 3.2845522e-18 2.9306085e-16 1.0000000e+00 5.8815107e-20
 2.5821586e-11 3.5795443e-20], sum to 1.0000
[2019-04-03 23:48:04,574] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1740
[2019-04-03 23:48:04,611] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.9, 92.66666666666667, 0.0, 0.0, 25.0, 24.73822608077396, 0.1785134811477616, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 925800.0000, 
sim time next is 926400.0000, 
raw observation next is [4.800000000000001, 93.33333333333334, 0.0, 0.0, 25.0, 24.79910174805816, 0.149546611544557, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.5955678670360112, 0.9333333333333335, 0.0, 0.0, 0.5833333333333334, 0.56659181233818, 0.5498488705148523, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.18992318], dtype=float32), -1.6299537]. 
=============================================
[2019-04-03 23:48:04,873] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0927423e-25 1.8659292e-17 6.2808467e-16 1.0000000e+00 3.5689402e-18
 1.3082718e-08 1.8011589e-19], sum to 1.0000
[2019-04-03 23:48:04,873] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5411
[2019-04-03 23:48:04,905] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.0, 100.0, 0.0, 0.0, 24.0, 22.94868889006186, -0.06516767303992381, 0.0, 1.0, 83151.11583022335], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 938400.0000, 
sim time next is 939000.0000, 
raw observation next is [5.0, 100.0, 0.0, 0.0, 24.0, 23.15856827699321, -0.04079039568104396, 0.0, 1.0, 55373.32537226928], 
processed observation next is [1.0, 0.8695652173913043, 0.6011080332409973, 1.0, 0.0, 0.0, 0.5, 0.4298806897494343, 0.486403201439652, 0.0, 1.0, 0.26368250177271085], 
reward next is 0.7363, 
noisyNet noise sample is [array([0.50222325], dtype=float32), -0.6812531]. 
=============================================
[2019-04-03 23:48:04,918] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[94.26125]
 [94.11208]
 [93.97648]
 [94.55726]
 [94.76086]], R is [[93.94120026]
 [93.60583496]
 [92.93636322]
 [92.05882263]
 [91.99250031]].
[2019-04-03 23:48:06,423] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1956659e-30 4.7926938e-23 2.9721104e-21 1.0000000e+00 2.7801302e-25
 1.3492401e-15 5.7192984e-24], sum to 1.0000
[2019-04-03 23:48:06,423] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8675
[2019-04-03 23:48:06,437] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.166666666666667, 93.66666666666666, 0.0, 0.0, 25.0, 24.4095553195681, 0.2165049369651614, 0.0, 1.0, 45726.61125170478], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 951600.0000, 
sim time next is 952200.0000, 
raw observation next is [5.25, 92.5, 0.0, 0.0, 25.0, 24.47646591686877, 0.2152356876144519, 0.0, 1.0, 18757.44413950766], 
processed observation next is [1.0, 0.0, 0.60803324099723, 0.925, 0.0, 0.0, 0.5833333333333334, 0.5397054930723973, 0.5717452292048173, 0.0, 1.0, 0.0893211625690841], 
reward next is 0.9107, 
noisyNet noise sample is [array([0.64961404], dtype=float32), 0.18431905]. 
=============================================
[2019-04-03 23:48:06,440] A3C_AGENT_WORKER-Thread-19 INFO:Local step 87500, global step 1386513: loss 0.0528
[2019-04-03 23:48:06,440] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 87500, global step 1386515: learning rate 0.0005
[2019-04-03 23:48:06,970] A3C_AGENT_WORKER-Thread-16 INFO:Local step 86500, global step 1386758: loss 3.0055
[2019-04-03 23:48:06,971] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 86500, global step 1386760: learning rate 0.0005
[2019-04-03 23:48:07,085] A3C_AGENT_WORKER-Thread-3 INFO:Local step 87000, global step 1386823: loss 1.4942
[2019-04-03 23:48:07,097] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 87000, global step 1386824: learning rate 0.0005
[2019-04-03 23:48:07,543] A3C_AGENT_WORKER-Thread-18 INFO:Local step 87000, global step 1387054: loss 1.4107
[2019-04-03 23:48:07,544] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 87000, global step 1387055: learning rate 0.0005
[2019-04-03 23:48:10,516] A3C_AGENT_WORKER-Thread-17 INFO:Local step 87000, global step 1388550: loss 1.1387
[2019-04-03 23:48:10,522] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 87000, global step 1388552: learning rate 0.0005
[2019-04-03 23:48:11,033] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.2781497e-25 1.9842256e-18 3.1108565e-17 1.0000000e+00 1.0416318e-19
 3.6329221e-11 7.2839182e-21], sum to 1.0000
[2019-04-03 23:48:11,033] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5227
[2019-04-03 23:48:11,047] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.983333333333333, 72.66666666666667, 0.0, 0.0, 24.0, 23.94930007275436, 0.1738636955836599, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1626600.0000, 
sim time next is 1627200.0000, 
raw observation next is [7.7, 74.0, 0.0, 0.0, 24.0, 23.88244845506122, 0.1588882349247061, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.6759002770083103, 0.74, 0.0, 0.0, 0.5, 0.49020403792176825, 0.552962744974902, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.95874786], dtype=float32), -0.55200005]. 
=============================================
[2019-04-03 23:48:11,855] A3C_AGENT_WORKER-Thread-11 INFO:Local step 86500, global step 1389268: loss 0.1253
[2019-04-03 23:48:11,856] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 86500, global step 1389268: learning rate 0.0005
[2019-04-03 23:48:12,170] A3C_AGENT_WORKER-Thread-4 INFO:Local step 86500, global step 1389433: loss 2.7407
[2019-04-03 23:48:12,172] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 86500, global step 1389433: learning rate 0.0005
[2019-04-03 23:48:13,268] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.4689323e-26 2.5109096e-18 3.4183433e-16 1.0000000e+00 4.4869999e-19
 3.4175190e-10 3.0238143e-20], sum to 1.0000
[2019-04-03 23:48:13,268] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8312
[2019-04-03 23:48:13,297] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 24.0, 23.38699469053176, 0.009233557745293072, 1.0, 1.0, 6226.803890076846], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1358400.0000, 
sim time next is 1359000.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 24.0, 23.44362414450136, -0.006893938476400435, 1.0, 1.0, 9340.205835115268], 
processed observation next is [1.0, 0.7391304347826086, 0.4764542936288089, 0.96, 0.0, 0.0, 0.5, 0.45363534537511335, 0.49770202050786655, 1.0, 1.0, 0.04447717064340604], 
reward next is 0.9555, 
noisyNet noise sample is [array([-1.1773487], dtype=float32), -0.9687716]. 
=============================================
[2019-04-03 23:48:13,311] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[94.64545 ]
 [94.4514  ]
 [94.50597 ]
 [95.004166]
 [95.0871  ]], R is [[94.37710571]
 [94.40368652]
 [93.5371933 ]
 [92.75105286]
 [92.80377197]].
[2019-04-03 23:48:14,031] A3C_AGENT_WORKER-Thread-20 INFO:Local step 86500, global step 1390347: loss 0.7103
[2019-04-03 23:48:14,045] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 86500, global step 1390347: learning rate 0.0005
[2019-04-03 23:48:15,263] A3C_AGENT_WORKER-Thread-14 INFO:Local step 86500, global step 1391017: loss 2.1653
[2019-04-03 23:48:15,265] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 86500, global step 1391018: learning rate 0.0005
[2019-04-03 23:48:16,028] A3C_AGENT_WORKER-Thread-6 INFO:Local step 87000, global step 1391477: loss 0.4065
[2019-04-03 23:48:16,034] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 87000, global step 1391477: learning rate 0.0005
[2019-04-03 23:48:16,354] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.5461465e-27 8.7095825e-17 1.5270748e-17 1.0000000e+00 8.0985389e-20
 1.2668552e-08 1.4991595e-20], sum to 1.0000
[2019-04-03 23:48:16,356] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7269
[2019-04-03 23:48:16,364] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.8, 93.0, 93.0, 0.0, 23.0, 22.60116688644425, -0.3040619837752809, 1.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 914400.0000, 
sim time next is 915000.0000, 
raw observation next is [3.9, 93.0, 92.0, 0.0, 23.0, 22.82398497632127, -0.2803529432575762, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5706371191135734, 0.93, 0.30666666666666664, 0.0, 0.4166666666666667, 0.40199874802677255, 0.40654901891414125, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2418883], dtype=float32), -0.2778688]. 
=============================================
[2019-04-03 23:48:16,371] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[99.907486]
 [99.85323 ]
 [99.6981  ]
 [99.665276]
 [99.618416]], R is [[99.97159576]
 [99.88292694]
 [99.79472351]
 [99.67785645]
 [99.68107605]].
[2019-04-03 23:48:16,689] A3C_AGENT_WORKER-Thread-12 INFO:Local step 87000, global step 1391883: loss 0.4647
[2019-04-03 23:48:16,695] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 87000, global step 1391883: learning rate 0.0005
[2019-04-03 23:48:16,748] A3C_AGENT_WORKER-Thread-5 INFO:Local step 87000, global step 1391914: loss 0.0538
[2019-04-03 23:48:16,755] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 87000, global step 1391914: learning rate 0.0005
[2019-04-03 23:48:16,766] A3C_AGENT_WORKER-Thread-15 INFO:Local step 86500, global step 1391922: loss 0.8042
[2019-04-03 23:48:16,774] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 86500, global step 1391922: learning rate 0.0005
[2019-04-03 23:48:17,033] A3C_AGENT_WORKER-Thread-2 INFO:Local step 88000, global step 1392075: loss 0.1287
[2019-04-03 23:48:17,035] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 88000, global step 1392076: learning rate 0.0005
[2019-04-03 23:48:17,617] A3C_AGENT_WORKER-Thread-13 INFO:Local step 87000, global step 1392415: loss 0.1199
[2019-04-03 23:48:17,617] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 87000, global step 1392415: learning rate 0.0005
[2019-04-03 23:48:18,252] A3C_AGENT_WORKER-Thread-10 INFO:Local step 86500, global step 1392753: loss 0.1805
[2019-04-03 23:48:18,252] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 86500, global step 1392753: learning rate 0.0005
[2019-04-03 23:48:20,253] A3C_AGENT_WORKER-Thread-16 INFO:Local step 87000, global step 1393836: loss 0.1538
[2019-04-03 23:48:20,253] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 87000, global step 1393836: learning rate 0.0005
[2019-04-03 23:48:22,667] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2195271e-23 9.6318755e-18 3.5248625e-16 1.0000000e+00 8.8079085e-18
 2.9061589e-10 6.5420339e-19], sum to 1.0000
[2019-04-03 23:48:22,667] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9186
[2019-04-03 23:48:22,679] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.35, 90.5, 0.0, 0.0, 24.0, 23.81623982445437, 0.1362919341731751, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1459800.0000, 
sim time next is 1460400.0000, 
raw observation next is [1.266666666666667, 91.0, 0.0, 0.0, 24.0, 23.82749507597963, 0.1294720884169459, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.4976915974145891, 0.91, 0.0, 0.0, 0.5, 0.4856245896649692, 0.5431573628056486, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4946926], dtype=float32), 0.7815946]. 
=============================================
[2019-04-03 23:48:22,700] A3C_AGENT_WORKER-Thread-3 INFO:Local step 87500, global step 1395132: loss 0.1338
[2019-04-03 23:48:22,707] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 87500, global step 1395132: learning rate 0.0005
[2019-04-03 23:48:23,009] A3C_AGENT_WORKER-Thread-19 INFO:Local step 88000, global step 1395296: loss 1.1185
[2019-04-03 23:48:23,016] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 88000, global step 1395298: learning rate 0.0005
[2019-04-03 23:48:23,169] A3C_AGENT_WORKER-Thread-18 INFO:Local step 87500, global step 1395389: loss 0.1210
[2019-04-03 23:48:23,170] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 87500, global step 1395390: learning rate 0.0005
[2019-04-03 23:48:24,271] A3C_AGENT_WORKER-Thread-11 INFO:Local step 87000, global step 1396071: loss 0.3740
[2019-04-03 23:48:24,273] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 87000, global step 1396072: learning rate 0.0005
[2019-04-03 23:48:25,581] A3C_AGENT_WORKER-Thread-4 INFO:Local step 87000, global step 1396795: loss 0.0005
[2019-04-03 23:48:25,586] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 87000, global step 1396796: learning rate 0.0005
[2019-04-03 23:48:25,747] A3C_AGENT_WORKER-Thread-20 INFO:Local step 87000, global step 1396891: loss 0.3075
[2019-04-03 23:48:25,748] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 87000, global step 1396891: learning rate 0.0005
[2019-04-03 23:48:26,222] A3C_AGENT_WORKER-Thread-17 INFO:Local step 87500, global step 1397174: loss 0.1283
[2019-04-03 23:48:26,223] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 87500, global step 1397174: learning rate 0.0005
[2019-04-03 23:48:26,474] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.5689605e-29 3.4748136e-22 6.8136470e-20 1.0000000e+00 2.0417628e-23
 2.1795896e-15 1.0559892e-22], sum to 1.0000
[2019-04-03 23:48:26,476] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7107
[2019-04-03 23:48:26,489] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 25.0, 24.42774269181675, 0.2420553583673788, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 1408800.0000, 
sim time next is 1409400.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 25.0, 24.4774026662995, 0.2364445604959613, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.44598337950138506, 1.0, 0.0, 0.0, 0.5833333333333334, 0.5397835555249584, 0.5788148534986538, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.67233974], dtype=float32), -1.3733294]. 
=============================================
[2019-04-03 23:48:27,035] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.6035492e-36 1.2320791e-28 6.0500011e-26 1.0000000e+00 8.0845769e-30
 4.6510916e-22 8.4398265e-29], sum to 1.0000
[2019-04-03 23:48:27,038] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5996
[2019-04-03 23:48:27,061] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [14.1, 98.0, 101.0, 0.0, 26.0, 25.03266206733034, 0.482319222479807, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1254600.0000, 
sim time next is 1255200.0000, 
raw observation next is [14.0, 98.66666666666666, 100.0, 0.0, 26.0, 25.00168925554586, 0.4785348004699688, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.8504155124653741, 0.9866666666666666, 0.3333333333333333, 0.0, 0.6666666666666666, 0.5834741046288215, 0.6595116001566562, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.126528], dtype=float32), 0.36769855]. 
=============================================
[2019-04-03 23:48:28,093] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0131358e-22 3.4585963e-17 1.6009463e-14 1.0000000e+00 1.4464737e-16
 1.2810149e-10 1.3775140e-18], sum to 1.0000
[2019-04-03 23:48:28,094] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3570
[2019-04-03 23:48:28,149] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 91.33333333333334, 0.0, 0.0, 24.0, 23.93918041820347, 0.1038200470994307, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1450200.0000, 
sim time next is 1450800.0000, 
raw observation next is [1.1, 92.0, 0.0, 0.0, 24.0, 23.88457000222984, 0.08929093409600743, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.49307479224376743, 0.92, 0.0, 0.0, 0.5, 0.4903808335191533, 0.5297636446986692, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7488184], dtype=float32), -0.553574]. 
=============================================
[2019-04-03 23:48:28,366] A3C_AGENT_WORKER-Thread-14 INFO:Local step 87000, global step 1398310: loss 0.0087
[2019-04-03 23:48:28,366] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 87000, global step 1398310: learning rate 0.0005
[2019-04-03 23:48:28,618] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.2812280e-28 4.4938174e-19 4.0695166e-19 1.0000000e+00 7.4668661e-22
 1.8230426e-13 6.5632989e-21], sum to 1.0000
[2019-04-03 23:48:28,620] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6694
[2019-04-03 23:48:28,663] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.8, 92.0, 18.0, 0.0, 24.0, 23.71461398552028, 0.1287443970025652, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1326600.0000, 
sim time next is 1327200.0000, 
raw observation next is [0.7000000000000001, 92.0, 22.5, 0.0, 24.0, 24.0362791111799, 0.148688227843093, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.4819944598337951, 0.92, 0.075, 0.0, 0.5, 0.5030232592649918, 0.5495627426143643, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.29583722], dtype=float32), -0.0025735225]. 
=============================================
[2019-04-03 23:48:28,925] A3C_AGENT_WORKER-Thread-15 INFO:Local step 87000, global step 1398651: loss 0.1830
[2019-04-03 23:48:28,928] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 87000, global step 1398651: learning rate 0.0005
[2019-04-03 23:48:29,772] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.9419012e-28 8.7187397e-21 6.3530077e-19 1.0000000e+00 9.9034310e-22
 1.3731734e-13 8.9401129e-22], sum to 1.0000
[2019-04-03 23:48:29,772] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7277
[2019-04-03 23:48:29,792] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.2, 96.0, 0.0, 0.0, 24.0, 23.49488240729164, 0.008423637913205861, 0.0, 1.0, 42322.80074399371], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1484400.0000, 
sim time next is 1485000.0000, 
raw observation next is [2.2, 96.0, 0.0, 0.0, 24.0, 23.470693482372, 0.006754973012913969, 0.0, 1.0, 48000.71112178118], 
processed observation next is [1.0, 0.17391304347826086, 0.5235457063711911, 0.96, 0.0, 0.0, 0.5, 0.4558911235309999, 0.5022516576709714, 0.0, 1.0, 0.22857481486562467], 
reward next is 0.7714, 
noisyNet noise sample is [array([-0.23841754], dtype=float32), 0.5501544]. 
=============================================
[2019-04-03 23:48:29,816] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[92.60352]
 [92.47202]
 [92.59452]
 [92.64948]
 [92.57504]], R is [[92.41775513]
 [92.29203796]
 [92.06365967]
 [92.01979065]
 [92.09959412]].
[2019-04-03 23:48:30,598] A3C_AGENT_WORKER-Thread-10 INFO:Local step 87000, global step 1399561: loss 0.1690
[2019-04-03 23:48:30,600] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 87000, global step 1399561: learning rate 0.0005
[2019-04-03 23:48:31,582] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-03 23:48:31,582] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-03 23:48:31,583] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:48:31,584] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-03 23:48:31,584] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:48:31,585] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-03 23:48:31,587] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:48:31,595] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run15
[2019-04-03 23:48:31,612] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run15
[2019-04-03 23:48:31,625] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run15
[2019-04-03 23:49:00,245] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.29355326], dtype=float32), 0.6614758]
[2019-04-03 23:49:00,245] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [10.86666666666667, 90.66666666666667, 90.0, 0.0, 21.0, 21.94900347357419, -0.471400403688755, 1.0, 1.0, 0.0]
[2019-04-03 23:49:00,245] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-03 23:49:00,246] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.2313305e-23 4.6827369e-18 1.6901308e-17 1.0000000e+00 2.3977944e-16
 2.5009494e-10 1.1789375e-19], sampled 0.5831710974073965
[2019-04-03 23:49:47,182] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.29355326], dtype=float32), 0.6614758]
[2019-04-03 23:49:47,183] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-3.0, 92.0, 98.33333333333334, 605.8333333333334, 21.0, 21.75391523233161, -0.4049673928457971, 1.0, 1.0, 0.0]
[2019-04-03 23:49:47,183] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-03 23:49:47,184] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.90627445e-17 5.52605345e-14 1.25276547e-12 9.99998808e-01
 1.22803885e-11 1.13405395e-06 1.74061188e-14], sampled 0.9999629211757964
[2019-04-03 23:49:59,019] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.29355326], dtype=float32), 0.6614758]
[2019-04-03 23:49:59,019] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-3.0, 68.0, 0.0, 0.0, 21.0, 20.8270983596928, -0.7229393833018193, 0.0, 1.0, 0.0]
[2019-04-03 23:49:59,019] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-03 23:49:59,020] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [4.9424480e-18 1.4330351e-15 3.2975039e-13 1.0000000e+00 2.1498244e-12
 2.4923033e-09 1.2424051e-14], sampled 0.610021732596796
[2019-04-03 23:50:14,104] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7190.6028 180633822.8574 -752.1305
[2019-04-03 23:50:28,291] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 6005.6443 197007404.5769 -1628.4245
[2019-04-03 23:50:41,542] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.29355326], dtype=float32), 0.6614758]
[2019-04-03 23:50:41,542] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [3.5543612355, 38.29743744166667, 265.9548260333333, 366.2352808666667, 22.0, 21.44979848355823, -0.4663621548672418, 0.0, 1.0, 0.0]
[2019-04-03 23:50:41,542] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-03 23:50:41,542] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.2359065e-20 1.8544206e-16 3.4594575e-15 1.0000000e+00 3.7399376e-15
 1.4964019e-10 3.2182739e-17], sampled 0.7617280840940174
[2019-04-03 23:50:44,957] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 6953.3984 223828880.8250 -1076.4263
[2019-04-03 23:50:45,989] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 1400000, evaluation results [1400000.0, 6005.644280143232, 197007404.57687482, -1628.424523985467, 7190.602839714756, 180633822.8574291, -752.1304803518439, 6953.398421453032, 223828880.82502633, -1076.4263339549093]
[2019-04-03 23:50:46,651] A3C_AGENT_WORKER-Thread-6 INFO:Local step 87500, global step 1400224: loss 0.0216
[2019-04-03 23:50:46,651] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 87500, global step 1400224: learning rate 0.0005
[2019-04-03 23:50:46,984] A3C_AGENT_WORKER-Thread-12 INFO:Local step 87500, global step 1400323: loss 0.1821
[2019-04-03 23:50:46,987] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 87500, global step 1400323: learning rate 0.0005
[2019-04-03 23:50:48,195] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.4106336e-22 5.8039107e-18 1.1878783e-15 1.0000000e+00 2.9420882e-17
 2.4086563e-10 9.2804578e-19], sum to 1.0000
[2019-04-03 23:50:48,196] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8126
[2019-04-03 23:50:48,202] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 88.0, 83.0, 0.0, 24.0, 23.99567942474825, 0.0726034689288713, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1683000.0000, 
sim time next is 1683600.0000, 
raw observation next is [1.1, 86.66666666666667, 87.0, 0.0, 24.0, 23.99224327418358, 0.0709737788789034, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.49307479224376743, 0.8666666666666667, 0.29, 0.0, 0.5, 0.49935360618196495, 0.5236579262929678, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.777837], dtype=float32), -0.60657567]. 
=============================================
[2019-04-03 23:50:48,969] A3C_AGENT_WORKER-Thread-13 INFO:Local step 87500, global step 1400918: loss 0.1264
[2019-04-03 23:50:48,971] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 87500, global step 1400918: learning rate 0.0005
[2019-04-03 23:50:49,344] A3C_AGENT_WORKER-Thread-5 INFO:Local step 87500, global step 1401030: loss 0.0174
[2019-04-03 23:50:49,345] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 87500, global step 1401030: learning rate 0.0005
[2019-04-03 23:50:54,482] A3C_AGENT_WORKER-Thread-2 INFO:Local step 88500, global step 1402735: loss 0.1056
[2019-04-03 23:50:54,513] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 88500, global step 1402744: learning rate 0.0005
[2019-04-03 23:50:54,681] A3C_AGENT_WORKER-Thread-16 INFO:Local step 87500, global step 1402798: loss 0.0510
[2019-04-03 23:50:54,683] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 87500, global step 1402798: learning rate 0.0005
[2019-04-03 23:50:55,434] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.68612256e-22 7.17567420e-20 6.60490828e-16 1.00000000e+00
 8.52774772e-16 1.02778476e-13 7.19154795e-18], sum to 1.0000
[2019-04-03 23:50:55,434] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9647
[2019-04-03 23:50:55,491] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.0, 82.0, 0.0, 0.0, 24.0, 23.23249311900745, -0.1209934395176239, 0.0, 1.0, 46853.46396253343], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1810800.0000, 
sim time next is 1811400.0000, 
raw observation next is [-5.0, 81.50000000000001, 0.0, 0.0, 24.0, 23.21400678448742, -0.125183703557082, 0.0, 1.0, 46767.28563947056], 
processed observation next is [0.0, 1.0, 0.32409972299168976, 0.8150000000000002, 0.0, 0.0, 0.5, 0.4345005653739517, 0.45827209881430603, 0.0, 1.0, 0.22270136018795506], 
reward next is 0.7773, 
noisyNet noise sample is [array([0.76327], dtype=float32), 0.64270747]. 
=============================================
[2019-04-03 23:50:56,437] A3C_AGENT_WORKER-Thread-18 INFO:Local step 88000, global step 1403378: loss 0.2814
[2019-04-03 23:50:56,438] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 88000, global step 1403378: learning rate 0.0005
[2019-04-03 23:50:56,943] A3C_AGENT_WORKER-Thread-3 INFO:Local step 88000, global step 1403556: loss 0.0448
[2019-04-03 23:50:56,945] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 88000, global step 1403556: learning rate 0.0005
[2019-04-03 23:50:57,868] A3C_AGENT_WORKER-Thread-11 INFO:Local step 87500, global step 1403882: loss 0.0372
[2019-04-03 23:50:57,906] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 87500, global step 1403892: learning rate 0.0005
[2019-04-03 23:51:00,144] A3C_AGENT_WORKER-Thread-20 INFO:Local step 87500, global step 1404571: loss 0.0090
[2019-04-03 23:51:00,145] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 87500, global step 1404571: learning rate 0.0005
[2019-04-03 23:51:01,554] A3C_AGENT_WORKER-Thread-17 INFO:Local step 88000, global step 1405030: loss 0.0062
[2019-04-03 23:51:01,555] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 88000, global step 1405030: learning rate 0.0005
[2019-04-03 23:51:02,566] A3C_AGENT_WORKER-Thread-4 INFO:Local step 87500, global step 1405361: loss 0.0564
[2019-04-03 23:51:02,568] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 87500, global step 1405361: learning rate 0.0005
[2019-04-03 23:51:03,607] A3C_AGENT_WORKER-Thread-15 INFO:Local step 87500, global step 1405739: loss 0.0048
[2019-04-03 23:51:03,608] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 87500, global step 1405739: learning rate 0.0005
[2019-04-03 23:51:03,865] A3C_AGENT_WORKER-Thread-19 INFO:Local step 88500, global step 1405820: loss 1.1716
[2019-04-03 23:51:03,866] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 88500, global step 1405820: learning rate 0.0005
[2019-04-03 23:51:05,001] A3C_AGENT_WORKER-Thread-14 INFO:Local step 87500, global step 1406234: loss 0.0834
[2019-04-03 23:51:05,003] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 87500, global step 1406234: learning rate 0.0005
[2019-04-03 23:51:06,465] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.7668455e-30 5.8385122e-24 6.4349017e-22 1.0000000e+00 6.5052361e-24
 2.5111971e-18 3.4959503e-24], sum to 1.0000
[2019-04-03 23:51:06,465] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5167
[2019-04-03 23:51:06,543] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.616666666666667, 73.83333333333334, 0.0, 0.0, 26.0, 25.31162300203305, 0.6312520361874848, 0.0, 1.0, 173744.9754613959], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1543800.0000, 
sim time next is 1544400.0000, 
raw observation next is [7.7, 74.0, 0.0, 0.0, 26.0, 25.46270076942233, 0.6889957759844082, 0.0, 1.0, 40839.04620413254], 
processed observation next is [1.0, 0.9130434782608695, 0.6759002770083103, 0.74, 0.0, 0.0, 0.6666666666666666, 0.6218917307851942, 0.7296652586614694, 0.0, 1.0, 0.19447164859110733], 
reward next is 0.8055, 
noisyNet noise sample is [array([-1.6661793], dtype=float32), -0.24078023]. 
=============================================
[2019-04-03 23:51:06,982] A3C_AGENT_WORKER-Thread-10 INFO:Local step 87500, global step 1406871: loss 0.0134
[2019-04-03 23:51:06,982] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 87500, global step 1406871: learning rate 0.0005
[2019-04-03 23:51:10,391] A3C_AGENT_WORKER-Thread-12 INFO:Local step 88000, global step 1407982: loss 0.0993
[2019-04-03 23:51:10,395] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 88000, global step 1407983: learning rate 0.0005
[2019-04-03 23:51:10,792] A3C_AGENT_WORKER-Thread-6 INFO:Local step 88000, global step 1408137: loss 0.0761
[2019-04-03 23:51:10,797] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 88000, global step 1408137: learning rate 0.0005
[2019-04-03 23:51:12,691] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3804030e-24 3.4631607e-19 6.0453919e-17 1.0000000e+00 6.2855383e-18
 4.5612767e-12 3.8820344e-19], sum to 1.0000
[2019-04-03 23:51:12,691] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9463
[2019-04-03 23:51:12,703] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.3333333333333334, 93.0, 0.0, 0.0, 23.0, 22.7498215912564, -0.1933242384975518, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 1722000.0000, 
sim time next is 1722600.0000, 
raw observation next is [0.25, 93.5, 0.0, 0.0, 23.0, 22.59309769116017, -0.2099074142867975, 0.0, 1.0, 97343.83345465524], 
processed observation next is [1.0, 0.9565217391304348, 0.46952908587257625, 0.935, 0.0, 0.0, 0.4166666666666667, 0.38275814093001426, 0.4300308619044008, 0.0, 1.0, 0.4635420640697868], 
reward next is 0.5365, 
noisyNet noise sample is [array([-0.5611878], dtype=float32), -0.40513477]. 
=============================================
[2019-04-03 23:51:12,913] A3C_AGENT_WORKER-Thread-13 INFO:Local step 88000, global step 1408849: loss 0.0533
[2019-04-03 23:51:12,937] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 88000, global step 1408852: learning rate 0.0005
[2019-04-03 23:51:13,477] A3C_AGENT_WORKER-Thread-5 INFO:Local step 88000, global step 1409042: loss 0.1013
[2019-04-03 23:51:13,478] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 88000, global step 1409042: learning rate 0.0005
[2019-04-03 23:51:14,197] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.9717854e-23 7.5626596e-17 7.4736999e-16 1.0000000e+00 2.1240931e-16
 1.9402906e-08 8.5044419e-19], sum to 1.0000
[2019-04-03 23:51:14,205] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1733
[2019-04-03 23:51:14,255] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 88.0, 0.0, 0.0, 23.0, 22.79108166049204, -0.1645790870870668, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 1709400.0000, 
sim time next is 1710000.0000, 
raw observation next is [1.1, 88.0, 0.0, 0.0, 23.0, 22.68079417188562, -0.1794259873870571, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.49307479224376743, 0.88, 0.0, 0.0, 0.4166666666666667, 0.39006618099046825, 0.4401913375376476, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6943024], dtype=float32), 0.9744958]. 
=============================================
[2019-04-03 23:51:14,336] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[83.49283 ]
 [83.348114]
 [83.35091 ]
 [83.54531 ]
 [83.755585]], R is [[83.66437531]
 [83.8277359 ]
 [83.98945618]
 [84.14955902]
 [84.30806732]].
[2019-04-03 23:51:18,796] A3C_AGENT_WORKER-Thread-16 INFO:Local step 88000, global step 1410320: loss 0.1135
[2019-04-03 23:51:18,802] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 88000, global step 1410321: learning rate 0.0005
[2019-04-03 23:51:21,482] A3C_AGENT_WORKER-Thread-11 INFO:Local step 88000, global step 1410987: loss 0.3033
[2019-04-03 23:51:21,484] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 88000, global step 1410987: learning rate 0.0005
[2019-04-03 23:51:22,636] A3C_AGENT_WORKER-Thread-2 INFO:Local step 89000, global step 1411296: loss 0.7333
[2019-04-03 23:51:22,637] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 89000, global step 1411297: learning rate 0.0005
[2019-04-03 23:51:22,725] A3C_AGENT_WORKER-Thread-20 INFO:Local step 88000, global step 1411316: loss 0.9316
[2019-04-03 23:51:22,727] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 88000, global step 1411316: learning rate 0.0005
[2019-04-03 23:51:25,716] A3C_AGENT_WORKER-Thread-15 INFO:Local step 88000, global step 1412003: loss 0.1024
[2019-04-03 23:51:25,718] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 88000, global step 1412003: learning rate 0.0005
[2019-04-03 23:51:26,417] A3C_AGENT_WORKER-Thread-18 INFO:Local step 88500, global step 1412156: loss 0.3196
[2019-04-03 23:51:26,417] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 88500, global step 1412156: learning rate 0.0005
[2019-04-03 23:51:27,921] A3C_AGENT_WORKER-Thread-4 INFO:Local step 88000, global step 1412583: loss 0.0542
[2019-04-03 23:51:27,922] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 88000, global step 1412583: learning rate 0.0005
[2019-04-03 23:51:28,368] A3C_AGENT_WORKER-Thread-3 INFO:Local step 88500, global step 1412687: loss 0.0695
[2019-04-03 23:51:28,368] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 88500, global step 1412687: learning rate 0.0005
[2019-04-03 23:51:30,293] A3C_AGENT_WORKER-Thread-14 INFO:Local step 88000, global step 1413183: loss 0.1417
[2019-04-03 23:51:30,298] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 88000, global step 1413184: learning rate 0.0005
[2019-04-03 23:51:30,611] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0888316e-24 1.3767156e-18 5.1590563e-17 1.0000000e+00 1.7891272e-19
 1.0356967e-11 2.7312064e-19], sum to 1.0000
[2019-04-03 23:51:30,611] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2145
[2019-04-03 23:51:30,672] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.633333333333333, 81.0, 89.0, 50.5, 24.0, 23.71559888019786, -0.097570704474828, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2104800.0000, 
sim time next is 2105400.0000, 
raw observation next is [-7.716666666666667, 81.5, 106.0, 63.99999999999999, 24.0, 23.65281495528537, -0.08188469501629315, 1.0, 1.0, 46863.97835095484], 
processed observation next is [1.0, 0.34782608695652173, 0.24884579870729456, 0.815, 0.35333333333333333, 0.07071823204419889, 0.5, 0.47106791294044736, 0.4727051016612356, 1.0, 1.0, 0.22316180167121355], 
reward next is 0.7768, 
noisyNet noise sample is [array([-0.7980006], dtype=float32), -1.5654846]. 
=============================================
[2019-04-03 23:51:32,336] A3C_AGENT_WORKER-Thread-10 INFO:Local step 88000, global step 1413691: loss 0.0288
[2019-04-03 23:51:32,336] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 88000, global step 1413691: learning rate 0.0005
[2019-04-03 23:51:32,854] A3C_AGENT_WORKER-Thread-17 INFO:Local step 88500, global step 1413822: loss 0.0619
[2019-04-03 23:51:32,854] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 88500, global step 1413822: learning rate 0.0005
[2019-04-03 23:51:34,753] A3C_AGENT_WORKER-Thread-19 INFO:Local step 89000, global step 1414289: loss 0.3921
[2019-04-03 23:51:34,754] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 89000, global step 1414289: learning rate 0.0005
[2019-04-03 23:51:42,025] A3C_AGENT_WORKER-Thread-12 INFO:Local step 88500, global step 1416109: loss 0.2442
[2019-04-03 23:51:42,025] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 88500, global step 1416109: learning rate 0.0005
[2019-04-03 23:51:43,455] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1204178e-21 8.2424816e-17 6.5186055e-15 1.0000000e+00 4.3297191e-17
 1.2014986e-10 1.4332031e-17], sum to 1.0000
[2019-04-03 23:51:43,455] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5040
[2019-04-03 23:51:43,533] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 71.0, 19.0, 0.0, 24.0, 23.5107746358095, -0.1005282742190566, 1.0, 1.0, 172828.968590814], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2221200.0000, 
sim time next is 2221800.0000, 
raw observation next is [-4.5, 70.5, 13.66666666666666, 0.0, 24.0, 23.5555413021323, -0.1822259228998713, 1.0, 1.0, 97927.14157211737], 
processed observation next is [1.0, 0.7391304347826086, 0.3379501385041552, 0.705, 0.04555555555555554, 0.0, 0.5, 0.46296177517769177, 0.4392580257000429, 1.0, 1.0, 0.4663197217719875], 
reward next is 0.5337, 
noisyNet noise sample is [array([-0.0741983], dtype=float32), -1.1177619]. 
=============================================
[2019-04-03 23:51:44,074] A3C_AGENT_WORKER-Thread-6 INFO:Local step 88500, global step 1416587: loss 0.2110
[2019-04-03 23:51:44,076] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 88500, global step 1416587: learning rate 0.0005
[2019-04-03 23:51:47,354] A3C_AGENT_WORKER-Thread-13 INFO:Local step 88500, global step 1417445: loss 0.1220
[2019-04-03 23:51:47,354] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 88500, global step 1417445: learning rate 0.0005
[2019-04-03 23:51:47,760] A3C_AGENT_WORKER-Thread-5 INFO:Local step 88500, global step 1417567: loss 0.0164
[2019-04-03 23:51:47,761] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 88500, global step 1417567: learning rate 0.0005
[2019-04-03 23:51:50,367] A3C_AGENT_WORKER-Thread-2 INFO:Local step 89500, global step 1418209: loss 0.5751
[2019-04-03 23:51:50,367] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 89500, global step 1418209: learning rate 0.0005
[2019-04-03 23:51:52,835] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.6989683e-25 1.5912219e-19 1.2815095e-16 1.0000000e+00 5.1987205e-19
 6.6636414e-14 3.9230628e-19], sum to 1.0000
[2019-04-03 23:51:52,835] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2336
[2019-04-03 23:51:52,849] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.45, 80.5, 0.0, 0.0, 24.0, 22.82780870762966, -0.2488736962098853, 0.0, 1.0, 44460.19773385986], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2611800.0000, 
sim time next is 2612400.0000, 
raw observation next is [-6.533333333333333, 79.66666666666667, 0.0, 0.0, 24.0, 22.76692710669765, -0.2577205463892642, 0.0, 1.0, 44617.24097266858], 
processed observation next is [1.0, 0.21739130434782608, 0.2816251154201293, 0.7966666666666667, 0.0, 0.0, 0.5, 0.39724392555813753, 0.4140931512035786, 0.0, 1.0, 0.21246305225080275], 
reward next is 0.7875, 
noisyNet noise sample is [array([-0.27659705], dtype=float32), 0.69223255]. 
=============================================
[2019-04-03 23:51:53,437] A3C_AGENT_WORKER-Thread-11 INFO:Local step 88500, global step 1418874: loss 0.2161
[2019-04-03 23:51:53,438] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 88500, global step 1418874: learning rate 0.0005
[2019-04-03 23:51:53,872] A3C_AGENT_WORKER-Thread-16 INFO:Local step 88500, global step 1418962: loss 0.0352
[2019-04-03 23:51:53,873] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 88500, global step 1418962: learning rate 0.0005
[2019-04-03 23:51:54,025] A3C_AGENT_WORKER-Thread-20 INFO:Local step 88500, global step 1418996: loss 0.0347
[2019-04-03 23:51:54,025] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 88500, global step 1418996: learning rate 0.0005
[2019-04-03 23:51:57,837] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0549631e-21 2.2485114e-15 2.4619643e-14 1.0000000e+00 3.7193487e-16
 5.5791787e-08 7.3224865e-18], sum to 1.0000
[2019-04-03 23:51:57,837] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1057
[2019-04-03 23:51:57,902] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.9, 82.0, 22.0, 0.0, 24.0, 24.11674404895611, 0.004658447208628032, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2047800.0000, 
sim time next is 2048400.0000, 
raw observation next is [-3.9, 82.0, 17.0, 0.0, 24.0, 24.22940313876791, 0.01429198129333201, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.3545706371191136, 0.82, 0.056666666666666664, 0.0, 0.5, 0.5191169282306592, 0.504763993764444, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.06210927], dtype=float32), -0.48787487]. 
=============================================
[2019-04-03 23:51:58,012] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.8435799e-23 3.4672981e-19 4.9032048e-16 1.0000000e+00 8.8212915e-18
 5.3440156e-12 3.1347873e-18], sum to 1.0000
[2019-04-03 23:51:58,012] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1379
[2019-04-03 23:51:58,036] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.299999999999999, 82.0, 0.0, 0.0, 24.0, 22.9873688906092, -0.1632800687691464, 0.0, 1.0, 43931.49516333063], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2159400.0000, 
sim time next is 2160000.0000, 
raw observation next is [-7.3, 82.0, 0.0, 0.0, 24.0, 22.9542445627141, -0.1706551664424963, 0.0, 1.0, 43920.69872783051], 
processed observation next is [1.0, 0.0, 0.26038781163434904, 0.82, 0.0, 0.0, 0.5, 0.41285371355950823, 0.44311494451916794, 0.0, 1.0, 0.2091461844182405], 
reward next is 0.7909, 
noisyNet noise sample is [array([0.52054435], dtype=float32), 0.40164122]. 
=============================================
[2019-04-03 23:51:58,128] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[78.11732 ]
 [78.02643 ]
 [77.94847 ]
 [77.89149 ]
 [77.878204]], R is [[78.03411102]
 [78.04457092]
 [78.05485535]
 [78.064888  ]
 [78.07441711]].
[2019-04-03 23:51:58,286] A3C_AGENT_WORKER-Thread-15 INFO:Local step 88500, global step 1419999: loss 0.0870
[2019-04-03 23:51:58,287] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 88500, global step 1419999: learning rate 0.0005
[2019-04-03 23:51:58,993] A3C_AGENT_WORKER-Thread-18 INFO:Local step 89000, global step 1420176: loss 0.0655
[2019-04-03 23:51:58,993] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 89000, global step 1420176: learning rate 0.0005
[2019-04-03 23:52:01,147] A3C_AGENT_WORKER-Thread-3 INFO:Local step 89000, global step 1420672: loss 0.1126
[2019-04-03 23:52:01,147] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 89000, global step 1420672: learning rate 0.0005
[2019-04-03 23:52:01,786] A3C_AGENT_WORKER-Thread-4 INFO:Local step 88500, global step 1420826: loss 0.1676
[2019-04-03 23:52:01,787] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 88500, global step 1420826: learning rate 0.0005
[2019-04-03 23:52:04,905] A3C_AGENT_WORKER-Thread-17 INFO:Local step 89000, global step 1421556: loss 0.0297
[2019-04-03 23:52:04,906] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 89000, global step 1421556: learning rate 0.0005
[2019-04-03 23:52:05,652] A3C_AGENT_WORKER-Thread-19 INFO:Local step 89500, global step 1421715: loss 1.9593
[2019-04-03 23:52:05,659] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 89500, global step 1421715: learning rate 0.0005
[2019-04-03 23:52:05,798] A3C_AGENT_WORKER-Thread-14 INFO:Local step 88500, global step 1421749: loss 0.1396
[2019-04-03 23:52:05,799] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 88500, global step 1421749: learning rate 0.0005
[2019-04-03 23:52:06,499] A3C_AGENT_WORKER-Thread-10 INFO:Local step 88500, global step 1421920: loss 0.1058
[2019-04-03 23:52:06,499] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 88500, global step 1421920: learning rate 0.0005
[2019-04-03 23:52:07,734] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.2543653e-27 4.3322072e-20 2.3332759e-18 1.0000000e+00 2.9032062e-21
 2.5575389e-14 1.9496857e-21], sum to 1.0000
[2019-04-03 23:52:07,735] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5715
[2019-04-03 23:52:07,785] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.95, 56.83333333333334, 228.3333333333333, 86.0, 26.0, 25.66554544412754, 0.366066898778638, 1.0, 1.0, 18703.93365229416], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2290200.0000, 
sim time next is 2290800.0000, 
raw observation next is [-2.7, 55.66666666666667, 245.1666666666667, 80.0, 26.0, 25.6774759926431, 0.3711757418762556, 1.0, 1.0, 18702.46535149451], 
processed observation next is [1.0, 0.5217391304347826, 0.38781163434903054, 0.5566666666666668, 0.8172222222222224, 0.08839779005524862, 0.6666666666666666, 0.6397896660535917, 0.6237252472920852, 1.0, 1.0, 0.08905935881664052], 
reward next is 0.9109, 
noisyNet noise sample is [array([0.59190154], dtype=float32), 0.8618542]. 
=============================================
[2019-04-03 23:52:14,871] A3C_AGENT_WORKER-Thread-12 INFO:Local step 89000, global step 1424137: loss 0.3061
[2019-04-03 23:52:14,885] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 89000, global step 1424137: learning rate 0.0005
[2019-04-03 23:52:18,124] A3C_AGENT_WORKER-Thread-6 INFO:Local step 89000, global step 1424835: loss 0.9712
[2019-04-03 23:52:18,124] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 89000, global step 1424835: learning rate 0.0005
[2019-04-03 23:52:19,399] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.3508525e-23 1.2642998e-18 1.2811241e-15 1.0000000e+00 6.3210289e-18
 5.1522645e-12 9.4829764e-19], sum to 1.0000
[2019-04-03 23:52:19,400] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8050
[2019-04-03 23:52:19,434] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.2, 69.0, 0.0, 0.0, 24.0, 23.54038521869585, -0.03139870537065816, 0.0, 1.0, 18745.84711148561], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2236800.0000, 
sim time next is 2237400.0000, 
raw observation next is [-5.3, 69.5, 0.0, 0.0, 24.0, 23.48957974160389, -0.03836085088903692, 0.0, 1.0, 51362.02744505167], 
processed observation next is [1.0, 0.9130434782608695, 0.31578947368421056, 0.695, 0.0, 0.0, 0.5, 0.45746497846699086, 0.48721304970365437, 0.0, 1.0, 0.24458108307167464], 
reward next is 0.7554, 
noisyNet noise sample is [array([0.17804855], dtype=float32), 1.2317573]. 
=============================================
[2019-04-03 23:52:20,408] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.6054178e-28 1.1723426e-23 2.6311572e-20 1.0000000e+00 3.4247544e-22
 3.4223715e-18 2.9316505e-22], sum to 1.0000
[2019-04-03 23:52:20,408] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9681
[2019-04-03 23:52:20,550] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.5, 45.33333333333333, 54.66666666666667, 44.0, 26.0, 24.95050243523065, 0.2848259805227845, 0.0, 1.0, 39591.54911159907], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2393400.0000, 
sim time next is 2394000.0000, 
raw observation next is [-0.6, 45.0, 42.5, 37.0, 26.0, 24.95700146172316, 0.2827603994452961, 0.0, 1.0, 34862.49519369117], 
processed observation next is [0.0, 0.7391304347826086, 0.44598337950138506, 0.45, 0.14166666666666666, 0.04088397790055249, 0.6666666666666666, 0.5797501218102633, 0.5942534664817654, 0.0, 1.0, 0.16601188187471985], 
reward next is 0.8340, 
noisyNet noise sample is [array([0.04619409], dtype=float32), -1.3095595]. 
=============================================
[2019-04-03 23:52:20,640] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[86.27638 ]
 [86.8053  ]
 [87.250275]
 [87.53183 ]
 [87.68706 ]], R is [[85.73641205]
 [85.69052124]
 [85.6158371 ]
 [85.5238266 ]
 [85.44232941]].
[2019-04-03 23:52:22,273] A3C_AGENT_WORKER-Thread-2 INFO:Local step 90000, global step 1425566: loss 0.1062
[2019-04-03 23:52:22,307] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 90000, global step 1425566: learning rate 0.0005
[2019-04-03 23:52:23,503] A3C_AGENT_WORKER-Thread-13 INFO:Local step 89000, global step 1425789: loss 1.6490
[2019-04-03 23:52:23,504] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 89000, global step 1425789: learning rate 0.0005
[2019-04-03 23:52:24,043] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.5538349e-23 3.8348155e-20 2.1072337e-16 1.0000000e+00 2.4690884e-18
 2.4901288e-15 8.1455448e-19], sum to 1.0000
[2019-04-03 23:52:24,043] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3924
[2019-04-03 23:52:24,066] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.716666666666667, 54.66666666666667, 0.0, 0.0, 25.0, 23.2885265087651, -0.1522651868698112, 0.0, 1.0, 44233.04881965275], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 2429400.0000, 
sim time next is 2430000.0000, 
raw observation next is [-7.8, 55.0, 0.0, 0.0, 25.0, 23.24289686962822, -0.1585297104538756, 0.0, 1.0, 44318.39559776771], 
processed observation next is [0.0, 0.13043478260869565, 0.24653739612188366, 0.55, 0.0, 0.0, 0.5833333333333334, 0.4369080724690182, 0.44715676318204145, 0.0, 1.0, 0.2110399790369891], 
reward next is 0.7890, 
noisyNet noise sample is [array([-0.05588338], dtype=float32), 0.02047763]. 
=============================================
[2019-04-03 23:52:24,215] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[74.58606]
 [74.36741]
 [74.14922]
 [73.95569]
 [73.80711]], R is [[74.80118561]
 [74.84254456]
 [74.88384247]
 [74.92499542]
 [74.96590424]].
[2019-04-03 23:52:25,528] A3C_AGENT_WORKER-Thread-5 INFO:Local step 89000, global step 1426121: loss 1.8561
[2019-04-03 23:52:25,559] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 89000, global step 1426121: learning rate 0.0005
[2019-04-03 23:52:30,838] A3C_AGENT_WORKER-Thread-20 INFO:Local step 89000, global step 1426763: loss 1.7110
[2019-04-03 23:52:30,851] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 89000, global step 1426763: learning rate 0.0005
[2019-04-03 23:52:33,520] A3C_AGENT_WORKER-Thread-11 INFO:Local step 89000, global step 1427100: loss 2.7106
[2019-04-03 23:52:33,520] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 89000, global step 1427100: learning rate 0.0005
[2019-04-03 23:52:34,558] A3C_AGENT_WORKER-Thread-16 INFO:Local step 89000, global step 1427258: loss 1.7890
[2019-04-03 23:52:34,592] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 89000, global step 1427258: learning rate 0.0005
[2019-04-03 23:52:35,712] A3C_AGENT_WORKER-Thread-18 INFO:Local step 89500, global step 1427466: loss 0.9115
[2019-04-03 23:52:35,713] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 89500, global step 1427466: learning rate 0.0005
[2019-04-03 23:52:38,382] A3C_AGENT_WORKER-Thread-15 INFO:Local step 89000, global step 1427974: loss 2.4830
[2019-04-03 23:52:38,411] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 89000, global step 1427974: learning rate 0.0005
[2019-04-03 23:52:39,990] A3C_AGENT_WORKER-Thread-3 INFO:Local step 89500, global step 1428258: loss 0.9742
[2019-04-03 23:52:39,991] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 89500, global step 1428258: learning rate 0.0005
[2019-04-03 23:52:45,390] A3C_AGENT_WORKER-Thread-17 INFO:Local step 89500, global step 1429278: loss 1.2454
[2019-04-03 23:52:45,397] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 89500, global step 1429280: learning rate 0.0005
[2019-04-03 23:52:45,907] A3C_AGENT_WORKER-Thread-4 INFO:Local step 89000, global step 1429390: loss 3.9500
[2019-04-03 23:52:45,928] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 89000, global step 1429391: learning rate 0.0005
[2019-04-03 23:52:47,621] A3C_AGENT_WORKER-Thread-19 INFO:Local step 90000, global step 1429817: loss 0.6753
[2019-04-03 23:52:47,621] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 90000, global step 1429817: learning rate 0.0005
[2019-04-03 23:52:48,500] A3C_AGENT_WORKER-Thread-10 INFO:Local step 89000, global step 1429993: loss 2.7757
[2019-04-03 23:52:48,514] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 89000, global step 1429993: learning rate 0.0005
[2019-04-03 23:52:50,781] A3C_AGENT_WORKER-Thread-14 INFO:Local step 89000, global step 1430419: loss 1.5950
[2019-04-03 23:52:50,827] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 89000, global step 1430419: learning rate 0.0005
[2019-04-03 23:52:54,143] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2099570e-25 7.8217330e-18 2.6415298e-17 1.0000000e+00 1.2451966e-19
 9.3016447e-13 2.8674584e-20], sum to 1.0000
[2019-04-03 23:52:54,144] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9713
[2019-04-03 23:52:54,190] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.333333333333334, 32.33333333333334, 0.0, 0.0, 24.0, 22.97189961553142, -0.1727985700641707, 1.0, 1.0, 103950.9516880961], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2830800.0000, 
sim time next is 2831400.0000, 
raw observation next is [4.0, 33.5, 0.0, 0.0, 24.0, 22.9361877904505, -0.1740343161970401, 1.0, 1.0, 90871.65736699835], 
processed observation next is [1.0, 0.782608695652174, 0.5734072022160666, 0.335, 0.0, 0.0, 0.5, 0.41134898253754165, 0.4419885612676533, 1.0, 1.0, 0.43272217793808737], 
reward next is 0.5673, 
noisyNet noise sample is [array([-0.92832917], dtype=float32), 1.3408571]. 
=============================================
[2019-04-03 23:52:59,165] A3C_AGENT_WORKER-Thread-12 INFO:Local step 89500, global step 1432059: loss 1.2635
[2019-04-03 23:52:59,167] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 89500, global step 1432059: learning rate 0.0005
[2019-04-03 23:53:04,653] A3C_AGENT_WORKER-Thread-6 INFO:Local step 89500, global step 1433052: loss 0.2628
[2019-04-03 23:53:04,703] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 89500, global step 1433052: learning rate 0.0005
[2019-04-03 23:53:04,865] A3C_AGENT_WORKER-Thread-2 INFO:Local step 90500, global step 1433084: loss 0.0329
[2019-04-03 23:53:04,889] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 90500, global step 1433084: learning rate 0.0005
[2019-04-03 23:53:06,893] A3C_AGENT_WORKER-Thread-13 INFO:Local step 89500, global step 1433452: loss 0.3791
[2019-04-03 23:53:06,911] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 89500, global step 1433452: learning rate 0.0005
[2019-04-03 23:53:11,748] A3C_AGENT_WORKER-Thread-20 INFO:Local step 89500, global step 1434505: loss 0.2188
[2019-04-03 23:53:11,749] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 89500, global step 1434505: learning rate 0.0005
[2019-04-03 23:53:11,783] A3C_AGENT_WORKER-Thread-5 INFO:Local step 89500, global step 1434508: loss 0.1853
[2019-04-03 23:53:11,784] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 89500, global step 1434508: learning rate 0.0005
[2019-04-03 23:53:12,161] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.9221209e-26 1.1804186e-18 6.4123253e-17 1.0000000e+00 1.6697734e-22
 1.0664933e-13 6.1117891e-21], sum to 1.0000
[2019-04-03 23:53:12,161] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6149
[2019-04-03 23:53:12,210] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.6, 58.0, 109.6666666666667, 789.8333333333334, 26.0, 26.14418452644852, 0.568922318535325, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2726400.0000, 
sim time next is 2727000.0000, 
raw observation next is [-5.4, 57.5, 109.0, 788.0, 26.0, 26.23116518445348, 0.5925562610664168, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.31301939058171746, 0.575, 0.36333333333333334, 0.8707182320441988, 0.6666666666666666, 0.68593043203779, 0.6975187536888056, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.0196658], dtype=float32), 0.8854293]. 
=============================================
[2019-04-03 23:53:12,250] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[81.15573 ]
 [81.237946]
 [81.35952 ]
 [81.44232 ]
 [81.192055]], R is [[81.21748352]
 [81.40531158]
 [81.59126282]
 [81.77535248]
 [81.95760345]].
[2019-04-03 23:53:14,527] A3C_AGENT_WORKER-Thread-11 INFO:Local step 89500, global step 1435035: loss 1.2060
[2019-04-03 23:53:14,528] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 89500, global step 1435035: learning rate 0.0005
[2019-04-03 23:53:16,723] A3C_AGENT_WORKER-Thread-18 INFO:Local step 90000, global step 1435407: loss 0.9203
[2019-04-03 23:53:16,723] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 90000, global step 1435407: learning rate 0.0005
[2019-04-03 23:53:19,552] A3C_AGENT_WORKER-Thread-16 INFO:Local step 89500, global step 1435901: loss 0.0860
[2019-04-03 23:53:19,557] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 89500, global step 1435901: learning rate 0.0005
[2019-04-03 23:53:19,715] A3C_AGENT_WORKER-Thread-15 INFO:Local step 89500, global step 1435937: loss 0.1095
[2019-04-03 23:53:19,732] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 89500, global step 1435937: learning rate 0.0005
[2019-04-03 23:53:20,329] A3C_AGENT_WORKER-Thread-3 INFO:Local step 90000, global step 1436096: loss 0.9749
[2019-04-03 23:53:20,330] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 90000, global step 1436096: learning rate 0.0005
[2019-04-03 23:53:24,024] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.3802332e-24 1.9617566e-21 1.6300896e-17 1.0000000e+00 8.0596227e-18
 2.6146460e-14 8.6323677e-20], sum to 1.0000
[2019-04-03 23:53:24,025] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0755
[2019-04-03 23:53:24,093] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 66.16666666666667, 28.33333333333333, 166.3333333333333, 24.0, 22.47955288106338, -0.3076899754774657, 0.0, 1.0, 41580.52733055435], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3052200.0000, 
sim time next is 3052800.0000, 
raw observation next is [-6.0, 64.0, 42.0, 214.5, 24.0, 22.46875843785377, -0.2949972353307781, 0.0, 1.0, 41462.76285977419], 
processed observation next is [0.0, 0.34782608695652173, 0.296398891966759, 0.64, 0.14, 0.23701657458563535, 0.5, 0.37239653648781407, 0.40166758822307397, 0.0, 1.0, 0.19744172790368664], 
reward next is 0.8026, 
noisyNet noise sample is [array([0.08292595], dtype=float32), 0.8962099]. 
=============================================
[2019-04-03 23:53:26,012] A3C_AGENT_WORKER-Thread-17 INFO:Local step 90000, global step 1437194: loss 0.7476
[2019-04-03 23:53:26,042] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 90000, global step 1437200: learning rate 0.0005
[2019-04-03 23:53:26,691] A3C_AGENT_WORKER-Thread-19 INFO:Local step 90500, global step 1437326: loss 0.2409
[2019-04-03 23:53:26,708] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 90500, global step 1437326: learning rate 0.0005
[2019-04-03 23:53:28,617] A3C_AGENT_WORKER-Thread-4 INFO:Local step 89500, global step 1437700: loss 0.2129
[2019-04-03 23:53:28,630] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 89500, global step 1437700: learning rate 0.0005
[2019-04-03 23:53:31,095] A3C_AGENT_WORKER-Thread-10 INFO:Local step 89500, global step 1438246: loss 1.4206
[2019-04-03 23:53:31,096] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 89500, global step 1438246: learning rate 0.0005
[2019-04-03 23:53:32,856] A3C_AGENT_WORKER-Thread-14 INFO:Local step 89500, global step 1438688: loss 0.3390
[2019-04-03 23:53:32,857] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 89500, global step 1438688: learning rate 0.0005
[2019-04-03 23:53:39,160] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4328098e-24 2.7096978e-16 2.8791595e-16 1.0000000e+00 3.2511391e-21
 3.0902930e-14 3.7499280e-20], sum to 1.0000
[2019-04-03 23:53:39,208] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9495
[2019-04-03 23:53:39,234] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 67.0, 68.66666666666666, 576.6666666666667, 24.0, 24.85137097368187, 0.2526199798210126, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3427800.0000, 
sim time next is 3428400.0000, 
raw observation next is [2.0, 67.0, 64.83333333333333, 544.8333333333333, 24.0, 24.95351531329213, 0.2762317383084758, 1.0, 1.0, 9340.205835115268], 
processed observation next is [1.0, 0.6956521739130435, 0.518005540166205, 0.67, 0.2161111111111111, 0.602025782688766, 0.5, 0.5794596094410108, 0.5920772461028253, 1.0, 1.0, 0.04447717064340604], 
reward next is 0.9555, 
noisyNet noise sample is [array([0.8396728], dtype=float32), 1.892958]. 
=============================================
[2019-04-03 23:53:40,982] A3C_AGENT_WORKER-Thread-12 INFO:Local step 90000, global step 1440415: loss 0.2367
[2019-04-03 23:53:40,983] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 90000, global step 1440415: learning rate 0.0005
[2019-04-03 23:53:41,544] A3C_AGENT_WORKER-Thread-2 INFO:Local step 91000, global step 1440555: loss 0.1557
[2019-04-03 23:53:41,544] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 91000, global step 1440555: learning rate 0.0005
[2019-04-03 23:53:43,430] A3C_AGENT_WORKER-Thread-6 INFO:Local step 90000, global step 1441054: loss 0.5351
[2019-04-03 23:53:43,430] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 90000, global step 1441054: learning rate 0.0005
[2019-04-03 23:53:46,789] A3C_AGENT_WORKER-Thread-13 INFO:Local step 90000, global step 1441800: loss 0.3687
[2019-04-03 23:53:46,789] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 90000, global step 1441800: learning rate 0.0005
[2019-04-03 23:53:50,207] A3C_AGENT_WORKER-Thread-20 INFO:Local step 90000, global step 1442432: loss 0.1726
[2019-04-03 23:53:50,208] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 90000, global step 1442432: learning rate 0.0005
[2019-04-03 23:53:50,514] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.7190739e-23 6.6573615e-19 1.4677063e-16 1.0000000e+00 5.2382010e-17
 2.0239297e-13 2.7117321e-18], sum to 1.0000
[2019-04-03 23:53:50,515] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5863
[2019-04-03 23:53:50,539] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.0, 71.0, 0.0, 0.0, 23.0, 22.36809666166306, -0.3399839173406591, 0.0, 1.0, 55601.29795792352], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 3027600.0000, 
sim time next is 3028200.0000, 
raw observation next is [-5.0, 71.0, 0.0, 0.0, 23.0, 22.37350214188897, -0.3339927029999442, 0.0, 1.0, 46197.28187615579], 
processed observation next is [0.0, 0.043478260869565216, 0.32409972299168976, 0.71, 0.0, 0.0, 0.4166666666666667, 0.36445851182408084, 0.38866909900001856, 0.0, 1.0, 0.21998705655312278], 
reward next is 0.7800, 
noisyNet noise sample is [array([0.01045549], dtype=float32), -0.057565637]. 
=============================================
[2019-04-03 23:53:51,244] A3C_AGENT_WORKER-Thread-18 INFO:Local step 90500, global step 1442741: loss 0.1136
[2019-04-03 23:53:51,245] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 90500, global step 1442741: learning rate 0.0005
[2019-04-03 23:53:52,189] A3C_AGENT_WORKER-Thread-5 INFO:Local step 90000, global step 1442999: loss 0.2302
[2019-04-03 23:53:52,192] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 90000, global step 1442999: learning rate 0.0005
[2019-04-03 23:53:52,983] A3C_AGENT_WORKER-Thread-11 INFO:Local step 90000, global step 1443212: loss 0.2187
[2019-04-03 23:53:53,015] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 90000, global step 1443212: learning rate 0.0005
[2019-04-03 23:53:54,773] A3C_AGENT_WORKER-Thread-3 INFO:Local step 90500, global step 1443603: loss 0.0332
[2019-04-03 23:53:54,774] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 90500, global step 1443603: learning rate 0.0005
[2019-04-03 23:53:54,999] A3C_AGENT_WORKER-Thread-15 INFO:Local step 90000, global step 1443662: loss 0.0702
[2019-04-03 23:53:55,014] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 90000, global step 1443662: learning rate 0.0005
[2019-04-03 23:53:58,799] A3C_AGENT_WORKER-Thread-16 INFO:Local step 90000, global step 1444591: loss 0.1840
[2019-04-03 23:53:58,811] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 90000, global step 1444591: learning rate 0.0005
[2019-04-03 23:53:59,932] A3C_AGENT_WORKER-Thread-17 INFO:Local step 90500, global step 1444904: loss 0.0047
[2019-04-03 23:53:59,956] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 90500, global step 1444904: learning rate 0.0005
[2019-04-03 23:54:00,867] A3C_AGENT_WORKER-Thread-19 INFO:Local step 91000, global step 1445209: loss 0.1536
[2019-04-03 23:54:00,867] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 91000, global step 1445209: learning rate 0.0005
[2019-04-03 23:54:02,809] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.2157810e-22 9.0472011e-19 5.9439330e-16 1.0000000e+00 2.6918331e-16
 4.0129112e-11 3.2337534e-18], sum to 1.0000
[2019-04-03 23:54:02,809] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7355
[2019-04-03 23:54:02,891] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 23.0, 22.53586660418514, -0.3024337563972898, 0.0, 1.0, 62243.63462339909], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 3102000.0000, 
sim time next is 3102600.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 23.0, 22.53414377251344, -0.2959682707264963, 0.0, 1.0, 46844.28979237438], 
processed observation next is [0.0, 0.9130434782608695, 0.4349030470914128, 1.0, 0.0, 0.0, 0.4166666666666667, 0.37784531437611985, 0.40134390975783457, 0.0, 1.0, 0.2230680466303542], 
reward next is 0.7769, 
noisyNet noise sample is [array([-1.4328864], dtype=float32), 2.992059]. 
=============================================
[2019-04-03 23:54:02,967] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3090406e-23 1.4812660e-14 1.4183896e-14 1.0000000e+00 1.3963061e-18
 3.0512626e-10 2.6733900e-18], sum to 1.0000
[2019-04-03 23:54:02,971] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5115
[2019-04-03 23:54:02,993] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.133333333333333, 74.0, 113.3333333333333, 813.0, 24.0, 24.5427218324095, 0.2652646451875602, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3238800.0000, 
sim time next is 3239400.0000, 
raw observation next is [-2.066666666666666, 72.5, 113.6666666666667, 815.0, 24.0, 24.28659450878363, 0.251962434790211, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.40535549399815335, 0.725, 0.378888888888889, 0.9005524861878453, 0.5, 0.5238828757319691, 0.5839874782634037, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.21452366], dtype=float32), -0.33911046]. 
=============================================
[2019-04-03 23:54:05,804] A3C_AGENT_WORKER-Thread-4 INFO:Local step 90000, global step 1446526: loss 0.4526
[2019-04-03 23:54:05,820] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 90000, global step 1446526: learning rate 0.0005
[2019-04-03 23:54:05,990] A3C_AGENT_WORKER-Thread-10 INFO:Local step 90000, global step 1446577: loss 0.0899
[2019-04-03 23:54:05,991] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 90000, global step 1446577: learning rate 0.0005
[2019-04-03 23:54:09,093] A3C_AGENT_WORKER-Thread-14 INFO:Local step 90000, global step 1447423: loss 0.3617
[2019-04-03 23:54:09,095] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 90000, global step 1447423: learning rate 0.0005
[2019-04-03 23:54:11,561] A3C_AGENT_WORKER-Thread-12 INFO:Local step 90500, global step 1448118: loss 0.0213
[2019-04-03 23:54:11,561] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 90500, global step 1448118: learning rate 0.0005
[2019-04-03 23:54:11,697] A3C_AGENT_WORKER-Thread-2 INFO:Local step 91500, global step 1448159: loss 0.3823
[2019-04-03 23:54:11,698] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 91500, global step 1448159: learning rate 0.0005
[2019-04-03 23:54:13,869] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.9329851e-22 8.9348900e-16 7.3583717e-14 1.0000000e+00 2.1466651e-16
 4.5690158e-09 1.2907232e-16], sum to 1.0000
[2019-04-03 23:54:13,874] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1614
[2019-04-03 23:54:13,893] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 94.66666666666666, 0.0, 0.0, 23.0, 22.44192001097429, -0.1990236661164895, 0.0, 1.0, 80071.95097106676], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 3220800.0000, 
sim time next is 3221400.0000, 
raw observation next is [-3.0, 93.33333333333334, 0.0, 0.0, 23.0, 22.40225726923201, -0.1953110670501622, 0.0, 1.0, 79975.38837312823], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.9333333333333335, 0.0, 0.0, 0.4166666666666667, 0.3668547724360008, 0.4348963109832793, 0.0, 1.0, 0.38083518272918204], 
reward next is 0.6192, 
noisyNet noise sample is [array([1.420203], dtype=float32), 0.041543376]. 
=============================================
[2019-04-03 23:54:15,265] A3C_AGENT_WORKER-Thread-6 INFO:Local step 90500, global step 1449026: loss 0.0567
[2019-04-03 23:54:15,269] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 90500, global step 1449028: learning rate 0.0005
[2019-04-03 23:54:18,153] A3C_AGENT_WORKER-Thread-13 INFO:Local step 90500, global step 1449781: loss 0.0086
[2019-04-03 23:54:18,155] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 90500, global step 1449781: learning rate 0.0005
[2019-04-03 23:54:18,863] A3C_AGENT_WORKER-Thread-20 INFO:Local step 90500, global step 1450015: loss 0.0212
[2019-04-03 23:54:18,863] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 90500, global step 1450015: learning rate 0.0005
[2019-04-03 23:54:19,899] A3C_AGENT_WORKER-Thread-18 INFO:Local step 91000, global step 1450309: loss 0.1148
[2019-04-03 23:54:19,901] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 91000, global step 1450310: learning rate 0.0005
[2019-04-03 23:54:21,428] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.7232304e-27 1.1081426e-20 1.4786414e-17 1.0000000e+00 1.1448108e-21
 4.7999612e-14 2.5835458e-20], sum to 1.0000
[2019-04-03 23:54:21,429] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3452
[2019-04-03 23:54:21,442] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-10.83333333333333, 76.0, 0.0, 0.0, 26.0, 24.28879578487395, 0.1601345598307612, 0.0, 1.0, 43689.38813523042], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3304200.0000, 
sim time next is 3304800.0000, 
raw observation next is [-11.0, 76.0, 0.0, 0.0, 26.0, 24.1860745699934, 0.1449105346446776, 0.0, 1.0, 43749.47675935891], 
processed observation next is [1.0, 0.2608695652173913, 0.15789473684210528, 0.76, 0.0, 0.0, 0.6666666666666666, 0.5155062141661167, 0.5483035115482259, 0.0, 1.0, 0.20833084171123292], 
reward next is 0.7917, 
noisyNet noise sample is [array([-1.0729593], dtype=float32), -1.6215942]. 
=============================================
[2019-04-03 23:54:21,717] A3C_AGENT_WORKER-Thread-11 INFO:Local step 90500, global step 1450856: loss 0.0397
[2019-04-03 23:54:21,739] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 90500, global step 1450861: learning rate 0.0005
[2019-04-03 23:54:23,443] A3C_AGENT_WORKER-Thread-3 INFO:Local step 91000, global step 1451371: loss 0.0036
[2019-04-03 23:54:23,446] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 91000, global step 1451371: learning rate 0.0005
[2019-04-03 23:54:23,826] A3C_AGENT_WORKER-Thread-5 INFO:Local step 90500, global step 1451499: loss 0.3450
[2019-04-03 23:54:23,828] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 90500, global step 1451500: learning rate 0.0005
[2019-04-03 23:54:24,132] A3C_AGENT_WORKER-Thread-15 INFO:Local step 90500, global step 1451579: loss 0.1317
[2019-04-03 23:54:24,133] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 90500, global step 1451579: learning rate 0.0005
[2019-04-03 23:54:24,877] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4670312e-28 2.3978055e-22 3.0680351e-21 1.0000000e+00 3.8783280e-22
 1.1171466e-17 6.7638625e-23], sum to 1.0000
[2019-04-03 23:54:24,878] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5741
[2019-04-03 23:54:24,892] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [9.666666666666666, 25.66666666666666, 0.0, 0.0, 24.0, 23.64313181995633, -0.08000979707964716, 0.0, 1.0, 23828.31375543566], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3649200.0000, 
sim time next is 3649800.0000, 
raw observation next is [9.833333333333334, 25.33333333333334, 0.0, 0.0, 24.0, 23.63443836104873, -0.07798793296381974, 0.0, 1.0, 29435.91271638295], 
processed observation next is [0.0, 0.21739130434782608, 0.7349953831948293, 0.2533333333333334, 0.0, 0.0, 0.5, 0.46953653008739415, 0.4740040223453934, 0.0, 1.0, 0.1401710129351569], 
reward next is 0.8598, 
noisyNet noise sample is [array([-0.2308429], dtype=float32), 0.02849306]. 
=============================================
[2019-04-03 23:54:27,429] A3C_AGENT_WORKER-Thread-17 INFO:Local step 91000, global step 1452723: loss 0.0001
[2019-04-03 23:54:27,430] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 91000, global step 1452723: learning rate 0.0005
[2019-04-03 23:54:28,581] A3C_AGENT_WORKER-Thread-16 INFO:Local step 90500, global step 1453142: loss 0.5324
[2019-04-03 23:54:28,600] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 90500, global step 1453147: learning rate 0.0005
[2019-04-03 23:54:29,058] A3C_AGENT_WORKER-Thread-19 INFO:Local step 91500, global step 1453315: loss 0.3753
[2019-04-03 23:54:29,058] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 91500, global step 1453315: learning rate 0.0005
[2019-04-03 23:54:29,553] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.2601878e-24 1.1161552e-15 1.0396728e-15 1.0000000e+00 4.6895050e-21
 2.4358007e-12 2.3461450e-19], sum to 1.0000
[2019-04-03 23:54:29,555] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3683
[2019-04-03 23:54:29,573] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 49.0, 115.0, 811.5, 24.0, 24.5840668936258, 0.1675497793641406, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3416400.0000, 
sim time next is 3417000.0000, 
raw observation next is [3.0, 49.0, 114.3333333333333, 809.6666666666666, 24.0, 24.26109549105941, 0.1272057768633482, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5457063711911359, 0.49, 0.381111111111111, 0.8946593001841621, 0.5, 0.5217579575882842, 0.542401925621116, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5210567], dtype=float32), -1.3905773]. 
=============================================
[2019-04-03 23:54:29,586] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[82.68443 ]
 [82.62266 ]
 [82.73015 ]
 [82.822205]
 [82.93964 ]], R is [[83.01462555]
 [83.18447876]
 [83.35263824]
 [83.51911163]
 [83.68392181]].
[2019-04-03 23:54:32,120] A3C_AGENT_WORKER-Thread-10 INFO:Local step 90500, global step 1454543: loss 0.0052
[2019-04-03 23:54:32,146] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 90500, global step 1454545: learning rate 0.0005
[2019-04-03 23:54:33,619] A3C_AGENT_WORKER-Thread-4 INFO:Local step 90500, global step 1455122: loss 0.1405
[2019-04-03 23:54:33,621] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 90500, global step 1455123: learning rate 0.0005
[2019-04-03 23:54:35,251] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.5196826e-25 1.2048448e-17 2.0227765e-16 1.0000000e+00 1.2964145e-21
 7.6034656e-12 6.7394056e-20], sum to 1.0000
[2019-04-03 23:54:35,251] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2827
[2019-04-03 23:54:35,292] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.833333333333334, 69.0, 108.6666666666667, 698.3333333333333, 26.0, 26.36081287082674, 0.5659200442699689, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3319800.0000, 
sim time next is 3320400.0000, 
raw observation next is [-7.666666666666667, 68.0, 109.8333333333333, 719.1666666666667, 26.0, 26.31550566217546, 0.5701834116374319, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.2502308402585411, 0.68, 0.366111111111111, 0.7946593001841622, 0.6666666666666666, 0.6929588051812884, 0.6900611372124773, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.28890088], dtype=float32), 1.3790511]. 
=============================================
[2019-04-03 23:54:35,567] A3C_AGENT_WORKER-Thread-14 INFO:Local step 90500, global step 1455893: loss 0.0069
[2019-04-03 23:54:35,569] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 90500, global step 1455893: learning rate 0.0005
[2019-04-03 23:54:35,826] A3C_AGENT_WORKER-Thread-2 INFO:Local step 92000, global step 1456006: loss 0.0479
[2019-04-03 23:54:35,826] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 92000, global step 1456006: learning rate 0.0005
[2019-04-03 23:54:36,254] A3C_AGENT_WORKER-Thread-12 INFO:Local step 91000, global step 1456173: loss 0.0772
[2019-04-03 23:54:36,256] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 91000, global step 1456173: learning rate 0.0005
[2019-04-03 23:54:38,583] A3C_AGENT_WORKER-Thread-6 INFO:Local step 91000, global step 1457199: loss 0.0022
[2019-04-03 23:54:38,584] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 91000, global step 1457199: learning rate 0.0005
[2019-04-03 23:54:39,280] A3C_AGENT_WORKER-Thread-20 INFO:Local step 91000, global step 1457489: loss 0.0389
[2019-04-03 23:54:39,281] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 91000, global step 1457490: learning rate 0.0005
[2019-04-03 23:54:40,476] A3C_AGENT_WORKER-Thread-18 INFO:Local step 91500, global step 1457975: loss 0.0075
[2019-04-03 23:54:40,477] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 91500, global step 1457976: learning rate 0.0005
[2019-04-03 23:54:40,730] A3C_AGENT_WORKER-Thread-13 INFO:Local step 91000, global step 1458067: loss 0.0028
[2019-04-03 23:54:40,732] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 91000, global step 1458067: learning rate 0.0005
[2019-04-03 23:54:42,195] A3C_AGENT_WORKER-Thread-3 INFO:Local step 91500, global step 1458670: loss 0.0782
[2019-04-03 23:54:42,200] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 91500, global step 1458670: learning rate 0.0005
[2019-04-03 23:54:43,337] A3C_AGENT_WORKER-Thread-11 INFO:Local step 91000, global step 1459176: loss 0.0019
[2019-04-03 23:54:43,339] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 91000, global step 1459177: learning rate 0.0005
[2019-04-03 23:54:44,045] A3C_AGENT_WORKER-Thread-15 INFO:Local step 91000, global step 1459495: loss 0.0788
[2019-04-03 23:54:44,064] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 91000, global step 1459497: learning rate 0.0005
[2019-04-03 23:54:45,580] A3C_AGENT_WORKER-Thread-5 INFO:Local step 91000, global step 1460118: loss 0.0514
[2019-04-03 23:54:45,587] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 91000, global step 1460118: learning rate 0.0005
[2019-04-03 23:54:45,906] A3C_AGENT_WORKER-Thread-17 INFO:Local step 91500, global step 1460234: loss 0.0415
[2019-04-03 23:54:45,906] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 91500, global step 1460234: learning rate 0.0005
[2019-04-03 23:54:49,297] A3C_AGENT_WORKER-Thread-19 INFO:Local step 92000, global step 1461385: loss 0.0900
[2019-04-03 23:54:49,307] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 92000, global step 1461385: learning rate 0.0005
[2019-04-03 23:54:50,684] A3C_AGENT_WORKER-Thread-16 INFO:Local step 91000, global step 1461867: loss 0.0799
[2019-04-03 23:54:50,685] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 91000, global step 1461867: learning rate 0.0005
[2019-04-03 23:54:52,791] A3C_AGENT_WORKER-Thread-10 INFO:Local step 91000, global step 1462559: loss 0.0081
[2019-04-03 23:54:52,793] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 91000, global step 1462559: learning rate 0.0005
[2019-04-03 23:54:53,755] A3C_AGENT_WORKER-Thread-2 INFO:Local step 92500, global step 1462847: loss 0.5240
[2019-04-03 23:54:53,757] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 92500, global step 1462847: learning rate 0.0005
[2019-04-03 23:54:54,897] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.0936175e-30 9.5597396e-23 2.0525730e-21 1.0000000e+00 1.9900858e-25
 1.3754026e-19 1.7960769e-24], sum to 1.0000
[2019-04-03 23:54:54,902] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4864
[2019-04-03 23:54:54,923] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 43.0, 74.5, 607.0, 26.0, 25.33888007281455, 0.4605966813778086, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3600000.0000, 
sim time next is 3600600.0000, 
raw observation next is [0.0, 42.33333333333334, 70.66666666666667, 576.3333333333333, 26.0, 25.32344716843504, 0.4532213354071443, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.46260387811634357, 0.42333333333333345, 0.23555555555555557, 0.6368324125230201, 0.6666666666666666, 0.6102872640362534, 0.6510737784690481, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.21519105], dtype=float32), 0.26248956]. 
=============================================
[2019-04-03 23:54:57,446] A3C_AGENT_WORKER-Thread-4 INFO:Local step 91000, global step 1463899: loss 0.0185
[2019-04-03 23:54:57,461] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.6548843e-25 4.1022930e-20 4.6788108e-17 1.0000000e+00 7.4838410e-20
 8.5598965e-13 8.8002711e-21], sum to 1.0000
[2019-04-03 23:54:57,467] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 91000, global step 1463899: learning rate 0.0005
[2019-04-03 23:54:57,474] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6649
[2019-04-03 23:54:57,501] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 24.0, 23.33883213500732, -0.0735736001641293, 0.0, 1.0, 48923.25932105503], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3819000.0000, 
sim time next is 3819600.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 24.0, 23.35719010940905, -0.06163355374208881, 0.0, 1.0, 46601.74116269847], 
processed observation next is [1.0, 0.21739130434782608, 0.3518005540166205, 0.71, 0.0, 0.0, 0.5, 0.44643250911742083, 0.4794554820859704, 0.0, 1.0, 0.221913053155707], 
reward next is 0.7781, 
noisyNet noise sample is [array([0.71159756], dtype=float32), -0.021099996]. 
=============================================
[2019-04-03 23:54:57,609] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.1214102e-26 4.5163410e-22 8.0198407e-19 1.0000000e+00 4.3334445e-21
 5.5380247e-18 2.4926112e-21], sum to 1.0000
[2019-04-03 23:54:57,609] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8097
[2019-04-03 23:54:57,624] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 60.0, 0.0, 0.0, 26.0, 25.33771256155839, 0.3669373596315055, 0.0, 1.0, 38961.89665042091], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3625200.0000, 
sim time next is 3625800.0000, 
raw observation next is [-1.0, 54.16666666666666, 0.0, 0.0, 26.0, 25.3169295858167, 0.3673528303892263, 0.0, 1.0, 38574.54536203213], 
processed observation next is [0.0, 1.0, 0.4349030470914128, 0.5416666666666665, 0.0, 0.0, 0.6666666666666666, 0.6097441321513916, 0.6224509434630754, 0.0, 1.0, 0.18368831124777205], 
reward next is 0.8163, 
noisyNet noise sample is [array([-0.45554104], dtype=float32), 1.514409]. 
=============================================
[2019-04-03 23:54:57,768] A3C_AGENT_WORKER-Thread-12 INFO:Local step 91500, global step 1463999: loss 0.1249
[2019-04-03 23:54:57,805] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 91500, global step 1464000: learning rate 0.0005
[2019-04-03 23:54:58,111] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.0864508e-28 4.6322263e-18 7.8036384e-19 1.0000000e+00 2.2721794e-23
 1.1931060e-14 4.4461518e-22], sum to 1.0000
[2019-04-03 23:54:58,117] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4079
[2019-04-03 23:54:58,138] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 42.33333333333334, 56.33333333333334, 489.0, 25.0, 26.00641377198259, 0.5512986732791713, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 3861600.0000, 
sim time next is 3862200.0000, 
raw observation next is [3.0, 41.66666666666667, 48.66666666666667, 427.0, 25.0, 26.18180702401207, 0.5709388641781743, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.5457063711911359, 0.41666666666666674, 0.16222222222222224, 0.4718232044198895, 0.5833333333333334, 0.6818172520010058, 0.6903129547260581, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.10416618], dtype=float32), -0.5840873]. 
=============================================
[2019-04-03 23:55:00,466] A3C_AGENT_WORKER-Thread-14 INFO:Local step 91000, global step 1464847: loss 0.0122
[2019-04-03 23:55:00,496] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 91000, global step 1464848: learning rate 0.0005
[2019-04-03 23:55:01,233] A3C_AGENT_WORKER-Thread-6 INFO:Local step 91500, global step 1465094: loss 0.0117
[2019-04-03 23:55:01,235] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 91500, global step 1465094: learning rate 0.0005
[2019-04-03 23:55:01,246] A3C_AGENT_WORKER-Thread-20 INFO:Local step 91500, global step 1465098: loss 0.1170
[2019-04-03 23:55:01,247] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 91500, global step 1465099: learning rate 0.0005
[2019-04-03 23:55:02,823] A3C_AGENT_WORKER-Thread-13 INFO:Local step 91500, global step 1465934: loss 0.0189
[2019-04-03 23:55:02,824] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 91500, global step 1465934: learning rate 0.0005
[2019-04-03 23:55:03,539] A3C_AGENT_WORKER-Thread-18 INFO:Local step 92000, global step 1466283: loss 0.0216
[2019-04-03 23:55:03,541] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 92000, global step 1466283: learning rate 0.0005
[2019-04-03 23:55:04,039] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.0322529e-30 7.3118412e-21 8.1081219e-20 1.0000000e+00 1.9230532e-25
 3.2953972e-17 9.3252500e-25], sum to 1.0000
[2019-04-03 23:55:04,052] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3776
[2019-04-03 23:55:04,078] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 43.66666666666667, 67.83333333333333, 578.6666666666666, 26.0, 26.92840615190945, 0.6413790513892155, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3860400.0000, 
sim time next is 3861000.0000, 
raw observation next is [3.0, 43.0, 64.0, 551.0, 26.0, 26.40243857273925, 0.6788532306095361, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.5457063711911359, 0.43, 0.21333333333333335, 0.6088397790055249, 0.6666666666666666, 0.7002032143949375, 0.7262844102031787, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.21882713], dtype=float32), -0.6593073]. 
=============================================
[2019-04-03 23:55:04,088] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[91.32422 ]
 [91.63597 ]
 [91.920334]
 [92.16301 ]
 [92.393875]], R is [[91.20574951]
 [91.29369354]
 [91.38076019]
 [91.46695709]
 [91.55229187]].
[2019-04-03 23:55:05,089] A3C_AGENT_WORKER-Thread-11 INFO:Local step 91500, global step 1466998: loss 0.0114
[2019-04-03 23:55:05,091] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 91500, global step 1466999: learning rate 0.0005
[2019-04-03 23:55:05,237] A3C_AGENT_WORKER-Thread-3 INFO:Local step 92000, global step 1467056: loss 0.0050
[2019-04-03 23:55:05,237] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 92000, global step 1467057: learning rate 0.0005
[2019-04-03 23:55:05,860] A3C_AGENT_WORKER-Thread-15 INFO:Local step 91500, global step 1467337: loss 0.0699
[2019-04-03 23:55:05,861] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 91500, global step 1467337: learning rate 0.0005
[2019-04-03 23:55:07,392] A3C_AGENT_WORKER-Thread-17 INFO:Local step 92000, global step 1467989: loss 0.0110
[2019-04-03 23:55:07,394] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 92000, global step 1467989: learning rate 0.0005
[2019-04-03 23:55:08,042] A3C_AGENT_WORKER-Thread-5 INFO:Local step 91500, global step 1468306: loss 0.1914
[2019-04-03 23:55:08,044] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 91500, global step 1468307: learning rate 0.0005
[2019-04-03 23:55:08,711] A3C_AGENT_WORKER-Thread-19 INFO:Local step 92500, global step 1468633: loss 0.2910
[2019-04-03 23:55:08,714] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 92500, global step 1468636: learning rate 0.0005
[2019-04-03 23:55:09,482] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.6195993e-28 1.5237142e-21 1.2251951e-19 1.0000000e+00 1.5880852e-21
 5.9424592e-16 1.7833938e-22], sum to 1.0000
[2019-04-03 23:55:09,487] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0799
[2019-04-03 23:55:09,502] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 51.0, 110.0, 53.0, 24.0, 23.47684623139997, -0.09091825121499249, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4264200.0000, 
sim time next is 4264800.0000, 
raw observation next is [3.0, 51.66666666666666, 122.0, 66.0, 24.0, 23.48473589615837, -0.0941180047009116, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.5457063711911359, 0.5166666666666666, 0.4066666666666667, 0.07292817679558011, 0.5, 0.4570613246798641, 0.4686273317663628, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.94071466], dtype=float32), 1.5557877]. 
=============================================
[2019-04-03 23:55:11,489] A3C_AGENT_WORKER-Thread-16 INFO:Local step 91500, global step 1469905: loss 0.0285
[2019-04-03 23:55:11,492] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 91500, global step 1469906: learning rate 0.0005
[2019-04-03 23:55:12,073] A3C_AGENT_WORKER-Thread-2 INFO:Local step 93000, global step 1470221: loss 0.6388
[2019-04-03 23:55:12,073] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 93000, global step 1470221: learning rate 0.0005
[2019-04-03 23:55:12,798] A3C_AGENT_WORKER-Thread-10 INFO:Local step 91500, global step 1470598: loss 0.2945
[2019-04-03 23:55:12,800] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 91500, global step 1470598: learning rate 0.0005
[2019-04-03 23:55:12,907] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.7677737e-23 1.8120978e-17 8.0508121e-16 1.0000000e+00 5.7672930e-19
 5.9501351e-13 4.7255686e-19], sum to 1.0000
[2019-04-03 23:55:12,908] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6032
[2019-04-03 23:55:12,931] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.5, 49.0, 113.0, 775.0, 24.0, 24.89227347451326, 0.2323813034246991, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3925800.0000, 
sim time next is 3926400.0000, 
raw observation next is [-6.333333333333334, 49.0, 114.1666666666667, 780.8333333333334, 24.0, 24.90993246671769, 0.2277977034960265, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.28716528162511545, 0.49, 0.38055555555555565, 0.8627992633517496, 0.5, 0.5758277055598077, 0.5759325678320089, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.62851775], dtype=float32), 0.71297145]. 
=============================================
[2019-04-03 23:55:15,271] A3C_AGENT_WORKER-Thread-4 INFO:Local step 91500, global step 1471804: loss 0.7815
[2019-04-03 23:55:15,272] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 91500, global step 1471804: learning rate 0.0005
[2019-04-03 23:55:16,263] A3C_AGENT_WORKER-Thread-12 INFO:Local step 92000, global step 1472293: loss 0.0224
[2019-04-03 23:55:16,264] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 92000, global step 1472293: learning rate 0.0005
[2019-04-03 23:55:16,994] A3C_AGENT_WORKER-Thread-14 INFO:Local step 91500, global step 1472646: loss 0.3334
[2019-04-03 23:55:16,995] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 91500, global step 1472646: learning rate 0.0005
[2019-04-03 23:55:18,054] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.8567624e-25 1.8649738e-19 8.7350365e-17 1.0000000e+00 2.1351291e-20
 2.9086095e-14 3.6139411e-20], sum to 1.0000
[2019-04-03 23:55:18,054] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8054
[2019-04-03 23:55:18,066] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.6666666666666666, 42.33333333333334, 0.0, 0.0, 24.0, 23.5806011425919, 0.03036693232881618, 0.0, 1.0, 18737.62518337158], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4146000.0000, 
sim time next is 4146600.0000, 
raw observation next is [-0.8333333333333334, 42.16666666666666, 0.0, 0.0, 24.0, 23.57606246376546, 0.02283047517726616, 0.0, 1.0, 26307.97713988713], 
processed observation next is [1.0, 1.0, 0.43951985226223456, 0.4216666666666666, 0.0, 0.0, 0.5, 0.464671871980455, 0.5076101583924221, 0.0, 1.0, 0.12527608161851014], 
reward next is 0.8747, 
noisyNet noise sample is [array([-1.3347358], dtype=float32), 0.48510128]. 
=============================================
[2019-04-03 23:55:18,150] A3C_AGENT_WORKER-Thread-18 INFO:Local step 92500, global step 1473216: loss 0.6248
[2019-04-03 23:55:18,157] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 92500, global step 1473220: learning rate 0.0005
[2019-04-03 23:55:18,575] A3C_AGENT_WORKER-Thread-20 INFO:Local step 92000, global step 1473404: loss 0.2299
[2019-04-03 23:55:18,576] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 92000, global step 1473404: learning rate 0.0005
[2019-04-03 23:55:18,658] A3C_AGENT_WORKER-Thread-6 INFO:Local step 92000, global step 1473444: loss 0.0013
[2019-04-03 23:55:18,661] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 92000, global step 1473444: learning rate 0.0005
[2019-04-03 23:55:19,617] A3C_AGENT_WORKER-Thread-3 INFO:Local step 92500, global step 1473903: loss 0.5755
[2019-04-03 23:55:19,617] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 92500, global step 1473903: learning rate 0.0005
[2019-04-03 23:55:21,348] A3C_AGENT_WORKER-Thread-13 INFO:Local step 92000, global step 1474799: loss 0.0779
[2019-04-03 23:55:21,349] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 92000, global step 1474800: learning rate 0.0005
[2019-04-03 23:55:21,956] A3C_AGENT_WORKER-Thread-17 INFO:Local step 92500, global step 1475118: loss 0.4133
[2019-04-03 23:55:21,958] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 92500, global step 1475118: learning rate 0.0005
[2019-04-03 23:55:22,161] A3C_AGENT_WORKER-Thread-15 INFO:Local step 92000, global step 1475213: loss 0.1425
[2019-04-03 23:55:22,162] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 92000, global step 1475213: learning rate 0.0005
[2019-04-03 23:55:22,283] A3C_AGENT_WORKER-Thread-11 INFO:Local step 92000, global step 1475273: loss 0.0722
[2019-04-03 23:55:22,283] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 92000, global step 1475273: learning rate 0.0005
[2019-04-03 23:55:23,118] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.9798647e-26 7.3449238e-18 1.8550139e-17 1.0000000e+00 1.1109567e-21
 1.2782244e-15 2.2740895e-21], sum to 1.0000
[2019-04-03 23:55:23,129] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4541
[2019-04-03 23:55:23,150] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [13.0, 36.5, 39.0, 0.0, 24.0, 26.27896321058062, 0.6314446835165302, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4379400.0000, 
sim time next is 4380000.0000, 
raw observation next is [13.0, 37.0, 34.0, 0.0, 24.0, 26.58197058300296, 0.6607346232458432, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.8227146814404434, 0.37, 0.11333333333333333, 0.0, 0.5, 0.7151642152502466, 0.7202448744152811, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.19574736], dtype=float32), 0.035983823]. 
=============================================
[2019-04-03 23:55:23,170] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[76.57477 ]
 [76.279755]
 [75.90965 ]
 [76.00978 ]
 [76.421425]], R is [[77.32217407]
 [77.5489502 ]
 [77.77346039]
 [77.99572754]
 [78.21577454]].
[2019-04-03 23:55:24,585] A3C_AGENT_WORKER-Thread-19 INFO:Local step 93000, global step 1476435: loss 1.3894
[2019-04-03 23:55:24,587] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 93000, global step 1476436: learning rate 0.0005
[2019-04-03 23:55:26,075] A3C_AGENT_WORKER-Thread-5 INFO:Local step 92000, global step 1477257: loss 0.1161
[2019-04-03 23:55:26,075] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 92000, global step 1477257: learning rate 0.0005
[2019-04-03 23:55:26,421] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:55:26,421] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:55:26,426] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run12
[2019-04-03 23:55:29,113] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.8022305e-27 1.2615819e-19 3.5287604e-18 1.0000000e+00 4.6039049e-23
 6.7715094e-14 1.2155851e-21], sum to 1.0000
[2019-04-03 23:55:29,114] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1922
[2019-04-03 23:55:29,123] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.65, 82.0, 120.0, 232.0, 25.0, 24.69445637131787, 0.3392837448975746, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 4437000.0000, 
sim time next is 4437600.0000, 
raw observation next is [1.533333333333333, 82.66666666666667, 127.5, 198.5, 25.0, 24.97777806111963, 0.3601012333465766, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.505078485687904, 0.8266666666666667, 0.425, 0.21933701657458562, 0.5833333333333334, 0.5814815050933024, 0.6200337444488588, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.18747154], dtype=float32), 0.18083912]. 
=============================================
[2019-04-03 23:55:29,627] A3C_AGENT_WORKER-Thread-16 INFO:Local step 92000, global step 1479011: loss 0.2601
[2019-04-03 23:55:29,630] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 92000, global step 1479012: learning rate 0.0005
[2019-04-03 23:55:29,733] A3C_AGENT_WORKER-Thread-10 INFO:Local step 92000, global step 1479065: loss 0.0638
[2019-04-03 23:55:29,737] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 92000, global step 1479066: learning rate 0.0005
[2019-04-03 23:55:30,036] A3C_AGENT_WORKER-Thread-12 INFO:Local step 92500, global step 1479218: loss 0.1269
[2019-04-03 23:55:30,043] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 92500, global step 1479218: learning rate 0.0005
[2019-04-03 23:55:32,714] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.0076312e-33 4.1963764e-26 9.8664618e-25 1.0000000e+00 1.8114735e-27
 5.2128918e-22 5.6752795e-27], sum to 1.0000
[2019-04-03 23:55:32,720] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1268
[2019-04-03 23:55:32,728] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.0, 52.0, 120.3333333333333, 838.6666666666667, 26.0, 25.23064866058924, 0.4070081318067773, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4277400.0000, 
sim time next is 4278000.0000, 
raw observation next is [7.0, 52.0, 120.1666666666667, 842.8333333333334, 26.0, 25.240490274976, 0.4119046715076942, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.6565096952908588, 0.52, 0.40055555555555566, 0.9313075506445673, 0.6666666666666666, 0.6033741895813334, 0.6373015571692314, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0682818], dtype=float32), -0.6472591]. 
=============================================
[2019-04-03 23:55:32,733] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[101.74978 ]
 [101.60104 ]
 [101.36723 ]
 [101.133545]
 [100.88971 ]], R is [[101.81825256]
 [101.80007172]
 [101.78207397]
 [101.76425171]
 [101.74661255]].
[2019-04-03 23:55:32,884] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.4044603e-35 2.3087061e-26 3.3285118e-24 1.0000000e+00 4.2662781e-28
 3.5488810e-22 2.0492729e-27], sum to 1.0000
[2019-04-03 23:55:32,888] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1554
[2019-04-03 23:55:32,902] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.0, 53.66666666666667, 176.6666666666667, 738.6666666666666, 26.0, 25.36080881384158, 0.4368638884630083, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4281600.0000, 
sim time next is 4282200.0000, 
raw observation next is [7.0, 54.5, 188.0, 717.0, 26.0, 25.36632955567419, 0.439885674945738, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.6565096952908588, 0.545, 0.6266666666666667, 0.7922651933701658, 0.6666666666666666, 0.6138607963061826, 0.646628558315246, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.79646736], dtype=float32), 0.48998502]. 
=============================================
[2019-04-03 23:55:33,025] A3C_AGENT_WORKER-Thread-20 INFO:Local step 92500, global step 1480647: loss 0.5169
[2019-04-03 23:55:33,026] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 92500, global step 1480648: learning rate 0.0005
[2019-04-03 23:55:33,367] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.1174230e-25 1.9785257e-19 3.2758324e-17 1.0000000e+00 6.4930266e-20
 1.2041511e-13 4.7339357e-21], sum to 1.0000
[2019-04-03 23:55:33,369] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3981
[2019-04-03 23:55:33,394] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 80.33333333333334, 67.33333333333334, 0.0, 25.0, 25.28446085478031, 0.3655551177139587, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 4462800.0000, 
sim time next is 4463400.0000, 
raw observation next is [0.0, 79.16666666666666, 63.66666666666666, 0.0, 25.0, 25.3138050973817, 0.2598615059462313, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.46260387811634357, 0.7916666666666665, 0.2122222222222222, 0.0, 0.5833333333333334, 0.6094837581151417, 0.586620501982077, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6021498], dtype=float32), -0.69650674]. 
=============================================
[2019-04-03 23:55:33,492] A3C_AGENT_WORKER-Thread-4 INFO:Local step 92000, global step 1480902: loss 0.2700
[2019-04-03 23:55:33,493] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 92000, global step 1480902: learning rate 0.0005
[2019-04-03 23:55:33,505] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.5272575e-24 1.4255407e-19 2.1925622e-18 1.0000000e+00 5.5034594e-20
 5.0098780e-16 1.7704151e-20], sum to 1.0000
[2019-04-03 23:55:33,510] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6236
[2019-04-03 23:55:33,531] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.666666666666667, 41.0, 0.0, 0.0, 24.0, 23.22199399347391, -0.0952765849371741, 0.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4818000.0000, 
sim time next is 4818600.0000, 
raw observation next is [1.5, 41.5, 0.0, 0.0, 24.0, 23.19625710354528, -0.1046654814394749, 0.0, 1.0, 26829.37599401037], 
processed observation next is [0.0, 0.782608695652174, 0.5041551246537397, 0.415, 0.0, 0.0, 0.5, 0.43302142529543985, 0.46511150618684166, 0.0, 1.0, 0.12775893330481128], 
reward next is 0.8722, 
noisyNet noise sample is [array([0.5053372], dtype=float32), -1.0850203]. 
=============================================
[2019-04-03 23:55:33,791] A3C_AGENT_WORKER-Thread-6 INFO:Local step 92500, global step 1481052: loss 0.5418
[2019-04-03 23:55:33,792] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 92500, global step 1481052: learning rate 0.0005
[2019-04-03 23:55:33,796] A3C_AGENT_WORKER-Thread-18 INFO:Local step 93000, global step 1481053: loss 1.7865
[2019-04-03 23:55:33,797] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 93000, global step 1481053: learning rate 0.0005
[2019-04-03 23:55:34,886] A3C_AGENT_WORKER-Thread-14 INFO:Local step 92000, global step 1481654: loss 0.0991
[2019-04-03 23:55:34,887] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 92000, global step 1481654: learning rate 0.0005
[2019-04-03 23:55:35,437] A3C_AGENT_WORKER-Thread-3 INFO:Local step 93000, global step 1481940: loss 1.5775
[2019-04-03 23:55:35,438] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 93000, global step 1481940: learning rate 0.0005
[2019-04-03 23:55:35,610] A3C_AGENT_WORKER-Thread-15 INFO:Local step 92500, global step 1482024: loss 1.0737
[2019-04-03 23:55:35,614] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 92500, global step 1482024: learning rate 0.0005
[2019-04-03 23:55:35,949] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.1168583e-26 2.3334339e-20 2.0535158e-18 1.0000000e+00 5.1487437e-21
 5.9159280e-16 4.6096033e-21], sum to 1.0000
[2019-04-03 23:55:35,952] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9644
[2019-04-03 23:55:35,977] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 25.0, 24.7078197731724, 0.2814132118317297, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 4563600.0000, 
sim time next is 4564200.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 25.0, 24.63327518771272, 0.2698774703628178, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.518005540166205, 0.52, 0.0, 0.0, 0.5833333333333334, 0.5527729323093933, 0.5899591567876059, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.20068736], dtype=float32), 1.2080837]. 
=============================================
[2019-04-03 23:55:36,263] A3C_AGENT_WORKER-Thread-11 INFO:Local step 92500, global step 1482343: loss 0.6466
[2019-04-03 23:55:36,266] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 92500, global step 1482344: learning rate 0.0005
[2019-04-03 23:55:37,026] A3C_AGENT_WORKER-Thread-13 INFO:Local step 92500, global step 1482758: loss 0.4060
[2019-04-03 23:55:37,029] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 92500, global step 1482759: learning rate 0.0005
[2019-04-03 23:55:37,549] A3C_AGENT_WORKER-Thread-17 INFO:Local step 93000, global step 1483049: loss 1.8551
[2019-04-03 23:55:37,550] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 93000, global step 1483050: learning rate 0.0005
[2019-04-03 23:55:39,316] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.9126359e-28 9.1060186e-21 4.7994730e-19 1.0000000e+00 3.8628649e-24
 3.0469903e-16 4.4855727e-22], sum to 1.0000
[2019-04-03 23:55:39,317] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1601
[2019-04-03 23:55:39,345] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.233333333333333, 49.0, 135.8333333333333, 820.6666666666666, 25.0, 26.3705759952915, 0.5355245565451566, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 4627200.0000, 
sim time next is 4627800.0000, 
raw observation next is [4.35, 49.0, 139.0, 813.0, 25.0, 26.06087445765047, 0.5954451378489422, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5831024930747922, 0.49, 0.4633333333333333, 0.8983425414364641, 0.5833333333333334, 0.6717395381375392, 0.6984817126163141, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.2218645], dtype=float32), 1.2695783]. 
=============================================
[2019-04-03 23:55:39,777] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:55:39,777] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:55:39,807] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run12
[2019-04-03 23:55:41,281] A3C_AGENT_WORKER-Thread-5 INFO:Local step 92500, global step 1484895: loss 0.2504
[2019-04-03 23:55:41,284] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 92500, global step 1484895: learning rate 0.0005
[2019-04-03 23:55:43,651] A3C_AGENT_WORKER-Thread-10 INFO:Local step 92500, global step 1486010: loss 0.9326
[2019-04-03 23:55:43,652] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 92500, global step 1486010: learning rate 0.0005
[2019-04-03 23:55:45,220] A3C_AGENT_WORKER-Thread-16 INFO:Local step 92500, global step 1486752: loss 1.0546
[2019-04-03 23:55:45,220] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 92500, global step 1486752: learning rate 0.0005
[2019-04-03 23:55:45,525] A3C_AGENT_WORKER-Thread-12 INFO:Local step 93000, global step 1486890: loss 2.8823
[2019-04-03 23:55:45,527] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 93000, global step 1486890: learning rate 0.0005
[2019-04-03 23:55:47,870] A3C_AGENT_WORKER-Thread-20 INFO:Local step 93000, global step 1488072: loss 3.5435
[2019-04-03 23:55:47,871] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 93000, global step 1488072: learning rate 0.0005
[2019-04-03 23:55:48,781] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:55:48,782] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:55:48,799] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run12
[2019-04-03 23:55:49,055] A3C_AGENT_WORKER-Thread-4 INFO:Local step 92500, global step 1488595: loss 0.2125
[2019-04-03 23:55:49,055] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 92500, global step 1488595: learning rate 0.0005
[2019-04-03 23:55:49,621] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:55:49,621] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:55:49,627] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run12
[2019-04-03 23:55:49,769] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.7115661e-29 4.4775865e-23 1.6103824e-21 1.0000000e+00 3.7874010e-23
 1.8012826e-19 5.5021892e-24], sum to 1.0000
[2019-04-03 23:55:49,769] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0290
[2019-04-03 23:55:49,806] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.166666666666667, 42.5, 0.0, 0.0, 25.0, 24.05535017933078, 0.1158537977465208, 0.0, 1.0, 28493.73164163102], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 4819800.0000, 
sim time next is 4820400.0000, 
raw observation next is [1.0, 43.0, 0.0, 0.0, 25.0, 24.05260713275, 0.1125104513172696, 0.0, 1.0, 31192.99686379452], 
processed observation next is [0.0, 0.8260869565217391, 0.4903047091412743, 0.43, 0.0, 0.0, 0.5833333333333334, 0.5043839277291667, 0.5375034837724232, 0.0, 1.0, 0.14853808030378343], 
reward next is 0.8515, 
noisyNet noise sample is [array([-1.0881895], dtype=float32), -1.7531738]. 
=============================================
[2019-04-03 23:55:50,137] A3C_AGENT_WORKER-Thread-6 INFO:Local step 93000, global step 1489063: loss 3.2046
[2019-04-03 23:55:50,138] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 93000, global step 1489063: learning rate 0.0005
[2019-04-03 23:55:50,415] A3C_AGENT_WORKER-Thread-14 INFO:Local step 92500, global step 1489166: loss 0.3787
[2019-04-03 23:55:50,416] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 92500, global step 1489166: learning rate 0.0005
[2019-04-03 23:55:50,874] A3C_AGENT_WORKER-Thread-15 INFO:Local step 93000, global step 1489324: loss 2.8980
[2019-04-03 23:55:50,878] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 93000, global step 1489324: learning rate 0.0005
[2019-04-03 23:55:52,502] A3C_AGENT_WORKER-Thread-11 INFO:Local step 93000, global step 1489967: loss 3.2239
[2019-04-03 23:55:52,503] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 93000, global step 1489969: learning rate 0.0005
[2019-04-03 23:55:52,629] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:55:52,629] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:55:52,635] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run12
[2019-04-03 23:55:53,552] A3C_AGENT_WORKER-Thread-13 INFO:Local step 93000, global step 1490288: loss 2.7040
[2019-04-03 23:55:53,555] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 93000, global step 1490288: learning rate 0.0005
[2019-04-03 23:55:59,274] A3C_AGENT_WORKER-Thread-5 INFO:Local step 93000, global step 1492591: loss 1.4863
[2019-04-03 23:55:59,275] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 93000, global step 1492591: learning rate 0.0005
[2019-04-03 23:56:00,129] A3C_AGENT_WORKER-Thread-10 INFO:Local step 93000, global step 1493012: loss 2.2620
[2019-04-03 23:56:00,133] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 93000, global step 1493013: learning rate 0.0005
[2019-04-03 23:56:00,933] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:56:00,933] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:56:00,952] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run12
[2019-04-03 23:56:02,866] A3C_AGENT_WORKER-Thread-16 INFO:Local step 93000, global step 1494157: loss 2.4059
[2019-04-03 23:56:02,867] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 93000, global step 1494157: learning rate 0.0005
[2019-04-03 23:56:03,108] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:56:03,108] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:56:03,112] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run12
[2019-04-03 23:56:06,293] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:56:06,293] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:56:06,297] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run12
[2019-04-03 23:56:07,041] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:56:07,041] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:56:07,049] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run12
[2019-04-03 23:56:07,283] A3C_AGENT_WORKER-Thread-4 INFO:Local step 93000, global step 1495632: loss 3.1370
[2019-04-03 23:56:07,284] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 93000, global step 1495632: learning rate 0.0005
[2019-04-03 23:56:07,676] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:56:07,676] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:56:07,679] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run12
[2019-04-03 23:56:08,586] A3C_AGENT_WORKER-Thread-14 INFO:Local step 93000, global step 1495966: loss 1.4983
[2019-04-03 23:56:08,587] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 93000, global step 1495966: learning rate 0.0005
[2019-04-03 23:56:10,547] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:56:10,547] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:56:10,572] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run12
[2019-04-03 23:56:12,947] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.7221946e-22 1.4138882e-16 1.3738315e-16 1.0000000e+00 6.3843540e-17
 9.2555352e-11 3.5473860e-18], sum to 1.0000
[2019-04-03 23:56:12,948] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9445
[2019-04-03 23:56:12,970] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.2, 96.0, 0.0, 0.0, 19.0, 18.89655518292279, -1.068174570018566, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4200.0000, 
sim time next is 4800.0000, 
raw observation next is [7.200000000000001, 96.0, 0.0, 0.0, 19.0, 18.9531349001357, -1.068474691441586, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.043478260869565216, 0.662049861495845, 0.96, 0.0, 0.0, 0.08333333333333333, 0.07942790834464179, 0.14384176951947136, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.41662285], dtype=float32), 0.75736487]. 
=============================================
[2019-04-03 23:56:16,465] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:56:16,465] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:56:16,468] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run12
[2019-04-03 23:56:17,764] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:56:17,764] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:56:17,769] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run12
[2019-04-03 23:56:20,903] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:56:20,903] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:56:20,915] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run12
[2019-04-03 23:56:21,102] A3C_AGENT_WORKER-Thread-6 INFO:Evaluating...
[2019-04-03 23:56:21,109] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-03 23:56:21,124] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:56:21,126] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-03 23:56:21,126] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-03 23:56:21,132] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:56:21,170] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:56:22,641] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run16
[2019-04-03 23:56:22,817] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run16
[2019-04-03 23:56:23,065] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run16
[2019-04-03 23:56:58,074] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.19914356], dtype=float32), 0.4879935]
[2019-04-03 23:56:58,074] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [3.983333333333333, 77.66666666666666, 208.3333333333333, 209.3333333333333, 21.0, 21.85528311517158, -0.4136118012127991, 1.0, 1.0, 0.0]
[2019-04-03 23:56:58,075] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-03 23:56:58,076] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.5901942e-18 4.1199742e-13 6.9240739e-13 9.9999809e-01 1.1673264e-12
 1.9015013e-06 2.0688788e-15], sampled 0.95728603658058
[2019-04-03 23:57:26,036] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.19914356], dtype=float32), 0.4879935]
[2019-04-03 23:57:26,036] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-5.033333333333333, 71.66666666666667, 258.6666666666666, 192.0, 21.0, 20.86853899445652, -0.6247046847577653, 1.0, 1.0, 0.0]
[2019-04-03 23:57:26,036] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-03 23:57:26,036] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [3.0981551e-15 5.8005937e-11 2.1155096e-10 9.9942493e-01 9.7653330e-10
 5.7513639e-04 8.2614238e-13], sampled 0.8374012909240313
[2019-04-03 23:57:44,242] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6512.6025 168625820.6132 -1262.1369
[2019-04-03 23:58:13,880] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7124.6414 211093582.4971 -1035.6221
[2019-04-03 23:58:30,235] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.19914356], dtype=float32), 0.4879935]
[2019-04-03 23:58:30,235] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-7.637830712, 80.69235678, 0.0, 0.0, 23.0, 22.15493851284799, -0.3482273615692845, 0.0, 1.0, 43114.59904288353]
[2019-04-03 23:58:30,235] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-03 23:58:30,236] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.4362555e-20 9.3145513e-17 6.2558174e-14 1.0000000e+00 1.9082260e-14
 9.8795494e-10 6.3417336e-16], sampled 0.6087566116663704
[2019-04-03 23:58:38,656] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7330.7297 236916111.7510 -502.9142
[2019-04-03 23:58:39,694] A3C_AGENT_WORKER-Thread-6 INFO:Global step: 1500000, evaluation results [1500000.0, 7124.641374189519, 211093582.49707678, -1035.6221334281254, 6512.602459225034, 168625820.61324105, -1262.1368642778864, 7330.72968149554, 236916111.75095758, -502.9141828175199]
[2019-04-03 23:58:43,029] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.0259426e-31 7.3992198e-20 4.7517386e-21 1.0000000e+00 3.4654459e-27
 9.6888848e-18 4.9199322e-25], sum to 1.0000
[2019-04-03 23:58:43,045] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1514
[2019-04-03 23:58:43,058] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [10.66666666666667, 20.83333333333334, 115.6666666666667, 846.3333333333333, 26.0, 26.95931581706393, 0.8919070215634486, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5061000.0000, 
sim time next is 5061600.0000, 
raw observation next is [11.0, 20.0, 114.5, 839.5, 26.0, 27.52130489734301, 0.9527745604826325, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.7673130193905818, 0.2, 0.38166666666666665, 0.9276243093922651, 0.6666666666666666, 0.7934420747785843, 0.8175915201608775, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2238947], dtype=float32), 0.51522106]. 
=============================================
[2019-04-03 23:58:45,733] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.4269438e-21 1.2697193e-15 7.2598389e-13 9.9999964e-01 1.6575061e-14
 2.9995863e-07 2.2566135e-15], sum to 1.0000
[2019-04-03 23:58:45,733] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2646
[2019-04-03 23:58:45,805] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-15.0, 69.0, 0.0, 0.0, 23.0, 20.56548351041357, -0.8027609679966989, 0.0, 1.0, 50813.97431225967], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 354600.0000, 
sim time next is 355200.0000, 
raw observation next is [-15.0, 69.0, 0.0, 0.0, 23.0, 20.53334306822202, -0.817698908796503, 0.0, 1.0, 50901.8844546972], 
processed observation next is [1.0, 0.08695652173913043, 0.04709141274238226, 0.69, 0.0, 0.0, 0.4166666666666667, 0.21111192235183504, 0.22743369706783234, 0.0, 1.0, 0.2423899259747486], 
reward next is 0.7576, 
noisyNet noise sample is [array([-0.5562266], dtype=float32), -0.9232426]. 
=============================================
[2019-04-03 23:58:47,351] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:58:47,351] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:58:47,365] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run12
[2019-04-03 23:58:49,761] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:58:49,761] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:58:49,786] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run12
[2019-04-03 23:58:57,183] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.9750206e-25 1.3101672e-19 3.2201543e-16 1.0000000e+00 1.0336998e-18
 4.0791988e-12 7.4645993e-19], sum to 1.0000
[2019-04-03 23:58:57,185] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9652
[2019-04-03 23:58:57,237] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.566666666666667, 68.33333333333334, 0.0, 0.0, 25.0, 23.62825632592043, -0.07674058971920096, 0.0, 1.0, 45312.81689896331], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 249600.0000, 
sim time next is 250200.0000, 
raw observation next is [-3.65, 70.0, 0.0, 0.0, 25.0, 23.57665874965971, -0.08586661711281877, 0.0, 1.0, 45318.04997656321], 
processed observation next is [1.0, 0.9130434782608695, 0.3614958448753463, 0.7, 0.0, 0.0, 0.5833333333333334, 0.46472156247164254, 0.47137779429572707, 0.0, 1.0, 0.21580023798363435], 
reward next is 0.7842, 
noisyNet noise sample is [array([-0.87384355], dtype=float32), 0.6262243]. 
=============================================
[2019-04-03 23:59:21,304] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.0287259e-19 6.7163750e-13 2.3484519e-12 9.9999893e-01 2.8101794e-14
 1.1051594e-06 5.5158684e-15], sum to 1.0000
[2019-04-03 23:59:21,305] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6349
[2019-04-03 23:59:21,356] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-10.05, 45.5, 24.0, 240.0, 23.0, 23.31178576227712, -0.2161900684598822, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 318600.0000, 
sim time next is 319200.0000, 
raw observation next is [-10.23333333333333, 46.66666666666666, 20.0, 201.0, 23.0, 23.27968091994043, -0.3483301167635177, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.1791320406278856, 0.46666666666666656, 0.06666666666666667, 0.22209944751381216, 0.4166666666666667, 0.4399734099950357, 0.3838899610788274, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.1471867], dtype=float32), -0.18517162]. 
=============================================
[2019-04-03 23:59:28,086] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.7337349e-25 5.2875714e-16 1.8028595e-16 1.0000000e+00 1.3995843e-20
 7.5898665e-14 8.9377393e-19], sum to 1.0000
[2019-04-03 23:59:28,086] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1613
[2019-04-03 23:59:28,153] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [13.38333333333333, 61.66666666666667, 0.0, 0.0, 23.0, 23.540208623189, 0.1136020780224365, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 1111800.0000, 
sim time next is 1112400.0000, 
raw observation next is [13.3, 62.0, 0.0, 0.0, 23.0, 23.48027095004814, 0.1023526808761221, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.8310249307479226, 0.62, 0.0, 0.0, 0.4166666666666667, 0.45668924583734505, 0.5341175602920407, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9941941], dtype=float32), -0.19108672]. 
=============================================
[2019-04-03 23:59:36,316] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.8642904e-19 2.3302911e-14 6.9098789e-12 9.9999964e-01 4.7668771e-14
 3.4848912e-07 1.1857144e-14], sum to 1.0000
[2019-04-03 23:59:36,334] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9693
[2019-04-03 23:59:36,370] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.9, 62.5, 0.0, 0.0, 24.0, 23.34537290790418, -0.1038325343262059, 0.0, 1.0, 45481.02527273976], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 768600.0000, 
sim time next is 769200.0000, 
raw observation next is [-6.0, 63.0, 0.0, 0.0, 24.0, 23.31843301042379, -0.1104577455553412, 0.0, 1.0, 45192.82706831178], 
processed observation next is [1.0, 0.9130434782608695, 0.296398891966759, 0.63, 0.0, 0.0, 0.5, 0.4432027508686491, 0.4631807514815529, 0.0, 1.0, 0.2152039384205323], 
reward next is 0.7848, 
noisyNet noise sample is [array([-0.05406359], dtype=float32), -0.097930185]. 
=============================================
[2019-04-03 23:59:53,691] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.5320837e-23 1.7653993e-18 7.3519109e-16 1.0000000e+00 1.6882171e-17
 7.1622111e-11 1.2034891e-18], sum to 1.0000
[2019-04-03 23:59:53,692] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5757
[2019-04-03 23:59:53,796] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.9, 66.0, 123.3333333333333, 34.00000000000001, 24.0, 23.20232825433985, -0.201755018959943, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 640200.0000, 
sim time next is 640800.0000, 
raw observation next is [-3.9, 65.0, 117.5, 25.5, 24.0, 23.14139990221228, -0.2215606075898604, 0.0, 1.0, 18725.02297458626], 
processed observation next is [0.0, 0.43478260869565216, 0.3545706371191136, 0.65, 0.39166666666666666, 0.0281767955801105, 0.5, 0.42844999185102345, 0.4261464641367132, 0.0, 1.0, 0.08916677606945839], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.03993028], dtype=float32), -0.11104459]. 
=============================================
[2019-04-03 23:59:56,973] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.4494729e-22 1.1407959e-15 6.3311986e-13 9.9999714e-01 4.9457581e-16
 2.8315053e-06 5.0156574e-16], sum to 1.0000
[2019-04-03 23:59:56,973] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5265
[2019-04-03 23:59:57,052] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.6, 100.0, 32.0, 0.0, 23.0, 23.304167526736, -0.0996545049141318, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 1414800.0000, 
sim time next is 1415400.0000, 
raw observation next is [-0.5, 99.16666666666667, 36.66666666666667, 0.0, 23.0, 23.38523489270952, -0.1049897323036254, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.44875346260387816, 0.9916666666666667, 0.12222222222222223, 0.0, 0.4166666666666667, 0.44876957439245996, 0.4650034225654582, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4822804], dtype=float32), 1.1062714]. 
=============================================
[2019-04-04 00:00:11,620] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.7762832e-24 1.2025799e-19 1.9033495e-16 1.0000000e+00 3.0119238e-19
 4.3282251e-13 3.8777106e-20], sum to 1.0000
[2019-04-04 00:00:11,620] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3833
[2019-04-04 00:00:11,711] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.899999999999999, 84.66666666666666, 0.0, 0.0, 25.0, 23.6104804044663, 0.1115910345482227, 1.0, 1.0, 194939.9403951659], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 844800.0000, 
sim time next is 845400.0000, 
raw observation next is [-3.9, 85.33333333333334, 0.0, 0.0, 25.0, 24.24487393735533, 0.1618056664334875, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.3545706371191136, 0.8533333333333334, 0.0, 0.0, 0.5833333333333334, 0.5204061614462775, 0.5539352221444959, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2467976], dtype=float32), 0.7183695]. 
=============================================
[2019-04-04 00:00:11,816] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.8539659e-23 2.1598709e-18 1.4373438e-15 1.0000000e+00 4.4337969e-18
 1.0605306e-11 3.3883886e-18], sum to 1.0000
[2019-04-04 00:00:11,817] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0931
[2019-04-04 00:00:11,868] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.9, 86.0, 0.0, 0.0, 25.0, 24.57618195524285, 0.1861776603852076, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 846000.0000, 
sim time next is 846600.0000, 
raw observation next is [-3.816666666666666, 85.5, 0.0, 0.0, 25.0, 24.77345028841043, 0.1972174447082683, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.3568790397045245, 0.855, 0.0, 0.0, 0.5833333333333334, 0.5644541907008692, 0.5657391482360894, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.16968359], dtype=float32), -0.45420554]. 
=============================================
[2019-04-04 00:00:13,602] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.4507375e-19 3.4284770e-16 1.5680159e-13 1.0000000e+00 6.7558223e-14
 1.5067302e-09 3.7019946e-16], sum to 1.0000
[2019-04-04 00:00:13,602] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8337
[2019-04-04 00:00:13,666] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.9, 69.0, 115.6666666666667, 42.5, 22.0, 21.28348665167988, -0.6326855135395534, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 638400.0000, 
sim time next is 639000.0000, 
raw observation next is [-3.9, 68.0, 135.0, 51.0, 22.0, 21.21296021743555, -0.6423027771564792, 0.0, 1.0, 18712.05536064773], 
processed observation next is [0.0, 0.391304347826087, 0.3545706371191136, 0.68, 0.45, 0.056353591160221, 0.3333333333333333, 0.26774668478629593, 0.2858990742811736, 0.0, 1.0, 0.08910502552689395], 
reward next is 0.9109, 
noisyNet noise sample is [array([-0.38153288], dtype=float32), 0.51944643]. 
=============================================
[2019-04-04 00:00:13,672] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[77.93629 ]
 [77.98469 ]
 [77.981285]
 [77.848015]
 [77.55048 ]], R is [[77.9679184 ]
 [78.18824005]
 [78.40635681]
 [78.62229156]
 [78.8360672 ]].
[2019-04-04 00:00:13,679] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.5184630e-30 4.2486862e-22 1.2242560e-19 1.0000000e+00 9.8618389e-24
 1.2689934e-15 1.0278193e-22], sum to 1.0000
[2019-04-04 00:00:13,679] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4451
[2019-04-04 00:00:13,739] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [11.23333333333333, 73.0, 0.0, 0.0, 24.0, 24.00269216364591, 0.2336699729317891, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1124400.0000, 
sim time next is 1125000.0000, 
raw observation next is [11.05, 74.0, 0.0, 0.0, 24.0, 23.99034032264055, 0.2264136785462335, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.0, 0.7686980609418284, 0.74, 0.0, 0.0, 0.5, 0.4991950268867124, 0.5754712261820778, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.27389297], dtype=float32), -0.3950655]. 
=============================================
[2019-04-04 00:00:13,829] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[94.21475 ]
 [94.12178 ]
 [94.415985]
 [95.334946]
 [95.43259 ]], R is [[94.28657532]
 [94.34371185]
 [94.40027618]
 [94.45627594]
 [94.51171112]].
[2019-04-04 00:00:20,189] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.5795791e-20 2.3937368e-16 3.0414249e-14 1.0000000e+00 1.2257685e-14
 1.7076974e-09 1.9833271e-16], sum to 1.0000
[2019-04-04 00:00:20,247] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8016
[2019-04-04 00:00:20,290] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.733333333333333, 71.33333333333334, 0.0, 0.0, 22.0, 21.45203574848836, -0.6137838528594257, 0.0, 1.0, 30452.03480629767], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 692400.0000, 
sim time next is 693000.0000, 
raw observation next is [-3.65, 71.5, 0.0, 0.0, 22.0, 21.43585830785827, -0.6178064255774945, 0.0, 1.0, 44928.93816293967], 
processed observation next is [1.0, 0.0, 0.3614958448753463, 0.715, 0.0, 0.0, 0.3333333333333333, 0.2863215256548557, 0.29406452480750184, 0.0, 1.0, 0.21394732458542698], 
reward next is 0.7861, 
noisyNet noise sample is [array([0.32819328], dtype=float32), -0.119790085]. 
=============================================
[2019-04-04 00:00:20,340] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[85.92237 ]
 [85.680084]
 [84.97508 ]
 [83.346375]
 [83.31411 ]], R is [[85.91264343]
 [85.9085083 ]
 [85.94813538]
 [85.9048996 ]
 [85.84832764]].
[2019-04-04 00:00:20,540] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.5674283e-22 4.9186763e-17 3.3063447e-14 1.0000000e+00 2.8797761e-15
 3.1504239e-08 2.0435389e-16], sum to 1.0000
[2019-04-04 00:00:20,540] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6008
[2019-04-04 00:00:20,560] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 72.5, 0.0, 0.0, 22.0, 21.48653698602616, -0.6104326522783728, 0.0, 1.0, 24207.19385396498], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 695400.0000, 
sim time next is 696000.0000, 
raw observation next is [-3.4, 73.0, 0.0, 0.0, 22.0, 21.48670705026378, -0.6202435894700173, 0.0, 1.0, 31564.18583428928], 
processed observation next is [1.0, 0.043478260869565216, 0.368421052631579, 0.73, 0.0, 0.0, 0.3333333333333333, 0.29055892085531515, 0.29325213684332757, 0.0, 1.0, 0.15030564682994896], 
reward next is 0.8497, 
noisyNet noise sample is [array([1.6123662], dtype=float32), 0.3985297]. 
=============================================
[2019-04-04 00:00:20,592] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[83.92441 ]
 [83.92506 ]
 [84.13201 ]
 [84.202446]
 [84.33015 ]], R is [[83.88587189]
 [83.93173981]
 [83.95077515]
 [84.02128601]
 [83.92368317]].
[2019-04-04 00:00:27,684] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6862140e-17 7.6666829e-14 2.3298657e-12 9.9999917e-01 1.4356871e-12
 8.3466790e-07 2.3818502e-14], sum to 1.0000
[2019-04-04 00:00:27,684] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0970
[2019-04-04 00:00:27,723] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.6, 61.0, 0.0, 0.0, 22.0, 21.51688049219335, -0.5135906043936267, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 766800.0000, 
sim time next is 767400.0000, 
raw observation next is [-5.7, 61.5, 0.0, 0.0, 22.0, 21.56074415951036, -0.5159884334332921, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.30470914127423826, 0.615, 0.0, 0.0, 0.3333333333333333, 0.29672867995919666, 0.32800385552223593, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0506996], dtype=float32), 0.92784137]. 
=============================================
[2019-04-04 00:00:34,023] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.5905386e-28 2.0182240e-21 6.9547501e-18 1.0000000e+00 1.6398095e-21
 4.1606375e-12 3.5314077e-21], sum to 1.0000
[2019-04-04 00:00:34,030] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9620
[2019-04-04 00:00:34,044] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [8.8, 83.0, 0.0, 0.0, 23.0, 22.7224862140295, -0.2265920967273328, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 970200.0000, 
sim time next is 970800.0000, 
raw observation next is [8.8, 83.0, 0.0, 0.0, 23.0, 22.6603650785473, -0.2249467821855864, 0.0, 1.0, 45908.72998605367], 
processed observation next is [1.0, 0.21739130434782608, 0.7063711911357342, 0.83, 0.0, 0.0, 0.4166666666666667, 0.3883637565456084, 0.4250177392714712, 0.0, 1.0, 0.21861299993358893], 
reward next is 0.7814, 
noisyNet noise sample is [array([-0.01804817], dtype=float32), 1.155706]. 
=============================================
[2019-04-04 00:00:37,648] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.5493005e-22 5.2189564e-16 3.0124371e-14 1.0000000e+00 2.3586937e-16
 2.0954458e-09 7.2293170e-17], sum to 1.0000
[2019-04-04 00:00:37,648] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2051
[2019-04-04 00:00:37,687] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.8999999999999999, 74.0, 0.0, 0.0, 22.0, 21.43181223923433, -0.5888032992998663, 0.0, 1.0, 96546.53418039324], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 880200.0000, 
sim time next is 880800.0000, 
raw observation next is [-0.8, 73.33333333333334, 0.0, 0.0, 22.0, 21.38841047021658, -0.5839698637809075, 0.0, 1.0, 89332.67014377145], 
processed observation next is [1.0, 0.17391304347826086, 0.4404432132963989, 0.7333333333333334, 0.0, 0.0, 0.3333333333333333, 0.2823675391847151, 0.30534337873969747, 0.0, 1.0, 0.42539366735129264], 
reward next is 0.5746, 
noisyNet noise sample is [array([0.11192149], dtype=float32), -0.21155001]. 
=============================================
[2019-04-04 00:00:38,351] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.7386920e-26 7.3207722e-21 2.8317081e-19 1.0000000e+00 5.6762743e-20
 8.4554835e-15 1.2715287e-21], sum to 1.0000
[2019-04-04 00:00:38,351] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8893
[2019-04-04 00:00:38,383] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.65, 96.0, 0.0, 0.0, 23.0, 21.93336843303425, -0.1416353642048429, 0.0, 1.0, 198324.6830844881], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 1279800.0000, 
sim time next is 1280400.0000, 
raw observation next is [6.466666666666667, 96.0, 0.0, 0.0, 23.0, 22.01289338843683, -0.08588639522275872, 0.0, 1.0, 177910.4067631308], 
processed observation next is [0.0, 0.8260869565217391, 0.6417359187442291, 0.96, 0.0, 0.0, 0.4166666666666667, 0.3344077823697358, 0.4713712015924138, 0.0, 1.0, 0.8471924131577657], 
reward next is 0.1528, 
noisyNet noise sample is [array([-1.6397235], dtype=float32), -0.37227225]. 
=============================================
[2019-04-04 00:00:39,524] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.19027435e-23 7.20654308e-18 1.10757545e-14 1.00000000e+00
 3.12434120e-18 2.30579271e-11 1.74270411e-17], sum to 1.0000
[2019-04-04 00:00:39,528] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9050
[2019-04-04 00:00:39,543] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.8999999999999999, 74.0, 0.0, 0.0, 23.0, 22.36433075554062, -0.3704930692853632, 0.0, 1.0, 96912.70668238038], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 880200.0000, 
sim time next is 880800.0000, 
raw observation next is [-0.8, 73.33333333333334, 0.0, 0.0, 23.0, 22.3198941734443, -0.3685627282052359, 0.0, 1.0, 67133.9655940036], 
processed observation next is [1.0, 0.17391304347826086, 0.4404432132963989, 0.7333333333333334, 0.0, 0.0, 0.4166666666666667, 0.3599911811203584, 0.37714575726492133, 0.0, 1.0, 0.3196855504476362], 
reward next is 0.6803, 
noisyNet noise sample is [array([0.4004793], dtype=float32), 0.35970154]. 
=============================================
[2019-04-04 00:00:44,608] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.4835621e-24 7.5023349e-20 8.8453553e-17 1.0000000e+00 8.2519200e-17
 3.7941174e-12 1.2622992e-19], sum to 1.0000
[2019-04-04 00:00:44,608] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2859
[2019-04-04 00:00:44,641] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.5, 100.0, 0.0, 0.0, 23.0, 22.87440247504213, -0.03436237805882769, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 1287600.0000, 
sim time next is 1288200.0000, 
raw observation next is [5.5, 100.0, 0.0, 0.0, 23.0, 22.81580568539555, -0.04640223837471991, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.9130434782608695, 0.6149584487534627, 1.0, 0.0, 0.0, 0.4166666666666667, 0.40131714044962913, 0.4845325872084267, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2911528], dtype=float32), -0.60239583]. 
=============================================
[2019-04-04 00:00:53,381] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.3852091e-23 1.2899737e-18 1.1763158e-16 1.0000000e+00 7.7103440e-18
 6.7025102e-13 1.9874669e-19], sum to 1.0000
[2019-04-04 00:00:53,385] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8506
[2019-04-04 00:00:53,478] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 25.0, 24.03491300850374, 0.2173393392369397, 0.0, 1.0, 81725.27051610503], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 1366200.0000, 
sim time next is 1366800.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 25.0, 23.93646440206934, 0.2277154896953645, 0.0, 1.0, 112816.7202706252], 
processed observation next is [1.0, 0.8260869565217391, 0.4764542936288089, 0.96, 0.0, 0.0, 0.5833333333333334, 0.4947053668391117, 0.5759051632317882, 0.0, 1.0, 0.5372224774791676], 
reward next is 0.4628, 
noisyNet noise sample is [array([3.343898], dtype=float32), 0.45406345]. 
=============================================
[2019-04-04 00:00:57,419] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.62275848e-29 2.71377542e-22 1.35395427e-20 1.00000000e+00
 1.04497675e-23 1.55982229e-16 4.95723624e-24], sum to 1.0000
[2019-04-04 00:00:57,420] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0238
[2019-04-04 00:00:57,480] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 95.0, 67.66666666666667, 0.0, 25.0, 25.24725295869426, 0.3237364160380545, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 1419600.0000, 
sim time next is 1420200.0000, 
raw observation next is [0.0, 95.0, 72.0, 0.0, 25.0, 25.20160255565508, 0.3181204974840217, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.46260387811634357, 0.95, 0.24, 0.0, 0.5833333333333334, 0.6001335463045899, 0.6060401658280072, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0562693], dtype=float32), 1.7270726]. 
=============================================
[2019-04-04 00:00:57,780] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.16030882e-21 1.83266116e-17 1.40311677e-14 1.00000000e+00
 6.43268796e-16 2.19937211e-11 1.03054316e-16], sum to 1.0000
[2019-04-04 00:00:57,791] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0522
[2019-04-04 00:00:57,827] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.2, 83.66666666666667, 0.0, 0.0, 24.0, 22.6863764302345, -0.254008195803218, 0.0, 1.0, 47775.45923299467], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1828200.0000, 
sim time next is 1828800.0000, 
raw observation next is [-6.2, 83.0, 0.0, 0.0, 24.0, 22.65425045075587, -0.2550594781966424, 0.0, 1.0, 47790.44301548493], 
processed observation next is [0.0, 0.17391304347826086, 0.2908587257617729, 0.83, 0.0, 0.0, 0.5, 0.38785420422965594, 0.4149801739344525, 0.0, 1.0, 0.22757353816897588], 
reward next is 0.7724, 
noisyNet noise sample is [array([1.3807912], dtype=float32), -0.22689681]. 
=============================================
[2019-04-04 00:01:06,494] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.4360881e-27 5.1454282e-20 4.1361419e-19 1.0000000e+00 1.3994936e-20
 1.2853499e-13 3.2916952e-22], sum to 1.0000
[2019-04-04 00:01:06,494] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7754
[2019-04-04 00:01:06,565] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 92.0, 59.0, 0.0, 23.0, 23.1772781801727, -0.1180360290828791, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 1436400.0000, 
sim time next is 1437000.0000, 
raw observation next is [1.1, 92.0, 54.66666666666666, 0.0, 23.0, 23.33023708402838, -0.1182322651401744, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.49307479224376743, 0.92, 0.1822222222222222, 0.0, 0.4166666666666667, 0.4441864236690316, 0.4605892449532752, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.8351803], dtype=float32), 0.18763979]. 
=============================================
[2019-04-04 00:01:06,593] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[95.890816]
 [95.74181 ]
 [95.711815]
 [95.60698 ]
 [95.44979 ]], R is [[96.07325745]
 [96.11252594]
 [96.15140533]
 [96.18989563]
 [96.22799683]].
[2019-04-04 00:01:20,927] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.2910962e-27 9.5277260e-18 8.4999707e-18 1.0000000e+00 2.5675789e-21
 1.7657304e-13 1.5342479e-21], sum to 1.0000
[2019-04-04 00:01:20,927] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1557
[2019-04-04 00:01:20,940] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [11.9, 51.0, 77.0, 478.0, 22.0, 23.37027847062404, -0.04466463820404007, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 1524600.0000, 
sim time next is 1525200.0000, 
raw observation next is [12.0, 50.66666666666666, 78.66666666666667, 403.0000000000001, 22.0, 22.85675995432129, -0.1034343627767235, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.7950138504155125, 0.5066666666666666, 0.26222222222222225, 0.44530386740331507, 0.3333333333333333, 0.4047299961934409, 0.4655218790744255, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.7512894], dtype=float32), -0.03911171]. 
=============================================
[2019-04-04 00:01:28,181] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.16516160e-23 1.84234828e-17 6.49117942e-17 1.00000000e+00
 1.19682835e-17 2.57836152e-13 6.80136748e-19], sum to 1.0000
[2019-04-04 00:01:28,181] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7312
[2019-04-04 00:01:28,273] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [11.06666666666667, 58.66666666666667, 0.0, 0.0, 22.0, 23.59655859879741, -0.01856196736712478, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 1618800.0000, 
sim time next is 1619400.0000, 
raw observation next is [10.78333333333333, 59.83333333333334, 0.0, 0.0, 22.0, 23.46730998849672, -0.02486344065819577, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.7613111726685133, 0.5983333333333334, 0.0, 0.0, 0.3333333333333333, 0.4556091657080599, 0.49171218644726805, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.294914], dtype=float32), -0.19231625]. 
=============================================
[2019-04-04 00:01:30,955] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.16283516e-20 2.13180485e-16 1.35795224e-13 1.00000000e+00
 5.05125388e-15 1.79945114e-09 1.18046482e-16], sum to 1.0000
[2019-04-04 00:01:30,955] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1663
[2019-04-04 00:01:31,079] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 73.0, 180.6666666666667, 69.33333333333334, 23.0, 22.20777969678779, -0.363524915626874, 0.0, 1.0, 27385.18279541245], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 1865400.0000, 
sim time next is 1866000.0000, 
raw observation next is [-4.5, 75.0, 183.3333333333333, 76.66666666666667, 23.0, 22.20220014627434, -0.363295018598042, 0.0, 1.0, 35243.70328431097], 
processed observation next is [0.0, 0.6086956521739131, 0.3379501385041552, 0.75, 0.6111111111111109, 0.0847145488029466, 0.4166666666666667, 0.3501833455228616, 0.3789016604673194, 0.0, 1.0, 0.1678271584967189], 
reward next is 0.8322, 
noisyNet noise sample is [array([-0.66078144], dtype=float32), 0.1934034]. 
=============================================
[2019-04-04 00:01:31,084] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[71.724884]
 [71.70215 ]
 [71.674934]
 [71.693535]
 [71.58788 ]], R is [[71.91895294]
 [72.0693512 ]
 [72.24198151]
 [72.35659027]
 [72.47484589]].
[2019-04-04 00:01:31,735] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.6600726e-21 1.4749747e-16 4.1008192e-14 1.0000000e+00 3.6369634e-15
 6.9147834e-09 1.4100424e-16], sum to 1.0000
[2019-04-04 00:01:31,736] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0104
[2019-04-04 00:01:31,819] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.3, 83.0, 122.5, 0.0, 23.0, 22.10409082164027, -0.3099537032394833, 0.0, 1.0, 31485.59816981677], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 1771200.0000, 
sim time next is 1771800.0000, 
raw observation next is [-2.383333333333333, 83.0, 123.6666666666667, 0.0, 23.0, 22.10872262756945, -0.3101951628719917, 0.0, 1.0, 33707.88441525917], 
processed observation next is [0.0, 0.5217391304347826, 0.3965835641735919, 0.83, 0.4122222222222223, 0.0, 0.4166666666666667, 0.3423935522974541, 0.3966016123760028, 0.0, 1.0, 0.16051373531075797], 
reward next is 0.8395, 
noisyNet noise sample is [array([0.9400174], dtype=float32), 1.8156357]. 
=============================================
[2019-04-04 00:01:37,236] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.5778385e-18 8.9827048e-15 9.0571951e-12 1.0000000e+00 1.8050997e-12
 2.7405468e-08 1.2019964e-13], sum to 1.0000
[2019-04-04 00:01:37,237] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8728
[2019-04-04 00:01:37,281] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.0, 86.0, 0.0, 0.0, 23.0, 22.41787791060882, -0.3194012201916954, 0.0, 1.0, 62188.4379816913], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 1807200.0000, 
sim time next is 1807800.0000, 
raw observation next is [-5.0, 85.33333333333334, 0.0, 0.0, 23.0, 22.38633354466347, -0.3201457277479948, 0.0, 1.0, 70363.138750304], 
processed observation next is [0.0, 0.9565217391304348, 0.32409972299168976, 0.8533333333333334, 0.0, 0.0, 0.4166666666666667, 0.3655277953886224, 0.39328475741733504, 0.0, 1.0, 0.33506256547763813], 
reward next is 0.6649, 
noisyNet noise sample is [array([-0.5796364], dtype=float32), 0.40686905]. 
=============================================
[2019-04-04 00:01:41,360] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.7213432e-22 8.5065276e-17 2.4413099e-14 1.0000000e+00 5.0167985e-17
 6.4741177e-09 3.9029285e-18], sum to 1.0000
[2019-04-04 00:01:41,361] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0073
[2019-04-04 00:01:41,398] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.416666666666667, 81.66666666666667, 132.0, 0.0, 23.0, 22.73488233810196, -0.3664252910695906, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 2027400.0000, 
sim time next is 2028000.0000, 
raw observation next is [-5.233333333333333, 80.33333333333334, 139.5, 0.0, 23.0, 22.68264327940041, -0.349274133664341, 1.0, 1.0, 196217.9094192961], 
processed observation next is [1.0, 0.4782608695652174, 0.31763619575253926, 0.8033333333333335, 0.465, 0.0, 0.4166666666666667, 0.39022027328336745, 0.383575288778553, 1.0, 1.0, 0.9343709972347434], 
reward next is 0.0656, 
noisyNet noise sample is [array([0.30901283], dtype=float32), -0.7684904]. 
=============================================
[2019-04-04 00:01:41,411] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[85.7002 ]
 [85.63036]
 [85.5971 ]
 [85.52839]
 [85.46688]], R is [[84.10600281]
 [84.26494598]
 [84.42229462]
 [84.57807159]
 [84.73229218]].
[2019-04-04 00:02:00,288] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.7273251e-25 3.6276051e-18 3.0953527e-15 1.0000000e+00 9.8230735e-20
 1.2873582e-11 5.4044326e-19], sum to 1.0000
[2019-04-04 00:02:00,288] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7049
[2019-04-04 00:02:00,319] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.2, 86.66666666666667, 0.0, 0.0, 24.0, 22.79024367609686, -0.2506414659154501, 0.0, 1.0, 44646.97500532381], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2259600.0000, 
sim time next is 2260200.0000, 
raw observation next is [-8.3, 86.83333333333333, 0.0, 0.0, 24.0, 22.7591357415609, -0.2508257817284509, 0.0, 1.0, 44630.19411200555], 
processed observation next is [1.0, 0.13043478260869565, 0.23268698060941828, 0.8683333333333333, 0.0, 0.0, 0.5, 0.39659464513007503, 0.41639140609051634, 0.0, 1.0, 0.2125247338666931], 
reward next is 0.7875, 
noisyNet noise sample is [array([0.52227503], dtype=float32), 1.3718967]. 
=============================================
[2019-04-04 00:02:19,675] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.6369195e-24 2.6562334e-16 2.2343946e-15 1.0000000e+00 1.2701684e-18
 8.4111659e-12 2.1406770e-18], sum to 1.0000
[2019-04-04 00:02:19,675] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6898
[2019-04-04 00:02:19,696] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.5, 81.5, 0.0, 0.0, 23.0, 22.44556961272729, -0.2469089316854673, 0.0, 1.0, 78254.82478082097], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 2932200.0000, 
sim time next is 2932800.0000, 
raw observation next is [-1.666666666666667, 82.66666666666667, 0.0, 0.0, 23.0, 22.4142947994749, -0.2441856929249012, 0.0, 1.0, 74786.26035180363], 
processed observation next is [1.0, 0.9565217391304348, 0.4164358264081256, 0.8266666666666667, 0.0, 0.0, 0.4166666666666667, 0.3678578999562416, 0.41860476902503296, 0.0, 1.0, 0.356125049294303], 
reward next is 0.6439, 
noisyNet noise sample is [array([-1.9385104], dtype=float32), 0.56935006]. 
=============================================
[2019-04-04 00:02:23,032] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.7009570e-26 2.8402884e-18 2.3131255e-16 1.0000000e+00 2.5558328e-21
 8.8134720e-14 1.6170500e-20], sum to 1.0000
[2019-04-04 00:02:23,032] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4313
[2019-04-04 00:02:23,077] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.8, 54.0, 13.5, 2.999999999999999, 24.0, 23.63724989221385, -0.1817205679448425, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2533200.0000, 
sim time next is 2533800.0000, 
raw observation next is [-2.8, 54.0, 21.0, 5.999999999999998, 24.0, 23.54873114244214, -0.1919822241887394, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.38504155124653744, 0.54, 0.07, 0.006629834254143645, 0.5, 0.46239426187017835, 0.4360059252704202, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.779416], dtype=float32), 0.866537]. 
=============================================
[2019-04-04 00:02:32,471] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.5633775e-25 1.3580790e-17 2.4456265e-17 1.0000000e+00 4.8070769e-19
 6.6714438e-12 2.4509182e-20], sum to 1.0000
[2019-04-04 00:02:32,472] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8163
[2019-04-04 00:02:32,481] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.716666666666666, 26.5, 163.0, 344.0, 23.0, 23.51363061017387, -0.1671374193755265, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 2556600.0000, 
sim time next is 2557200.0000, 
raw observation next is [3.633333333333334, 27.0, 161.0, 309.5, 23.0, 23.58914088221317, -0.157460496550499, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5632502308402586, 0.27, 0.5366666666666666, 0.3419889502762431, 0.4166666666666667, 0.46576174018443073, 0.4475131678165003, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([3.1371489], dtype=float32), 0.19287978]. 
=============================================
[2019-04-04 00:02:38,577] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.9019835e-25 3.0649368e-20 1.6459986e-17 1.0000000e+00 1.0551326e-20
 3.5342524e-13 4.1368973e-20], sum to 1.0000
[2019-04-04 00:02:38,577] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5788
[2019-04-04 00:02:38,615] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.0, 74.0, 0.0, 0.0, 23.0, 22.52054195521547, -0.3411960854855969, 0.0, 1.0, 20148.27355907443], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 2599800.0000, 
sim time next is 2600400.0000, 
raw observation next is [-5.0, 74.0, 0.0, 0.0, 23.0, 22.4684163180296, -0.351070962661284, 0.0, 1.0, 56658.68829305444], 
processed observation next is [1.0, 0.08695652173913043, 0.32409972299168976, 0.74, 0.0, 0.0, 0.4166666666666667, 0.3723680265024667, 0.382976345779572, 0.0, 1.0, 0.2698032775859735], 
reward next is 0.7302, 
noisyNet noise sample is [array([0.20308124], dtype=float32), 0.9454471]. 
=============================================
[2019-04-04 00:02:40,772] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.2286982e-23 1.4688810e-18 7.8897625e-17 1.0000000e+00 1.6992267e-17
 1.7496485e-13 6.6240748e-19], sum to 1.0000
[2019-04-04 00:02:40,772] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5429
[2019-04-04 00:02:40,835] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.3, 50.0, 50.5, 540.5, 23.0, 22.94234764505195, -0.2774239832690328, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 2451600.0000, 
sim time next is 2452200.0000, 
raw observation next is [-7.016666666666667, 48.83333333333334, 54.0, 582.0, 23.0, 22.99122547914588, -0.2748370287223452, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.2682363804247461, 0.48833333333333345, 0.18, 0.6430939226519337, 0.4166666666666667, 0.41593545659548986, 0.4083876570925516, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.41067538], dtype=float32), -0.45888677]. 
=============================================
[2019-04-04 00:03:03,590] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.5829002e-23 3.2002165e-17 5.9463142e-16 1.0000000e+00 1.8023145e-18
 1.7495284e-13 2.4868487e-19], sum to 1.0000
[2019-04-04 00:03:03,590] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0525
[2019-04-04 00:03:03,627] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.5, 57.5, 83.0, 667.0, 23.0, 22.53176148196409, -0.1964225571493078, 0.0, 1.0, 18693.25104905411], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 2993400.0000, 
sim time next is 2994000.0000, 
raw observation next is [-1.333333333333333, 56.66666666666667, 78.5, 634.8333333333334, 23.0, 22.54483340447217, -0.1957176418724177, 0.0, 1.0, 18693.01039091175], 
processed observation next is [0.0, 0.6521739130434783, 0.42566943674976926, 0.5666666666666668, 0.26166666666666666, 0.7014732965009208, 0.4166666666666667, 0.37873611703934734, 0.4347607860425275, 0.0, 1.0, 0.08901433519481786], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.30911526], dtype=float32), 1.0174547]. 
=============================================
[2019-04-04 00:03:03,653] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[83.15289 ]
 [83.13783 ]
 [83.094734]
 [83.07378 ]
 [83.11475 ]], R is [[83.0815506 ]
 [83.16172028]
 [83.24108124]
 [83.31964874]
 [83.39176941]].
[2019-04-04 00:03:04,759] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.6324983e-24 1.3610656e-18 8.8051245e-17 1.0000000e+00 1.0786366e-17
 2.0985012e-13 5.4217781e-19], sum to 1.0000
[2019-04-04 00:03:04,760] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6216
[2019-04-04 00:03:04,793] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.333333333333333, 81.66666666666667, 0.0, 0.0, 24.0, 22.99995086861933, -0.1330517830109947, 0.0, 1.0, 43698.28590849193], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2956800.0000, 
sim time next is 2957400.0000, 
raw observation next is [-3.5, 80.5, 0.0, 0.0, 24.0, 22.9786112854781, -0.1351178873761179, 0.0, 1.0, 43643.77690042953], 
processed observation next is [0.0, 0.21739130434782608, 0.36565096952908593, 0.805, 0.0, 0.0, 0.5, 0.41488427378984155, 0.4549607042079607, 0.0, 1.0, 0.20782750904966443], 
reward next is 0.7922, 
noisyNet noise sample is [array([-1.0783027], dtype=float32), -1.2401091]. 
=============================================
[2019-04-04 00:03:08,048] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.9982350e-23 3.6484098e-17 1.3351039e-15 1.0000000e+00 7.6008030e-18
 1.1142144e-13 2.8248154e-18], sum to 1.0000
[2019-04-04 00:03:08,049] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1001
[2019-04-04 00:03:08,076] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 24.0, 23.28252306885583, -0.0813086909106822, 0.0, 1.0, 47323.26802671398], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2762400.0000, 
sim time next is 2763000.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 24.0, 23.28292002945152, -0.08449144388924933, 0.0, 1.0, 46710.66259600733], 
processed observation next is [1.0, 1.0, 0.296398891966759, 0.59, 0.0, 0.0, 0.5, 0.44024333578762676, 0.47183618537025024, 0.0, 1.0, 0.22243172664765395], 
reward next is 0.7776, 
noisyNet noise sample is [array([1.0859832], dtype=float32), -1.2431555]. 
=============================================
[2019-04-04 00:03:08,127] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[79.620865]
 [79.549576]
 [79.29542 ]
 [79.34821 ]
 [79.07706 ]], R is [[79.72819519]
 [79.70556641]
 [79.67716217]
 [79.63305664]
 [79.5451889 ]].
[2019-04-04 00:03:39,198] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.2504451e-22 6.2804158e-16 3.5981055e-14 1.0000000e+00 2.5020763e-17
 1.4421284e-10 3.8109779e-17], sum to 1.0000
[2019-04-04 00:03:39,232] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1217
[2019-04-04 00:03:39,269] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.0, 64.0, 0.0, 0.0, 23.0, 22.2885521467064, -0.3801712099418479, 0.0, 1.0, 52134.30018648185], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 2783400.0000, 
sim time next is 2784000.0000, 
raw observation next is [-7.0, 64.0, 0.0, 0.0, 23.0, 22.30592670805448, -0.3775341590631909, 0.0, 1.0, 46108.41209359873], 
processed observation next is [1.0, 0.21739130434782608, 0.2686980609418283, 0.64, 0.0, 0.0, 0.4166666666666667, 0.35882722567120656, 0.3741552803122697, 0.0, 1.0, 0.2195638671123749], 
reward next is 0.7804, 
noisyNet noise sample is [array([1.6465204], dtype=float32), -0.2200287]. 
=============================================
[2019-04-04 00:03:39,413] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[79.07928 ]
 [79.06352 ]
 [78.9989  ]
 [79.007225]
 [79.083824]], R is [[79.07263947]
 [79.03365326]
 [78.93483734]
 [78.83307648]
 [78.87475586]].
[2019-04-04 00:03:55,515] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.3362568e-28 1.2396982e-20 6.6501527e-19 1.0000000e+00 1.6862433e-22
 1.8439514e-17 5.5680085e-22], sum to 1.0000
[2019-04-04 00:03:55,519] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9162
[2019-04-04 00:03:55,531] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.666666666666667, 100.0, 0.0, 0.0, 25.0, 24.88428302947937, 0.4908511658364763, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 3187200.0000, 
sim time next is 3187800.0000, 
raw observation next is [2.5, 100.0, 0.0, 0.0, 25.0, 24.91878345922447, 0.4906663622374616, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.5318559556786704, 1.0, 0.0, 0.0, 0.5833333333333334, 0.5765652882687059, 0.6635554540791538, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2889166], dtype=float32), -1.354059]. 
=============================================
[2019-04-04 00:03:58,366] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.1176849e-24 2.3081103e-16 2.4029601e-15 1.0000000e+00 1.5816722e-19
 1.1743281e-11 3.7733932e-18], sum to 1.0000
[2019-04-04 00:03:58,374] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4297
[2019-04-04 00:03:58,417] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.333333333333333, 60.0, 89.0, 461.3333333333333, 23.0, 23.41702893733501, -0.1347210093187893, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 3400800.0000, 
sim time next is 3401400.0000, 
raw observation next is [-1.166666666666667, 60.0, 91.0, 500.6666666666666, 23.0, 23.50096649468925, -0.1017945275660583, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.43028624192059095, 0.6, 0.30333333333333334, 0.5532228360957642, 0.4166666666666667, 0.4584138745574376, 0.4660684908113139, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.688406], dtype=float32), -0.27835858]. 
=============================================
[2019-04-04 00:03:58,525] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.6266843e-26 3.6443426e-20 1.4009425e-18 1.0000000e+00 1.8007252e-20
 5.0616843e-17 7.0554815e-21], sum to 1.0000
[2019-04-04 00:03:58,526] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0545
[2019-04-04 00:03:58,533] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.833333333333333, 59.66666666666667, 0.0, 0.0, 24.0, 23.11885712075324, -0.1306439770556236, 0.0, 1.0, 53181.20016866629], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3697800.0000, 
sim time next is 3698400.0000, 
raw observation next is [3.666666666666667, 60.33333333333334, 0.0, 0.0, 24.0, 23.07232773327629, -0.1249248925775386, 0.0, 1.0, 196217.9094192961], 
processed observation next is [0.0, 0.8260869565217391, 0.564173591874423, 0.6033333333333334, 0.0, 0.0, 0.5, 0.4226939777730241, 0.45835836914082045, 0.0, 1.0, 0.9343709972347434], 
reward next is 0.0656, 
noisyNet noise sample is [array([-0.28566223], dtype=float32), -0.20571688]. 
=============================================
[2019-04-04 00:04:02,969] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.2758782e-27 2.9568018e-19 1.8012286e-18 1.0000000e+00 1.9428839e-22
 1.1370585e-15 3.3320851e-22], sum to 1.0000
[2019-04-04 00:04:02,969] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0586
[2019-04-04 00:04:02,985] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 39.66666666666666, 46.66666666666667, 390.6666666666667, 24.0, 23.76171171446975, 0.05616645082922406, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3603000.0000, 
sim time next is 3603600.0000, 
raw observation next is [0.0, 39.0, 38.5, 328.5, 24.0, 23.72203641461812, 0.03995785184445771, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.7391304347826086, 0.46260387811634357, 0.39, 0.12833333333333333, 0.3629834254143646, 0.5, 0.47683636788484335, 0.5133192839481525, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.37087572], dtype=float32), -1.2142987]. 
=============================================
[2019-04-04 00:04:03,220] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.2776356e-24 2.2837611e-17 2.4148693e-15 1.0000000e+00 1.3088292e-18
 4.1136101e-11 3.2819580e-19], sum to 1.0000
[2019-04-04 00:04:03,221] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5591
[2019-04-04 00:04:03,273] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.5, 73.5, 104.0, 615.0, 23.0, 23.48544011763178, -0.09051807938034727, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 3317400.0000, 
sim time next is 3318000.0000, 
raw observation next is [-8.333333333333334, 72.33333333333333, 105.1666666666667, 635.8333333333333, 23.0, 23.50870706531924, -0.07711766003858618, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.23176361957525393, 0.7233333333333333, 0.3505555555555557, 0.7025782688766113, 0.4166666666666667, 0.4590589221099366, 0.4742941133204713, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4753774], dtype=float32), -0.6162904]. 
=============================================
[2019-04-04 00:04:03,283] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[88.91755 ]
 [88.71279 ]
 [88.600555]
 [88.45062 ]
 [88.132164]], R is [[89.06124878]
 [89.17063904]
 [89.27893066]
 [89.38613892]
 [89.49227905]].
[2019-04-04 00:04:04,535] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.3478044e-23 5.6725598e-17 9.1204730e-15 1.0000000e+00 2.9967629e-18
 4.8326024e-13 4.1064621e-18], sum to 1.0000
[2019-04-04 00:04:04,535] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8877
[2019-04-04 00:04:04,549] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 39.0, 0.0, 0.0, 23.0, 22.59583297841126, -0.2276911185095461, 0.0, 1.0, 36485.41631888047], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 4150800.0000, 
sim time next is 4151400.0000, 
raw observation next is [-1.166666666666667, 40.16666666666666, 0.0, 0.0, 23.0, 22.57994066619002, -0.2319901612677066, 0.0, 1.0, 42607.84085143828], 
processed observation next is [0.0, 0.043478260869565216, 0.43028624192059095, 0.40166666666666656, 0.0, 0.0, 0.4166666666666667, 0.3816617221825016, 0.42266994624409776, 0.0, 1.0, 0.2028944802449442], 
reward next is 0.7971, 
noisyNet noise sample is [array([0.11875951], dtype=float32), -0.47834578]. 
=============================================
[2019-04-04 00:04:08,919] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.8448249e-23 7.1842349e-16 7.0006193e-16 1.0000000e+00 4.4788451e-18
 2.0560412e-13 4.9258063e-19], sum to 1.0000
[2019-04-04 00:04:08,919] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1085
[2019-04-04 00:04:08,935] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 71.0, 0.0, 0.0, 23.0, 23.08381518544576, -0.1312269594129435, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 3781800.0000, 
sim time next is 3782400.0000, 
raw observation next is [-2.0, 71.0, 0.0, 0.0, 23.0, 22.95408087046744, -0.1541422233306238, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.40720221606648205, 0.71, 0.0, 0.0, 0.4166666666666667, 0.41284007253895333, 0.4486192588897921, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.06158994], dtype=float32), -1.099517]. 
=============================================
[2019-04-04 00:04:22,962] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.3383174e-27 6.5979196e-20 6.3790713e-17 1.0000000e+00 6.5918247e-22
 4.9527605e-15 2.7014445e-21], sum to 1.0000
[2019-04-04 00:04:22,963] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1973
[2019-04-04 00:04:22,988] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 23.0, 22.48915979166145, -0.2574341796849909, 0.0, 1.0, 50789.63036720732], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 3479400.0000, 
sim time next is 3480000.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 23.0, 22.53931441669179, -0.2515674319877953, 0.0, 1.0, 18744.56865813227], 
processed observation next is [1.0, 0.2608695652173913, 0.46260387811634357, 0.72, 0.0, 0.0, 0.4166666666666667, 0.3782762013909826, 0.41614418933740155, 0.0, 1.0, 0.08925985075301081], 
reward next is 0.9107, 
noisyNet noise sample is [array([1.6735972], dtype=float32), -0.004466108]. 
=============================================
[2019-04-04 00:04:22,999] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[87.30742 ]
 [87.17224 ]
 [87.025696]
 [86.97922 ]
 [86.98192 ]], R is [[87.39855194]
 [87.28271484]
 [87.09923553]
 [86.92549896]
 [86.90074158]].
[2019-04-04 00:04:25,556] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.6309960e-28 4.8994876e-22 2.1702113e-18 1.0000000e+00 7.5382400e-23
 6.5533833e-16 1.5683804e-21], sum to 1.0000
[2019-04-04 00:04:25,556] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3702
[2019-04-04 00:04:25,583] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 25.0, 24.27333827817185, 0.0965311957517047, 0.0, 1.0, 42897.94876621236], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 3735000.0000, 
sim time next is 3735600.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 25.0, 24.30741209894205, 0.0935642385574561, 0.0, 1.0, 42610.38782085636], 
processed observation next is [1.0, 0.21739130434782608, 0.3795013850415513, 0.65, 0.0, 0.0, 0.5833333333333334, 0.5256176749118374, 0.531188079519152, 0.0, 1.0, 0.2029066086707446], 
reward next is 0.7971, 
noisyNet noise sample is [array([-1.2101855], dtype=float32), 0.67405707]. 
=============================================
[2019-04-04 00:04:26,543] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-04 00:04:26,543] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 00:04:26,543] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 00:04:26,544] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:04:26,544] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:04:26,544] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 00:04:26,546] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:04:26,548] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run17
[2019-04-04 00:04:26,569] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run17
[2019-04-04 00:04:26,597] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run17
[2019-04-04 00:04:51,774] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.40754104], dtype=float32), 0.29582602]
[2019-04-04 00:04:51,775] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [11.4, 87.66666666666667, 0.0, 0.0, 20.0, 20.10633327205375, -0.7866384464864344, 0.0, 1.0, 0.0]
[2019-04-04 00:04:51,776] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 00:04:51,777] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [9.3324532e-22 4.1785708e-16 6.6234817e-15 1.0000000e+00 9.6116979e-16
 5.5984325e-11 2.6984425e-17], sampled 0.2147455945743152
[2019-04-04 00:04:58,639] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.40754104], dtype=float32), 0.29582602]
[2019-04-04 00:04:58,639] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [15.7, 72.0, 99.5, 212.8333333333333, 20.0, 22.40984017792214, -0.3401489468790218, 1.0, 0.0, 0.0]
[2019-04-04 00:04:58,639] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 00:04:58,640] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [9.1110053e-17 1.5107113e-11 4.3198314e-12 1.0000000e+00 1.3623971e-12
 3.2869865e-09 1.6236504e-14], sampled 0.9479928277539683
[2019-04-04 00:05:17,191] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.40754104], dtype=float32), 0.29582602]
[2019-04-04 00:05:17,191] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [0.5, 49.0, 141.6666666666667, 192.3333333333333, 19.0, 19.97179130507721, -1.070956665005701, 1.0, 1.0, 0.0]
[2019-04-04 00:05:17,191] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 00:05:17,192] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [8.1519976e-15 1.6223135e-10 8.7822923e-11 9.9992394e-01 8.6367331e-09
 7.5997974e-05 6.3429546e-13], sampled 0.08350948349528431
[2019-04-04 00:05:43,520] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5812.3658 154576343.7500 -1756.6129
[2019-04-04 00:05:51,592] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5235.6445 177528288.9448 -2624.9392
[2019-04-04 00:06:01,391] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5277.5015 196254965.0316 -2255.1713
[2019-04-04 00:06:02,415] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 1600000, evaluation results [1600000.0, 5235.644538084292, 177528288.94481263, -2624.9392322890817, 5812.365763067929, 154576343.74997193, -1756.6129477985123, 5277.501486081478, 196254965.0316347, -2255.1713192631096]
[2019-04-04 00:06:07,143] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.5785659e-29 4.2428554e-22 4.1847675e-20 1.0000000e+00 3.8147625e-23
 3.6140278e-18 6.2041674e-24], sum to 1.0000
[2019-04-04 00:06:07,144] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7791
[2019-04-04 00:06:07,151] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 34.0, 166.0, 758.0, 23.0, 22.45331035816401, -0.2206404191524359, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 4194000.0000, 
sim time next is 4194600.0000, 
raw observation next is [2.0, 35.0, 182.0, 728.3333333333333, 23.0, 22.46157927035352, -0.2199768451870234, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.518005540166205, 0.35, 0.6066666666666667, 0.8047882136279926, 0.4166666666666667, 0.3717982725294601, 0.42667438493765886, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3547012], dtype=float32), 0.9112564]. 
=============================================
[2019-04-04 00:06:07,269] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.0756016e-26 2.2959541e-18 2.6844703e-16 1.0000000e+00 1.5191474e-20
 2.2881697e-13 3.1154375e-19], sum to 1.0000
[2019-04-04 00:06:07,270] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9686
[2019-04-04 00:06:07,281] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 23.0, 22.56073915748011, -0.2045965346841029, 0.0, 1.0, 23211.64079370904], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 3891600.0000, 
sim time next is 3892200.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 23.0, 22.59545245582807, -0.1979361419032497, 0.0, 1.0, 18737.13726670246], 
processed observation next is [1.0, 0.043478260869565216, 0.40720221606648205, 0.65, 0.0, 0.0, 0.4166666666666667, 0.38295437131900584, 0.4340212860322501, 0.0, 1.0, 0.08922446317477362], 
reward next is 0.9108, 
noisyNet noise sample is [array([-1.087945], dtype=float32), 1.4554553]. 
=============================================
[2019-04-04 00:06:08,049] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.3822963e-26 3.5347504e-21 3.2693017e-18 1.0000000e+00 2.1484548e-20
 9.3109673e-16 7.6919719e-21], sum to 1.0000
[2019-04-04 00:06:08,049] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8361
[2019-04-04 00:06:08,062] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.466666666666667, 75.16666666666666, 0.0, 0.0, 24.0, 23.65096948511382, -0.007711961024334681, 0.0, 1.0, 23473.51370827555], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4315800.0000, 
sim time next is 4316400.0000, 
raw observation next is [4.4, 75.0, 0.0, 0.0, 24.0, 23.66925477496454, -0.001595047986244796, 0.0, 1.0, 18724.58340659316], 
processed observation next is [0.0, 1.0, 0.5844875346260389, 0.75, 0.0, 0.0, 0.5, 0.4724378979137116, 0.4994683173379184, 0.0, 1.0, 0.08916468288853885], 
reward next is 0.9108, 
noisyNet noise sample is [array([-0.9052992], dtype=float32), -1.114207]. 
=============================================
[2019-04-04 00:06:09,365] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.8125409e-24 1.2716270e-19 7.7414080e-18 1.0000000e+00 3.8347171e-18
 7.0125667e-16 9.2545749e-20], sum to 1.0000
[2019-04-04 00:06:09,365] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5226
[2019-04-04 00:06:09,378] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 23.0, 22.52298265435366, -0.3240489847370458, 0.0, 1.0, 51992.55376476211], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 4251000.0000, 
sim time next is 4251600.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 23.0, 22.52518358347351, -0.3170604244653428, 0.0, 1.0, 40797.65554527568], 
processed observation next is [0.0, 0.21739130434782608, 0.5457063711911359, 0.45, 0.0, 0.0, 0.4166666666666667, 0.3770986319561258, 0.3943131918448857, 0.0, 1.0, 0.19427455021559847], 
reward next is 0.8057, 
noisyNet noise sample is [array([1.5037645], dtype=float32), -1.0411851]. 
=============================================
[2019-04-04 00:06:09,831] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.8383077e-24 4.3505634e-17 3.3986204e-15 1.0000000e+00 3.6748632e-18
 5.0642173e-12 5.4554733e-18], sum to 1.0000
[2019-04-04 00:06:09,835] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6845
[2019-04-04 00:06:09,854] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.333333333333334, 36.33333333333333, 0.0, 0.0, 23.0, 22.3951819862299, -0.3499223409972592, 0.0, 1.0, 72073.8902495475], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 4077600.0000, 
sim time next is 4078200.0000, 
raw observation next is [-4.166666666666667, 35.16666666666667, 0.0, 0.0, 23.0, 22.37300696732915, -0.3271797458266592, 0.0, 1.0, 68716.54371061285], 
processed observation next is [1.0, 0.17391304347826086, 0.3471837488457987, 0.35166666666666674, 0.0, 0.0, 0.4166666666666667, 0.3644172472774292, 0.3909400847244469, 0.0, 1.0, 0.32722163671720406], 
reward next is 0.6728, 
noisyNet noise sample is [array([-2.3247678], dtype=float32), -0.853225]. 
=============================================
[2019-04-04 00:06:13,975] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.87054848e-25 4.33254209e-18 2.10439915e-15 1.00000000e+00
 1.04416204e-19 8.27106667e-13 3.99688399e-19], sum to 1.0000
[2019-04-04 00:06:13,978] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9975
[2019-04-04 00:06:14,000] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 71.0, 0.0, 0.0, 23.0, 22.50699494416896, -0.2043698371062973, 0.0, 1.0, 37367.53184666903], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 3798000.0000, 
sim time next is 3798600.0000, 
raw observation next is [-3.0, 71.0, 0.0, 0.0, 23.0, 22.5290009152809, -0.2068669872003945, 0.0, 1.0, 25649.8813674286], 
processed observation next is [1.0, 1.0, 0.3795013850415513, 0.71, 0.0, 0.0, 0.4166666666666667, 0.377416742940075, 0.4310443375998685, 0.0, 1.0, 0.12214229222585048], 
reward next is 0.8779, 
noisyNet noise sample is [array([0.0929492], dtype=float32), 0.15420367]. 
=============================================
[2019-04-04 00:06:17,947] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.1389148e-26 1.2137468e-19 1.5953930e-17 1.0000000e+00 2.1144927e-20
 5.7129059e-14 1.8010000e-20], sum to 1.0000
[2019-04-04 00:06:17,948] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9547
[2019-04-04 00:06:17,962] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 59.16666666666666, 0.0, 0.0, 23.0, 22.62178694453053, -0.08064696228353889, 0.0, 1.0, 33204.54057497959], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 3877800.0000, 
sim time next is 3878400.0000, 
raw observation next is [-1.0, 58.33333333333334, 0.0, 0.0, 23.0, 22.76476183706458, -0.06967888625849161, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.4349030470914128, 0.5833333333333335, 0.0, 0.0, 0.4166666666666667, 0.3970634864220483, 0.4767737045805028, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.34378088], dtype=float32), 1.3571215]. 
=============================================
[2019-04-04 00:06:21,707] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0006545e-26 7.3287747e-18 1.2535240e-17 1.0000000e+00 3.4903970e-22
 1.1063171e-14 1.1940943e-21], sum to 1.0000
[2019-04-04 00:06:21,708] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7351
[2019-04-04 00:06:21,726] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.3333333333333333, 58.66666666666666, 140.6666666666667, 681.0, 24.0, 25.10999610573301, 0.2952947810845172, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4615800.0000, 
sim time next is 4616400.0000, 
raw observation next is [0.6666666666666666, 57.33333333333334, 134.8333333333333, 724.0, 24.0, 25.16852440163206, 0.3130836400239649, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4810710987996307, 0.5733333333333335, 0.4494444444444443, 0.8, 0.5, 0.5973770334693382, 0.6043612133413216, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.9769485], dtype=float32), 2.2788627]. 
=============================================
[2019-04-04 00:06:23,967] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.8479049e-26 8.1688516e-19 2.2890816e-17 1.0000000e+00 5.9563563e-21
 7.5600706e-15 4.8027121e-20], sum to 1.0000
[2019-04-04 00:06:23,969] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7560
[2019-04-04 00:06:23,980] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [8.2, 30.0, 0.0, 0.0, 23.0, 23.63313713270524, 0.02454401093758643, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 5096400.0000, 
sim time next is 5097000.0000, 
raw observation next is [8.15, 32.5, 0.0, 0.0, 23.0, 23.60175899208209, 0.01100020243793585, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.6883656509695293, 0.325, 0.0, 0.0, 0.4166666666666667, 0.46681324934017415, 0.5036667341459786, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.25816265], dtype=float32), 0.055357568]. 
=============================================
[2019-04-04 00:06:23,995] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[85.78501 ]
 [85.20696 ]
 [84.664246]
 [84.159904]
 [83.710724]], R is [[86.49476624]
 [86.62982178]
 [86.76352692]
 [86.89588928]
 [87.02693176]].
[2019-04-04 00:06:24,001] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.1226059e-25 7.0100920e-20 5.1723943e-18 1.0000000e+00 7.5931117e-21
 3.4649384e-15 1.8665387e-20], sum to 1.0000
[2019-04-04 00:06:24,004] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3441
[2019-04-04 00:06:24,011] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [9.8, 60.0, 0.0, 0.0, 23.0, 24.42236591037606, 0.2571015142029393, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 4397400.0000, 
sim time next is 4398000.0000, 
raw observation next is [9.666666666666668, 60.33333333333333, 0.0, 0.0, 23.0, 24.36535753734251, 0.2463902984996732, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.7303785780240075, 0.6033333333333333, 0.0, 0.0, 0.4166666666666667, 0.5304464614452092, 0.5821300994998911, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.34599686], dtype=float32), -0.54237294]. 
=============================================
[2019-04-04 00:06:24,021] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[84.578766]
 [84.43386 ]
 [84.23558 ]
 [84.10815 ]
 [83.80253 ]], R is [[84.82263184]
 [84.97440338]
 [85.12465668]
 [85.27341461]
 [85.42068481]].
[2019-04-04 00:06:25,090] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:06:25,091] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:06:25,149] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run13
[2019-04-04 00:06:26,062] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.4554276e-28 6.0064137e-22 2.0429734e-20 1.0000000e+00 1.5121393e-21
 3.1271482e-15 2.1594301e-22], sum to 1.0000
[2019-04-04 00:06:26,063] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8593
[2019-04-04 00:06:26,087] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.666666666666667, 53.66666666666667, 185.6666666666667, 209.8333333333333, 24.0, 23.45403708907193, -0.09216020913414369, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4268400.0000, 
sim time next is 4269000.0000, 
raw observation next is [3.833333333333333, 53.83333333333333, 189.3333333333333, 288.6666666666666, 24.0, 23.40563399779835, -0.08208715333574844, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.5687903970452447, 0.5383333333333333, 0.631111111111111, 0.31896869244935533, 0.5, 0.45046949981652923, 0.47263761555475053, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.04285941], dtype=float32), 2.530554]. 
=============================================
[2019-04-04 00:06:26,179] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[95.1043  ]
 [94.612854]
 [94.24781 ]
 [93.89381 ]
 [93.55182 ]], R is [[95.68742371]
 [95.73055267]
 [95.77324677]
 [95.81551361]
 [95.85736084]].
[2019-04-04 00:06:27,967] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.1224567e-27 3.3839804e-20 3.7408023e-18 1.0000000e+00 7.3503464e-22
 1.9183005e-15 2.7230685e-21], sum to 1.0000
[2019-04-04 00:06:27,968] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3515
[2019-04-04 00:06:28,110] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 80.0, 0.0, 0.0, 23.0, 22.72234515426623, -0.1881076838559916, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 4431600.0000, 
sim time next is 4432200.0000, 
raw observation next is [2.0, 80.0, 0.0, 0.0, 23.0, 22.64265075954135, -0.1898085421628604, 0.0, 1.0, 160088.4615780564], 
processed observation next is [1.0, 0.30434782608695654, 0.518005540166205, 0.8, 0.0, 0.0, 0.4166666666666667, 0.38688756329511254, 0.43673048594571323, 0.0, 1.0, 0.7623260075145543], 
reward next is 0.2377, 
noisyNet noise sample is [array([-0.6055351], dtype=float32), -0.9871347]. 
=============================================
[2019-04-04 00:06:36,175] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.1065749e-21 2.0907166e-15 1.3281656e-14 1.0000000e+00 1.2698451e-16
 3.1258918e-10 6.1930256e-18], sum to 1.0000
[2019-04-04 00:06:36,175] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3346
[2019-04-04 00:06:36,183] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.833333333333333, 57.66666666666666, 186.6666666666667, 12.0, 23.0, 23.72866285691234, -0.05857150672809097, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 4531800.0000, 
sim time next is 4532400.0000, 
raw observation next is [2.0, 57.0, 169.5, 9.0, 23.0, 23.75883981831787, -0.05685018326170793, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.518005540166205, 0.57, 0.565, 0.009944751381215469, 0.4166666666666667, 0.47990331819315585, 0.48104993891276404, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8368761], dtype=float32), 0.7685515]. 
=============================================
[2019-04-04 00:06:42,359] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.6197691e-28 1.9044921e-21 1.4084605e-18 1.0000000e+00 1.6826577e-22
 5.3700008e-15 2.8753984e-21], sum to 1.0000
[2019-04-04 00:06:42,360] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7689
[2019-04-04 00:06:42,388] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.0, 67.0, 0.0, 0.0, 23.0, 22.71006556337841, -0.1083521127012978, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 4417200.0000, 
sim time next is 4417800.0000, 
raw observation next is [4.916666666666667, 67.0, 0.0, 0.0, 23.0, 22.72623719151051, -0.1134475134530795, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.5987996306555864, 0.67, 0.0, 0.0, 0.4166666666666667, 0.3938530992925425, 0.46218416218230685, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.06567416], dtype=float32), -1.1282487]. 
=============================================
[2019-04-04 00:06:43,861] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.2273115e-25 8.4728747e-18 1.6780165e-15 1.0000000e+00 1.1462891e-19
 8.1168735e-14 6.6382656e-19], sum to 1.0000
[2019-04-04 00:06:43,862] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6610
[2019-04-04 00:06:43,898] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.5, 73.5, 0.0, 0.0, 23.0, 22.47851576212436, -0.2609116452720613, 0.0, 1.0, 21027.70353831596], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 4600200.0000, 
sim time next is 4600800.0000, 
raw observation next is [-2.6, 74.0, 0.0, 0.0, 23.0, 22.54077638344745, -0.257588303072691, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.3905817174515236, 0.74, 0.0, 0.0, 0.4166666666666667, 0.3783980319539542, 0.414137232309103, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.29931858], dtype=float32), -0.77221966]. 
=============================================
[2019-04-04 00:06:52,375] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.7080259e-22 2.5551194e-15 2.4187981e-14 1.0000000e+00 1.9682756e-16
 9.2533909e-09 1.7642760e-17], sum to 1.0000
[2019-04-04 00:06:52,375] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9646
[2019-04-04 00:06:52,434] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 78.0, 52.66666666666666, 0.0, 23.0, 23.49321113984709, -0.08341976533360258, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 4465200.0000, 
sim time next is 4465800.0000, 
raw observation next is [0.0, 78.0, 49.0, 0.0, 23.0, 23.48799964277736, -0.1711913350007815, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.46260387811634357, 0.78, 0.16333333333333333, 0.0, 0.4166666666666667, 0.45733330356478, 0.44293622166640617, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.353433], dtype=float32), -1.3170962]. 
=============================================
[2019-04-04 00:07:04,360] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.5316325e-23 2.8850290e-17 2.1629992e-15 1.0000000e+00 2.1890187e-18
 8.6451979e-10 1.9264679e-17], sum to 1.0000
[2019-04-04 00:07:04,360] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1605
[2019-04-04 00:07:04,389] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.3, 68.0, 0.0, 0.0, 23.0, 22.66405723007941, -0.2353606868835509, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 4591800.0000, 
sim time next is 4592400.0000, 
raw observation next is [-1.366666666666667, 68.33333333333334, 0.0, 0.0, 23.0, 22.67156091119983, -0.2423496049809145, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.42474607571560485, 0.6833333333333335, 0.0, 0.0, 0.4166666666666667, 0.3892967425999858, 0.4192167983396951, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5803397], dtype=float32), -0.42734876]. 
=============================================
[2019-04-04 00:07:13,622] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:07:13,622] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:07:13,625] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run13
[2019-04-04 00:07:14,795] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.5169200e-26 2.6133543e-18 3.9898501e-17 1.0000000e+00 2.8479560e-20
 2.9139709e-12 2.9326490e-20], sum to 1.0000
[2019-04-04 00:07:14,795] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4736
[2019-04-04 00:07:14,852] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 25.0, 24.82308815947785, 0.3084667214781341, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 4562400.0000, 
sim time next is 4563000.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 25.0, 24.7798345602764, 0.2968651917648911, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.518005540166205, 0.52, 0.0, 0.0, 0.5833333333333334, 0.5649862133563666, 0.5989550639216303, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.22136249], dtype=float32), 1.2526981]. 
=============================================
[2019-04-04 00:07:14,863] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[91.441765]
 [92.368065]
 [92.78689 ]
 [93.24883 ]
 [92.74147 ]], R is [[91.89382172]
 [91.97488403]
 [92.05513763]
 [92.13459015]
 [92.14643097]].
[2019-04-04 00:07:15,912] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.6468424e-26 3.1460140e-17 5.5577125e-16 1.0000000e+00 2.1769418e-20
 1.3870929e-10 5.3664268e-19], sum to 1.0000
[2019-04-04 00:07:15,912] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9730
[2019-04-04 00:07:15,990] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 92.0, 71.66666666666666, 0.0, 23.0, 23.2749658060668, -0.1618126562225403, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 4696800.0000, 
sim time next is 4697400.0000, 
raw observation next is [0.0, 92.0, 80.33333333333333, 0.0, 23.0, 23.23506476733147, -0.1629983617025553, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.46260387811634357, 0.92, 0.2677777777777778, 0.0, 0.4166666666666667, 0.4362553972776224, 0.44566721276581495, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.08859255], dtype=float32), -0.37719965]. 
=============================================
[2019-04-04 00:07:22,842] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:07:22,842] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:07:22,849] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run13
[2019-04-04 00:07:26,198] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.97616446e-26 1.59082300e-17 1.89445417e-16 1.00000000e+00
 1.31420541e-21 2.34005066e-14 1.01551316e-19], sum to 1.0000
[2019-04-04 00:07:26,199] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4976
[2019-04-04 00:07:26,248] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.333333333333333, 40.16666666666667, 114.6666666666667, 772.0, 24.0, 25.10319424777379, 0.2912674339127916, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 5047800.0000, 
sim time next is 5048400.0000, 
raw observation next is [3.666666666666667, 39.33333333333334, 115.3333333333333, 790.5, 24.0, 25.21439752549892, 0.3158336267244141, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.564173591874423, 0.3933333333333334, 0.3844444444444443, 0.8734806629834254, 0.5, 0.6011997937915767, 0.6052778755748047, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.3445888], dtype=float32), 0.8059811]. 
=============================================
[2019-04-04 00:07:29,117] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:07:29,117] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:07:29,120] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run13
[2019-04-04 00:07:31,778] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:07:31,779] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:07:31,783] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run13
[2019-04-04 00:07:33,481] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:07:33,481] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:07:33,516] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run13
[2019-04-04 00:07:35,428] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.6272454e-23 4.5905755e-16 1.5548901e-14 1.0000000e+00 1.0806706e-18
 2.7029412e-12 2.8250956e-18], sum to 1.0000
[2019-04-04 00:07:35,428] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0428
[2019-04-04 00:07:35,474] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 53.0, 0.0, 0.0, 23.0, 24.16625313014902, 0.1348164122198449, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 4647600.0000, 
sim time next is 4648200.0000, 
raw observation next is [2.833333333333333, 52.83333333333334, 0.0, 0.0, 23.0, 24.0010701818919, 0.1157316719957129, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.541089566020314, 0.5283333333333334, 0.0, 0.0, 0.4166666666666667, 0.500089181824325, 0.538577223998571, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.96994084], dtype=float32), -0.20512931]. 
=============================================
[2019-04-04 00:07:39,697] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:07:39,697] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:07:39,701] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run13
[2019-04-04 00:07:40,738] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.3321406e-26 1.7195639e-18 3.7738127e-17 1.0000000e+00 4.9532715e-22
 1.6245717e-16 1.3817275e-21], sum to 1.0000
[2019-04-04 00:07:40,779] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0720
[2019-04-04 00:07:40,785] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [11.0, 17.0, 0.0, 0.0, 23.0, 25.74926171912026, 0.4779523195024207, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 5079600.0000, 
sim time next is 5080200.0000, 
raw observation next is [10.83333333333333, 17.33333333333334, 0.0, 0.0, 23.0, 25.67862993056513, 0.465679424199281, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.76269621421976, 0.1733333333333334, 0.0, 0.0, 0.4166666666666667, 0.6398858275470941, 0.6552264747330937, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.26907623], dtype=float32), 0.55117416]. 
=============================================
[2019-04-04 00:07:43,018] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:07:43,025] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:07:43,028] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run13
[2019-04-04 00:07:56,866] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:07:56,866] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:07:56,872] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run13
[2019-04-04 00:07:56,910] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:07:56,910] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:07:56,915] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run13
[2019-04-04 00:07:57,053] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:07:57,054] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:07:57,059] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run13
[2019-04-04 00:07:57,677] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:07:57,677] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:07:57,681] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run13
[2019-04-04 00:07:58,256] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:07:58,257] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:07:58,260] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run13
[2019-04-04 00:08:05,744] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:08:05,744] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:08:05,756] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run13
[2019-04-04 00:08:06,236] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:08:06,236] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:08:06,240] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run13
[2019-04-04 00:08:12,619] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:08:12,620] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:08:12,625] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run13
[2019-04-04 00:08:13,355] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.4124193e-21 3.4867881e-16 1.9845921e-15 1.0000000e+00 7.3566802e-16
 1.1036790e-09 9.8435320e-17], sum to 1.0000
[2019-04-04 00:08:13,367] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2637
[2019-04-04 00:08:13,381] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.2, 96.0, 0.0, 0.0, 19.0, 18.97235468660214, -1.072640072431482, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 6000.0000, 
sim time next is 6600.0000, 
raw observation next is [7.199999999999999, 96.0, 0.0, 0.0, 19.0, 18.982971795259, -1.077203294678529, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.043478260869565216, 0.662049861495845, 0.96, 0.0, 0.0, 0.08333333333333333, 0.08191431627158341, 0.140932235107157, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3693832], dtype=float32), 0.3129486]. 
=============================================
[2019-04-04 00:08:18,841] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.0517340e-17 1.5821179e-12 1.3603838e-09 9.9995780e-01 8.3688324e-13
 4.2255797e-05 7.9644830e-13], sum to 1.0000
[2019-04-04 00:08:18,841] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5090
[2019-04-04 00:08:18,881] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-12.9, 77.83333333333334, 0.0, 0.0, 23.0, 22.12908018022441, -0.4056382976116748, 0.0, 1.0, 58778.97465873478], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 335400.0000, 
sim time next is 336000.0000, 
raw observation next is [-13.0, 78.66666666666667, 0.0, 0.0, 23.0, 22.10287999805911, -0.4140060036438937, 0.0, 1.0, 52695.38217739692], 
processed observation next is [1.0, 0.9130434782608695, 0.10249307479224376, 0.7866666666666667, 0.0, 0.0, 0.4166666666666667, 0.34190666650492574, 0.36199799878536876, 0.0, 1.0, 0.2509303913209377], 
reward next is 0.7491, 
noisyNet noise sample is [array([-0.8881962], dtype=float32), -0.0042250194]. 
=============================================
[2019-04-04 00:08:18,885] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[74.59941]
 [74.32203]
 [74.03329]
 [74.08302]
 [73.63064]], R is [[74.48517609]
 [74.46042633]
 [74.35784149]
 [74.11739349]
 [74.26145935]].
[2019-04-04 00:08:26,523] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.3965452e-20 5.6387991e-15 1.6569379e-13 9.9999964e-01 1.7177999e-14
 3.2515922e-07 2.2120384e-15], sum to 1.0000
[2019-04-04 00:08:26,525] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9419
[2019-04-04 00:08:26,585] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.133333333333333, 80.16666666666667, 132.6666666666667, 462.3333333333334, 22.0, 21.21214394600453, -0.5571954360255141, 0.0, 1.0, 18694.73885491186], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 564600.0000, 
sim time next is 565200.0000, 
raw observation next is [-1.2, 80.0, 134.0, 495.5, 22.0, 21.19359245932265, -0.5592484293667732, 0.0, 1.0, 22617.76090817582], 
processed observation next is [0.0, 0.5652173913043478, 0.42936288088642666, 0.8, 0.44666666666666666, 0.5475138121546961, 0.3333333333333333, 0.2661327049435543, 0.31358385687774226, 0.0, 1.0, 0.10770362337226581], 
reward next is 0.8923, 
noisyNet noise sample is [array([2.0357628], dtype=float32), 0.26347286]. 
=============================================
[2019-04-04 00:08:29,955] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.0819754e-21 1.6701557e-14 2.8975652e-12 9.9999976e-01 8.6468517e-16
 2.1072307e-07 5.4451449e-16], sum to 1.0000
[2019-04-04 00:08:29,955] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9124
[2019-04-04 00:08:30,024] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.033333333333333, 77.0, 71.50000000000001, 49.33333333333332, 22.0, 22.0938969164055, -0.5170618202672418, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 206400.0000, 
sim time next is 207000.0000, 
raw observation next is [-7.85, 76.5, 79.0, 0.0, 22.0, 22.15349362827872, -0.5205725610644264, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.24515235457063714, 0.765, 0.2633333333333333, 0.0, 0.3333333333333333, 0.34612446902322674, 0.3264758129785245, 1.0, 1.0, 0.0], 
reward next is 0.7943, 
noisyNet noise sample is [array([1.4368542], dtype=float32), -0.6800817]. 
=============================================
[2019-04-04 00:08:30,054] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[82.45438 ]
 [82.24214 ]
 [81.748   ]
 [81.207344]
 [80.87752 ]], R is [[82.56127167]
 [82.56504059]
 [82.52989197]
 [82.46379089]
 [82.42050934]].
[2019-04-04 00:08:41,571] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.2203003e-19 1.8675395e-11 1.0798069e-10 9.9981052e-01 1.2033232e-14
 1.8947481e-04 1.5712773e-14], sum to 1.0000
[2019-04-04 00:08:41,571] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6164
[2019-04-04 00:08:41,670] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-9.5, 44.0, 88.5, 627.0, 22.0, 22.79722712081787, -0.3028061188608848, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 309600.0000, 
sim time next is 310200.0000, 
raw observation next is [-9.5, 43.66666666666667, 86.33333333333333, 625.6666666666666, 22.0, 22.64312683049432, -0.3328236749113516, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.1994459833795014, 0.4366666666666667, 0.28777777777777774, 0.6913443830570902, 0.3333333333333333, 0.38692723587452676, 0.3890587750295495, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.18931453], dtype=float32), -0.16462621]. 
=============================================
[2019-04-04 00:08:45,356] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.8484786e-17 2.1514006e-11 8.8229751e-10 9.9937063e-01 3.9896219e-13
 6.2935118e-04 1.8227546e-12], sum to 1.0000
[2019-04-04 00:08:45,356] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8381
[2019-04-04 00:08:45,409] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-11.7, 57.0, 0.0, 0.0, 23.0, 22.96606028212154, -0.2331096888532833, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 324000.0000, 
sim time next is 324600.0000, 
raw observation next is [-11.8, 58.0, 0.0, 0.0, 23.0, 23.07023099590297, -0.2331491348336711, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.13573407202216065, 0.58, 0.0, 0.0, 0.4166666666666667, 0.42251924965858095, 0.4222836217221096, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5422995], dtype=float32), -0.68933976]. 
=============================================
[2019-04-04 00:09:02,448] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.4198590e-25 9.3916896e-17 3.2048377e-15 1.0000000e+00 1.2020616e-19
 6.7751208e-12 3.9024797e-19], sum to 1.0000
[2019-04-04 00:09:02,448] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6513
[2019-04-04 00:09:02,454] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.883333333333334, 96.33333333333334, 101.6666666666667, 0.0, 24.0, 23.6905304893437, -0.1276678824854921, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 907800.0000, 
sim time next is 908400.0000, 
raw observation next is [3.066666666666667, 95.66666666666667, 102.8333333333333, 0.0, 24.0, 23.70619212250866, -0.1361014579475901, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.5475530932594646, 0.9566666666666667, 0.3427777777777777, 0.0, 0.5, 0.47551601020905504, 0.4546328473508033, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.10920893], dtype=float32), -0.0071302284]. 
=============================================
[2019-04-04 00:09:08,865] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.8164451e-26 2.6711806e-16 3.9134727e-16 1.0000000e+00 2.4775113e-20
 7.1202124e-09 8.3430378e-19], sum to 1.0000
[2019-04-04 00:09:08,910] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2945
[2019-04-04 00:09:08,931] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [10.0, 92.0, 43.5, 0.0, 24.0, 24.03154676685271, 0.07405315271730854, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 982800.0000, 
sim time next is 983400.0000, 
raw observation next is [10.08333333333333, 92.16666666666667, 49.00000000000001, 0.0, 24.0, 24.13793918234845, 0.07934813980191818, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.7419205909510619, 0.9216666666666667, 0.16333333333333336, 0.0, 0.5, 0.5114949318623708, 0.5264493799339728, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.97382027], dtype=float32), 0.28813803]. 
=============================================
[2019-04-04 00:09:22,462] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.6175095e-24 1.4739657e-19 6.2997075e-16 1.0000000e+00 1.6774299e-20
 3.4368434e-13 7.9917750e-19], sum to 1.0000
[2019-04-04 00:09:22,462] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9347
[2019-04-04 00:09:22,534] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.6666666666666667, 82.33333333333334, 87.0, 136.0, 26.0, 24.88267499145611, 0.2643393816656686, 0.0, 1.0, 30607.10606677158], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 559200.0000, 
sim time next is 559800.0000, 
raw observation next is [-0.7, 82.0, 89.0, 135.0, 26.0, 24.85067338292065, 0.2608210934166168, 0.0, 1.0, 51552.5123980178], 
processed observation next is [0.0, 0.4782608695652174, 0.443213296398892, 0.82, 0.2966666666666667, 0.14917127071823205, 0.6666666666666666, 0.5708894485767209, 0.5869403644722057, 0.0, 1.0, 0.24548815427627527], 
reward next is 0.7545, 
noisyNet noise sample is [array([-1.3037782], dtype=float32), 1.1193374]. 
=============================================
[2019-04-04 00:09:22,708] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.6344562e-24 8.7025704e-18 1.7331525e-15 1.0000000e+00 2.2435386e-18
 4.6997028e-10 4.8064376e-18], sum to 1.0000
[2019-04-04 00:09:22,708] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4764
[2019-04-04 00:09:22,729] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [11.6, 81.0, 0.0, 0.0, 24.0, 23.72455749087871, 0.1364333287541376, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1143600.0000, 
sim time next is 1144200.0000, 
raw observation next is [11.6, 82.0, 0.0, 0.0, 24.0, 23.67091409356116, 0.1407744796112974, 0.0, 1.0, 133929.3112400427], 
processed observation next is [0.0, 0.21739130434782608, 0.7839335180055402, 0.82, 0.0, 0.0, 0.5, 0.47257617446342987, 0.5469248265370991, 0.0, 1.0, 0.6377586249525843], 
reward next is 0.3622, 
noisyNet noise sample is [array([0.5344515], dtype=float32), 0.47864166]. 
=============================================
[2019-04-04 00:09:40,776] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.18654698e-33 5.83740460e-22 1.77071823e-21 1.00000000e+00
 1.21716245e-29 3.31636090e-22 5.49926647e-26], sum to 1.0000
[2019-04-04 00:09:40,776] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2097
[2019-04-04 00:09:40,832] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [17.98333333333333, 49.83333333333334, 23.66666666666667, 0.9999999999999998, 25.0, 26.92365368530732, 0.8068470959706122, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 1097400.0000, 
sim time next is 1098000.0000, 
raw observation next is [17.7, 50.0, 18.0, 1.5, 25.0, 27.09582810632589, 0.8255167178219133, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.9529085872576178, 0.5, 0.06, 0.0016574585635359116, 0.5833333333333334, 0.7579856755271575, 0.7751722392739712, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.19633585], dtype=float32), -0.62999326]. 
=============================================
[2019-04-04 00:09:40,900] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[80.463776]
 [80.690735]
 [80.80531 ]
 [80.780594]
 [80.88298 ]], R is [[80.27495575]
 [80.47220612]
 [80.6674881 ]
 [80.86081696]
 [81.05220795]].
[2019-04-04 00:10:00,049] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.7144749e-30 3.3856426e-22 1.0298685e-21 1.0000000e+00 1.6445859e-25
 5.7676861e-20 5.0865388e-24], sum to 1.0000
[2019-04-04 00:10:00,049] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1837
[2019-04-04 00:10:00,065] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [15.16666666666667, 95.0, 0.0, 0.0, 23.0, 21.9806267634576, -0.2298081004936438, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 1226400.0000, 
sim time next is 1227000.0000, 
raw observation next is [15.08333333333333, 95.5, 0.0, 0.0, 23.0, 21.95898046601255, -0.2331410771049282, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.17391304347826086, 0.8804247460757156, 0.955, 0.0, 0.0, 0.4166666666666667, 0.3299150388343793, 0.42228630763169056, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.34555092], dtype=float32), -0.2715688]. 
=============================================
[2019-04-04 00:10:00,171] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[80.86369 ]
 [80.93377 ]
 [81.01571 ]
 [81.095894]
 [81.18323 ]], R is [[80.98062897]
 [81.17082214]
 [81.3591156 ]
 [81.5455246 ]
 [81.73007202]].
[2019-04-04 00:10:04,846] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0014043e-23 4.3228081e-15 2.7491555e-14 9.9999988e-01 5.9394582e-19
 9.7512249e-08 3.0430837e-18], sum to 1.0000
[2019-04-04 00:10:04,846] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2152
[2019-04-04 00:10:04,994] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.300000000000001, 79.0, 149.3333333333333, 0.0, 26.0, 24.7429061380336, 0.2739097431345742, 1.0, 1.0, 198788.4873447882], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2035200.0000, 
sim time next is 2035800.0000, 
raw observation next is [-4.2, 79.0, 148.0, 0.0, 26.0, 24.26805091679549, 0.3236295589390896, 1.0, 1.0, 200285.758482561], 
processed observation next is [1.0, 0.5652173913043478, 0.34626038781163443, 0.79, 0.49333333333333335, 0.0, 0.6666666666666666, 0.5223375763996243, 0.6078765196463632, 1.0, 1.0, 0.9537417070598143], 
reward next is 0.0463, 
noisyNet noise sample is [array([-0.66573334], dtype=float32), -1.000787]. 
=============================================
[2019-04-04 00:10:06,748] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.4084574e-25 3.1519448e-19 5.0587309e-17 1.0000000e+00 1.2517785e-19
 1.6527391e-11 5.7211863e-20], sum to 1.0000
[2019-04-04 00:10:06,748] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8044
[2019-04-04 00:10:06,777] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [11.6, 79.0, 0.0, 0.0, 24.0, 23.90326859138229, 0.185147617314561, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1142400.0000, 
sim time next is 1143000.0000, 
raw observation next is [11.6, 80.0, 0.0, 0.0, 24.0, 23.88248604228933, 0.1774890562790125, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.21739130434782608, 0.7839335180055402, 0.8, 0.0, 0.0, 0.5, 0.4902071701907775, 0.5591630187596709, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.0023642], dtype=float32), -0.028530734]. 
=============================================
[2019-04-04 00:10:06,809] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[94.784004]
 [94.6061  ]
 [94.56602 ]
 [94.48792 ]
 [94.392044]], R is [[95.05318451]
 [95.1026535 ]
 [95.15162659]
 [95.20011139]
 [95.24810791]].
[2019-04-04 00:10:10,598] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.0556825e-32 4.7868154e-21 1.3871362e-20 1.0000000e+00 2.7948441e-27
 8.8605565e-16 2.7939637e-24], sum to 1.0000
[2019-04-04 00:10:10,654] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2641
[2019-04-04 00:10:10,680] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [14.2, 76.0, 0.0, 0.0, 26.0, 26.09314054369578, 0.6230632514625386, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1041600.0000, 
sim time next is 1042200.0000, 
raw observation next is [14.1, 76.5, 0.0, 0.0, 26.0, 26.05494603747082, 0.6116059808938309, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.8531855955678671, 0.765, 0.0, 0.0, 0.6666666666666666, 0.6712455031225684, 0.7038686602979437, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.07324927], dtype=float32), 1.1927938]. 
=============================================
[2019-04-04 00:10:41,349] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.5928242e-24 1.1010160e-15 3.0334418e-14 1.0000000e+00 9.1441392e-20
 1.1883760e-09 1.0570801e-17], sum to 1.0000
[2019-04-04 00:10:41,357] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0172
[2019-04-04 00:10:41,398] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.2, 95.0, 0.0, 0.0, 24.0, 23.54924015435718, 0.03576263900179186, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1481400.0000, 
sim time next is 1482000.0000, 
raw observation next is [2.2, 95.33333333333334, 0.0, 0.0, 24.0, 23.60716220462358, 0.03049850894731728, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.5235457063711911, 0.9533333333333335, 0.0, 0.0, 0.5, 0.46726351705196506, 0.5101661696491058, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6986453], dtype=float32), 0.6348911]. 
=============================================
[2019-04-04 00:10:41,469] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[83.326   ]
 [83.095024]
 [83.248   ]
 [83.49048 ]
 [83.62284 ]], R is [[83.48593903]
 [83.65107727]
 [83.43661499]
 [83.24100494]
 [83.1638031 ]].
[2019-04-04 00:10:42,397] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.0645438e-24 2.1894450e-14 8.0975949e-13 1.0000000e+00 4.0207534e-20
 2.8279212e-10 2.5439419e-17], sum to 1.0000
[2019-04-04 00:10:42,397] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4201
[2019-04-04 00:10:42,413] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.183333333333333, 92.0, 0.0, 0.0, 26.0, 25.48572265482082, 0.4890197058090386, 0.0, 1.0, 53211.39032144423], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1462200.0000, 
sim time next is 1462800.0000, 
raw observation next is [1.266666666666667, 92.0, 0.0, 0.0, 26.0, 25.44637543739728, 0.4826488303519598, 0.0, 1.0, 62796.20467640156], 
processed observation next is [1.0, 0.9565217391304348, 0.4976915974145891, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6205312864497733, 0.6608829434506532, 0.0, 1.0, 0.29902954607810267], 
reward next is 0.7010, 
noisyNet noise sample is [array([-1.0532696], dtype=float32), -0.9991501]. 
=============================================
[2019-04-04 00:10:45,406] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.8031738e-32 2.7221545e-21 8.9328898e-21 1.0000000e+00 6.0210117e-27
 1.0732083e-14 9.8147707e-25], sum to 1.0000
[2019-04-04 00:10:45,407] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5190
[2019-04-04 00:10:45,459] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [9.066666666666666, 66.33333333333334, 83.33333333333334, 694.6666666666667, 25.0, 25.64733894796686, 0.5059103614124297, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 1518000.0000, 
sim time next is 1518600.0000, 
raw observation next is [9.533333333333333, 64.66666666666666, 81.66666666666666, 688.3333333333333, 25.0, 25.77500415393276, 0.5311113858274438, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.7266851338873501, 0.6466666666666666, 0.2722222222222222, 0.7605893186003683, 0.5833333333333334, 0.64791701282773, 0.677037128609148, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.37715924], dtype=float32), 0.46500117]. 
=============================================
[2019-04-04 00:10:47,676] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.5324528e-29 1.6729246e-20 3.3808226e-18 1.0000000e+00 9.5502528e-25
 1.2258013e-14 4.7572680e-22], sum to 1.0000
[2019-04-04 00:10:47,677] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4549
[2019-04-04 00:10:47,719] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.2, 93.66666666666667, 0.0, 0.0, 26.0, 25.4393909711956, 0.4595478879379597, 0.0, 1.0, 38981.04016386138], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1479000.0000, 
sim time next is 1479600.0000, 
raw observation next is [2.2, 94.0, 0.0, 0.0, 26.0, 25.38788867884931, 0.4529513200523902, 0.0, 1.0, 63587.78607616605], 
processed observation next is [1.0, 0.13043478260869565, 0.5235457063711911, 0.94, 0.0, 0.0, 0.6666666666666666, 0.6156573899041092, 0.6509837733507967, 0.0, 1.0, 0.30279898131507643], 
reward next is 0.6972, 
noisyNet noise sample is [array([-0.41336468], dtype=float32), 1.645809]. 
=============================================
[2019-04-04 00:11:04,018] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.8032487e-26 9.3199538e-17 2.9471910e-15 1.0000000e+00 1.9707350e-21
 2.6337651e-11 3.2363555e-19], sum to 1.0000
[2019-04-04 00:11:04,018] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9322
[2019-04-04 00:11:04,092] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.633333333333333, 81.0, 89.0, 50.5, 25.0, 24.8009233179375, 0.1385425976936138, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 2104800.0000, 
sim time next is 2105400.0000, 
raw observation next is [-7.716666666666667, 81.5, 106.0, 63.99999999999999, 25.0, 24.71904535141054, 0.1472432138848429, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.24884579870729456, 0.815, 0.35333333333333333, 0.07071823204419889, 0.5833333333333334, 0.5599204459508783, 0.5490810712949477, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1748817], dtype=float32), 0.3661405]. 
=============================================
[2019-04-04 00:11:31,848] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.1775889e-28 8.5913408e-19 1.8312574e-16 1.0000000e+00 4.0580681e-23
 3.3821260e-14 4.7360152e-20], sum to 1.0000
[2019-04-04 00:11:31,848] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8356
[2019-04-04 00:11:31,914] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.75278610295399, 0.2482569066099388, 0.0, 1.0, 42516.48711759126], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2768400.0000, 
sim time next is 2769000.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.7322259131166, 0.2515767992983295, 0.0, 1.0, 42376.02429465584], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5610188260930501, 0.5838589330994431, 0.0, 1.0, 0.20179059187931353], 
reward next is 0.7982, 
noisyNet noise sample is [array([-0.5321576], dtype=float32), 0.5770486]. 
=============================================
[2019-04-04 00:11:31,991] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[88.45686 ]
 [88.418915]
 [88.408676]
 [88.42002 ]
 [88.46913 ]], R is [[88.50347137]
 [88.41597748]
 [88.3285141 ]
 [88.24085236]
 [88.15300751]].
[2019-04-04 00:11:35,665] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.33179789e-22 1.21517684e-14 4.41313956e-12 1.00000000e+00
 2.89860862e-17 1.76006250e-08 2.29983209e-15], sum to 1.0000
[2019-04-04 00:11:35,665] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5469
[2019-04-04 00:11:35,709] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.2, 87.66666666666667, 0.0, 0.0, 24.0, 23.09131632150602, -0.1694751558219324, 0.0, 1.0, 44369.49713526303], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2085600.0000, 
sim time next is 2086200.0000, 
raw observation next is [-5.3, 88.5, 0.0, 0.0, 24.0, 23.0875068429721, -0.1800824897813214, 0.0, 1.0, 44430.94826436666], 
processed observation next is [1.0, 0.13043478260869565, 0.31578947368421056, 0.885, 0.0, 0.0, 0.5, 0.42395890358100835, 0.43997250340622623, 0.0, 1.0, 0.21157594411603173], 
reward next is 0.7884, 
noisyNet noise sample is [array([-1.1027802], dtype=float32), -1.9217234]. 
=============================================
[2019-04-04 00:11:42,761] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.78671681e-22 1.34652065e-14 2.27973565e-13 1.00000000e+00
 1.25072016e-17 6.27291952e-09 8.25173144e-17], sum to 1.0000
[2019-04-04 00:11:42,784] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7927
[2019-04-04 00:11:42,826] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.0, 70.0, 0.0, 0.0, 25.0, 24.15508493216317, 0.1584331976888295, 0.0, 1.0, 161522.6182168943], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 2233200.0000, 
sim time next is 2233800.0000, 
raw observation next is [-5.0, 69.5, 0.0, 0.0, 25.0, 24.15292763705778, 0.1746191414286122, 0.0, 1.0, 98325.0887517393], 
processed observation next is [1.0, 0.8695652173913043, 0.32409972299168976, 0.695, 0.0, 0.0, 0.5833333333333334, 0.512743969754815, 0.5582063804762041, 0.0, 1.0, 0.4682147083416157], 
reward next is 0.5318, 
noisyNet noise sample is [array([0.86986876], dtype=float32), -0.05334718]. 
=============================================
[2019-04-04 00:12:26,213] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.2396744e-24 8.2965912e-15 1.5415154e-15 1.0000000e+00 5.1621416e-19
 2.3242496e-11 3.0566715e-19], sum to 1.0000
[2019-04-04 00:12:26,213] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5565
[2019-04-04 00:12:26,324] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.3, 29.0, 121.5, 338.3333333333333, 24.0, 24.35976618305367, 0.02676630047139286, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2560800.0000, 
sim time next is 2561400.0000, 
raw observation next is [3.3, 29.0, 114.0, 351.0, 24.0, 24.33659462107448, 0.03146243408771825, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.554016620498615, 0.29, 0.38, 0.3878453038674033, 0.5, 0.5280495517562066, 0.5104874780292394, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.19540392], dtype=float32), -0.95055604]. 
=============================================
[2019-04-04 00:12:27,697] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1516064e-25 9.4209234e-19 3.1889563e-17 1.0000000e+00 1.1617089e-20
 2.0793954e-14 5.0160252e-20], sum to 1.0000
[2019-04-04 00:12:27,697] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1892
[2019-04-04 00:12:27,777] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.74942145749847, 0.2361187200820852, 0.0, 1.0, 39074.71739689349], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2341200.0000, 
sim time next is 2341800.0000, 
raw observation next is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.72768656070928, 0.2266493304233598, 0.0, 1.0, 39179.86495712556], 
processed observation next is [0.0, 0.08695652173913043, 0.3988919667590028, 0.62, 0.0, 0.0, 0.6666666666666666, 0.5606405467257733, 0.5755497768077866, 0.0, 1.0, 0.1865707855101217], 
reward next is 0.8134, 
noisyNet noise sample is [array([0.35006344], dtype=float32), 0.80685467]. 
=============================================
[2019-04-04 00:12:27,863] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.6427183e-24 6.1515683e-16 1.4671032e-14 1.0000000e+00 4.3970569e-20
 4.4577970e-13 4.8624638e-18], sum to 1.0000
[2019-04-04 00:12:27,863] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1845
[2019-04-04 00:12:27,887] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.8, 56.0, 0.0, 0.0, 25.0, 24.58708473765512, 0.2166080713425397, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 2581800.0000, 
sim time next is 2582400.0000, 
raw observation next is [-2.8, 56.0, 0.0, 0.0, 25.0, 24.59238779889948, 0.2069335134329517, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.38504155124653744, 0.56, 0.0, 0.0, 0.5833333333333334, 0.5493656499082901, 0.5689778378109839, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.93569], dtype=float32), -0.14778507]. 
=============================================
[2019-04-04 00:12:29,649] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-04 00:12:29,653] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 00:12:29,655] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:12:29,656] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run18
[2019-04-04 00:12:29,677] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 00:12:29,680] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 00:12:29,681] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:12:29,683] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run18
[2019-04-04 00:12:29,702] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:12:29,705] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run18
[2019-04-04 00:13:30,621] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.1755727], dtype=float32), 0.16247801]
[2019-04-04 00:13:30,621] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [3.766666666666667, 61.0, 0.0, 0.0, 22.0, 21.19904169418903, -0.5680115787603258, 0.0, 1.0, 35143.21633923615]
[2019-04-04 00:13:30,621] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 00:13:30,622] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.3767724e-18 2.0664594e-12 2.7468874e-12 9.9999940e-01 8.4212699e-13
 5.7049846e-07 7.4805164e-15], sampled 0.571328422167236
[2019-04-04 00:14:03,462] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7178.0129 181444512.7432 -707.2861
[2019-04-04 00:14:25,596] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 6934.7374 224034951.2248 -1072.1081
[2019-04-04 00:14:27,896] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7364.7509 223068162.2461 -420.3982
[2019-04-04 00:14:28,921] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 1700000, evaluation results [1700000.0, 7364.750927367178, 223068162.2461488, -420.3982035470716, 7178.012938304872, 181444512.74320635, -707.28609655625, 6934.737387508279, 224034951.2247661, -1072.108073003586]
[2019-04-04 00:14:53,078] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.1221212e-28 7.7175163e-20 1.0964673e-17 1.0000000e+00 6.8233574e-25
 3.5308610e-18 2.8000803e-22], sum to 1.0000
[2019-04-04 00:14:53,092] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1864
[2019-04-04 00:14:53,121] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 50.0, 64.66666666666667, 507.3333333333333, 26.0, 26.46902547399803, 0.4400294537078733, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2736600.0000, 
sim time next is 2737200.0000, 
raw observation next is [-3.0, 50.0, 59.33333333333333, 480.6666666666667, 26.0, 26.33701207124362, 0.5544268880441695, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.3795013850415513, 0.5, 0.19777777777777777, 0.5311233885819522, 0.6666666666666666, 0.6947510059369684, 0.6848089626813899, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0203354], dtype=float32), 1.1265075]. 
=============================================
[2019-04-04 00:14:53,882] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2771902e-24 1.1697144e-18 2.4656459e-16 1.0000000e+00 6.1465129e-20
 7.2551074e-13 4.7913873e-19], sum to 1.0000
[2019-04-04 00:14:53,882] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9009
[2019-04-04 00:14:53,894] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 25.0, 24.21605761092307, 0.09030877671825506, 0.0, 1.0, 38702.9410950251], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 3023400.0000, 
sim time next is 3024000.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 25.0, 24.17595007140545, 0.08350109684284117, 0.0, 1.0, 38659.53500635647], 
processed observation next is [0.0, 0.0, 0.3518005540166205, 0.65, 0.0, 0.0, 0.5833333333333334, 0.514662505950454, 0.5278336989476137, 0.0, 1.0, 0.1840930238397927], 
reward next is 0.8159, 
noisyNet noise sample is [array([-0.5924074], dtype=float32), -0.19579978]. 
=============================================
[2019-04-04 00:14:53,910] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[86.22878 ]
 [86.18691 ]
 [86.121254]
 [86.02358 ]
 [85.89169 ]], R is [[86.56867218]
 [86.51868439]
 [86.46879578]
 [86.4190979 ]
 [86.36927032]].
[2019-04-04 00:14:58,022] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.7405020e-30 1.2415994e-22 9.5827562e-20 1.0000000e+00 3.6384437e-25
 2.6288625e-18 1.8167090e-23], sum to 1.0000
[2019-04-04 00:14:58,022] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4630
[2019-04-04 00:14:58,061] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.333333333333333, 51.66666666666666, 113.1666666666667, 815.1666666666667, 25.0, 24.17191922751417, 0.1578515453587236, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 3069600.0000, 
sim time next is 3070200.0000, 
raw observation next is [-2.166666666666667, 50.83333333333334, 112.3333333333333, 813.3333333333334, 25.0, 24.19690106976057, 0.1597575178421742, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.4025854108956602, 0.5083333333333334, 0.37444444444444436, 0.8987108655616943, 0.5833333333333334, 0.5164084224800476, 0.5532525059473914, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.56958354], dtype=float32), -1.7505528]. 
=============================================
[2019-04-04 00:14:59,866] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.9118430e-22 1.9951868e-17 5.1630190e-15 1.0000000e+00 1.8531468e-17
 2.7673313e-11 1.0858868e-16], sum to 1.0000
[2019-04-04 00:14:59,869] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2368
[2019-04-04 00:14:59,895] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 77.0, 0.0, 0.0, 24.0, 22.83857298714479, -0.1857627407938076, 0.0, 1.0, 55212.476779515], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2960400.0000, 
sim time next is 2961000.0000, 
raw observation next is [-4.0, 77.0, 0.0, 0.0, 24.0, 22.77482504955494, -0.1982336986762712, 0.0, 1.0, 59173.82696806929], 
processed observation next is [0.0, 0.2608695652173913, 0.3518005540166205, 0.77, 0.0, 0.0, 0.5, 0.3979020874629118, 0.43392210044124296, 0.0, 1.0, 0.28178012841937755], 
reward next is 0.7182, 
noisyNet noise sample is [array([0.4701177], dtype=float32), -0.993705]. 
=============================================
[2019-04-04 00:14:59,910] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[82.41289 ]
 [82.470634]
 [82.62626 ]
 [82.887474]
 [83.12066 ]], R is [[82.33930969]
 [82.25299835]
 [82.1997757 ]
 [82.1704483 ]
 [82.1413269 ]].
[2019-04-04 00:15:08,469] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0182293e-25 1.3083999e-18 1.4201206e-16 1.0000000e+00 1.9633812e-21
 1.8408372e-14 8.9569850e-20], sum to 1.0000
[2019-04-04 00:15:08,469] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0069
[2019-04-04 00:15:08,506] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.666666666666667, 61.66666666666666, 0.0, 0.0, 26.0, 25.17113146315366, 0.5090669382967724, 0.0, 1.0, 87021.9668362622], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3357600.0000, 
sim time next is 3358200.0000, 
raw observation next is [-3.833333333333333, 63.33333333333334, 0.0, 0.0, 26.0, 25.33048288837686, 0.5262915572536248, 0.0, 1.0, 56861.79718457848], 
processed observation next is [1.0, 0.8695652173913043, 0.3564173591874424, 0.6333333333333334, 0.0, 0.0, 0.6666666666666666, 0.610873574031405, 0.6754305190845415, 0.0, 1.0, 0.27077046278370703], 
reward next is 0.7292, 
noisyNet noise sample is [array([-1.7250856], dtype=float32), 0.720437]. 
=============================================
[2019-04-04 00:15:10,155] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.6338904e-28 4.3013104e-22 1.0339753e-19 1.0000000e+00 1.7109301e-22
 1.3901538e-15 1.6336276e-22], sum to 1.0000
[2019-04-04 00:15:10,155] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4580
[2019-04-04 00:15:10,210] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 65.0, 243.5, 351.8333333333333, 26.0, 24.96629816549112, 0.3649065302834422, 0.0, 1.0, 44559.86370669986], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2979600.0000, 
sim time next is 2980200.0000, 
raw observation next is [-3.0, 65.0, 231.0, 419.6666666666666, 26.0, 24.98612165110945, 0.3724173672374017, 0.0, 1.0, 31355.37444311673], 
processed observation next is [0.0, 0.4782608695652174, 0.3795013850415513, 0.65, 0.77, 0.4637200736648249, 0.6666666666666666, 0.5821768042591208, 0.6241391224124673, 0.0, 1.0, 0.14931130687198443], 
reward next is 0.8507, 
noisyNet noise sample is [array([-0.59656346], dtype=float32), 0.72566795]. 
=============================================
[2019-04-04 00:15:11,179] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.4083464e-26 7.5019951e-19 7.0910852e-17 1.0000000e+00 5.9421719e-21
 5.7056584e-16 1.9844365e-19], sum to 1.0000
[2019-04-04 00:15:11,181] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0966
[2019-04-04 00:15:11,208] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 29.66666666666666, 0.0, 0.0, 26.0, 25.34801593868119, 0.5092812757729499, 0.0, 1.0, 88507.2393243135], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4048800.0000, 
sim time next is 4049400.0000, 
raw observation next is [-4.0, 29.33333333333333, 0.0, 0.0, 26.0, 25.49583099816474, 0.5230051593952731, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.3518005540166205, 0.2933333333333333, 0.0, 0.0, 0.6666666666666666, 0.6246525831803952, 0.6743350531317578, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.46063447], dtype=float32), 1.8597043]. 
=============================================
[2019-04-04 00:15:12,924] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.1053018e-26 2.9017906e-20 3.0673375e-18 1.0000000e+00 7.8396139e-21
 6.9171066e-16 5.7469726e-21], sum to 1.0000
[2019-04-04 00:15:12,928] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8587
[2019-04-04 00:15:12,953] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 25.0, 24.54920003943377, 0.2376680402952681, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 3614400.0000, 
sim time next is 3615000.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 25.0, 24.69642917460156, 0.2414944834241664, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.4349030470914128, 0.42, 0.0, 0.0, 0.5833333333333334, 0.5580357645501298, 0.5804981611413887, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.24860005], dtype=float32), 0.5932028]. 
=============================================
[2019-04-04 00:15:12,959] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[88.04751 ]
 [88.1687  ]
 [88.162575]
 [87.99617 ]
 [87.503716]], R is [[88.05900574]
 [88.17841339]
 [87.98651123]
 [87.5541687 ]
 [86.72813416]].
[2019-04-04 00:15:16,293] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.00493558e-30 1.06786747e-20 4.74791784e-20 1.00000000e+00
 1.01603707e-25 7.03173914e-18 9.82444058e-24], sum to 1.0000
[2019-04-04 00:15:16,293] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1817
[2019-04-04 00:15:16,305] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.6, 99.0, 71.0, 589.0, 25.0, 26.73729799769957, 0.7516589070938152, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 3168000.0000, 
sim time next is 3168600.0000, 
raw observation next is [6.5, 99.16666666666666, 66.66666666666666, 559.0, 25.0, 26.80662246756119, 0.5420861403729279, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.6426592797783934, 0.9916666666666666, 0.22222222222222218, 0.6176795580110497, 0.5833333333333334, 0.7338852056300992, 0.6806953801243093, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1772738], dtype=float32), 0.6358332]. 
=============================================
[2019-04-04 00:15:24,246] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.2694996e-29 4.0094939e-21 1.4149752e-20 1.0000000e+00 1.3089800e-24
 1.7583155e-18 4.9194306e-23], sum to 1.0000
[2019-04-04 00:15:24,249] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3542
[2019-04-04 00:15:24,257] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.333333333333333, 62.66666666666667, 20.0, 190.0, 26.0, 25.43104711960028, 0.3950721046577046, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4297200.0000, 
sim time next is 4297800.0000, 
raw observation next is [6.266666666666667, 63.33333333333334, 16.0, 152.0, 26.0, 25.38715151724765, 0.3793285364039294, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.7391304347826086, 0.6361957525392429, 0.6333333333333334, 0.05333333333333334, 0.16795580110497238, 0.6666666666666666, 0.6155959597706374, 0.6264428454679765, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.88287205], dtype=float32), -0.11163506]. 
=============================================
[2019-04-04 00:15:25,135] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.7618800e-22 2.2319466e-14 6.2249068e-14 1.0000000e+00 4.3910459e-17
 3.5719764e-11 2.7637201e-17], sum to 1.0000
[2019-04-04 00:15:25,135] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7810
[2019-04-04 00:15:25,143] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 24.0, 23.79380366946028, 0.1235480805864143, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3263400.0000, 
sim time next is 3264000.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 24.0, 23.739572282603, 0.1037033941482218, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.3518005540166205, 0.65, 0.0, 0.0, 0.5, 0.47829769021691665, 0.5345677980494072, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.044428], dtype=float32), -1.3291324]. 
=============================================
[2019-04-04 00:15:25,146] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[73.590645]
 [73.32331 ]
 [72.98856 ]
 [72.77994 ]
 [72.05786 ]], R is [[74.26327515]
 [74.52064514]
 [74.7754364 ]
 [75.02767944]
 [75.27740479]].
[2019-04-04 00:15:32,922] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.09019776e-26 1.12327024e-17 5.82869708e-17 1.00000000e+00
 1.13282458e-22 6.68127120e-13 1.31899749e-20], sum to 1.0000
[2019-04-04 00:15:32,923] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7039
[2019-04-04 00:15:32,945] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.8333333333333334, 69.16666666666667, 95.33333333333334, 579.6666666666667, 25.0, 24.95669654953347, 0.2990125774635163, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 3489000.0000, 
sim time next is 3489600.0000, 
raw observation next is [-0.6666666666666667, 67.33333333333334, 97.16666666666666, 624.8333333333334, 25.0, 25.02365195865848, 0.3076737991420587, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.44413665743305636, 0.6733333333333335, 0.32388888888888884, 0.6904235727440148, 0.5833333333333334, 0.5853043298882067, 0.6025579330473528, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.06166934], dtype=float32), 1.6539984]. 
=============================================
[2019-04-04 00:15:42,108] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.0043242e-26 1.7339227e-19 7.5487669e-17 1.0000000e+00 3.1795446e-23
 3.2759748e-14 7.0111312e-21], sum to 1.0000
[2019-04-04 00:15:42,109] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3745
[2019-04-04 00:15:42,153] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-9.666666666666666, 56.33333333333333, 0.0, 0.0, 26.0, 24.94038451318945, 0.3369741578818621, 0.0, 1.0, 44064.10335972557], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3973200.0000, 
sim time next is 3973800.0000, 
raw observation next is [-9.833333333333334, 57.16666666666667, 0.0, 0.0, 26.0, 24.89091814088342, 0.3253783423070863, 0.0, 1.0, 44080.81737009955], 
processed observation next is [1.0, 1.0, 0.1902123730378578, 0.5716666666666668, 0.0, 0.0, 0.6666666666666666, 0.5742431784069518, 0.6084594474356955, 0.0, 1.0, 0.20990865414333118], 
reward next is 0.7901, 
noisyNet noise sample is [array([0.23262379], dtype=float32), -0.23868454]. 
=============================================
[2019-04-04 00:15:48,382] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.9947414e-25 9.8766213e-19 2.8032188e-17 1.0000000e+00 4.2287534e-20
 1.2883511e-14 1.8248600e-19], sum to 1.0000
[2019-04-04 00:15:48,407] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4265
[2019-04-04 00:15:48,427] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 45.0, 0.0, 0.0, 25.0, 24.48382482493022, 0.1578645589941186, 0.0, 1.0, 65332.84074836397], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 4224600.0000, 
sim time next is 4225200.0000, 
raw observation next is [1.0, 45.66666666666667, 0.0, 0.0, 25.0, 24.47408309333179, 0.1645228672933227, 0.0, 1.0, 52806.8083219197], 
processed observation next is [0.0, 0.9130434782608695, 0.4903047091412743, 0.4566666666666667, 0.0, 0.0, 0.5833333333333334, 0.5395069244443157, 0.5548409557644409, 0.0, 1.0, 0.2514609920091414], 
reward next is 0.7485, 
noisyNet noise sample is [array([0.18051869], dtype=float32), 0.1725973]. 
=============================================
[2019-04-04 00:15:48,447] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.0486398e-27 1.0251665e-18 2.5207289e-17 1.0000000e+00 2.1747556e-23
 1.7640212e-16 4.1686413e-21], sum to 1.0000
[2019-04-04 00:15:48,447] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6270
[2019-04-04 00:15:48,513] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.333333333333333, 24.66666666666666, 27.83333333333333, 252.1666666666667, 26.0, 26.65136175476729, 0.4656338248543763, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4036800.0000, 
sim time next is 4037400.0000, 
raw observation next is [-2.5, 25.0, 20.0, 193.0, 26.0, 26.41004438577312, 0.5394922820076713, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.39335180055401664, 0.25, 0.06666666666666667, 0.2132596685082873, 0.6666666666666666, 0.7008370321477599, 0.6798307606692239, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4898042], dtype=float32), 1.2955208]. 
=============================================
[2019-04-04 00:15:55,035] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.3836049e-28 1.2440287e-19 8.7824723e-19 1.0000000e+00 3.7439639e-22
 7.8084447e-16 1.0687472e-21], sum to 1.0000
[2019-04-04 00:15:55,037] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3171
[2019-04-04 00:15:55,086] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.133333333333334, 65.66666666666667, 0.0, 0.0, 25.0, 24.43432809289428, 0.1418825689724318, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 4299600.0000, 
sim time next is 4300200.0000, 
raw observation next is [6.1, 66.5, 0.0, 0.0, 25.0, 24.37254982586524, 0.128466537248201, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.782608695652174, 0.6315789473684211, 0.665, 0.0, 0.0, 0.5833333333333334, 0.5310458188221032, 0.5428221790827337, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.31612363], dtype=float32), 1.1380104]. 
=============================================
[2019-04-04 00:15:56,465] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.8175296e-33 2.2750849e-23 7.0277794e-23 1.0000000e+00 1.3285919e-28
 2.1078646e-20 2.9158836e-27], sum to 1.0000
[2019-04-04 00:15:56,469] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1292
[2019-04-04 00:15:56,477] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 58.0, 93.5, 739.5, 26.0, 26.32713060809812, 0.6358875780255049, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3423600.0000, 
sim time next is 3424200.0000, 
raw observation next is [2.833333333333333, 59.5, 90.33333333333334, 727.6666666666666, 26.0, 26.42931079648637, 0.6504881926136526, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.541089566020314, 0.595, 0.30111111111111116, 0.8040515653775322, 0.6666666666666666, 0.7024425663738642, 0.7168293975378842, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6639328], dtype=float32), 1.5002682]. 
=============================================
[2019-04-04 00:15:59,000] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.1721784e-25 8.4540522e-18 3.8540368e-15 1.0000000e+00 1.8928972e-20
 6.9623630e-12 3.8966486e-19], sum to 1.0000
[2019-04-04 00:15:59,002] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6734
[2019-04-04 00:15:59,017] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 38.33333333333334, 0.0, 0.0, 25.0, 24.30997236702834, 0.1605037127786755, 0.0, 1.0, 41588.08247709431], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 4062000.0000, 
sim time next is 4062600.0000, 
raw observation next is [-6.0, 39.0, 0.0, 0.0, 25.0, 24.29493942431053, 0.1611500635244789, 0.0, 1.0, 41561.4055170728], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.39, 0.0, 0.0, 0.5833333333333334, 0.5245782853592109, 0.553716687841493, 0.0, 1.0, 0.19791145484320383], 
reward next is 0.8021, 
noisyNet noise sample is [array([0.32469907], dtype=float32), -2.651614]. 
=============================================
[2019-04-04 00:16:04,096] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.6314551e-30 6.6538001e-21 8.7938367e-19 1.0000000e+00 6.5895519e-25
 1.8779707e-15 1.5087102e-22], sum to 1.0000
[2019-04-04 00:16:04,096] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6140
[2019-04-04 00:16:04,148] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 72.0, 32.99999999999999, 233.6666666666666, 26.0, 25.25816673488693, 0.3102407226243033, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3743400.0000, 
sim time next is 3744000.0000, 
raw observation next is [-4.0, 71.0, 47.0, 282.5, 26.0, 25.2529428737334, 0.3018062537488634, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.3518005540166205, 0.71, 0.15666666666666668, 0.31215469613259667, 0.6666666666666666, 0.60441190614445, 0.6006020845829544, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.352523], dtype=float32), 0.36359605]. 
=============================================
[2019-04-04 00:16:04,204] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[91.981125]
 [91.89947 ]
 [91.41811 ]
 [90.29546 ]
 [89.54366 ]], R is [[91.25019836]
 [91.33769989]
 [91.42432404]
 [91.51007843]
 [91.59497833]].
[2019-04-04 00:16:08,560] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.9887805e-25 5.0632792e-20 2.2949046e-17 1.0000000e+00 1.3949259e-20
 3.3757905e-14 8.5887804e-20], sum to 1.0000
[2019-04-04 00:16:08,562] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9702
[2019-04-04 00:16:08,586] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 45.0, 0.0, 0.0, 25.0, 24.48362725596297, 0.157589824644647, 0.0, 1.0, 65331.59905575563], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 4224600.0000, 
sim time next is 4225200.0000, 
raw observation next is [1.0, 45.66666666666667, 0.0, 0.0, 25.0, 24.47396062385916, 0.1642522853960076, 0.0, 1.0, 52767.86005421029], 
processed observation next is [0.0, 0.9130434782608695, 0.4903047091412743, 0.4566666666666667, 0.0, 0.0, 0.5833333333333334, 0.53949671865493, 0.5547507617986692, 0.0, 1.0, 0.251275524067668], 
reward next is 0.7487, 
noisyNet noise sample is [array([0.37531683], dtype=float32), -0.11961555]. 
=============================================
[2019-04-04 00:16:12,140] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.9151862e-25 2.2558375e-17 5.3420859e-17 1.0000000e+00 5.9217867e-20
 1.6707640e-11 3.6059561e-19], sum to 1.0000
[2019-04-04 00:16:12,141] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6530
[2019-04-04 00:16:12,165] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.5, 50.5, 247.0, 48.0, 25.0, 25.16856157756578, 0.3571416430549992, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 4541400.0000, 
sim time next is 4542000.0000, 
raw observation next is [2.666666666666667, 50.0, 249.8333333333333, 58.83333333333333, 25.0, 25.28451176127323, 0.3769853810269799, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5364727608494922, 0.5, 0.8327777777777776, 0.06500920810313075, 0.5833333333333334, 0.6070426467727691, 0.62566179367566, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.36204877], dtype=float32), -0.5068764]. 
=============================================
[2019-04-04 00:16:12,177] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[86.625984]
 [86.61898 ]
 [86.56663 ]
 [86.39367 ]
 [86.04661 ]], R is [[86.49459076]
 [86.6296463 ]
 [86.76335144]
 [86.89572144]
 [87.02676392]].
[2019-04-04 00:16:17,937] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.6079108e-26 9.3768595e-19 1.9013321e-16 1.0000000e+00 6.7833490e-22
 8.7600933e-14 2.4873895e-19], sum to 1.0000
[2019-04-04 00:16:17,937] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8070
[2019-04-04 00:16:17,954] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 41.00000000000001, 0.0, 0.0, 25.0, 24.33859535379692, 0.155877592429841, 0.0, 1.0, 41577.65254351248], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 4065000.0000, 
sim time next is 4065600.0000, 
raw observation next is [-6.0, 41.0, 0.0, 0.0, 25.0, 24.37710054924565, 0.1603814934261597, 0.0, 1.0, 38612.58619429279], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.41, 0.0, 0.0, 0.5833333333333334, 0.5314250457704709, 0.5534604978087199, 0.0, 1.0, 0.1838694580680609], 
reward next is 0.8161, 
noisyNet noise sample is [array([0.4111861], dtype=float32), -0.037567686]. 
=============================================
[2019-04-04 00:16:21,995] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:16:21,995] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:16:22,010] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run14
[2019-04-04 00:16:23,127] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.5729239e-26 9.4188037e-19 9.8518989e-19 1.0000000e+00 1.8506704e-21
 6.9222609e-14 1.1778125e-21], sum to 1.0000
[2019-04-04 00:16:23,127] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2667
[2019-04-04 00:16:23,157] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 39.5, 0.0, 0.0, 25.0, 24.40577165405644, 0.2921040974671931, 0.0, 1.0, 197892.401305175], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 4134600.0000, 
sim time next is 4135200.0000, 
raw observation next is [1.0, 38.33333333333334, 0.0, 0.0, 25.0, 24.43383593974923, 0.3360115537847632, 0.0, 1.0, 167333.8256957931], 
processed observation next is [1.0, 0.8695652173913043, 0.4903047091412743, 0.3833333333333334, 0.0, 0.0, 0.5833333333333334, 0.5361529949791025, 0.6120038512615877, 0.0, 1.0, 0.7968277414085385], 
reward next is 0.2032, 
noisyNet noise sample is [array([1.072777], dtype=float32), -0.26394454]. 
=============================================
[2019-04-04 00:16:27,013] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.3950368e-30 3.0356158e-19 2.1842629e-19 1.0000000e+00 1.2004735e-23
 6.8790759e-18 5.6116334e-23], sum to 1.0000
[2019-04-04 00:16:27,014] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5086
[2019-04-04 00:16:27,036] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [13.0, 35.5, 60.33333333333333, 0.0, 25.0, 27.68904446353223, 0.9124169923816501, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 4378200.0000, 
sim time next is 4378800.0000, 
raw observation next is [13.0, 36.0, 49.66666666666666, 0.0, 25.0, 27.80633150860155, 0.757816565473659, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.8227146814404434, 0.36, 0.1655555555555555, 0.0, 0.5833333333333334, 0.8171942923834624, 0.752605521824553, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.3970132], dtype=float32), -0.88871205]. 
=============================================
[2019-04-04 00:16:27,488] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [9.6802543e-30 3.1997949e-21 4.0260966e-19 1.0000000e+00 1.2178250e-24
 3.3589869e-15 8.4848200e-23], sum to 1.0000
[2019-04-04 00:16:27,488] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4216
[2019-04-04 00:16:27,533] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.166666666666667, 72.0, 94.33333333333333, 524.6666666666667, 26.0, 26.03934268807851, 0.4965182927819223, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3833400.0000, 
sim time next is 3834000.0000, 
raw observation next is [-4.0, 71.0, 96.0, 563.5, 26.0, 26.07642396462595, 0.5107616109111106, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.3518005540166205, 0.71, 0.32, 0.6226519337016575, 0.6666666666666666, 0.6730353303854958, 0.6702538703037035, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.5481095], dtype=float32), 0.46238795]. 
=============================================
[2019-04-04 00:16:27,577] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[93.51946]
 [93.63211]
 [93.63594]
 [93.45842]
 [93.33232]], R is [[93.54963684]
 [93.61414337]
 [93.6780014 ]
 [93.74121857]
 [93.80381012]].
[2019-04-04 00:16:32,629] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.1272436e-30 1.5338258e-22 1.1479826e-19 1.0000000e+00 1.2514500e-25
 3.6328240e-19 3.1136965e-23], sum to 1.0000
[2019-04-04 00:16:32,629] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7225
[2019-04-04 00:16:32,731] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.833333333333333, 76.0, 0.0, 0.0, 26.0, 24.9910764640507, 0.3233972873815944, 0.0, 1.0, 36285.09653617784], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4605000.0000, 
sim time next is 4605600.0000, 
raw observation next is [-2.666666666666667, 75.0, 0.0, 0.0, 26.0, 24.96572628692597, 0.3437870490899593, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.38873499538319484, 0.75, 0.0, 0.0, 0.6666666666666666, 0.5804771905771643, 0.6145956830299865, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.10892294], dtype=float32), 1.2207065]. 
=============================================
[2019-04-04 00:16:37,554] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.6735634e-27 6.1672345e-18 5.5172757e-18 1.0000000e+00 1.5242134e-22
 6.7350210e-17 7.0443976e-22], sum to 1.0000
[2019-04-04 00:16:37,555] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8782
[2019-04-04 00:16:37,575] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.733333333333333, 44.0, 130.0, 144.0, 25.0, 27.09960513946852, 0.7655602532038231, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 4638000.0000, 
sim time next is 4638600.0000, 
raw observation next is [5.6, 44.5, 117.0, 147.0, 25.0, 27.13333459915582, 0.7724878806540932, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.6177285318559557, 0.445, 0.39, 0.16243093922651933, 0.5833333333333334, 0.7611112165963183, 0.757495960218031, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.41073698], dtype=float32), -1.147664]. 
=============================================
[2019-04-04 00:16:57,111] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:16:57,111] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:16:57,117] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run14
[2019-04-04 00:16:58,735] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.8893815e-29 6.9051499e-20 2.0120301e-19 1.0000000e+00 1.5163314e-23
 7.5918240e-17 5.9498325e-23], sum to 1.0000
[2019-04-04 00:16:58,736] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7033
[2019-04-04 00:16:58,775] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [11.16666666666667, 56.66666666666667, 0.0, 0.0, 26.0, 27.1369959870731, 0.8884524558913597, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4391400.0000, 
sim time next is 4392000.0000, 
raw observation next is [11.0, 58.0, 0.0, 0.0, 26.0, 27.06427582399939, 0.8790244297339092, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.7673130193905818, 0.58, 0.0, 0.0, 0.6666666666666666, 0.755356318666616, 0.7930081432446364, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.38461983], dtype=float32), 0.11239368]. 
=============================================
[2019-04-04 00:16:58,790] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[89.34869]
 [89.33941]
 [89.21854]
 [89.11134]
 [89.06116]], R is [[88.70449066]
 [88.81744385]
 [88.92926788]
 [89.03997803]
 [89.14958191]].
[2019-04-04 00:17:03,257] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.46067796e-26 3.45816090e-17 1.03368506e-16 1.00000000e+00
 1.16367763e-21 8.21552976e-14 2.17793747e-21], sum to 1.0000
[2019-04-04 00:17:03,258] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1437
[2019-04-04 00:17:03,267] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 86.0, 160.1666666666667, 24.33333333333333, 26.0, 26.50255109870992, 0.6429350075699661, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4447200.0000, 
sim time next is 4447800.0000, 
raw observation next is [1.0, 86.0, 142.0, 0.0, 26.0, 26.47555848149021, 0.6340263250838769, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4903047091412743, 0.86, 0.47333333333333333, 0.0, 0.6666666666666666, 0.7062965401241842, 0.7113421083612922, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.345729], dtype=float32), 1.0683262]. 
=============================================
[2019-04-04 00:17:04,539] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.15372397e-20 1.08217006e-14 5.34366778e-12 9.99999762e-01
 3.50278680e-15 2.75276989e-07 1.06224750e-14], sum to 1.0000
[2019-04-04 00:17:04,539] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0254
[2019-04-04 00:17:04,572] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-15.6, 73.0, 0.0, 0.0, 23.0, 20.4551155400331, -0.8061049112970816, 0.0, 1.0, 50703.32410021711], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 361200.0000, 
sim time next is 361800.0000, 
raw observation next is [-15.6, 73.0, 0.0, 0.0, 23.0, 20.38872068168492, -0.8076409368237237, 0.0, 1.0, 50640.27507918828], 
processed observation next is [1.0, 0.17391304347826086, 0.030470914127423816, 0.73, 0.0, 0.0, 0.4166666666666667, 0.1990600568070766, 0.23078635439209208, 0.0, 1.0, 0.24114416704375372], 
reward next is 0.7589, 
noisyNet noise sample is [array([-0.13846768], dtype=float32), 1.3088143]. 
=============================================
[2019-04-04 00:17:09,978] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.8330236e-28 2.1014598e-18 8.6016419e-18 1.0000000e+00 5.1601257e-22
 7.4041724e-17 1.2680121e-21], sum to 1.0000
[2019-04-04 00:17:09,978] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5797
[2019-04-04 00:17:10,049] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.333333333333333, 25.16666666666667, 22.66666666666667, 202.6666666666667, 24.0, 25.88599654780948, 0.5366076279224807, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4989000.0000, 
sim time next is 4989600.0000, 
raw observation next is [6.0, 25.0, 17.0, 152.0, 24.0, 25.94440035700161, 0.5278809588678771, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.6288088642659281, 0.25, 0.056666666666666664, 0.16795580110497238, 0.5, 0.6620333630834674, 0.6759603196226257, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.04908511], dtype=float32), 1.8030193]. 
=============================================
[2019-04-04 00:17:18,221] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:17:18,221] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:17:18,226] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run14
[2019-04-04 00:17:18,601] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:17:18,601] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:17:18,610] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run14
[2019-04-04 00:17:20,815] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:17:20,815] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:17:20,870] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run14
[2019-04-04 00:17:21,821] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:17:21,821] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:17:21,830] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run14
[2019-04-04 00:17:24,349] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:17:24,349] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:17:24,353] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run14
[2019-04-04 00:17:37,854] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.9107327e-20 7.8539944e-12 4.6983580e-12 9.9999702e-01 2.2988683e-12
 3.0173569e-06 3.7383431e-15], sum to 1.0000
[2019-04-04 00:17:37,854] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2520
[2019-04-04 00:17:38,034] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [8.100000000000001, 86.0, 88.5, 0.0, 19.0, 18.16280310554908, -1.205606516617211, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 48000.0000, 
sim time next is 48600.0000, 
raw observation next is [8.0, 86.0, 87.0, 0.0, 19.0, 18.13969536199948, -1.209009933556422, 0.0, 1.0, 18680.41167023054], 
processed observation next is [0.0, 0.5652173913043478, 0.6842105263157896, 0.86, 0.29, 0.0, 0.08333333333333333, 0.011641280166623247, 0.09699668881452601, 0.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.4802373], dtype=float32), 0.9386716]. 
=============================================
[2019-04-04 00:17:39,029] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:17:39,029] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:17:39,031] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run14
[2019-04-04 00:17:39,644] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:17:39,644] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:17:39,648] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run14
[2019-04-04 00:18:02,349] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:18:02,349] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:18:02,352] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run14
[2019-04-04 00:18:04,825] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:18:04,825] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:18:04,828] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run14
[2019-04-04 00:18:05,549] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:18:05,562] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:18:05,565] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run14
[2019-04-04 00:18:08,987] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.6589118e-28 2.0978673e-22 2.1918764e-18 1.0000000e+00 1.1810693e-22
 3.4627432e-16 3.1435226e-22], sum to 1.0000
[2019-04-04 00:18:08,987] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0175
[2019-04-04 00:18:09,064] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.50439021713825, 0.1573399110352363, 0.0, 1.0, 39519.87498449384], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4863600.0000, 
sim time next is 4864200.0000, 
raw observation next is [-4.0, 66.0, 0.0, 0.0, 26.0, 24.47134994841542, 0.1521100045226674, 0.0, 1.0, 39551.29489214635], 
processed observation next is [0.0, 0.30434782608695654, 0.3518005540166205, 0.66, 0.0, 0.0, 0.6666666666666666, 0.5392791623679516, 0.5507033348408891, 0.0, 1.0, 0.18833949948641118], 
reward next is 0.8117, 
noisyNet noise sample is [array([0.8045932], dtype=float32), 1.6895589]. 
=============================================
[2019-04-04 00:18:10,426] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.1365039e-16 2.9272262e-10 8.0073568e-07 9.9997520e-01 8.9616014e-12
 2.3926159e-05 5.1933506e-11], sum to 1.0000
[2019-04-04 00:18:10,468] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9454
[2019-04-04 00:18:10,537] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 21.0, 20.086676002181, -0.8962964983208125, 0.0, 1.0, 46704.70229635036], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 189600.0000, 
sim time next is 190200.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 21.0, 20.07064398848355, -0.8942908244009516, 0.0, 1.0, 46761.80833432575], 
processed observation next is [1.0, 0.17391304347826086, 0.21606648199445982, 0.78, 0.0, 0.0, 0.25, 0.17255366570696248, 0.20190305853301613, 0.0, 1.0, 0.2226752777825036], 
reward next is 0.7773, 
noisyNet noise sample is [array([-1.8260995], dtype=float32), 0.2437294]. 
=============================================
[2019-04-04 00:18:13,437] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.4491642e-31 9.6276264e-23 1.4300894e-18 1.0000000e+00 2.3899392e-25
 1.0168477e-17 8.0559979e-25], sum to 1.0000
[2019-04-04 00:18:13,437] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4971
[2019-04-04 00:18:13,556] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 45.0, 132.5, 369.5, 26.0, 25.12920696874578, 0.3744379699089966, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4896000.0000, 
sim time next is 4896600.0000, 
raw observation next is [3.0, 45.0, 122.3333333333333, 352.0, 26.0, 25.15638992414499, 0.3716202752273615, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.5457063711911359, 0.45, 0.4077777777777777, 0.3889502762430939, 0.6666666666666666, 0.5963658270120824, 0.6238734250757871, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.22518438], dtype=float32), -0.6258946]. 
=============================================
[2019-04-04 00:18:19,774] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:18:19,774] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:18:19,794] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run14
[2019-04-04 00:18:27,552] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.8457547e-32 1.6741678e-22 7.8276644e-20 1.0000000e+00 3.5513268e-25
 7.4355220e-20 2.5570174e-25], sum to 1.0000
[2019-04-04 00:18:27,552] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0455
[2019-04-04 00:18:27,687] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.5, 44.5, 33.0, 187.0, 26.0, 25.03137744004336, 0.3203625150116684, 0.0, 1.0, 48673.88458231579], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4901400.0000, 
sim time next is 4902000.0000, 
raw observation next is [2.333333333333333, 44.33333333333334, 27.5, 155.8333333333333, 26.0, 25.00375125127747, 0.3206817119936164, 0.0, 1.0, 49421.48333967391], 
processed observation next is [0.0, 0.7391304347826086, 0.5272391505078486, 0.4433333333333334, 0.09166666666666666, 0.17219152854511965, 0.6666666666666666, 0.5836459376064559, 0.6068939039978721, 0.0, 1.0, 0.23534039685559005], 
reward next is 0.7647, 
noisyNet noise sample is [array([1.2166797], dtype=float32), -0.8771802]. 
=============================================
[2019-04-04 00:18:27,751] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[100.27405 ]
 [100.280235]
 [100.20679 ]
 [100.07648 ]
 [ 99.93414 ]], R is [[100.0371933 ]
 [ 99.80503845]
 [ 99.6055603 ]
 [ 99.47661591]
 [ 99.39289856]].
[2019-04-04 00:18:28,616] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:18:28,616] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:18:28,619] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run14
[2019-04-04 00:18:42,581] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:18:42,581] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:18:42,584] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run14
[2019-04-04 00:18:53,455] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:18:53,455] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:18:53,459] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run14
[2019-04-04 00:19:07,322] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.24375762e-25 1.04296988e-15 3.72221432e-13 1.00000000e+00
 1.19257963e-19 1.08656495e-11 7.99326908e-19], sum to 1.0000
[2019-04-04 00:19:07,339] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2270
[2019-04-04 00:19:07,421] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.783333333333333, 81.66666666666667, 0.0, 0.0, 23.0, 22.7028227361022, -0.2325068599214815, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 958200.0000, 
sim time next is 958800.0000, 
raw observation next is [6.966666666666667, 81.33333333333334, 0.0, 0.0, 23.0, 22.69477744485016, -0.2372458374600881, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.6555863342566944, 0.8133333333333335, 0.0, 0.0, 0.4166666666666667, 0.3912314537375134, 0.42091805417997064, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.44010442], dtype=float32), -0.0058637084]. 
=============================================
[2019-04-04 00:19:08,837] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.7091772e-26 6.1588679e-17 1.0025214e-14 1.0000000e+00 1.4328285e-21
 7.1459723e-15 1.7426092e-19], sum to 1.0000
[2019-04-04 00:19:08,846] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8198
[2019-04-04 00:19:08,916] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-11.3, 49.33333333333334, 55.5, 890.0, 24.0, 24.51614623322325, 0.08377633631817416, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 393600.0000, 
sim time next is 394200.0000, 
raw observation next is [-11.1, 48.5, 55.0, 887.0, 24.0, 24.58032880704467, 0.09601392810618746, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.1551246537396122, 0.485, 0.18333333333333332, 0.980110497237569, 0.5, 0.5483607339203891, 0.5320046427020625, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5382901], dtype=float32), -1.4377023]. 
=============================================
[2019-04-04 00:19:11,179] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.26695580e-17 1.02607776e-10 1.05559181e-07 9.99999404e-01
 5.94813424e-13 4.28083041e-07 4.56725456e-12], sum to 1.0000
[2019-04-04 00:19:11,180] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0875
[2019-04-04 00:19:11,202] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.199999999999999, 70.33333333333334, 0.0, 0.0, 22.0, 21.34935737617131, -0.6065904224420863, 0.0, 1.0, 49384.05052746004], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 265800.0000, 
sim time next is 266400.0000, 
raw observation next is [-7.3, 71.0, 0.0, 0.0, 22.0, 21.38477844959486, -0.6115668294317188, 0.0, 1.0, 41139.20009228999], 
processed observation next is [1.0, 0.08695652173913043, 0.26038781163434904, 0.71, 0.0, 0.0, 0.3333333333333333, 0.2820648707995718, 0.29614439018942706, 0.0, 1.0, 0.19590095282042852], 
reward next is 0.8041, 
noisyNet noise sample is [array([-0.2863661], dtype=float32), -0.84769994]. 
=============================================
[2019-04-04 00:19:12,205] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.3650850e-20 9.2052397e-14 3.7496486e-09 1.0000000e+00 1.5911773e-14
 4.6168178e-10 2.8610084e-15], sum to 1.0000
[2019-04-04 00:19:12,205] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4320
[2019-04-04 00:19:12,246] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-13.0, 78.66666666666667, 0.0, 0.0, 24.0, 22.90337942926517, -0.2244752574444362, 0.0, 1.0, 49605.19592716584], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 336000.0000, 
sim time next is 336600.0000, 
raw observation next is [-13.1, 79.5, 0.0, 0.0, 24.0, 22.85497696772266, -0.2367953477130513, 0.0, 1.0, 49248.26272485621], 
processed observation next is [1.0, 0.9130434782608695, 0.0997229916897507, 0.795, 0.0, 0.0, 0.5, 0.40458141397688835, 0.42106821742898287, 0.0, 1.0, 0.2345155367850296], 
reward next is 0.7655, 
noisyNet noise sample is [array([0.18235107], dtype=float32), -2.1330926]. 
=============================================
[2019-04-04 00:19:18,935] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.1330339e-11 3.0786549e-07 2.8770030e-04 9.9970239e-01 1.5132304e-07
 9.3572407e-06 5.1764544e-09], sum to 1.0000
[2019-04-04 00:19:18,936] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0126
[2019-04-04 00:19:19,023] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.4, 68.0, 0.0, 0.0, 21.0, 20.5414313815464, -0.7521557996800184, 0.0, 1.0, 77230.8769860437], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 160800.0000, 
sim time next is 161400.0000, 
raw observation next is [-8.4, 68.0, 0.0, 0.0, 21.0, 20.47584078859532, -0.7532136845573927, 0.0, 1.0, 95114.49080374266], 
processed observation next is [1.0, 0.8695652173913043, 0.2299168975069252, 0.68, 0.0, 0.0, 0.25, 0.2063200657162767, 0.24892877181420245, 0.0, 1.0, 0.4529261466844889], 
reward next is 0.5471, 
noisyNet noise sample is [array([0.46513206], dtype=float32), 0.42021328]. 
=============================================
[2019-04-04 00:19:27,979] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.3053363e-13 1.3309158e-07 1.0561884e-04 9.9760842e-01 3.5832435e-09
 2.2857892e-03 6.1245042e-10], sum to 1.0000
[2019-04-04 00:19:27,979] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4627
[2019-04-04 00:19:28,063] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-12.8, 70.0, 0.0, 0.0, 22.0, 21.65716140986937, -0.5484359544646475, 0.0, 1.0, 197553.9778000464], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 331200.0000, 
sim time next is 331800.0000, 
raw observation next is [-12.8, 71.16666666666667, 0.0, 0.0, 22.0, 21.55749266546248, -0.5239459328233943, 0.0, 1.0, 138682.0878523423], 
processed observation next is [1.0, 0.8695652173913043, 0.1080332409972299, 0.7116666666666667, 0.0, 0.0, 0.3333333333333333, 0.2964577221218733, 0.32535135572553525, 0.0, 1.0, 0.6603908945349632], 
reward next is 0.3396, 
noisyNet noise sample is [array([-0.5100225], dtype=float32), 0.89180887]. 
=============================================
[2019-04-04 00:19:37,553] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.8064136e-21 8.6236416e-13 4.0299502e-09 9.9999976e-01 6.0461809e-15
 2.3577775e-07 1.1303026e-16], sum to 1.0000
[2019-04-04 00:19:37,553] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3638
[2019-04-04 00:19:37,563] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 88.00000000000001, 102.3333333333333, 0.0, 23.0, 22.84544565460968, -0.2029165988133155, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 1689000.0000, 
sim time next is 1689600.0000, 
raw observation next is [1.1, 88.0, 101.1666666666667, 0.0, 23.0, 22.76510855360872, -0.2012841645504059, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.49307479224376743, 0.88, 0.3372222222222223, 0.0, 0.4166666666666667, 0.3970923794673933, 0.432905278483198, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3261782], dtype=float32), -0.21126573]. 
=============================================
[2019-04-04 00:19:37,790] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.0133633e-22 1.3063368e-12 1.6697909e-09 1.0000000e+00 7.5330065e-16
 4.2217035e-08 5.8542563e-17], sum to 1.0000
[2019-04-04 00:19:37,790] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1001
[2019-04-04 00:19:37,839] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.8, 93.0, 95.0, 0.0, 22.0, 21.41561501254698, -0.5338184022851595, 1.0, 1.0, 38413.32311075307], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 913200.0000, 
sim time next is 913800.0000, 
raw observation next is [3.8, 93.0, 94.0, 0.0, 22.0, 21.77906022176829, -0.4978757311068862, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5678670360110805, 0.93, 0.31333333333333335, 0.0, 0.3333333333333333, 0.3149216851473575, 0.3340414229643713, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.64722234], dtype=float32), 0.5003471]. 
=============================================
[2019-04-04 00:19:45,581] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.9152126e-19 2.2481980e-11 3.8047915e-08 9.9948102e-01 5.2917338e-13
 5.1899510e-04 6.5834776e-15], sum to 1.0000
[2019-04-04 00:19:45,588] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1507
[2019-04-04 00:19:45,616] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.066666666666666, 72.33333333333333, 90.83333333333333, 0.0, 22.0, 22.15275341790974, -0.5323718810766096, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 816000.0000, 
sim time next is 816600.0000, 
raw observation next is [-4.783333333333333, 71.66666666666667, 94.66666666666666, 0.0, 22.0, 22.1215043204872, -0.5325608838217432, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.3301015697137581, 0.7166666666666667, 0.31555555555555553, 0.0, 0.3333333333333333, 0.3434586933739334, 0.3224797053927523, 1.0, 1.0, 0.0], 
reward next is 0.6744, 
noisyNet noise sample is [array([-0.28847313], dtype=float32), 0.56354606]. 
=============================================
[2019-04-04 00:19:56,116] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.8607803e-21 2.8741024e-12 4.7430775e-09 9.9999857e-01 5.6570633e-15
 1.4082481e-06 1.2811662e-15], sum to 1.0000
[2019-04-04 00:19:56,118] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0702
[2019-04-04 00:19:56,209] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.8999999999999999, 57.0, 81.0, 56.0, 23.0, 22.03236351613803, -0.4448937371636016, 0.0, 1.0, 43845.19103664235], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 657000.0000, 
sim time next is 657600.0000, 
raw observation next is [-0.8, 56.0, 81.33333333333333, 53.0, 23.0, 22.01994667331877, -0.444492268823387, 0.0, 1.0, 42927.33985927145], 
processed observation next is [0.0, 0.6086956521739131, 0.4404432132963989, 0.56, 0.2711111111111111, 0.05856353591160221, 0.4166666666666667, 0.33499555610989756, 0.35183591039220435, 0.0, 1.0, 0.20441590409176882], 
reward next is 0.7956, 
noisyNet noise sample is [array([-1.1207554], dtype=float32), 1.179088]. 
=============================================
[2019-04-04 00:20:03,332] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-04 00:20:03,334] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 00:20:03,334] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:20:03,334] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 00:20:03,337] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:20:03,335] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 00:20:03,342] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run19
[2019-04-04 00:20:03,379] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run19
[2019-04-04 00:20:03,406] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:20:03,411] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run19
[2019-04-04 00:21:37,848] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.42536095], dtype=float32), 0.07101599]
[2019-04-04 00:21:37,848] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [3.3, 62.66666666666666, 0.0, 0.0, 23.0, 22.90674469246598, -0.2371435865230249, 0.0, 1.0, 0.0]
[2019-04-04 00:21:37,848] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 00:21:37,849] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [8.7477814e-23 2.9515480e-13 4.7569771e-11 1.0000000e+00 1.3128358e-16
 2.5173661e-09 6.7446620e-17], sampled 0.8208213475523274
[2019-04-04 00:22:12,680] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7298.4228 191128478.3625 -345.2391
[2019-04-04 00:22:29,225] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7228.0903 219844508.5221 -614.3216
[2019-04-04 00:22:31,457] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.42536095], dtype=float32), 0.07101599]
[2019-04-04 00:22:31,457] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-5.0301959035, 93.67675933, 0.0, 0.0, 23.0, 22.06007457738968, -0.3766196350359505, 0.0, 1.0, 43269.94797143908]
[2019-04-04 00:22:31,457] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 00:22:31,458] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.8992477e-20 2.9666458e-13 4.0639817e-10 1.0000000e+00 9.8840292e-15
 2.7821118e-08 3.2314771e-15], sampled 0.6595059065301747
[2019-04-04 00:22:35,937] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7091.9317 233023772.5325 -696.7951
[2019-04-04 00:22:36,960] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 1800000, evaluation results [1800000.0, 7228.090316288254, 219844508.52207345, -614.3215889178709, 7298.42276327032, 191128478.36252323, -345.2390531967081, 7091.931685165315, 233023772.53253585, -696.7951339601025]
[2019-04-04 00:22:37,479] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.8143908e-30 4.8237613e-17 6.8044137e-16 1.0000000e+00 3.3839621e-23
 2.9160601e-13 1.4186598e-22], sum to 1.0000
[2019-04-04 00:22:37,481] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5923
[2019-04-04 00:22:37,497] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.966666666666667, 81.33333333333334, 0.0, 0.0, 24.0, 23.65821772105414, 0.0006363166809248653, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 958800.0000, 
sim time next is 959400.0000, 
raw observation next is [7.15, 81.0, 0.0, 0.0, 24.0, 23.6461031471113, -0.006988402021949326, 0.0, 1.0, 19332.55566887927], 
processed observation next is [1.0, 0.08695652173913043, 0.6606648199445985, 0.81, 0.0, 0.0, 0.5, 0.47050859559260844, 0.49767053265935024, 0.0, 1.0, 0.09205978889942511], 
reward next is 0.9079, 
noisyNet noise sample is [array([2.247097], dtype=float32), 0.17855859]. 
=============================================
[2019-04-04 00:22:39,472] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.9365794e-24 5.2030591e-14 2.2003382e-10 1.0000000e+00 9.5316518e-18
 1.9774755e-09 1.0962492e-18], sum to 1.0000
[2019-04-04 00:22:39,472] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3624
[2019-04-04 00:22:39,529] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.9, 82.66666666666667, 0.0, 0.0, 24.0, 23.14053602225976, -0.2176753714794243, 1.0, 1.0, 21476.71038960479], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 843000.0000, 
sim time next is 843600.0000, 
raw observation next is [-3.9, 83.33333333333334, 0.0, 0.0, 24.0, 23.15380792602923, -0.325683625252523, 1.0, 1.0, 20729.97769728087], 
processed observation next is [1.0, 0.782608695652174, 0.3545706371191136, 0.8333333333333335, 0.0, 0.0, 0.5, 0.42948399383576924, 0.39143879158249234, 1.0, 1.0, 0.09871417951086128], 
reward next is 0.9013, 
noisyNet noise sample is [array([1.3778644], dtype=float32), 0.15346391]. 
=============================================
[2019-04-04 00:22:45,519] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.8461062e-30 1.1553054e-20 9.8186959e-19 1.0000000e+00 4.4772407e-25
 7.1135077e-20 2.3664350e-24], sum to 1.0000
[2019-04-04 00:22:45,519] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0839
[2019-04-04 00:22:45,528] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [16.6, 75.0, 0.0, 0.0, 23.0, 22.49666138970471, -0.1333194861265329, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 1206000.0000, 
sim time next is 1206600.0000, 
raw observation next is [16.51666666666667, 75.5, 0.0, 0.0, 23.0, 22.47386812913305, -0.1365760011569918, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 1.0, 0.9201292705447832, 0.755, 0.0, 0.0, 0.4166666666666667, 0.3728223440944207, 0.4544746662810027, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7360044], dtype=float32), 0.030119345]. 
=============================================
[2019-04-04 00:22:51,446] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.4231914e-27 1.7430098e-17 4.6952405e-14 1.0000000e+00 8.6655590e-20
 1.0441084e-15 3.1528676e-21], sum to 1.0000
[2019-04-04 00:22:51,470] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2409
[2019-04-04 00:22:51,485] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [11.1, 77.0, 0.0, 0.0, 24.0, 23.67674722229295, 0.1879522269730853, 0.0, 1.0, 34528.60808680234], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1137600.0000, 
sim time next is 1138200.0000, 
raw observation next is [11.18333333333333, 77.0, 0.0, 0.0, 24.0, 23.7476474390788, 0.1935028369501398, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.17391304347826086, 0.7723915050784858, 0.77, 0.0, 0.0, 0.5, 0.47897061992323336, 0.5645009456500466, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.38500324], dtype=float32), -2.5317028]. 
=============================================
[2019-04-04 00:22:55,294] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.2883408e-23 9.8302802e-14 3.5386782e-12 9.9999976e-01 1.7132724e-17
 2.7258415e-07 4.2145849e-18], sum to 1.0000
[2019-04-04 00:22:55,296] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2684
[2019-04-04 00:22:55,333] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 88.0, 0.0, 0.0, 23.0, 22.10370926432161, -0.2479773097689646, 1.0, 1.0, 40928.13522345208], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 1711800.0000, 
sim time next is 1712400.0000, 
raw observation next is [1.1, 88.0, 0.0, 0.0, 23.0, 22.08027053270347, -0.2443807908788645, 1.0, 1.0, 45965.12678287376], 
processed observation next is [1.0, 0.8260869565217391, 0.49307479224376743, 0.88, 0.0, 0.0, 0.4166666666666667, 0.3400225443919558, 0.41853973637371183, 1.0, 1.0, 0.21888155610892268], 
reward next is 0.7811, 
noisyNet noise sample is [array([-0.60799974], dtype=float32), -0.81147337]. 
=============================================
[2019-04-04 00:22:56,231] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.0006441e-24 6.4519367e-14 5.9251940e-14 1.0000000e+00 5.0984431e-18
 1.4937832e-11 4.7326586e-19], sum to 1.0000
[2019-04-04 00:22:56,232] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2873
[2019-04-04 00:22:56,238] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 92.0, 22.66666666666666, 0.0, 23.0, 23.20330579626661, -0.1514048389136935, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 1441200.0000, 
sim time next is 1441800.0000, 
raw observation next is [1.1, 92.0, 18.0, 0.0, 23.0, 23.19099538919563, -0.1574600911585056, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.49307479224376743, 0.92, 0.06, 0.0, 0.4166666666666667, 0.43258294909963596, 0.4475133029471648, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7529556], dtype=float32), 2.3427029]. 
=============================================
[2019-04-04 00:22:57,458] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.2306427e-23 3.7570844e-14 2.0125297e-11 1.0000000e+00 3.4614190e-18
 4.0493444e-09 8.3841717e-17], sum to 1.0000
[2019-04-04 00:22:57,470] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6576
[2019-04-04 00:22:57,483] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-9.100000000000001, 91.0, 0.0, 0.0, 24.0, 22.3184331612751, -0.3703223897287111, 0.0, 1.0, 44094.82648947415], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2269200.0000, 
sim time next is 2269800.0000, 
raw observation next is [-9.2, 91.0, 0.0, 0.0, 24.0, 22.25231564424488, -0.3735761111813872, 0.0, 1.0, 44076.54919753018], 
processed observation next is [1.0, 0.2608695652173913, 0.20775623268698065, 0.91, 0.0, 0.0, 0.5, 0.35435963702040674, 0.3754746296062043, 0.0, 1.0, 0.20988832951204847], 
reward next is 0.7901, 
noisyNet noise sample is [array([0.21225375], dtype=float32), 0.63469315]. 
=============================================
[2019-04-04 00:22:57,906] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.7686487e-22 2.0526105e-15 4.7399812e-13 1.0000000e+00 1.3500271e-16
 1.5725130e-11 5.7253432e-17], sum to 1.0000
[2019-04-04 00:22:57,906] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7797
[2019-04-04 00:22:57,959] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.700000000000001, 78.0, 47.16666666666666, 15.66666666666666, 24.0, 23.10204847031807, -0.139858663433156, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1845600.0000, 
sim time next is 1846200.0000, 
raw observation next is [-6.7, 78.0, 67.33333333333333, 31.33333333333333, 24.0, 23.43816977430269, -0.1162681474357765, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.2770083102493075, 0.78, 0.22444444444444442, 0.034622467771639034, 0.5, 0.45318081452522413, 0.4612439508547412, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.75399876], dtype=float32), 1.1274656]. 
=============================================
[2019-04-04 00:22:58,795] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.0988594e-24 5.2369632e-13 8.2755521e-12 1.0000000e+00 1.1364726e-16
 4.6103324e-10 4.4532393e-18], sum to 1.0000
[2019-04-04 00:22:58,804] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5357
[2019-04-04 00:22:58,814] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.3, 93.0, 78.0, 0.0, 23.0, 23.11150701826896, -0.245015780222793, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 917400.0000, 
sim time next is 918000.0000, 
raw observation next is [4.4, 93.0, 72.0, 0.0, 23.0, 23.12613839068423, -0.2409668489848565, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5844875346260389, 0.93, 0.24, 0.0, 0.4166666666666667, 0.4271781992236858, 0.41967771700504786, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.05127413], dtype=float32), -0.9178282]. 
=============================================
[2019-04-04 00:22:58,817] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[94.52411 ]
 [94.52267 ]
 [94.586754]
 [94.59801 ]
 [94.60409 ]], R is [[94.5567627 ]
 [94.61119843]
 [94.66508484]
 [94.71843719]
 [94.77125549]].
[2019-04-04 00:23:05,465] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.4776635e-32 6.6200298e-20 8.5285806e-19 1.0000000e+00 2.8016358e-25
 4.9436949e-17 7.4885586e-26], sum to 1.0000
[2019-04-04 00:23:05,468] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4773
[2019-04-04 00:23:05,473] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [13.8, 78.0, 0.0, 0.0, 23.0, 22.85696351252255, -0.1300836732952779, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 1044000.0000, 
sim time next is 1044600.0000, 
raw observation next is [13.9, 77.83333333333333, 0.0, 0.0, 23.0, 22.86588018032853, -0.1239012941358298, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.847645429362881, 0.7783333333333333, 0.0, 0.0, 0.4166666666666667, 0.40549001502737764, 0.4586995686213901, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5953928], dtype=float32), 1.9713933]. 
=============================================
[2019-04-04 00:23:08,757] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.6012386e-22 1.6231835e-13 2.6393156e-12 1.0000000e+00 6.0294545e-17
 1.4273988e-08 7.9202186e-17], sum to 1.0000
[2019-04-04 00:23:08,758] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8809
[2019-04-04 00:23:08,770] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 85.66666666666667, 0.0, 0.0, 23.0, 22.35788226721935, -0.375639364593934, 0.0, 1.0, 42614.40115979443], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 2004000.0000, 
sim time next is 2004600.0000, 
raw observation next is [-6.1, 86.33333333333333, 0.0, 0.0, 23.0, 22.35570729181846, -0.3768480253178777, 0.0, 1.0, 42626.4912305332], 
processed observation next is [1.0, 0.17391304347826086, 0.29362880886426596, 0.8633333333333333, 0.0, 0.0, 0.4166666666666667, 0.36297560765153847, 0.37438399156070745, 0.0, 1.0, 0.20298329157396763], 
reward next is 0.7970, 
noisyNet noise sample is [array([0.5174915], dtype=float32), 0.6272266]. 
=============================================
[2019-04-04 00:23:15,698] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.0420752e-24 4.7448880e-15 4.4173186e-11 1.0000000e+00 1.7795645e-18
 7.2581462e-11 3.9857259e-18], sum to 1.0000
[2019-04-04 00:23:15,699] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2798
[2019-04-04 00:23:15,707] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.9, 92.0, 0.0, 0.0, 23.0, 22.7662901313521, -0.1973831233928884, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 1474200.0000, 
sim time next is 1474800.0000, 
raw observation next is [2.0, 92.0, 0.0, 0.0, 23.0, 22.72271133086853, -0.2101000687034615, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.518005540166205, 0.92, 0.0, 0.0, 0.4166666666666667, 0.3935592775723776, 0.4299666437655128, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.097115], dtype=float32), 1.2820041]. 
=============================================
[2019-04-04 00:23:21,140] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.30721849e-18 2.58716074e-12 9.04667175e-10 9.99999881e-01
 1.06452180e-13 1.09476325e-07 3.40223455e-14], sum to 1.0000
[2019-04-04 00:23:21,141] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2772
[2019-04-04 00:23:21,202] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.05, 84.5, 0.0, 0.0, 23.0, 22.14880556318098, -0.3785871784648689, 0.0, 1.0, 123794.3726278113], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 1884600.0000, 
sim time next is 1885200.0000, 
raw observation next is [-5.233333333333333, 85.0, 0.0, 0.0, 23.0, 22.24744246529533, -0.3622380570255143, 0.0, 1.0, 72296.16098812467], 
processed observation next is [0.0, 0.8260869565217391, 0.31763619575253926, 0.85, 0.0, 0.0, 0.4166666666666667, 0.3539535387746107, 0.3792539809914952, 0.0, 1.0, 0.34426743327678416], 
reward next is 0.6557, 
noisyNet noise sample is [array([-1.1707275], dtype=float32), -1.0128373]. 
=============================================
[2019-04-04 00:23:25,249] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [8.8977246e-20 5.5963904e-11 2.6693883e-11 9.9999952e-01 4.5314204e-15
 4.6687680e-07 4.0395311e-16], sum to 1.0000
[2019-04-04 00:23:25,251] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2460
[2019-04-04 00:23:25,323] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 70.0, 8.333333333333332, 0.0, 23.0, 22.51536558295381, -0.3225807514917885, 1.0, 1.0, 164401.5428952509], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 2222400.0000, 
sim time next is 2223000.0000, 
raw observation next is [-4.5, 69.5, 0.0, 0.0, 23.0, 22.59028178607747, -0.3138153439056798, 1.0, 1.0, 94407.10002637716], 
processed observation next is [1.0, 0.7391304347826086, 0.3379501385041552, 0.695, 0.0, 0.0, 0.4166666666666667, 0.38252348217312243, 0.39539488536477335, 1.0, 1.0, 0.44955761917322457], 
reward next is 0.5504, 
noisyNet noise sample is [array([-0.3428841], dtype=float32), -0.40875867]. 
=============================================
[2019-04-04 00:23:25,333] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[75.98711 ]
 [76.36545 ]
 [76.661736]
 [77.10714 ]
 [77.40351 ]], R is [[76.02768707]
 [75.48454285]
 [75.72969818]
 [75.97240448]
 [76.21268463]].
[2019-04-04 00:23:28,632] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3253909e-23 4.0303900e-16 6.0123673e-13 1.0000000e+00 1.0691245e-17
 7.2394846e-10 8.7270677e-19], sum to 1.0000
[2019-04-04 00:23:28,635] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7685
[2019-04-04 00:23:28,662] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.0, 69.0, 0.0, 0.0, 23.0, 22.3533427915945, -0.2338028619959298, 0.0, 1.0, 86595.66031550415], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 2234400.0000, 
sim time next is 2235000.0000, 
raw observation next is [-5.0, 68.5, 0.0, 0.0, 23.0, 22.50506020899186, -0.2244532253749721, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.32409972299168976, 0.685, 0.0, 0.0, 0.4166666666666667, 0.375421684082655, 0.42518225820834266, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5509878], dtype=float32), 1.156968]. 
=============================================
[2019-04-04 00:23:28,686] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[83.86903]
 [84.09839]
 [84.00302]
 [84.45116]
 [85.11722]], R is [[83.77251434]
 [83.52243042]
 [82.9492569 ]
 [82.17144775]
 [82.16677094]].
[2019-04-04 00:23:31,256] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.1609452e-23 2.5579559e-16 4.4452757e-12 1.0000000e+00 6.1110961e-19
 8.2921781e-11 1.3624318e-18], sum to 1.0000
[2019-04-04 00:23:31,260] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0018
[2019-04-04 00:23:31,274] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 23.0, 22.36370726439086, -0.3722786330523157, 0.0, 1.0, 50360.82305601485], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 2000400.0000, 
sim time next is 2001000.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 23.0, 22.34402774493376, -0.3771208780105471, 0.0, 1.0, 47134.22824702401], 
processed observation next is [1.0, 0.13043478260869565, 0.30747922437673136, 0.83, 0.0, 0.0, 0.4166666666666667, 0.36200231207781347, 0.374293040663151, 0.0, 1.0, 0.22444870593820956], 
reward next is 0.7756, 
noisyNet noise sample is [array([1.0949388], dtype=float32), 0.12254815]. 
=============================================
[2019-04-04 00:23:31,289] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[82.11801]
 [82.17446]
 [82.17061]
 [82.06475]
 [81.89457]], R is [[82.10729218]
 [82.04640198]
 [82.06452179]
 [81.97968292]
 [81.83837891]].
[2019-04-04 00:23:52,526] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.6208640e-24 5.5936547e-17 8.0602083e-14 1.0000000e+00 1.6107405e-19
 8.6783511e-11 3.0093757e-18], sum to 1.0000
[2019-04-04 00:23:52,526] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7722
[2019-04-04 00:23:52,564] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.0, 73.0, 0.0, 0.0, 23.0, 22.45350764838648, -0.3280034625798332, 0.0, 1.0, 64233.42322687983], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 2598600.0000, 
sim time next is 2599200.0000, 
raw observation next is [-5.0, 74.0, 0.0, 0.0, 23.0, 22.51699183358206, -0.3303428210743981, 0.0, 1.0, 18748.21272313392], 
processed observation next is [1.0, 0.08695652173913043, 0.32409972299168976, 0.74, 0.0, 0.0, 0.4166666666666667, 0.3764159861318384, 0.38988572630853396, 0.0, 1.0, 0.08927720344349485], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.48946813], dtype=float32), 0.68601346]. 
=============================================
[2019-04-04 00:23:53,506] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.1651429e-22 1.4975709e-12 1.0459562e-11 1.0000000e+00 2.9958629e-16
 4.8475007e-10 2.0697018e-17], sum to 1.0000
[2019-04-04 00:23:53,507] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8855
[2019-04-04 00:23:53,519] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.0, 24.0, 106.5, 0.0, 23.0, 22.85855153578065, -0.2953174204726204, 1.0, 1.0, 4151.202593384575], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 2818800.0000, 
sim time next is 2819400.0000, 
raw observation next is [6.933333333333334, 24.16666666666666, 102.6666666666667, 0.0, 23.0, 22.64760793939557, -0.3105039317924328, 1.0, 1.0, 22018.80455364216], 
processed observation next is [1.0, 0.6521739130434783, 0.6546629732225301, 0.2416666666666666, 0.3422222222222223, 0.0, 0.4166666666666667, 0.3873006616162975, 0.3964986894025224, 1.0, 1.0, 0.10485145025543885], 
reward next is 0.8951, 
noisyNet noise sample is [array([-1.6044891], dtype=float32), 0.09994219]. 
=============================================
[2019-04-04 00:23:59,958] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0664923e-26 4.4246414e-17 8.6581794e-15 1.0000000e+00 4.9790030e-20
 2.7212206e-13 5.3898095e-21], sum to 1.0000
[2019-04-04 00:23:59,958] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5585
[2019-04-04 00:23:59,984] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.833333333333333, 100.0, 0.0, 0.0, 24.0, 23.62883253873775, 0.1964011454944337, 0.0, 1.0, 196217.9094192961], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3186600.0000, 
sim time next is 3187200.0000, 
raw observation next is [2.666666666666667, 100.0, 0.0, 0.0, 24.0, 23.61378496693102, 0.2234235153054557, 0.0, 1.0, 160450.6326777239], 
processed observation next is [1.0, 0.9130434782608695, 0.5364727608494922, 1.0, 0.0, 0.0, 0.5, 0.46781541391091847, 0.5744745051018186, 0.0, 1.0, 0.7640506317986852], 
reward next is 0.2359, 
noisyNet noise sample is [array([-0.04841951], dtype=float32), 0.29466635]. 
=============================================
[2019-04-04 00:24:13,915] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.6093173e-23 1.1875062e-16 5.8967295e-13 1.0000000e+00 6.8821733e-18
 1.6576157e-12 9.3501105e-18], sum to 1.0000
[2019-04-04 00:24:13,915] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5190
[2019-04-04 00:24:13,990] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 69.0, 31.16666666666667, 0.0, 24.0, 22.91804741908514, -0.2018091615922075, 0.0, 1.0, 9378.409572912582], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2362800.0000, 
sim time next is 2363400.0000, 
raw observation next is [-3.4, 69.0, 37.0, 0.0, 24.0, 23.04675621141857, -0.1945954392882185, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.368421052631579, 0.69, 0.12333333333333334, 0.0, 0.5, 0.420563017618214, 0.43513485357059384, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3720094], dtype=float32), 1.0464633]. 
=============================================
[2019-04-04 00:24:22,706] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2295438e-28 1.1501040e-19 6.6185683e-19 1.0000000e+00 2.8490276e-23
 1.0658175e-18 1.5561268e-22], sum to 1.0000
[2019-04-04 00:24:22,708] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6849
[2019-04-04 00:24:22,746] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.85, 37.16666666666666, 79.33333333333333, 794.3333333333334, 23.0, 22.54407363374519, -0.3202264239870607, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 2458200.0000, 
sim time next is 2458800.0000, 
raw observation next is [-2.3, 36.0, 81.0, 803.0, 23.0, 22.52490096712567, -0.3239238592127663, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.3988919667590028, 0.36, 0.27, 0.887292817679558, 0.4166666666666667, 0.3770750805938059, 0.39202538026241124, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6794256], dtype=float32), 0.80559015]. 
=============================================
[2019-04-04 00:24:22,813] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4263454e-27 7.3009225e-19 6.9072943e-17 1.0000000e+00 2.5706157e-22
 9.0916328e-17 2.0492871e-21], sum to 1.0000
[2019-04-04 00:24:22,821] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8674
[2019-04-04 00:24:22,854] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 24.0, 23.43037048939077, -0.04957258929416403, 0.0, 1.0, 19137.62446593648], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3608400.0000, 
sim time next is 3609000.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 24.0, 23.39069197993232, -0.05974215843842398, 0.0, 1.0, 18687.25333527253], 
processed observation next is [0.0, 0.782608695652174, 0.4349030470914128, 0.42, 0.0, 0.0, 0.5, 0.4492243316610267, 0.48008594718719205, 0.0, 1.0, 0.08898692064415491], 
reward next is 0.9110, 
noisyNet noise sample is [array([1.202759], dtype=float32), 0.16824041]. 
=============================================
[2019-04-04 00:24:22,865] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[84.56586]
 [84.41356]
 [84.16944]
 [84.18824]
 [85.20549]], R is [[84.7352829 ]
 [84.79679871]
 [84.84983826]
 [84.88603973]
 [84.92501831]].
[2019-04-04 00:24:25,745] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0541263e-21 1.5983287e-10 2.7558866e-10 1.0000000e+00 9.2289989e-16
 2.5639132e-10 5.0098780e-16], sum to 1.0000
[2019-04-04 00:24:25,746] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6054
[2019-04-04 00:24:25,760] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.0, 24.0, 106.5, 0.0, 23.0, 22.85861863153064, -0.2952968015164794, 1.0, 1.0, 4151.202593384575], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 2818800.0000, 
sim time next is 2819400.0000, 
raw observation next is [6.933333333333334, 24.16666666666666, 102.6666666666667, 0.0, 23.0, 22.6476737568177, -0.3104836526013962, 1.0, 1.0, 22018.50920648978], 
processed observation next is [1.0, 0.6521739130434783, 0.6546629732225301, 0.2416666666666666, 0.3422222222222223, 0.0, 0.4166666666666667, 0.3873061464014749, 0.39650544913286795, 1.0, 1.0, 0.10485004384042752], 
reward next is 0.8951, 
noisyNet noise sample is [array([1.0159891], dtype=float32), -0.6727804]. 
=============================================
[2019-04-04 00:24:26,237] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.5609103e-29 4.1629545e-20 7.2079764e-17 1.0000000e+00 6.9902675e-23
 4.3218973e-17 3.8621469e-22], sum to 1.0000
[2019-04-04 00:24:26,238] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5789
[2019-04-04 00:24:26,255] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.2, 100.0, 0.0, 0.0, 24.0, 23.44534991470695, -0.1229738589529214, 0.0, 1.0, 97107.58987993977], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3122400.0000, 
sim time next is 3123000.0000, 
raw observation next is [2.3, 100.0, 0.0, 0.0, 24.0, 23.44733455062168, -0.1226418682302452, 0.0, 1.0, 69327.02053105138], 
processed observation next is [1.0, 0.13043478260869565, 0.5263157894736843, 1.0, 0.0, 0.0, 0.5, 0.45394454588513994, 0.4591193772565849, 0.0, 1.0, 0.3301286691954828], 
reward next is 0.6699, 
noisyNet noise sample is [array([-0.05160087], dtype=float32), -0.5696601]. 
=============================================
[2019-04-04 00:24:26,270] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[95.27028 ]
 [94.869995]
 [94.71256 ]
 [94.802444]
 [94.99864 ]], R is [[95.26232147]
 [94.84728241]
 [94.43570709]
 [94.33135223]
 [94.38803864]].
[2019-04-04 00:24:31,859] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.7075449e-26 3.3840131e-16 4.7547813e-15 1.0000000e+00 2.4810580e-20
 1.2021867e-13 7.8911908e-21], sum to 1.0000
[2019-04-04 00:24:31,863] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9779
[2019-04-04 00:24:31,897] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.6366462e-27 4.1052757e-16 1.5457685e-13 1.0000000e+00 1.9413660e-21
 8.8825584e-15 2.4463774e-21], sum to 1.0000
[2019-04-04 00:24:31,898] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7185
[2019-04-04 00:24:31,907] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.0, 100.0, 0.0, 0.0, 24.0, 25.37362153444444, 0.4634391348015134, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3175200.0000, 
sim time next is 3175800.0000, 
raw observation next is [5.666666666666667, 100.0, 0.0, 0.0, 24.0, 25.38858778200419, 0.4528518530817602, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.6195752539242845, 1.0, 0.0, 0.0, 0.5, 0.6157156485003492, 0.65095061769392, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.23658757], dtype=float32), 0.90544415]. 
=============================================
[2019-04-04 00:24:31,911] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 24.0, 23.60017781126437, 0.1282617241202041, 1.0, 1.0, 118084.9087943923], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3265200.0000, 
sim time next is 3265800.0000, 
raw observation next is [-4.0, 66.0, 0.0, 0.0, 24.0, 23.61318484943021, 0.1696824180440124, 1.0, 1.0, 62301.59476665444], 
processed observation next is [1.0, 0.8260869565217391, 0.3518005540166205, 0.66, 0.0, 0.0, 0.5, 0.46776540411918405, 0.5565608060146708, 1.0, 1.0, 0.29667426079359255], 
reward next is 0.7033, 
noisyNet noise sample is [array([1.8953873], dtype=float32), -0.3430037]. 
=============================================
[2019-04-04 00:24:34,128] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.4371515e-23 4.5885247e-17 1.5372580e-15 1.0000000e+00 5.7493251e-19
 2.9805870e-13 2.5011016e-18], sum to 1.0000
[2019-04-04 00:24:34,128] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3623
[2019-04-04 00:24:34,151] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.1, 33.33333333333334, 0.0, 0.0, 23.0, 22.49314637473308, -0.3709737532079266, 0.0, 1.0, 56134.53139161791], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 2499000.0000, 
sim time next is 2499600.0000, 
raw observation next is [-1.0, 33.66666666666667, 0.0, 0.0, 23.0, 22.46825714399417, -0.3698678796194803, 0.0, 1.0, 59306.3633287461], 
processed observation next is [0.0, 0.9565217391304348, 0.4349030470914128, 0.3366666666666667, 0.0, 0.0, 0.4166666666666667, 0.37235476199951406, 0.37671070679350654, 0.0, 1.0, 0.28241125394641], 
reward next is 0.7176, 
noisyNet noise sample is [array([1.3108656], dtype=float32), -0.5076245]. 
=============================================
[2019-04-04 00:24:40,834] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.4546421e-22 7.5918975e-13 1.4511634e-12 1.0000000e+00 4.4678386e-18
 4.8689377e-12 8.0830497e-19], sum to 1.0000
[2019-04-04 00:24:40,836] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7319
[2019-04-04 00:24:40,903] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 67.0, 20.16666666666666, 186.3333333333333, 23.0, 24.42118982869333, -0.01406131637815411, 1.0, 1.0, 9340.205835115268], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 3432000.0000, 
sim time next is 3432600.0000, 
raw observation next is [2.0, 67.0, 12.0, 121.0, 23.0, 23.64147724349884, -0.07930699778259313, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.518005540166205, 0.67, 0.04, 0.13370165745856355, 0.4166666666666667, 0.4701231036249034, 0.47356433407246895, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.569808], dtype=float32), -1.7888371]. 
=============================================
[2019-04-04 00:24:43,060] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.6750893e-29 1.6433242e-19 1.7698641e-17 1.0000000e+00 2.0107984e-23
 7.3643453e-16 4.0079168e-22], sum to 1.0000
[2019-04-04 00:24:43,060] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1454
[2019-04-04 00:24:43,162] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 74.33333333333334, 0.0, 0.0, 24.0, 23.51511251025975, -0.08712061206015999, 0.0, 1.0, 40270.78014819238], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2856000.0000, 
sim time next is 2856600.0000, 
raw observation next is [1.0, 75.5, 0.0, 0.0, 24.0, 23.49034462595518, -0.1033634997777012, 0.0, 1.0, 58334.24273019091], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 0.755, 0.0, 0.0, 0.5, 0.45752871882959845, 0.46554550007409956, 0.0, 1.0, 0.27778210823900434], 
reward next is 0.7222, 
noisyNet noise sample is [array([-0.8253136], dtype=float32), -0.5054317]. 
=============================================
[2019-04-04 00:24:45,202] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.3824445e-21 2.3814447e-13 1.9043130e-11 1.0000000e+00 3.9456843e-17
 2.8010176e-09 2.3154916e-16], sum to 1.0000
[2019-04-04 00:24:45,202] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7936
[2019-04-04 00:24:45,261] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.5, 71.0, 0.0, 0.0, 23.0, 22.54822949386564, -0.3442308149694963, 0.0, 1.0, 18742.69710417181], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 3738600.0000, 
sim time next is 3739200.0000, 
raw observation next is [-3.666666666666667, 73.0, 0.0, 0.0, 23.0, 22.47979799410498, -0.3472071436588344, 0.0, 1.0, 61661.05335411015], 
processed observation next is [1.0, 0.2608695652173913, 0.3610341643582641, 0.73, 0.0, 0.0, 0.4166666666666667, 0.3733164995087484, 0.3842642854470552, 0.0, 1.0, 0.2936240635910007], 
reward next is 0.7064, 
noisyNet noise sample is [array([0.14856906], dtype=float32), 1.0378722]. 
=============================================
[2019-04-04 00:24:50,358] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.9762584e-24 6.5148418e-18 8.0193065e-15 1.0000000e+00 9.2889932e-19
 1.5323540e-12 8.2583675e-19], sum to 1.0000
[2019-04-04 00:24:50,358] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5253
[2019-04-04 00:24:50,385] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 68.0, 0.0, 0.0, 23.0, 22.52997466254416, -0.2374868537107294, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 3558600.0000, 
sim time next is 3559200.0000, 
raw observation next is [-4.666666666666666, 67.0, 0.0, 0.0, 23.0, 22.54590428809043, -0.2496966359049122, 0.0, 1.0, 18746.93264274243], 
processed observation next is [0.0, 0.17391304347826086, 0.33333333333333337, 0.67, 0.0, 0.0, 0.4166666666666667, 0.37882535734086914, 0.41676778803169595, 0.0, 1.0, 0.089271107822583], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.32395115], dtype=float32), 0.6563126]. 
=============================================
[2019-04-04 00:24:56,957] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.1843545e-22 5.0831366e-12 1.2426938e-11 1.0000000e+00 3.5364451e-16
 1.6713690e-08 1.7837521e-17], sum to 1.0000
[2019-04-04 00:24:56,957] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3613
[2019-04-04 00:24:57,006] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 93.0, 0.0, 0.0, 23.0, 22.48708483907544, -0.2401506957753002, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 2916000.0000, 
sim time next is 2916600.0000, 
raw observation next is [0.6666666666666667, 92.83333333333333, 0.0, 0.0, 23.0, 22.55078767077131, -0.2310321173501418, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.4810710987996307, 0.9283333333333332, 0.0, 0.0, 0.4166666666666667, 0.3792323058976092, 0.4229892942166194, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.9219587], dtype=float32), -0.14807174]. 
=============================================
[2019-04-04 00:24:59,079] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.1442607e-26 5.3969337e-19 2.0615316e-16 1.0000000e+00 1.3216965e-21
 3.0771517e-15 1.2666446e-20], sum to 1.0000
[2019-04-04 00:24:59,079] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3386
[2019-04-04 00:24:59,128] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.5, 62.5, 0.0, 0.0, 24.0, 23.11715737394024, -0.06872642555141423, 0.0, 1.0, 156789.3432401241], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3007800.0000, 
sim time next is 3008400.0000, 
raw observation next is [-2.666666666666667, 63.33333333333333, 0.0, 0.0, 24.0, 23.2930996488927, -0.04127245680255076, 0.0, 1.0, 83982.83868647684], 
processed observation next is [0.0, 0.8260869565217391, 0.38873499538319484, 0.6333333333333333, 0.0, 0.0, 0.5, 0.44109163740772495, 0.4862425143991498, 0.0, 1.0, 0.39991827945941355], 
reward next is 0.6001, 
noisyNet noise sample is [array([1.9795495], dtype=float32), 1.3986821]. 
=============================================
[2019-04-04 00:25:05,515] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.4416209e-28 4.6696924e-20 3.0500486e-16 1.0000000e+00 9.6666353e-23
 6.6955233e-16 5.5978458e-22], sum to 1.0000
[2019-04-04 00:25:05,515] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9449
[2019-04-04 00:25:05,630] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.833333333333333, 54.16666666666666, 109.3333333333333, 789.6666666666667, 24.0, 23.2948855190451, -0.0760281073064968, 0.0, 1.0, 18709.83238744741], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3064200.0000, 
sim time next is 3064800.0000, 
raw observation next is [-3.666666666666667, 54.33333333333334, 110.1666666666667, 797.3333333333334, 24.0, 23.30243297780856, -0.0793096290890714, 0.0, 1.0, 18709.34803092388], 
processed observation next is [0.0, 0.4782608695652174, 0.3610341643582641, 0.5433333333333334, 0.36722222222222234, 0.8810313075506446, 0.5, 0.44186941481738007, 0.47356345697030955, 0.0, 1.0, 0.0890921334805899], 
reward next is 0.9109, 
noisyNet noise sample is [array([-1.5756426], dtype=float32), -0.06270981]. 
=============================================
[2019-04-04 00:25:08,809] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.2286308e-32 7.2118396e-21 2.8295282e-18 1.0000000e+00 2.3060390e-26
 9.2078499e-19 1.6134598e-25], sum to 1.0000
[2019-04-04 00:25:08,809] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3973
[2019-04-04 00:25:08,834] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.0, 46.33333333333334, 98.33333333333334, 752.3333333333333, 24.0, 23.8054729672444, 0.07996899889972973, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3682200.0000, 
sim time next is 3682800.0000, 
raw observation next is [6.0, 47.0, 95.5, 740.5, 24.0, 23.7939787828135, 0.07692545734710564, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.6288088642659281, 0.47, 0.31833333333333336, 0.8182320441988951, 0.5, 0.48283156523445836, 0.5256418191157018, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.8401827], dtype=float32), -1.333867]. 
=============================================
[2019-04-04 00:25:16,336] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.8235465e-23 5.5047099e-16 3.7102848e-13 1.0000000e+00 2.5488667e-17
 5.7559589e-13 9.1755613e-18], sum to 1.0000
[2019-04-04 00:25:16,336] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6934
[2019-04-04 00:25:16,362] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 84.0, 0.0, 0.0, 23.0, 22.34588348332699, -0.2907578636345668, 0.0, 1.0, 45185.93732578849], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 2952000.0000, 
sim time next is 2952600.0000, 
raw observation next is [-3.0, 84.0, 0.0, 0.0, 23.0, 22.33258857255074, -0.2895788265552899, 0.0, 1.0, 44899.70860586032], 
processed observation next is [0.0, 0.17391304347826086, 0.3795013850415513, 0.84, 0.0, 0.0, 0.4166666666666667, 0.3610490477125617, 0.40347372448157004, 0.0, 1.0, 0.2138081362183825], 
reward next is 0.7862, 
noisyNet noise sample is [array([0.3690798], dtype=float32), -0.5791572]. 
=============================================
[2019-04-04 00:25:23,216] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.1457818e-27 6.5363746e-18 8.4437835e-15 1.0000000e+00 1.1553879e-22
 6.4764694e-15 8.6943613e-21], sum to 1.0000
[2019-04-04 00:25:23,216] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3038
[2019-04-04 00:25:23,243] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 60.0, 113.5, 798.5, 24.0, 24.95449082363515, 0.2490107926569196, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3841200.0000, 
sim time next is 3841800.0000, 
raw observation next is [-1.0, 60.0, 114.6666666666667, 806.3333333333334, 24.0, 24.98377737025535, 0.2372402241368698, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4349030470914128, 0.6, 0.38222222222222235, 0.89097605893186, 0.5, 0.5819814475212791, 0.57908007471229, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2065078], dtype=float32), -0.9971393]. 
=============================================
[2019-04-04 00:25:29,771] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.4295280e-21 4.1139315e-15 1.7520079e-11 1.0000000e+00 3.8652962e-17
 1.1949189e-10 4.2485532e-16], sum to 1.0000
[2019-04-04 00:25:29,771] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6401
[2019-04-04 00:25:29,798] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-13.5, 66.0, 0.0, 0.0, 23.0, 21.65645317679725, -0.5020396478531265, 0.0, 1.0, 44740.87258680706], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 3997800.0000, 
sim time next is 3998400.0000, 
raw observation next is [-13.66666666666667, 67.0, 0.0, 0.0, 23.0, 21.61233335081505, -0.5068238437359366, 0.0, 1.0, 44631.18279507889], 
processed observation next is [1.0, 0.2608695652173913, 0.08402585410895651, 0.67, 0.0, 0.0, 0.4166666666666667, 0.30102777923458746, 0.3310587187546878, 0.0, 1.0, 0.21252944188132802], 
reward next is 0.7875, 
noisyNet noise sample is [array([0.69540554], dtype=float32), -1.0396085]. 
=============================================
[2019-04-04 00:25:29,799] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.1980109e-21 5.9443017e-14 3.4819073e-10 1.0000000e+00 9.8874129e-17
 2.0752314e-10 2.5216450e-15], sum to 1.0000
[2019-04-04 00:25:29,800] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6101
[2019-04-04 00:25:29,816] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-9.666666666666666, 56.33333333333333, 0.0, 0.0, 23.0, 22.39152878831888, -0.2689165866448728, 0.0, 1.0, 50510.62753978503], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 3973200.0000, 
sim time next is 3973800.0000, 
raw observation next is [-9.833333333333334, 57.16666666666667, 0.0, 0.0, 23.0, 22.37549153587187, -0.2734049548337842, 0.0, 1.0, 56790.63422247645], 
processed observation next is [1.0, 1.0, 0.1902123730378578, 0.5716666666666668, 0.0, 0.0, 0.4166666666666667, 0.36462429465598917, 0.40886501505540523, 0.0, 1.0, 0.27043159153560214], 
reward next is 0.7296, 
noisyNet noise sample is [array([-0.840041], dtype=float32), 0.35750464]. 
=============================================
[2019-04-04 00:25:35,665] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.2906973e-25 2.7487794e-17 7.3360758e-14 1.0000000e+00 1.4191267e-20
 4.7065291e-14 4.5525860e-19], sum to 1.0000
[2019-04-04 00:25:35,665] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9100
[2019-04-04 00:25:35,707] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-12.0, 63.0, 0.0, 0.0, 24.0, 23.05250238508833, -0.1491183466983763, 0.0, 1.0, 45059.17709998106], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3982200.0000, 
sim time next is 3982800.0000, 
raw observation next is [-12.0, 63.00000000000001, 0.0, 0.0, 24.0, 22.99131067215698, -0.1531478884116098, 0.0, 1.0, 45030.40714519058], 
processed observation next is [1.0, 0.08695652173913043, 0.13019390581717452, 0.6300000000000001, 0.0, 0.0, 0.5, 0.4159425560130818, 0.44895070386279673, 0.0, 1.0, 0.21443051021519322], 
reward next is 0.7856, 
noisyNet noise sample is [array([-0.7395398], dtype=float32), 0.9103565]. 
=============================================
[2019-04-04 00:25:41,479] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.4288326e-27 3.3846389e-20 3.6217702e-18 1.0000000e+00 5.4936141e-23
 4.5762884e-17 1.7077480e-22], sum to 1.0000
[2019-04-04 00:25:41,499] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8273
[2019-04-04 00:25:41,519] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 71.16666666666667, 0.0, 0.0, 24.0, 23.47950974208256, 0.00682811807568182, 0.0, 1.0, 30708.75579024581], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3467400.0000, 
sim time next is 3468000.0000, 
raw observation next is [1.0, 70.33333333333334, 0.0, 0.0, 24.0, 23.58279301946222, 0.006595946429114198, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.4903047091412743, 0.7033333333333335, 0.0, 0.0, 0.5, 0.46523275162185157, 0.5021986488097047, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.95349526], dtype=float32), -0.9378927]. 
=============================================
[2019-04-04 00:25:41,548] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[90.48267]
 [90.51766]
 [90.43504]
 [90.34697]
 [90.34983]], R is [[90.34046936]
 [90.29083252]
 [90.0293045 ]
 [89.78579712]
 [89.64324951]].
[2019-04-04 00:25:49,172] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.7281918e-23 5.0902809e-18 2.1641217e-15 1.0000000e+00 4.4112384e-19
 1.3033724e-12 2.5744187e-18], sum to 1.0000
[2019-04-04 00:25:49,172] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2653
[2019-04-04 00:25:49,209] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-13.83333333333333, 68.0, 0.0, 0.0, 24.0, 22.24807718699427, -0.3525402361426338, 0.0, 1.0, 43999.3701863768], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3999000.0000, 
sim time next is 3999600.0000, 
raw observation next is [-14.0, 69.0, 0.0, 0.0, 24.0, 22.22677362672816, -0.3585869836852962, 0.0, 1.0, 43878.90105458448], 
processed observation next is [1.0, 0.30434782608695654, 0.07479224376731301, 0.69, 0.0, 0.0, 0.5, 0.35223113556068003, 0.3804710054382346, 0.0, 1.0, 0.2089471478789737], 
reward next is 0.7911, 
noisyNet noise sample is [array([-2.0177798], dtype=float32), -0.2543215]. 
=============================================
[2019-04-04 00:25:58,048] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.6407744e-27 2.5497114e-18 2.1912254e-17 1.0000000e+00 1.0288314e-22
 2.5967913e-16 7.7438141e-22], sum to 1.0000
[2019-04-04 00:25:58,070] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3606
[2019-04-04 00:25:58,078] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.333333333333334, 44.0, 116.8333333333333, 826.8333333333334, 24.0, 23.764066363289, 0.0601829567659409, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3673200.0000, 
sim time next is 3673800.0000, 
raw observation next is [4.5, 43.5, 117.0, 829.0, 24.0, 23.71553237041585, 0.05646541370350051, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.5872576177285319, 0.435, 0.39, 0.9160220994475138, 0.5, 0.4762943642013209, 0.5188218045678336, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.85455346], dtype=float32), -0.012874392]. 
=============================================
[2019-04-04 00:25:58,817] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.09521030e-22 9.86173355e-15 1.10556364e-11 1.00000000e+00
 9.31664277e-18 1.23942612e-09 4.98733694e-17], sum to 1.0000
[2019-04-04 00:25:58,819] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8759
[2019-04-04 00:25:58,864] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.166666666666667, 67.33333333333334, 0.0, 0.0, 23.0, 22.54403564415015, -0.2418446731163024, 0.0, 1.0, 42802.65665503726], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 4590600.0000, 
sim time next is 4591200.0000, 
raw observation next is [-1.233333333333333, 67.66666666666667, 0.0, 0.0, 23.0, 22.58529671884263, -0.2326741896622956, 0.0, 1.0, 18738.50890538112], 
processed observation next is [1.0, 0.13043478260869565, 0.4284395198522623, 0.6766666666666667, 0.0, 0.0, 0.4166666666666667, 0.3821080599035526, 0.42244193677923475, 0.0, 1.0, 0.08923099478752915], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.8404946], dtype=float32), -0.050629858]. 
=============================================
[2019-04-04 00:26:07,951] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0097000e-26 1.4841829e-20 2.3243590e-16 1.0000000e+00 1.2070597e-21
 3.6009422e-17 4.0017160e-21], sum to 1.0000
[2019-04-04 00:26:07,951] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4962
[2019-04-04 00:26:07,967] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 47.0, 0.0, 0.0, 24.0, 23.55324134162907, -0.0555707910901046, 0.0, 1.0, 42279.38174492102], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4226400.0000, 
sim time next is 4227000.0000, 
raw observation next is [1.0, 47.00000000000001, 0.0, 0.0, 24.0, 23.56124720644662, -0.05681159070801254, 0.0, 1.0, 32075.13130957244], 
processed observation next is [0.0, 0.9565217391304348, 0.4903047091412743, 0.4700000000000001, 0.0, 0.0, 0.5, 0.46343726720388495, 0.4810628030973292, 0.0, 1.0, 0.15273872052177354], 
reward next is 0.8473, 
noisyNet noise sample is [array([0.3132431], dtype=float32), 0.31095248]. 
=============================================
[2019-04-04 00:26:08,000] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[88.31281 ]
 [88.099525]
 [87.881096]
 [87.63963 ]
 [87.43579 ]], R is [[88.45352173]
 [88.36765289]
 [88.23561859]
 [87.99777222]
 [87.81908417]].
[2019-04-04 00:26:10,448] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.7746469e-25 3.8052706e-16 1.7134712e-14 1.0000000e+00 7.9189643e-21
 3.9573083e-16 4.1479902e-20], sum to 1.0000
[2019-04-04 00:26:10,450] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4182
[2019-04-04 00:26:10,460] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.166666666666667, 66.0, 116.3333333333333, 824.3333333333334, 23.0, 23.89133010098342, -0.01196060428025091, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 3757800.0000, 
sim time next is 3758400.0000, 
raw observation next is [-2.0, 65.0, 117.0, 825.5, 23.0, 23.8838674801376, -0.00704491958093945, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.40720221606648205, 0.65, 0.39, 0.9121546961325967, 0.4166666666666667, 0.4903222900114666, 0.4976516934730202, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7235516], dtype=float32), -0.016826253]. 
=============================================
[2019-04-04 00:26:17,661] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:26:17,661] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:26:17,669] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run15
[2019-04-04 00:26:26,067] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1840370e-23 4.9046077e-16 8.5306192e-14 1.0000000e+00 2.7622853e-18
 1.0196378e-11 7.9746906e-19], sum to 1.0000
[2019-04-04 00:26:26,073] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3730
[2019-04-04 00:26:26,088] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.25, 71.5, 0.0, 0.0, 23.0, 22.9096569210206, -0.2600122914132842, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 4329000.0000, 
sim time next is 4329600.0000, 
raw observation next is [4.166666666666667, 71.33333333333333, 0.0, 0.0, 23.0, 22.81078475058973, -0.2729634029444729, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.5780240073868884, 0.7133333333333333, 0.0, 0.0, 0.4166666666666667, 0.40089872921581077, 0.409012199018509, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.7218003], dtype=float32), 0.2925282]. 
=============================================
[2019-04-04 00:26:27,324] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.9262426e-25 4.0313722e-17 6.9799208e-14 1.0000000e+00 2.0394072e-20
 1.6060324e-14 9.6587618e-20], sum to 1.0000
[2019-04-04 00:26:27,324] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0000
[2019-04-04 00:26:27,342] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.8, 71.0, 0.0, 0.0, 24.0, 23.61728664748294, -9.716122374525682e-05, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4510800.0000, 
sim time next is 4511400.0000, 
raw observation next is [-0.8333333333333334, 71.0, 0.0, 0.0, 24.0, 23.61777364524091, -0.01863267479472755, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.43951985226223456, 0.71, 0.0, 0.0, 0.5, 0.46814780377007575, 0.49378910840175744, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9774607], dtype=float32), -0.883523]. 
=============================================
[2019-04-04 00:26:28,483] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.0699923e-26 2.8959891e-17 4.8251073e-16 1.0000000e+00 2.7960912e-20
 7.2073993e-13 5.6459975e-20], sum to 1.0000
[2019-04-04 00:26:28,484] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6745
[2019-04-04 00:26:28,521] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.95, 70.5, 0.0, 0.0, 23.0, 22.76909015126169, -0.2500139715908249, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 4332600.0000, 
sim time next is 4333200.0000, 
raw observation next is [3.933333333333333, 70.33333333333333, 0.0, 0.0, 23.0, 22.76313538327497, -0.2608368395327851, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.5715604801477379, 0.7033333333333333, 0.0, 0.0, 0.4166666666666667, 0.3969279486062476, 0.413054386822405, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8488062], dtype=float32), -1.1066244]. 
=============================================
[2019-04-04 00:26:37,254] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:26:37,254] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:26:37,261] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run15
[2019-04-04 00:26:40,493] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3160426e-27 3.0019100e-19 8.4081056e-15 1.0000000e+00 1.9183011e-22
 3.3229275e-14 7.7524191e-21], sum to 1.0000
[2019-04-04 00:26:40,493] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2753
[2019-04-04 00:26:40,531] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 24.0, 23.89336703035486, 0.1110907848995212, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4665600.0000, 
sim time next is 4666200.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 24.0, 23.85085596537844, 0.09719913409508718, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.518005540166205, 0.52, 0.0, 0.0, 0.5, 0.4875713304482033, 0.5323997113650291, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.01868295], dtype=float32), -1.0100017]. 
=============================================
[2019-04-04 00:26:49,971] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.6812811e-18 3.1325914e-12 2.7056584e-09 1.0000000e+00 1.7626382e-13
 8.7713508e-09 4.7552189e-14], sum to 1.0000
[2019-04-04 00:26:49,972] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9425
[2019-04-04 00:26:50,004] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-10.96666666666667, 61.0, 90.16666666666666, 536.3333333333334, 20.0, 20.57201407302752, -0.8659141416096388, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 297600.0000, 
sim time next is 298200.0000, 
raw observation next is [-10.78333333333333, 60.5, 93.33333333333334, 560.6666666666666, 20.0, 20.57253581696691, -0.8636951723972838, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.1638965835641737, 0.605, 0.3111111111111111, 0.6195211786372007, 0.16666666666666666, 0.21437798474724237, 0.2121016092009054, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.0637088], dtype=float32), -0.4736695]. 
=============================================
[2019-04-04 00:26:51,038] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.1455061e-27 2.0718515e-16 1.1977659e-15 1.0000000e+00 2.3810432e-22
 5.1886639e-16 1.4651181e-21], sum to 1.0000
[2019-04-04 00:26:51,045] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8544
[2019-04-04 00:26:51,074] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.0, 49.0, 129.5, 836.0, 23.0, 24.00407371818021, 0.1224044343601637, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 4626000.0000, 
sim time next is 4626600.0000, 
raw observation next is [4.116666666666667, 49.0, 132.6666666666667, 828.3333333333334, 23.0, 24.187403455759, 0.1377056118181854, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5766389658356418, 0.49, 0.4422222222222224, 0.9152854511970534, 0.4166666666666667, 0.5156169546465833, 0.5459018706060618, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.34087247], dtype=float32), -0.3383478]. 
=============================================
[2019-04-04 00:26:53,254] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:26:53,254] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:26:53,313] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run15
[2019-04-04 00:26:56,673] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:26:56,673] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:26:56,702] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run15
[2019-04-04 00:26:58,335] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:26:58,335] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:26:58,388] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run15
[2019-04-04 00:27:04,812] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.3841213e-28 2.9296777e-18 4.1017692e-16 1.0000000e+00 1.6360589e-23
 4.5572302e-17 4.8378651e-22], sum to 1.0000
[2019-04-04 00:27:04,813] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8015
[2019-04-04 00:27:04,824] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 37.0, 131.0, 737.0, 23.0, 22.50896585851297, -0.1958935914249136, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 4805400.0000, 
sim time next is 4806000.0000, 
raw observation next is [3.0, 37.0, 122.5, 734.5, 23.0, 22.5136692575008, -0.1872684961456421, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.5457063711911359, 0.37, 0.4083333333333333, 0.8116022099447514, 0.4166666666666667, 0.37613910479173346, 0.43757716795145263, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6930957], dtype=float32), -0.3136332]. 
=============================================
[2019-04-04 00:27:04,854] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[90.39013 ]
 [90.71876 ]
 [91.00608 ]
 [91.129616]
 [91.18188 ]], R is [[90.23799896]
 [90.33561707]
 [90.43225861]
 [90.52793884]
 [90.62265778]].
[2019-04-04 00:27:04,860] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:27:04,860] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:27:04,902] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run15
[2019-04-04 00:27:05,814] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.74621568e-25 7.68098741e-18 1.06584807e-14 1.00000000e+00
 1.55588589e-19 1.03932894e-13 1.72235139e-21], sum to 1.0000
[2019-04-04 00:27:05,815] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4533
[2019-04-04 00:27:05,822] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 23.0, 22.81930911542644, -0.1454871027445087, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 4471200.0000, 
sim time next is 4471800.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 23.0, 22.8949996858197, -0.1510046133637058, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.46260387811634357, 0.72, 0.0, 0.0, 0.4166666666666667, 0.40791664048497484, 0.44966512887876475, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9418288], dtype=float32), -0.25726512]. 
=============================================
[2019-04-04 00:27:10,031] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:27:10,032] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:27:10,038] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run15
[2019-04-04 00:27:12,053] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-04 00:27:12,072] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 00:27:12,072] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:27:12,075] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run20
[2019-04-04 00:27:12,100] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 00:27:12,102] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:27:12,102] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 00:27:12,103] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:27:12,106] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run20
[2019-04-04 00:27:12,122] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run20
[2019-04-04 00:27:47,519] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.76674515], dtype=float32), -0.055795476]
[2019-04-04 00:27:47,520] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-1.9365283875, 72.48023068, 136.0955682, 43.50301162, 20.0, 19.88754603173794, -0.9767283966039709, 1.0, 1.0, 0.0]
[2019-04-04 00:27:47,520] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 00:27:47,520] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [7.9857192e-16 6.5432181e-11 6.2813250e-08 9.9999952e-01 1.2552673e-10
 3.4842759e-07 7.0122001e-13], sampled 0.08437300586520635
[2019-04-04 00:29:30,879] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.76674515], dtype=float32), -0.055795476]
[2019-04-04 00:29:30,879] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [4.055096226333333, 46.72704632666667, 101.1482249133333, 885.6939453333333, 20.0, 20.46900210638785, -0.682096428867276, 0.0, 1.0, 0.0]
[2019-04-04 00:29:30,879] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 00:29:30,880] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.9478342e-21 5.8303769e-13 6.1323481e-11 1.0000000e+00 1.2998446e-16
 2.5324083e-11 1.8426445e-16], sampled 0.2898919454083583
[2019-04-04 00:29:32,811] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5809.7125 156139486.4740 -1723.3007
[2019-04-04 00:29:42,033] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5225.8116 179172362.7378 -2505.1061
[2019-04-04 00:29:50,560] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5276.9738 196700483.7194 -2224.2784
[2019-04-04 00:29:51,583] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 1900000, evaluation results [1900000.0, 5225.811625636795, 179172362.73779064, -2505.1061198071825, 5809.712497251498, 156139486.47400925, -1723.3007399773855, 5276.973798758629, 196700483.71941224, -2224.278359649255]
[2019-04-04 00:29:53,956] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:29:53,957] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:29:53,962] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run15
[2019-04-04 00:29:59,953] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.0980822e-29 4.8612532e-20 3.1942571e-14 1.0000000e+00 6.7359476e-24
 3.5555689e-18 1.5293679e-22], sum to 1.0000
[2019-04-04 00:29:59,953] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2684
[2019-04-04 00:29:59,964] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 55.0, 0.0, 0.0, 25.0, 24.53446237933613, 0.1906757394434219, 0.0, 1.0, 22430.24066451574], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 5025000.0000, 
sim time next is 5025600.0000, 
raw observation next is [-1.0, 55.0, 0.0, 0.0, 25.0, 24.48666869600667, 0.1834915867773628, 0.0, 1.0, 51881.77308413189], 
processed observation next is [1.0, 0.17391304347826086, 0.4349030470914128, 0.55, 0.0, 0.0, 0.5833333333333334, 0.5405557246672226, 0.561163862259121, 0.0, 1.0, 0.24705606230538996], 
reward next is 0.7529, 
noisyNet noise sample is [array([-1.5657077], dtype=float32), -0.2723575]. 
=============================================
[2019-04-04 00:30:00,037] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:30:00,037] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:30:00,040] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run15
[2019-04-04 00:30:00,197] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.1164623e-15 1.4329755e-08 1.5956786e-04 9.9983859e-01 6.4280881e-10
 1.7732373e-06 1.3140401e-11], sum to 1.0000
[2019-04-04 00:30:00,198] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5334
[2019-04-04 00:30:00,252] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.8999999999999999, 55.5, 27.0, 15.0, 20.0, 19.20432527880854, -1.096188926410328, 0.0, 1.0, 37603.31211814118], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 664200.0000, 
sim time next is 664800.0000, 
raw observation next is [-1.0, 56.0, 22.5, 12.83333333333333, 20.0, 19.19393400753282, -1.09699033000077, 0.0, 1.0, 41431.65068105124], 
processed observation next is [0.0, 0.6956521739130435, 0.4349030470914128, 0.56, 0.075, 0.014180478821362795, 0.16666666666666666, 0.09949450062773509, 0.13433655666641, 0.0, 1.0, 0.19729357467167255], 
reward next is 0.8027, 
noisyNet noise sample is [array([1.0502552], dtype=float32), -1.2793125]. 
=============================================
[2019-04-04 00:30:02,305] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:30:02,305] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:30:02,329] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run15
[2019-04-04 00:30:02,899] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.6317308e-28 6.8205347e-21 2.0075568e-15 1.0000000e+00 6.5579898e-22
 4.4935822e-17 1.6695322e-21], sum to 1.0000
[2019-04-04 00:30:02,900] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4982
[2019-04-04 00:30:02,946] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 24.0, 23.10412266716801, -0.1117726645817787, 0.0, 1.0, 42264.28565484816], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4768800.0000, 
sim time next is 4769400.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 24.0, 23.09898332579088, -0.1205786102180492, 0.0, 1.0, 42284.83303849374], 
processed observation next is [0.0, 0.17391304347826086, 0.296398891966759, 0.92, 0.0, 0.0, 0.5, 0.42491527714923993, 0.4598071299273169, 0.0, 1.0, 0.20135634780235115], 
reward next is 0.7986, 
noisyNet noise sample is [array([-1.0590253], dtype=float32), 1.27904]. 
=============================================
[2019-04-04 00:30:05,217] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:30:05,217] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:30:05,235] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run15
[2019-04-04 00:30:11,215] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:30:11,215] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:30:11,230] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run15
[2019-04-04 00:30:16,189] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.3482832e-08 1.3485760e-03 6.9427818e-02 8.8361651e-01 1.3888198e-04
 4.5466747e-02 1.3490371e-06], sum to 1.0000
[2019-04-04 00:30:16,189] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7550
[2019-04-04 00:30:16,242] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.7, 25.0, 125.5, 0.0, 20.0, 19.68895888052037, -1.104704048622417, 1.0, 1.0, 92140.96381610027], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 475200.0000, 
sim time next is 475800.0000, 
raw observation next is [-1.616666666666667, 25.5, 126.6666666666667, 0.0, 20.0, 19.83392283187322, -1.079434424080895, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.4178208679593721, 0.255, 0.42222222222222233, 0.0, 0.16666666666666666, 0.1528269026561017, 0.1401885253063683, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.6853305], dtype=float32), 0.620486]. 
=============================================
[2019-04-04 00:30:16,866] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.0849259e-25 8.1057301e-15 2.3004575e-13 1.0000000e+00 5.6573489e-16
 1.5916076e-15 3.5071676e-18], sum to 1.0000
[2019-04-04 00:30:16,868] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9651
[2019-04-04 00:30:16,879] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 38.0, 0.0, 0.0, 23.0, 22.91163437192412, -0.2281689428576756, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 4912200.0000, 
sim time next is 4912800.0000, 
raw observation next is [1.0, 37.33333333333334, 0.0, 0.0, 23.0, 22.89885224689279, -0.2352127136038575, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.4903047091412743, 0.3733333333333334, 0.0, 0.0, 0.4166666666666667, 0.40823768724106585, 0.42159576213204747, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.094478], dtype=float32), -0.95758545]. 
=============================================
[2019-04-04 00:30:17,845] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:30:17,846] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:30:17,852] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run15
[2019-04-04 00:30:20,219] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:30:20,219] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:30:20,226] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run15
[2019-04-04 00:30:21,893] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.8712705e-18 1.4397408e-09 7.4813711e-09 1.0000000e+00 1.4318310e-10
 4.3630319e-08 4.8202853e-13], sum to 1.0000
[2019-04-04 00:30:21,893] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1258
[2019-04-04 00:30:21,930] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 21.0, 20.39700714167293, -0.8453733046800281, 0.0, 1.0, 36825.31062114818], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 248400.0000, 
sim time next is 249000.0000, 
raw observation next is [-3.483333333333333, 66.66666666666667, 0.0, 0.0, 21.0, 20.42725077501907, -0.8467799181169449, 0.0, 1.0, 26384.35644417362], 
processed observation next is [1.0, 0.9130434782608695, 0.3661126500461681, 0.6666666666666667, 0.0, 0.0, 0.25, 0.2022708979182557, 0.2177400272943517, 0.0, 1.0, 0.12563979259130295], 
reward next is 0.8744, 
noisyNet noise sample is [array([1.557365], dtype=float32), 0.7057293]. 
=============================================
[2019-04-04 00:30:21,942] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[73.79724 ]
 [72.137695]
 [70.61065 ]
 [68.80892 ]
 [66.26649 ]], R is [[75.27645874]
 [75.34833527]
 [75.33331299]
 [75.25708008]
 [75.01478577]].
[2019-04-04 00:30:22,433] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:30:22,433] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:30:22,436] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run15
[2019-04-04 00:30:25,075] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.6408454e-10 1.2532271e-07 1.3378316e-04 9.9985921e-01 9.9623875e-08
 6.6566395e-06 1.4056280e-07], sum to 1.0000
[2019-04-04 00:30:25,075] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3063
[2019-04-04 00:30:25,129] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-12.46666666666667, 68.0, 40.83333333333333, 385.5, 21.0, 21.36768845072118, -0.6874347319009139, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 290400.0000, 
sim time next is 291000.0000, 
raw observation next is [-12.38333333333333, 67.5, 51.66666666666666, 385.0, 21.0, 21.45356086466939, -0.7009172629569774, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.1195752539242845, 0.675, 0.1722222222222222, 0.425414364640884, 0.25, 0.2877967387224493, 0.2663609123476742, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.1063569], dtype=float32), -0.53123164]. 
=============================================
[2019-04-04 00:30:25,132] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[40.861073]
 [42.7719  ]
 [44.906815]
 [47.305145]
 [49.94375 ]], R is [[38.97481537]
 [38.58506775]
 [38.19921875]
 [37.81722641]
 [37.4390564 ]].
[2019-04-04 00:30:28,505] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:30:28,505] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:30:28,508] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run15
[2019-04-04 00:30:29,539] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.0462550e-22 5.7461251e-11 5.0745880e-10 1.0000000e+00 2.1208994e-12
 1.8617145e-08 1.4130855e-15], sum to 1.0000
[2019-04-04 00:30:29,553] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8221
[2019-04-04 00:30:29,582] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.4333333333333333, 95.66666666666666, 0.0, 0.0, 20.0, 19.57879866856202, -0.8986198225008475, 0.0, 1.0, 68743.21097333306], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 80400.0000, 
sim time next is 81000.0000, 
raw observation next is [0.4, 95.5, 0.0, 0.0, 20.0, 19.59257328991198, -0.8924088635064592, 0.0, 1.0, 40108.91988702583], 
processed observation next is [0.0, 0.9565217391304348, 0.4736842105263158, 0.955, 0.0, 0.0, 0.16666666666666666, 0.13271444082599823, 0.20253037883118027, 0.0, 1.0, 0.1909948566048849], 
reward next is 0.8090, 
noisyNet noise sample is [array([-1.1797457], dtype=float32), -0.24986085]. 
=============================================
[2019-04-04 00:30:29,605] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[97.814224]
 [97.827805]
 [97.64703 ]
 [97.61687 ]
 [97.52442 ]], R is [[97.61667633]
 [97.31316376]
 [96.86100006]
 [96.38555908]
 [96.42170715]].
[2019-04-04 00:30:38,060] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2334573e-20 1.2330426e-10 6.3894812e-10 1.0000000e+00 4.9790424e-12
 1.2893556e-08 8.0522177e-14], sum to 1.0000
[2019-04-04 00:30:38,080] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6065
[2019-04-04 00:30:38,095] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 21.0, 20.39558858122388, -0.7978899722561207, 0.0, 1.0, 94658.17092669816], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 545400.0000, 
sim time next is 546000.0000, 
raw observation next is [0.5, 92.0, 0.0, 0.0, 21.0, 20.41628825762515, -0.787911538804365, 0.0, 1.0, 53640.24273230652], 
processed observation next is [0.0, 0.30434782608695654, 0.4764542936288089, 0.92, 0.0, 0.0, 0.25, 0.20135735480209593, 0.23736282039854498, 0.0, 1.0, 0.2554297272966977], 
reward next is 0.7446, 
noisyNet noise sample is [array([-0.80411935], dtype=float32), 0.7108383]. 
=============================================
[2019-04-04 00:30:38,116] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[92.702835]
 [92.67822 ]
 [92.67833 ]
 [92.77627 ]
 [92.87274 ]], R is [[92.53535461]
 [92.15924835]
 [91.75423431]
 [91.49832153]
 [91.49414062]].
[2019-04-04 00:30:38,276] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.0163182e-22 2.1514419e-11 7.2224515e-10 1.0000000e+00 1.4065534e-12
 2.0053050e-09 7.4613125e-15], sum to 1.0000
[2019-04-04 00:30:38,276] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1415
[2019-04-04 00:30:38,297] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.3333333333333334, 91.66666666666667, 28.5, 87.5, 21.0, 20.56308560340352, -0.7752902947464854, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 548400.0000, 
sim time next is 549000.0000, 
raw observation next is [0.25, 91.5, 34.0, 104.0, 21.0, 20.51630657756792, -0.7858549064932476, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.46952908587257625, 0.915, 0.11333333333333333, 0.11491712707182321, 0.25, 0.20969221479732672, 0.23804836450225078, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8970721], dtype=float32), 0.93224835]. 
=============================================
[2019-04-04 00:30:38,301] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[94.29364]
 [94.5138 ]
 [94.7342 ]
 [94.61774]
 [94.19834]], R is [[94.16736603]
 [94.22569275]
 [94.28343964]
 [94.34060669]
 [94.27676392]].
[2019-04-04 00:30:43,351] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.6129829e-24 1.0114249e-15 1.6392979e-12 1.0000000e+00 1.1247066e-17
 4.7928463e-13 9.0451049e-17], sum to 1.0000
[2019-04-04 00:30:43,351] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4266
[2019-04-04 00:30:43,392] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 25.0, 22.08594251481182, -0.4168151239766931, 0.0, 1.0, 45032.3819685438], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 187800.0000, 
sim time next is 188400.0000, 
raw observation next is [-8.900000000000002, 78.0, 0.0, 0.0, 25.0, 22.05187191901153, -0.4294431338478016, 0.0, 1.0, 45074.08108957193], 
processed observation next is [1.0, 0.17391304347826086, 0.2160664819944598, 0.78, 0.0, 0.0, 0.5833333333333334, 0.3376559932509607, 0.3568522887173995, 0.0, 1.0, 0.21463848137891398], 
reward next is 0.7854, 
noisyNet noise sample is [array([0.23688763], dtype=float32), 0.27701762]. 
=============================================
[2019-04-04 00:30:48,118] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.6472143e-28 2.3088399e-17 5.2026342e-17 1.0000000e+00 2.1173063e-23
 5.2545371e-15 3.9423816e-21], sum to 1.0000
[2019-04-04 00:30:48,120] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3893
[2019-04-04 00:30:48,156] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.4, 71.0, 0.0, 0.0, 26.0, 23.32103149779281, -0.1270998470938741, 0.0, 1.0, 44802.29883032703], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 172800.0000, 
sim time next is 173400.0000, 
raw observation next is [-8.483333333333334, 71.5, 0.0, 0.0, 26.0, 23.27364557723923, -0.1332551171413144, 0.0, 1.0, 44769.83813019574], 
processed observation next is [1.0, 0.0, 0.2276084949215143, 0.715, 0.0, 0.0, 0.6666666666666666, 0.43947046476993573, 0.4555816276195619, 0.0, 1.0, 0.21318970538188448], 
reward next is 0.7868, 
noisyNet noise sample is [array([-1.9125261], dtype=float32), 1.0646031]. 
=============================================
[2019-04-04 00:31:00,069] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.1396537e-22 2.2761390e-07 9.7458486e-09 9.9991238e-01 4.3999591e-13
 8.7379405e-05 2.5012201e-15], sum to 1.0000
[2019-04-04 00:31:00,078] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4103
[2019-04-04 00:31:00,093] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [15.31666666666667, 76.0, 46.33333333333334, 0.0, 23.0, 24.37535407768858, 0.1157355862409305, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 1007400.0000, 
sim time next is 1008000.0000, 
raw observation next is [15.5, 75.0, 41.0, 0.0, 23.0, 24.45027062200328, 0.1205880022985469, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.8919667590027703, 0.75, 0.13666666666666666, 0.0, 0.4166666666666667, 0.5375225518336068, 0.5401960007661822, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8669536], dtype=float32), -1.1009246]. 
=============================================
[2019-04-04 00:31:00,109] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[98.33364]
 [98.43693]
 [98.78924]
 [99.09115]
 [99.35496]], R is [[98.14196014]
 [98.16053772]
 [98.17893219]
 [98.19714355]
 [98.21517181]].
[2019-04-04 00:31:05,108] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.3691003e-22 2.4879759e-13 3.8881246e-12 1.0000000e+00 6.0079505e-14
 6.8726314e-12 2.2988059e-15], sum to 1.0000
[2019-04-04 00:31:05,111] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8070
[2019-04-04 00:31:05,127] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 66.33333333333334, 0.0, 0.0, 23.5, 22.97333039728517, -0.2678141098577213, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.5], 
sim time this is 678000.0000, 
sim time next is 678600.0000, 
raw observation next is [-3.1, 67.0, 0.0, 0.0, 23.5, 22.99250466797892, -0.2753110076372868, 0.0, 1.0, 18755.49476044805], 
processed observation next is [0.0, 0.8695652173913043, 0.37673130193905824, 0.67, 0.0, 0.0, 0.4583333333333333, 0.41604205566491004, 0.40822966412090445, 0.0, 1.0, 0.08931187981165739], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.6391603], dtype=float32), 2.4101014]. 
=============================================
[2019-04-04 00:31:07,917] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.46227860e-27 3.78696678e-16 2.43101776e-14 1.00000000e+00
 1.83210104e-18 1.41415666e-14 1.94580070e-20], sum to 1.0000
[2019-04-04 00:31:07,917] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1754
[2019-04-04 00:31:07,927] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.783333333333333, 81.66666666666667, 0.0, 0.0, 23.0, 22.70584200195966, -0.230619275989016, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 958200.0000, 
sim time next is 958800.0000, 
raw observation next is [6.966666666666667, 81.33333333333334, 0.0, 0.0, 23.0, 22.69836802147007, -0.2352856524865271, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.6555863342566944, 0.8133333333333335, 0.0, 0.0, 0.4166666666666667, 0.3915306684558392, 0.4215714491711576, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.93544924], dtype=float32), 0.29800737]. 
=============================================
[2019-04-04 00:31:18,413] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.2526598e-21 1.2104560e-13 2.1276465e-11 1.0000000e+00 2.7098101e-17
 8.1965762e-10 5.8814729e-14], sum to 1.0000
[2019-04-04 00:31:18,419] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2408
[2019-04-04 00:31:18,436] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.299999999999999, 71.0, 0.0, 0.0, 24.0, 22.91549526172698, -0.2081368247773622, 0.0, 1.0, 42724.0906416233], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 780600.0000, 
sim time next is 781200.0000, 
raw observation next is [-7.3, 71.0, 0.0, 0.0, 24.0, 22.89981061371366, -0.2108436237011069, 0.0, 1.0, 42691.95337012943], 
processed observation next is [1.0, 0.043478260869565216, 0.26038781163434904, 0.71, 0.0, 0.0, 0.5, 0.40831755114280516, 0.42971879209963104, 0.0, 1.0, 0.20329501604823538], 
reward next is 0.7967, 
noisyNet noise sample is [array([0.42384875], dtype=float32), -0.9033858]. 
=============================================
[2019-04-04 00:31:25,444] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.0981549e-30 4.7832473e-17 3.0026689e-17 1.0000000e+00 3.8996485e-21
 2.2010462e-15 5.3591974e-22], sum to 1.0000
[2019-04-04 00:31:25,448] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8681
[2019-04-04 00:31:25,460] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [12.93333333333334, 81.0, 0.0, 0.0, 24.0, 23.75412286614757, 0.1832219698742608, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1063200.0000, 
sim time next is 1063800.0000, 
raw observation next is [12.75, 81.5, 0.0, 0.0, 24.0, 23.97005742627158, 0.1909845201530214, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.8157894736842106, 0.815, 0.0, 0.0, 0.5, 0.4975047855226317, 0.5636615067176738, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2696394], dtype=float32), 0.23602994]. 
=============================================
[2019-04-04 00:31:27,651] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2197306e-25 3.5091804e-15 1.7536035e-10 1.0000000e+00 1.5747795e-18
 1.3114363e-13 4.0879096e-18], sum to 1.0000
[2019-04-04 00:31:27,654] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9687
[2019-04-04 00:31:27,665] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.6, 97.0, 0.0, 0.0, 24.5, 24.22742182743846, 0.2309985266192085, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.5], 
sim time this is 1656600.0000, 
sim time next is 1657200.0000, 
raw observation next is [6.6, 97.0, 0.0, 0.0, 24.5, 24.26241549861977, 0.2321394204686535, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.6454293628808865, 0.97, 0.0, 0.0, 0.5416666666666666, 0.5218679582183142, 0.5773798068228845, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.68349016], dtype=float32), -0.10992558]. 
=============================================
[2019-04-04 00:31:28,820] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.4298366e-28 6.6157733e-17 3.6243031e-16 1.0000000e+00 8.9632604e-23
 2.7863238e-15 2.7408371e-21], sum to 1.0000
[2019-04-04 00:31:28,822] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1810
[2019-04-04 00:31:28,846] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.116666666666667, 92.0, 20.33333333333334, 0.0, 24.0, 23.59329289031761, 0.03610478784807206, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1671000.0000, 
sim time next is 1671600.0000, 
raw observation next is [2.933333333333334, 92.0, 25.16666666666667, 0.0, 24.0, 23.52619513635782, 0.05235188806168073, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.543859649122807, 0.92, 0.0838888888888889, 0.0, 0.5, 0.4605162613631517, 0.5174506293538935, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.10560039], dtype=float32), -0.10067765]. 
=============================================
[2019-04-04 00:31:33,779] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1707528e-21 5.1210621e-15 6.8483973e-12 1.0000000e+00 4.0287145e-16
 8.4940693e-11 6.0533564e-16], sum to 1.0000
[2019-04-04 00:31:33,780] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9940
[2019-04-04 00:31:33,822] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.9, 82.0, 0.0, 0.0, 24.5, 23.57179188401613, -0.006338833538603446, 0.0, 1.0, 43210.15582740026], 
current ob forecast is [], 
actual action is [24.5], 
sim time this is 1792200.0000, 
sim time next is 1792800.0000, 
raw observation next is [-3.9, 82.0, 0.0, 0.0, 24.5, 23.57240017435992, -0.007543629398465729, 0.0, 1.0, 42281.74855561768], 
processed observation next is [0.0, 0.782608695652174, 0.3545706371191136, 0.82, 0.0, 0.0, 0.5416666666666666, 0.46436668119666, 0.4974854568671781, 0.0, 1.0, 0.20134165978865565], 
reward next is 0.7987, 
noisyNet noise sample is [array([0.26580426], dtype=float32), 0.014677445]. 
=============================================
[2019-04-04 00:31:43,555] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0675151e-24 4.2286738e-12 3.6520554e-11 1.0000000e+00 9.1995400e-13
 1.8574937e-08 1.7770207e-18], sum to 1.0000
[2019-04-04 00:31:43,556] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1678
[2019-04-04 00:31:43,589] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [14.4, 77.0, 0.0, 0.0, 24.0, 23.51039573469247, 0.05321420267261973, 0.0, 1.0, 196217.9094192961], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1023600.0000, 
sim time next is 1024200.0000, 
raw observation next is [14.4, 77.0, 0.0, 0.0, 24.0, 23.47588044382506, 0.07875530730492303, 0.0, 1.0, 197216.155496735], 
processed observation next is [1.0, 0.8695652173913043, 0.8614958448753465, 0.77, 0.0, 0.0, 0.5, 0.45632337031875486, 0.526251769101641, 0.0, 1.0, 0.9391245499844524], 
reward next is 0.0609, 
noisyNet noise sample is [array([-2.51559], dtype=float32), -2.2811358]. 
=============================================
[2019-04-04 00:31:56,584] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.0054879e-37 6.7655073e-25 9.6768502e-24 1.0000000e+00 1.1234824e-28
 3.5217934e-26 1.2297220e-28], sum to 1.0000
[2019-04-04 00:31:56,590] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3095
[2019-04-04 00:31:56,597] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [15.9, 86.33333333333334, 0.0, 0.0, 25.0, 23.44509610083879, 0.1130010780520811, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 1218000.0000, 
sim time next is 1218600.0000, 
raw observation next is [15.8, 88.0, 0.0, 0.0, 25.0, 23.43259688943134, 0.1062587824300735, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.08695652173913043, 0.9002770083102495, 0.88, 0.0, 0.0, 0.5833333333333334, 0.45271640745261177, 0.5354195941433578, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6757117], dtype=float32), 0.13346677]. 
=============================================
[2019-04-04 00:32:12,889] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.9194741e-28 8.8667058e-17 2.1323265e-15 1.0000000e+00 5.6568382e-21
 1.8831133e-16 5.5467720e-20], sum to 1.0000
[2019-04-04 00:32:12,890] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2972
[2019-04-04 00:32:12,902] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 26.0, 25.23806378453705, 0.4685704660468196, 0.0, 1.0, 39926.42350890626], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1389600.0000, 
sim time next is 1390200.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 26.0, 25.2129600109425, 0.4690087301531626, 0.0, 1.0, 39784.10413056616], 
processed observation next is [1.0, 0.08695652173913043, 0.46260387811634357, 0.95, 0.0, 0.0, 0.6666666666666666, 0.601080000911875, 0.6563362433843875, 0.0, 1.0, 0.1894481149074579], 
reward next is 0.8106, 
noisyNet noise sample is [array([-0.44702733], dtype=float32), -0.57676935]. 
=============================================
[2019-04-04 00:32:19,956] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.1685683e-22 1.6720684e-14 6.3943545e-11 1.0000000e+00 3.7510820e-15
 4.5700197e-12 4.2746636e-15], sum to 1.0000
[2019-04-04 00:32:19,975] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7138
[2019-04-04 00:32:20,048] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 69.0, 31.16666666666667, 0.0, 24.0, 22.91303112282966, -0.2027271924170867, 0.0, 1.0, 11489.2777921238], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2362800.0000, 
sim time next is 2363400.0000, 
raw observation next is [-3.4, 69.0, 37.0, 0.0, 24.0, 23.04252437493982, -0.1951036354792016, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.368421052631579, 0.69, 0.12333333333333334, 0.0, 0.5, 0.42021036457831834, 0.43496545484026616, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8967572], dtype=float32), -0.5979736]. 
=============================================
[2019-04-04 00:32:23,782] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.2268684e-31 2.2045816e-17 9.2455954e-16 1.0000000e+00 2.2119869e-24
 9.1981442e-15 1.8570918e-21], sum to 1.0000
[2019-04-04 00:32:23,782] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7345
[2019-04-04 00:32:23,793] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.0, 82.0, 0.0, 0.0, 26.0, 25.60149077807083, 0.5734062844466937, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1558200.0000, 
sim time next is 1558800.0000, 
raw observation next is [5.0, 82.0, 0.0, 0.0, 26.0, 25.68541211814303, 0.5628362439758472, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.6011080332409973, 0.82, 0.0, 0.0, 0.6666666666666666, 0.6404510098452526, 0.6876120813252825, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.158008], dtype=float32), 1.3952184]. 
=============================================
[2019-04-04 00:32:43,840] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.0465974e-23 8.1346815e-14 1.8175484e-11 1.0000000e+00 4.6726374e-18
 1.2146741e-10 1.3116101e-15], sum to 1.0000
[2019-04-04 00:32:43,840] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7774
[2019-04-04 00:32:43,856] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.2, 78.33333333333334, 0.0, 0.0, 24.0, 22.63414687642395, -0.2898121823584525, 0.0, 1.0, 42785.19190190649], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2181000.0000, 
sim time next is 2181600.0000, 
raw observation next is [-6.2, 79.0, 0.0, 0.0, 24.0, 22.58146329604108, -0.2931631847344018, 0.0, 1.0, 42784.41073472161], 
processed observation next is [1.0, 0.2608695652173913, 0.2908587257617729, 0.79, 0.0, 0.0, 0.5, 0.38178860800342324, 0.4022789384218661, 0.0, 1.0, 0.20373528921296005], 
reward next is 0.7963, 
noisyNet noise sample is [array([-0.885235], dtype=float32), 0.52004063]. 
=============================================
[2019-04-04 00:32:45,214] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.17110595e-29 2.58822818e-15 4.55179666e-15 1.00000000e+00
 1.38901442e-22 1.06302655e-13 3.47537228e-21], sum to 1.0000
[2019-04-04 00:32:45,214] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9878
[2019-04-04 00:32:45,236] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.0, 100.0, 102.8333333333333, 770.1666666666667, 25.0, 25.96305076797923, 0.5845386769607143, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 3162000.0000, 
sim time next is 3162600.0000, 
raw observation next is [7.0, 100.0, 101.0, 763.0, 25.0, 25.10473277485976, 0.5478093151068327, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.6565096952908588, 1.0, 0.33666666666666667, 0.8430939226519337, 0.5833333333333334, 0.5920610645716465, 0.6826031050356108, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.10407104], dtype=float32), 0.08241258]. 
=============================================
[2019-04-04 00:32:53,643] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.4382842e-26 3.4995137e-16 1.6632521e-13 1.0000000e+00 6.8841880e-20
 3.5793131e-12 2.8893559e-18], sum to 1.0000
[2019-04-04 00:32:53,643] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4145
[2019-04-04 00:32:53,694] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.333333333333333, 68.66666666666667, 0.0, 0.0, 24.5, 23.91295259082728, 0.03914031390497132, 0.0, 1.0, 45304.96234523314], 
current ob forecast is [], 
actual action is [24.5], 
sim time this is 2850000.0000, 
sim time next is 2850600.0000, 
raw observation next is [1.166666666666667, 70.33333333333333, 0.0, 0.0, 24.5, 23.90080137817119, 0.02090329793663402, 0.0, 1.0, 54395.83805696925], 
processed observation next is [1.0, 1.0, 0.49492151431209613, 0.7033333333333333, 0.0, 0.0, 0.5416666666666666, 0.4917334481809326, 0.5069677659788779, 0.0, 1.0, 0.25902780027128214], 
reward next is 0.7410, 
noisyNet noise sample is [array([-1.0561669], dtype=float32), 0.035330296]. 
=============================================
[2019-04-04 00:33:01,462] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.5392147e-23 5.1954401e-15 1.6061750e-12 1.0000000e+00 5.9988270e-17
 1.3567990e-11 2.1753192e-16], sum to 1.0000
[2019-04-04 00:33:01,462] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5798
[2019-04-04 00:33:01,527] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.95, 89.0, 45.0, 16.0, 24.0, 23.7483430486819, -0.09114489324110699, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2277000.0000, 
sim time next is 2277600.0000, 
raw observation next is [-8.766666666666666, 88.33333333333334, 54.33333333333333, 19.83333333333333, 24.0, 23.89042707293198, -0.07554475402458867, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.2197599261311173, 0.8833333333333334, 0.18111111111111108, 0.021915285451197048, 0.5, 0.49086892274433175, 0.47481841532513713, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0017546], dtype=float32), -0.04962632]. 
=============================================
[2019-04-04 00:33:02,028] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0784336e-27 3.5293795e-18 7.6907590e-16 1.0000000e+00 1.1918371e-21
 1.7032975e-15 7.1627896e-21], sum to 1.0000
[2019-04-04 00:33:02,030] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5971
[2019-04-04 00:33:02,057] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 92.0, 0.0, 0.0, 24.5, 23.91341081327739, 0.1604018929652809, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.5], 
sim time this is 3223200.0000, 
sim time next is 3223800.0000, 
raw observation next is [-3.0, 92.0, 1.0, 82.0, 24.5, 24.1293721175946, 0.1608751899271537, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.3795013850415513, 0.92, 0.0033333333333333335, 0.09060773480662983, 0.5416666666666666, 0.5107810097995499, 0.5536250633090513, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.08540941], dtype=float32), -2.3234515]. 
=============================================
[2019-04-04 00:33:04,558] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.7645747e-26 2.1123192e-17 2.2975699e-15 1.0000000e+00 4.8932753e-19
 9.3789536e-15 9.5245912e-20], sum to 1.0000
[2019-04-04 00:33:04,558] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7993
[2019-04-04 00:33:04,599] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 69.0, 0.0, 0.0, 24.5, 24.38506169996656, 0.2515617165647784, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.5], 
sim time this is 3267600.0000, 
sim time next is 3268200.0000, 
raw observation next is [-4.0, 70.0, 0.0, 0.0, 24.5, 24.29576059250556, 0.2300349142274188, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.3518005540166205, 0.7, 0.0, 0.0, 0.5416666666666666, 0.5246467160421299, 0.576678304742473, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6559814], dtype=float32), -1.3849268]. 
=============================================
[2019-04-04 00:33:06,577] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.6922633e-23 1.5518569e-13 2.8571043e-11 1.0000000e+00 3.0836146e-15
 2.7868197e-10 3.7089490e-16], sum to 1.0000
[2019-04-04 00:33:06,579] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1323
[2019-04-04 00:33:06,617] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.2, 64.66666666666667, 0.0, 0.0, 23.5, 22.83593465209384, -0.1026463657945931, 0.0, 1.0, 67546.5252940865], 
current ob forecast is [], 
actual action is [23.5], 
sim time this is 2667000.0000, 
sim time next is 2667600.0000, 
raw observation next is [-1.2, 65.0, 0.0, 0.0, 23.5, 23.03463236314242, -0.08057751543029755, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.42936288088642666, 0.65, 0.0, 0.0, 0.4583333333333333, 0.4195526969285351, 0.47314082818990083, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6383792], dtype=float32), -1.0420817]. 
=============================================
[2019-04-04 00:33:07,283] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.5293902e-23 3.0927799e-15 2.6955570e-13 1.0000000e+00 6.9995941e-19
 5.8023476e-15 8.7230159e-17], sum to 1.0000
[2019-04-04 00:33:07,284] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1237
[2019-04-04 00:33:07,304] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.7, 39.0, 0.0, 0.0, 24.0, 23.27960869341537, -0.1708204444457377, 0.0, 1.0, 59152.81733155325], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2511000.0000, 
sim time next is 2511600.0000, 
raw observation next is [-1.7, 38.66666666666667, 0.0, 0.0, 24.0, 23.40118998040879, -0.161415383930837, 0.0, 1.0, 30987.46480695668], 
processed observation next is [1.0, 0.043478260869565216, 0.4155124653739613, 0.3866666666666667, 0.0, 0.0, 0.5, 0.45009916503406594, 0.44619487202305436, 0.0, 1.0, 0.14755935622360322], 
reward next is 0.8524, 
noisyNet noise sample is [array([-0.6090341], dtype=float32), -2.5153146]. 
=============================================
[2019-04-04 00:33:07,647] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.0954707e-27 1.6438015e-18 2.8605923e-16 1.0000000e+00 2.3434424e-20
 4.6362415e-17 3.9201585e-20], sum to 1.0000
[2019-04-04 00:33:07,648] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8714
[2019-04-04 00:33:07,710] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.383333333333333, 62.5, 148.3333333333333, 402.0, 24.0, 23.0592342457737, -0.1262577078380557, 0.0, 1.0, 26195.91316641029], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2371800.0000, 
sim time next is 2372400.0000, 
raw observation next is [-2.3, 62.0, 153.0, 378.0, 24.0, 23.06936309158581, -0.1244390314451163, 0.0, 1.0, 21648.18612952248], 
processed observation next is [0.0, 0.4782608695652174, 0.3988919667590028, 0.62, 0.51, 0.4176795580110497, 0.5, 0.4224469242988175, 0.4585203228516279, 0.0, 1.0, 0.10308660061677372], 
reward next is 0.8969, 
noisyNet noise sample is [array([-2.0280478], dtype=float32), -0.20240512]. 
=============================================
[2019-04-04 00:33:15,263] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.4767539e-26 8.0946909e-17 1.4781135e-14 1.0000000e+00 4.2562333e-20
 1.3079870e-16 8.8570623e-19], sum to 1.0000
[2019-04-04 00:33:15,263] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5729
[2019-04-04 00:33:15,324] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 54.0, 106.8333333333333, 766.6666666666666, 23.0, 22.30227298494595, -0.3063991620465249, 0.0, 1.0, 18709.33555365107], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 3062400.0000, 
sim time next is 3063000.0000, 
raw observation next is [-4.0, 54.0, 107.6666666666667, 774.3333333333333, 23.0, 22.29749485665157, -0.2970620850077038, 0.0, 1.0, 18708.78818157545], 
processed observation next is [0.0, 0.43478260869565216, 0.3518005540166205, 0.54, 0.358888888888889, 0.8556169429097605, 0.4166666666666667, 0.35812457138763093, 0.400979304997432, 0.0, 1.0, 0.08908946753131168], 
reward next is 0.9109, 
noisyNet noise sample is [array([1.2732856], dtype=float32), -0.9518331]. 
=============================================
[2019-04-04 00:33:15,335] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[78.03987 ]
 [78.24223 ]
 [78.260124]
 [78.24772 ]
 [78.16797 ]], R is [[77.78858948]
 [77.9216156 ]
 [78.05330658]
 [78.18367767]
 [78.31273651]].
[2019-04-04 00:33:25,126] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.2390847e-25 8.0329928e-15 1.0275221e-13 1.0000000e+00 1.3674095e-18
 4.2185273e-14 2.1681023e-17], sum to 1.0000
[2019-04-04 00:33:25,128] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5682
[2019-04-04 00:33:25,166] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 24.0, 23.27152391088733, -0.130128478631602, 0.0, 1.0, 44412.96526115353], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2773800.0000, 
sim time next is 2774400.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 24.0, 23.28681555520779, -0.1270049938854285, 0.0, 1.0, 43076.89817413629], 
processed observation next is [1.0, 0.08695652173913043, 0.296398891966759, 0.59, 0.0, 0.0, 0.5, 0.44056796293398265, 0.4576650020381905, 0.0, 1.0, 0.20512808654350614], 
reward next is 0.7949, 
noisyNet noise sample is [array([-0.16106562], dtype=float32), 0.24906324]. 
=============================================
[2019-04-04 00:33:26,039] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.8798178e-24 7.2122394e-17 8.9882669e-13 1.0000000e+00 7.6141818e-18
 5.5732773e-15 1.4315801e-17], sum to 1.0000
[2019-04-04 00:33:26,039] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0954
[2019-04-04 00:33:26,060] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.666666666666667, 79.33333333333333, 0.0, 0.0, 23.5, 22.65803206228842, -0.2222446897685636, 0.0, 1.0, 43888.24034103844], 
current ob forecast is [], 
actual action is [23.5], 
sim time this is 2958000.0000, 
sim time next is 2958600.0000, 
raw observation next is [-3.833333333333333, 78.16666666666667, 0.0, 0.0, 23.5, 22.63914378644555, -0.2280764589658476, 0.0, 1.0, 43846.51250875685], 
processed observation next is [0.0, 0.21739130434782608, 0.3564173591874424, 0.7816666666666667, 0.0, 0.0, 0.4583333333333333, 0.3865953155371293, 0.4239745136780508, 0.0, 1.0, 0.20879291670836597], 
reward next is 0.7912, 
noisyNet noise sample is [array([-0.10010981], dtype=float32), 1.0158699]. 
=============================================
[2019-04-04 00:33:26,985] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1262911e-24 5.4487168e-17 3.4504460e-13 1.0000000e+00 2.0635448e-17
 6.6867112e-14 3.1806097e-17], sum to 1.0000
[2019-04-04 00:33:26,985] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2449
[2019-04-04 00:33:27,005] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.5, 100.0, 0.0, 0.0, 23.0, 22.54679454455104, -0.3138144502226032, 0.0, 1.0, 30885.79198752392], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 3112200.0000, 
sim time next is 3112800.0000, 
raw observation next is [0.6666666666666666, 100.0, 0.0, 0.0, 23.0, 22.53725752465758, -0.3157877780088486, 0.0, 1.0, 37544.10246988027], 
processed observation next is [1.0, 0.0, 0.4810710987996307, 1.0, 0.0, 0.0, 0.4166666666666667, 0.378104793721465, 0.39473740733038376, 0.0, 1.0, 0.1787814403327632], 
reward next is 0.8212, 
noisyNet noise sample is [array([-0.79370946], dtype=float32), -0.46051297]. 
=============================================
[2019-04-04 00:33:59,960] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.2880439e-29 2.1275110e-20 1.3000273e-17 1.0000000e+00 4.2078279e-23
 1.5500049e-17 1.0600950e-20], sum to 1.0000
[2019-04-04 00:33:59,960] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3911
[2019-04-04 00:33:59,984] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-13.95, 87.0, 0.0, 0.0, 26.0, 23.69572964040876, 0.0258420922296286, 0.0, 1.0, 44524.80785912919], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2691000.0000, 
sim time next is 2691600.0000, 
raw observation next is [-14.3, 88.33333333333334, 0.0, 0.0, 26.0, 23.60213693655862, 0.01520665941244506, 0.0, 1.0, 44530.61766185311], 
processed observation next is [1.0, 0.13043478260869565, 0.06648199445983377, 0.8833333333333334, 0.0, 0.0, 0.6666666666666666, 0.46684474471321824, 0.505068886470815, 0.0, 1.0, 0.21205056029453861], 
reward next is 0.7879, 
noisyNet noise sample is [array([-0.94440895], dtype=float32), 0.5490844]. 
=============================================
[2019-04-04 00:34:14,486] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.3492813e-25 4.7159130e-13 1.6364729e-13 1.0000000e+00 6.5822106e-19
 1.8921820e-13 2.6590695e-18], sum to 1.0000
[2019-04-04 00:34:14,512] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1552
[2019-04-04 00:34:14,568] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.333333333333334, 65.66666666666667, 0.0, 0.0, 25.0, 25.03739091120637, 0.4107278353961075, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 4412400.0000, 
sim time next is 4413000.0000, 
raw observation next is [6.216666666666666, 65.83333333333333, 0.0, 0.0, 25.0, 25.05218675257229, 0.4078438221709543, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.6348107109879964, 0.6583333333333333, 0.0, 0.0, 0.5833333333333334, 0.5876822293810241, 0.6359479407236514, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6068628], dtype=float32), -0.73415834]. 
=============================================
[2019-04-04 00:34:14,618] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[82.67362]
 [82.94324]
 [83.19174]
 [83.45013]
 [83.71217]], R is [[82.54354858]
 [82.71811676]
 [82.89093781]
 [83.06202698]
 [83.23140717]].
[2019-04-04 00:34:16,454] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-04-04 00:34:16,457] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 00:34:16,457] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:34:16,459] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run21
[2019-04-04 00:34:16,586] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 00:34:16,587] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:34:16,589] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run21
[2019-04-04 00:34:16,682] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 00:34:16,683] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:34:16,685] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run21
[2019-04-04 00:34:38,956] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.36066568], dtype=float32), -0.036395147]
[2019-04-04 00:34:38,956] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-9.65, 56.0, 133.0, 571.0, 19.0, 19.96168119411274, -0.9476322681029092, 1.0, 1.0, 0.0]
[2019-04-04 00:34:38,956] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 00:34:38,957] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [7.10470526e-15 3.65115693e-09 1.69918522e-07 9.99999046e-01
 1.16027394e-07 6.65015534e-07 5.62268704e-11], sampled 0.9699723828470682
[2019-04-04 00:35:01,191] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.36066568], dtype=float32), -0.036395147]
[2019-04-04 00:35:01,191] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-2.845011825, 86.614561255, 0.0, 0.0, 19.0, 18.57053872794179, -1.245901229005143, 0.0, 1.0, 37072.06266787464]
[2019-04-04 00:35:01,191] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 00:35:01,192] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [9.9018016e-15 3.8150525e-08 1.1319739e-06 9.9997687e-01 1.5507355e-07
 2.1897302e-05 4.1450920e-10], sampled 0.4777073271968182
[2019-04-04 00:35:25,022] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.36066568], dtype=float32), -0.036395147]
[2019-04-04 00:35:25,022] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-8.4, 78.0, 0.0, 0.0, 19.0, 18.4869056045635, -1.278224567354356, 0.0, 1.0, 46544.84994583672]
[2019-04-04 00:35:25,022] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 00:35:25,023] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.7676878e-14 3.4522707e-09 6.7277898e-07 9.9999261e-01 9.9842246e-08
 6.5643021e-06 5.7577038e-10], sampled 0.5127270578339038
[2019-04-04 00:36:31,096] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5734.8221 153532582.9583 -1822.5091
[2019-04-04 00:36:45,070] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5230.2655 179172060.0189 -2512.1539
[2019-04-04 00:36:56,733] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5434.1225 195954217.4902 -2226.9717
[2019-04-04 00:36:57,763] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 2000000, evaluation results [2000000.0, 5230.265480240372, 179172060.01891604, -2512.1538676195905, 5734.822143518929, 153532582.9582647, -1822.509139824684, 5434.122467003905, 195954217.4902176, -2226.9716797039914]
[2019-04-04 00:37:00,614] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.7961003e-21 2.9474695e-13 9.8551348e-11 1.0000000e+00 1.1938162e-14
 8.4822947e-12 1.3097445e-16], sum to 1.0000
[2019-04-04 00:37:00,614] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7940
[2019-04-04 00:37:00,640] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.6666666666666667, 68.66666666666667, 0.0, 0.0, 23.0, 22.82332394205393, -0.2417629897675803, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 3706800.0000, 
sim time next is 3707400.0000, 
raw observation next is [0.3333333333333333, 70.33333333333333, 0.0, 0.0, 23.0, 22.7314809185719, -0.2611510159985843, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.9130434782608695, 0.4718374884579871, 0.7033333333333333, 0.0, 0.0, 0.4166666666666667, 0.3942900765476584, 0.41294966133380523, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.04971992], dtype=float32), -1.017729]. 
=============================================
[2019-04-04 00:37:00,828] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.5477423e-22 1.4266774e-12 1.5511158e-11 1.0000000e+00 1.1140411e-15
 6.3988532e-12 3.7199302e-16], sum to 1.0000
[2019-04-04 00:37:00,828] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3384
[2019-04-04 00:37:00,841] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.3333333333333333, 74.0, 0.0, 0.0, 23.5, 23.64965587248208, 0.09237503119220354, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.5], 
sim time this is 3532800.0000, 
sim time next is 3533400.0000, 
raw observation next is [-0.5, 75.0, 0.0, 0.0, 23.5, 23.65298786764502, 0.0821442037933882, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.44875346260387816, 0.75, 0.0, 0.0, 0.4583333333333333, 0.4710823223037517, 0.5273814012644628, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9105872], dtype=float32), 0.6836973]. 
=============================================
[2019-04-04 00:37:05,382] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.3030036e-21 7.3870667e-12 3.8386402e-10 1.0000000e+00 1.7803626e-14
 6.3435823e-13 7.7430657e-17], sum to 1.0000
[2019-04-04 00:37:05,383] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9775
[2019-04-04 00:37:05,421] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 71.0, 0.0, 0.0, 23.0, 22.84104356684891, -0.1715818466061657, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 3783000.0000, 
sim time next is 3783600.0000, 
raw observation next is [-2.0, 71.0, 0.0, 0.0, 23.0, 22.75097543998664, -0.2059446900731826, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.40720221606648205, 0.71, 0.0, 0.0, 0.4166666666666667, 0.3959146199988866, 0.43135176997560576, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.58976954], dtype=float32), 0.34488472]. 
=============================================
[2019-04-04 00:37:07,885] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.6371322e-27 7.8010744e-20 1.8580744e-16 1.0000000e+00 2.6526725e-21
 1.9426357e-18 1.5810078e-20], sum to 1.0000
[2019-04-04 00:37:07,927] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5636
[2019-04-04 00:37:07,952] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 24.0, 23.66917176457614, 0.03839896298994389, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3614400.0000, 
sim time next is 3615000.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 24.0, 23.81823743632943, 0.04247508661886071, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.4349030470914128, 0.42, 0.0, 0.0, 0.5, 0.4848531196941191, 0.514158362206287, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8565602], dtype=float32), 0.50806195]. 
=============================================
[2019-04-04 00:37:08,011] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[85.42182 ]
 [85.55102 ]
 [85.259636]
 [84.6971  ]
 [83.90601 ]], R is [[85.43836975]
 [85.58398438]
 [85.45707703]
 [84.8650589 ]
 [84.06951141]].
[2019-04-04 00:37:10,075] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0632791e-26 2.4148314e-16 8.3349704e-16 1.0000000e+00 2.4658788e-18
 2.3956563e-15 2.3001334e-20], sum to 1.0000
[2019-04-04 00:37:10,078] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4278
[2019-04-04 00:37:10,091] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [8.5, 29.5, 4.0, 121.0, 23.5, 23.13240022137055, -0.1862189220506314, 0.0, 1.0, 36054.70059096558], 
current ob forecast is [], 
actual action is [23.5], 
sim time this is 3655800.0000, 
sim time next is 3656400.0000, 
raw observation next is [8.333333333333334, 30.33333333333334, 18.16666666666666, 168.0, 23.5, 23.12818122967576, -0.1751391679907582, 0.0, 1.0, 32591.12687083356], 
processed observation next is [0.0, 0.30434782608695654, 0.6934441366574331, 0.3033333333333334, 0.060555555555555536, 0.1856353591160221, 0.4583333333333333, 0.4273484358063134, 0.44162027733641396, 0.0, 1.0, 0.15519584224206456], 
reward next is 0.8448, 
noisyNet noise sample is [array([1.2275618], dtype=float32), 0.30700007]. 
=============================================
[2019-04-04 00:37:11,112] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0738446e-26 4.3739435e-17 4.0977053e-15 1.0000000e+00 2.0885601e-19
 7.0176548e-15 4.7647193e-20], sum to 1.0000
[2019-04-04 00:37:11,113] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2092
[2019-04-04 00:37:11,206] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.6666666666666666, 71.33333333333333, 17.16666666666666, 155.6666666666667, 24.0, 23.44534077654924, -0.06625872856257653, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3483600.0000, 
sim time next is 3484200.0000, 
raw observation next is [-0.8333333333333334, 71.16666666666667, 31.33333333333333, 204.3333333333333, 24.0, 23.35828872000376, -0.03962514286056951, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.43951985226223456, 0.7116666666666667, 0.10444444444444442, 0.22578268876611413, 0.5, 0.4465240600003133, 0.4867916190464768, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.28969303], dtype=float32), 1.0777127]. 
=============================================
[2019-04-04 00:37:14,568] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.6671812e-24 1.2781903e-15 2.3772208e-12 1.0000000e+00 4.6420437e-18
 1.8255653e-13 4.5512017e-17], sum to 1.0000
[2019-04-04 00:37:14,570] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6082
[2019-04-04 00:37:14,616] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.833333333333334, 40.5, 0.0, 0.0, 24.0, 23.28105092481232, -0.1554852069207917, 0.0, 1.0, 41338.96352455727], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4085400.0000, 
sim time next is 4086000.0000, 
raw observation next is [-5.0, 41.0, 0.0, 0.0, 24.0, 23.27201218585099, -0.1581888102036551, 0.0, 1.0, 41249.58941843362], 
processed observation next is [1.0, 0.30434782608695654, 0.32409972299168976, 0.41, 0.0, 0.0, 0.5, 0.4393343488209158, 0.44727039659878165, 0.0, 1.0, 0.19642661627825533], 
reward next is 0.8036, 
noisyNet noise sample is [array([-0.1528619], dtype=float32), 1.2440945]. 
=============================================
[2019-04-04 00:37:14,628] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[84.98631 ]
 [85.19441 ]
 [85.36864 ]
 [85.49858 ]
 [85.583405]], R is [[84.75785828]
 [84.71342468]
 [84.66890717]
 [84.62398529]
 [84.57748413]].
[2019-04-04 00:37:16,742] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.9103748e-25 9.4283396e-17 1.3515839e-15 1.0000000e+00 6.5663107e-19
 1.5245968e-15 1.7374526e-18], sum to 1.0000
[2019-04-04 00:37:16,742] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2910
[2019-04-04 00:37:16,757] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 24.0, 23.29972365377377, -0.07179864968471465, 0.0, 1.0, 73648.08526641119], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3375600.0000, 
sim time next is 3376200.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 24.0, 23.32132789699862, -0.07134385003326427, 0.0, 1.0, 54128.56396763489], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.77, 0.0, 0.0, 0.5, 0.4434439914165518, 0.4762187166555785, 0.0, 1.0, 0.2577550665125471], 
reward next is 0.7422, 
noisyNet noise sample is [array([-1.2804325], dtype=float32), -0.4547074]. 
=============================================
[2019-04-04 00:37:46,014] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.9197811e-26 8.2109389e-18 7.0920017e-18 1.0000000e+00 4.3348688e-20
 3.6315512e-18 5.6505839e-21], sum to 1.0000
[2019-04-04 00:37:46,015] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7871
[2019-04-04 00:37:46,050] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.8, 70.33333333333334, 0.0, 0.0, 23.0, 22.54667511339852, -0.2822912038304797, 0.0, 1.0, 196217.9094192961], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 4303200.0000, 
sim time next is 4303800.0000, 
raw observation next is [5.7, 71.0, 0.0, 0.0, 23.0, 22.51007527651764, -0.2628724474940891, 0.0, 1.0, 196217.9094192961], 
processed observation next is [0.0, 0.8260869565217391, 0.6204986149584488, 0.71, 0.0, 0.0, 0.4166666666666667, 0.37583960637646996, 0.41237585083530365, 0.0, 1.0, 0.9343709972347434], 
reward next is 0.0656, 
noisyNet noise sample is [array([1.0012765], dtype=float32), 1.4170152]. 
=============================================
[2019-04-04 00:37:48,099] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.9779002e-32 3.3605241e-23 1.6200454e-19 1.0000000e+00 2.9775289e-25
 9.4387420e-24 6.6928845e-26], sum to 1.0000
[2019-04-04 00:37:48,102] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8778
[2019-04-04 00:37:48,131] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 42.33333333333334, 70.66666666666667, 576.3333333333333, 25.0, 24.48204456069402, 0.2609789617872429, 0.0, 1.0, 9342.04199101278], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 3600600.0000, 
sim time next is 3601200.0000, 
raw observation next is [0.0, 41.66666666666667, 66.83333333333333, 545.6666666666666, 25.0, 24.47486861631351, 0.2573583331942906, 0.0, 1.0, 9342.134881357224], 
processed observation next is [0.0, 0.6956521739130435, 0.46260387811634357, 0.41666666666666674, 0.22277777777777777, 0.6029465930018416, 0.5833333333333334, 0.5395723846927926, 0.5857861110647635, 0.0, 1.0, 0.044486356577891545], 
reward next is 0.9555, 
noisyNet noise sample is [array([0.4632868], dtype=float32), -0.80921286]. 
=============================================
[2019-04-04 00:37:52,494] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.9874713e-24 2.6417212e-17 4.1686483e-14 1.0000000e+00 6.6478736e-18
 2.9940739e-13 8.3409141e-17], sum to 1.0000
[2019-04-04 00:37:52,494] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6205
[2019-04-04 00:37:52,537] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-13.66666666666667, 67.0, 0.0, 0.0, 24.0, 22.25402111940327, -0.347122702547443, 0.0, 1.0, 44116.84277676159], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3998400.0000, 
sim time next is 3999000.0000, 
raw observation next is [-13.83333333333333, 68.0, 0.0, 0.0, 24.0, 22.24807771204348, -0.3525400585114024, 0.0, 1.0, 43999.36995833052], 
processed observation next is [1.0, 0.2608695652173913, 0.07940904893813489, 0.68, 0.0, 0.0, 0.5, 0.3540064760036232, 0.38248664716286584, 0.0, 1.0, 0.20952080932538344], 
reward next is 0.7905, 
noisyNet noise sample is [array([-0.51132506], dtype=float32), 0.15143365]. 
=============================================
[2019-04-04 00:37:52,549] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[81.31606 ]
 [81.2822  ]
 [81.262955]
 [81.24136 ]
 [81.19823 ]], R is [[81.31860352]
 [81.29533386]
 [81.27179718]
 [81.24793243]
 [81.22370911]].
[2019-04-04 00:37:52,594] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:37:52,594] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:37:52,600] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run16
[2019-04-04 00:37:59,047] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.1832662e-28 1.4164287e-19 8.2131895e-19 1.0000000e+00 9.7638078e-23
 2.2183555e-19 8.7384139e-22], sum to 1.0000
[2019-04-04 00:37:59,048] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2703
[2019-04-04 00:37:59,066] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.66633888777684, 0.6105736360300071, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3531600.0000, 
sim time next is 3532200.0000, 
raw observation next is [-0.1666666666666667, 73.0, 0.0, 0.0, 26.0, 25.84262895824235, 0.620250218382216, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.4579870729455217, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6535524131868625, 0.706750072794072, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6703741], dtype=float32), -0.7701377]. 
=============================================
[2019-04-04 00:38:00,661] A3C_AGENT_WORKER-Thread-2 INFO:Local step 127500, global step 2021630: loss 0.9436
[2019-04-04 00:38:00,662] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 127500, global step 2021630: learning rate 0.0005
[2019-04-04 00:38:06,878] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:38:06,879] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:38:06,903] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run16
[2019-04-04 00:38:14,524] A3C_AGENT_WORKER-Thread-19 INFO:Local step 127500, global step 2028318: loss 0.8910
[2019-04-04 00:38:14,526] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 127500, global step 2028318: learning rate 0.0005
[2019-04-04 00:38:14,569] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [8.2748833e-36 2.7548041e-25 1.0949931e-22 1.0000000e+00 6.0919387e-27
 1.0440279e-24 1.4774047e-27], sum to 1.0000
[2019-04-04 00:38:14,571] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8554
[2019-04-04 00:38:14,582] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [8.333333333333334, 25.66666666666667, 123.8333333333333, 861.6666666666667, 24.5, 26.27941538520233, 0.5862460910234943, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.5], 
sim time this is 5055600.0000, 
sim time next is 5056200.0000, 
raw observation next is [8.5, 25.5, 124.0, 865.0, 24.5, 26.34235054407721, 0.6164670439896163, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.698060941828255, 0.255, 0.41333333333333333, 0.9558011049723757, 0.5416666666666666, 0.6951958786731008, 0.7054890146632055, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.78647476], dtype=float32), 0.25691226]. 
=============================================
[2019-04-04 00:38:16,113] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.3113071e-18 3.1337360e-10 1.4007164e-09 1.0000000e+00 7.3157319e-10
 3.4492686e-09 1.3562491e-13], sum to 1.0000
[2019-04-04 00:38:16,114] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1288
[2019-04-04 00:38:16,143] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.433333333333334, 87.0, 0.0, 0.0, 19.0, 18.1405915909395, -1.197524454104037, 0.0, 1.0, 196217.9094192961], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 69600.0000, 
sim time next is 70200.0000, 
raw observation next is [3.25, 87.5, 0.0, 0.0, 19.0, 18.15725591059596, -1.167414118275946, 0.0, 1.0, 197528.8984568237], 
processed observation next is [0.0, 0.8260869565217391, 0.5526315789473685, 0.875, 0.0, 0.0, 0.08333333333333333, 0.013104659216330047, 0.11086196057468463, 0.0, 1.0, 0.940613802175351], 
reward next is 0.0594, 
noisyNet noise sample is [array([-0.72597], dtype=float32), 0.43464112]. 
=============================================
[2019-04-04 00:38:16,993] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:38:16,997] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:38:17,026] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run16
[2019-04-04 00:38:17,450] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:38:17,450] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:38:17,464] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run16
[2019-04-04 00:38:17,901] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:38:17,901] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:38:17,905] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run16
[2019-04-04 00:38:18,285] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.9181781e-29 6.2687767e-18 1.0235732e-16 1.0000000e+00 2.1011212e-20
 5.5017050e-17 3.5704479e-20], sum to 1.0000
[2019-04-04 00:38:18,285] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8837
[2019-04-04 00:38:18,297] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.2, 75.0, 0.0, 0.0, 24.0, 23.65164670685832, -0.02409031368940033, 0.0, 1.0, 28393.41345797834], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4323600.0000, 
sim time next is 4324200.0000, 
raw observation next is [4.25, 74.5, 0.0, 0.0, 24.0, 23.68152132954114, -0.01926188524704441, 0.0, 1.0, 18722.78651416004], 
processed observation next is [1.0, 0.043478260869565216, 0.5803324099722993, 0.745, 0.0, 0.0, 0.5, 0.4734601107950951, 0.4935793715843186, 0.0, 1.0, 0.08915612625790496], 
reward next is 0.9108, 
noisyNet noise sample is [array([-0.7127853], dtype=float32), -2.4248466]. 
=============================================
[2019-04-04 00:38:18,529] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.1010810e-25 3.3309007e-16 2.8197449e-14 1.0000000e+00 1.5068102e-17
 5.9631721e-15 2.9557996e-18], sum to 1.0000
[2019-04-04 00:38:18,529] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5849
[2019-04-04 00:38:18,576] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.166666666666667, 46.66666666666667, 30.99999999999999, 186.6666666666666, 23.0, 23.04101704655991, -0.2663637866396582, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 4953000.0000, 
sim time next is 4953600.0000, 
raw observation next is [-2.0, 46.0, 46.5, 280.0, 23.0, 23.00844284484858, -0.2616767067849727, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.40720221606648205, 0.46, 0.155, 0.30939226519337015, 0.4166666666666667, 0.4173702370707151, 0.4127744310716757, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2082894], dtype=float32), -0.34046832]. 
=============================================
[2019-04-04 00:38:20,381] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:38:20,381] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:38:20,431] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run16
[2019-04-04 00:38:20,616] A3C_AGENT_WORKER-Thread-2 INFO:Local step 128000, global step 2030998: loss 0.9058
[2019-04-04 00:38:20,617] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 128000, global step 2030998: learning rate 0.0005
[2019-04-04 00:38:22,260] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:38:22,260] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:38:22,265] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run16
[2019-04-04 00:38:24,451] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.11169508e-26 1.38462706e-17 1.51345812e-17 1.00000000e+00
 7.86091822e-18 1.66113180e-16 3.44972314e-20], sum to 1.0000
[2019-04-04 00:38:24,451] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8559
[2019-04-04 00:38:24,461] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.2, 96.0, 0.0, 0.0, 19.0, 18.84420692969017, -1.073207186061248, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3600.0000, 
sim time next is 4200.0000, 
raw observation next is [7.2, 96.0, 0.0, 0.0, 19.0, 18.89655518292279, -1.068174570018566, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.043478260869565216, 0.662049861495845, 0.96, 0.0, 0.0, 0.08333333333333333, 0.07471293191023243, 0.14394180999381131, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.06899118], dtype=float32), 1.6904064]. 
=============================================
[2019-04-04 00:38:25,151] A3C_AGENT_WORKER-Thread-17 INFO:Local step 127500, global step 2032506: loss 0.8212
[2019-04-04 00:38:25,155] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 127500, global step 2032508: learning rate 0.0005
[2019-04-04 00:38:25,568] A3C_AGENT_WORKER-Thread-11 INFO:Local step 127500, global step 2032721: loss 0.7439
[2019-04-04 00:38:25,570] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 127500, global step 2032721: learning rate 0.0005
[2019-04-04 00:38:25,799] A3C_AGENT_WORKER-Thread-20 INFO:Local step 127500, global step 2032823: loss 0.7980
[2019-04-04 00:38:25,801] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 127500, global step 2032823: learning rate 0.0005
[2019-04-04 00:38:27,106] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:38:27,109] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:38:27,121] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run16
[2019-04-04 00:38:28,629] A3C_AGENT_WORKER-Thread-18 INFO:Local step 127500, global step 2033957: loss 0.9252
[2019-04-04 00:38:28,630] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 127500, global step 2033957: learning rate 0.0005
[2019-04-04 00:38:28,730] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.2823145e-19 5.0720739e-13 3.4271065e-11 1.0000000e+00 4.6877801e-13
 1.0233309e-09 4.9371369e-13], sum to 1.0000
[2019-04-04 00:38:28,733] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9849
[2019-04-04 00:38:28,751] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-10.7, 50.0, 0.0, 0.0, 21.0, 20.1577033422085, -0.9220429036652599, 0.0, 1.0, 48325.50245172132], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 442200.0000, 
sim time next is 442800.0000, 
raw observation next is [-10.6, 49.0, 0.0, 0.0, 21.0, 20.17738689596538, -0.9257234801080574, 0.0, 1.0, 48321.53228042791], 
processed observation next is [1.0, 0.13043478260869565, 0.1689750692520776, 0.49, 0.0, 0.0, 0.25, 0.18144890799711502, 0.19142550663064753, 0.0, 1.0, 0.23010253466870434], 
reward next is 0.7699, 
noisyNet noise sample is [array([0.54270625], dtype=float32), 0.79566455]. 
=============================================
[2019-04-04 00:38:30,116] A3C_AGENT_WORKER-Thread-3 INFO:Local step 127500, global step 2034641: loss 0.9599
[2019-04-04 00:38:30,117] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 127500, global step 2034641: learning rate 0.0005
[2019-04-04 00:38:30,501] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:38:30,501] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:38:30,504] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run16
[2019-04-04 00:38:31,096] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.3440436e-22 1.8140762e-13 1.7905545e-11 1.0000000e+00 1.7401292e-14
 4.0454741e-14 9.2403417e-16], sum to 1.0000
[2019-04-04 00:38:31,097] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8818
[2019-04-04 00:38:31,122] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 24.0, 23.1556788614012, -0.04825108251251038, 0.0, 1.0, 51395.07937190213], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4564200.0000, 
sim time next is 4564800.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 24.0, 23.10330485480167, -0.04623810093270467, 1.0, 1.0, 62330.36914596392], 
processed observation next is [1.0, 0.8695652173913043, 0.518005540166205, 0.52, 0.0, 0.0, 0.5, 0.4252754045668059, 0.48458729968909847, 1.0, 1.0, 0.29681128164744724], 
reward next is 0.7032, 
noisyNet noise sample is [array([-1.4412404], dtype=float32), 0.26706186]. 
=============================================
[2019-04-04 00:38:34,817] A3C_AGENT_WORKER-Thread-16 INFO:Local step 127500, global step 2036298: loss 0.8841
[2019-04-04 00:38:34,820] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 127500, global step 2036298: learning rate 0.0005
[2019-04-04 00:38:36,310] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.0867794e-31 1.9609132e-19 2.9427968e-16 1.0000000e+00 1.7856734e-23
 3.7544368e-20 1.0092019e-21], sum to 1.0000
[2019-04-04 00:38:36,311] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3681
[2019-04-04 00:38:36,324] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.1666666666666666, 42.5, 0.0, 0.0, 26.0, 25.32313375536267, 0.45500716672102, 0.0, 1.0, 97280.44618172916], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4143000.0000, 
sim time next is 4143600.0000, 
raw observation next is [0.0, 43.0, 0.0, 0.0, 26.0, 25.32393794256821, 0.4615206811666361, 0.0, 1.0, 60228.6579257585], 
processed observation next is [1.0, 1.0, 0.46260387811634357, 0.43, 0.0, 0.0, 0.6666666666666666, 0.6103281618806843, 0.6538402270555453, 0.0, 1.0, 0.2868031329798024], 
reward next is 0.7132, 
noisyNet noise sample is [array([1.7680446], dtype=float32), -0.67472774]. 
=============================================
[2019-04-04 00:38:37,428] A3C_AGENT_WORKER-Thread-19 INFO:Local step 128000, global step 2037410: loss 6.9484
[2019-04-04 00:38:37,428] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 128000, global step 2037410: learning rate 0.0005
[2019-04-04 00:38:38,090] A3C_AGENT_WORKER-Thread-10 INFO:Local step 127500, global step 2037734: loss 1.1104
[2019-04-04 00:38:38,092] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 127500, global step 2037735: learning rate 0.0005
[2019-04-04 00:38:38,196] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.5117395e-21 5.2611199e-10 1.8371549e-06 9.9999821e-01 1.0716467e-12
 8.3677905e-11 5.9091797e-14], sum to 1.0000
[2019-04-04 00:38:38,198] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2181
[2019-04-04 00:38:38,207] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 92.0, 140.5, 3.0, 25.0, 25.19465759867634, 0.3982911442112658, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 4456800.0000, 
sim time next is 4457400.0000, 
raw observation next is [0.0, 90.83333333333334, 122.0, 2.0, 25.0, 25.30128088709104, 0.404464988932064, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.46260387811634357, 0.9083333333333334, 0.4066666666666667, 0.0022099447513812156, 0.5833333333333334, 0.6084400739242533, 0.6348216629773547, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.8648401], dtype=float32), 1.3100479]. 
=============================================
[2019-04-04 00:38:39,332] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.4105073e-28 2.7463716e-15 3.3557867e-14 1.0000000e+00 1.2675443e-19
 1.2616554e-17 5.1636753e-20], sum to 1.0000
[2019-04-04 00:38:39,334] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1062
[2019-04-04 00:38:39,355] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [8.0, 26.0, 123.5, 855.0, 23.5, 25.28262653571419, 0.2103934766192233, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.5], 
sim time this is 5054400.0000, 
sim time next is 5055000.0000, 
raw observation next is [8.166666666666668, 25.83333333333334, 123.6666666666667, 858.3333333333334, 23.5, 25.35971045170377, 0.3697994012142948, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.6888273314866113, 0.2583333333333334, 0.4122222222222223, 0.9484346224677717, 0.4583333333333333, 0.6133092043086474, 0.6232664670714315, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6069589], dtype=float32), 0.4949834]. 
=============================================
[2019-04-04 00:38:39,359] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[85.68355 ]
 [85.50165 ]
 [85.188095]
 [84.80897 ]
 [84.48564 ]], R is [[86.08177948]
 [86.22096252]
 [86.35875702]
 [86.49517059]
 [86.63021851]].
[2019-04-04 00:38:41,850] A3C_AGENT_WORKER-Thread-2 INFO:Local step 128500, global step 2039380: loss 1.1751
[2019-04-04 00:38:41,851] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 128500, global step 2039380: learning rate 0.0005
[2019-04-04 00:38:42,207] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:38:42,207] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:38:42,212] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run16
[2019-04-04 00:38:44,438] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:38:44,439] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:38:44,451] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run16
[2019-04-04 00:38:46,379] A3C_AGENT_WORKER-Thread-17 INFO:Local step 128000, global step 2040907: loss 13.2423
[2019-04-04 00:38:46,380] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 128000, global step 2040907: learning rate 0.0005
[2019-04-04 00:38:47,212] A3C_AGENT_WORKER-Thread-11 INFO:Local step 128000, global step 2041174: loss 15.8916
[2019-04-04 00:38:47,212] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 128000, global step 2041174: learning rate 0.0005
[2019-04-04 00:38:47,894] A3C_AGENT_WORKER-Thread-20 INFO:Local step 128000, global step 2041402: loss 14.2263
[2019-04-04 00:38:47,895] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 128000, global step 2041402: learning rate 0.0005
[2019-04-04 00:38:50,347] A3C_AGENT_WORKER-Thread-12 INFO:Local step 127500, global step 2042291: loss 0.9820
[2019-04-04 00:38:50,349] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 127500, global step 2042291: learning rate 0.0005
[2019-04-04 00:38:50,637] A3C_AGENT_WORKER-Thread-18 INFO:Local step 128000, global step 2042398: loss 25.2948
[2019-04-04 00:38:50,638] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 128000, global step 2042398: learning rate 0.0005
[2019-04-04 00:38:50,696] A3C_AGENT_WORKER-Thread-3 INFO:Local step 128000, global step 2042415: loss 35.9787
[2019-04-04 00:38:50,698] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 128000, global step 2042415: learning rate 0.0005
[2019-04-04 00:38:51,142] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.5550394e-34 1.1233162e-19 8.2764739e-18 1.0000000e+00 3.9500979e-24
 1.8755542e-19 1.5702526e-23], sum to 1.0000
[2019-04-04 00:38:51,142] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0314
[2019-04-04 00:38:51,148] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.216666666666666, 65.83333333333333, 0.0, 0.0, 26.0, 25.81511118496234, 0.5775548109319683, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4413000.0000, 
sim time next is 4413600.0000, 
raw observation next is [6.1, 66.0, 0.0, 0.0, 26.0, 25.79312712407749, 0.5621590606547132, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.6315789473684211, 0.66, 0.0, 0.0, 0.6666666666666666, 0.6494272603397908, 0.6873863535515711, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.17019868], dtype=float32), -1.994836]. 
=============================================
[2019-04-04 00:38:52,328] A3C_AGENT_WORKER-Thread-6 INFO:Local step 127500, global step 2043133: loss 0.3243
[2019-04-04 00:38:52,357] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 127500, global step 2043147: learning rate 0.0005
[2019-04-04 00:38:52,414] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.5834536e-27 7.7869162e-14 9.4325409e-11 1.0000000e+00 3.6691241e-16
 2.4529285e-13 6.4019834e-18], sum to 1.0000
[2019-04-04 00:38:52,414] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7945
[2019-04-04 00:38:52,427] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.7, 44.5, 272.0, 388.0, 24.0, 23.23878039881379, -0.06849384357172704, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4887000.0000, 
sim time next is 4887600.0000, 
raw observation next is [1.8, 44.33333333333334, 266.0, 385.6666666666667, 24.0, 23.23097420123042, -0.06648004679397317, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.5124653739612189, 0.4433333333333334, 0.8866666666666667, 0.4261510128913444, 0.5, 0.43591451676920173, 0.4778399844020089, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2835023], dtype=float32), -0.8085684]. 
=============================================
[2019-04-04 00:38:53,037] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:38:53,037] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:38:53,040] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run16
[2019-04-04 00:38:53,257] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.42150202e-15 8.07772293e-10 5.66540592e-08 1.00000000e+00
 5.03398323e-10 1.08957676e-10 8.37018843e-10], sum to 1.0000
[2019-04-04 00:38:53,260] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7449
[2019-04-04 00:38:53,302] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-9.5, 40.0, 0.0, 0.0, 21.0, 21.24206557154967, -0.7333138497324091, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 413400.0000, 
sim time next is 414000.0000, 
raw observation next is [-9.5, 40.0, 0.0, 0.0, 21.0, 21.02609952499067, -0.7751071094103833, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.1994459833795014, 0.4, 0.0, 0.0, 0.25, 0.25217496041588916, 0.24163096352987223, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.68020517], dtype=float32), -1.111749]. 
=============================================
[2019-04-04 00:38:53,305] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[55.824898]
 [55.6538  ]
 [55.660984]
 [55.811737]
 [56.307575]], R is [[55.27800751]
 [54.72522736]
 [54.1779747 ]
 [53.63619614]
 [53.09983444]].
[2019-04-04 00:38:56,201] A3C_AGENT_WORKER-Thread-16 INFO:Local step 128000, global step 2044500: loss 26.1273
[2019-04-04 00:38:56,201] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 128000, global step 2044500: learning rate 0.0005
[2019-04-04 00:38:59,963] A3C_AGENT_WORKER-Thread-19 INFO:Local step 128500, global step 2045853: loss 0.5023
[2019-04-04 00:38:59,964] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 128500, global step 2045853: learning rate 0.0005
[2019-04-04 00:39:00,730] A3C_AGENT_WORKER-Thread-10 INFO:Local step 128000, global step 2046196: loss 33.5737
[2019-04-04 00:39:00,731] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 128000, global step 2046196: learning rate 0.0005
[2019-04-04 00:39:00,884] A3C_AGENT_WORKER-Thread-14 INFO:Local step 127500, global step 2046280: loss 0.4369
[2019-04-04 00:39:00,891] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 127500, global step 2046280: learning rate 0.0005
[2019-04-04 00:39:01,515] A3C_AGENT_WORKER-Thread-2 INFO:Local step 129000, global step 2046605: loss 9.6066
[2019-04-04 00:39:01,517] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 129000, global step 2046605: learning rate 0.0005
[2019-04-04 00:39:02,713] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:39:02,713] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:39:02,717] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run16
[2019-04-04 00:39:03,766] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.5343237e-29 3.7050015e-17 7.2274530e-14 1.0000000e+00 2.7170738e-19
 1.3889689e-19 9.9112645e-20], sum to 1.0000
[2019-04-04 00:39:03,772] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1404
[2019-04-04 00:39:03,801] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.7666666666666667, 47.83333333333334, 282.3333333333333, 335.3333333333333, 25.0, 24.1109356221755, 0.131583034054707, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 4881000.0000, 
sim time next is 4881600.0000, 
raw observation next is [1.0, 47.0, 282.0, 349.0, 25.0, 24.10434795060487, 0.1302454350813241, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.4903047091412743, 0.47, 0.94, 0.3856353591160221, 0.5833333333333334, 0.5086956625504057, 0.543415145027108, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.29018572], dtype=float32), 0.74637085]. 
=============================================
[2019-04-04 00:39:04,697] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:39:04,697] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:39:04,717] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run16
[2019-04-04 00:39:07,220] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [9.0836173e-16 6.8959700e-08 7.3893375e-07 9.9994528e-01 4.9601294e-07
 5.3388623e-05 1.3191994e-09], sum to 1.0000
[2019-04-04 00:39:07,225] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1018
[2019-04-04 00:39:07,247] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.0, 69.0, 0.0, 0.0, 20.5, 19.87771764639699, -0.9522653006596992, 0.0, 1.0, 90299.68963344718], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 264600.0000, 
sim time next is 265200.0000, 
raw observation next is [-7.1, 69.66666666666666, 0.0, 0.0, 20.5, 19.90796041850081, -0.9422222313868788, 0.0, 1.0, 50303.88647700471], 
processed observation next is [1.0, 0.043478260869565216, 0.2659279778393352, 0.6966666666666665, 0.0, 0.0, 0.20833333333333334, 0.15899670154173418, 0.1859259228710404, 0.0, 1.0, 0.23954231655716526], 
reward next is 0.7605, 
noisyNet noise sample is [array([1.0280188], dtype=float32), 0.031149184]. 
=============================================
[2019-04-04 00:39:07,712] A3C_AGENT_WORKER-Thread-17 INFO:Local step 128500, global step 2048819: loss 1.0125
[2019-04-04 00:39:07,712] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 128500, global step 2048819: learning rate 0.0005
[2019-04-04 00:39:09,149] A3C_AGENT_WORKER-Thread-11 INFO:Local step 128500, global step 2049237: loss 0.4964
[2019-04-04 00:39:09,149] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 128500, global step 2049237: learning rate 0.0005
[2019-04-04 00:39:09,974] A3C_AGENT_WORKER-Thread-20 INFO:Local step 128500, global step 2049551: loss 0.2640
[2019-04-04 00:39:09,975] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 128500, global step 2049551: learning rate 0.0005
[2019-04-04 00:39:10,908] A3C_AGENT_WORKER-Thread-5 INFO:Local step 127500, global step 2049926: loss 0.2401
[2019-04-04 00:39:10,909] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 127500, global step 2049926: learning rate 0.0005
[2019-04-04 00:39:11,854] A3C_AGENT_WORKER-Thread-18 INFO:Local step 128500, global step 2050356: loss 0.4124
[2019-04-04 00:39:11,855] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 128500, global step 2050356: learning rate 0.0005
[2019-04-04 00:39:12,038] A3C_AGENT_WORKER-Thread-3 INFO:Local step 128500, global step 2050444: loss 0.6503
[2019-04-04 00:39:12,040] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 128500, global step 2050445: learning rate 0.0005
[2019-04-04 00:39:12,590] A3C_AGENT_WORKER-Thread-12 INFO:Local step 128000, global step 2050640: loss 32.5043
[2019-04-04 00:39:12,591] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 128000, global step 2050640: learning rate 0.0005
[2019-04-04 00:39:12,700] A3C_AGENT_WORKER-Thread-15 INFO:Local step 127500, global step 2050683: loss 0.3148
[2019-04-04 00:39:12,701] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 127500, global step 2050683: learning rate 0.0005
[2019-04-04 00:39:13,313] A3C_AGENT_WORKER-Thread-6 INFO:Local step 128000, global step 2050930: loss 31.3147
[2019-04-04 00:39:13,314] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 128000, global step 2050930: learning rate 0.0005
[2019-04-04 00:39:13,462] A3C_AGENT_WORKER-Thread-2 INFO:Local step 129500, global step 2050992: loss 1.3453
[2019-04-04 00:39:13,469] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 129500, global step 2050992: learning rate 0.0005
[2019-04-04 00:39:17,096] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:39:17,096] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:39:17,126] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run16
[2019-04-04 00:39:17,317] A3C_AGENT_WORKER-Thread-16 INFO:Local step 128500, global step 2052628: loss 0.6863
[2019-04-04 00:39:17,330] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 128500, global step 2052630: learning rate 0.0005
[2019-04-04 00:39:21,753] A3C_AGENT_WORKER-Thread-19 INFO:Local step 129000, global step 2054102: loss 16.8789
[2019-04-04 00:39:21,755] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 129000, global step 2054103: learning rate 0.0005
[2019-04-04 00:39:23,273] A3C_AGENT_WORKER-Thread-10 INFO:Local step 128500, global step 2054642: loss 0.0467
[2019-04-04 00:39:23,282] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 128500, global step 2054643: learning rate 0.0005
[2019-04-04 00:39:23,785] A3C_AGENT_WORKER-Thread-14 INFO:Local step 128000, global step 2054795: loss 49.0677
[2019-04-04 00:39:23,785] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 128000, global step 2054796: learning rate 0.0005
[2019-04-04 00:39:25,112] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.74160484e-19 1.55416205e-06 3.61310731e-10 9.99897599e-01
 5.02525168e-08 1.00737365e-04 5.78154712e-12], sum to 1.0000
[2019-04-04 00:39:25,112] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9335
[2019-04-04 00:39:25,129] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.7, 93.0, 17.5, 0.0, 19.0, 19.01404290899509, -1.058696562357085, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 30000.0000, 
sim time next is 30600.0000, 
raw observation next is [7.7, 93.0, 21.0, 0.0, 19.0, 19.00322306321381, -1.064548642245508, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.6759002770083103, 0.93, 0.07, 0.0, 0.08333333333333333, 0.08360192193448412, 0.1451504525848307, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.14488824], dtype=float32), 0.3398264]. 
=============================================
[2019-04-04 00:39:25,360] A3C_AGENT_WORKER-Thread-13 INFO:Local step 127500, global step 2055391: loss 0.0275
[2019-04-04 00:39:25,361] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 127500, global step 2055391: learning rate 0.0005
[2019-04-04 00:39:28,865] A3C_AGENT_WORKER-Thread-17 INFO:Local step 129000, global step 2056807: loss 21.2805
[2019-04-04 00:39:28,867] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 129000, global step 2056807: learning rate 0.0005
[2019-04-04 00:39:29,804] A3C_AGENT_WORKER-Thread-2 INFO:Local step 130000, global step 2057205: loss 3.2998
[2019-04-04 00:39:29,804] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 130000, global step 2057205: learning rate 0.0005
[2019-04-04 00:39:30,172] A3C_AGENT_WORKER-Thread-11 INFO:Local step 129000, global step 2057360: loss 21.6876
[2019-04-04 00:39:30,184] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 129000, global step 2057366: learning rate 0.0005
[2019-04-04 00:39:31,573] A3C_AGENT_WORKER-Thread-20 INFO:Local step 129000, global step 2057922: loss 12.6027
[2019-04-04 00:39:31,574] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 129000, global step 2057922: learning rate 0.0005
[2019-04-04 00:39:32,792] A3C_AGENT_WORKER-Thread-18 INFO:Local step 129000, global step 2058474: loss 21.1788
[2019-04-04 00:39:32,794] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 129000, global step 2058474: learning rate 0.0005
[2019-04-04 00:39:33,227] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:39:33,227] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:39:33,244] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run16
[2019-04-04 00:39:34,014] A3C_AGENT_WORKER-Thread-19 INFO:Local step 129500, global step 2058972: loss 0.3792
[2019-04-04 00:39:34,015] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 129500, global step 2058972: learning rate 0.0005
[2019-04-04 00:39:34,166] A3C_AGENT_WORKER-Thread-6 INFO:Local step 128500, global step 2059024: loss 0.3541
[2019-04-04 00:39:34,167] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 128500, global step 2059024: learning rate 0.0005
[2019-04-04 00:39:34,293] A3C_AGENT_WORKER-Thread-12 INFO:Local step 128500, global step 2059076: loss 0.4230
[2019-04-04 00:39:34,301] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 128500, global step 2059078: learning rate 0.0005
[2019-04-04 00:39:34,567] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.5611882e-17 2.6492876e-06 1.7392024e-08 9.9992001e-01 7.7199024e-05
 1.7480262e-07 6.8423434e-11], sum to 1.0000
[2019-04-04 00:39:34,569] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5575
[2019-04-04 00:39:34,573] A3C_AGENT_WORKER-Thread-3 INFO:Local step 129000, global step 2059195: loss 11.2486
[2019-04-04 00:39:34,575] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 129000, global step 2059196: learning rate 0.0005
[2019-04-04 00:39:34,589] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.366666666666667, 27.0, 127.3333333333333, 0.0, 21.5, 21.28042609384664, -0.7815630630206677, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.5], 
sim time this is 477600.0000, 
sim time next is 478200.0000, 
raw observation next is [-1.283333333333333, 27.5, 125.6666666666667, 0.0, 21.5, 21.16485281549354, -0.7792597741955949, 1.0, 1.0, 149588.7719487119], 
processed observation next is [1.0, 0.5217391304347826, 0.4270544783010157, 0.275, 0.418888888888889, 0.0, 0.2916666666666667, 0.2637377346244616, 0.2402467419348017, 1.0, 1.0, 0.7123274854700566], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.6955962], dtype=float32), -1.7271382]. 
=============================================
[2019-04-04 00:39:35,125] A3C_AGENT_WORKER-Thread-15 INFO:Local step 128000, global step 2059430: loss 10.7059
[2019-04-04 00:39:35,130] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 128000, global step 2059430: learning rate 0.0005
[2019-04-04 00:39:35,767] A3C_AGENT_WORKER-Thread-5 INFO:Local step 128000, global step 2059711: loss 14.6199
[2019-04-04 00:39:35,768] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 128000, global step 2059711: learning rate 0.0005
[2019-04-04 00:39:38,212] A3C_AGENT_WORKER-Thread-16 INFO:Local step 129000, global step 2060745: loss 5.3486
[2019-04-04 00:39:38,213] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 129000, global step 2060745: learning rate 0.0005
[2019-04-04 00:39:40,994] A3C_AGENT_WORKER-Thread-4 INFO:Local step 127500, global step 2062136: loss 0.7217
[2019-04-04 00:39:40,996] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 127500, global step 2062136: learning rate 0.0005
[2019-04-04 00:39:41,869] A3C_AGENT_WORKER-Thread-17 INFO:Local step 129500, global step 2062607: loss 0.2504
[2019-04-04 00:39:41,871] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 129500, global step 2062607: learning rate 0.0005
[2019-04-04 00:39:43,025] A3C_AGENT_WORKER-Thread-11 INFO:Local step 129500, global step 2063230: loss 0.0226
[2019-04-04 00:39:43,027] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 129500, global step 2063231: learning rate 0.0005
[2019-04-04 00:39:44,124] A3C_AGENT_WORKER-Thread-10 INFO:Local step 129000, global step 2063827: loss 6.4324
[2019-04-04 00:39:44,125] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 129000, global step 2063828: learning rate 0.0005
[2019-04-04 00:39:44,267] A3C_AGENT_WORKER-Thread-20 INFO:Local step 129500, global step 2063902: loss 0.6065
[2019-04-04 00:39:44,267] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 129500, global step 2063902: learning rate 0.0005
[2019-04-04 00:39:44,567] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.0640432e-28 8.0447335e-16 7.0639965e-12 1.0000000e+00 3.1727348e-16
 2.4910529e-13 4.7378295e-17], sum to 1.0000
[2019-04-04 00:39:44,568] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4696
[2019-04-04 00:39:44,583] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.0, 96.66666666666666, 0.0, 0.0, 23.0, 22.57469038624352, -0.2493279449185397, 0.0, 1.0, 18738.88082876785], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 942600.0000, 
sim time next is 943200.0000, 
raw observation next is [5.0, 96.0, 0.0, 0.0, 23.0, 22.54035590216311, -0.2529532657987675, 0.0, 1.0, 42311.57877565894], 
processed observation next is [1.0, 0.9565217391304348, 0.6011080332409973, 0.96, 0.0, 0.0, 0.4166666666666667, 0.37836299184692584, 0.4156822447337442, 0.0, 1.0, 0.20148370845551877], 
reward next is 0.7985, 
noisyNet noise sample is [array([0.4713544], dtype=float32), 0.6040404]. 
=============================================
[2019-04-04 00:39:45,149] A3C_AGENT_WORKER-Thread-14 INFO:Local step 128500, global step 2064371: loss 1.2952
[2019-04-04 00:39:45,149] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 128500, global step 2064371: learning rate 0.0005
[2019-04-04 00:39:45,366] A3C_AGENT_WORKER-Thread-18 INFO:Local step 129500, global step 2064484: loss 0.0333
[2019-04-04 00:39:45,367] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 129500, global step 2064485: learning rate 0.0005
[2019-04-04 00:39:46,427] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.2940114e-17 1.7610394e-07 3.5379348e-06 9.9997830e-01 1.3413710e-05
 4.6392984e-06 1.0747835e-09], sum to 1.0000
[2019-04-04 00:39:46,429] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5386
[2019-04-04 00:39:46,470] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.4, 79.00000000000001, 93.66666666666666, 0.0, 21.5, 21.59708778311025, -0.5829285091725288, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.5], 
sim time this is 825000.0000, 
sim time next is 825600.0000, 
raw observation next is [-4.300000000000001, 79.0, 92.33333333333334, 0.0, 21.5, 21.76844737324696, -0.5600519771816778, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.34349030470914127, 0.79, 0.3077777777777778, 0.0, 0.2916666666666667, 0.3140372811039134, 0.3133160076061074, 1.0, 1.0, 0.0], 
reward next is 0.3995, 
noisyNet noise sample is [array([-1.627287], dtype=float32), -0.83736444]. 
=============================================
[2019-04-04 00:39:46,523] A3C_AGENT_WORKER-Thread-2 INFO:Local step 130500, global step 2065049: loss 0.0069
[2019-04-04 00:39:46,524] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 130500, global step 2065049: learning rate 0.0005
[2019-04-04 00:39:46,971] A3C_AGENT_WORKER-Thread-3 INFO:Local step 129500, global step 2065267: loss 0.0440
[2019-04-04 00:39:46,971] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 129500, global step 2065267: learning rate 0.0005
[2019-04-04 00:39:48,401] A3C_AGENT_WORKER-Thread-13 INFO:Local step 128000, global step 2065851: loss 0.3272
[2019-04-04 00:39:48,402] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 128000, global step 2065851: learning rate 0.0005
[2019-04-04 00:39:48,558] A3C_AGENT_WORKER-Thread-19 INFO:Local step 130000, global step 2065923: loss 0.4543
[2019-04-04 00:39:48,560] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 130000, global step 2065923: learning rate 0.0005
[2019-04-04 00:39:50,308] A3C_AGENT_WORKER-Thread-16 INFO:Local step 129500, global step 2066838: loss 0.0800
[2019-04-04 00:39:50,309] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 129500, global step 2066838: learning rate 0.0005
[2019-04-04 00:39:52,999] A3C_AGENT_WORKER-Thread-12 INFO:Local step 129000, global step 2068175: loss 3.0053
[2019-04-04 00:39:53,000] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 129000, global step 2068175: learning rate 0.0005
[2019-04-04 00:39:53,462] A3C_AGENT_WORKER-Thread-6 INFO:Local step 129000, global step 2068370: loss 2.7438
[2019-04-04 00:39:53,463] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 129000, global step 2068370: learning rate 0.0005
[2019-04-04 00:39:54,298] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.4142921e-17 8.5543963e-13 9.7487298e-09 1.0000000e+00 7.4808435e-09
 2.7558485e-11 8.5331714e-11], sum to 1.0000
[2019-04-04 00:39:54,298] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3823
[2019-04-04 00:39:54,354] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-9.5, 40.0, 0.0, 0.0, 23.0, 22.69413359630181, -0.3586284971529397, 1.0, 1.0, 142936.9993158506], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 413400.0000, 
sim time next is 414000.0000, 
raw observation next is [-9.5, 40.0, 0.0, 0.0, 23.0, 22.4780641876246, -0.3631545350355863, 1.0, 1.0, 96228.17244684711], 
processed observation next is [1.0, 0.8260869565217391, 0.1994459833795014, 0.4, 0.0, 0.0, 0.4166666666666667, 0.3731720156353833, 0.37894848832147127, 1.0, 1.0, 0.45822939260403384], 
reward next is 0.5418, 
noisyNet noise sample is [array([0.8733336], dtype=float32), -1.2526039]. 
=============================================
[2019-04-04 00:39:54,364] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[58.30578 ]
 [58.334457]
 [57.96518 ]
 [57.8385  ]
 [58.299683]], R is [[58.54241562]
 [58.27634048]
 [58.69357681]
 [59.10663986]
 [59.51557541]].
[2019-04-04 00:39:55,950] A3C_AGENT_WORKER-Thread-10 INFO:Local step 129500, global step 2069624: loss 0.0125
[2019-04-04 00:39:55,951] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 129500, global step 2069624: learning rate 0.0005
[2019-04-04 00:39:56,106] A3C_AGENT_WORKER-Thread-15 INFO:Local step 128500, global step 2069703: loss 1.0661
[2019-04-04 00:39:56,110] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 128500, global step 2069705: learning rate 0.0005
[2019-04-04 00:39:56,588] A3C_AGENT_WORKER-Thread-17 INFO:Local step 130000, global step 2069923: loss 0.3262
[2019-04-04 00:39:56,589] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 130000, global step 2069923: learning rate 0.0005
[2019-04-04 00:39:57,152] A3C_AGENT_WORKER-Thread-5 INFO:Local step 128500, global step 2070205: loss 0.3125
[2019-04-04 00:39:57,153] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 128500, global step 2070205: learning rate 0.0005
[2019-04-04 00:39:58,342] A3C_AGENT_WORKER-Thread-20 INFO:Local step 130000, global step 2070774: loss 0.1978
[2019-04-04 00:39:58,345] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 130000, global step 2070774: learning rate 0.0005
[2019-04-04 00:39:58,897] A3C_AGENT_WORKER-Thread-11 INFO:Local step 130000, global step 2071022: loss 0.2211
[2019-04-04 00:39:58,899] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 130000, global step 2071022: learning rate 0.0005
[2019-04-04 00:39:59,827] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.4874103e-25 2.9928919e-13 1.0430914e-11 1.0000000e+00 1.0192526e-15
 1.4789159e-12 4.7880147e-16], sum to 1.0000
[2019-04-04 00:39:59,831] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2468
[2019-04-04 00:39:59,851] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.35, 92.0, 0.0, 0.0, 24.0, 23.47821366477229, 0.01541598432887055, 0.0, 1.0, 112270.0605274948], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1463400.0000, 
sim time next is 1464000.0000, 
raw observation next is [1.433333333333333, 92.0, 0.0, 0.0, 24.0, 23.40188466745615, 0.02477329181559168, 0.0, 1.0, 113113.7275078566], 
processed observation next is [1.0, 0.9565217391304348, 0.502308402585411, 0.92, 0.0, 0.0, 0.5, 0.45015705562134595, 0.5082577639385305, 0.0, 1.0, 0.53863679765646], 
reward next is 0.4614, 
noisyNet noise sample is [array([-0.40301287], dtype=float32), -0.9159459]. 
=============================================
[2019-04-04 00:39:59,878] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[85.11853 ]
 [86.1941  ]
 [87.074524]
 [87.76139 ]
 [88.59452 ]], R is [[84.16501617]
 [83.78874207]
 [83.68690491]
 [83.69788361]
 [83.77175903]].
[2019-04-04 00:40:01,681] A3C_AGENT_WORKER-Thread-18 INFO:Local step 130000, global step 2072353: loss 0.2426
[2019-04-04 00:40:01,683] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 130000, global step 2072355: learning rate 0.0005
[2019-04-04 00:40:01,837] A3C_AGENT_WORKER-Thread-4 INFO:Local step 128000, global step 2072429: loss 5.5176
[2019-04-04 00:40:01,838] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 128000, global step 2072429: learning rate 0.0005
[2019-04-04 00:40:03,752] A3C_AGENT_WORKER-Thread-3 INFO:Local step 130000, global step 2073504: loss 0.4325
[2019-04-04 00:40:03,757] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 130000, global step 2073506: learning rate 0.0005
[2019-04-04 00:40:03,836] A3C_AGENT_WORKER-Thread-16 INFO:Local step 130000, global step 2073547: loss 0.9734
[2019-04-04 00:40:03,837] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 130000, global step 2073548: learning rate 0.0005
[2019-04-04 00:40:04,369] A3C_AGENT_WORKER-Thread-12 INFO:Local step 129500, global step 2073856: loss 0.0889
[2019-04-04 00:40:04,370] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 129500, global step 2073856: learning rate 0.0005
[2019-04-04 00:40:04,388] A3C_AGENT_WORKER-Thread-19 INFO:Local step 130500, global step 2073869: loss 0.0116
[2019-04-04 00:40:04,392] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 130500, global step 2073869: learning rate 0.0005
[2019-04-04 00:40:04,711] A3C_AGENT_WORKER-Thread-14 INFO:Local step 129000, global step 2074055: loss 2.7025
[2019-04-04 00:40:04,712] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 129000, global step 2074056: learning rate 0.0005
[2019-04-04 00:40:05,436] A3C_AGENT_WORKER-Thread-6 INFO:Local step 129500, global step 2074403: loss 0.1226
[2019-04-04 00:40:05,437] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 129500, global step 2074403: learning rate 0.0005
[2019-04-04 00:40:07,629] A3C_AGENT_WORKER-Thread-2 INFO:Local step 131000, global step 2075467: loss 0.2913
[2019-04-04 00:40:07,629] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 131000, global step 2075467: learning rate 0.0005
[2019-04-04 00:40:09,726] A3C_AGENT_WORKER-Thread-13 INFO:Local step 128500, global step 2076431: loss 1.0848
[2019-04-04 00:40:09,729] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 128500, global step 2076431: learning rate 0.0005
[2019-04-04 00:40:10,839] A3C_AGENT_WORKER-Thread-10 INFO:Local step 130000, global step 2076910: loss 2.0175
[2019-04-04 00:40:10,841] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 130000, global step 2076910: learning rate 0.0005
[2019-04-04 00:40:12,430] A3C_AGENT_WORKER-Thread-17 INFO:Local step 130500, global step 2077561: loss 0.0951
[2019-04-04 00:40:12,433] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 130500, global step 2077561: learning rate 0.0005
[2019-04-04 00:40:14,411] A3C_AGENT_WORKER-Thread-20 INFO:Local step 130500, global step 2078411: loss 0.1183
[2019-04-04 00:40:14,412] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 130500, global step 2078411: learning rate 0.0005
[2019-04-04 00:40:15,774] A3C_AGENT_WORKER-Thread-11 INFO:Local step 130500, global step 2079033: loss 0.0074
[2019-04-04 00:40:15,776] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 130500, global step 2079033: learning rate 0.0005
[2019-04-04 00:40:16,524] A3C_AGENT_WORKER-Thread-15 INFO:Local step 129000, global step 2079326: loss 0.4291
[2019-04-04 00:40:16,524] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 129000, global step 2079326: learning rate 0.0005
[2019-04-04 00:40:16,877] A3C_AGENT_WORKER-Thread-14 INFO:Local step 129500, global step 2079471: loss 0.0536
[2019-04-04 00:40:16,882] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 129500, global step 2079474: learning rate 0.0005
[2019-04-04 00:40:16,925] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.4596299e-22 4.4451701e-10 9.5748440e-09 1.0000000e+00 7.0048651e-09
 3.1672182e-10 1.1180378e-13], sum to 1.0000
[2019-04-04 00:40:16,927] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5521
[2019-04-04 00:40:16,946] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.3, 96.0, 80.5, 354.0, 21.5, 22.04513948217412, -0.3964166482715999, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.5], 
sim time this is 1508400.0000, 
sim time next is 1509000.0000, 
raw observation next is [3.483333333333333, 95.5, 83.0, 472.0000000000001, 21.5, 22.1327722261031, -0.3779224654667186, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.559095106186519, 0.955, 0.27666666666666667, 0.521546961325967, 0.2916666666666667, 0.3443976855085917, 0.37402584484442714, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6214731], dtype=float32), 1.3691536]. 
=============================================
[2019-04-04 00:40:16,952] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[93.1745  ]
 [92.530495]
 [92.08242 ]
 [91.13225 ]
 [91.46592 ]], R is [[93.76922607]
 [93.83153534]
 [93.89321899]
 [93.95428467]
 [94.01473999]].
[2019-04-04 00:40:18,254] A3C_AGENT_WORKER-Thread-12 INFO:Local step 130000, global step 2080047: loss 3.7659
[2019-04-04 00:40:18,258] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 130000, global step 2080047: learning rate 0.0005
[2019-04-04 00:40:18,698] A3C_AGENT_WORKER-Thread-5 INFO:Local step 129000, global step 2080233: loss 0.5220
[2019-04-04 00:40:18,699] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 129000, global step 2080233: learning rate 0.0005
[2019-04-04 00:40:18,735] A3C_AGENT_WORKER-Thread-18 INFO:Local step 130500, global step 2080249: loss 0.0039
[2019-04-04 00:40:18,742] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 130500, global step 2080249: learning rate 0.0005
[2019-04-04 00:40:19,807] A3C_AGENT_WORKER-Thread-6 INFO:Local step 130000, global step 2080705: loss 2.2943
[2019-04-04 00:40:19,809] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 130000, global step 2080705: learning rate 0.0005
[2019-04-04 00:40:20,052] A3C_AGENT_WORKER-Thread-16 INFO:Local step 130500, global step 2080821: loss 0.1305
[2019-04-04 00:40:20,053] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 130500, global step 2080821: learning rate 0.0005
[2019-04-04 00:40:20,548] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.9960854e-19 5.0079082e-12 5.2812834e-07 9.9999952e-01 1.8228498e-10
 4.4080919e-11 1.3536892e-10], sum to 1.0000
[2019-04-04 00:40:20,552] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5888
[2019-04-04 00:40:20,576] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.9, 86.0, 0.0, 0.0, 21.5, 20.94411687016483, -0.6861944113836943, 0.0, 1.0, 52790.92003936043], 
current ob forecast is [], 
actual action is [21.5], 
sim time this is 609000.0000, 
sim time next is 609600.0000, 
raw observation next is [-3.9, 86.0, 0.0, 0.0, 21.5, 20.93414147357359, -0.6796411565345508, 0.0, 1.0, 51922.93203379629], 
processed observation next is [0.0, 0.043478260869565216, 0.3545706371191136, 0.86, 0.0, 0.0, 0.2916666666666667, 0.2445117894644658, 0.2734529478218164, 0.0, 1.0, 0.24725205730379188], 
reward next is 0.7527, 
noisyNet noise sample is [array([0.6946129], dtype=float32), 1.4040523]. 
=============================================
[2019-04-04 00:40:21,009] A3C_AGENT_WORKER-Thread-3 INFO:Local step 130500, global step 2081264: loss 0.0077
[2019-04-04 00:40:21,010] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 130500, global step 2081264: learning rate 0.0005
[2019-04-04 00:40:21,298] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.6619688e-23 1.7839980e-16 5.2555247e-13 1.0000000e+00 8.0618452e-16
 1.4379924e-13 3.8673356e-15], sum to 1.0000
[2019-04-04 00:40:21,311] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1059
[2019-04-04 00:40:21,335] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.6, 83.66666666666667, 0.0, 0.0, 23.0, 22.44054549216015, -0.2296085011381731, 0.0, 1.0, 43765.34727208391], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 1745400.0000, 
sim time next is 1746000.0000, 
raw observation next is [-0.6, 83.0, 0.0, 0.0, 23.0, 22.4389673381247, -0.2307210743171544, 0.0, 1.0, 43765.90373065844], 
processed observation next is [0.0, 0.21739130434782608, 0.44598337950138506, 0.83, 0.0, 0.0, 0.4166666666666667, 0.36991394484372514, 0.4230929752276152, 0.0, 1.0, 0.20840906538408782], 
reward next is 0.7916, 
noisyNet noise sample is [array([0.24911672], dtype=float32), 0.9662217]. 
=============================================
[2019-04-04 00:40:21,348] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[76.75302 ]
 [76.8114  ]
 [76.88185 ]
 [76.975464]
 [77.08205 ]], R is [[76.71553802]
 [76.7399826 ]
 [76.76398468]
 [76.7877655 ]
 [76.81199646]].
[2019-04-04 00:40:22,029] A3C_AGENT_WORKER-Thread-4 INFO:Local step 128500, global step 2081753: loss 3.0016
[2019-04-04 00:40:22,033] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 128500, global step 2081754: learning rate 0.0005
[2019-04-04 00:40:22,979] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0200721e-25 3.7355800e-12 2.2644317e-11 1.0000000e+00 1.6478150e-16
 1.7711967e-11 3.4323244e-16], sum to 1.0000
[2019-04-04 00:40:22,982] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4824
[2019-04-04 00:40:22,992] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.6, 93.0, 0.0, 0.0, 21.5, 21.78816860180589, -0.3633390831552968, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.5], 
sim time this is 1645200.0000, 
sim time next is 1645800.0000, 
raw observation next is [6.7, 93.5, 0.0, 0.0, 21.5, 21.7776821430961, -0.3750237849878401, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.6481994459833795, 0.935, 0.0, 0.0, 0.2916666666666667, 0.3148068452580084, 0.37499207167071996, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.71735674], dtype=float32), 1.3546076]. 
=============================================
[2019-04-04 00:40:26,115] A3C_AGENT_WORKER-Thread-19 INFO:Local step 131000, global step 2083424: loss 0.5357
[2019-04-04 00:40:26,118] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 131000, global step 2083425: learning rate 0.0005
[2019-04-04 00:40:27,836] A3C_AGENT_WORKER-Thread-2 INFO:Local step 131500, global step 2084246: loss 0.0394
[2019-04-04 00:40:27,837] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 131500, global step 2084246: learning rate 0.0005
[2019-04-04 00:40:28,043] A3C_AGENT_WORKER-Thread-10 INFO:Local step 130500, global step 2084353: loss 0.0319
[2019-04-04 00:40:28,056] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 130500, global step 2084359: learning rate 0.0005
[2019-04-04 00:40:29,427] A3C_AGENT_WORKER-Thread-15 INFO:Local step 129500, global step 2084984: loss 0.0975
[2019-04-04 00:40:29,429] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 129500, global step 2084984: learning rate 0.0005
[2019-04-04 00:40:29,953] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.4755873e-20 7.9006697e-12 2.8991417e-08 1.0000000e+00 1.1327969e-10
 1.4667481e-10 1.2071090e-12], sum to 1.0000
[2019-04-04 00:40:29,958] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8255
[2019-04-04 00:40:30,010] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.3, 87.0, 91.66666666666667, 0.0, 21.5, 20.63364554122809, -0.6467578813792191, 0.0, 1.0, 57951.38587648356], 
current ob forecast is [], 
actual action is [21.5], 
sim time this is 1765200.0000, 
sim time next is 1765800.0000, 
raw observation next is [-2.3, 87.0, 97.0, 0.0, 21.5, 20.65123733242489, -0.643214937846993, 0.0, 1.0, 34578.16585424112], 
processed observation next is [0.0, 0.43478260869565216, 0.3988919667590028, 0.87, 0.3233333333333333, 0.0, 0.2916666666666667, 0.22093644436874094, 0.285595020717669, 0.0, 1.0, 0.16465793263924342], 
reward next is 0.8353, 
noisyNet noise sample is [array([0.35680136], dtype=float32), -0.04440079]. 
=============================================
[2019-04-04 00:40:31,158] A3C_AGENT_WORKER-Thread-13 INFO:Local step 129000, global step 2085711: loss 0.4084
[2019-04-04 00:40:31,164] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 129000, global step 2085712: learning rate 0.0005
[2019-04-04 00:40:31,478] A3C_AGENT_WORKER-Thread-14 INFO:Local step 130000, global step 2085845: loss 1.8252
[2019-04-04 00:40:31,481] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 130000, global step 2085845: learning rate 0.0005
[2019-04-04 00:40:31,712] A3C_AGENT_WORKER-Thread-5 INFO:Local step 129500, global step 2085956: loss 0.0288
[2019-04-04 00:40:31,715] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 129500, global step 2085957: learning rate 0.0005
[2019-04-04 00:40:33,920] A3C_AGENT_WORKER-Thread-17 INFO:Local step 131000, global step 2086865: loss 0.5089
[2019-04-04 00:40:33,929] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 131000, global step 2086866: learning rate 0.0005
[2019-04-04 00:40:35,211] A3C_AGENT_WORKER-Thread-20 INFO:Local step 131000, global step 2087442: loss 4.9709
[2019-04-04 00:40:35,213] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 131000, global step 2087444: learning rate 0.0005
[2019-04-04 00:40:35,360] A3C_AGENT_WORKER-Thread-12 INFO:Local step 130500, global step 2087492: loss 0.1710
[2019-04-04 00:40:35,361] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 130500, global step 2087492: learning rate 0.0005
[2019-04-04 00:40:35,992] A3C_AGENT_WORKER-Thread-6 INFO:Local step 130500, global step 2087715: loss 0.1488
[2019-04-04 00:40:35,994] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 130500, global step 2087715: learning rate 0.0005
[2019-04-04 00:40:38,519] A3C_AGENT_WORKER-Thread-11 INFO:Local step 131000, global step 2088785: loss 0.3274
[2019-04-04 00:40:38,520] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 131000, global step 2088785: learning rate 0.0005
[2019-04-04 00:40:39,694] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.3439519e-31 2.0590177e-15 2.1600298e-16 1.0000000e+00 5.7355870e-20
 1.2745622e-16 1.8708514e-20], sum to 1.0000
[2019-04-04 00:40:39,699] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5718
[2019-04-04 00:40:39,700] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5023518e-23 3.6996407e-14 2.4948392e-12 1.0000000e+00 2.7519494e-16
 2.1748755e-12 1.7148370e-15], sum to 1.0000
[2019-04-04 00:40:39,702] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3631
[2019-04-04 00:40:39,705] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [12.8, 63.66666666666666, 0.0, 0.0, 23.0, 23.23852281273152, 0.05160827389074595, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 1115400.0000, 
sim time next is 1116000.0000, 
raw observation next is [12.7, 64.0, 0.0, 0.0, 23.0, 23.19351116349337, 0.04161470837966576, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.8144044321329641, 0.64, 0.0, 0.0, 0.4166666666666667, 0.4327925969577808, 0.5138715694598887, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6972518], dtype=float32), -0.37492353]. 
=============================================
[2019-04-04 00:40:39,710] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[104.92314]
 [105.06857]
 [105.54379]
 [105.92656]
 [106.31868]], R is [[104.5307312 ]
 [104.48542786]
 [104.44057465]
 [104.39617157]
 [104.352211  ]].
[2019-04-04 00:40:39,718] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.3, 63.0, 0.0, 0.0, 22.0, 21.43889480048175, -0.540440142260672, 0.0, 1.0, 68009.15587380428], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 2335200.0000, 
sim time next is 2335800.0000, 
raw observation next is [-2.3, 62.5, 0.0, 0.0, 22.0, 21.4249889435145, -0.537950944094108, 0.0, 1.0, 58479.66057712032], 
processed observation next is [0.0, 0.0, 0.3988919667590028, 0.625, 0.0, 0.0, 0.3333333333333333, 0.28541574529287494, 0.3206830186352973, 0.0, 1.0, 0.2784745741767634], 
reward next is 0.7215, 
noisyNet noise sample is [array([-2.1237757], dtype=float32), 1.2232666]. 
=============================================
[2019-04-04 00:40:41,547] A3C_AGENT_WORKER-Thread-16 INFO:Local step 131000, global step 2090055: loss 1.0381
[2019-04-04 00:40:41,547] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 131000, global step 2090055: learning rate 0.0005
[2019-04-04 00:40:41,560] A3C_AGENT_WORKER-Thread-4 INFO:Local step 129000, global step 2090063: loss 3.2664
[2019-04-04 00:40:41,560] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 129000, global step 2090063: learning rate 0.0005
[2019-04-04 00:40:42,234] A3C_AGENT_WORKER-Thread-18 INFO:Local step 131000, global step 2090350: loss 0.2135
[2019-04-04 00:40:42,235] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 131000, global step 2090351: learning rate 0.0005
[2019-04-04 00:40:43,558] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.4636278e-22 9.0874773e-12 4.9558302e-08 1.0000000e+00 2.1905843e-11
 1.0245647e-10 4.9256968e-14], sum to 1.0000
[2019-04-04 00:40:43,558] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7600
[2019-04-04 00:40:43,623] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 79.0, 152.0, 0.0, 23.0, 22.073085892395, -0.326334123351516, 1.0, 1.0, 153841.6188414704], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 2034000.0000, 
sim time next is 2034600.0000, 
raw observation next is [-4.4, 79.00000000000001, 150.6666666666667, 0.0, 23.0, 22.57228381692076, -0.2943638293810939, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.3407202216066482, 0.7900000000000001, 0.5022222222222223, 0.0, 0.4166666666666667, 0.3810236514100633, 0.40187872353963533, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7316031], dtype=float32), -0.13539535]. 
=============================================
[2019-04-04 00:40:43,764] A3C_AGENT_WORKER-Thread-13 INFO:Local step 129500, global step 2090983: loss 0.0293
[2019-04-04 00:40:43,765] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 129500, global step 2090983: learning rate 0.0005
[2019-04-04 00:40:43,932] A3C_AGENT_WORKER-Thread-3 INFO:Local step 131000, global step 2091059: loss 0.1098
[2019-04-04 00:40:43,933] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 131000, global step 2091059: learning rate 0.0005
[2019-04-04 00:40:44,786] A3C_AGENT_WORKER-Thread-15 INFO:Local step 130000, global step 2091429: loss 0.2705
[2019-04-04 00:40:44,788] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 130000, global step 2091429: learning rate 0.0005
[2019-04-04 00:40:46,167] A3C_AGENT_WORKER-Thread-19 INFO:Local step 131500, global step 2091948: loss 0.0353
[2019-04-04 00:40:46,169] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 131500, global step 2091948: learning rate 0.0005
[2019-04-04 00:40:46,355] A3C_AGENT_WORKER-Thread-2 INFO:Local step 132000, global step 2092042: loss 0.0691
[2019-04-04 00:40:46,358] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 132000, global step 2092045: learning rate 0.0005
[2019-04-04 00:40:47,351] A3C_AGENT_WORKER-Thread-5 INFO:Local step 130000, global step 2092540: loss 0.0283
[2019-04-04 00:40:47,354] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 130000, global step 2092540: learning rate 0.0005
[2019-04-04 00:40:48,461] A3C_AGENT_WORKER-Thread-14 INFO:Local step 130500, global step 2093054: loss 0.0216
[2019-04-04 00:40:48,464] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 130500, global step 2093054: learning rate 0.0005
[2019-04-04 00:40:50,844] A3C_AGENT_WORKER-Thread-10 INFO:Local step 131000, global step 2094084: loss 0.1767
[2019-04-04 00:40:50,846] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 131000, global step 2094084: learning rate 0.0005
[2019-04-04 00:40:53,290] A3C_AGENT_WORKER-Thread-4 INFO:Local step 129500, global step 2095137: loss 0.0787
[2019-04-04 00:40:53,295] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 129500, global step 2095142: learning rate 0.0005
[2019-04-04 00:40:55,021] A3C_AGENT_WORKER-Thread-17 INFO:Local step 131500, global step 2095943: loss 0.0244
[2019-04-04 00:40:55,048] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 131500, global step 2095958: learning rate 0.0005
[2019-04-04 00:40:55,221] A3C_AGENT_WORKER-Thread-20 INFO:Local step 131500, global step 2096033: loss 0.1031
[2019-04-04 00:40:55,223] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 131500, global step 2096034: learning rate 0.0005
[2019-04-04 00:40:56,300] A3C_AGENT_WORKER-Thread-12 INFO:Local step 131000, global step 2096502: loss 0.6185
[2019-04-04 00:40:56,301] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 131000, global step 2096502: learning rate 0.0005
[2019-04-04 00:40:56,340] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0293045e-18 2.1985237e-12 7.0562201e-10 1.0000000e+00 9.4699604e-12
 5.0220984e-11 1.3469543e-11], sum to 1.0000
[2019-04-04 00:40:56,341] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1407
[2019-04-04 00:40:56,403] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.016666666666667, 48.83333333333334, 54.0, 582.0, 21.5, 21.38164667429139, -0.6364335664925137, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.5], 
sim time this is 2452200.0000, 
sim time next is 2452800.0000, 
raw observation next is [-6.733333333333333, 47.66666666666667, 57.5, 623.5, 21.5, 21.3664260919548, -0.6415024293280421, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.2760849492151431, 0.47666666666666674, 0.19166666666666668, 0.6889502762430939, 0.2916666666666667, 0.2805355076629, 0.2861658568906526, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0573888], dtype=float32), 0.20135386]. 
=============================================
[2019-04-04 00:40:58,585] A3C_AGENT_WORKER-Thread-6 INFO:Local step 131000, global step 2097477: loss 0.1200
[2019-04-04 00:40:58,585] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 131000, global step 2097477: learning rate 0.0005
[2019-04-04 00:40:58,877] A3C_AGENT_WORKER-Thread-13 INFO:Local step 130000, global step 2097603: loss 0.3513
[2019-04-04 00:40:58,877] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 130000, global step 2097603: learning rate 0.0005
[2019-04-04 00:40:59,524] A3C_AGENT_WORKER-Thread-11 INFO:Local step 131500, global step 2097907: loss 0.0009
[2019-04-04 00:40:59,526] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 131500, global step 2097907: learning rate 0.0005
[2019-04-04 00:41:01,078] A3C_AGENT_WORKER-Thread-15 INFO:Local step 130500, global step 2098575: loss 0.0003
[2019-04-04 00:41:01,079] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 130500, global step 2098575: learning rate 0.0005
[2019-04-04 00:41:01,183] A3C_AGENT_WORKER-Thread-16 INFO:Local step 131500, global step 2098613: loss 0.0298
[2019-04-04 00:41:01,186] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 131500, global step 2098614: learning rate 0.0005
[2019-04-04 00:41:03,489] A3C_AGENT_WORKER-Thread-19 INFO:Local step 132000, global step 2099576: loss 0.0138
[2019-04-04 00:41:03,489] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 132000, global step 2099576: learning rate 0.0005
[2019-04-04 00:41:03,523] A3C_AGENT_WORKER-Thread-18 INFO:Local step 131500, global step 2099591: loss 0.0104
[2019-04-04 00:41:03,523] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 131500, global step 2099591: learning rate 0.0005
[2019-04-04 00:41:04,264] A3C_AGENT_WORKER-Thread-2 INFO:Local step 132500, global step 2099961: loss 0.0021
[2019-04-04 00:41:04,267] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 132500, global step 2099962: learning rate 0.0005
[2019-04-04 00:41:04,348] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-04 00:41:04,350] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 00:41:04,351] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:41:04,351] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 00:41:04,351] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:41:04,352] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 00:41:04,353] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:41:04,356] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run22
[2019-04-04 00:41:04,384] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run22
[2019-04-04 00:41:04,397] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run22
[2019-04-04 00:43:17,329] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6520.7600 170041441.4925 -1181.0143
[2019-04-04 00:43:43,579] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 6640.6723 206351835.5778 -1270.5073
[2019-04-04 00:43:47,959] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 6282.5377 216003519.6679 -1412.7978
[2019-04-04 00:43:48,995] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 2100000, evaluation results [2100000.0, 6640.672277155529, 206351835.57776225, -1270.5072648337327, 6520.759955320599, 170041441.4924678, -1181.0142505757894, 6282.537734399273, 216003519.66791537, -1412.7978025734712]
[2019-04-04 00:43:49,394] A3C_AGENT_WORKER-Thread-5 INFO:Local step 130500, global step 2100089: loss 0.0136
[2019-04-04 00:43:49,407] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 130500, global step 2100089: learning rate 0.0005
[2019-04-04 00:43:49,885] A3C_AGENT_WORKER-Thread-3 INFO:Local step 131500, global step 2100186: loss 0.0053
[2019-04-04 00:43:49,886] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 131500, global step 2100186: learning rate 0.0005
[2019-04-04 00:43:53,907] A3C_AGENT_WORKER-Thread-4 INFO:Local step 130000, global step 2101129: loss 0.2194
[2019-04-04 00:43:53,909] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 130000, global step 2101130: learning rate 0.0005
[2019-04-04 00:43:58,546] A3C_AGENT_WORKER-Thread-14 INFO:Local step 131000, global step 2102152: loss 0.7327
[2019-04-04 00:43:58,547] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 131000, global step 2102152: learning rate 0.0005
[2019-04-04 00:44:01,120] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.3482115e-28 3.4672240e-16 5.7967064e-15 1.0000000e+00 7.0451946e-17
 1.2831825e-16 1.9912547e-18], sum to 1.0000
[2019-04-04 00:44:01,127] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2880
[2019-04-04 00:44:01,144] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.9, 51.66666666666667, 0.0, 0.0, 24.0, 23.29634108100934, -0.1986580241739536, 0.0, 1.0, 47321.24407780262], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2521200.0000, 
sim time next is 2521800.0000, 
raw observation next is [-2.0, 53.0, 0.0, 0.0, 24.0, 23.31372704677429, -0.1946169169746781, 0.0, 1.0, 42290.62843145595], 
processed observation next is [1.0, 0.17391304347826086, 0.40720221606648205, 0.53, 0.0, 0.0, 0.5, 0.44281058723119077, 0.43512769434177395, 0.0, 1.0, 0.201383944911695], 
reward next is 0.7986, 
noisyNet noise sample is [array([2.3728588], dtype=float32), 1.2310505]. 
=============================================
[2019-04-04 00:44:02,668] A3C_AGENT_WORKER-Thread-10 INFO:Local step 131500, global step 2102980: loss 0.0005
[2019-04-04 00:44:02,691] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 131500, global step 2102981: learning rate 0.0005
[2019-04-04 00:44:05,257] A3C_AGENT_WORKER-Thread-17 INFO:Local step 132000, global step 2103550: loss 0.0362
[2019-04-04 00:44:05,305] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 132000, global step 2103550: learning rate 0.0005
[2019-04-04 00:44:06,298] A3C_AGENT_WORKER-Thread-20 INFO:Local step 132000, global step 2103767: loss 0.0274
[2019-04-04 00:44:06,322] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 132000, global step 2103767: learning rate 0.0005
[2019-04-04 00:44:10,912] A3C_AGENT_WORKER-Thread-13 INFO:Local step 130500, global step 2104604: loss 0.0001
[2019-04-04 00:44:10,912] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 130500, global step 2104604: learning rate 0.0005
[2019-04-04 00:44:11,366] A3C_AGENT_WORKER-Thread-12 INFO:Local step 131500, global step 2104699: loss 0.1150
[2019-04-04 00:44:11,367] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 131500, global step 2104699: learning rate 0.0005
[2019-04-04 00:44:15,073] A3C_AGENT_WORKER-Thread-6 INFO:Local step 131500, global step 2105621: loss 0.0299
[2019-04-04 00:44:15,074] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 131500, global step 2105621: learning rate 0.0005
[2019-04-04 00:44:15,105] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.0704006e-27 6.9788050e-17 1.3322155e-13 1.0000000e+00 1.9229740e-16
 7.4085512e-17 3.2554131e-18], sum to 1.0000
[2019-04-04 00:44:15,105] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3538
[2019-04-04 00:44:15,135] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.8666666666666667, 29.0, 89.5, 842.8333333333334, 22.0, 21.2883148087623, -0.577553197614866, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 2464800.0000, 
sim time next is 2465400.0000, 
raw observation next is [1.233333333333333, 28.5, 89.0, 840.6666666666666, 22.0, 21.26732696801226, -0.5775184310446965, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.49676823638042483, 0.285, 0.2966666666666667, 0.9289134438305708, 0.3333333333333333, 0.27227724733435493, 0.3074938563184345, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.68566024], dtype=float32), 0.3583882]. 
=============================================
[2019-04-04 00:44:17,960] A3C_AGENT_WORKER-Thread-11 INFO:Local step 132000, global step 2106221: loss 0.0328
[2019-04-04 00:44:17,968] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 132000, global step 2106221: learning rate 0.0005
[2019-04-04 00:44:19,913] A3C_AGENT_WORKER-Thread-16 INFO:Local step 132000, global step 2106579: loss 0.0308
[2019-04-04 00:44:19,945] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 132000, global step 2106579: learning rate 0.0005
[2019-04-04 00:44:23,255] A3C_AGENT_WORKER-Thread-2 INFO:Local step 133000, global step 2107217: loss 0.3957
[2019-04-04 00:44:23,276] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 133000, global step 2107217: learning rate 0.0005
[2019-04-04 00:44:23,467] A3C_AGENT_WORKER-Thread-19 INFO:Local step 132500, global step 2107268: loss 0.0070
[2019-04-04 00:44:23,485] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 132500, global step 2107268: learning rate 0.0005
[2019-04-04 00:44:24,692] A3C_AGENT_WORKER-Thread-15 INFO:Local step 131000, global step 2107564: loss 0.3501
[2019-04-04 00:44:24,692] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 131000, global step 2107564: learning rate 0.0005
[2019-04-04 00:44:25,772] A3C_AGENT_WORKER-Thread-18 INFO:Local step 132000, global step 2107816: loss 0.9031
[2019-04-04 00:44:25,776] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 132000, global step 2107817: learning rate 0.0005
[2019-04-04 00:44:27,440] A3C_AGENT_WORKER-Thread-3 INFO:Local step 132000, global step 2108199: loss 0.4013
[2019-04-04 00:44:27,441] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 132000, global step 2108199: learning rate 0.0005
[2019-04-04 00:44:27,587] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.0882248e-25 2.9451454e-15 1.5056592e-12 1.0000000e+00 4.2233271e-17
 8.0559770e-15 1.5834201e-15], sum to 1.0000
[2019-04-04 00:44:27,588] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3452
[2019-04-04 00:44:27,675] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.666666666666667, 53.33333333333334, 113.5, 815.0, 22.0, 21.64422641233929, -0.4383370093534875, 0.0, 1.0, 20859.38715690312], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 3068400.0000, 
sim time next is 3069000.0000, 
raw observation next is [-2.5, 52.5, 114.0, 817.0, 22.0, 21.66488828034368, -0.4305607346703613, 0.0, 1.0, 18698.70533538394], 
processed observation next is [0.0, 0.5217391304347826, 0.39335180055401664, 0.525, 0.38, 0.9027624309392265, 0.3333333333333333, 0.30540735669530655, 0.35647975510987956, 0.0, 1.0, 0.08904145397801876], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.35035333], dtype=float32), -1.027484]. 
=============================================
[2019-04-04 00:44:27,766] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[77.66822 ]
 [77.62757 ]
 [77.53113 ]
 [77.53603 ]
 [77.631355]], R is [[77.81723022]
 [77.93972778]
 [78.00119019]
 [77.97816467]
 [78.19838715]].
[2019-04-04 00:44:28,922] A3C_AGENT_WORKER-Thread-4 INFO:Local step 130500, global step 2108493: loss 0.0013
[2019-04-04 00:44:28,923] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 130500, global step 2108493: learning rate 0.0005
[2019-04-04 00:44:30,961] A3C_AGENT_WORKER-Thread-5 INFO:Local step 131000, global step 2108860: loss 0.3927
[2019-04-04 00:44:30,961] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 131000, global step 2108860: learning rate 0.0005
[2019-04-04 00:44:36,795] A3C_AGENT_WORKER-Thread-14 INFO:Local step 131500, global step 2110175: loss 0.0084
[2019-04-04 00:44:36,796] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 131500, global step 2110175: learning rate 0.0005
[2019-04-04 00:44:39,477] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.93964289e-19 6.65729885e-07 1.37973684e-05 9.99973059e-01
 3.35983081e-08 1.24094095e-05 2.42012343e-09], sum to 1.0000
[2019-04-04 00:44:39,478] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6356
[2019-04-04 00:44:39,520] A3C_AGENT_WORKER-Thread-10 INFO:Local step 132000, global step 2110765: loss 0.0012
[2019-04-04 00:44:39,537] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 132000, global step 2110765: learning rate 0.0005
[2019-04-04 00:44:39,583] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 100.0, 0.0, 0.0, 22.0, 22.82669086438334, -0.06927673192088725, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 3182400.0000, 
sim time next is 3183000.0000, 
raw observation next is [3.0, 100.0, 0.0, 0.0, 22.0, 22.76917110329901, -0.08383406987659475, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.5457063711911359, 1.0, 0.0, 0.0, 0.3333333333333333, 0.39743092527491736, 0.4720553100411351, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4996412], dtype=float32), -1.5840548]. 
=============================================
[2019-04-04 00:44:39,628] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[82.88098]
 [81.96368]
 [82.13566]
 [82.59506]
 [83.25124]], R is [[82.90994263]
 [83.08084106]
 [83.25003052]
 [83.41753387]
 [83.58335876]].
[2019-04-04 00:44:43,842] A3C_AGENT_WORKER-Thread-17 INFO:Local step 132500, global step 2111665: loss 0.0054
[2019-04-04 00:44:43,843] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 132500, global step 2111665: learning rate 0.0005
[2019-04-04 00:44:44,703] A3C_AGENT_WORKER-Thread-20 INFO:Local step 132500, global step 2111907: loss 0.0041
[2019-04-04 00:44:44,707] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 132500, global step 2111907: learning rate 0.0005
[2019-04-04 00:44:44,816] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.9400500e-22 6.0503611e-14 1.6819483e-11 1.0000000e+00 1.8490152e-14
 1.0868012e-11 1.3472034e-15], sum to 1.0000
[2019-04-04 00:44:44,816] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1642
[2019-04-04 00:44:44,872] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.8999999999999999, 34.0, 0.0, 0.0, 23.0, 22.45618336700622, -0.3688506307575818, 0.0, 1.0, 52178.90760610312], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 2500200.0000, 
sim time next is 2500800.0000, 
raw observation next is [-0.8, 34.33333333333334, 0.0, 0.0, 23.0, 22.46059393204615, -0.3669553936570536, 0.0, 1.0, 41747.61904962733], 
processed observation next is [0.0, 0.9565217391304348, 0.4404432132963989, 0.34333333333333343, 0.0, 0.0, 0.4166666666666667, 0.37171616100384597, 0.3776815354476488, 0.0, 1.0, 0.1987981859506063], 
reward next is 0.8012, 
noisyNet noise sample is [array([-0.08132333], dtype=float32), 1.2182205]. 
=============================================
[2019-04-04 00:44:47,414] A3C_AGENT_WORKER-Thread-12 INFO:Local step 132000, global step 2112578: loss 0.0184
[2019-04-04 00:44:47,425] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 132000, global step 2112578: learning rate 0.0005
[2019-04-04 00:44:49,948] A3C_AGENT_WORKER-Thread-13 INFO:Local step 131000, global step 2113149: loss 0.4154
[2019-04-04 00:44:49,957] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 131000, global step 2113153: learning rate 0.0005
[2019-04-04 00:44:52,485] A3C_AGENT_WORKER-Thread-6 INFO:Local step 132000, global step 2113654: loss 0.1416
[2019-04-04 00:44:52,487] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 132000, global step 2113654: learning rate 0.0005
[2019-04-04 00:44:53,944] A3C_AGENT_WORKER-Thread-11 INFO:Local step 132500, global step 2114001: loss 0.0568
[2019-04-04 00:44:54,000] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 132500, global step 2114001: learning rate 0.0005
[2019-04-04 00:44:55,127] A3C_AGENT_WORKER-Thread-16 INFO:Local step 132500, global step 2114350: loss 0.0131
[2019-04-04 00:44:55,134] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 132500, global step 2114351: learning rate 0.0005
[2019-04-04 00:44:55,677] A3C_AGENT_WORKER-Thread-2 INFO:Local step 133500, global step 2114507: loss 3.0017
[2019-04-04 00:44:55,679] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 133500, global step 2114507: learning rate 0.0005
[2019-04-04 00:44:56,832] A3C_AGENT_WORKER-Thread-19 INFO:Local step 133000, global step 2114787: loss 0.2971
[2019-04-04 00:44:56,835] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 133000, global step 2114788: learning rate 0.0005
[2019-04-04 00:45:00,715] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.4606953e-24 9.2562364e-12 5.2351886e-08 1.0000000e+00 1.3555515e-12
 1.4521853e-08 3.5489562e-14], sum to 1.0000
[2019-04-04 00:45:00,715] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3520
[2019-04-04 00:45:00,784] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 85.0, 0.0, 0.0, 23.0, 22.41332917845475, -0.2399509217398068, 0.0, 1.0, 46719.52177067685], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 2934000.0000, 
sim time next is 2934600.0000, 
raw observation next is [-2.0, 85.00000000000001, 0.0, 0.0, 23.0, 22.42665376685711, -0.2366136579332209, 0.0, 1.0, 37339.37935065736], 
processed observation next is [1.0, 1.0, 0.40720221606648205, 0.8500000000000001, 0.0, 0.0, 0.4166666666666667, 0.3688878139047593, 0.42112878068892634, 0.0, 1.0, 0.1778065683364636], 
reward next is 0.8222, 
noisyNet noise sample is [array([1.6526316], dtype=float32), 0.072768666]. 
=============================================
[2019-04-04 00:45:01,840] A3C_AGENT_WORKER-Thread-15 INFO:Local step 131500, global step 2115934: loss 0.0018
[2019-04-04 00:45:01,846] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 131500, global step 2115934: learning rate 0.0005
[2019-04-04 00:45:01,862] A3C_AGENT_WORKER-Thread-18 INFO:Local step 132500, global step 2115942: loss 0.0041
[2019-04-04 00:45:01,873] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 132500, global step 2115944: learning rate 0.0005
[2019-04-04 00:45:03,463] A3C_AGENT_WORKER-Thread-3 INFO:Local step 132500, global step 2116386: loss 0.0008
[2019-04-04 00:45:03,467] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 132500, global step 2116387: learning rate 0.0005
[2019-04-04 00:45:07,077] A3C_AGENT_WORKER-Thread-5 INFO:Local step 131500, global step 2117368: loss 0.0143
[2019-04-04 00:45:07,118] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 131500, global step 2117374: learning rate 0.0005
[2019-04-04 00:45:08,344] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.6560577e-25 6.9586620e-15 6.1392643e-15 1.0000000e+00 9.9496171e-13
 7.1932266e-13 3.5476329e-16], sum to 1.0000
[2019-04-04 00:45:08,344] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1872
[2019-04-04 00:45:08,390] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 92.0, 0.0, 0.0, 24.0, 23.61181068913758, -0.03871994566596834, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3096000.0000, 
sim time next is 3096600.0000, 
raw observation next is [-1.0, 93.33333333333334, 0.0, 0.0, 24.0, 23.69293771472165, -0.04112830245572226, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.4349030470914128, 0.9333333333333335, 0.0, 0.0, 0.5, 0.4744114762268043, 0.48629056584809255, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.32057998], dtype=float32), 0.28900334]. 
=============================================
[2019-04-04 00:45:08,964] A3C_AGENT_WORKER-Thread-4 INFO:Local step 131000, global step 2117850: loss 0.1705
[2019-04-04 00:45:08,964] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 131000, global step 2117850: learning rate 0.0005
[2019-04-04 00:45:10,488] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.08888411e-23 4.60123069e-14 1.32835002e-11 1.00000000e+00
 5.83526588e-15 3.79148268e-09 1.00150465e-14], sum to 1.0000
[2019-04-04 00:45:10,494] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3892
[2019-04-04 00:45:10,510] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 23.0, 22.49481743468035, -0.2942309718020389, 0.0, 1.0, 25903.57421663044], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 3016800.0000, 
sim time next is 3017400.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 23.0, 22.49300664753979, -0.292361260092705, 0.0, 1.0, 31978.9380697478], 
processed observation next is [0.0, 0.9565217391304348, 0.3518005540166205, 0.65, 0.0, 0.0, 0.4166666666666667, 0.37441722062831584, 0.402546246635765, 0.0, 1.0, 0.15228065747498953], 
reward next is 0.8477, 
noisyNet noise sample is [array([-0.8811124], dtype=float32), -1.2185377]. 
=============================================
[2019-04-04 00:45:10,645] A3C_AGENT_WORKER-Thread-14 INFO:Local step 132000, global step 2118227: loss 0.0784
[2019-04-04 00:45:10,646] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 132000, global step 2118227: learning rate 0.0005
[2019-04-04 00:45:12,322] A3C_AGENT_WORKER-Thread-10 INFO:Local step 132500, global step 2118666: loss 0.0728
[2019-04-04 00:45:12,328] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 132500, global step 2118666: learning rate 0.0005
[2019-04-04 00:45:14,425] A3C_AGENT_WORKER-Thread-17 INFO:Local step 133000, global step 2119301: loss 0.5425
[2019-04-04 00:45:14,429] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 133000, global step 2119301: learning rate 0.0005
[2019-04-04 00:45:15,843] A3C_AGENT_WORKER-Thread-20 INFO:Local step 133000, global step 2119718: loss 0.8519
[2019-04-04 00:45:15,891] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 133000, global step 2119718: learning rate 0.0005
[2019-04-04 00:45:17,865] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4245739e-21 2.7336938e-10 2.1984972e-09 9.9998641e-01 3.2598013e-13
 1.3646069e-05 9.1734082e-14], sum to 1.0000
[2019-04-04 00:45:17,866] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8662
[2019-04-04 00:45:17,962] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 22.5, 22.06677289315156, -0.3986338810387184, 0.0, 1.0, 44655.76908849418], 
current ob forecast is [], 
actual action is [22.5], 
sim time this is 3374400.0000, 
sim time next is 3375000.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 22.5, 21.98017465638191, -0.4063017249750835, 0.0, 1.0, 87966.35703441042], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.77, 0.0, 0.0, 0.375, 0.33168122136515904, 0.36456609167497217, 0.0, 1.0, 0.41888741444957345], 
reward next is 0.5811, 
noisyNet noise sample is [array([1.2252076], dtype=float32), -0.15731944]. 
=============================================
[2019-04-04 00:45:18,117] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[75.59863]
 [75.53883]
 [75.65931]
 [75.83824]
 [76.01323]], R is [[75.79733276]
 [75.82671356]
 [76.06845093]
 [76.30776978]
 [76.54469299]].
[2019-04-04 00:45:18,350] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.3573943e-27 2.5989321e-12 9.0873863e-13 1.0000000e+00 9.6102952e-18
 1.6356953e-10 4.3109146e-17], sum to 1.0000
[2019-04-04 00:45:18,350] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0967
[2019-04-04 00:45:18,402] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 93.0, 0.0, 0.0, 24.0, 23.64786451790288, 0.1443291189092654, 0.0, 1.0, 18727.68541353094], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3196800.0000, 
sim time next is 3197400.0000, 
raw observation next is [1.833333333333333, 94.16666666666666, 0.0, 0.0, 24.0, 23.63661960571494, 0.137123153969363, 0.0, 1.0, 24847.44852900254], 
processed observation next is [1.0, 0.0, 0.5133887349953832, 0.9416666666666665, 0.0, 0.0, 0.5, 0.4697183004762451, 0.5457077179897877, 0.0, 1.0, 0.11832118347144067], 
reward next is 0.8817, 
noisyNet noise sample is [array([-1.2511557], dtype=float32), -1.1857022]. 
=============================================
[2019-04-04 00:45:20,516] A3C_AGENT_WORKER-Thread-12 INFO:Local step 132500, global step 2120946: loss 0.0804
[2019-04-04 00:45:20,516] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 132500, global step 2120946: learning rate 0.0005
[2019-04-04 00:45:22,708] A3C_AGENT_WORKER-Thread-13 INFO:Local step 131500, global step 2121581: loss 0.0099
[2019-04-04 00:45:22,755] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 131500, global step 2121594: learning rate 0.0005
[2019-04-04 00:45:22,886] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.9827906e-25 1.2892324e-12 1.3008219e-12 1.0000000e+00 2.6294295e-14
 8.4893764e-10 6.8375409e-15], sum to 1.0000
[2019-04-04 00:45:22,887] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5219
[2019-04-04 00:45:22,895] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.666666666666666, 54.00000000000001, 117.6666666666667, 808.8333333333334, 23.5, 24.46872189072187, 0.1373430382437038, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.5], 
sim time this is 3327600.0000, 
sim time next is 3328200.0000, 
raw observation next is [-5.5, 54.0, 118.0, 811.0, 23.5, 24.36665255818349, 0.1265910322494959, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.3102493074792244, 0.54, 0.3933333333333333, 0.8961325966850828, 0.4583333333333333, 0.5305543798486241, 0.542197010749832, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.1971028], dtype=float32), -0.33728778]. 
=============================================
[2019-04-04 00:45:23,565] A3C_AGENT_WORKER-Thread-11 INFO:Local step 133000, global step 2121835: loss 0.0044
[2019-04-04 00:45:23,573] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 133000, global step 2121835: learning rate 0.0005
[2019-04-04 00:45:23,625] A3C_AGENT_WORKER-Thread-16 INFO:Local step 133000, global step 2121855: loss 0.4119
[2019-04-04 00:45:23,626] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 133000, global step 2121855: learning rate 0.0005
[2019-04-04 00:45:23,741] A3C_AGENT_WORKER-Thread-6 INFO:Local step 132500, global step 2121898: loss 0.0139
[2019-04-04 00:45:23,742] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 132500, global step 2121898: learning rate 0.0005
[2019-04-04 00:45:24,131] A3C_AGENT_WORKER-Thread-2 INFO:Local step 134000, global step 2122031: loss 0.0418
[2019-04-04 00:45:24,132] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 134000, global step 2122032: learning rate 0.0005
[2019-04-04 00:45:24,512] A3C_AGENT_WORKER-Thread-19 INFO:Local step 133500, global step 2122134: loss 1.4495
[2019-04-04 00:45:24,514] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 133500, global step 2122134: learning rate 0.0005
[2019-04-04 00:45:28,669] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.6968753e-21 2.9326298e-08 1.1275451e-09 9.9999881e-01 3.6415187e-08
 1.1055714e-06 1.0071982e-11], sum to 1.0000
[2019-04-04 00:45:28,669] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0740
[2019-04-04 00:45:28,791] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.5, 45.66666666666667, 205.8333333333333, 142.6666666666667, 23.5, 23.82402260370616, -0.04732417077803842, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.5], 
sim time this is 2644800.0000, 
sim time next is 2645400.0000, 
raw observation next is [0.5, 46.33333333333334, 195.6666666666667, 155.3333333333333, 23.5, 23.58757617805216, -0.0688453367281062, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.4764542936288089, 0.46333333333333343, 0.6522222222222224, 0.17163904235727434, 0.4583333333333333, 0.4656313481710133, 0.47705155442396463, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0324578], dtype=float32), 0.57998693]. 
=============================================
[2019-04-04 00:45:30,236] A3C_AGENT_WORKER-Thread-18 INFO:Local step 133000, global step 2123584: loss 0.1109
[2019-04-04 00:45:30,239] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 133000, global step 2123584: learning rate 0.0005
[2019-04-04 00:45:31,893] A3C_AGENT_WORKER-Thread-3 INFO:Local step 133000, global step 2124025: loss 0.2628
[2019-04-04 00:45:31,894] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 133000, global step 2124025: learning rate 0.0005
[2019-04-04 00:45:33,395] A3C_AGENT_WORKER-Thread-15 INFO:Local step 132000, global step 2124459: loss 0.2230
[2019-04-04 00:45:33,397] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 132000, global step 2124460: learning rate 0.0005
[2019-04-04 00:45:33,445] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.1573468e-22 1.5704718e-07 4.4185636e-08 9.9999988e-01 7.7879414e-10
 1.4139847e-09 3.2644914e-14], sum to 1.0000
[2019-04-04 00:45:33,445] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6713
[2019-04-04 00:45:33,494] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.0, 100.0, 0.0, 0.0, 23.0, 24.64973593724088, 0.2874914590750694, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 3175200.0000, 
sim time next is 3175800.0000, 
raw observation next is [5.666666666666667, 100.0, 0.0, 0.0, 23.0, 24.66614504954028, 0.2769573550241224, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.6195752539242845, 1.0, 0.0, 0.0, 0.4166666666666667, 0.55551208746169, 0.5923191183413742, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9975881], dtype=float32), 0.037899897]. 
=============================================
[2019-04-04 00:45:37,470] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.0038349e-26 2.1967516e-15 2.5313191e-13 1.0000000e+00 2.3442048e-18
 4.2674782e-15 7.3992274e-18], sum to 1.0000
[2019-04-04 00:45:37,471] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1670
[2019-04-04 00:45:37,547] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.666666666666667, 60.00000000000001, 72.83333333333333, 369.5, 24.0, 23.71518735853407, 0.002034786702284066, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3399600.0000, 
sim time next is 3400200.0000, 
raw observation next is [-1.5, 60.0, 87.0, 422.0, 24.0, 23.94933974591811, 0.02557727743931707, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.4210526315789474, 0.6, 0.29, 0.4662983425414365, 0.5, 0.49577831215984247, 0.508525759146439, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.667799], dtype=float32), 0.55545586]. 
=============================================
[2019-04-04 00:45:38,329] A3C_AGENT_WORKER-Thread-5 INFO:Local step 132000, global step 2125623: loss 0.0108
[2019-04-04 00:45:38,351] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 132000, global step 2125623: learning rate 0.0005
[2019-04-04 00:45:41,684] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.7283255e-24 1.1468043e-13 2.8262698e-11 1.0000000e+00 9.1794522e-17
 4.6282460e-13 7.4864226e-17], sum to 1.0000
[2019-04-04 00:45:41,703] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9559
[2019-04-04 00:45:41,783] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 23.5, 22.93421735718007, -0.2147151357992017, 0.0, 1.0, 41820.06310671345], 
current ob forecast is [], 
actual action is [23.5], 
sim time this is 2771400.0000, 
sim time next is 2772000.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 23.5, 22.90198021662677, -0.2245625959225994, 0.0, 1.0, 56114.44340703208], 
processed observation next is [1.0, 0.08695652173913043, 0.296398891966759, 0.59, 0.0, 0.0, 0.4583333333333333, 0.4084983513855643, 0.42514580135913355, 0.0, 1.0, 0.2672116352715813], 
reward next is 0.7328, 
noisyNet noise sample is [array([-0.42464125], dtype=float32), 0.25788128]. 
=============================================
[2019-04-04 00:45:41,796] A3C_AGENT_WORKER-Thread-17 INFO:Local step 133500, global step 2126648: loss 3.0454
[2019-04-04 00:45:41,798] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 133500, global step 2126648: learning rate 0.0005
[2019-04-04 00:45:41,797] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[81.90862 ]
 [81.99499 ]
 [81.887314]
 [81.77671 ]
 [81.87747 ]], R is [[81.85057068]
 [81.83292389]
 [81.68618011]
 [81.56124115]
 [81.59564209]].
[2019-04-04 00:45:41,931] A3C_AGENT_WORKER-Thread-10 INFO:Local step 133000, global step 2126683: loss 0.3694
[2019-04-04 00:45:41,932] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 133000, global step 2126683: learning rate 0.0005
[2019-04-04 00:45:43,306] A3C_AGENT_WORKER-Thread-4 INFO:Local step 131500, global step 2127083: loss 0.0031
[2019-04-04 00:45:43,307] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 131500, global step 2127083: learning rate 0.0005
[2019-04-04 00:45:43,774] A3C_AGENT_WORKER-Thread-20 INFO:Local step 133500, global step 2127238: loss 2.0929
[2019-04-04 00:45:43,775] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 133500, global step 2127238: learning rate 0.0005
[2019-04-04 00:45:44,490] A3C_AGENT_WORKER-Thread-14 INFO:Local step 132500, global step 2127400: loss 0.0080
[2019-04-04 00:45:44,491] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 132500, global step 2127400: learning rate 0.0005
[2019-04-04 00:45:45,627] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.6058972e-21 1.7625592e-11 5.1963256e-10 1.0000000e+00 5.3135793e-14
 5.6279017e-09 1.4760753e-12], sum to 1.0000
[2019-04-04 00:45:45,627] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4041
[2019-04-04 00:45:45,658] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.666666666666666, 77.66666666666667, 0.0, 0.0, 23.5, 22.89477189935657, 0.01070739733809589, 0.0, 1.0, 82591.41201932494], 
current ob forecast is [], 
actual action is [23.5], 
sim time this is 3271200.0000, 
sim time next is 3271800.0000, 
raw observation next is [-4.833333333333334, 79.33333333333333, 0.0, 0.0, 23.5, 23.00998711793419, 0.03100972436155471, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.32871652816251157, 0.7933333333333333, 0.0, 0.0, 0.4583333333333333, 0.4174989264945159, 0.510336574787185, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.12785055], dtype=float32), -2.0511744]. 
=============================================
[2019-04-04 00:45:48,097] A3C_AGENT_WORKER-Thread-12 INFO:Local step 133000, global step 2128435: loss 0.1742
[2019-04-04 00:45:48,097] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 133000, global step 2128435: learning rate 0.0005
[2019-04-04 00:45:50,655] A3C_AGENT_WORKER-Thread-16 INFO:Local step 133500, global step 2129245: loss 2.7312
[2019-04-04 00:45:50,660] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 133500, global step 2129245: learning rate 0.0005
[2019-04-04 00:45:51,194] A3C_AGENT_WORKER-Thread-19 INFO:Local step 134000, global step 2129387: loss 0.0944
[2019-04-04 00:45:51,195] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 134000, global step 2129387: learning rate 0.0005
[2019-04-04 00:45:52,172] A3C_AGENT_WORKER-Thread-6 INFO:Local step 133000, global step 2129601: loss 0.0128
[2019-04-04 00:45:52,176] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 133000, global step 2129601: learning rate 0.0005
[2019-04-04 00:45:52,969] A3C_AGENT_WORKER-Thread-2 INFO:Local step 134500, global step 2129801: loss 0.4805
[2019-04-04 00:45:52,971] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 134500, global step 2129802: learning rate 0.0005
[2019-04-04 00:45:53,071] A3C_AGENT_WORKER-Thread-11 INFO:Local step 133500, global step 2129837: loss 3.3113
[2019-04-04 00:45:53,071] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 133500, global step 2129837: learning rate 0.0005
[2019-04-04 00:45:53,791] A3C_AGENT_WORKER-Thread-13 INFO:Local step 132000, global step 2130039: loss 0.0026
[2019-04-04 00:45:53,792] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 132000, global step 2130039: learning rate 0.0005
[2019-04-04 00:46:00,425] A3C_AGENT_WORKER-Thread-18 INFO:Local step 133500, global step 2131909: loss 3.0160
[2019-04-04 00:46:00,465] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 133500, global step 2131912: learning rate 0.0005
[2019-04-04 00:46:01,778] A3C_AGENT_WORKER-Thread-3 INFO:Local step 133500, global step 2132318: loss 3.7766
[2019-04-04 00:46:01,782] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 133500, global step 2132320: learning rate 0.0005
[2019-04-04 00:46:04,844] A3C_AGENT_WORKER-Thread-15 INFO:Local step 132500, global step 2133218: loss 0.0009
[2019-04-04 00:46:04,844] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 132500, global step 2133218: learning rate 0.0005
[2019-04-04 00:46:05,628] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.2789513e-32 8.8511851e-19 1.8280546e-15 1.0000000e+00 3.5260047e-24
 1.7257618e-17 2.8965579e-21], sum to 1.0000
[2019-04-04 00:46:05,697] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6885
[2019-04-04 00:46:05,710] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.666666666666666, 48.0, 89.83333333333333, 716.8333333333333, 24.0, 23.82856306124399, 0.08999224450160535, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3684000.0000, 
sim time next is 3684600.0000, 
raw observation next is [5.5, 48.5, 87.0, 705.0, 24.0, 23.82730693622583, 0.08696218670054899, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.6149584487534627, 0.485, 0.29, 0.7790055248618785, 0.5, 0.4856089113521526, 0.5289873955668497, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.34323207], dtype=float32), 0.7862302]. 
=============================================
[2019-04-04 00:46:07,935] A3C_AGENT_WORKER-Thread-17 INFO:Local step 134000, global step 2134175: loss 0.0488
[2019-04-04 00:46:07,937] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 134000, global step 2134175: learning rate 0.0005
[2019-04-04 00:46:08,563] A3C_AGENT_WORKER-Thread-10 INFO:Local step 133500, global step 2134377: loss 2.8333
[2019-04-04 00:46:08,566] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 133500, global step 2134378: learning rate 0.0005
[2019-04-04 00:46:08,942] A3C_AGENT_WORKER-Thread-5 INFO:Local step 132500, global step 2134494: loss 0.0007
[2019-04-04 00:46:08,961] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 132500, global step 2134494: learning rate 0.0005
[2019-04-04 00:46:09,670] A3C_AGENT_WORKER-Thread-20 INFO:Local step 134000, global step 2134703: loss 0.0182
[2019-04-04 00:46:09,672] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 134000, global step 2134703: learning rate 0.0005
[2019-04-04 00:46:13,666] A3C_AGENT_WORKER-Thread-14 INFO:Local step 133000, global step 2135902: loss 0.2832
[2019-04-04 00:46:13,667] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 133000, global step 2135902: learning rate 0.0005
[2019-04-04 00:46:14,531] A3C_AGENT_WORKER-Thread-4 INFO:Local step 132000, global step 2136208: loss 0.5746
[2019-04-04 00:46:14,531] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 132000, global step 2136208: learning rate 0.0005
[2019-04-04 00:46:15,414] A3C_AGENT_WORKER-Thread-12 INFO:Local step 133500, global step 2136492: loss 4.4161
[2019-04-04 00:46:15,415] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 133500, global step 2136492: learning rate 0.0005
[2019-04-04 00:46:16,455] A3C_AGENT_WORKER-Thread-16 INFO:Local step 134000, global step 2136823: loss 0.0475
[2019-04-04 00:46:16,456] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 134000, global step 2136823: learning rate 0.0005
[2019-04-04 00:46:16,949] A3C_AGENT_WORKER-Thread-2 INFO:Local step 135000, global step 2136980: loss 0.5494
[2019-04-04 00:46:16,952] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 135000, global step 2136980: learning rate 0.0005
[2019-04-04 00:46:17,368] A3C_AGENT_WORKER-Thread-19 INFO:Local step 134500, global step 2137106: loss 0.2631
[2019-04-04 00:46:17,377] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 134500, global step 2137106: learning rate 0.0005
[2019-04-04 00:46:18,125] A3C_AGENT_WORKER-Thread-11 INFO:Local step 134000, global step 2137335: loss 0.0046
[2019-04-04 00:46:18,130] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 134000, global step 2137335: learning rate 0.0005
[2019-04-04 00:46:18,879] A3C_AGENT_WORKER-Thread-6 INFO:Local step 133500, global step 2137595: loss 4.9903
[2019-04-04 00:46:18,880] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 133500, global step 2137596: learning rate 0.0005
[2019-04-04 00:46:21,935] A3C_AGENT_WORKER-Thread-13 INFO:Local step 132500, global step 2138653: loss 0.0019
[2019-04-04 00:46:21,983] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 132500, global step 2138653: learning rate 0.0005
[2019-04-04 00:46:25,820] A3C_AGENT_WORKER-Thread-18 INFO:Local step 134000, global step 2139966: loss 0.2692
[2019-04-04 00:46:25,821] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 134000, global step 2139966: learning rate 0.0005
[2019-04-04 00:46:26,156] A3C_AGENT_WORKER-Thread-3 INFO:Local step 134000, global step 2140084: loss 0.3396
[2019-04-04 00:46:26,157] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 134000, global step 2140084: learning rate 0.0005
[2019-04-04 00:46:26,616] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.7041039e-21 1.9938716e-12 4.2268096e-09 1.0000000e+00 1.7247995e-11
 1.1941328e-10 5.5367789e-14], sum to 1.0000
[2019-04-04 00:46:26,616] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6056
[2019-04-04 00:46:26,658] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.5, 68.0, 0.0, 0.0, 23.0, 22.51655770174002, -0.1204374810881016, 0.0, 1.0, 141089.7200031324], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 3789000.0000, 
sim time next is 3789600.0000, 
raw observation next is [-2.666666666666667, 69.0, 0.0, 0.0, 23.0, 22.61477577549752, -0.1078915664597931, 0.0, 1.0, 22862.65494541454], 
processed observation next is [1.0, 0.8695652173913043, 0.38873499538319484, 0.69, 0.0, 0.0, 0.4166666666666667, 0.38456464795812667, 0.4640361445134023, 0.0, 1.0, 0.10886978545435495], 
reward next is 0.8911, 
noisyNet noise sample is [array([0.968616], dtype=float32), 0.259604]. 
=============================================
[2019-04-04 00:46:29,422] A3C_AGENT_WORKER-Thread-15 INFO:Local step 133000, global step 2141220: loss 0.5699
[2019-04-04 00:46:29,424] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 133000, global step 2141220: learning rate 0.0005
[2019-04-04 00:46:32,153] A3C_AGENT_WORKER-Thread-10 INFO:Local step 134000, global step 2142016: loss 0.0054
[2019-04-04 00:46:32,154] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 134000, global step 2142016: learning rate 0.0005
[2019-04-04 00:46:32,922] A3C_AGENT_WORKER-Thread-17 INFO:Local step 134500, global step 2142294: loss 0.4585
[2019-04-04 00:46:32,924] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 134500, global step 2142295: learning rate 0.0005
[2019-04-04 00:46:33,183] A3C_AGENT_WORKER-Thread-5 INFO:Local step 133000, global step 2142396: loss 0.1140
[2019-04-04 00:46:33,183] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 133000, global step 2142396: learning rate 0.0005
[2019-04-04 00:46:33,215] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.8547920e-25 1.3185633e-15 4.4558232e-10 1.0000000e+00 3.8787670e-17
 9.2927261e-13 8.4509609e-17], sum to 1.0000
[2019-04-04 00:46:33,227] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4898
[2019-04-04 00:46:33,262] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.833333333333333, 84.16666666666666, 0.0, 0.0, 24.5, 23.75382453664247, 0.04041223570394267, 0.0, 1.0, 43617.36571434722], 
current ob forecast is [], 
actual action is [24.5], 
sim time this is 2947800.0000, 
sim time next is 2948400.0000, 
raw observation next is [-3.0, 84.0, 0.0, 0.0, 24.5, 23.73324185522936, 0.03242189469608824, 0.0, 1.0, 43526.62887131415], 
processed observation next is [0.0, 0.13043478260869565, 0.3795013850415513, 0.84, 0.0, 0.0, 0.5416666666666666, 0.4777701546024466, 0.5108072982320294, 0.0, 1.0, 0.20726966129197213], 
reward next is 0.7927, 
noisyNet noise sample is [array([0.26308298], dtype=float32), 0.46398827]. 
=============================================
[2019-04-04 00:46:33,359] A3C_AGENT_WORKER-Thread-20 INFO:Local step 134500, global step 2142452: loss 0.2183
[2019-04-04 00:46:33,361] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 134500, global step 2142452: learning rate 0.0005
[2019-04-04 00:46:37,084] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.02999251e-24 1.15662445e-11 4.37067216e-10 9.99999762e-01
 5.44400251e-16 1.81091806e-07 6.33624320e-16], sum to 1.0000
[2019-04-04 00:46:37,084] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4969
[2019-04-04 00:46:37,133] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.4, 72.5, 0.0, 0.0, 25.0, 24.24684590198017, 0.1642277627767642, 0.0, 1.0, 44210.00443136491], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 4491000.0000, 
sim time next is 4491600.0000, 
raw observation next is [-0.4333333333333333, 72.66666666666667, 0.0, 0.0, 25.0, 24.23117375713923, 0.1605519965758711, 0.0, 1.0, 44146.24787519989], 
processed observation next is [1.0, 1.0, 0.45060018467220686, 0.7266666666666667, 0.0, 0.0, 0.5833333333333334, 0.5192644797616026, 0.553517332191957, 0.0, 1.0, 0.21022022797714235], 
reward next is 0.7898, 
noisyNet noise sample is [array([0.20344637], dtype=float32), 1.1140751]. 
=============================================
[2019-04-04 00:46:38,130] A3C_AGENT_WORKER-Thread-12 INFO:Local step 134000, global step 2144062: loss 0.0154
[2019-04-04 00:46:38,131] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 134000, global step 2144062: learning rate 0.0005
[2019-04-04 00:46:39,134] A3C_AGENT_WORKER-Thread-14 INFO:Local step 133500, global step 2144342: loss 9.6305
[2019-04-04 00:46:39,135] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 133500, global step 2144342: learning rate 0.0005
[2019-04-04 00:46:39,339] A3C_AGENT_WORKER-Thread-2 INFO:Local step 135500, global step 2144399: loss 0.0048
[2019-04-04 00:46:39,339] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 135500, global step 2144399: learning rate 0.0005
[2019-04-04 00:46:40,941] A3C_AGENT_WORKER-Thread-16 INFO:Local step 134500, global step 2144906: loss 0.2394
[2019-04-04 00:46:40,950] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 134500, global step 2144907: learning rate 0.0005
[2019-04-04 00:46:40,975] A3C_AGENT_WORKER-Thread-19 INFO:Local step 135000, global step 2144922: loss 0.6548
[2019-04-04 00:46:40,975] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 135000, global step 2144922: learning rate 0.0005
[2019-04-04 00:46:41,711] A3C_AGENT_WORKER-Thread-6 INFO:Local step 134000, global step 2145171: loss 0.0300
[2019-04-04 00:46:41,713] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 134000, global step 2145171: learning rate 0.0005
[2019-04-04 00:46:41,846] A3C_AGENT_WORKER-Thread-4 INFO:Local step 132500, global step 2145222: loss 0.0307
[2019-04-04 00:46:41,847] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 132500, global step 2145222: learning rate 0.0005
[2019-04-04 00:46:42,677] A3C_AGENT_WORKER-Thread-11 INFO:Local step 134500, global step 2145512: loss 0.0368
[2019-04-04 00:46:42,677] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 134500, global step 2145512: learning rate 0.0005
[2019-04-04 00:46:44,012] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.0203077e-28 1.3883254e-15 3.3175020e-13 1.0000000e+00 2.7785909e-19
 2.4195885e-13 1.9033837e-18], sum to 1.0000
[2019-04-04 00:46:44,012] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0074
[2019-04-04 00:46:44,024] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.3333333333333333, 70.33333333333333, 0.0, 0.0, 26.0, 25.419771744275, 0.4088275647180898, 0.0, 1.0, 77450.54053443388], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3707400.0000, 
sim time next is 3708000.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.41330569190532, 0.4125815607615664, 0.0, 1.0, 57409.02672310872], 
processed observation next is [0.0, 0.9565217391304348, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6177754743254434, 0.6375271869205221, 0.0, 1.0, 0.27337631772908916], 
reward next is 0.7266, 
noisyNet noise sample is [array([-1.1357394], dtype=float32), 1.6656339]. 
=============================================
[2019-04-04 00:46:44,053] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[89.61889 ]
 [89.75722 ]
 [89.850746]
 [89.94381 ]
 [90.10059 ]], R is [[89.27744293]
 [89.01585388]
 [88.69934082]
 [88.42222595]
 [88.31596375]].
[2019-04-04 00:46:46,944] A3C_AGENT_WORKER-Thread-13 INFO:Local step 133000, global step 2146987: loss 0.4014
[2019-04-04 00:46:46,945] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 133000, global step 2146987: learning rate 0.0005
[2019-04-04 00:46:50,686] A3C_AGENT_WORKER-Thread-3 INFO:Local step 134500, global step 2148213: loss 0.0287
[2019-04-04 00:46:50,687] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 134500, global step 2148214: learning rate 0.0005
[2019-04-04 00:46:51,096] A3C_AGENT_WORKER-Thread-18 INFO:Local step 134500, global step 2148424: loss 0.0397
[2019-04-04 00:46:51,098] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 134500, global step 2148424: learning rate 0.0005
[2019-04-04 00:46:53,117] A3C_AGENT_WORKER-Thread-15 INFO:Local step 133500, global step 2149405: loss 5.0464
[2019-04-04 00:46:53,122] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 133500, global step 2149407: learning rate 0.0005
[2019-04-04 00:46:53,291] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.7161830e-25 1.7164572e-11 9.5019123e-12 1.0000000e+00 4.1827152e-17
 6.5244468e-09 1.9247206e-16], sum to 1.0000
[2019-04-04 00:46:53,295] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5228
[2019-04-04 00:46:53,337] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.333333333333333, 67.33333333333334, 99.33333333333333, 641.1666666666667, 26.0, 26.15985090293763, 0.535459317876598, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3835200.0000, 
sim time next is 3835800.0000, 
raw observation next is [-3.0, 65.5, 101.0, 680.0, 26.0, 26.23448265163452, 0.5548457038078833, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.3795013850415513, 0.655, 0.33666666666666667, 0.7513812154696132, 0.6666666666666666, 0.6862068876362099, 0.6849485679359612, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.31588537], dtype=float32), -0.66296786]. 
=============================================
[2019-04-04 00:46:54,255] A3C_AGENT_WORKER-Thread-17 INFO:Local step 135000, global step 2149954: loss 0.2843
[2019-04-04 00:46:54,256] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 135000, global step 2149955: learning rate 0.0005
[2019-04-04 00:46:54,376] A3C_AGENT_WORKER-Thread-10 INFO:Local step 134500, global step 2150015: loss 0.0232
[2019-04-04 00:46:54,377] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 134500, global step 2150015: learning rate 0.0005
[2019-04-04 00:46:55,075] A3C_AGENT_WORKER-Thread-20 INFO:Local step 135000, global step 2150353: loss 0.2332
[2019-04-04 00:46:55,078] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 135000, global step 2150353: learning rate 0.0005
[2019-04-04 00:46:55,396] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.0650651e-23 1.2303806e-09 1.7539614e-09 9.9999940e-01 6.5675229e-15
 5.7612340e-07 2.0757654e-13], sum to 1.0000
[2019-04-04 00:46:55,399] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1011
[2019-04-04 00:46:55,408] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.4, 80.0, 111.0, 781.5, 24.5, 25.0015778257012, 0.3737591657501801, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.5], 
sim time this is 3236400.0000, 
sim time next is 3237000.0000, 
raw observation next is [-2.333333333333333, 78.5, 111.6666666666667, 791.3333333333334, 24.5, 25.03417443531684, 0.3819988786108397, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.3979686057248385, 0.785, 0.37222222222222234, 0.874401473296501, 0.5416666666666666, 0.5861812029430699, 0.6273329595369466, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4624897], dtype=float32), -0.2767184]. 
=============================================
[2019-04-04 00:46:55,420] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[84.00212 ]
 [84.251785]
 [84.567345]
 [84.84723 ]
 [85.10352 ]], R is [[84.00440216]
 [84.16436005]
 [84.32271576]
 [84.47949219]
 [84.63469696]].
[2019-04-04 00:46:55,601] A3C_AGENT_WORKER-Thread-5 INFO:Local step 133500, global step 2150619: loss 3.2536
[2019-04-04 00:46:55,610] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 133500, global step 2150619: learning rate 0.0005
[2019-04-04 00:46:57,086] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:46:57,086] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:46:57,106] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run17
[2019-04-04 00:46:58,915] A3C_AGENT_WORKER-Thread-12 INFO:Local step 134500, global step 2152157: loss 0.0440
[2019-04-04 00:46:58,916] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 134500, global step 2152158: learning rate 0.0005
[2019-04-04 00:46:59,224] A3C_AGENT_WORKER-Thread-14 INFO:Local step 134000, global step 2152290: loss 0.1541
[2019-04-04 00:46:59,225] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 134000, global step 2152290: learning rate 0.0005
[2019-04-04 00:47:00,750] A3C_AGENT_WORKER-Thread-16 INFO:Local step 135000, global step 2152900: loss 0.2481
[2019-04-04 00:47:00,750] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 135000, global step 2152900: learning rate 0.0005
[2019-04-04 00:47:01,471] A3C_AGENT_WORKER-Thread-4 INFO:Local step 133000, global step 2153200: loss 0.6804
[2019-04-04 00:47:01,473] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 133000, global step 2153200: learning rate 0.0005
[2019-04-04 00:47:01,619] A3C_AGENT_WORKER-Thread-11 INFO:Local step 135000, global step 2153274: loss 0.3048
[2019-04-04 00:47:01,633] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 135000, global step 2153278: learning rate 0.0005
[2019-04-04 00:47:01,690] A3C_AGENT_WORKER-Thread-6 INFO:Local step 134500, global step 2153306: loss 0.0264
[2019-04-04 00:47:01,690] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 134500, global step 2153306: learning rate 0.0005
[2019-04-04 00:47:02,345] A3C_AGENT_WORKER-Thread-19 INFO:Local step 135500, global step 2153604: loss 0.0169
[2019-04-04 00:47:02,347] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 135500, global step 2153604: learning rate 0.0005
[2019-04-04 00:47:05,187] A3C_AGENT_WORKER-Thread-13 INFO:Local step 133500, global step 2154892: loss 4.1927
[2019-04-04 00:47:05,188] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 133500, global step 2154892: learning rate 0.0005
[2019-04-04 00:47:06,993] A3C_AGENT_WORKER-Thread-18 INFO:Local step 135000, global step 2155645: loss 0.1182
[2019-04-04 00:47:07,003] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 135000, global step 2155648: learning rate 0.0005
[2019-04-04 00:47:07,273] A3C_AGENT_WORKER-Thread-3 INFO:Local step 135000, global step 2155792: loss 0.2506
[2019-04-04 00:47:07,275] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 135000, global step 2155792: learning rate 0.0005
[2019-04-04 00:47:09,318] A3C_AGENT_WORKER-Thread-15 INFO:Local step 134000, global step 2156808: loss 0.0294
[2019-04-04 00:47:09,321] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 134000, global step 2156809: learning rate 0.0005
[2019-04-04 00:47:09,777] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.0494014e-26 7.9974981e-14 1.4319205e-11 1.0000000e+00 6.5540202e-20
 2.9685563e-11 1.0471758e-15], sum to 1.0000
[2019-04-04 00:47:09,779] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1023
[2019-04-04 00:47:09,806] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.2, 65.0, 0.0, 0.0, 25.5, 24.96514064359981, 0.3283413638464136, 0.0, 1.0, 24370.8214803227], 
current ob forecast is [], 
actual action is [25.5], 
sim time this is 4586400.0000, 
sim time next is 4587000.0000, 
raw observation next is [-0.35, 65.33333333333334, 0.0, 0.0, 25.5, 24.99225760887205, 0.3228183484507369, 0.0, 1.0, 18752.78532838444], 
processed observation next is [1.0, 0.08695652173913043, 0.45290858725761773, 0.6533333333333334, 0.0, 0.0, 0.625, 0.5826881340726707, 0.6076061161502456, 0.0, 1.0, 0.08929897775421163], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.17273964], dtype=float32), 0.5815001]. 
=============================================
[2019-04-04 00:47:09,820] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[81.623566]
 [81.701805]
 [81.764755]
 [81.786255]
 [81.79917 ]], R is [[81.64228821]
 [81.70981598]
 [81.80340576]
 [81.74719238]
 [81.79664612]].
[2019-04-04 00:47:10,822] A3C_AGENT_WORKER-Thread-10 INFO:Local step 135000, global step 2157420: loss 0.0684
[2019-04-04 00:47:10,825] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 135000, global step 2157420: learning rate 0.0005
[2019-04-04 00:47:12,025] A3C_AGENT_WORKER-Thread-17 INFO:Local step 135500, global step 2157905: loss 0.0101
[2019-04-04 00:47:12,029] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 135500, global step 2157906: learning rate 0.0005
[2019-04-04 00:47:12,159] A3C_AGENT_WORKER-Thread-5 INFO:Local step 134000, global step 2157966: loss 0.0565
[2019-04-04 00:47:12,170] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 134000, global step 2157971: learning rate 0.0005
[2019-04-04 00:47:13,781] A3C_AGENT_WORKER-Thread-20 INFO:Local step 135500, global step 2158712: loss 0.0065
[2019-04-04 00:47:13,782] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 135500, global step 2158712: learning rate 0.0005
[2019-04-04 00:47:15,873] A3C_AGENT_WORKER-Thread-12 INFO:Local step 135000, global step 2159570: loss 0.0791
[2019-04-04 00:47:15,887] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 135000, global step 2159570: learning rate 0.0005
[2019-04-04 00:47:18,233] A3C_AGENT_WORKER-Thread-6 INFO:Local step 135000, global step 2160589: loss 0.0866
[2019-04-04 00:47:18,235] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 135000, global step 2160591: learning rate 0.0005
[2019-04-04 00:47:18,328] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:47:18,328] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:47:18,332] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run17
[2019-04-04 00:47:18,451] A3C_AGENT_WORKER-Thread-14 INFO:Local step 134500, global step 2160680: loss 0.1178
[2019-04-04 00:47:18,456] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 134500, global step 2160680: learning rate 0.0005
[2019-04-04 00:47:18,775] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.1601877e-30 2.3466897e-20 6.6432377e-17 1.0000000e+00 1.1483136e-22
 3.7108195e-11 4.0015961e-20], sum to 1.0000
[2019-04-04 00:47:18,778] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1788
[2019-04-04 00:47:18,799] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 46.33333333333334, 0.0, 0.0, 26.0, 25.2267707015685, 0.4167155122616671, 0.0, 1.0, 54983.1924474128], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4823400.0000, 
sim time next is 4824000.0000, 
raw observation next is [1.0, 47.0, 0.0, 0.0, 26.0, 25.37422702790192, 0.4309554999778565, 0.0, 1.0, 43381.98605422236], 
processed observation next is [0.0, 0.8695652173913043, 0.4903047091412743, 0.47, 0.0, 0.0, 0.6666666666666666, 0.6145189189918266, 0.6436518333259521, 0.0, 1.0, 0.20658088597248744], 
reward next is 0.7934, 
noisyNet noise sample is [array([1.2484548], dtype=float32), 0.18657099]. 
=============================================
[2019-04-04 00:47:18,811] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[87.81727 ]
 [87.540924]
 [87.121826]
 [86.43442 ]
 [85.9318  ]], R is [[87.83779907]
 [87.69759369]
 [87.40777588]
 [86.78723907]
 [85.97198486]].
[2019-04-04 00:47:19,152] A3C_AGENT_WORKER-Thread-16 INFO:Local step 135500, global step 2160961: loss 0.0980
[2019-04-04 00:47:19,152] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 135500, global step 2160961: learning rate 0.0005
[2019-04-04 00:47:19,385] A3C_AGENT_WORKER-Thread-11 INFO:Local step 135500, global step 2161056: loss 0.1718
[2019-04-04 00:47:19,386] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 135500, global step 2161056: learning rate 0.0005
[2019-04-04 00:47:19,659] A3C_AGENT_WORKER-Thread-4 INFO:Local step 133500, global step 2161162: loss 4.7051
[2019-04-04 00:47:19,668] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 133500, global step 2161162: learning rate 0.0005
[2019-04-04 00:47:21,433] A3C_AGENT_WORKER-Thread-13 INFO:Local step 134000, global step 2161822: loss 0.2173
[2019-04-04 00:47:21,440] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 134000, global step 2161822: learning rate 0.0005
[2019-04-04 00:47:23,131] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.5931908e-27 1.2196956e-14 2.9796914e-15 1.0000000e+00 2.0440893e-19
 9.2872696e-09 3.8286112e-16], sum to 1.0000
[2019-04-04 00:47:23,133] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7456
[2019-04-04 00:47:23,165] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 40.0, 0.0, 0.0, 26.0, 25.4480615924065, 0.4259466537379533, 0.0, 1.0, 77136.91445928003], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5017200.0000, 
sim time next is 5017800.0000, 
raw observation next is [1.0, 40.0, 0.0, 0.0, 26.0, 25.4168746136633, 0.4317211322384494, 0.0, 1.0, 70674.03427397521], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 0.4, 0.0, 0.0, 0.6666666666666666, 0.6180728844719416, 0.6439070440794831, 0.0, 1.0, 0.3365430203522629], 
reward next is 0.6635, 
noisyNet noise sample is [array([-0.11363719], dtype=float32), 0.7568318]. 
=============================================
[2019-04-04 00:47:25,338] A3C_AGENT_WORKER-Thread-18 INFO:Local step 135500, global step 2163488: loss 0.0382
[2019-04-04 00:47:25,339] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 135500, global step 2163488: learning rate 0.0005
[2019-04-04 00:47:26,530] A3C_AGENT_WORKER-Thread-3 INFO:Local step 135500, global step 2163978: loss 0.0174
[2019-04-04 00:47:26,545] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 135500, global step 2163981: learning rate 0.0005
[2019-04-04 00:47:27,485] A3C_AGENT_WORKER-Thread-15 INFO:Local step 134500, global step 2164412: loss 0.1684
[2019-04-04 00:47:27,486] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 134500, global step 2164412: learning rate 0.0005
[2019-04-04 00:47:28,230] A3C_AGENT_WORKER-Thread-10 INFO:Local step 135500, global step 2164719: loss 0.0112
[2019-04-04 00:47:28,231] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 135500, global step 2164719: learning rate 0.0005
[2019-04-04 00:47:28,668] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:47:28,669] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:47:28,674] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run17
[2019-04-04 00:47:29,309] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.0449761e-23 7.5118037e-11 6.9665738e-08 9.9648881e-01 1.5873182e-14
 3.5110959e-03 9.7958166e-15], sum to 1.0000
[2019-04-04 00:47:29,310] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6797
[2019-04-04 00:47:29,325] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 40.0, 0.0, 0.0, 26.0, 25.57142408372217, 0.4439165639081971, 0.0, 1.0, 27150.10053849979], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5015400.0000, 
sim time next is 5016000.0000, 
raw observation next is [1.0, 40.0, 0.0, 0.0, 26.0, 25.55237983404711, 0.437279270380446, 0.0, 1.0, 40200.76012415036], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 0.4, 0.0, 0.0, 0.6666666666666666, 0.6293649861705924, 0.645759756793482, 0.0, 1.0, 0.19143219106738266], 
reward next is 0.8086, 
noisyNet noise sample is [array([2.4694068], dtype=float32), -0.005151131]. 
=============================================
[2019-04-04 00:47:29,330] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[79.18457 ]
 [79.22934 ]
 [79.46923 ]
 [79.779205]
 [80.04161 ]], R is [[79.24255371]
 [79.32083893]
 [79.52763367]
 [79.73236084]
 [79.84576416]].
[2019-04-04 00:47:29,671] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:47:29,671] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:47:29,680] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run17
[2019-04-04 00:47:31,235] A3C_AGENT_WORKER-Thread-5 INFO:Local step 134500, global step 2165777: loss 0.0049
[2019-04-04 00:47:31,238] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 134500, global step 2165778: learning rate 0.0005
[2019-04-04 00:47:33,582] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.1161416e-25 3.2444479e-14 1.4819591e-13 1.0000000e+00 8.2782780e-17
 1.4840999e-10 4.8631179e-16], sum to 1.0000
[2019-04-04 00:47:33,588] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9316
[2019-04-04 00:47:33,626] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 36.0, 84.5, 578.6666666666666, 26.0, 25.18052814695407, 0.426535866080735, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4810800.0000, 
sim time next is 4811400.0000, 
raw observation next is [3.0, 35.5, 82.0, 549.0, 26.0, 25.18355740274244, 0.4224257665344246, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.5457063711911359, 0.355, 0.2733333333333333, 0.6066298342541436, 0.6666666666666666, 0.59862978356187, 0.6408085888448082, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.091864], dtype=float32), 1.2909981]. 
=============================================
[2019-04-04 00:47:34,966] A3C_AGENT_WORKER-Thread-12 INFO:Local step 135500, global step 2167152: loss 0.0195
[2019-04-04 00:47:34,967] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 135500, global step 2167152: learning rate 0.0005
[2019-04-04 00:47:35,181] A3C_AGENT_WORKER-Thread-14 INFO:Local step 135000, global step 2167244: loss 0.0178
[2019-04-04 00:47:35,183] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 135000, global step 2167244: learning rate 0.0005
[2019-04-04 00:47:35,617] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:47:35,617] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:47:35,625] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run17
[2019-04-04 00:47:35,681] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:47:35,681] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:47:35,684] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run17
[2019-04-04 00:47:36,549] A3C_AGENT_WORKER-Thread-6 INFO:Local step 135500, global step 2167714: loss 0.0645
[2019-04-04 00:47:36,550] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 135500, global step 2167714: learning rate 0.0005
[2019-04-04 00:47:36,865] A3C_AGENT_WORKER-Thread-4 INFO:Local step 134000, global step 2167826: loss 0.0781
[2019-04-04 00:47:36,868] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 134000, global step 2167827: learning rate 0.0005
[2019-04-04 00:47:41,824] A3C_AGENT_WORKER-Thread-13 INFO:Local step 134500, global step 2169425: loss 0.0440
[2019-04-04 00:47:41,828] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 134500, global step 2169426: learning rate 0.0005
[2019-04-04 00:47:42,769] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:47:42,769] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:47:42,771] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run17
[2019-04-04 00:47:43,529] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:47:43,529] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:47:43,533] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run17
[2019-04-04 00:47:45,469] A3C_AGENT_WORKER-Thread-15 INFO:Local step 135000, global step 2170688: loss 0.0846
[2019-04-04 00:47:45,472] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 135000, global step 2170688: learning rate 0.0005
[2019-04-04 00:47:45,781] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:47:45,781] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:47:45,785] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run17
[2019-04-04 00:47:49,415] A3C_AGENT_WORKER-Thread-5 INFO:Local step 135000, global step 2171808: loss 0.0581
[2019-04-04 00:47:49,415] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 135000, global step 2171808: learning rate 0.0005
[2019-04-04 00:47:52,276] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:47:52,276] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:47:52,279] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run17
[2019-04-04 00:47:52,338] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.0783348e-23 1.0291529e-15 1.5096742e-11 9.9999988e-01 1.7519692e-13
 1.2100418e-07 5.6499537e-15], sum to 1.0000
[2019-04-04 00:47:52,338] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3786
[2019-04-04 00:47:52,373] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.333333333333333, 51.33333333333334, 0.0, 0.0, 26.0, 25.07792249075656, 0.3218529918886317, 0.0, 1.0, 39465.55509200083], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4162800.0000, 
sim time next is 4163400.0000, 
raw observation next is [-3.5, 52.0, 0.0, 0.0, 26.0, 25.038535044446, 0.3135407557079548, 0.0, 1.0, 39468.73972642425], 
processed observation next is [0.0, 0.17391304347826086, 0.36565096952908593, 0.52, 0.0, 0.0, 0.6666666666666666, 0.5865445870371667, 0.604513585235985, 0.0, 1.0, 0.1879463796496393], 
reward next is 0.8121, 
noisyNet noise sample is [array([1.6585137], dtype=float32), -1.5620615]. 
=============================================
[2019-04-04 00:47:54,216] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:47:54,216] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:47:54,224] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run17
[2019-04-04 00:47:55,156] A3C_AGENT_WORKER-Thread-14 INFO:Local step 135500, global step 2173573: loss 0.0784
[2019-04-04 00:47:55,161] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 135500, global step 2173574: learning rate 0.0005
[2019-04-04 00:47:57,452] A3C_AGENT_WORKER-Thread-4 INFO:Local step 134500, global step 2174307: loss 0.0046
[2019-04-04 00:47:57,456] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 134500, global step 2174309: learning rate 0.0005
[2019-04-04 00:48:00,577] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.9683998e-21 3.5982259e-11 2.4385939e-08 9.9999380e-01 5.4854545e-12
 6.2035465e-06 2.5989336e-10], sum to 1.0000
[2019-04-04 00:48:00,579] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9433
[2019-04-04 00:48:00,587] A3C_AGENT_WORKER-Thread-13 INFO:Local step 135000, global step 2175197: loss 0.0192
[2019-04-04 00:48:00,588] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 135000, global step 2175197: learning rate 0.0005
[2019-04-04 00:48:00,651] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-12.1, 65.66666666666667, 84.16666666666667, 383.5, 26.0, 25.60368705321486, 0.3061026438331344, 1.0, 1.0, 31515.0096188519], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 292800.0000, 
sim time next is 293400.0000, 
raw observation next is [-12.0, 65.0, 95.0, 383.0, 26.0, 25.65336074839954, 0.3190588560416536, 1.0, 1.0, 41533.57333657467], 
processed observation next is [1.0, 0.391304347826087, 0.13019390581717452, 0.65, 0.31666666666666665, 0.42320441988950275, 0.6666666666666666, 0.6377800623666282, 0.6063529520138845, 1.0, 1.0, 0.19777892065035557], 
reward next is 0.8022, 
noisyNet noise sample is [array([0.00141021], dtype=float32), -0.13002802]. 
=============================================
[2019-04-04 00:48:04,367] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.4381179e-22 5.2745801e-08 1.2275281e-07 9.9999464e-01 3.4438299e-11
 5.2686532e-06 1.0281974e-12], sum to 1.0000
[2019-04-04 00:48:04,367] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6731
[2019-04-04 00:48:04,394] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.333333333333333, 29.33333333333334, 117.8333333333333, 810.0, 26.0, 26.62167368248985, 0.598060464280007, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4963200.0000, 
sim time next is 4963800.0000, 
raw observation next is [2.666666666666667, 29.16666666666667, 118.6666666666667, 817.0, 26.0, 26.6870527537916, 0.6029303173073478, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.5364727608494922, 0.29166666666666674, 0.39555555555555566, 0.9027624309392265, 0.6666666666666666, 0.7239210628159668, 0.7009767724357826, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1847756], dtype=float32), -0.07180701]. 
=============================================
[2019-04-04 00:48:05,761] A3C_AGENT_WORKER-Thread-15 INFO:Local step 135500, global step 2176804: loss 0.1093
[2019-04-04 00:48:05,763] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 135500, global step 2176804: learning rate 0.0005
[2019-04-04 00:48:09,038] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.6380340e-21 2.7988590e-13 1.0488189e-07 9.9995399e-01 1.4132948e-12
 4.5861369e-05 3.6019552e-11], sum to 1.0000
[2019-04-04 00:48:09,040] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8565
[2019-04-04 00:48:09,083] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-15.6, 73.0, 0.0, 0.0, 26.0, 22.2136456498226, -0.3668071119520696, 0.0, 1.0, 49408.90426086033], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 361200.0000, 
sim time next is 361800.0000, 
raw observation next is [-15.6, 73.0, 0.0, 0.0, 26.0, 22.13599414652255, -0.3703473520948763, 0.0, 1.0, 49354.97190163555], 
processed observation next is [1.0, 0.17391304347826086, 0.030470914127423816, 0.73, 0.0, 0.0, 0.6666666666666666, 0.34466617887687906, 0.3765508826350412, 0.0, 1.0, 0.23502367572207405], 
reward next is 0.7650, 
noisyNet noise sample is [array([-0.27482942], dtype=float32), -0.15017378]. 
=============================================
[2019-04-04 00:48:09,326] A3C_AGENT_WORKER-Thread-5 INFO:Local step 135500, global step 2178087: loss 0.0319
[2019-04-04 00:48:09,327] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 135500, global step 2178087: learning rate 0.0005
[2019-04-04 00:48:10,283] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.0743333e-24 2.0564394e-11 1.5649899e-12 8.1688734e-10 2.0046853e-13
 1.0000000e+00 4.4232767e-16], sum to 1.0000
[2019-04-04 00:48:10,283] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6838
[2019-04-04 00:48:10,302] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 86.0, 178.3333333333333, 48.66666666666666, 26.0, 26.41791089354276, 0.6261037835954445, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4446600.0000, 
sim time next is 4447200.0000, 
raw observation next is [1.0, 86.0, 160.1666666666667, 24.33333333333333, 26.0, 26.43861888688298, 0.6190047411335149, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4903047091412743, 0.86, 0.5338888888888891, 0.026887661141804783, 0.6666666666666666, 0.7032182405735817, 0.7063349137111716, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.09028682], dtype=float32), 1.0722622]. 
=============================================
[2019-04-04 00:48:13,289] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:48:13,299] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:48:13,301] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run17
[2019-04-04 00:48:16,325] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.9061745e-22 1.0049512e-13 1.7275865e-09 9.9999988e-01 1.1833514e-14
 7.6663895e-08 1.1281818e-11], sum to 1.0000
[2019-04-04 00:48:16,325] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1477
[2019-04-04 00:48:16,354] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.066666666666666, 92.33333333333333, 20.66666666666666, 69.83333333333331, 26.0, 23.81821082074467, 0.0910224678358283, 0.0, 1.0, 41900.6950655554], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4779600.0000, 
sim time next is 4780200.0000, 
raw observation next is [-6.033333333333333, 92.16666666666667, 41.33333333333332, 139.6666666666666, 26.0, 23.79812785420696, 0.09708652829320298, 0.0, 1.0, 41798.26239300558], 
processed observation next is [0.0, 0.30434782608695654, 0.29547553093259465, 0.9216666666666667, 0.13777777777777775, 0.1543278084714548, 0.6666666666666666, 0.4831773211839134, 0.5323621760977343, 0.0, 1.0, 0.199039344728598], 
reward next is 0.8010, 
noisyNet noise sample is [array([-0.98528963], dtype=float32), 1.2435014]. 
=============================================
[2019-04-04 00:48:16,862] A3C_AGENT_WORKER-Thread-4 INFO:Local step 135000, global step 2180356: loss 0.0690
[2019-04-04 00:48:16,863] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 135000, global step 2180356: learning rate 0.0005
[2019-04-04 00:48:21,142] A3C_AGENT_WORKER-Thread-13 INFO:Local step 135500, global step 2181532: loss 0.0608
[2019-04-04 00:48:21,143] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 135500, global step 2181532: learning rate 0.0005
[2019-04-04 00:48:22,024] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.3106681e-25 3.5671317e-17 1.2228514e-11 1.0000000e+00 1.0729528e-17
 1.4966935e-13 3.4949911e-16], sum to 1.0000
[2019-04-04 00:48:22,024] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0152
[2019-04-04 00:48:22,082] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.9, 70.0, 96.33333333333334, 34.00000000000001, 26.0, 25.23970231981072, 0.2571368497019855, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 637800.0000, 
sim time next is 638400.0000, 
raw observation next is [-3.9, 69.0, 115.6666666666667, 42.5, 26.0, 25.23250275655518, 0.2511601084670555, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.3545706371191136, 0.69, 0.38555555555555565, 0.04696132596685083, 0.6666666666666666, 0.6027085630462651, 0.5837200361556851, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.7216332], dtype=float32), -1.1900604]. 
=============================================
[2019-04-04 00:48:23,398] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:48:23,398] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:48:23,402] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run17
[2019-04-04 00:48:27,860] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:48:27,860] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:48:27,863] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run17
[2019-04-04 00:48:32,095] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.26497906e-26 2.19776389e-12 2.48313713e-11 2.12630853e-02
 1.37654765e-17 9.78736877e-01 1.96116312e-15], sum to 1.0000
[2019-04-04 00:48:32,120] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7655
[2019-04-04 00:48:32,129] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 26.0, 26.15009032616587, 0.6442693152650081, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1039200.0000, 
sim time next is 1039800.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 26.0, 26.1217317280713, 0.6385479318201348, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.8614958448753465, 0.75, 0.0, 0.0, 0.6666666666666666, 0.676810977339275, 0.7128493106067116, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.37168294], dtype=float32), 0.22681338]. 
=============================================
[2019-04-04 00:48:33,354] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.6425876e-21 1.4165700e-13 1.4870495e-09 9.0396196e-01 4.2736867e-13
 9.6038073e-02 5.5762170e-12], sum to 1.0000
[2019-04-04 00:48:33,363] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1227
[2019-04-04 00:48:33,417] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-15.0, 69.0, 0.0, 0.0, 26.0, 22.79227548002491, -0.2504573007340115, 0.0, 1.0, 49236.30754481525], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 356400.0000, 
sim time next is 357000.0000, 
raw observation next is [-15.1, 69.66666666666667, 0.0, 0.0, 26.0, 22.71984467153439, -0.2592939679955462, 0.0, 1.0, 49273.69848364575], 
processed observation next is [1.0, 0.13043478260869565, 0.0443213296398892, 0.6966666666666668, 0.0, 0.0, 0.6666666666666666, 0.39332038929453245, 0.41356867733481795, 0.0, 1.0, 0.23463665944593215], 
reward next is 0.7654, 
noisyNet noise sample is [array([-0.65778184], dtype=float32), -0.8560521]. 
=============================================
[2019-04-04 00:48:33,491] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[79.37737]
 [79.15171]
 [78.92979]
 [78.77257]
 [78.62992]], R is [[79.54154205]
 [79.51166534]
 [79.48247528]
 [79.45405579]
 [79.42642975]].
[2019-04-04 00:48:34,811] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.6060443e-22 1.0271415e-14 7.6326501e-10 1.0000000e+00 4.7546179e-15
 9.4976460e-10 3.2971268e-13], sum to 1.0000
[2019-04-04 00:48:34,816] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7401
[2019-04-04 00:48:34,843] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-10.9, 50.5, 0.0, 0.0, 26.0, 23.12673114875191, -0.1947119448261162, 0.0, 1.0, 46075.73713205785], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 444600.0000, 
sim time next is 445200.0000, 
raw observation next is [-11.0, 51.0, 0.0, 0.0, 26.0, 23.03649557454714, -0.2065462860352267, 0.0, 1.0, 46186.05855328736], 
processed observation next is [1.0, 0.13043478260869565, 0.15789473684210528, 0.51, 0.0, 0.0, 0.6666666666666666, 0.41970796454559495, 0.4311512379882578, 0.0, 1.0, 0.21993361215851123], 
reward next is 0.7801, 
noisyNet noise sample is [array([-0.75034356], dtype=float32), -1.0492276]. 
=============================================
[2019-04-04 00:48:38,098] A3C_AGENT_WORKER-Thread-4 INFO:Local step 135500, global step 2186481: loss 0.1635
[2019-04-04 00:48:38,099] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 135500, global step 2186481: learning rate 0.0005
[2019-04-04 00:48:39,345] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.9029506e-21 6.2659547e-14 4.4012735e-10 9.9999976e-01 2.0961965e-13
 1.9233070e-07 4.6071797e-12], sum to 1.0000
[2019-04-04 00:48:39,345] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6613
[2019-04-04 00:48:39,426] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-9.2, 38.0, 0.0, 0.0, 26.0, 25.45151757485354, 0.2725311653466223, 1.0, 1.0, 192827.1645230725], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 408600.0000, 
sim time next is 409200.0000, 
raw observation next is [-9.3, 38.66666666666666, 0.0, 0.0, 26.0, 24.88261956037384, 0.3013723546361165, 1.0, 1.0, 199524.2947295409], 
processed observation next is [1.0, 0.7391304347826086, 0.20498614958448752, 0.38666666666666655, 0.0, 0.0, 0.6666666666666666, 0.5735516300311533, 0.6004574515453721, 1.0, 1.0, 0.95011568918829], 
reward next is 0.0499, 
noisyNet noise sample is [array([-0.47233167], dtype=float32), 1.1125523]. 
=============================================
[2019-04-04 00:48:40,245] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:48:40,245] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:48:40,248] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run17
[2019-04-04 00:48:40,623] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.34826572e-25 8.59315057e-17 2.19967585e-13 9.99999881e-01
 1.13294396e-17 1.03368770e-07 1.43024954e-15], sum to 1.0000
[2019-04-04 00:48:40,623] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3169
[2019-04-04 00:48:40,674] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-11.1, 51.5, 0.0, 0.0, 26.0, 22.9720452163597, -0.2261826763573075, 0.0, 1.0, 46296.97713308496], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 445800.0000, 
sim time next is 446400.0000, 
raw observation next is [-11.2, 52.0, 0.0, 0.0, 26.0, 22.89047154384539, -0.2423697250005558, 0.0, 1.0, 46411.00150381069], 
processed observation next is [1.0, 0.17391304347826086, 0.15235457063711913, 0.52, 0.0, 0.0, 0.6666666666666666, 0.40753929532044914, 0.4192100916664814, 0.0, 1.0, 0.22100476906576516], 
reward next is 0.7790, 
noisyNet noise sample is [array([-1.0093197], dtype=float32), 0.5225128]. 
=============================================
[2019-04-04 00:48:44,485] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.2991723e-21 1.0766970e-08 6.1132637e-06 9.9991798e-01 9.2180539e-09
 7.5901182e-05 1.8117440e-11], sum to 1.0000
[2019-04-04 00:48:44,486] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9632
[2019-04-04 00:48:44,505] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [13.8, 100.0, 45.66666666666666, 0.0, 26.0, 24.67463927016391, 0.4288394770317341, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1266000.0000, 
sim time next is 1266600.0000, 
raw observation next is [13.8, 100.0, 40.33333333333334, 0.0, 26.0, 24.65851175827936, 0.4266285067958504, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.844875346260388, 1.0, 0.13444444444444448, 0.0, 0.6666666666666666, 0.5548759798566133, 0.6422095022652835, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.10436982], dtype=float32), -0.4488304]. 
=============================================
[2019-04-04 00:48:50,930] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.2321904e-23 6.6017091e-16 1.6513471e-10 1.0000000e+00 5.1037051e-15
 4.0368629e-11 1.1856922e-14], sum to 1.0000
[2019-04-04 00:48:50,938] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4055
[2019-04-04 00:48:50,955] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 75.0, 0.0, 0.0, 26.0, 24.04153277663004, 0.05030083859711611, 0.0, 1.0, 43956.93443759321], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 619200.0000, 
sim time next is 619800.0000, 
raw observation next is [-4.5, 73.83333333333333, 0.0, 0.0, 26.0, 23.99214639224208, 0.04176967813179629, 0.0, 1.0, 44149.83632966952], 
processed observation next is [0.0, 0.17391304347826086, 0.3379501385041552, 0.7383333333333333, 0.0, 0.0, 0.6666666666666666, 0.49934553268684007, 0.513923226043932, 0.0, 1.0, 0.21023731585556912], 
reward next is 0.7898, 
noisyNet noise sample is [array([-2.0870044], dtype=float32), 0.13197456]. 
=============================================
[2019-04-04 00:48:55,306] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.5392687e-22 7.0753890e-13 7.3226025e-10 9.9910611e-01 9.5972518e-15
 8.9382019e-04 1.7007060e-13], sum to 1.0000
[2019-04-04 00:48:55,306] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3296
[2019-04-04 00:48:55,358] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.6, 54.0, 64.33333333333334, 30.33333333333334, 26.0, 24.87715807167872, 0.2206365408365444, 0.0, 1.0, 45828.59347067726], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 661800.0000, 
sim time next is 662400.0000, 
raw observation next is [-0.6, 54.0, 55.0, 26.5, 26.0, 24.87783272878885, 0.2228826561111545, 0.0, 1.0, 42052.8577720455], 
processed observation next is [0.0, 0.6956521739130435, 0.44598337950138506, 0.54, 0.18333333333333332, 0.029281767955801105, 0.6666666666666666, 0.5731527273990707, 0.5742942187037182, 0.0, 1.0, 0.20025170367640713], 
reward next is 0.7997, 
noisyNet noise sample is [array([-0.260674], dtype=float32), 0.2706102]. 
=============================================
[2019-04-04 00:48:56,810] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:48:56,810] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:48:56,814] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run17
[2019-04-04 00:49:01,118] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.28218987e-22 8.08744183e-11 2.64494433e-11 9.61484611e-01
 1.08132836e-13 3.85153927e-02 1.21180106e-11], sum to 1.0000
[2019-04-04 00:49:01,118] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6457
[2019-04-04 00:49:01,140] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.2, 76.0, 0.0, 0.0, 26.0, 24.70210571318511, 0.1995940573651867, 0.0, 1.0, 39049.19395474908], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 878400.0000, 
sim time next is 879000.0000, 
raw observation next is [-1.1, 75.33333333333334, 0.0, 0.0, 26.0, 24.77348095951842, 0.1951855006168226, 0.0, 1.0, 38993.43996844014], 
processed observation next is [1.0, 0.17391304347826086, 0.4321329639889197, 0.7533333333333334, 0.0, 0.0, 0.6666666666666666, 0.564456746626535, 0.5650618335389409, 0.0, 1.0, 0.18568304746876257], 
reward next is 0.8143, 
noisyNet noise sample is [array([-0.2888924], dtype=float32), -0.555107]. 
=============================================
[2019-04-04 00:49:01,154] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[84.468376]
 [84.428505]
 [84.3935  ]
 [84.35996 ]
 [84.33472 ]], R is [[84.4488678 ]
 [84.41842651]
 [84.38802338]
 [84.35771942]
 [84.32752228]].
[2019-04-04 00:49:17,983] A3C_AGENT_WORKER-Thread-4 INFO:Evaluating...
[2019-04-04 00:49:17,983] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 00:49:17,984] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:49:17,985] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 00:49:17,986] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:49:17,986] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 00:49:18,002] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:49:18,006] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run23
[2019-04-04 00:49:18,025] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run23
[2019-04-04 00:49:18,025] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run23
[2019-04-04 00:49:32,835] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.5843968], dtype=float32), 0.18380305]
[2019-04-04 00:49:32,835] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-6.616666666666666, 60.0, 62.0, 210.3333333333333, 26.0, 25.2257114935467, 0.2869308131957438, 1.0, 1.0, 0.0]
[2019-04-04 00:49:32,835] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 00:49:32,835] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [3.7405929e-23 2.0387045e-15 2.8495162e-12 1.0000000e+00 2.1190361e-15
 1.2072542e-08 2.6251586e-15], sampled 0.8664678026433721
[2019-04-04 00:51:49,726] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7354.8669 239637953.1352 1604.9215
[2019-04-04 00:52:23,773] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7242.3598 263264444.8073 1558.2911
[2019-04-04 00:52:28,475] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.9733 275735612.4904 1232.5533
[2019-04-04 00:52:29,514] A3C_AGENT_WORKER-Thread-4 INFO:Global step: 2200000, evaluation results [2200000.0, 7242.359786631755, 263264444.80732435, 1558.2910726216553, 7354.866889832542, 239637953.1351703, 1604.9214805558893, 7182.973273855296, 275735612.49038523, 1232.5533206192808]
[2019-04-04 00:52:33,471] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.8780658e-24 7.2897894e-12 8.2197040e-09 1.0000000e+00 2.9051760e-15
 1.0068350e-10 2.6041134e-15], sum to 1.0000
[2019-04-04 00:52:33,471] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2025
[2019-04-04 00:52:33,507] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 26.0, 25.55349342537548, 0.5326126861190765, 0.0, 1.0, 18744.18471438357], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1315800.0000, 
sim time next is 1316400.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 26.0, 25.48199456931002, 0.5365429671520704, 0.0, 1.0, 55773.93730070069], 
processed observation next is [1.0, 0.21739130434782608, 0.5069252077562327, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6234995474425018, 0.6788476557173567, 0.0, 1.0, 0.26559017762238424], 
reward next is 0.7344, 
noisyNet noise sample is [array([-0.9953151], dtype=float32), -0.31811467]. 
=============================================
[2019-04-04 00:52:35,053] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0193308e-24 2.5119096e-10 2.4511157e-10 6.1092007e-01 3.0259473e-12
 3.8907990e-01 4.0216995e-17], sum to 1.0000
[2019-04-04 00:52:35,054] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4828
[2019-04-04 00:52:35,121] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [13.83333333333333, 82.66666666666667, 114.8333333333333, 0.0, 26.0, 26.42197765638981, 0.6687173052200306, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 999600.0000, 
sim time next is 1000200.0000, 
raw observation next is [14.11666666666667, 81.83333333333333, 110.6666666666667, 0.0, 26.0, 26.54770366519919, 0.6813896098128222, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.8536472760849494, 0.8183333333333332, 0.368888888888889, 0.0, 0.6666666666666666, 0.7123086387665992, 0.7271298699376074, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6667264], dtype=float32), 0.056656882]. 
=============================================
[2019-04-04 00:52:49,574] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2607987e-21 1.6379139e-11 2.2769504e-09 9.9972719e-01 4.4965659e-13
 2.7279055e-04 1.3128940e-15], sum to 1.0000
[2019-04-04 00:52:49,576] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5552
[2019-04-04 00:52:49,688] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 89.33333333333334, 0.0, 0.0, 26.0, 24.76302134006368, 0.395740096401934, 1.0, 1.0, 196667.1661404195], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1446000.0000, 
sim time next is 1446600.0000, 
raw observation next is [1.1, 88.66666666666667, 0.0, 0.0, 26.0, 24.70533876764672, 0.4589109921019354, 1.0, 1.0, 192286.768703094], 
processed observation next is [1.0, 0.7391304347826086, 0.49307479224376743, 0.8866666666666667, 0.0, 0.0, 0.6666666666666666, 0.5587782306372265, 0.6529703307006451, 1.0, 1.0, 0.915651279538543], 
reward next is 0.0843, 
noisyNet noise sample is [array([0.24121952], dtype=float32), -1.9730124]. 
=============================================
[2019-04-04 00:52:50,160] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2562302e-22 9.4452322e-15 3.9917902e-10 1.0000000e+00 9.6303007e-17
 2.2103253e-09 3.7216070e-15], sum to 1.0000
[2019-04-04 00:52:50,161] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5932
[2019-04-04 00:52:50,241] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.7, 67.0, 0.0, 0.0, 26.0, 23.46783717207456, -0.08347931424113175, 0.0, 1.0, 42028.37084340598], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 802800.0000, 
sim time next is 803400.0000, 
raw observation next is [-6.700000000000001, 68.33333333333334, 0.0, 0.0, 26.0, 23.44899548622577, -0.0870857850243562, 0.0, 1.0, 42001.75589588666], 
processed observation next is [1.0, 0.30434782608695654, 0.2770083102493075, 0.6833333333333335, 0.0, 0.0, 0.6666666666666666, 0.45408295718548075, 0.4709714049918812, 0.0, 1.0, 0.2000083614089841], 
reward next is 0.8000, 
noisyNet noise sample is [array([0.83465135], dtype=float32), 1.8057544]. 
=============================================
[2019-04-04 00:53:08,762] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.7129511e-26 3.9980643e-16 5.4911726e-12 1.0000000e+00 1.1977406e-19
 6.2303211e-11 3.0817534e-17], sum to 1.0000
[2019-04-04 00:53:08,763] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8528
[2019-04-04 00:53:08,847] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.4521717865064, 0.4728225685131693, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1409400.0000, 
sim time next is 1410000.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.56933423944265, 0.4633458153665186, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.44598337950138506, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6307778532868875, 0.6544486051221728, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.11553089], dtype=float32), -0.37020823]. 
=============================================
[2019-04-04 00:53:08,850] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[89.35905]
 [89.78966]
 [87.84609]
 [87.85689]
 [87.85154]], R is [[90.86235809]
 [90.95373535]
 [91.04419708]
 [90.94978333]
 [90.85632324]].
[2019-04-04 00:53:09,806] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.6990601e-25 5.1112839e-15 4.1893003e-14 1.0000000e+00 3.6629165e-15
 6.9265210e-12 3.9426321e-16], sum to 1.0000
[2019-04-04 00:53:09,808] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2785
[2019-04-04 00:53:09,868] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 88.0, 93.33333333333333, 0.0, 26.0, 26.19253724932379, 0.6223953189185744, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1691400.0000, 
sim time next is 1692000.0000, 
raw observation next is [1.1, 88.0, 90.0, 0.0, 26.0, 26.27104721177096, 0.6224104924028043, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.49307479224376743, 0.88, 0.3, 0.0, 0.6666666666666666, 0.6892539343142466, 0.7074701641342681, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.22210109], dtype=float32), 0.21733917]. 
=============================================
[2019-04-04 00:53:09,882] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[89.319016]
 [89.61097 ]
 [89.831   ]
 [90.04417 ]
 [90.04939 ]], R is [[89.19792175]
 [89.30594635]
 [89.41288757]
 [89.51876068]
 [89.6235733 ]].
[2019-04-04 00:53:11,055] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.0099354e-22 4.2137758e-16 6.2012205e-12 9.9993265e-01 2.0685363e-17
 6.7388872e-05 6.4761225e-14], sum to 1.0000
[2019-04-04 00:53:11,055] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5552
[2019-04-04 00:53:11,138] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.5, 78.16666666666666, 0.0, 0.0, 26.0, 24.37725955057537, 0.1595965917916347, 0.0, 1.0, 45854.25237134483], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1817400.0000, 
sim time next is 1818000.0000, 
raw observation next is [-5.6, 78.0, 0.0, 0.0, 26.0, 24.3601497330822, 0.152808618397502, 0.0, 1.0, 45927.67758069971], 
processed observation next is [0.0, 0.043478260869565216, 0.30747922437673136, 0.78, 0.0, 0.0, 0.6666666666666666, 0.5300124777568499, 0.5509362061325006, 0.0, 1.0, 0.21870322657476052], 
reward next is 0.7813, 
noisyNet noise sample is [array([-0.07896112], dtype=float32), 1.2399433]. 
=============================================
[2019-04-04 00:53:11,218] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[73.640396]
 [73.75407 ]
 [73.85021 ]
 [73.928314]
 [73.87959 ]], R is [[73.58210754]
 [73.62793732]
 [73.67365265]
 [73.71924591]
 [73.76468658]].
[2019-04-04 00:53:14,119] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.1951773e-22 3.6955345e-11 2.7296787e-09 1.0000000e+00 2.3233442e-12
 7.2807125e-09 6.8965890e-14], sum to 1.0000
[2019-04-04 00:53:14,119] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9484
[2019-04-04 00:53:14,181] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 88.0, 66.5, 0.0, 26.0, 25.47219553885174, 0.5261216293007269, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1695600.0000, 
sim time next is 1696200.0000, 
raw observation next is [1.183333333333333, 86.83333333333334, 62.0, 0.0, 26.0, 25.78788085745114, 0.5504790195760081, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.49538319482917825, 0.8683333333333334, 0.20666666666666667, 0.0, 0.6666666666666666, 0.6489900714542616, 0.6834930065253361, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9835059], dtype=float32), 0.58578616]. 
=============================================
[2019-04-04 00:53:23,233] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.1887560e-22 1.4958033e-09 4.6170756e-09 9.9800688e-01 1.6157157e-08
 1.9931013e-03 5.7263157e-13], sum to 1.0000
[2019-04-04 00:53:23,236] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2406
[2019-04-04 00:53:23,244] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [9.4, 61.0, 0.0, 0.0, 26.0, 26.06674359677837, 0.6380866570748062, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1537200.0000, 
sim time next is 1537800.0000, 
raw observation next is [9.033333333333333, 63.0, 0.0, 0.0, 26.0, 25.95150095551052, 0.6253362283057887, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.7128347183748848, 0.63, 0.0, 0.0, 0.6666666666666666, 0.6626250796258768, 0.7084454094352629, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2221464], dtype=float32), 0.43781027]. 
=============================================
[2019-04-04 00:53:24,294] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.5535781e-22 4.3274234e-12 1.1243141e-08 8.2389587e-01 1.2910528e-15
 1.7610411e-01 2.6096529e-13], sum to 1.0000
[2019-04-04 00:53:24,294] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9970
[2019-04-04 00:53:24,352] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 100.0, 9.0, 0.0, 26.0, 25.46553991691798, 0.4414357817555201, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1497600.0000, 
sim time next is 1498200.0000, 
raw observation next is [1.183333333333333, 100.0, 12.0, 0.0, 26.0, 25.42188118891358, 0.4351236921965113, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.49538319482917825, 1.0, 0.04, 0.0, 0.6666666666666666, 0.6184900990761317, 0.6450412307321705, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6055459], dtype=float32), -1.0048697]. 
=============================================
[2019-04-04 00:53:31,021] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.8030984e-29 6.4292733e-12 1.7123374e-09 9.9999726e-01 1.3440882e-16
 2.7831950e-06 5.8093879e-18], sum to 1.0000
[2019-04-04 00:53:31,031] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6081
[2019-04-04 00:53:31,036] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [18.8, 54.0, 155.5, 0.0, 26.0, 26.56284651954485, 0.8771040540821878, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1087200.0000, 
sim time next is 1087800.0000, 
raw observation next is [18.9, 53.16666666666666, 150.3333333333333, 0.0, 26.0, 27.08501186855667, 0.9199626699782971, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.9861495844875346, 0.5316666666666666, 0.501111111111111, 0.0, 0.6666666666666666, 0.7570843223797225, 0.806654223326099, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6418426], dtype=float32), -0.2753761]. 
=============================================
[2019-04-04 00:53:31,529] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.7385525e-26 7.1757568e-13 1.8096033e-11 7.3159075e-01 2.1064018e-13
 2.6840925e-01 3.9580492e-16], sum to 1.0000
[2019-04-04 00:53:31,529] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4211
[2019-04-04 00:53:31,559] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [10.33333333333333, 92.66666666666667, 66.0, 0.0, 26.0, 26.42678555817263, 0.6122448966140762, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 985200.0000, 
sim time next is 985800.0000, 
raw observation next is [10.41666666666667, 92.83333333333333, 72.0, 0.0, 26.0, 26.4441512667448, 0.6190132156630398, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.7511542012927056, 0.9283333333333332, 0.24, 0.0, 0.6666666666666666, 0.7036792722287334, 0.7063377385543466, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.19698739], dtype=float32), 0.75865453]. 
=============================================
[2019-04-04 00:53:47,775] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.4014549e-19 7.1812032e-09 3.0670631e-06 9.9997687e-01 1.1827485e-07
 1.9916037e-05 1.9267563e-11], sum to 1.0000
[2019-04-04 00:53:47,779] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7958
[2019-04-04 00:53:47,798] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.8, 62.0, 93.0, 0.0, 26.0, 25.70749824427405, 0.3171405877555445, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1954800.0000, 
sim time next is 1955400.0000, 
raw observation next is [-2.8, 62.0, 86.66666666666666, 0.0, 26.0, 25.6252337616927, 0.3167742454476157, 1.0, 1.0, 77679.6988074688], 
processed observation next is [1.0, 0.6521739130434783, 0.38504155124653744, 0.62, 0.28888888888888886, 0.0, 0.6666666666666666, 0.6354361468077251, 0.6055914151492052, 1.0, 1.0, 0.3699033276546133], 
reward next is 0.6301, 
noisyNet noise sample is [array([-1.2711293], dtype=float32), -0.6212533]. 
=============================================
[2019-04-04 00:53:55,349] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.2525888e-24 1.4593605e-15 3.3494502e-10 1.0000000e+00 4.6451128e-16
 7.7121290e-13 1.3752224e-14], sum to 1.0000
[2019-04-04 00:53:55,349] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8553
[2019-04-04 00:53:55,449] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.1, 87.0, 66.0, 0.0, 26.0, 25.01621641577987, 0.3453893092684663, 0.0, 1.0, 34213.97359695251], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1783800.0000, 
sim time next is 1784400.0000, 
raw observation next is [-3.2, 87.0, 59.66666666666666, 0.0, 26.0, 25.04321569076254, 0.339432681804232, 0.0, 1.0, 26058.54554191305], 
processed observation next is [0.0, 0.6521739130434783, 0.37396121883656513, 0.87, 0.19888888888888887, 0.0, 0.6666666666666666, 0.5869346408968784, 0.6131442272680773, 0.0, 1.0, 0.12408831210434786], 
reward next is 0.8759, 
noisyNet noise sample is [array([-0.5237356], dtype=float32), -1.307144]. 
=============================================
[2019-04-04 00:54:15,662] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.8877019e-19 2.2665793e-10 3.6525000e-07 9.9706465e-01 1.0911085e-11
 2.9350065e-03 1.8858499e-10], sum to 1.0000
[2019-04-04 00:54:15,663] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6617
[2019-04-04 00:54:15,716] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.9, 86.0, 0.0, 0.0, 26.0, 25.13294525314291, 0.4035302184003691, 0.0, 1.0, 114457.4275307651], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2060400.0000, 
sim time next is 2061000.0000, 
raw observation next is [-3.9, 86.0, 0.0, 0.0, 26.0, 25.21338945775128, 0.419767938018975, 0.0, 1.0, 77953.67519637893], 
processed observation next is [1.0, 0.8695652173913043, 0.3545706371191136, 0.86, 0.0, 0.0, 0.6666666666666666, 0.60111578814594, 0.639922646006325, 0.0, 1.0, 0.37120797712561393], 
reward next is 0.6288, 
noisyNet noise sample is [array([-0.9395264], dtype=float32), -0.78130984]. 
=============================================
[2019-04-04 00:54:15,721] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[80.27385]
 [80.94049]
 [81.20004]
 [81.67556]
 [80.54251]], R is [[80.31778717]
 [79.96957397]
 [79.94417572]
 [79.42045593]
 [78.95013428]].
[2019-04-04 00:54:16,899] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.6606761e-23 8.0758544e-15 1.3144209e-10 1.0000000e+00 4.4801484e-16
 1.8460069e-09 1.9806764e-13], sum to 1.0000
[2019-04-04 00:54:16,899] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1080
[2019-04-04 00:54:16,978] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 59.83333333333334, 0.0, 0.0, 26.0, 25.33915838214396, 0.441130638365584, 0.0, 1.0, 51553.19855912903], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2757000.0000, 
sim time next is 2757600.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 25.34746894536017, 0.4394204882815978, 0.0, 1.0, 48974.70889501705], 
processed observation next is [1.0, 0.9565217391304348, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.6122890787800142, 0.646473496093866, 0.0, 1.0, 0.2332128995000812], 
reward next is 0.7668, 
noisyNet noise sample is [array([-0.5655409], dtype=float32), 0.3877822]. 
=============================================
[2019-04-04 00:54:34,644] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.8755390e-20 3.6648715e-16 8.3272132e-13 1.0000000e+00 2.9963674e-15
 1.4873928e-10 3.7979511e-14], sum to 1.0000
[2019-04-04 00:54:34,650] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8780
[2019-04-04 00:54:34,663] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-9.3, 60.66666666666667, 0.0, 0.0, 26.0, 23.07485645306396, -0.1900064544038937, 0.0, 1.0, 44038.47756616594], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2443200.0000, 
sim time next is 2443800.0000, 
raw observation next is [-9.4, 60.83333333333334, 0.0, 0.0, 26.0, 23.0320306406718, -0.1995336962861374, 0.0, 1.0, 44055.83455877505], 
processed observation next is [0.0, 0.2608695652173913, 0.20221606648199447, 0.6083333333333334, 0.0, 0.0, 0.6666666666666666, 0.41933588672264993, 0.4334887679046209, 0.0, 1.0, 0.2097896883751193], 
reward next is 0.7902, 
noisyNet noise sample is [array([1.1128474], dtype=float32), 0.4522694]. 
=============================================
[2019-04-04 00:55:03,910] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.9304161e-25 2.7557853e-15 2.9099735e-11 1.0000000e+00 2.9577524e-17
 1.5190574e-09 2.1046148e-16], sum to 1.0000
[2019-04-04 00:55:03,910] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8248
[2019-04-04 00:55:03,986] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.566666666666666, 68.33333333333333, 102.8333333333333, 121.6666666666667, 26.0, 25.87460440898645, 0.3648229675162591, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2626800.0000, 
sim time next is 2627400.0000, 
raw observation next is [-5.283333333333333, 66.66666666666667, 112.6666666666667, 152.3333333333333, 26.0, 25.86923242645365, 0.3641059510341473, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.31625115420129274, 0.6666666666666667, 0.37555555555555564, 0.16832412523020251, 0.6666666666666666, 0.6557693688711375, 0.6213686503447158, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.62702537], dtype=float32), -1.6617229]. 
=============================================
[2019-04-04 00:55:04,595] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0751245e-24 6.8062346e-15 2.9813690e-11 1.0000000e+00 4.4022518e-16
 1.9114264e-08 2.9807228e-17], sum to 1.0000
[2019-04-04 00:55:04,595] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9633
[2019-04-04 00:55:04,631] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.083333333333333, 62.5, 176.0, 240.3333333333333, 26.0, 25.73676605548741, 0.3659317729961122, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2631000.0000, 
sim time next is 2631600.0000, 
raw observation next is [-3.9, 62.0, 188.0, 223.0, 26.0, 25.72985843315115, 0.3624199799104912, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.3545706371191136, 0.62, 0.6266666666666667, 0.24640883977900552, 0.6666666666666666, 0.6441548694292626, 0.6208066599701637, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.76443034], dtype=float32), 0.35411385]. 
=============================================
[2019-04-04 00:55:06,286] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.7371586e-25 2.4366406e-13 2.5105465e-10 1.0000000e+00 4.1829748e-15
 1.9447717e-10 7.2345109e-15], sum to 1.0000
[2019-04-04 00:55:06,346] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6398
[2019-04-04 00:55:06,474] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.166666666666667, 49.33333333333334, 231.5, 157.6666666666667, 26.0, 25.08748607496305, 0.3135134003341007, 1.0, 1.0, 132766.9309857152], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2637600.0000, 
sim time next is 2638200.0000, 
raw observation next is [-0.8833333333333332, 48.16666666666667, 218.0, 168.3333333333333, 26.0, 24.39306537328271, 0.2998017787494172, 1.0, 1.0, 197762.74930347], 
processed observation next is [1.0, 0.5217391304347826, 0.43813481071098803, 0.4816666666666667, 0.7266666666666667, 0.18600368324125224, 0.6666666666666666, 0.5327554477735591, 0.5999339262498057, 1.0, 1.0, 0.9417273776355713], 
reward next is 0.0583, 
noisyNet noise sample is [array([0.13731061], dtype=float32), -0.030977184]. 
=============================================
[2019-04-04 00:55:08,210] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.5638953e-24 1.6683141e-16 1.0476483e-13 1.0000000e+00 5.6610132e-18
 2.7311380e-12 2.6331921e-15], sum to 1.0000
[2019-04-04 00:55:08,211] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5709
[2019-04-04 00:55:08,449] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.666666666666667, 84.0, 0.0, 0.0, 26.0, 25.00518029910502, 0.2538409788692038, 0.0, 1.0, 51403.9463334106], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1876800.0000, 
sim time next is 1877400.0000, 
raw observation next is [-4.75, 84.5, 0.0, 0.0, 26.0, 25.03683700963919, 0.2530670203546473, 0.0, 1.0, 32805.08194966387], 
processed observation next is [0.0, 0.7391304347826086, 0.3310249307479225, 0.845, 0.0, 0.0, 0.6666666666666666, 0.5864030841365991, 0.584355673451549, 0.0, 1.0, 0.15621467595078034], 
reward next is 0.8438, 
noisyNet noise sample is [array([1.1945856], dtype=float32), -0.5870978]. 
=============================================
[2019-04-04 00:55:10,594] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.97030931e-27 1.62836195e-17 7.32515936e-13 1.00000000e+00
 2.28661689e-20 1.07964075e-10 1.58942771e-17], sum to 1.0000
[2019-04-04 00:55:10,595] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4755
[2019-04-04 00:55:10,697] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.7, 49.0, 0.0, 0.0, 26.0, 24.9741687627777, 0.1690730175068663, 0.0, 1.0, 38367.95605025042], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2520000.0000, 
sim time next is 2520600.0000, 
raw observation next is [-1.8, 50.33333333333334, 0.0, 0.0, 26.0, 24.89522487947797, 0.1570736424320954, 0.0, 1.0, 38391.08831720001], 
processed observation next is [1.0, 0.17391304347826086, 0.41274238227146814, 0.5033333333333334, 0.0, 0.0, 0.6666666666666666, 0.5746020732898307, 0.5523578808106985, 0.0, 1.0, 0.182814706272381], 
reward next is 0.8172, 
noisyNet noise sample is [array([0.30575952], dtype=float32), 0.50016487]. 
=============================================
[2019-04-04 00:55:25,254] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.2959906e-21 1.6820856e-10 1.2831081e-08 9.2737031e-01 4.4011881e-13
 7.2629638e-02 4.1942175e-12], sum to 1.0000
[2019-04-04 00:55:25,258] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5495
[2019-04-04 00:55:25,353] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.45, 50.5, 245.0, 147.0, 26.0, 25.7189860141021, 0.2802000410164106, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2637000.0000, 
sim time next is 2637600.0000, 
raw observation next is [-1.166666666666667, 49.33333333333334, 231.5, 157.6666666666667, 26.0, 25.08746415376084, 0.3134845329182852, 1.0, 1.0, 132770.847182314], 
processed observation next is [1.0, 0.5217391304347826, 0.43028624192059095, 0.4933333333333334, 0.7716666666666666, 0.17421731123388587, 0.6666666666666666, 0.5906220128134034, 0.604494844306095, 1.0, 1.0, 0.6322421294395905], 
reward next is 0.3678, 
noisyNet noise sample is [array([-0.4352081], dtype=float32), -1.402444]. 
=============================================
[2019-04-04 00:55:35,379] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.9704086e-23 4.5182844e-16 3.7680702e-11 1.0000000e+00 4.5358302e-16
 2.6784854e-13 6.1876511e-15], sum to 1.0000
[2019-04-04 00:55:35,379] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0578
[2019-04-04 00:55:35,437] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-15.0, 88.33333333333334, 0.0, 0.0, 26.0, 23.42026933596614, -0.02408754100400805, 0.0, 1.0, 44413.35831516428], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2694000.0000, 
sim time next is 2694600.0000, 
raw observation next is [-15.0, 87.0, 0.0, 0.0, 26.0, 23.40176396610905, -0.03556850351821052, 0.0, 1.0, 44318.16222772298], 
processed observation next is [1.0, 0.17391304347826086, 0.04709141274238226, 0.87, 0.0, 0.0, 0.6666666666666666, 0.4501469971757543, 0.4881438321605965, 0.0, 1.0, 0.21103886775106181], 
reward next is 0.7890, 
noisyNet noise sample is [array([1.7461165], dtype=float32), 0.80242866]. 
=============================================
[2019-04-04 00:55:45,825] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.1153545e-25 5.2891794e-18 5.8165231e-14 1.0000000e+00 1.5349586e-18
 1.7930510e-13 1.2697870e-16], sum to 1.0000
[2019-04-04 00:55:45,827] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5763
[2019-04-04 00:55:45,842] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.9, 65.66666666666667, 0.0, 0.0, 26.0, 24.4427616996161, 0.1668569815157765, 0.0, 1.0, 40315.23292843454], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2347800.0000, 
sim time next is 2348400.0000, 
raw observation next is [-3.0, 66.33333333333334, 0.0, 0.0, 26.0, 24.41773042815542, 0.1584465165824036, 0.0, 1.0, 40426.02913380557], 
processed observation next is [0.0, 0.17391304347826086, 0.3795013850415513, 0.6633333333333334, 0.0, 0.0, 0.6666666666666666, 0.5348108690129516, 0.5528155055274678, 0.0, 1.0, 0.19250490063716938], 
reward next is 0.8075, 
noisyNet noise sample is [array([0.75817263], dtype=float32), -1.8197836]. 
=============================================
[2019-04-04 00:55:49,222] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.8916180e-29 8.4487649e-18 2.0293284e-18 1.0000000e+00 1.9240737e-17
 2.6684733e-11 2.0771752e-18], sum to 1.0000
[2019-04-04 00:55:49,222] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9995
[2019-04-04 00:55:49,280] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 37.0, 0.0, 0.0, 26.0, 24.55469814144192, 0.345491538378601, 1.0, 1.0, 86658.04813989266], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2833200.0000, 
sim time next is 2833800.0000, 
raw observation next is [2.833333333333333, 38.16666666666667, 0.0, 0.0, 26.0, 24.96692640575685, 0.3923565403455422, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.541089566020314, 0.3816666666666667, 0.0, 0.0, 0.6666666666666666, 0.5805772004797376, 0.6307855134485141, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2394129], dtype=float32), -0.22012702]. 
=============================================
[2019-04-04 00:55:50,271] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.5960300e-23 5.4762908e-15 4.5926737e-10 1.0000000e+00 4.9693549e-16
 1.2094137e-08 8.8393267e-15], sum to 1.0000
[2019-04-04 00:55:50,271] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7544
[2019-04-04 00:55:50,346] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.5, 45.33333333333333, 54.66666666666667, 44.0, 26.0, 24.95061024972545, 0.2839013873257187, 0.0, 1.0, 39577.80578972819], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2393400.0000, 
sim time next is 2394000.0000, 
raw observation next is [-0.6, 45.0, 42.5, 37.0, 26.0, 24.95703715286958, 0.2818529532711981, 0.0, 1.0, 34895.3951592772], 
processed observation next is [0.0, 0.7391304347826086, 0.44598337950138506, 0.45, 0.14166666666666666, 0.04088397790055249, 0.6666666666666666, 0.5797530960724652, 0.5939509844237327, 0.0, 1.0, 0.16616854837751047], 
reward next is 0.8338, 
noisyNet noise sample is [array([-0.11938034], dtype=float32), -0.6255197]. 
=============================================
[2019-04-04 00:55:50,373] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[81.42602 ]
 [81.68649 ]
 [81.85308 ]
 [81.959984]
 [82.03893 ]], R is [[81.22980499]
 [81.22904205]
 [81.19932556]
 [81.15210724]
 [81.11450958]].
[2019-04-04 00:55:54,242] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.4885087e-33 1.1839234e-24 3.0104849e-21 1.0000000e+00 8.0037590e-28
 4.7793246e-25 6.1698038e-24], sum to 1.0000
[2019-04-04 00:55:54,242] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8614
[2019-04-04 00:55:54,277] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.8333333333333334, 41.5, 102.3333333333333, 785.3333333333334, 26.0, 25.12034281418397, 0.3612589453798183, 0.0, 1.0, 18706.82169031437], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3075000.0000, 
sim time next is 3075600.0000, 
raw observation next is [-0.6666666666666667, 41.0, 100.6666666666667, 780.1666666666667, 26.0, 25.11697608629562, 0.3632910547282286, 0.0, 1.0, 18705.95737051069], 
processed observation next is [0.0, 0.6086956521739131, 0.44413665743305636, 0.41, 0.33555555555555566, 0.8620626151012892, 0.6666666666666666, 0.5930813405246349, 0.6210970182427429, 0.0, 1.0, 0.08907598747862233], 
reward next is 0.9109, 
noisyNet noise sample is [array([0.2574037], dtype=float32), -1.4096409]. 
=============================================
[2019-04-04 00:56:00,345] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.5967679e-25 1.1677284e-14 6.8514023e-12 1.0000000e+00 1.8126786e-17
 4.9404484e-11 1.3229010e-15], sum to 1.0000
[2019-04-04 00:56:00,348] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7628
[2019-04-04 00:56:00,364] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.8, 56.00000000000001, 0.0, 0.0, 26.0, 25.3745426211639, 0.362843163447521, 1.0, 1.0, 9344.359328775383], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2658000.0000, 
sim time next is 2658600.0000, 
raw observation next is [-0.8999999999999999, 57.0, 0.0, 0.0, 26.0, 25.167722052435, 0.3456179791254293, 1.0, 1.0, 50181.58767030433], 
processed observation next is [1.0, 0.782608695652174, 0.43767313019390586, 0.57, 0.0, 0.0, 0.6666666666666666, 0.59731017103625, 0.6152059930418098, 1.0, 1.0, 0.2389599412871635], 
reward next is 0.7610, 
noisyNet noise sample is [array([0.16730118], dtype=float32), 0.014062804]. 
=============================================
[2019-04-04 00:56:18,384] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.5684789e-30 1.8957227e-20 7.1624690e-19 1.0000000e+00 3.6758812e-21
 4.0504556e-17 1.1624043e-21], sum to 1.0000
[2019-04-04 00:56:18,385] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7059
[2019-04-04 00:56:18,397] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [9.0, 27.0, 0.0, 0.0, 26.0, 25.51561673115183, 0.3641242772991639, 0.0, 1.0, 18750.96512437823], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3646800.0000, 
sim time next is 3647400.0000, 
raw observation next is [9.166666666666666, 26.66666666666666, 0.0, 0.0, 26.0, 25.52456377656383, 0.3618514305991867, 0.0, 1.0, 18748.95827495787], 
processed observation next is [0.0, 0.21739130434782608, 0.7165281625115422, 0.2666666666666666, 0.0, 0.0, 0.6666666666666666, 0.6270469813803192, 0.6206171435330622, 0.0, 1.0, 0.08928075369027556], 
reward next is 0.9107, 
noisyNet noise sample is [array([0.11673013], dtype=float32), 0.9173277]. 
=============================================
[2019-04-04 00:56:19,153] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.9393362e-27 4.3481576e-17 1.6413047e-14 1.0000000e+00 6.1912211e-21
 2.4783109e-08 2.3001896e-17], sum to 1.0000
[2019-04-04 00:56:19,173] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1765
[2019-04-04 00:56:19,202] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.333333333333333, 64.0, 0.0, 0.0, 26.0, 25.34028635199557, 0.481236656820613, 0.0, 1.0, 44482.60878294804], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3540000.0000, 
sim time next is 3540600.0000, 
raw observation next is [-1.5, 63.0, 0.0, 0.0, 26.0, 25.37054143794169, 0.4800210406966203, 0.0, 1.0, 43548.74729626052], 
processed observation next is [1.0, 1.0, 0.4210526315789474, 0.63, 0.0, 0.0, 0.6666666666666666, 0.6142117864951407, 0.6600070135655401, 0.0, 1.0, 0.2073749871250501], 
reward next is 0.7926, 
noisyNet noise sample is [array([-0.16844246], dtype=float32), -0.6702098]. 
=============================================
[2019-04-04 00:56:24,552] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.5963275e-29 1.5785206e-18 7.2403806e-18 1.0000000e+00 1.7821102e-20
 9.7567682e-16 1.0134389e-19], sum to 1.0000
[2019-04-04 00:56:24,552] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0801
[2019-04-04 00:56:24,611] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 92.0, 15.0, 130.0, 26.0, 25.66660883735847, 0.5158153595985593, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3224400.0000, 
sim time next is 3225000.0000, 
raw observation next is [-3.0, 92.0, 29.0, 178.0, 26.0, 25.57962983900522, 0.5142397652183853, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.3795013850415513, 0.92, 0.09666666666666666, 0.19668508287292819, 0.6666666666666666, 0.6316358199171018, 0.6714132550727951, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.1000705], dtype=float32), -0.67277265]. 
=============================================
[2019-04-04 00:56:24,614] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[84.15651]
 [83.16435]
 [81.49063]
 [79.79534]
 [79.67723]], R is [[84.78121948]
 [84.93341064]
 [85.08407593]
 [85.23323822]
 [85.185112  ]].
[2019-04-04 00:56:25,783] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.52853049e-23 1.45137175e-11 6.07986417e-09 1.00000000e+00
 1.84912537e-13 1.73532193e-08 4.17630638e-13], sum to 1.0000
[2019-04-04 00:56:25,797] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1980
[2019-04-04 00:56:25,823] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 68.0, 0.0, 0.0, 26.0, 25.32854127673248, 0.4629032664763867, 0.0, 1.0, 34916.78643287063], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3785400.0000, 
sim time next is 3786000.0000, 
raw observation next is [-2.0, 67.0, 0.0, 0.0, 26.0, 25.2419984892507, 0.4507185936948661, 1.0, 1.0, 28442.82575474951], 
processed observation next is [1.0, 0.8260869565217391, 0.40720221606648205, 0.67, 0.0, 0.0, 0.6666666666666666, 0.6034998741042251, 0.650239531231622, 1.0, 1.0, 0.1354420274035691], 
reward next is 0.8646, 
noisyNet noise sample is [array([0.6578302], dtype=float32), 1.0206877]. 
=============================================
[2019-04-04 00:56:25,863] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[83.17288 ]
 [84.69973 ]
 [84.112114]
 [83.61538 ]
 [83.60603 ]], R is [[83.97348785]
 [83.96748352]
 [83.88330841]
 [83.6876297 ]
 [83.85075378]].
[2019-04-04 00:56:28,127] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.5670889e-26 4.2243398e-18 1.8591716e-14 1.0000000e+00 9.6319760e-20
 9.5236613e-17 3.6298890e-18], sum to 1.0000
[2019-04-04 00:56:28,129] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2845
[2019-04-04 00:56:28,177] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 26.0, 25.01438500675448, 0.3510708815203189, 0.0, 1.0, 41135.62162065809], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3556800.0000, 
sim time next is 3557400.0000, 
raw observation next is [-4.166666666666667, 70.0, 0.0, 0.0, 26.0, 24.97779879854684, 0.3431213644226294, 0.0, 1.0, 41074.59006358775], 
processed observation next is [0.0, 0.17391304347826086, 0.3471837488457987, 0.7, 0.0, 0.0, 0.6666666666666666, 0.5814832332122366, 0.6143737881408765, 0.0, 1.0, 0.1955932860170845], 
reward next is 0.8044, 
noisyNet noise sample is [array([0.280957], dtype=float32), -1.2566756]. 
=============================================
[2019-04-04 00:56:35,790] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.4181623e-30 1.9102631e-20 4.0025338e-17 1.0000000e+00 2.5687729e-22
 1.6503988e-18 2.0006090e-21], sum to 1.0000
[2019-04-04 00:56:35,790] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3367
[2019-04-04 00:56:35,797] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.0, 59.0, 12.5, 137.5, 26.0, 25.26009771338172, 0.3749861077671206, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3692400.0000, 
sim time next is 3693000.0000, 
raw observation next is [4.0, 59.0, 0.0, 0.0, 26.0, 25.21108283601148, 0.3504559633207134, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.7391304347826086, 0.5734072022160666, 0.59, 0.0, 0.0, 0.6666666666666666, 0.6009235696676235, 0.6168186544402378, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.134917], dtype=float32), 0.022079227]. 
=============================================
[2019-04-04 00:56:35,806] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[88.611176]
 [88.32228 ]
 [88.411026]
 [88.51651 ]
 [88.741   ]], R is [[87.78154755]
 [87.9037323 ]
 [88.02469635]
 [88.14444733]
 [88.26300049]].
[2019-04-04 00:56:38,077] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.6820656e-24 1.5927297e-17 7.9638960e-12 1.0000000e+00 2.2570771e-17
 6.5617919e-15 1.7450665e-16], sum to 1.0000
[2019-04-04 00:56:38,081] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9236
[2019-04-04 00:56:38,102] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 84.0, 0.0, 0.0, 26.0, 24.58296004328574, 0.2628305880344677, 0.0, 1.0, 42665.98821846299], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2949600.0000, 
sim time next is 2950200.0000, 
raw observation next is [-3.0, 84.0, 0.0, 0.0, 26.0, 24.57705727517726, 0.2571048791079123, 0.0, 1.0, 42648.24105146756], 
processed observation next is [0.0, 0.13043478260869565, 0.3795013850415513, 0.84, 0.0, 0.0, 0.6666666666666666, 0.5480881062647717, 0.5857016263693041, 0.0, 1.0, 0.2030868621498455], 
reward next is 0.7969, 
noisyNet noise sample is [array([-0.49022812], dtype=float32), -0.35638878]. 
=============================================
[2019-04-04 00:56:38,521] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.7469364e-29 8.9027969e-18 1.9508522e-15 1.0000000e+00 1.2902530e-20
 9.6584550e-15 2.6716453e-19], sum to 1.0000
[2019-04-04 00:56:38,523] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6061
[2019-04-04 00:56:38,540] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.166666666666667, 72.33333333333334, 0.0, 0.0, 26.0, 25.54373337554387, 0.3710844044511732, 0.0, 1.0, 18745.35534359251], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4342800.0000, 
sim time next is 4343400.0000, 
raw observation next is [3.1, 73.0, 0.0, 0.0, 26.0, 25.50076588056787, 0.3576436311184957, 0.0, 1.0, 41921.71259038074], 
processed observation next is [1.0, 0.2608695652173913, 0.5484764542936289, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6250638233806557, 0.6192145437061652, 0.0, 1.0, 0.19962720281133686], 
reward next is 0.8004, 
noisyNet noise sample is [array([0.1765707], dtype=float32), -0.14052004]. 
=============================================
[2019-04-04 00:56:43,965] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.3514206e-28 1.3105228e-18 1.4710798e-17 1.0000000e+00 6.1584824e-20
 7.7664319e-18 2.2746211e-19], sum to 1.0000
[2019-04-04 00:56:43,970] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3224
[2019-04-04 00:56:44,004] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.02918763801711, 0.4674202097590885, 0.0, 1.0, 64135.78235947451], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3528600.0000, 
sim time next is 3529200.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.05920206909923, 0.4844453275559772, 0.0, 1.0, 198637.8904713204], 
processed observation next is [1.0, 0.8695652173913043, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5882668390916024, 0.6614817758519924, 0.0, 1.0, 0.9458947165300972], 
reward next is 0.0541, 
noisyNet noise sample is [array([2.4197955], dtype=float32), -1.4329672]. 
=============================================
[2019-04-04 00:56:54,418] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.2274081e-23 2.0248312e-15 6.8131481e-14 1.0000000e+00 3.7074619e-17
 6.6473200e-13 1.6091724e-14], sum to 1.0000
[2019-04-04 00:56:54,418] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0190
[2019-04-04 00:56:54,433] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 71.0, 0.0, 0.0, 26.0, 25.37528686729002, 0.3801123986245942, 0.0, 1.0, 40245.16032351731], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3716400.0000, 
sim time next is 3717000.0000, 
raw observation next is [-3.0, 71.0, 0.0, 0.0, 26.0, 25.36689833865439, 0.3782261356882961, 0.0, 1.0, 42495.71419493083], 
processed observation next is [1.0, 0.0, 0.3795013850415513, 0.71, 0.0, 0.0, 0.6666666666666666, 0.6139081948878659, 0.6260753785627654, 0.0, 1.0, 0.2023605437853849], 
reward next is 0.7976, 
noisyNet noise sample is [array([0.5254904], dtype=float32), 1.6088692]. 
=============================================
[2019-04-04 00:56:54,476] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[81.54186]
 [81.28567]
 [81.05419]
 [80.44103]
 [80.02724]], R is [[81.60109711]
 [81.59344482]
 [81.59944153]
 [81.5856781 ]
 [81.5643692 ]].
[2019-04-04 00:57:02,547] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.5567609e-21 2.3503157e-12 7.8771040e-10 9.9999106e-01 2.3588428e-13
 8.9640016e-06 2.3483171e-12], sum to 1.0000
[2019-04-04 00:57:02,549] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5082
[2019-04-04 00:57:02,598] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 80.33333333333334, 0.0, 0.0, 26.0, 24.98333174728362, 0.4203858112094365, 0.0, 1.0, 171073.4272882395], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2924400.0000, 
sim time next is 2925000.0000, 
raw observation next is [-1.0, 81.5, 0.0, 0.0, 26.0, 25.00065946482207, 0.4417761512350137, 0.0, 1.0, 101758.6463561644], 
processed observation next is [1.0, 0.8695652173913043, 0.4349030470914128, 0.815, 0.0, 0.0, 0.6666666666666666, 0.5833882887351726, 0.6472587170783379, 0.0, 1.0, 0.4845649826484019], 
reward next is 0.5154, 
noisyNet noise sample is [array([-1.2661387], dtype=float32), -2.0917988]. 
=============================================
[2019-04-04 00:57:02,603] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[83.885506]
 [83.839424]
 [83.15279 ]
 [84.233284]
 [83.7032  ]], R is [[83.45393372]
 [82.80475616]
 [82.88395691]
 [82.817276  ]
 [82.50325775]].
[2019-04-04 00:57:09,996] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.70627989e-30 1.30336290e-19 3.13600159e-14 1.00000000e+00
 3.80887516e-22 4.08933114e-17 1.12058505e-20], sum to 1.0000
[2019-04-04 00:57:10,001] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6149
[2019-04-04 00:57:10,007] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.0, 52.0, 154.0, 782.0, 26.0, 25.33938448630455, 0.4336116699350031, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4280400.0000, 
sim time next is 4281000.0000, 
raw observation next is [7.0, 52.83333333333334, 165.3333333333333, 760.3333333333333, 26.0, 25.34440090261619, 0.4355215013439211, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.6565096952908588, 0.5283333333333334, 0.551111111111111, 0.840147329650092, 0.6666666666666666, 0.6120334085513491, 0.645173833781307, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.76760525], dtype=float32), 0.25314635]. 
=============================================
[2019-04-04 00:57:10,013] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[88.395004]
 [88.75062 ]
 [89.06436 ]
 [89.26773 ]
 [89.54947 ]], R is [[88.20615387]
 [88.32409668]
 [88.44085693]
 [88.55644989]
 [88.67088318]].
[2019-04-04 00:57:13,760] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.6382889e-30 1.1447055e-22 4.6844675e-19 1.0000000e+00 3.7598861e-26
 1.5133938e-18 2.1944176e-20], sum to 1.0000
[2019-04-04 00:57:13,761] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6748
[2019-04-04 00:57:13,772] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 40.0, 70.5, 579.5, 26.0, 25.20610172347641, 0.3642359128500446, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3081600.0000, 
sim time next is 3082200.0000, 
raw observation next is [0.8333333333333334, 45.33333333333334, 66.0, 548.3333333333334, 26.0, 25.19182519781747, 0.3523510247570028, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.4856879039704525, 0.4533333333333334, 0.22, 0.6058931860036832, 0.6666666666666666, 0.5993187664847891, 0.6174503415856676, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7119677], dtype=float32), -0.42024252]. 
=============================================
[2019-04-04 00:57:15,056] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.3763428e-27 3.2848814e-16 2.1098108e-13 1.0000000e+00 2.1810824e-19
 2.2131652e-12 1.7994370e-17], sum to 1.0000
[2019-04-04 00:57:15,058] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5319
[2019-04-04 00:57:15,078] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 74.0, 0.0, 0.0, 26.0, 25.68245426306795, 0.4714969956790478, 0.0, 1.0, 21416.23246632047], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3536400.0000, 
sim time next is 3537000.0000, 
raw observation next is [-1.0, 72.0, 0.0, 0.0, 26.0, 25.46402075431349, 0.4557778522315464, 0.0, 1.0, 143780.3385663668], 
processed observation next is [1.0, 0.9565217391304348, 0.4349030470914128, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6220017295261243, 0.6519259507438487, 0.0, 1.0, 0.6846682788874608], 
reward next is 0.3153, 
noisyNet noise sample is [array([0.8924438], dtype=float32), -0.65325457]. 
=============================================
[2019-04-04 00:57:15,101] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[81.96371 ]
 [82.03183 ]
 [82.14354 ]
 [82.20884 ]
 [82.298386]], R is [[81.72221375]
 [81.80300903]
 [81.98497772]
 [82.16513062]
 [82.34348297]].
[2019-04-04 00:57:22,860] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:57:22,860] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:57:22,865] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run18
[2019-04-04 00:57:25,523] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.4356006e-26 1.3802590e-19 1.2043825e-14 1.0000000e+00 4.2549835e-20
 1.5668065e-15 8.2444273e-18], sum to 1.0000
[2019-04-04 00:57:25,526] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3701
[2019-04-04 00:57:25,541] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.5, 70.0, 3.0, 121.0, 26.0, 24.28509184019728, 0.1903714042394187, 0.0, 1.0, 41541.95723133453], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3569400.0000, 
sim time next is 3570000.0000, 
raw observation next is [-6.666666666666666, 70.0, 17.16666666666666, 171.6666666666667, 26.0, 24.24941717518623, 0.1926312816335434, 0.0, 1.0, 41553.31699536266], 
processed observation next is [0.0, 0.30434782608695654, 0.2779316712834719, 0.7, 0.0572222222222222, 0.18968692449355437, 0.6666666666666666, 0.5207847645988526, 0.5642104272111811, 0.0, 1.0, 0.1978729380731555], 
reward next is 0.8021, 
noisyNet noise sample is [array([-1.8536489], dtype=float32), 1.065353]. 
=============================================
[2019-04-04 00:57:25,557] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[82.411606]
 [81.45544 ]
 [81.439835]
 [81.36401 ]
 [81.21477 ]], R is [[82.96872711]
 [82.94121552]
 [82.91436005]
 [82.88835907]
 [82.86316681]].
[2019-04-04 00:57:33,354] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.0904349e-16 2.1225197e-09 1.2660006e-06 9.9999738e-01 6.6087957e-09
 1.3227976e-06 1.7315771e-09], sum to 1.0000
[2019-04-04 00:57:33,363] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8290
[2019-04-04 00:57:33,388] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.433333333333334, 87.0, 0.0, 0.0, 21.0, 19.99613599749467, -0.799653524784666, 0.0, 1.0, 197552.3431681361], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 69600.0000, 
sim time next is 70200.0000, 
raw observation next is [3.25, 87.5, 0.0, 0.0, 21.0, 20.00544338693089, -0.77109435870309, 0.0, 1.0, 198971.5778061892], 
processed observation next is [0.0, 0.8260869565217391, 0.5526315789473685, 0.875, 0.0, 0.0, 0.25, 0.16712028224424072, 0.24296854709897, 0.0, 1.0, 0.9474837038389962], 
reward next is 0.0525, 
noisyNet noise sample is [array([0.46403623], dtype=float32), 0.46455437]. 
=============================================
[2019-04-04 00:57:35,400] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2859735e-27 3.7994232e-17 1.2872419e-16 1.0000000e+00 2.8176450e-20
 2.5579623e-11 6.6800785e-19], sum to 1.0000
[2019-04-04 00:57:35,402] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7676
[2019-04-04 00:57:35,419] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 61.0, 0.0, 0.0, 26.0, 25.63057685862328, 0.4573007259407311, 0.0, 1.0, 20857.50725856494], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4573200.0000, 
sim time next is 4573800.0000, 
raw observation next is [1.0, 61.0, 0.0, 0.0, 26.0, 25.44565729005065, 0.4462753513764803, 0.0, 1.0, 123393.6431894945], 
processed observation next is [1.0, 0.9565217391304348, 0.4903047091412743, 0.61, 0.0, 0.0, 0.6666666666666666, 0.6204714408375542, 0.6487584504588267, 0.0, 1.0, 0.587588777092831], 
reward next is 0.4124, 
noisyNet noise sample is [array([0.8383669], dtype=float32), -0.5319549]. 
=============================================
[2019-04-04 00:57:36,720] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.0665864e-25 1.4922582e-13 1.5578177e-13 1.0000000e+00 1.5044376e-14
 5.8368159e-14 5.9153192e-16], sum to 1.0000
[2019-04-04 00:57:36,725] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4117
[2019-04-04 00:57:36,750] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 86.0, 214.6666666666667, 97.33333333333334, 26.0, 26.48339642014194, 0.6621060509676183, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4445400.0000, 
sim time next is 4446000.0000, 
raw observation next is [1.0, 86.0, 196.5, 73.0, 26.0, 26.50142613285453, 0.6608249025613815, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4903047091412743, 0.86, 0.655, 0.08066298342541436, 0.6666666666666666, 0.7084521777378775, 0.7202749675204605, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.34457505], dtype=float32), 1.9098668]. 
=============================================
[2019-04-04 00:57:36,784] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[88.79272 ]
 [89.496994]
 [90.300224]
 [90.1507  ]
 [90.06297 ]], R is [[88.61362457]
 [88.72748566]
 [88.84020996]
 [88.95180511]
 [89.06228638]].
[2019-04-04 00:57:43,058] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:57:43,059] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:57:43,063] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run18
[2019-04-04 00:57:47,542] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.22513745e-29 2.88603084e-17 2.80464425e-15 1.00000000e+00
 4.86877433e-18 1.19917853e-15 1.38097002e-19], sum to 1.0000
[2019-04-04 00:57:47,545] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1130
[2019-04-04 00:57:47,565] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.333333333333333, 51.0, 123.1666666666667, 822.0, 26.0, 26.28285076258002, 0.683146884742761, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4620000.0000, 
sim time next is 4620600.0000, 
raw observation next is [2.5, 50.5, 122.0, 833.0, 26.0, 26.54401718499428, 0.4739466457177952, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.5318559556786704, 0.505, 0.4066666666666667, 0.9204419889502763, 0.6666666666666666, 0.7120014320828568, 0.657982215239265, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.36913085], dtype=float32), 1.0675312]. 
=============================================
[2019-04-04 00:57:49,033] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:57:49,033] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:57:49,044] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run18
[2019-04-04 00:57:50,702] A3C_AGENT_WORKER-Thread-5 INFO:Evaluating...
[2019-04-04 00:57:50,704] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 00:57:50,704] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 00:57:50,705] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 00:57:50,705] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:57:50,706] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:57:50,706] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:57:50,719] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run24
[2019-04-04 00:57:50,741] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run24
[2019-04-04 00:57:50,759] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run24
[2019-04-04 00:58:16,502] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.82234186], dtype=float32), 0.27940354]
[2019-04-04 00:58:16,502] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-0.6, 72.0, 0.0, 0.0, 20.0, 19.52587779455542, -0.9929954209118926, 0.0, 1.0, 31681.58943766747]
[2019-04-04 00:58:16,502] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 00:58:16,503] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.3624029e-16 7.8723014e-11 2.2865253e-08 9.9999964e-01 3.5411407e-09
 3.5193150e-07 5.1554333e-10], sampled 0.7098425865896207
[2019-04-04 00:59:31,823] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6525.6980 170077457.7864 -1178.7050
[2019-04-04 00:59:33,510] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.82234186], dtype=float32), 0.27940354]
[2019-04-04 00:59:33,510] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [2.666666666666667, 72.0, 0.0, 0.0, 20.5, 20.48735173437634, -0.6421212042679884, 0.0, 1.0, 0.0]
[2019-04-04 00:59:33,510] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 00:59:33,511] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [4.4015926e-18 1.7971753e-11 2.6736267e-09 1.0000000e+00 1.3329496e-10
 2.8806598e-08 3.7689473e-11], sampled 0.8977440189672233
[2019-04-04 00:59:48,170] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5672.4884 190622080.9075 -1999.2471
[2019-04-04 00:59:59,809] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5762.0287 206087623.5081 -1798.2057
[2019-04-04 01:00:00,842] A3C_AGENT_WORKER-Thread-5 INFO:Global step: 2300000, evaluation results [2300000.0, 5672.488362986919, 190622080.90749383, -1999.2471141905035, 6525.698004145879, 170077457.78638378, -1178.7049819272231, 5762.028725444081, 206087623.5081306, -1798.2057195330483]
[2019-04-04 01:00:05,889] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:00:05,889] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:00:05,894] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run18
[2019-04-04 01:00:08,125] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.5537649e-30 1.9504444e-21 1.8608682e-17 1.0000000e+00 1.9174003e-23
 1.6727086e-19 3.2863285e-20], sum to 1.0000
[2019-04-04 01:00:08,125] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3232
[2019-04-04 01:00:08,195] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 60.0, 253.5, 171.5, 26.0, 25.3317129439091, 0.3481971328752305, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4874400.0000, 
sim time next is 4875000.0000, 
raw observation next is [-1.733333333333333, 58.66666666666666, 269.0, 169.0, 26.0, 25.26849334802282, 0.3408681177014224, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.41458910433979695, 0.5866666666666666, 0.8966666666666666, 0.1867403314917127, 0.6666666666666666, 0.6057077790019015, 0.6136227059004741, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9450848], dtype=float32), 0.3583809]. 
=============================================
[2019-04-04 01:00:08,201] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[91.85555 ]
 [91.66257 ]
 [91.5561  ]
 [91.262566]
 [91.110634]], R is [[92.03559875]
 [92.115242  ]
 [92.1940918 ]
 [92.27214813]
 [92.34942627]].
[2019-04-04 01:00:10,809] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:00:10,809] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:00:10,817] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run18
[2019-04-04 01:00:10,894] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:00:10,894] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:00:10,905] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run18
[2019-04-04 01:00:15,963] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.3419725e-32 1.5029331e-20 1.1290649e-18 1.0000000e+00 5.8370646e-24
 2.5828304e-17 6.4826044e-21], sum to 1.0000
[2019-04-04 01:00:15,963] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8994
[2019-04-04 01:00:15,998] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.333333333333333, 38.0, 0.0, 0.0, 26.0, 25.5637414848604, 0.4741315999495587, 0.0, 1.0, 23996.92106792808], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5010000.0000, 
sim time next is 5010600.0000, 
raw observation next is [2.166666666666667, 39.0, 0.0, 0.0, 26.0, 25.5390214445266, 0.4672986797307825, 0.0, 1.0, 40666.95153230627], 
processed observation next is [1.0, 1.0, 0.5226223453370269, 0.39, 0.0, 0.0, 0.6666666666666666, 0.6282517870438834, 0.6557662265769275, 0.0, 1.0, 0.1936521501538394], 
reward next is 0.8063, 
noisyNet noise sample is [array([-0.82398844], dtype=float32), -0.6914264]. 
=============================================
[2019-04-04 01:00:17,738] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.7554940e-33 2.1898233e-23 1.5742546e-21 1.0000000e+00 9.6921768e-25
 6.3808622e-20 6.5772023e-23], sum to 1.0000
[2019-04-04 01:00:17,739] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2257
[2019-04-04 01:00:17,787] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.8666666666666667, 73.0, 0.0, 0.0, 26.0, 25.28927929575293, 0.4422116573135302, 0.0, 1.0, 42637.20057423736], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4502400.0000, 
sim time next is 4503000.0000, 
raw observation next is [-0.9333333333333333, 73.0, 0.0, 0.0, 26.0, 25.34750034108225, 0.4472501679564501, 0.0, 1.0, 42507.52371027879], 
processed observation next is [1.0, 0.08695652173913043, 0.4367497691597415, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6122916950901874, 0.6490833893188167, 0.0, 1.0, 0.20241677957275614], 
reward next is 0.7976, 
noisyNet noise sample is [array([0.40873086], dtype=float32), 1.2048852]. 
=============================================
[2019-04-04 01:00:17,794] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[91.9045  ]
 [91.907295]
 [91.92257 ]
 [91.93204 ]
 [91.9127  ]], R is [[91.83653259]
 [91.71513367]
 [91.59423065]
 [91.47274017]
 [91.34677887]].
[2019-04-04 01:00:19,788] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.6967418e-31 3.2324797e-20 2.5466885e-18 1.0000000e+00 7.5789690e-19
 1.2769262e-18 3.3351279e-20], sum to 1.0000
[2019-04-04 01:00:19,789] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0165
[2019-04-04 01:00:19,838] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.0, 25.5, 34.0, 304.0, 26.0, 27.50535370749518, 0.8256300319805696, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4987800.0000, 
sim time next is 4988400.0000, 
raw observation next is [6.666666666666667, 25.33333333333333, 28.33333333333334, 253.3333333333333, 26.0, 27.33733219351777, 0.8477117401566402, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.6472760849492153, 0.2533333333333333, 0.09444444444444447, 0.2799263351749539, 0.6666666666666666, 0.7781110161264809, 0.7825705800522135, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5288347], dtype=float32), -0.8592273]. 
=============================================
[2019-04-04 01:00:22,062] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.3620739e-32 6.6462052e-24 4.8066318e-21 1.0000000e+00 1.8714696e-23
 2.0642659e-20 1.5215295e-22], sum to 1.0000
[2019-04-04 01:00:22,062] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0068
[2019-04-04 01:00:22,082] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 48.0, 0.0, 0.0, 26.0, 25.41394390547854, 0.3549407132981248, 0.0, 1.0, 41809.07381580624], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4234200.0000, 
sim time next is 4234800.0000, 
raw observation next is [2.0, 48.0, 0.0, 0.0, 26.0, 25.39829208549215, 0.3531649025041461, 0.0, 1.0, 48531.51551235069], 
processed observation next is [0.0, 0.0, 0.518005540166205, 0.48, 0.0, 0.0, 0.6666666666666666, 0.6165243404576793, 0.6177216341680487, 0.0, 1.0, 0.2311024548207176], 
reward next is 0.7689, 
noisyNet noise sample is [array([-0.12813705], dtype=float32), 1.3291042]. 
=============================================
[2019-04-04 01:00:23,981] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:00:23,981] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:00:23,984] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run18
[2019-04-04 01:00:27,673] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:00:27,673] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:00:27,677] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run18
[2019-04-04 01:00:29,495] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:00:29,496] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:00:29,500] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run18
[2019-04-04 01:00:32,696] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.47987080e-33 7.00119273e-20 1.10936244e-17 1.00000000e+00
 3.34388735e-23 3.11529479e-19 4.61060106e-23], sum to 1.0000
[2019-04-04 01:00:32,697] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9798
[2019-04-04 01:00:32,734] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [10.86666666666667, 39.33333333333334, 113.6666666666667, 762.8333333333333, 26.0, 27.42520780474619, 0.8111307691832889, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4357200.0000, 
sim time next is 4357800.0000, 
raw observation next is [11.3, 38.0, 115.0, 780.0, 26.0, 27.51645465978666, 0.8248093227614935, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.7756232686980611, 0.38, 0.38333333333333336, 0.861878453038674, 0.6666666666666666, 0.793037888315555, 0.7749364409204978, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.737621], dtype=float32), 1.4947287]. 
=============================================
[2019-04-04 01:00:37,743] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:00:37,743] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:00:37,781] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run18
[2019-04-04 01:00:37,824] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:00:37,825] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:00:37,828] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run18
[2019-04-04 01:00:44,102] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.7213972e-23 1.7335898e-14 7.3745860e-14 1.0000000e+00 1.7519500e-16
 2.1236194e-12 5.2630355e-14], sum to 1.0000
[2019-04-04 01:00:44,103] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8433
[2019-04-04 01:00:44,137] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.7, 67.0, 0.0, 0.0, 24.0, 22.6840857602154, -0.2934812793778566, 0.0, 1.0, 45965.36874353389], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 262800.0000, 
sim time next is 263400.0000, 
raw observation next is [-6.800000000000001, 67.66666666666667, 0.0, 0.0, 24.0, 22.67181998757518, -0.3005852264828151, 0.0, 1.0, 46130.39106084298], 
processed observation next is [1.0, 0.043478260869565216, 0.2742382271468144, 0.6766666666666667, 0.0, 0.0, 0.5, 0.38931833229793167, 0.3998049245057283, 0.0, 1.0, 0.21966852886115704], 
reward next is 0.7803, 
noisyNet noise sample is [array([-1.3615519], dtype=float32), -0.024854941]. 
=============================================
[2019-04-04 01:00:50,171] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.8069904e-21 1.0701407e-13 5.3442119e-11 1.0000000e+00 1.2516583e-13
 2.4881909e-12 1.2138570e-13], sum to 1.0000
[2019-04-04 01:00:50,173] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0524
[2019-04-04 01:00:50,246] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-9.5, 43.0, 82.0, 623.0, 24.0, 24.85542026880065, 0.1335873483606498, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 311400.0000, 
sim time next is 312000.0000, 
raw observation next is [-9.5, 42.66666666666667, 80.0, 598.6666666666667, 24.0, 24.82704536126876, 0.03134146354756694, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.1994459833795014, 0.4266666666666667, 0.26666666666666666, 0.661510128913444, 0.5, 0.5689204467723966, 0.5104471545158557, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4419726], dtype=float32), -1.5133444]. 
=============================================
[2019-04-04 01:00:50,251] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[70.440094]
 [70.72204 ]
 [70.92772 ]
 [71.191124]
 [71.47474 ]], R is [[70.28507996]
 [70.58222961]
 [70.87641144]
 [71.16764832]
 [71.45597076]].
[2019-04-04 01:00:59,875] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:00:59,875] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:00:59,878] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run18
[2019-04-04 01:01:11,381] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:01:11,381] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:01:11,384] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run18
[2019-04-04 01:01:12,786] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.5849137e-28 2.2567681e-16 2.3011276e-14 1.0000000e+00 4.1722432e-18
 8.1818545e-12 7.1525340e-18], sum to 1.0000
[2019-04-04 01:01:12,857] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7731
[2019-04-04 01:01:12,925] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [8.066666666666666, 83.0, 0.0, 0.0, 25.0, 24.45657273538009, 0.1883349768547359, 0.0, 1.0, 69356.09207494682], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 966000.0000, 
sim time next is 966600.0000, 
raw observation next is [8.25, 83.0, 0.0, 0.0, 25.0, 24.39363047745879, 0.2005898797352099, 0.0, 1.0, 85252.85908293285], 
processed observation next is [1.0, 0.17391304347826086, 0.6911357340720222, 0.83, 0.0, 0.0, 0.5833333333333334, 0.5328025397882324, 0.56686329324507, 0.0, 1.0, 0.40596599563301355], 
reward next is 0.5940, 
noisyNet noise sample is [array([-0.09293889], dtype=float32), 0.129715]. 
=============================================
[2019-04-04 01:01:25,317] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:01:25,317] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:01:25,323] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run18
[2019-04-04 01:01:29,174] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.8808527e-25 1.4987975e-15 3.9234695e-14 1.0000000e+00 8.7577871e-18
 1.0828625e-13 3.7110982e-17], sum to 1.0000
[2019-04-04 01:01:29,175] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8097
[2019-04-04 01:01:29,246] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.75, 73.5, 124.0, 0.0, 26.0, 25.23273163166807, 0.1754076120514994, 1.0, 1.0, 23602.60516309991], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 210600.0000, 
sim time next is 211200.0000, 
raw observation next is [-6.566666666666666, 73.0, 128.8333333333333, 0.0, 26.0, 25.18179659951319, 0.1789337984429734, 1.0, 1.0, 53456.06245352139], 
processed observation next is [1.0, 0.43478260869565216, 0.28070175438596495, 0.73, 0.4294444444444443, 0.0, 0.6666666666666666, 0.5984830499594326, 0.5596445994809911, 1.0, 1.0, 0.2545526783501018], 
reward next is 0.7454, 
noisyNet noise sample is [array([-0.20471723], dtype=float32), -0.17792094]. 
=============================================
[2019-04-04 01:01:33,812] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.1149941e-20 1.2272230e-10 6.1902691e-11 9.9958950e-01 1.3968944e-10
 4.1045077e-04 5.2266622e-13], sum to 1.0000
[2019-04-04 01:01:33,812] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1747
[2019-04-04 01:01:33,911] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.816666666666666, 57.16666666666667, 0.0, 0.0, 24.0, 22.98434956570868, -0.1442250726806614, 1.0, 1.0, 147970.4307562446], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 762600.0000, 
sim time next is 763200.0000, 
raw observation next is [-5.0, 58.0, 0.0, 0.0, 24.0, 23.03651668501442, -0.1267850024951875, 1.0, 1.0, 66923.6953201105], 
processed observation next is [1.0, 0.8695652173913043, 0.32409972299168976, 0.58, 0.0, 0.0, 0.5, 0.4197097237512016, 0.4577383325016042, 1.0, 1.0, 0.31868426342909767], 
reward next is 0.6813, 
noisyNet noise sample is [array([0.13236612], dtype=float32), 0.44384593]. 
=============================================
[2019-04-04 01:01:38,160] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:01:38,160] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:01:38,165] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run18
[2019-04-04 01:01:41,281] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.1485070e-27 8.3790363e-14 9.1439050e-14 1.0000000e+00 2.7211486e-20
 1.2406945e-08 3.8687944e-16], sum to 1.0000
[2019-04-04 01:01:41,336] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9558
[2019-04-04 01:01:41,388] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.683333333333334, 87.83333333333334, 0.0, 0.0, 26.0, 25.26991766831391, 0.4114841737233352, 0.0, 1.0, 38035.39307326966], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 954600.0000, 
sim time next is 955200.0000, 
raw observation next is [5.866666666666667, 86.66666666666667, 0.0, 0.0, 26.0, 25.29609988223449, 0.4158280189462833, 0.0, 1.0, 38005.76720794215], 
processed observation next is [1.0, 0.043478260869565216, 0.6251154201292707, 0.8666666666666667, 0.0, 0.0, 0.6666666666666666, 0.6080083235195408, 0.6386093396487611, 0.0, 1.0, 0.18097984384734359], 
reward next is 0.8190, 
noisyNet noise sample is [array([-0.20871025], dtype=float32), -0.3161219]. 
=============================================
[2019-04-04 01:01:52,797] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.8146707e-27 6.8919354e-12 2.7094167e-12 9.9998963e-01 1.1208776e-19
 1.0365884e-05 1.6968967e-17], sum to 1.0000
[2019-04-04 01:01:52,797] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7311
[2019-04-04 01:01:52,847] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [13.1, 62.66666666666667, 0.0, 0.0, 26.0, 25.84929363861049, 0.7027969756978591, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1113600.0000, 
sim time next is 1114200.0000, 
raw observation next is [13.0, 63.0, 0.0, 0.0, 26.0, 25.88002521682872, 0.7012323361074376, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.8227146814404434, 0.63, 0.0, 0.0, 0.6666666666666666, 0.65666876806906, 0.7337441120358125, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1343588], dtype=float32), -0.32039976]. 
=============================================
[2019-04-04 01:01:55,604] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.6623154e-34 1.7161201e-22 2.4579512e-19 1.0000000e+00 5.9717436e-26
 2.6696968e-21 1.5864002e-23], sum to 1.0000
[2019-04-04 01:01:55,604] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0399
[2019-04-04 01:01:55,629] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [13.63333333333333, 81.0, 26.0, 0.1666666666666666, 26.0, 25.65444377614094, 0.6046314142666973, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1153200.0000, 
sim time next is 1153800.0000, 
raw observation next is [14.1, 79.5, 31.0, 0.0, 26.0, 25.64777309647376, 0.6211856652676127, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.8531855955678671, 0.795, 0.10333333333333333, 0.0, 0.6666666666666666, 0.6373144247061466, 0.7070618884225376, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.35022998], dtype=float32), -0.21378545]. 
=============================================
[2019-04-04 01:01:56,311] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.8581956e-29 3.8989352e-13 2.2308919e-17 1.0000000e+00 1.5500609e-21
 2.3996362e-14 9.0604931e-20], sum to 1.0000
[2019-04-04 01:01:56,312] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5035
[2019-04-04 01:01:56,336] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [18.63333333333333, 63.66666666666667, 169.1666666666667, 0.0, 26.0, 25.06760878339589, 0.4960410250142971, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1167600.0000, 
sim time next is 1168200.0000, 
raw observation next is [18.55, 64.0, 171.0, 0.0, 26.0, 25.06022441594892, 0.4984492036282945, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.976454293628809, 0.64, 0.57, 0.0, 0.6666666666666666, 0.5883520346624099, 0.6661497345427648, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1014767], dtype=float32), 0.9031101]. 
=============================================
[2019-04-04 01:02:00,746] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.7869109e-30 9.0298632e-14 3.3468887e-14 1.0000000e+00 1.0574011e-20
 2.9139362e-13 2.3025400e-19], sum to 1.0000
[2019-04-04 01:02:00,746] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9805
[2019-04-04 01:02:00,777] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [14.2, 77.33333333333333, 0.0, 0.0, 26.0, 25.63798730659612, 0.541041275528498, 0.0, 1.0, 131227.4331752819], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1046400.0000, 
sim time next is 1047000.0000, 
raw observation next is [14.3, 77.16666666666667, 0.0, 0.0, 26.0, 25.54818323839955, 0.5591444165871982, 0.0, 1.0, 125542.5804714812], 
processed observation next is [1.0, 0.08695652173913043, 0.8587257617728533, 0.7716666666666667, 0.0, 0.0, 0.6666666666666666, 0.629015269866629, 0.6863814721957328, 0.0, 1.0, 0.5978218117689581], 
reward next is 0.4022, 
noisyNet noise sample is [array([0.33816898], dtype=float32), -0.34870666]. 
=============================================
[2019-04-04 01:02:00,780] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[103.1316  ]
 [103.10537 ]
 [102.99439 ]
 [102.888115]
 [102.82923 ]], R is [[102.8428421 ]
 [102.18952179]
 [102.16762543]
 [102.14595032]
 [102.12448883]].
[2019-04-04 01:02:04,437] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:02:04,437] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:02:04,441] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run18
[2019-04-04 01:02:09,866] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.1030793e-24 6.2447991e-15 7.8887473e-15 1.0000000e+00 7.1810834e-16
 4.3589327e-11 8.7664527e-16], sum to 1.0000
[2019-04-04 01:02:09,877] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8573
[2019-04-04 01:02:09,933] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.5, 60.5, 0.0, 0.0, 26.0, 24.96066356751826, 0.2980282784869804, 0.0, 1.0, 45558.303481328], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 766200.0000, 
sim time next is 766800.0000, 
raw observation next is [-5.6, 61.0, 0.0, 0.0, 26.0, 24.93011746929758, 0.2891779462868398, 0.0, 1.0, 44601.06599741115], 
processed observation next is [1.0, 0.9130434782608695, 0.30747922437673136, 0.61, 0.0, 0.0, 0.6666666666666666, 0.5775097891081318, 0.5963926487622799, 0.0, 1.0, 0.21238602855910071], 
reward next is 0.7876, 
noisyNet noise sample is [array([0.23850606], dtype=float32), -0.6473038]. 
=============================================
[2019-04-04 01:02:10,110] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.6285399e-29 3.9339958e-15 7.6963236e-19 1.0000000e+00 6.4387812e-22
 1.8269934e-13 1.1163702e-19], sum to 1.0000
[2019-04-04 01:02:10,110] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7866
[2019-04-04 01:02:10,114] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [18.71666666666667, 63.33333333333333, 46.0, 0.0, 26.0, 24.98658365929025, 0.4667390459282062, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1181400.0000, 
sim time next is 1182000.0000, 
raw observation next is [18.63333333333333, 63.66666666666667, 37.5, 0.0, 26.0, 24.96827308029314, 0.4618864171974393, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.9787626962142197, 0.6366666666666667, 0.125, 0.0, 0.6666666666666666, 0.5806894233577617, 0.6539621390658131, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.1188107], dtype=float32), 2.3002236]. 
=============================================
[2019-04-04 01:02:10,118] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[77.763985]
 [77.74059 ]
 [77.67834 ]
 [77.63915 ]
 [77.60348 ]], R is [[77.9794693 ]
 [78.19967651]
 [78.41767883]
 [78.63349915]
 [78.84716797]].
[2019-04-04 01:02:10,292] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.1471190e-24 6.7804704e-09 8.5968761e-09 1.0000000e+00 6.0878733e-12
 2.5381433e-11 1.1141109e-12], sum to 1.0000
[2019-04-04 01:02:10,292] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3589
[2019-04-04 01:02:10,339] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [14.4, 75.0, 114.0, 0.0, 26.0, 27.24611225761553, 0.8673639293895086, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1074600.0000, 
sim time next is 1075200.0000, 
raw observation next is [14.76666666666667, 73.33333333333334, 137.3333333333333, 35.83333333333333, 26.0, 27.29046877814564, 0.8851752061682433, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.8716528162511544, 0.7333333333333334, 0.4577777777777776, 0.03959484346224677, 0.6666666666666666, 0.7742057315121368, 0.795058402056081, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1518649], dtype=float32), -0.20620231]. 
=============================================
[2019-04-04 01:02:14,482] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.4157207e-27 5.0379411e-21 5.1414752e-16 1.0000000e+00 1.7465481e-20
 6.5921314e-13 8.9302374e-20], sum to 1.0000
[2019-04-04 01:02:14,482] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2636
[2019-04-04 01:02:14,501] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.399999999999999, 75.0, 0.0, 0.0, 26.0, 24.06787263884637, 0.0535052306503478, 0.0, 1.0, 43761.35650387439], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 618600.0000, 
sim time next is 619200.0000, 
raw observation next is [-4.5, 75.0, 0.0, 0.0, 26.0, 24.01581918629505, 0.04420214259672147, 0.0, 1.0, 43976.56631024884], 
processed observation next is [0.0, 0.17391304347826086, 0.3379501385041552, 0.75, 0.0, 0.0, 0.6666666666666666, 0.5013182655245876, 0.5147340475322405, 0.0, 1.0, 0.20941222052499447], 
reward next is 0.7906, 
noisyNet noise sample is [array([2.4682944], dtype=float32), -0.1747202]. 
=============================================
[2019-04-04 01:02:15,369] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.0223724e-18 1.1058702e-09 5.8667529e-09 8.2178694e-01 9.7838591e-09
 1.7821303e-01 7.3631012e-10], sum to 1.0000
[2019-04-04 01:02:15,369] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9894
[2019-04-04 01:02:15,473] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.3, 86.0, 91.5, 0.0, 22.0, 19.94119878292591, -0.7875284862284743, 0.0, 1.0, 196526.2977925907], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 46800.0000, 
sim time next is 47400.0000, 
raw observation next is [8.200000000000001, 86.0, 90.0, 0.0, 23.0, 19.9990994846436, -0.7170747535476781, 0.0, 1.0, 197777.1067038872], 
processed observation next is [0.0, 0.5652173913043478, 0.6897506925207757, 0.86, 0.3, 0.0, 0.4166666666666667, 0.1665916237203001, 0.260975082150774, 0.0, 1.0, 0.9417957462089867], 
reward next is 0.0582, 
noisyNet noise sample is [array([0.06173525], dtype=float32), -0.8334268]. 
=============================================
[2019-04-04 01:02:21,468] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.8974110e-23 8.9848219e-13 2.4629110e-12 1.0000000e+00 2.4657012e-17
 4.6318033e-10 3.0131266e-14], sum to 1.0000
[2019-04-04 01:02:21,468] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5565
[2019-04-04 01:02:21,502] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.2, 79.83333333333334, 0.0, 0.0, 26.0, 24.82746661028526, 0.2253905653071542, 0.0, 1.0, 39429.10131493116], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 868200.0000, 
sim time next is 868800.0000, 
raw observation next is [-2.1, 79.66666666666667, 0.0, 0.0, 26.0, 24.82017729472067, 0.2191664038326843, 0.0, 1.0, 39401.86118142401], 
processed observation next is [1.0, 0.043478260869565216, 0.404432132963989, 0.7966666666666667, 0.0, 0.0, 0.6666666666666666, 0.5683481078933891, 0.5730554679442281, 0.0, 1.0, 0.18762791038773335], 
reward next is 0.8124, 
noisyNet noise sample is [array([-1.3208896], dtype=float32), 0.8331196]. 
=============================================
[2019-04-04 01:02:26,579] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.9029046e-33 2.1510523e-22 2.1552916e-21 1.0000000e+00 2.9795414e-24
 5.9806817e-18 4.4284947e-23], sum to 1.0000
[2019-04-04 01:02:26,579] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5429
[2019-04-04 01:02:26,632] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [12.7, 83.33333333333333, 11.0, 0.6666666666666667, 26.0, 25.71302622843694, 0.6115705059706682, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1151400.0000, 
sim time next is 1152000.0000, 
raw observation next is [12.7, 84.0, 16.0, 0.5, 26.0, 25.6967281720962, 0.6079072211304959, 0.0, 1.0, 25111.86445797232], 
processed observation next is [0.0, 0.34782608695652173, 0.8144044321329641, 0.84, 0.05333333333333334, 0.0005524861878453039, 0.6666666666666666, 0.64139401434135, 0.7026357403768321, 0.0, 1.0, 0.11958030694272533], 
reward next is 0.8804, 
noisyNet noise sample is [array([-0.27993828], dtype=float32), -0.34408924]. 
=============================================
[2019-04-04 01:02:26,693] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[95.069595]
 [94.76881 ]
 [94.68645 ]
 [94.62482 ]
 [94.54294 ]], R is [[95.07004547]
 [95.11934662]
 [95.16815186]
 [95.12731171]
 [95.08688354]].
[2019-04-04 01:02:27,218] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.6675173e-21 5.4775292e-13 1.2951847e-12 1.0000000e+00 4.3435828e-13
 2.7391697e-10 1.2674363e-14], sum to 1.0000
[2019-04-04 01:02:27,219] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1581
[2019-04-04 01:02:27,284] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.55, 62.5, 0.0, 0.0, 24.0, 23.29356793747352, -0.1457459080879886, 1.0, 1.0, 43600.12119945329], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 153000.0000, 
sim time next is 153600.0000, 
raw observation next is [-7.633333333333333, 63.0, 0.0, 0.0, 24.0, 23.1945498952237, -0.1537326287976248, 1.0, 1.0, 69834.61277816324], 
processed observation next is [1.0, 0.782608695652174, 0.2511542012927055, 0.63, 0.0, 0.0, 0.5, 0.43287915793530846, 0.4487557904007917, 1.0, 1.0, 0.33254577513411065], 
reward next is 0.6675, 
noisyNet noise sample is [array([-1.469717], dtype=float32), -0.47067294]. 
=============================================
[2019-04-04 01:02:48,613] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.6601216e-27 4.0662333e-16 3.1290440e-16 1.0000000e+00 4.4979291e-21
 4.0265927e-13 1.9232371e-17], sum to 1.0000
[2019-04-04 01:02:48,613] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2364
[2019-04-04 01:02:48,647] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.7, 74.0, 0.0, 0.0, 26.0, 25.46390612230567, 0.6891123282241733, 0.0, 1.0, 40118.42881929375], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1544400.0000, 
sim time next is 1545000.0000, 
raw observation next is [7.516666666666667, 74.33333333333334, 0.0, 0.0, 26.0, 25.87500188218392, 0.7031951437236031, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.6708217913204063, 0.7433333333333334, 0.0, 0.0, 0.6666666666666666, 0.65625015684866, 0.734398381241201, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.61517674], dtype=float32), -0.76069397]. 
=============================================
[2019-04-04 01:02:48,662] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[88.92509 ]
 [88.62406 ]
 [88.517006]
 [88.554955]
 [88.37402 ]], R is [[88.93128967]
 [88.85093689]
 [88.13529968]
 [87.30909729]
 [86.49949646]].
[2019-04-04 01:02:55,652] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.0887407e-29 3.1350130e-19 5.7482841e-15 1.0000000e+00 4.4966493e-18
 2.1760257e-15 1.5904261e-20], sum to 1.0000
[2019-04-04 01:02:55,652] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7653
[2019-04-04 01:02:55,689] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [13.8, 100.0, 77.0, 0.0, 26.0, 24.7520290171458, 0.4514896421447948, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1261800.0000, 
sim time next is 1262400.0000, 
raw observation next is [13.8, 100.0, 72.66666666666667, 0.0, 26.0, 24.77210270088641, 0.4498947560260182, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.844875346260388, 1.0, 0.24222222222222223, 0.0, 0.6666666666666666, 0.5643418917405342, 0.6499649186753395, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.7471164], dtype=float32), -0.24189386]. 
=============================================
[2019-04-04 01:03:12,841] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1093387e-29 6.3105033e-17 6.4744618e-17 1.0000000e+00 2.5614758e-20
 1.4224108e-13 5.8220244e-19], sum to 1.0000
[2019-04-04 01:03:12,841] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0724
[2019-04-04 01:03:12,860] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [10.8, 57.5, 75.0, 663.0, 26.0, 26.86852462507566, 0.7757886728065707, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1521000.0000, 
sim time next is 1521600.0000, 
raw observation next is [11.06666666666667, 55.66666666666667, 75.33333333333333, 632.1666666666666, 26.0, 26.9544384340705, 0.791707884189471, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.7691597414589106, 0.5566666666666668, 0.2511111111111111, 0.6985267034990792, 0.6666666666666666, 0.7462032028392084, 0.763902628063157, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.8138306], dtype=float32), 1.2600423]. 
=============================================
[2019-04-04 01:03:15,248] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.8699999e-30 1.6202634e-17 2.3873690e-18 1.0000000e+00 2.0306459e-17
 8.2417505e-19 3.2640630e-21], sum to 1.0000
[2019-04-04 01:03:15,248] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4757
[2019-04-04 01:03:15,280] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [14.4, 81.0, 75.5, 0.0, 26.0, 26.43599648859703, 0.6751357709764233, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1004400.0000, 
sim time next is 1005000.0000, 
raw observation next is [14.58333333333333, 80.0, 69.33333333333333, 0.0, 26.0, 26.56834622024664, 0.7018300715502925, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.8665743305632503, 0.8, 0.2311111111111111, 0.0, 0.6666666666666666, 0.7140288516872199, 0.7339433571834308, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.5094826], dtype=float32), 0.7934147]. 
=============================================
[2019-04-04 01:03:15,360] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[106.71077 ]
 [106.916145]
 [107.33641 ]
 [107.10405 ]
 [106.85332 ]], R is [[106.6644516 ]
 [106.59780884]
 [106.53182983]
 [106.46651459]
 [105.50239563]].
[2019-04-04 01:03:24,154] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.3271618e-21 1.0288757e-12 5.6615476e-12 9.9978620e-01 3.8784709e-13
 2.1382720e-04 2.6006224e-12], sum to 1.0000
[2019-04-04 01:03:24,154] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9382
[2019-04-04 01:03:24,204] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.45, 54.5, 262.0, 74.0, 26.0, 25.69172852613715, 0.3595282096109749, 1.0, 1.0, 9350.50063831302], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2291400.0000, 
sim time next is 2292000.0000, 
raw observation next is [-2.2, 53.33333333333334, 255.1666666666667, 73.16666666666667, 26.0, 25.57696240052286, 0.3270634741217158, 1.0, 1.0, 9349.545816984239], 
processed observation next is [1.0, 0.5217391304347826, 0.4016620498614959, 0.5333333333333334, 0.8505555555555557, 0.08084714548802947, 0.6666666666666666, 0.631413533376905, 0.6090211580405719, 1.0, 1.0, 0.04452164674754399], 
reward next is 0.9555, 
noisyNet noise sample is [array([0.0790524], dtype=float32), -2.1016595]. 
=============================================
[2019-04-04 01:03:24,276] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[89.881546]
 [89.737564]
 [89.61494 ]
 [89.47484 ]
 [89.249214]], R is [[89.57936096]
 [89.63904572]
 [89.65359497]
 [89.66799164]
 [89.68223572]].
[2019-04-04 01:03:42,799] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.5180186e-19 6.5498605e-12 4.1823571e-09 1.0000000e+00 6.1053213e-14
 1.5634767e-09 2.5912663e-11], sum to 1.0000
[2019-04-04 01:03:42,832] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3871
[2019-04-04 01:03:42,925] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.633333333333333, 75.33333333333334, 0.0, 0.0, 24.0, 23.10715400594047, -0.2324747266278255, 0.0, 1.0, 42438.71375746214], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 706800.0000, 
sim time next is 707400.0000, 
raw observation next is [-2.55, 75.5, 0.0, 0.0, 24.0, 23.13814022169018, -0.2350396635806288, 0.0, 1.0, 42398.02538922845], 
processed observation next is [1.0, 0.17391304347826086, 0.3919667590027701, 0.755, 0.0, 0.0, 0.5, 0.4281783518075149, 0.4216534454731237, 0.0, 1.0, 0.20189535899632596], 
reward next is 0.7981, 
noisyNet noise sample is [array([0.6067161], dtype=float32), 1.7773614]. 
=============================================
[2019-04-04 01:03:45,360] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.8409355e-27 8.2143638e-23 3.5753174e-18 1.0000000e+00 4.2954546e-23
 9.9076504e-17 4.5823431e-20], sum to 1.0000
[2019-04-04 01:03:45,360] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4816
[2019-04-04 01:03:45,485] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.7, 78.0, 27.0, 0.0, 26.0, 23.50562216288881, 0.05709980143596749, 0.0, 1.0, 203468.6338762721], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1845000.0000, 
sim time next is 1845600.0000, 
raw observation next is [-6.700000000000001, 78.0, 47.16666666666666, 15.66666666666666, 26.0, 24.17086862706986, 0.1631208807201727, 0.0, 1.0, 161282.7374443913], 
processed observation next is [0.0, 0.34782608695652173, 0.2770083102493075, 0.78, 0.15722222222222218, 0.017311233885819514, 0.6666666666666666, 0.5142390522558218, 0.5543736269067242, 0.0, 1.0, 0.7680130354494824], 
reward next is 0.2320, 
noisyNet noise sample is [array([-1.0801638], dtype=float32), -0.27525157]. 
=============================================
[2019-04-04 01:04:04,108] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.6919852e-21 8.3228265e-14 1.0442773e-12 1.0000000e+00 1.3682433e-12
 2.8410276e-09 6.7530104e-15], sum to 1.0000
[2019-04-04 01:04:04,108] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0373
[2019-04-04 01:04:04,204] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.9, 83.33333333333334, 0.0, 0.0, 24.0, 23.22378949033082, -0.1944571588572917, 1.0, 1.0, 35954.00872453377], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 843600.0000, 
sim time next is 844200.0000, 
raw observation next is [-3.9, 84.0, 0.0, 0.0, 24.0, 23.03257947365096, -0.1964507667215832, 1.0, 1.0, 142186.6003087822], 
processed observation next is [1.0, 0.782608695652174, 0.3545706371191136, 0.84, 0.0, 0.0, 0.5, 0.41938162280424657, 0.43451641109280564, 1.0, 1.0, 0.677079049089439], 
reward next is 0.3229, 
noisyNet noise sample is [array([-0.5292452], dtype=float32), -0.5444395]. 
=============================================
[2019-04-04 01:04:08,997] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.4493207e-27 1.1194924e-18 1.4909273e-16 1.0000000e+00 1.3216481e-18
 6.2593919e-16 3.0808468e-18], sum to 1.0000
[2019-04-04 01:04:08,997] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6260
[2019-04-04 01:04:09,071] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [11.1, 77.0, 0.0, 0.0, 26.0, 25.64969827173706, 0.6305756546434772, 0.0, 1.0, 21711.59108606631], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1134600.0000, 
sim time next is 1135200.0000, 
raw observation next is [11.1, 77.0, 0.0, 0.0, 26.0, 25.65300219922633, 0.630106203992724, 0.0, 1.0, 21082.0897924904], 
processed observation next is [0.0, 0.13043478260869565, 0.7700831024930749, 0.77, 0.0, 0.0, 0.6666666666666666, 0.6377501832688607, 0.710035401330908, 0.0, 1.0, 0.1003909037737638], 
reward next is 0.8996, 
noisyNet noise sample is [array([-0.31474683], dtype=float32), -0.52502203]. 
=============================================
[2019-04-04 01:04:15,913] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.8559358e-24 1.1207321e-17 2.9097745e-18 1.0000000e+00 4.2788802e-19
 6.3812172e-12 1.2677839e-15], sum to 1.0000
[2019-04-04 01:04:15,913] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2529
[2019-04-04 01:04:15,985] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 49.33333333333334, 237.8333333333333, 404.3333333333334, 26.0, 24.97848994393044, 0.3390201947238816, 0.0, 1.0, 18729.13673650162], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2377200.0000, 
sim time next is 2377800.0000, 
raw observation next is [-0.8999999999999999, 50.5, 252.0, 424.0, 26.0, 25.0071048586864, 0.3487691339432951, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.43767313019390586, 0.505, 0.84, 0.4685082872928177, 0.6666666666666666, 0.5839254048905334, 0.6162563779810983, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.80458057], dtype=float32), -0.4117182]. 
=============================================
[2019-04-04 01:04:21,193] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.6915286e-29 9.4729658e-18 1.4099717e-17 1.0000000e+00 6.9840636e-21
 2.5512429e-15 2.8998458e-18], sum to 1.0000
[2019-04-04 01:04:21,193] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6905
[2019-04-04 01:04:21,299] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.333333333333334, 95.33333333333334, 0.0, 0.0, 26.0, 25.60038768576192, 0.5618773885876884, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1664400.0000, 
sim time next is 1665000.0000, 
raw observation next is [5.25, 94.5, 0.0, 0.0, 26.0, 25.65575547411355, 0.548351114499316, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.60803324099723, 0.945, 0.0, 0.0, 0.6666666666666666, 0.637979622842796, 0.6827837048331054, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3632917], dtype=float32), 1.2841082]. 
=============================================
[2019-04-04 01:04:21,314] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[88.22237 ]
 [88.43262 ]
 [88.771225]
 [89.037704]
 [89.21649 ]], R is [[88.18875122]
 [88.30686188]
 [88.33452606]
 [88.22246552]
 [87.96318817]].
[2019-04-04 01:04:23,779] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.0619271e-24 3.9888570e-19 4.2595069e-16 1.0000000e+00 2.9049042e-19
 5.7322521e-16 3.6304034e-17], sum to 1.0000
[2019-04-04 01:04:23,779] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9244
[2019-04-04 01:04:23,829] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.133333333333334, 43.33333333333334, 0.0, 0.0, 26.0, 25.03548610031248, 0.2459190656865807, 0.0, 1.0, 43025.29125322062], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2410800.0000, 
sim time next is 2411400.0000, 
raw observation next is [-4.316666666666666, 43.66666666666667, 0.0, 0.0, 26.0, 24.98803965430824, 0.2370262232736589, 0.0, 1.0, 43019.06283735986], 
processed observation next is [0.0, 0.9130434782608695, 0.34302862419205915, 0.4366666666666667, 0.0, 0.0, 0.6666666666666666, 0.5823366378590199, 0.5790087410912196, 0.0, 1.0, 0.2048526801779041], 
reward next is 0.7951, 
noisyNet noise sample is [array([-0.5842337], dtype=float32), 0.76431507]. 
=============================================
[2019-04-04 01:04:35,231] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.6413414e-21 2.3948642e-11 1.5441570e-11 9.9999917e-01 2.0769450e-13
 8.8380568e-07 4.8746857e-12], sum to 1.0000
[2019-04-04 01:04:35,231] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0378
[2019-04-04 01:04:35,305] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.6, 45.0, 171.0, 64.5, 26.0, 25.78176785692045, 0.4595354304181087, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2296800.0000, 
sim time next is 2297400.0000, 
raw observation next is [-0.3166666666666667, 44.66666666666666, 154.3333333333333, 63.0, 26.0, 26.05343736821079, 0.4835725866508563, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.45383194829178214, 0.44666666666666655, 0.5144444444444443, 0.06961325966850829, 0.6666666666666666, 0.6711197806842325, 0.6611908622169521, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.31976515], dtype=float32), 0.11757861]. 
=============================================
[2019-04-04 01:04:39,058] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.1675807e-25 9.1236308e-17 6.1719505e-15 1.0000000e+00 3.6189251e-18
 6.6278552e-11 2.5206147e-16], sum to 1.0000
[2019-04-04 01:04:39,063] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4220
[2019-04-04 01:04:39,075] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [11.1, 77.0, 0.0, 0.0, 25.0, 24.68223618923061, 0.3998244724066407, 0.0, 1.0, 22843.55375425546], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 1134600.0000, 
sim time next is 1135200.0000, 
raw observation next is [11.1, 77.0, 0.0, 0.0, 25.0, 24.69587071527958, 0.401104260229815, 0.0, 1.0, 18719.85560943948], 
processed observation next is [0.0, 0.13043478260869565, 0.7700831024930749, 0.77, 0.0, 0.0, 0.5833333333333334, 0.5579892262732983, 0.633701420076605, 0.0, 1.0, 0.08914216956875944], 
reward next is 0.9109, 
noisyNet noise sample is [array([1.984149], dtype=float32), -0.28449094]. 
=============================================
[2019-04-04 01:04:47,794] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.8960687e-23 1.1841232e-15 3.8557191e-12 1.0000000e+00 2.2131099e-16
 1.5496275e-12 1.0253245e-15], sum to 1.0000
[2019-04-04 01:04:47,794] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7137
[2019-04-04 01:04:47,832] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.7, 78.0, 0.0, 0.0, 26.0, 24.49827186711158, 0.1599556119023285, 0.0, 1.0, 42362.53995941225], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2170800.0000, 
sim time next is 2171400.0000, 
raw observation next is [-6.700000000000001, 78.0, 0.0, 0.0, 26.0, 24.51259164712354, 0.1552537675898578, 0.0, 1.0, 42295.48598891326], 
processed observation next is [1.0, 0.13043478260869565, 0.2770083102493075, 0.78, 0.0, 0.0, 0.6666666666666666, 0.5427159705936283, 0.5517512558632859, 0.0, 1.0, 0.20140707613768216], 
reward next is 0.7986, 
noisyNet noise sample is [array([-0.25527266], dtype=float32), -0.04606265]. 
=============================================
[2019-04-04 01:04:48,909] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.3625134e-22 5.4238114e-13 7.6865136e-14 1.0000000e+00 3.0591991e-14
 3.9451155e-08 5.6217401e-14], sum to 1.0000
[2019-04-04 01:04:48,910] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3946
[2019-04-04 01:04:48,943] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.266666666666667, 63.0, 164.0, 257.6666666666667, 26.0, 25.75080220548784, 0.3652442688748106, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2630400.0000, 
sim time next is 2631000.0000, 
raw observation next is [-4.083333333333333, 62.5, 176.0, 240.3333333333333, 26.0, 25.73681440745922, 0.3659427251614054, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.34949215143120965, 0.625, 0.5866666666666667, 0.265561694290976, 0.6666666666666666, 0.6447345339549351, 0.6219809083871352, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.22587773], dtype=float32), 0.33330008]. 
=============================================
[2019-04-04 01:04:48,948] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[84.074196]
 [84.20985 ]
 [84.16072 ]
 [84.07708 ]
 [84.08283 ]], R is [[84.17247009]
 [84.33074951]
 [84.48744202]
 [84.6425705 ]
 [84.79614258]].
[2019-04-04 01:04:58,797] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.0072365e-23 1.8140759e-17 4.9638684e-13 1.0000000e+00 1.4072797e-17
 5.0960530e-10 2.0289688e-14], sum to 1.0000
[2019-04-04 01:04:58,797] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7137
[2019-04-04 01:04:58,811] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.2, 79.00000000000001, 0.0, 0.0, 26.0, 23.64565436868814, -0.01041258504905686, 0.0, 1.0, 47093.48540013806], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1833000.0000, 
sim time next is 1833600.0000, 
raw observation next is [-6.2, 79.0, 0.0, 0.0, 26.0, 23.61501851023412, -0.005444429883486086, 0.0, 1.0, 47108.85943252089], 
processed observation next is [0.0, 0.21739130434782608, 0.2908587257617729, 0.79, 0.0, 0.0, 0.6666666666666666, 0.4679182091861766, 0.49818519003883793, 0.0, 1.0, 0.2243279020596233], 
reward next is 0.7757, 
noisyNet noise sample is [array([0.91628814], dtype=float32), -1.1087136]. 
=============================================
[2019-04-04 01:05:14,516] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.1433881e-21 6.5860136e-16 1.0197198e-14 1.0000000e+00 2.1952174e-16
 1.2697673e-09 5.4417848e-15], sum to 1.0000
[2019-04-04 01:05:14,516] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7462
[2019-04-04 01:05:14,539] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.95, 43.0, 0.0, 0.0, 26.0, 25.08273740930848, 0.2546779955890766, 0.0, 1.0, 43029.60066967836], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2410200.0000, 
sim time next is 2410800.0000, 
raw observation next is [-4.133333333333334, 43.33333333333334, 0.0, 0.0, 26.0, 25.03532398158132, 0.2458797998079546, 0.0, 1.0, 43025.41531673247], 
processed observation next is [0.0, 0.9130434782608695, 0.34810710987996313, 0.4333333333333334, 0.0, 0.0, 0.6666666666666666, 0.58627699846511, 0.5819599332693182, 0.0, 1.0, 0.20488293007967842], 
reward next is 0.7951, 
noisyNet noise sample is [array([-0.6934795], dtype=float32), -0.06880225]. 
=============================================
[2019-04-04 01:05:24,188] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.3994527e-25 3.3540393e-18 9.4950805e-16 1.0000000e+00 1.7597707e-19
 2.2050096e-18 1.0693902e-16], sum to 1.0000
[2019-04-04 01:05:24,188] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8333
[2019-04-04 01:05:24,212] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 60.0, 98.0, 737.0, 26.0, 25.17887106220188, 0.4111749143149208, 0.0, 1.0, 18706.68236647244], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2990400.0000, 
sim time next is 2991000.0000, 
raw observation next is [-2.0, 60.0, 95.0, 723.0, 26.0, 25.14808768150025, 0.407444469561183, 0.0, 1.0, 25332.34138756968], 
processed observation next is [0.0, 0.6086956521739131, 0.40720221606648205, 0.6, 0.31666666666666665, 0.7988950276243094, 0.6666666666666666, 0.5956739734583542, 0.635814823187061, 0.0, 1.0, 0.12063019708366514], 
reward next is 0.8794, 
noisyNet noise sample is [array([1.4317346], dtype=float32), -0.94335765]. 
=============================================
[2019-04-04 01:05:24,220] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[78.19445 ]
 [78.35956 ]
 [78.47841 ]
 [78.572495]
 [78.70578 ]], R is [[78.26554108]
 [78.39380646]
 [78.60987091]
 [78.73467255]
 [78.85821533]].
[2019-04-04 01:05:25,625] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4062053e-26 1.3281794e-16 7.6551223e-20 1.0000000e+00 5.6426497e-16
 6.8344899e-15 1.3165206e-18], sum to 1.0000
[2019-04-04 01:05:25,626] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7502
[2019-04-04 01:05:25,651] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.1, 29.0, 77.33333333333333, 193.5, 26.0, 24.83979607239821, 0.3204284490590025, 1.0, 1.0, 196217.909419296], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2564400.0000, 
sim time next is 2565000.0000, 
raw observation next is [3.0, 29.0, 70.0, 162.0, 26.0, 25.27829650611213, 0.3505429956306195, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.5457063711911359, 0.29, 0.23333333333333334, 0.17900552486187846, 0.6666666666666666, 0.6065247088426776, 0.6168476652102065, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.93092847], dtype=float32), 1.1019509]. 
=============================================
[2019-04-04 01:05:25,681] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[84.81669 ]
 [84.389114]
 [84.4203  ]
 [84.684944]
 [84.95833 ]], R is [[84.74324036]
 [83.96143341]
 [83.98867798]
 [84.14878845]
 [84.30730438]].
[2019-04-04 01:05:26,773] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.2183509e-25 7.3704256e-21 3.6109703e-17 1.0000000e+00 2.6236101e-20
 5.0256811e-19 6.4451589e-18], sum to 1.0000
[2019-04-04 01:05:26,773] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7850
[2019-04-04 01:05:26,791] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.483333333333334, 60.83333333333334, 0.0, 0.0, 26.0, 23.35181871248332, -0.1199306643461645, 0.0, 1.0, 44333.77417171908], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2437800.0000, 
sim time next is 2438400.0000, 
raw observation next is [-8.566666666666666, 60.66666666666667, 0.0, 0.0, 26.0, 23.32129655485999, -0.1268380226844066, 0.0, 1.0, 44292.31263392385], 
processed observation next is [0.0, 0.21739130434782608, 0.22530009233610343, 0.6066666666666667, 0.0, 0.0, 0.6666666666666666, 0.4434413795716659, 0.4577206591051978, 0.0, 1.0, 0.21091577444725643], 
reward next is 0.7891, 
noisyNet noise sample is [array([-0.07913249], dtype=float32), -0.09475057]. 
=============================================
[2019-04-04 01:05:35,317] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.6544084e-24 2.0991475e-15 6.0876550e-15 1.0000000e+00 5.7322086e-16
 2.5118132e-12 1.0335481e-14], sum to 1.0000
[2019-04-04 01:05:35,319] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2209
[2019-04-04 01:05:35,344] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.5, 64.0, 112.0, 781.0, 26.0, 26.033823016073, 0.4770825572276076, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2719800.0000, 
sim time next is 2720400.0000, 
raw observation next is [-8.333333333333334, 64.0, 112.1666666666667, 784.0, 26.0, 25.99634032777746, 0.4760688833187641, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.23176361957525393, 0.64, 0.373888888888889, 0.8662983425414365, 0.6666666666666666, 0.6663616939814551, 0.6586896277729214, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3559612], dtype=float32), 1.043374]. 
=============================================
[2019-04-04 01:05:37,303] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.6579398e-24 3.1257392e-16 4.4057127e-16 1.0000000e+00 2.8108001e-17
 2.2960347e-09 2.6553837e-15], sum to 1.0000
[2019-04-04 01:05:37,304] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7707
[2019-04-04 01:05:37,377] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-14.16666666666667, 89.66666666666667, 85.66666666666667, 424.0, 26.0, 26.02387744580084, 0.4276904710413622, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2710200.0000, 
sim time next is 2710800.0000, 
raw observation next is [-14.0, 91.0, 88.5, 471.0, 26.0, 26.08031215106807, 0.423904479795562, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.07479224376731301, 0.91, 0.295, 0.5204419889502763, 0.6666666666666666, 0.6733593459223393, 0.6413014932651874, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7584818], dtype=float32), 0.06124633]. 
=============================================
[2019-04-04 01:05:39,202] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.22247207e-21 5.84745739e-14 4.14767699e-11 1.00000000e+00
 1.09119835e-15 5.48283952e-09 1.28619285e-14], sum to 1.0000
[2019-04-04 01:05:39,208] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0303
[2019-04-04 01:05:39,271] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.7, 83.0, 0.0, 0.0, 26.0, 25.21743246658303, 0.3976406148815008, 0.0, 1.0, 42206.7550276419], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2152800.0000, 
sim time next is 2153400.0000, 
raw observation next is [-6.800000000000001, 82.83333333333334, 0.0, 0.0, 26.0, 25.19153468016379, 0.3491039573280801, 0.0, 1.0, 42519.41389330066], 
processed observation next is [1.0, 0.9565217391304348, 0.2742382271468144, 0.8283333333333335, 0.0, 0.0, 0.6666666666666666, 0.599294556680316, 0.6163679857760267, 0.0, 1.0, 0.2024733994919079], 
reward next is 0.7975, 
noisyNet noise sample is [array([0.02101847], dtype=float32), 0.63727665]. 
=============================================
[2019-04-04 01:05:43,645] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.02974806e-26 4.20422220e-21 3.65179147e-18 1.00000000e+00
 6.65514210e-22 1.34776739e-17 4.98541031e-19], sum to 1.0000
[2019-04-04 01:05:43,645] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9316
[2019-04-04 01:05:43,667] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 64.0, 42.0, 214.5, 26.0, 23.55990319350811, -0.02588781647924058, 0.0, 1.0, 40645.81319289443], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3052800.0000, 
sim time next is 3053400.0000, 
raw observation next is [-6.0, 63.16666666666667, 55.66666666666668, 262.6666666666667, 26.0, 23.59626688894883, -0.0103038918506459, 0.0, 1.0, 40458.15795546132], 
processed observation next is [0.0, 0.34782608695652173, 0.296398891966759, 0.6316666666666667, 0.18555555555555558, 0.29023941068139963, 0.6666666666666666, 0.4663555740790691, 0.496565369383118, 0.0, 1.0, 0.19265789502600628], 
reward next is 0.8073, 
noisyNet noise sample is [array([0.42083886], dtype=float32), -2.062172]. 
=============================================
[2019-04-04 01:05:46,724] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.2751615e-23 2.4137400e-12 5.4765361e-13 9.9999988e-01 9.7282024e-13
 9.3870391e-08 1.4138348e-15], sum to 1.0000
[2019-04-04 01:05:46,724] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0857
[2019-04-04 01:05:46,751] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.833333333333333, 28.33333333333334, 26.99999999999999, 56.0, 26.0, 25.7595381888477, 0.3971209892336132, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2826600.0000, 
sim time next is 2827200.0000, 
raw observation next is [5.666666666666666, 28.66666666666667, 16.0, 51.0, 26.0, 25.82035715788949, 0.3766784933971404, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.6195752539242845, 0.28666666666666674, 0.05333333333333334, 0.056353591160221, 0.6666666666666666, 0.6516964298241241, 0.6255594977990467, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1200908], dtype=float32), 1.1791537]. 
=============================================
[2019-04-04 01:05:47,656] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.3530958e-22 1.2176206e-12 3.9702013e-14 9.9999964e-01 5.9287671e-13
 3.1158535e-07 4.5736147e-14], sum to 1.0000
[2019-04-04 01:05:47,660] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9445
[2019-04-04 01:05:47,709] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 65.5, 46.0, 0.0, 26.0, 25.38679859872904, 0.4328805444617909, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2131800.0000, 
sim time next is 2132400.0000, 
raw observation next is [-4.5, 66.0, 36.0, 0.0, 26.0, 25.83531603402085, 0.4591824988334284, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.3379501385041552, 0.66, 0.12, 0.0, 0.6666666666666666, 0.6529430028350708, 0.6530608329444761, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.12231327], dtype=float32), 2.4421577]. 
=============================================
[2019-04-04 01:05:55,408] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.4187911e-24 4.2614386e-17 2.2240551e-14 1.0000000e+00 6.1672138e-17
 3.0768159e-12 5.5695127e-16], sum to 1.0000
[2019-04-04 01:05:55,408] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3911
[2019-04-04 01:05:55,438] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 84.0, 0.0, 0.0, 26.0, 24.37448035221565, 0.2082843166255128, 0.0, 1.0, 42967.12803655701], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2953800.0000, 
sim time next is 2954400.0000, 
raw observation next is [-3.0, 84.0, 0.0, 0.0, 26.0, 24.34404148095724, 0.2018397138617077, 0.0, 1.0, 42971.54468726517], 
processed observation next is [0.0, 0.17391304347826086, 0.3795013850415513, 0.84, 0.0, 0.0, 0.6666666666666666, 0.5286701234131034, 0.5672799046205692, 0.0, 1.0, 0.20462640327269127], 
reward next is 0.7954, 
noisyNet noise sample is [array([1.5726727], dtype=float32), -1.4890598]. 
=============================================
[2019-04-04 01:06:01,229] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.1051356e-23 2.4036788e-12 6.0426371e-13 1.0000000e+00 3.3125417e-15
 2.1971113e-08 7.0977978e-13], sum to 1.0000
[2019-04-04 01:06:01,229] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2073
[2019-04-04 01:06:01,270] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 100.0, 63.5, 0.0, 26.0, 25.43274737194858, 0.3044532570484235, 1.0, 1.0, 18680.81852495727], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2887200.0000, 
sim time next is 2887800.0000, 
raw observation next is [0.1666666666666667, 98.83333333333334, 68.33333333333334, 0.0, 26.0, 25.41579385271889, 0.3059617298696837, 1.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.43478260869565216, 0.4672206832871654, 0.9883333333333334, 0.2277777777777778, 0.0, 0.6666666666666666, 0.6179828210599073, 0.6019872432898946, 1.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.05316239], dtype=float32), -1.5411947]. 
=============================================
[2019-04-04 01:06:18,230] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6360497e-20 3.3556654e-11 3.2927952e-11 9.9997377e-01 1.3894189e-13
 2.6280337e-05 1.0669745e-12], sum to 1.0000
[2019-04-04 01:06:18,230] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6861
[2019-04-04 01:06:18,261] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 83.83333333333334, 0.0, 0.0, 26.0, 25.26908667651514, 0.488895987370339, 0.0, 1.0, 50436.12190603395], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2926200.0000, 
sim time next is 2926800.0000, 
raw observation next is [-1.0, 85.0, 0.0, 0.0, 26.0, 25.39526910475397, 0.4969711581039364, 0.0, 1.0, 33712.03571022642], 
processed observation next is [1.0, 0.9130434782608695, 0.4349030470914128, 0.85, 0.0, 0.0, 0.6666666666666666, 0.6162724253961643, 0.6656570527013121, 0.0, 1.0, 0.16053350338203057], 
reward next is 0.8395, 
noisyNet noise sample is [array([0.41326374], dtype=float32), 1.6084119]. 
=============================================
[2019-04-04 01:06:20,995] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.0668794e-21 2.0844762e-11 1.9242030e-11 9.9999940e-01 1.6458046e-15
 6.1245197e-07 3.5462241e-12], sum to 1.0000
[2019-04-04 01:06:20,998] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3685
[2019-04-04 01:06:21,017] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.3537185577275, 0.4436272549694203, 0.0, 1.0, 56437.03714848099], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3888000.0000, 
sim time next is 3888600.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.32963021895982, 0.4405293473916509, 0.0, 1.0, 47035.98454960285], 
processed observation next is [1.0, 0.0, 0.40720221606648205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6108025182466518, 0.646843115797217, 0.0, 1.0, 0.22398087880763262], 
reward next is 0.7760, 
noisyNet noise sample is [array([0.5267561], dtype=float32), -0.018913304]. 
=============================================
[2019-04-04 01:06:22,592] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.6406159e-22 1.8061767e-13 2.0004580e-12 1.0000000e+00 1.9442252e-15
 2.5061475e-08 3.8478027e-14], sum to 1.0000
[2019-04-04 01:06:22,592] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5431
[2019-04-04 01:06:22,636] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 76.0, 0.0, 0.0, 26.0, 25.44486376548257, 0.5611676948222386, 0.0, 1.0, 133646.6483442676], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3270600.0000, 
sim time next is 3271200.0000, 
raw observation next is [-4.666666666666666, 77.66666666666667, 0.0, 0.0, 26.0, 25.43835054168396, 0.5765680684346806, 0.0, 1.0, 81434.17428652363], 
processed observation next is [1.0, 0.8695652173913043, 0.33333333333333337, 0.7766666666666667, 0.0, 0.0, 0.6666666666666666, 0.61986254514033, 0.6921893561448935, 0.0, 1.0, 0.3877817823167792], 
reward next is 0.6122, 
noisyNet noise sample is [array([-1.0114532], dtype=float32), 0.48812908]. 
=============================================
[2019-04-04 01:06:22,780] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-04 01:06:22,791] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 01:06:22,791] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 01:06:22,791] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:06:22,791] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 01:06:22,793] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:06:22,791] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:06:22,795] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run25
[2019-04-04 01:06:22,813] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run25
[2019-04-04 01:06:22,828] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run25
[2019-04-04 01:07:54,249] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.8054561], dtype=float32), 0.4373115]
[2019-04-04 01:07:54,249] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-1.541237552333333, 58.06988197, 23.45526998666667, 591.4040949, 25.0, 25.61178702894939, 0.4403350714600358, 1.0, 1.0, 0.0]
[2019-04-04 01:07:54,249] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 01:07:54,250] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.41118886e-23 1.08624306e-16 1.25587593e-14 1.00000000e+00
 4.63933974e-16 1.62962973e-13 6.56569489e-16], sampled 0.0006600012555608714
[2019-04-04 01:08:07,775] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7440.6538 218388497.4575 714.0625
[2019-04-04 01:08:39,657] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7315.7688 243691760.9449 588.3002
[2019-04-04 01:08:44,940] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7260.7246 256299445.0903 293.8160
[2019-04-04 01:08:45,978] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 2400000, evaluation results [2400000.0, 7315.768821096713, 243691760.94486234, 588.3002196579296, 7440.653815711956, 218388497.4574989, 714.0625366063027, 7260.7245766618735, 256299445.0902545, 293.8160098576712]
[2019-04-04 01:08:46,710] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.5197705e-26 8.9164644e-17 1.1078448e-16 1.0000000e+00 1.5683237e-17
 3.5772693e-10 3.0048345e-17], sum to 1.0000
[2019-04-04 01:08:46,710] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3112
[2019-04-04 01:08:46,750] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.26820810998661, 0.3519765923540665, 0.0, 1.0, 47732.19232841785], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3722400.0000, 
sim time next is 3723000.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.31658280376754, 0.3528623578511403, 0.0, 1.0, 42944.71901902523], 
processed observation next is [1.0, 0.08695652173913043, 0.3795013850415513, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6097152336472952, 0.6176207859503801, 0.0, 1.0, 0.20449866199535824], 
reward next is 0.7955, 
noisyNet noise sample is [array([-0.7220822], dtype=float32), -0.7065694]. 
=============================================
[2019-04-04 01:08:46,796] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[87.053986]
 [87.01289 ]
 [86.933174]
 [86.76425 ]
 [86.71938 ]], R is [[87.06043243]
 [86.96253204]
 [86.8021698 ]
 [86.52490997]
 [86.35173798]].
[2019-04-04 01:09:14,960] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.2716128e-26 6.1678287e-16 1.3917084e-15 1.0000000e+00 7.7336490e-17
 5.8383133e-16 1.2222560e-16], sum to 1.0000
[2019-04-04 01:09:14,960] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3826
[2019-04-04 01:09:15,029] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.0, 35.0, 213.5, 429.0, 26.0, 24.53247781116334, 0.2878993267457603, 1.0, 1.0, 196217.9094192959], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2811600.0000, 
sim time next is 2812200.0000, 
raw observation next is [4.333333333333334, 34.16666666666667, 225.3333333333333, 343.6666666666666, 26.0, 24.82358914400068, 0.3719089884107497, 1.0, 1.0, 119036.3807049319], 
processed observation next is [1.0, 0.5652173913043478, 0.58264081255771, 0.34166666666666673, 0.751111111111111, 0.37974217311233877, 0.6666666666666666, 0.5686324286667235, 0.6239696628035832, 1.0, 1.0, 0.5668399081187233], 
reward next is 0.4332, 
noisyNet noise sample is [array([1.2962322], dtype=float32), 0.8224605]. 
=============================================
[2019-04-04 01:09:20,227] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.2141929e-24 6.0136486e-14 5.8276420e-13 1.0000000e+00 1.0427232e-15
 1.0614033e-10 8.0764889e-18], sum to 1.0000
[2019-04-04 01:09:20,227] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0014
[2019-04-04 01:09:20,268] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.666666666666667, 50.0, 249.8333333333333, 58.83333333333333, 26.0, 26.07559853908306, 0.5724836785332171, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4542000.0000, 
sim time next is 4542600.0000, 
raw observation next is [2.833333333333333, 49.5, 252.6666666666667, 69.66666666666666, 26.0, 26.19064175976543, 0.5861291239737395, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.541089566020314, 0.495, 0.8422222222222224, 0.07697974217311233, 0.6666666666666666, 0.6825534799804526, 0.6953763746579131, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.30487713], dtype=float32), -0.21182737]. 
=============================================
[2019-04-04 01:09:22,651] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.41797057e-25 8.38445055e-19 1.86659936e-16 1.00000000e+00
 1.46886339e-19 1.22864245e-11 5.25920441e-17], sum to 1.0000
[2019-04-04 01:09:22,658] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0170
[2019-04-04 01:09:22,727] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 93.33333333333334, 0.0, 0.0, 26.0, 25.52151478475839, 0.3645676028614246, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3096600.0000, 
sim time next is 3097200.0000, 
raw observation next is [-1.0, 94.66666666666667, 0.0, 0.0, 26.0, 25.50463230539388, 0.355690749446373, 0.0, 1.0, 18751.8726136592], 
processed observation next is [0.0, 0.8695652173913043, 0.4349030470914128, 0.9466666666666668, 0.0, 0.0, 0.6666666666666666, 0.62538602544949, 0.618563583148791, 0.0, 1.0, 0.08929463149361525], 
reward next is 0.9107, 
noisyNet noise sample is [array([0.9775043], dtype=float32), -0.28064233]. 
=============================================
[2019-04-04 01:09:26,082] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.9605502e-25 7.5058930e-14 3.1558116e-16 1.0000000e+00 1.5757013e-15
 7.8763520e-09 6.4947959e-17], sum to 1.0000
[2019-04-04 01:09:26,083] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5044
[2019-04-04 01:09:26,131] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 93.0, 82.5, 0.0, 26.0, 25.45361233430659, 0.325949820450791, 1.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2890800.0000, 
sim time next is 2891400.0000, 
raw observation next is [1.0, 94.16666666666666, 84.0, 0.0, 26.0, 25.46512647691016, 0.3221370677409177, 1.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.4782608695652174, 0.4903047091412743, 0.9416666666666665, 0.28, 0.0, 0.6666666666666666, 0.6220938730758467, 0.6073790225803058, 1.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-1.0664926], dtype=float32), 1.9221625]. 
=============================================
[2019-04-04 01:09:28,772] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.3910896e-27 7.1317121e-20 4.8172493e-18 1.0000000e+00 3.7988234e-20
 1.1919407e-15 1.7217760e-18], sum to 1.0000
[2019-04-04 01:09:28,772] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0980
[2019-04-04 01:09:28,799] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 49.0, 18.33333333333333, 8.833333333333332, 26.0, 25.32264133618043, 0.3202055926922069, 0.0, 1.0, 39123.36808163523], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4261200.0000, 
sim time next is 4261800.0000, 
raw observation next is [3.0, 49.0, 36.66666666666666, 17.66666666666666, 26.0, 25.3184201559124, 0.3238035564525794, 0.0, 1.0, 39074.20430722988], 
processed observation next is [0.0, 0.30434782608695654, 0.5457063711911359, 0.49, 0.12222222222222219, 0.01952117863720073, 0.6666666666666666, 0.6098683463260333, 0.6079345188175265, 0.0, 1.0, 0.18606763955823752], 
reward next is 0.8139, 
noisyNet noise sample is [array([-0.5495608], dtype=float32), -1.2538584]. 
=============================================
[2019-04-04 01:09:37,002] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.8890979e-27 2.1333261e-16 4.4614395e-16 1.0000000e+00 4.5755181e-18
 1.0977194e-08 1.6647477e-16], sum to 1.0000
[2019-04-04 01:09:37,002] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6806
[2019-04-04 01:09:37,052] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.833333333333333, 100.0, 0.0, 0.0, 26.0, 25.299862780231, 0.4869737238847069, 0.0, 1.0, 40613.96516648969], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3217800.0000, 
sim time next is 3218400.0000, 
raw observation next is [-3.0, 100.0, 0.0, 0.0, 26.0, 25.30832605811382, 0.4896546090975618, 0.0, 1.0, 40654.58199069299], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 1.0, 0.0, 0.0, 0.6666666666666666, 0.609027171509485, 0.6632182030325205, 0.0, 1.0, 0.1935932475747285], 
reward next is 0.8064, 
noisyNet noise sample is [array([0.63905287], dtype=float32), -0.25003114]. 
=============================================
[2019-04-04 01:09:39,156] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.2145107e-27 2.3533404e-21 6.4495334e-19 1.0000000e+00 5.1504724e-21
 5.6259954e-15 1.2596110e-18], sum to 1.0000
[2019-04-04 01:09:39,160] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6056
[2019-04-04 01:09:39,226] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 60.0, 92.0, 709.0, 26.0, 25.11951198130225, 0.4082948392194926, 0.0, 1.0, 38941.26198704973], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2991600.0000, 
sim time next is 2992200.0000, 
raw observation next is [-1.833333333333333, 59.16666666666666, 89.0, 695.0, 26.0, 25.1052777846311, 0.4113537750134078, 0.0, 1.0, 34831.6492498919], 
processed observation next is [0.0, 0.6521739130434783, 0.41181902123730385, 0.5916666666666666, 0.2966666666666667, 0.7679558011049724, 0.6666666666666666, 0.5921064820525915, 0.6371179250044693, 0.0, 1.0, 0.1658649964280567], 
reward next is 0.8341, 
noisyNet noise sample is [array([1.8529801], dtype=float32), -0.574628]. 
=============================================
[2019-04-04 01:09:43,700] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.9964902e-23 4.2153040e-13 1.3859907e-13 1.0000000e+00 1.8677053e-14
 1.9155979e-12 2.8709477e-14], sum to 1.0000
[2019-04-04 01:09:43,701] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6188
[2019-04-04 01:09:43,785] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 35.0, 19.16666666666667, 47.5, 26.0, 26.50212081981737, 0.6801624320780539, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4124400.0000, 
sim time next is 4125000.0000, 
raw observation next is [3.0, 34.5, 15.33333333333334, 38.00000000000001, 26.0, 26.50602761192136, 0.6799468416164927, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.5457063711911359, 0.345, 0.05111111111111113, 0.0419889502762431, 0.6666666666666666, 0.7088356343267801, 0.7266489472054976, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.98612946], dtype=float32), 1.9657826]. 
=============================================
[2019-04-04 01:09:43,853] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[80.83118 ]
 [80.33655 ]
 [80.411575]
 [80.40544 ]
 [80.632835]], R is [[81.29869843]
 [81.48571014]
 [81.67085266]
 [81.85414124]
 [82.03559875]].
[2019-04-04 01:09:48,108] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.2167388e-27 4.5335871e-21 1.0298303e-19 1.0000000e+00 1.7303735e-20
 1.5439798e-17 4.8236252e-20], sum to 1.0000
[2019-04-04 01:09:48,108] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6810
[2019-04-04 01:09:48,145] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.666666666666667, 52.66666666666667, 0.0, 0.0, 26.0, 25.07883207773865, 0.3133510256215601, 0.0, 1.0, 39458.02352600822], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4164000.0000, 
sim time next is 4164600.0000, 
raw observation next is [-3.833333333333333, 53.33333333333333, 0.0, 0.0, 26.0, 25.03886688179523, 0.30735950517371, 0.0, 1.0, 39449.3742286233], 
processed observation next is [0.0, 0.17391304347826086, 0.3564173591874424, 0.5333333333333333, 0.0, 0.0, 0.6666666666666666, 0.5865722401496024, 0.6024531683912366, 0.0, 1.0, 0.1878541629934443], 
reward next is 0.8121, 
noisyNet noise sample is [array([-2.2138], dtype=float32), 1.0002854]. 
=============================================
[2019-04-04 01:10:06,716] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.3947602e-23 2.7043810e-15 2.0057945e-13 1.0000000e+00 2.5413715e-17
 9.6102924e-11 1.2600599e-15], sum to 1.0000
[2019-04-04 01:10:06,727] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3853
[2019-04-04 01:10:06,758] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.45410625830962, 0.4930845095057195, 0.0, 1.0, 71363.99660297186], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3361800.0000, 
sim time next is 3362400.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.40980512704072, 0.5007251447154893, 0.0, 1.0, 77217.31015499444], 
processed observation next is [1.0, 0.9565217391304348, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6174837605867266, 0.6669083815718299, 0.0, 1.0, 0.36770147692854493], 
reward next is 0.6323, 
noisyNet noise sample is [array([0.18080956], dtype=float32), -2.4281933]. 
=============================================
[2019-04-04 01:10:08,199] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3399275e-27 1.2488818e-14 3.9071892e-14 1.0000000e+00 1.5539344e-19
 3.0912746e-12 2.6543177e-17], sum to 1.0000
[2019-04-04 01:10:08,199] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0127
[2019-04-04 01:10:08,209] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [13.8, 31.0, 120.0, 828.0, 26.0, 27.85682888911943, 0.9431828828571377, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4361400.0000, 
sim time next is 4362000.0000, 
raw observation next is [14.2, 30.0, 119.6666666666667, 832.1666666666666, 26.0, 27.95673320189018, 0.9694978286952661, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.8559556786703602, 0.3, 0.398888888888889, 0.9195211786372007, 0.6666666666666666, 0.8297277668241817, 0.823165942898422, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.47137928], dtype=float32), 1.2968625]. 
=============================================
[2019-04-04 01:10:08,267] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[88.672554]
 [88.89159 ]
 [89.09859 ]
 [89.34475 ]
 [89.65011 ]], R is [[88.51488495]
 [88.62973785]
 [88.74343872]
 [88.85600281]
 [88.96744537]].
[2019-04-04 01:10:11,946] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.1780149e-26 2.1705013e-14 5.0534440e-14 1.0000000e+00 5.7296478e-17
 5.3107350e-13 4.1907484e-17], sum to 1.0000
[2019-04-04 01:10:11,947] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9264
[2019-04-04 01:10:11,961] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [8.2, 62.33333333333334, 0.0, 0.0, 26.0, 25.69804457609294, 0.5830388107395839, 0.0, 1.0, 196217.9094192961], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4404000.0000, 
sim time next is 4404600.0000, 
raw observation next is [8.05, 62.5, 0.0, 0.0, 26.0, 25.64182709101944, 0.5986691829377929, 0.0, 1.0, 196217.9094192961], 
processed observation next is [1.0, 1.0, 0.6855955678670361, 0.625, 0.0, 0.0, 0.6666666666666666, 0.63681892425162, 0.6995563943125976, 0.0, 1.0, 0.9343709972347434], 
reward next is 0.0656, 
noisyNet noise sample is [array([0.21741293], dtype=float32), -0.16967838]. 
=============================================
[2019-04-04 01:10:21,116] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.5422907e-26 3.1512765e-16 4.9971280e-16 1.0000000e+00 2.1746023e-19
 7.5633380e-13 2.2263096e-17], sum to 1.0000
[2019-04-04 01:10:21,116] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7227
[2019-04-04 01:10:21,150] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.6666666666666666, 73.0, 0.0, 0.0, 26.0, 25.28615792716076, 0.4469861873018406, 0.0, 1.0, 44357.91784710548], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4500600.0000, 
sim time next is 4501200.0000, 
raw observation next is [-0.7333333333333334, 73.0, 0.0, 0.0, 26.0, 25.33885906602432, 0.4438235217205223, 0.0, 1.0, 43165.02142796417], 
processed observation next is [1.0, 0.08695652173913043, 0.44228993536472766, 0.73, 0.0, 0.0, 0.6666666666666666, 0.61157158883536, 0.6479411739068408, 0.0, 1.0, 0.20554772108554367], 
reward next is 0.7945, 
noisyNet noise sample is [array([0.33032107], dtype=float32), -1.015768]. 
=============================================
[2019-04-04 01:10:21,419] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:10:21,419] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:10:21,468] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run19
[2019-04-04 01:10:26,523] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.31436454e-23 9.67206001e-15 3.23692557e-14 1.00000000e+00
 1.44803681e-15 3.52607366e-08 1.86126766e-15], sum to 1.0000
[2019-04-04 01:10:26,523] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5452
[2019-04-04 01:10:26,560] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.5, 69.0, 0.0, 0.0, 26.0, 25.41596714110353, 0.4051910841534459, 0.0, 1.0, 51551.6133787604], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4593600.0000, 
sim time next is 4594200.0000, 
raw observation next is [-1.583333333333333, 69.33333333333333, 0.0, 0.0, 26.0, 25.34915849724785, 0.4013062267275433, 0.0, 1.0, 66966.71728149321], 
processed observation next is [1.0, 0.17391304347826086, 0.4187442289935365, 0.6933333333333332, 0.0, 0.0, 0.6666666666666666, 0.6124298747706541, 0.6337687422425144, 0.0, 1.0, 0.31888912991187246], 
reward next is 0.6811, 
noisyNet noise sample is [array([-1.006692], dtype=float32), -1.0627378]. 
=============================================
[2019-04-04 01:10:32,568] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.69483530e-27 1.02674637e-13 2.40496837e-15 1.00000000e+00
 3.28636025e-16 2.16365639e-14 1.02767735e-16], sum to 1.0000
[2019-04-04 01:10:32,579] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0133
[2019-04-04 01:10:32,591] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [14.0, 31.83333333333333, 164.3333333333334, 419.3333333333334, 26.0, 28.11013359615735, 1.102538603385856, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4373400.0000, 
sim time next is 4374000.0000, 
raw observation next is [13.9, 32.0, 149.0, 314.5, 26.0, 28.38996245216211, 1.118782809580581, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.847645429362881, 0.32, 0.49666666666666665, 0.3475138121546961, 0.6666666666666666, 0.8658302043468424, 0.8729276031935269, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4986303], dtype=float32), 1.1520667]. 
=============================================
[2019-04-04 01:10:32,617] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[79.97574]
 [80.97778]
 [81.99621]
 [81.97643]
 [82.2127 ]], R is [[79.59075928]
 [79.79485321]
 [79.99690247]
 [80.19693756]
 [80.39496613]].
[2019-04-04 01:10:43,559] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.7822303e-26 1.0698967e-15 5.3116919e-15 1.0000000e+00 2.0358881e-18
 6.6838179e-10 1.2083257e-15], sum to 1.0000
[2019-04-04 01:10:43,559] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6391
[2019-04-04 01:10:43,585] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 76.0, 107.3333333333333, 737.6666666666667, 26.0, 26.2642710703577, 0.5274286849435309, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3751800.0000, 
sim time next is 3752400.0000, 
raw observation next is [-3.0, 75.0, 109.1666666666667, 753.3333333333334, 26.0, 26.30277702184318, 0.5386856863514983, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.3795013850415513, 0.75, 0.363888888888889, 0.8324125230202579, 0.6666666666666666, 0.6918980851535984, 0.6795618954504995, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.42310703], dtype=float32), -0.52785736]. 
=============================================
[2019-04-04 01:10:52,749] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:10:52,749] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:10:52,754] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run19
[2019-04-04 01:10:53,358] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:10:53,358] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:10:53,372] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run19
[2019-04-04 01:10:53,670] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.8841340e-21 2.0663128e-14 3.7778552e-12 1.0000000e+00 8.8224015e-18
 7.8427195e-11 1.4605670e-14], sum to 1.0000
[2019-04-04 01:10:53,670] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0923
[2019-04-04 01:10:53,706] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 71.0, 0.0, 0.0, 26.0, 25.37800148234994, 0.3836181275366101, 0.0, 1.0, 37469.70259707844], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3715800.0000, 
sim time next is 3716400.0000, 
raw observation next is [-3.0, 71.0, 0.0, 0.0, 26.0, 25.37512917707953, 0.3801004034332092, 0.0, 1.0, 40289.91626026251], 
processed observation next is [1.0, 0.0, 0.3795013850415513, 0.71, 0.0, 0.0, 0.6666666666666666, 0.6145940980899608, 0.6267001344777364, 0.0, 1.0, 0.19185674409648812], 
reward next is 0.8081, 
noisyNet noise sample is [array([-1.1359555], dtype=float32), 0.5613127]. 
=============================================
[2019-04-04 01:11:12,678] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:11:12,678] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:11:12,685] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run19
[2019-04-04 01:11:14,099] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:11:14,099] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:11:14,141] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run19
[2019-04-04 01:11:18,985] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:11:18,985] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:11:18,991] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run19
[2019-04-04 01:11:21,096] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.02470722e-25 8.14298258e-15 2.05879489e-14 1.00000000e+00
 7.39883610e-17 1.01786135e-08 8.10328363e-16], sum to 1.0000
[2019-04-04 01:11:21,096] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2900
[2019-04-04 01:11:21,177] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 62.0, 0.0, 0.0, 26.0, 25.61562106632081, 0.4858419102797787, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4675800.0000, 
sim time next is 4676400.0000, 
raw observation next is [2.0, 62.0, 0.0, 0.0, 26.0, 25.64512059553973, 0.4866321604599407, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.518005540166205, 0.62, 0.0, 0.0, 0.6666666666666666, 0.6370933829616442, 0.6622107201533135, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.36962953], dtype=float32), 1.1860707]. 
=============================================
[2019-04-04 01:11:23,502] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.11187335e-27 4.07634697e-22 1.55671783e-18 1.00000000e+00
 7.81376262e-20 1.97431416e-16 9.32338371e-21], sum to 1.0000
[2019-04-04 01:11:23,503] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6478
[2019-04-04 01:11:23,520] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.52031880384176, 0.247530540309415, 0.0, 1.0, 40752.52412759356], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4765200.0000, 
sim time next is 4765800.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.47097935866443, 0.2378004310724876, 0.0, 1.0, 40833.59703865072], 
processed observation next is [0.0, 0.13043478260869565, 0.296398891966759, 0.92, 0.0, 0.0, 0.6666666666666666, 0.5392482798887025, 0.5792668103574958, 0.0, 1.0, 0.19444570018405105], 
reward next is 0.8056, 
noisyNet noise sample is [array([-0.8766682], dtype=float32), 0.5753759]. 
=============================================
[2019-04-04 01:11:28,418] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:11:28,418] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:11:28,424] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run19
[2019-04-04 01:11:37,823] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.3900459e-25 5.0299002e-19 5.1473770e-18 1.0000000e+00 7.6712621e-19
 1.1453730e-12 7.4084628e-18], sum to 1.0000
[2019-04-04 01:11:37,824] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2163
[2019-04-04 01:11:37,877] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.733333333333333, 86.33333333333333, 0.0, 0.0, 26.0, 24.33834314366776, 0.1125612025477871, 0.0, 1.0, 42349.1172050096], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 607200.0000, 
sim time next is 607800.0000, 
raw observation next is [-3.816666666666666, 86.16666666666667, 0.0, 0.0, 26.0, 24.30890240600146, 0.104699798554753, 0.0, 1.0, 42320.11438019131], 
processed observation next is [0.0, 0.0, 0.3568790397045245, 0.8616666666666667, 0.0, 0.0, 0.6666666666666666, 0.5257418671667885, 0.5348999328515843, 0.0, 1.0, 0.2015243541913872], 
reward next is 0.7985, 
noisyNet noise sample is [array([-0.20276377], dtype=float32), 0.7199612]. 
=============================================
[2019-04-04 01:11:39,368] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.2512305e-25 5.8511497e-18 1.4242988e-15 1.0000000e+00 1.0118788e-18
 1.5388322e-09 5.5600392e-18], sum to 1.0000
[2019-04-04 01:11:39,368] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9902
[2019-04-04 01:11:39,381] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.333333333333333, 39.0, 60.66666666666667, 485.5000000000001, 26.0, 25.52601863362, 0.4356629555755001, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4207200.0000, 
sim time next is 4207800.0000, 
raw observation next is [2.166666666666667, 39.5, 53.33333333333334, 421.0, 26.0, 25.50132489556255, 0.4204809668251972, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.5226223453370269, 0.395, 0.1777777777777778, 0.46519337016574586, 0.6666666666666666, 0.6251104079635459, 0.6401603222750657, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.99683666], dtype=float32), -0.05883182]. 
=============================================
[2019-04-04 01:11:39,411] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:11:39,411] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:11:39,443] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run19
[2019-04-04 01:11:43,662] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:11:43,663] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:11:43,787] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run19
[2019-04-04 01:11:47,258] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.2434306e-23 2.2323464e-18 1.2918759e-16 1.0000000e+00 4.3790145e-19
 5.0663423e-16 1.5821151e-16], sum to 1.0000
[2019-04-04 01:11:47,258] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4295
[2019-04-04 01:11:47,272] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.9, 71.0, 0.0, 0.0, 26.0, 24.34296629779266, 0.07957260494494439, 0.0, 1.0, 41197.30972380556], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 691200.0000, 
sim time next is 691800.0000, 
raw observation next is [-3.816666666666666, 71.16666666666667, 0.0, 0.0, 26.0, 24.34737210601549, 0.07497819441791292, 0.0, 1.0, 41111.93123972334], 
processed observation next is [1.0, 0.0, 0.3568790397045245, 0.7116666666666667, 0.0, 0.0, 0.6666666666666666, 0.528947675501291, 0.5249927314726376, 0.0, 1.0, 0.1957711011415397], 
reward next is 0.8042, 
noisyNet noise sample is [array([-0.04394275], dtype=float32), -0.38596824]. 
=============================================
[2019-04-04 01:11:52,881] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:11:52,881] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:11:52,886] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run19
[2019-04-04 01:12:00,810] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:12:00,810] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:12:00,819] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run19
[2019-04-04 01:12:02,605] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.9496831e-21 5.2087752e-14 2.6152594e-13 9.9995935e-01 1.3396608e-16
 4.0614246e-05 4.8888091e-13], sum to 1.0000
[2019-04-04 01:12:02,605] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6658
[2019-04-04 01:12:02,691] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.3, 68.0, 0.0, 0.0, 26.0, 23.33237722701795, 0.03561126579312247, 1.0, 1.0, 203502.1070719522], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 113400.0000, 
sim time next is 114000.0000, 
raw observation next is [-7.3, 68.0, 0.0, 0.0, 26.0, 24.06018125913774, 0.1267711719256595, 0.0, 1.0, 159115.7189847176], 
processed observation next is [1.0, 0.30434782608695654, 0.26038781163434904, 0.68, 0.0, 0.0, 0.6666666666666666, 0.5050151049281449, 0.5422570573085531, 0.0, 1.0, 0.7576938999272266], 
reward next is 0.2423, 
noisyNet noise sample is [array([-0.6818564], dtype=float32), 1.752196]. 
=============================================
[2019-04-04 01:12:02,719] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[83.58425]
 [82.55382]
 [81.47549]
 [81.40783]
 [81.40943]], R is [[82.44282532]
 [81.6493454 ]
 [80.86936188]
 [80.84199524]
 [80.81515503]].
[2019-04-04 01:12:05,588] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.6679594e-18 5.8055436e-09 8.9566160e-11 9.9957103e-01 1.6457448e-09
 4.2902483e-04 1.1461785e-10], sum to 1.0000
[2019-04-04 01:12:05,588] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4799
[2019-04-04 01:12:05,609] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.5, 52.5, 0.0, 0.0, 26.0, 25.94272587925209, 0.604734554461656, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4649400.0000, 
sim time next is 4650000.0000, 
raw observation next is [2.333333333333333, 52.33333333333333, 0.0, 0.0, 26.0, 25.85016964631271, 0.5883975286841542, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.5272391505078486, 0.5233333333333333, 0.0, 0.0, 0.6666666666666666, 0.6541808038593926, 0.6961325095613847, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.15989095], dtype=float32), -1.5866199]. 
=============================================
[2019-04-04 01:12:05,626] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[76.829926]
 [75.91335 ]
 [76.316025]
 [77.24391 ]
 [77.38798 ]], R is [[77.96902466]
 [78.18933868]
 [78.40744781]
 [78.62337494]
 [78.83714294]].
[2019-04-04 01:12:17,624] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.7590752e-22 2.5413145e-14 1.3357591e-12 9.9999893e-01 2.0092315e-16
 1.0555335e-06 8.1109063e-14], sum to 1.0000
[2019-04-04 01:12:17,625] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9185
[2019-04-04 01:12:17,657] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:12:17,657] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:12:17,669] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run19
[2019-04-04 01:12:17,843] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 75.0, 0.0, 0.0, 26.0, 24.26459243342196, 0.0516270977403713, 0.0, 1.0, 41438.84001103333], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 702000.0000, 
sim time next is 702600.0000, 
raw observation next is [-3.3, 75.0, 0.0, 0.0, 26.0, 24.26395575394782, 0.0532671186612699, 0.0, 1.0, 41511.02418408817], 
processed observation next is [1.0, 0.13043478260869565, 0.37119113573407203, 0.75, 0.0, 0.0, 0.6666666666666666, 0.521996312828985, 0.5177557062204233, 0.0, 1.0, 0.1976715437337532], 
reward next is 0.8023, 
noisyNet noise sample is [array([1.3395367], dtype=float32), 2.2491963]. 
=============================================
[2019-04-04 01:12:18,549] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0458862e-22 2.4312174e-13 1.5932610e-13 9.9999964e-01 1.4649153e-15
 3.0205186e-07 1.8047176e-15], sum to 1.0000
[2019-04-04 01:12:18,549] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8022
[2019-04-04 01:12:18,579] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 53.66666666666667, 0.0, 0.0, 26.0, 25.6452854745305, 0.5305700106428256, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4664400.0000, 
sim time next is 4665000.0000, 
raw observation next is [2.0, 52.83333333333334, 0.0, 0.0, 26.0, 25.61324869712805, 0.517949819529952, 0.0, 1.0, 18734.84405827772], 
processed observation next is [1.0, 1.0, 0.518005540166205, 0.5283333333333334, 0.0, 0.0, 0.6666666666666666, 0.6344373914273375, 0.6726499398433173, 0.0, 1.0, 0.0892135431346558], 
reward next is 0.9108, 
noisyNet noise sample is [array([-0.33953705], dtype=float32), 0.52046317]. 
=============================================
[2019-04-04 01:12:18,582] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[85.52869]
 [85.6496 ]
 [85.75414]
 [85.87599]
 [85.86792]], R is [[85.5091629 ]
 [85.65407562]
 [85.79753876]
 [85.93956757]
 [85.99088287]].
[2019-04-04 01:12:22,109] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.0299309e-22 2.2692004e-15 2.5706569e-13 1.0000000e+00 1.4714237e-15
 3.3976535e-12 2.8965336e-15], sum to 1.0000
[2019-04-04 01:12:22,109] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1033
[2019-04-04 01:12:22,138] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.35, 86.5, 0.0, 0.0, 26.0, 24.45864800063221, 0.1636852822310623, 0.0, 1.0, 40753.50356339286], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 538200.0000, 
sim time next is 538800.0000, 
raw observation next is [1.266666666666667, 87.0, 0.0, 0.0, 26.0, 24.43456171453269, 0.1688453241099761, 0.0, 1.0, 40783.97056796255], 
processed observation next is [0.0, 0.21739130434782608, 0.4976915974145891, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5362134762110576, 0.5562817747033254, 0.0, 1.0, 0.19420938365696452], 
reward next is 0.8058, 
noisyNet noise sample is [array([-0.59631866], dtype=float32), 1.0416987]. 
=============================================
[2019-04-04 01:12:32,169] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.3297480e-28 5.4715966e-21 1.7993606e-18 1.0000000e+00 1.3857317e-24
 9.2859866e-18 2.1170845e-19], sum to 1.0000
[2019-04-04 01:12:32,169] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8972
[2019-04-04 01:12:32,224] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-9.666666666666666, 41.0, 44.33333333333333, 790.3333333333334, 26.0, 26.274385426095, 0.5470552325278039, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 399000.0000, 
sim time next is 399600.0000, 
raw observation next is [-9.5, 40.0, 42.5, 769.5, 26.0, 26.47937754531433, 0.557720980507073, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.1994459833795014, 0.4, 0.14166666666666666, 0.8502762430939227, 0.6666666666666666, 0.7066147954428607, 0.6859069935023577, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.50408363], dtype=float32), -1.2095938]. 
=============================================
[2019-04-04 01:12:33,862] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.9262318e-25 4.2853184e-18 3.4204562e-16 1.0000000e+00 5.5703141e-18
 1.2332837e-08 4.5223195e-16], sum to 1.0000
[2019-04-04 01:12:33,862] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8742
[2019-04-04 01:12:33,900] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.25, 95.0, 0.0, 0.0, 26.0, 24.34763197534753, 0.15553155574752, 0.0, 1.0, 40100.40022372659], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 83400.0000, 
sim time next is 84000.0000, 
raw observation next is [0.2, 95.0, 0.0, 0.0, 26.0, 24.32487457801853, 0.1522594618074109, 0.0, 1.0, 40096.46212865053], 
processed observation next is [0.0, 1.0, 0.46814404432132967, 0.95, 0.0, 0.0, 0.6666666666666666, 0.5270728815015442, 0.5507531539358036, 0.0, 1.0, 0.1909355339459549], 
reward next is 0.8091, 
noisyNet noise sample is [array([-0.3107157], dtype=float32), -1.8176037]. 
=============================================
[2019-04-04 01:12:33,933] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[81.676834]
 [81.66999 ]
 [81.66264 ]
 [81.6651  ]
 [81.668106]], R is [[81.67751312]
 [81.66978455]
 [81.66209412]
 [81.65441132]
 [81.64669037]].
[2019-04-04 01:12:36,845] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:12:36,845] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:12:36,871] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run19
[2019-04-04 01:12:47,517] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:12:47,517] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:12:47,520] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run19
[2019-04-04 01:13:00,152] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:13:00,152] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:13:00,172] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run19
[2019-04-04 01:13:03,974] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.2543559e-31 1.3698424e-18 2.4094251e-20 1.0000000e+00 4.7558842e-24
 4.0513385e-16 2.2080772e-22], sum to 1.0000
[2019-04-04 01:13:03,974] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7730
[2019-04-04 01:13:03,979] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [15.16666666666667, 95.0, 0.0, 0.0, 26.0, 23.74465567396898, 0.1976759482279565, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1226400.0000, 
sim time next is 1227000.0000, 
raw observation next is [15.08333333333333, 95.5, 0.0, 0.0, 26.0, 23.71983418066965, 0.1930998636668102, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.17391304347826086, 0.8804247460757156, 0.955, 0.0, 0.0, 0.6666666666666666, 0.4766528483891375, 0.5643666212222701, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.428296], dtype=float32), -1.2008647]. 
=============================================
[2019-04-04 01:13:03,984] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[84.4734  ]
 [84.53283 ]
 [84.59295 ]
 [84.64688 ]
 [84.696236]], R is [[84.55660248]
 [84.71103668]
 [84.86392975]
 [85.01528931]
 [85.16513824]].
[2019-04-04 01:13:12,267] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0538341e-26 5.5196489e-20 3.4077027e-18 1.0000000e+00 1.1285040e-20
 2.8616920e-14 2.0357949e-18], sum to 1.0000
[2019-04-04 01:13:12,295] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4133
[2019-04-04 01:13:12,309] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 83.0, 0.0, 0.0, 26.0, 24.88320884900574, 0.2669331526028197, 0.0, 1.0, 41634.83407009109], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 855600.0000, 
sim time next is 856200.0000, 
raw observation next is [-3.4, 83.0, 0.0, 0.0, 26.0, 24.88317023566036, 0.2622257697678632, 0.0, 1.0, 41621.68668529503], 
processed observation next is [1.0, 0.9130434782608695, 0.368421052631579, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5735975196383633, 0.587408589922621, 0.0, 1.0, 0.19819850802521444], 
reward next is 0.8018, 
noisyNet noise sample is [array([-1.824109], dtype=float32), 1.2249788]. 
=============================================
[2019-04-04 01:13:15,037] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:13:15,037] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:13:15,040] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run19
[2019-04-04 01:13:16,730] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.8001793e-31 6.9614917e-18 1.3948893e-19 1.0000000e+00 4.7596096e-22
 5.5876748e-20 1.5123693e-22], sum to 1.0000
[2019-04-04 01:13:16,734] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3499
[2019-04-04 01:13:16,784] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [18.71666666666667, 54.33333333333333, 160.6666666666667, 0.0, 26.0, 27.30958403204682, 0.9375120810988783, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1086600.0000, 
sim time next is 1087200.0000, 
raw observation next is [18.8, 54.0, 155.5, 0.0, 26.0, 26.70608936472293, 0.911369685669056, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.9833795013850417, 0.54, 0.5183333333333333, 0.0, 0.6666666666666666, 0.7255074470602443, 0.8037898952230186, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.05921224], dtype=float32), 0.63030314]. 
=============================================
[2019-04-04 01:13:29,624] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.59150675e-27 1.39844292e-19 1.91483213e-18 1.00000000e+00
 1.96556955e-21 2.16876023e-13 1.23726096e-19], sum to 1.0000
[2019-04-04 01:13:29,624] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1819
[2019-04-04 01:13:29,659] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.9, 97.33333333333333, 0.0, 0.0, 26.0, 25.31842794240374, 0.5942471447317478, 0.0, 1.0, 40704.57575060406], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1282800.0000, 
sim time next is 1283400.0000, 
raw observation next is [5.8, 98.0, 0.0, 0.0, 26.0, 25.40989368099625, 0.6068195100493022, 0.0, 1.0, 19254.45153334337], 
processed observation next is [0.0, 0.8695652173913043, 0.6232686980609419, 0.98, 0.0, 0.0, 0.6666666666666666, 0.617491140083021, 0.7022731700164341, 0.0, 1.0, 0.09168786444449224], 
reward next is 0.9083, 
noisyNet noise sample is [array([0.35139415], dtype=float32), 0.5179395]. 
=============================================
[2019-04-04 01:13:42,560] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.75698836e-25 3.57308309e-17 6.23082465e-17 1.00000000e+00
 2.61093294e-18 1.73970560e-09 1.10875744e-17], sum to 1.0000
[2019-04-04 01:13:42,561] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8621
[2019-04-04 01:13:42,606] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.8, 100.0, 0.0, 0.0, 26.0, 25.02639051676014, 0.3159099489769575, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 934800.0000, 
sim time next is 935400.0000, 
raw observation next is [4.9, 100.0, 0.0, 0.0, 26.0, 24.91509986977017, 0.2978465098939637, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.5983379501385043, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5762583224808475, 0.5992821699646546, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6467349], dtype=float32), 1.5229415]. 
=============================================
[2019-04-04 01:13:57,045] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.4055835e-23 2.2083065e-14 1.3194114e-12 9.9999869e-01 6.0023470e-16
 1.3367467e-06 2.4958982e-15], sum to 1.0000
[2019-04-04 01:13:57,045] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3655
[2019-04-04 01:13:57,058] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 92.0, 74.5, 0.0, 26.0, 25.89561181739189, 0.5374617044388138, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1681200.0000, 
sim time next is 1681800.0000, 
raw observation next is [1.1, 90.66666666666667, 77.33333333333334, 0.0, 26.0, 25.9078023087147, 0.5333394617945301, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.49307479224376743, 0.9066666666666667, 0.25777777777777783, 0.0, 0.6666666666666666, 0.658983525726225, 0.6777798205981767, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9133958], dtype=float32), 1.09352]. 
=============================================
[2019-04-04 01:14:05,397] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.9952548e-25 1.7583951e-19 4.1411390e-15 1.0000000e+00 3.4355661e-20
 2.1532234e-11 1.2375886e-16], sum to 1.0000
[2019-04-04 01:14:05,398] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5598
[2019-04-04 01:14:05,518] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.3, 42.5, 0.0, 0.0, 26.0, 22.44813024446991, -0.3466871566553491, 0.0, 1.0, 46061.04471715296], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 457800.0000, 
sim time next is 458400.0000, 
raw observation next is [-8.2, 42.0, 0.0, 0.0, 26.0, 22.48984629028509, -0.2769924374619298, 1.0, 1.0, 202323.8693695061], 
processed observation next is [1.0, 0.30434782608695654, 0.23545706371191139, 0.42, 0.0, 0.0, 0.6666666666666666, 0.3741538575237575, 0.4076691875126901, 1.0, 1.0, 0.9634469969976481], 
reward next is 0.0366, 
noisyNet noise sample is [array([1.7007717], dtype=float32), -0.94182366]. 
=============================================
[2019-04-04 01:14:08,307] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4544265e-21 6.7660064e-15 1.7004613e-14 1.0000000e+00 1.0212770e-14
 3.8256717e-11 4.9839424e-13], sum to 1.0000
[2019-04-04 01:14:08,307] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5305
[2019-04-04 01:14:08,375] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 83.0, 0.0, 0.0, 26.0, 24.97952969128835, 0.3015629330517869, 0.0, 1.0, 61246.03734440381], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 851400.0000, 
sim time next is 852000.0000, 
raw observation next is [-3.4, 83.0, 0.0, 0.0, 26.0, 25.04555194526013, 0.3012969163268137, 0.0, 1.0, 48174.03590802514], 
processed observation next is [1.0, 0.8695652173913043, 0.368421052631579, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5871293287716775, 0.6004323054422712, 0.0, 1.0, 0.2294001709905959], 
reward next is 0.7706, 
noisyNet noise sample is [array([-1.808689], dtype=float32), -0.46627071]. 
=============================================
[2019-04-04 01:14:08,389] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[76.58501]
 [76.84006]
 [77.26729]
 [78.05769]
 [79.71254]], R is [[76.4029541 ]
 [76.34727478]
 [76.12001801]
 [75.67996979]
 [75.45184326]].
[2019-04-04 01:14:10,763] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.0051225e-23 1.1385727e-16 3.6604584e-15 1.0000000e+00 3.0568731e-17
 6.9454692e-11 7.6528601e-16], sum to 1.0000
[2019-04-04 01:14:10,764] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3376
[2019-04-04 01:14:10,809] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.233333333333333, 80.33333333333334, 139.5, 0.0, 26.0, 25.57392189982873, 0.3274415714430776, 1.0, 1.0, 25690.84937600487], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2028000.0000, 
sim time next is 2028600.0000, 
raw observation next is [-5.05, 79.0, 147.0, 0.0, 26.0, 25.61356100802428, 0.3350927950274445, 1.0, 1.0, 24356.00543985774], 
processed observation next is [1.0, 0.4782608695652174, 0.32271468144044324, 0.79, 0.49, 0.0, 0.6666666666666666, 0.6344634173353567, 0.6116975983424815, 1.0, 1.0, 0.11598097828503687], 
reward next is 0.8840, 
noisyNet noise sample is [array([-1.9456986], dtype=float32), -0.07493267]. 
=============================================
[2019-04-04 01:14:13,996] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.8744813e-34 2.4557930e-21 7.1759608e-24 1.0000000e+00 1.6182019e-30
 3.8548023e-22 8.6394573e-26], sum to 1.0000
[2019-04-04 01:14:13,999] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3539
[2019-04-04 01:14:14,005] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [16.6, 75.0, 0.0, 0.0, 26.0, 24.35609187480546, 0.3284409230010756, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1204800.0000, 
sim time next is 1205400.0000, 
raw observation next is [16.6, 75.0, 0.0, 0.0, 26.0, 24.35476348641222, 0.324286294414744, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.9565217391304348, 0.922437673130194, 0.75, 0.0, 0.0, 0.6666666666666666, 0.5295636238676851, 0.6080954314715813, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4808053], dtype=float32), -0.97337204]. 
=============================================
[2019-04-04 01:14:21,006] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.8463416e-22 7.0572488e-16 7.2588962e-14 1.0000000e+00 5.5933986e-17
 1.6361328e-13 1.3768602e-14], sum to 1.0000
[2019-04-04 01:14:21,006] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6023
[2019-04-04 01:14:21,071] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.4, 60.0, 0.0, 0.0, 26.0, 25.01477564310801, 0.3140688362637107, 0.0, 1.0, 48544.78668474748], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 765600.0000, 
sim time next is 766200.0000, 
raw observation next is [-5.5, 60.5, 0.0, 0.0, 26.0, 24.99169031413968, 0.3064463037897469, 0.0, 1.0, 45527.81423134218], 
processed observation next is [1.0, 0.8695652173913043, 0.3102493074792244, 0.605, 0.0, 0.0, 0.6666666666666666, 0.5826408595116401, 0.6021487679299157, 0.0, 1.0, 0.2167991153873437], 
reward next is 0.7832, 
noisyNet noise sample is [array([1.9834021], dtype=float32), 1.0109655]. 
=============================================
[2019-04-04 01:14:22,276] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.8189451e-28 5.8024557e-16 9.7131703e-16 1.0000000e+00 7.7924417e-22
 8.1058119e-14 3.6722849e-18], sum to 1.0000
[2019-04-04 01:14:22,276] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5556
[2019-04-04 01:14:22,286] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [13.3, 80.0, 0.0, 0.0, 26.0, 25.60056947928113, 0.5807919417885258, 0.0, 1.0, 46296.97843358361], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1062000.0000, 
sim time next is 1062600.0000, 
raw observation next is [13.11666666666667, 80.5, 0.0, 0.0, 26.0, 25.61790693785032, 0.5920954346765009, 0.0, 1.0, 24667.44743798228], 
processed observation next is [1.0, 0.30434782608695654, 0.8259464450600187, 0.805, 0.0, 0.0, 0.6666666666666666, 0.6348255781541935, 0.697365144892167, 0.0, 1.0, 0.11746403541896323], 
reward next is 0.8825, 
noisyNet noise sample is [array([0.24868001], dtype=float32), -1.3626608]. 
=============================================
[2019-04-04 01:14:29,523] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.2162301e-21 9.3562571e-16 3.8102316e-12 1.0000000e+00 1.5337194e-16
 5.9361981e-11 1.9824301e-13], sum to 1.0000
[2019-04-04 01:14:29,526] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5149
[2019-04-04 01:14:29,537] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.2, 86.66666666666667, 0.0, 0.0, 26.0, 24.0954880549745, 0.07449934298941902, 0.0, 1.0, 43646.69492912235], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2259600.0000, 
sim time next is 2260200.0000, 
raw observation next is [-8.3, 86.83333333333333, 0.0, 0.0, 26.0, 24.05794477343279, 0.07267935838360541, 0.0, 1.0, 43638.89425515765], 
processed observation next is [1.0, 0.13043478260869565, 0.23268698060941828, 0.8683333333333333, 0.0, 0.0, 0.6666666666666666, 0.5048287311193992, 0.5242264527945352, 0.0, 1.0, 0.20780425835789357], 
reward next is 0.7922, 
noisyNet noise sample is [array([0.37542656], dtype=float32), -2.3892179]. 
=============================================
[2019-04-04 01:14:32,981] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5691157e-22 5.7602661e-16 2.6396892e-15 1.0000000e+00 6.8960526e-17
 2.2655387e-10 3.9786984e-15], sum to 1.0000
[2019-04-04 01:14:32,981] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2221
[2019-04-04 01:14:33,051] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.7, 78.0, 139.5, 44.5, 26.0, 25.62979922158014, 0.3258249589951434, 1.0, 1.0, 18730.00018030485], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2282400.0000, 
sim time next is 2283000.0000, 
raw observation next is [-6.416666666666667, 76.33333333333333, 152.3333333333333, 46.33333333333333, 26.0, 25.66683085360515, 0.3367280124119924, 1.0, 1.0, 18727.75820244983], 
processed observation next is [1.0, 0.43478260869565216, 0.2848568790397045, 0.7633333333333333, 0.5077777777777777, 0.05119705340699815, 0.6666666666666666, 0.6389025711337624, 0.6122426708039975, 1.0, 1.0, 0.0891798009640468], 
reward next is 0.9108, 
noisyNet noise sample is [array([-0.01255207], dtype=float32), -1.1358805]. 
=============================================
[2019-04-04 01:14:33,057] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[81.17682]
 [80.97745]
 [80.60121]
 [80.31031]
 [80.1494 ]], R is [[81.46685791]
 [81.56300354]
 [81.65817261]
 [81.75238037]
 [81.84563446]].
[2019-04-04 01:14:36,607] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.8749796e-22 4.7266963e-15 1.3393897e-10 1.0000000e+00 2.5071699e-16
 4.0078976e-09 2.6367870e-14], sum to 1.0000
[2019-04-04 01:14:36,608] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4170
[2019-04-04 01:14:36,675] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 25.09448683785163, 0.399293073589527, 0.0, 1.0, 109824.6022045531], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2146800.0000, 
sim time next is 2147400.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 25.13664308078208, 0.4101438140650486, 0.0, 1.0, 73815.70719700003], 
processed observation next is [1.0, 0.8695652173913043, 0.30747922437673136, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5947202567318399, 0.6367146046883495, 0.0, 1.0, 0.35150336760476203], 
reward next is 0.6485, 
noisyNet noise sample is [array([-1.6355898], dtype=float32), 0.2430706]. 
=============================================
[2019-04-04 01:14:41,367] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.4395132e-20 4.5351575e-13 6.6916903e-11 9.9999702e-01 4.8125244e-14
 3.0361800e-06 4.5850754e-12], sum to 1.0000
[2019-04-04 01:14:41,367] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1805
[2019-04-04 01:14:41,411] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.833333333333334, 81.33333333333334, 99.33333333333334, 496.8333333333333, 26.0, 25.64870115633582, 0.3023856455496316, 1.0, 1.0, 19330.54918603731], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1935600.0000, 
sim time next is 1936200.0000, 
raw observation next is [-7.566666666666666, 80.16666666666667, 113.6666666666667, 444.6666666666667, 26.0, 25.64999699484768, 0.3051915118769312, 1.0, 1.0, 21622.00349215545], 
processed observation next is [1.0, 0.391304347826087, 0.2530009233610342, 0.8016666666666667, 0.378888888888889, 0.4913443830570903, 0.6666666666666666, 0.6374997495706399, 0.6017305039589771, 1.0, 1.0, 0.10296192139121643], 
reward next is 0.8970, 
noisyNet noise sample is [array([1.5783129], dtype=float32), 0.47078454]. 
=============================================
[2019-04-04 01:14:41,968] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.1332033e-25 1.8260966e-22 6.1025670e-20 1.0000000e+00 5.2311613e-21
 2.0882414e-19 1.9729559e-19], sum to 1.0000
[2019-04-04 01:14:41,978] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5407
[2019-04-04 01:14:41,989] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.3, 42.0, 0.0, 0.0, 26.0, 24.65072527340649, 0.1709570272571092, 0.0, 1.0, 43107.59123436782], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2417400.0000, 
sim time next is 2418000.0000, 
raw observation next is [-5.4, 42.33333333333333, 0.0, 0.0, 26.0, 24.65030907283533, 0.1622517307354123, 0.0, 1.0, 43105.57703984434], 
processed observation next is [0.0, 1.0, 0.31301939058171746, 0.4233333333333333, 0.0, 0.0, 0.6666666666666666, 0.5541924227362776, 0.5540839102451375, 0.0, 1.0, 0.20526465257068735], 
reward next is 0.7947, 
noisyNet noise sample is [array([0.18632057], dtype=float32), 0.5780533]. 
=============================================
[2019-04-04 01:14:41,996] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[75.71723 ]
 [75.73432 ]
 [75.760155]
 [75.81556 ]
 [75.903885]], R is [[75.72160339]
 [75.7591095 ]
 [75.79624176]
 [75.83309174]
 [75.86969757]].
[2019-04-04 01:14:43,797] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.9264487e-25 4.0226600e-18 8.1064065e-16 1.0000000e+00 1.1153438e-20
 1.5867566e-09 2.7147779e-16], sum to 1.0000
[2019-04-04 01:14:43,798] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4281
[2019-04-04 01:14:43,835] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.700000000000001, 77.5, 0.0, 0.0, 26.0, 24.41804437713682, 0.1933044134821621, 0.0, 1.0, 44309.10370308564], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2247000.0000, 
sim time next is 2247600.0000, 
raw observation next is [-6.700000000000001, 77.0, 0.0, 0.0, 26.0, 24.39116305956205, 0.191397966256817, 0.0, 1.0, 44341.91251174437], 
processed observation next is [1.0, 0.0, 0.2770083102493075, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5325969216301708, 0.5637993220856057, 0.0, 1.0, 0.21115196434163985], 
reward next is 0.7888, 
noisyNet noise sample is [array([-0.29484084], dtype=float32), 0.72764194]. 
=============================================
[2019-04-04 01:14:44,682] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.16525740e-26 1.87050800e-21 1.11015405e-17 1.00000000e+00
 5.51587822e-21 4.40439785e-14 1.46630016e-18], sum to 1.0000
[2019-04-04 01:14:44,684] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1032
[2019-04-04 01:14:44,749] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.166666666666666, 45.33333333333333, 63.5, 683.6666666666667, 26.0, 25.26756359479243, 0.251907208653498, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2454000.0000, 
sim time next is 2454600.0000, 
raw observation next is [-5.883333333333333, 44.16666666666667, 66.0, 702.3333333333333, 26.0, 25.22074616501087, 0.2466252676509381, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.2996306555863343, 0.4416666666666667, 0.22, 0.7760589318600367, 0.6666666666666666, 0.6017288470842391, 0.5822084225503127, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.03663173], dtype=float32), -1.5239832]. 
=============================================
[2019-04-04 01:14:46,982] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.7095524e-24 2.7402794e-16 4.1824801e-15 1.0000000e+00 1.4583955e-18
 6.5575173e-10 7.7644332e-14], sum to 1.0000
[2019-04-04 01:14:46,983] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8740
[2019-04-04 01:14:47,027] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 89.33333333333334, 0.0, 0.0, 26.0, 24.78875484392167, 0.2692994182809259, 0.0, 1.0, 42627.89281929154], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2072400.0000, 
sim time next is 2073000.0000, 
raw observation next is [-4.5, 90.16666666666667, 0.0, 0.0, 26.0, 24.74853738319781, 0.261066201496676, 0.0, 1.0, 42669.73244534907], 
processed observation next is [1.0, 1.0, 0.3379501385041552, 0.9016666666666667, 0.0, 0.0, 0.6666666666666666, 0.5623781152664842, 0.5870220671655587, 0.0, 1.0, 0.20318920212070987], 
reward next is 0.7968, 
noisyNet noise sample is [array([0.7153212], dtype=float32), 0.5836662]. 
=============================================
[2019-04-04 01:14:47,052] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[83.649025]
 [83.54402 ]
 [83.46404 ]
 [83.37673 ]
 [83.28621 ]], R is [[83.71164703]
 [83.67154694]
 [83.63211823]
 [83.59344482]
 [83.55561829]].
[2019-04-04 01:14:49,714] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-04 01:14:49,723] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 01:14:49,723] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:14:49,730] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 01:14:49,731] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 01:14:49,731] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:14:49,738] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:14:50,406] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run26
[2019-04-04 01:14:50,474] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run26
[2019-04-04 01:14:50,493] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run26
[2019-04-04 01:15:15,336] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.7143832], dtype=float32), 0.5080172]
[2019-04-04 01:15:15,336] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-3.933333333333333, 40.0, 0.0, 0.0, 22.0, 21.15718712047317, -0.674982802640654, 0.0, 1.0, 37485.5931475323]
[2019-04-04 01:15:15,337] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 01:15:15,337] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.1675318e-13 1.2889858e-11 2.8615379e-09 9.9999893e-01 2.0516360e-09
 1.0827455e-06 7.2673991e-09], sampled 0.6025049119827837
[2019-04-04 01:15:26,181] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.7143832], dtype=float32), 0.5080172]
[2019-04-04 01:15:26,181] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [2.2, 92.0, 0.0, 0.0, 23.0, 22.57953024600979, -0.1058739995488041, 0.0, 1.0, 81163.42692282698]
[2019-04-04 01:15:26,181] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 01:15:26,182] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [6.5186444e-18 1.9645514e-12 7.0965997e-11 9.9999988e-01 2.6649824e-12
 1.4389184e-07 2.8629626e-11], sampled 0.7424504232003587
[2019-04-04 01:16:13,762] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.7143832], dtype=float32), 0.5080172]
[2019-04-04 01:16:13,762] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [11.95, 89.0, 0.0, 0.0, 23.0, 23.87851706139877, 0.2061374547173307, 0.0, 1.0, 0.0]
[2019-04-04 01:16:13,762] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 01:16:13,763] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [2.2946998e-20 8.4354859e-13 3.4045427e-12 1.0000000e+00 1.5645952e-13
 2.0097056e-08 5.9923209e-13], sampled 0.45323282076790194
[2019-04-04 01:16:14,092] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.7143832], dtype=float32), 0.5080172]
[2019-04-04 01:16:14,093] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [18.03333333333333, 61.33333333333334, 0.0, 0.0, 23.0, 27.43014861689009, 1.057435204925914, 1.0, 0.0, 0.0]
[2019-04-04 01:16:14,093] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 01:16:14,094] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [3.9204905e-26 3.0529634e-13 1.7287408e-16 1.0000000e+00 5.6494906e-19
 7.1730453e-13 1.5997740e-18], sampled 0.3253627181248079
[2019-04-04 01:16:18,399] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7448.4827 188706968.0216 -426.4094
[2019-04-04 01:16:22,324] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.7143832], dtype=float32), 0.5080172]
[2019-04-04 01:16:22,324] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [9.885384287, 28.34397408333334, 0.0, 0.0, 23.0, 22.77470856194068, -0.3015939650111857, 0.0, 1.0, 0.0]
[2019-04-04 01:16:22,325] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 01:16:22,325] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.5231382e-22 7.9692552e-17 1.5368945e-15 1.0000000e+00 1.1428110e-16
 6.9302441e-13 9.6855316e-16], sampled 0.7014930984223374
[2019-04-04 01:16:47,485] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7395.8449 224534362.0814 -391.2541
[2019-04-04 01:16:55,371] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7117.7758 233712053.4719 -686.9365
[2019-04-04 01:16:56,413] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 2500000, evaluation results [2500000.0, 7395.844936211406, 224534362.08139628, -391.2540980219688, 7448.482653209532, 188706968.02155134, -426.4094211808285, 7117.775770531691, 233712053.47185668, -686.9364664508074]
[2019-04-04 01:17:14,077] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.6518131e-29 5.7874029e-23 4.8085213e-21 1.0000000e+00 2.1937195e-23
 5.5927344e-18 1.4512010e-21], sum to 1.0000
[2019-04-04 01:17:14,077] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1745
[2019-04-04 01:17:14,095] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.0, 74.0, 0.0, 0.0, 26.0, 24.8187774039292, 0.2214883557735218, 0.0, 1.0, 41722.77731382594], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2601600.0000, 
sim time next is 2602200.0000, 
raw observation next is [-5.0, 74.0, 0.0, 0.0, 26.0, 24.82868219071535, 0.2116186252627564, 0.0, 1.0, 41765.24935838187], 
processed observation next is [1.0, 0.08695652173913043, 0.32409972299168976, 0.74, 0.0, 0.0, 0.6666666666666666, 0.5690568492262793, 0.5705395417542521, 0.0, 1.0, 0.19888213980181843], 
reward next is 0.8011, 
noisyNet noise sample is [array([0.02167484], dtype=float32), -0.5580576]. 
=============================================
[2019-04-04 01:17:23,267] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.6037705e-25 4.7147269e-20 2.9491110e-17 1.0000000e+00 8.2338269e-20
 6.6142508e-12 9.4417232e-18], sum to 1.0000
[2019-04-04 01:17:23,290] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1162
[2019-04-04 01:17:23,370] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.2, 63.33333333333333, 0.0, 0.0, 26.0, 25.00054736059715, 0.3737379236951375, 0.0, 1.0, 24795.44538729483], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2664600.0000, 
sim time next is 2665200.0000, 
raw observation next is [-1.2, 63.66666666666667, 0.0, 0.0, 26.0, 25.02737172896851, 0.3828743749174444, 0.0, 1.0, 189147.0901491676], 
processed observation next is [1.0, 0.8695652173913043, 0.42936288088642666, 0.6366666666666667, 0.0, 0.0, 0.6666666666666666, 0.5856143107473759, 0.6276247916391481, 0.0, 1.0, 0.9007004292817505], 
reward next is 0.0993, 
noisyNet noise sample is [array([-1.0584034], dtype=float32), -0.8965942]. 
=============================================
[2019-04-04 01:17:32,833] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.34811325e-30 1.05394428e-23 2.22657694e-21 1.00000000e+00
 8.36988076e-26 9.37506150e-21 9.53759277e-23], sum to 1.0000
[2019-04-04 01:17:32,879] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1355
[2019-04-04 01:17:32,927] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.8, 55.66666666666667, 79.33333333333333, 23.0, 26.0, 25.60572173534502, 0.2596454989040634, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2537400.0000, 
sim time next is 2538000.0000, 
raw observation next is [-2.8, 56.0, 93.5, 25.5, 26.0, 25.597847112832, 0.2684046746556085, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.38504155124653744, 0.56, 0.31166666666666665, 0.0281767955801105, 0.6666666666666666, 0.6331539260693333, 0.5894682248852029, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.74482214], dtype=float32), -0.44217724]. 
=============================================
[2019-04-04 01:17:32,930] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[91.81515]
 [91.8139 ]
 [91.77569]
 [91.86881]
 [91.82131]], R is [[91.78248596]
 [91.86466217]
 [91.9460144 ]
 [92.02655792]
 [92.04682159]].
[2019-04-04 01:17:37,645] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.5232210e-25 8.6477729e-17 2.3098124e-14 1.0000000e+00 7.0539307e-18
 1.2751471e-14 4.3644122e-16], sum to 1.0000
[2019-04-04 01:17:37,645] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1756
[2019-04-04 01:17:37,671] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 88.0, 0.0, 0.0, 26.0, 25.81039024567147, 0.5004017369258565, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1704600.0000, 
sim time next is 1705200.0000, 
raw observation next is [1.1, 88.0, 0.0, 0.0, 26.0, 25.71887876183871, 0.5271138134446458, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.49307479224376743, 0.88, 0.0, 0.0, 0.6666666666666666, 0.6432398968198925, 0.6757046044815486, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3891078], dtype=float32), -0.42225137]. 
=============================================
[2019-04-04 01:17:38,990] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.4155158e-23 2.3610525e-16 1.6558247e-15 1.0000000e+00 1.0595235e-15
 2.4396016e-14 3.2406014e-14], sum to 1.0000
[2019-04-04 01:17:38,990] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9781
[2019-04-04 01:17:39,076] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.2, 49.0, 134.5, 39.0, 26.0, 25.82124005532908, 0.30726970959544, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2541600.0000, 
sim time next is 2542200.0000, 
raw observation next is [-1.1, 48.66666666666667, 134.0, 41.0, 26.0, 25.83881863128012, 0.3040084827020466, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4321329639889197, 0.4866666666666667, 0.44666666666666666, 0.045303867403314914, 0.6666666666666666, 0.65323488594001, 0.6013361609006822, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.66186494], dtype=float32), 0.78610224]. 
=============================================
[2019-04-04 01:18:04,554] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.4456360e-34 3.5938978e-27 4.0509800e-25 1.0000000e+00 4.1845887e-29
 3.4307214e-27 8.0976843e-25], sum to 1.0000
[2019-04-04 01:18:04,555] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5177
[2019-04-04 01:18:04,606] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 27.33333333333334, 85.5, 824.0, 26.0, 24.96007340600312, 0.2733529458788288, 0.0, 1.0, 18708.74472892249], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2468400.0000, 
sim time next is 2469000.0000, 
raw observation next is [2.1, 27.16666666666666, 84.0, 816.0, 26.0, 24.95792374235744, 0.275919537153273, 0.0, 1.0, 18708.29398960462], 
processed observation next is [0.0, 0.5652173913043478, 0.5207756232686982, 0.2716666666666666, 0.28, 0.901657458563536, 0.6666666666666666, 0.5798269785297867, 0.591973179051091, 0.0, 1.0, 0.08908711423621248], 
reward next is 0.9109, 
noisyNet noise sample is [array([-2.3442535], dtype=float32), -0.59665805]. 
=============================================
[2019-04-04 01:18:04,609] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[84.46158 ]
 [84.648964]
 [84.74299 ]
 [84.83556 ]
 [84.82998 ]], R is [[84.34238434]
 [84.40987396]
 [84.47667694]
 [84.63191223]
 [84.78559113]].
[2019-04-04 01:18:08,458] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.3809086e-23 7.3259797e-18 6.1210677e-16 1.0000000e+00 1.3937357e-18
 6.0550349e-11 5.0566135e-15], sum to 1.0000
[2019-04-04 01:18:08,458] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4326
[2019-04-04 01:18:08,560] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.733333333333334, 80.66666666666667, 0.0, 0.0, 26.0, 23.63600302676152, -0.08378801256607404, 0.0, 1.0, 45304.20826995366], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1917600.0000, 
sim time next is 1918200.0000, 
raw observation next is [-8.816666666666666, 81.33333333333334, 0.0, 0.0, 26.0, 23.61909281924698, -0.08199286101699993, 0.0, 1.0, 45289.47449999175], 
processed observation next is [1.0, 0.17391304347826086, 0.21837488457987075, 0.8133333333333335, 0.0, 0.0, 0.6666666666666666, 0.4682577349372483, 0.47266904632766665, 0.0, 1.0, 0.215664164285675], 
reward next is 0.7843, 
noisyNet noise sample is [array([0.60851234], dtype=float32), 0.30340767]. 
=============================================
[2019-04-04 01:18:32,983] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.0036776e-25 1.9683561e-18 9.9852049e-17 1.0000000e+00 4.7788082e-20
 3.9773652e-14 1.8531963e-17], sum to 1.0000
[2019-04-04 01:18:32,983] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2403
[2019-04-04 01:18:33,028] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 86.0, 0.0, 0.0, 26.0, 24.57035902273183, 0.2092831662664165, 0.0, 1.0, 42741.71745526532], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2080800.0000, 
sim time next is 2081400.0000, 
raw observation next is [-4.583333333333333, 86.0, 0.0, 0.0, 26.0, 24.59129613072602, 0.2134904653535564, 0.0, 1.0, 42736.71546059498], 
processed observation next is [1.0, 0.08695652173913043, 0.3356417359187443, 0.86, 0.0, 0.0, 0.6666666666666666, 0.5492746775605015, 0.5711634884511855, 0.0, 1.0, 0.2035081688599761], 
reward next is 0.7965, 
noisyNet noise sample is [array([0.2779733], dtype=float32), 1.058186]. 
=============================================
[2019-04-04 01:18:37,235] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.9051911e-27 3.0880578e-19 2.9794711e-18 1.0000000e+00 3.8467877e-19
 8.0342304e-12 4.9034967e-19], sum to 1.0000
[2019-04-04 01:18:37,235] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4961
[2019-04-04 01:18:37,248] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.466666666666667, 28.0, 151.5, 287.6666666666667, 26.0, 25.84318506020445, 0.3811589009830172, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2558400.0000, 
sim time next is 2559000.0000, 
raw observation next is [3.383333333333333, 28.5, 144.0, 300.3333333333333, 26.0, 25.82019455185507, 0.3776960506233857, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5563250230840259, 0.285, 0.48, 0.3318600368324125, 0.6666666666666666, 0.6516828793212559, 0.6258986835411285, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.7559507], dtype=float32), -1.6991401]. 
=============================================
[2019-04-04 01:18:37,250] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[86.195984]
 [86.588326]
 [87.11742 ]
 [87.55617 ]
 [87.84518 ]], R is [[86.09729767]
 [86.23632812]
 [86.3739624 ]
 [86.51022339]
 [86.64511871]].
[2019-04-04 01:18:43,755] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.1273194e-28 2.5275408e-21 2.3243477e-18 1.0000000e+00 2.0839818e-21
 8.9737999e-15 2.9454767e-19], sum to 1.0000
[2019-04-04 01:18:43,755] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5515
[2019-04-04 01:18:43,810] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.716666666666667, 85.33333333333334, 0.0, 0.0, 26.0, 24.24048468821145, 0.1016902163409926, 0.0, 1.0, 43950.31764644528], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2256600.0000, 
sim time next is 2257200.0000, 
raw observation next is [-7.8, 86.0, 0.0, 0.0, 26.0, 24.12458785535122, 0.09641960786509296, 0.0, 1.0, 43748.21625982985], 
processed observation next is [1.0, 0.13043478260869565, 0.24653739612188366, 0.86, 0.0, 0.0, 0.6666666666666666, 0.5103823212792683, 0.5321398692883643, 0.0, 1.0, 0.2083248393325231], 
reward next is 0.7917, 
noisyNet noise sample is [array([0.8920545], dtype=float32), -0.980142]. 
=============================================
[2019-04-04 01:18:50,447] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.2492259e-22 1.4138500e-15 4.0522340e-15 9.9999928e-01 8.9525513e-17
 6.7942983e-07 6.5123339e-16], sum to 1.0000
[2019-04-04 01:18:50,447] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0928
[2019-04-04 01:18:50,526] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.899999999999999, 69.0, 126.1666666666667, 47.49999999999999, 26.0, 26.37578982576617, 0.5122659760153855, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2212800.0000, 
sim time next is 2213400.0000, 
raw observation next is [-3.9, 68.5, 132.3333333333333, 94.99999999999999, 26.0, 26.37744441032904, 0.5145133899573286, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3545706371191136, 0.685, 0.44111111111111095, 0.10497237569060772, 0.6666666666666666, 0.69812036752742, 0.6715044633191095, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5749515], dtype=float32), -2.1213555]. 
=============================================
[2019-04-04 01:18:57,710] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.3754711e-28 1.1197951e-19 5.6456957e-20 1.0000000e+00 1.4891623e-20
 1.5906199e-13 3.7527346e-19], sum to 1.0000
[2019-04-04 01:18:57,711] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4257
[2019-04-04 01:18:57,752] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 67.0, 64.83333333333333, 544.8333333333333, 26.0, 25.81173415692475, 0.5925010942891918, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3428400.0000, 
sim time next is 3429000.0000, 
raw observation next is [2.0, 67.0, 61.0, 513.0, 26.0, 26.13630691979093, 0.6168859111089703, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.518005540166205, 0.67, 0.20333333333333334, 0.5668508287292817, 0.6666666666666666, 0.6780255766492441, 0.7056286370363235, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4009755], dtype=float32), -0.2523242]. 
=============================================
[2019-04-04 01:18:57,756] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[88.15855]
 [88.31427]
 [88.01787]
 [88.2287 ]
 [88.67846]], R is [[88.08595276]
 [88.20509338]
 [87.86643982]
 [87.98777771]
 [88.09011078]].
[2019-04-04 01:19:09,256] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.4137422e-24 2.0726793e-18 2.0654190e-17 1.0000000e+00 3.1210536e-18
 8.6429310e-12 1.3893359e-17], sum to 1.0000
[2019-04-04 01:19:09,256] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9156
[2019-04-04 01:19:09,306] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 55.0, 0.0, 0.0, 26.0, 25.54508309760305, 0.5119056181226452, 1.0, 1.0, 42567.51680923666], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3351600.0000, 
sim time next is 3352200.0000, 
raw observation next is [-3.0, 55.0, 0.0, 0.0, 26.0, 25.56970928472168, 0.5112033098430627, 1.0, 1.0, 31645.36277230544], 
processed observation next is [1.0, 0.8260869565217391, 0.3795013850415513, 0.55, 0.0, 0.0, 0.6666666666666666, 0.6308091070601399, 0.670401103281021, 1.0, 1.0, 0.15069220367764496], 
reward next is 0.8493, 
noisyNet noise sample is [array([0.07549098], dtype=float32), 0.25826353]. 
=============================================
[2019-04-04 01:19:10,312] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.00853285e-20 4.66183725e-15 5.20295525e-12 1.00000000e+00
 1.09104375e-16 6.24994089e-13 5.84657973e-13], sum to 1.0000
[2019-04-04 01:19:10,312] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5427
[2019-04-04 01:19:10,370] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-11.0, 76.0, 0.0, 0.0, 26.0, 24.18613801973261, 0.1449318683728176, 0.0, 1.0, 43749.42652865235], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3304800.0000, 
sim time next is 3305400.0000, 
raw observation next is [-11.0, 76.0, 0.0, 0.0, 26.0, 24.11793709941718, 0.1380074312320739, 0.0, 1.0, 43805.50939683188], 
processed observation next is [1.0, 0.2608695652173913, 0.15789473684210528, 0.76, 0.0, 0.0, 0.6666666666666666, 0.5098280916180983, 0.5460024770773579, 0.0, 1.0, 0.20859766379443753], 
reward next is 0.7914, 
noisyNet noise sample is [array([1.748161], dtype=float32), 0.08024478]. 
=============================================
[2019-04-04 01:19:16,622] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2232464e-21 2.9354545e-15 6.8292453e-13 1.0000000e+00 3.2794492e-15
 1.6389753e-09 5.4580215e-15], sum to 1.0000
[2019-04-04 01:19:16,623] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4720
[2019-04-04 01:19:16,651] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.0, 77.0, 0.0, 0.0, 26.0, 24.97820978889614, 0.3489026623973899, 0.0, 1.0, 43816.69660523232], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3291000.0000, 
sim time next is 3291600.0000, 
raw observation next is [-8.0, 77.0, 0.0, 0.0, 26.0, 24.93338138084548, 0.3467009613226509, 0.0, 1.0, 43831.64941103532], 
processed observation next is [1.0, 0.08695652173913043, 0.24099722991689754, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5777817817371232, 0.6155669871075503, 0.0, 1.0, 0.20872214005254916], 
reward next is 0.7913, 
noisyNet noise sample is [array([-0.45485756], dtype=float32), -0.6620874]. 
=============================================
[2019-04-04 01:19:19,517] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.1746004e-26 1.3094121e-20 1.8403455e-18 1.0000000e+00 3.8166400e-23
 6.2329233e-15 1.3069931e-18], sum to 1.0000
[2019-04-04 01:19:19,517] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5231
[2019-04-04 01:19:19,557] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.5, 61.0, 0.0, 0.0, 26.0, 24.94630533871232, 0.3544340803718564, 0.0, 1.0, 199164.5343236049], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3699000.0000, 
sim time next is 3699600.0000, 
raw observation next is [3.333333333333333, 61.66666666666667, 0.0, 0.0, 26.0, 25.00459511940042, 0.398618395760452, 0.0, 1.0, 138851.2002389469], 
processed observation next is [0.0, 0.8260869565217391, 0.5549399815327793, 0.6166666666666667, 0.0, 0.0, 0.6666666666666666, 0.5837162599500351, 0.6328727985868173, 0.0, 1.0, 0.6611961916140329], 
reward next is 0.3388, 
noisyNet noise sample is [array([-0.22555093], dtype=float32), -1.1118467]. 
=============================================
[2019-04-04 01:19:21,018] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.3843933e-25 1.1300770e-19 6.9421100e-17 1.0000000e+00 7.6667242e-20
 8.1239938e-15 7.6536365e-19], sum to 1.0000
[2019-04-04 01:19:21,019] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5049
[2019-04-04 01:19:21,093] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.166666666666667, 66.0, 204.0, 110.6666666666666, 26.0, 24.97634778393286, 0.3313958141839803, 0.0, 1.0, 18747.26943325809], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2976600.0000, 
sim time next is 2977200.0000, 
raw observation next is [-3.0, 65.0, 217.0, 154.0, 26.0, 24.97851781779625, 0.3390317122333972, 0.0, 1.0, 18743.28183583772], 
processed observation next is [0.0, 0.4782608695652174, 0.3795013850415513, 0.65, 0.7233333333333334, 0.17016574585635358, 0.6666666666666666, 0.5815431514830207, 0.6130105707444657, 0.0, 1.0, 0.08925372302779866], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.38467458], dtype=float32), -0.8602466]. 
=============================================
[2019-04-04 01:19:25,568] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.6824805e-27 3.8428326e-24 3.3341069e-22 1.0000000e+00 1.3993145e-24
 2.9094970e-18 9.0779599e-21], sum to 1.0000
[2019-04-04 01:19:25,572] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8246
[2019-04-04 01:19:25,592] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-9.5, 60.5, 0.0, 0.0, 26.0, 22.94498010267443, -0.2167337204719176, 0.0, 1.0, 44127.59119289468], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2445000.0000, 
sim time next is 2445600.0000, 
raw observation next is [-9.5, 60.0, 0.0, 0.0, 26.0, 22.91169646573985, -0.2238539246343356, 0.0, 1.0, 44170.62234710282], 
processed observation next is [0.0, 0.30434782608695654, 0.1994459833795014, 0.6, 0.0, 0.0, 0.6666666666666666, 0.4093080388116543, 0.4253820251218881, 0.0, 1.0, 0.2103362968909658], 
reward next is 0.7897, 
noisyNet noise sample is [array([1.3676085], dtype=float32), -0.46720168]. 
=============================================
[2019-04-04 01:19:54,689] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.64082815e-27 3.88713211e-19 1.01209214e-16 1.00000000e+00
 2.39994125e-19 1.14272117e-13 2.88255238e-18], sum to 1.0000
[2019-04-04 01:19:54,690] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2453
[2019-04-04 01:19:54,778] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.0, 53.0, 95.5, 579.0, 26.0, 25.90696389088133, 0.4877949079078086, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3920400.0000, 
sim time next is 3921000.0000, 
raw observation next is [-7.833333333333334, 52.33333333333334, 97.0, 616.6666666666666, 26.0, 25.89694684767064, 0.4879557929688749, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.2456140350877193, 0.5233333333333334, 0.3233333333333333, 0.6813996316758747, 0.6666666666666666, 0.6580789039725534, 0.6626519309896249, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.92455274], dtype=float32), -0.2801923]. 
=============================================
[2019-04-04 01:19:54,781] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[85.54685 ]
 [85.67511 ]
 [85.95068 ]
 [86.30853 ]
 [86.374245]], R is [[85.69567108]
 [85.8387146 ]
 [85.98033142]
 [86.12052917]
 [86.25932312]].
[2019-04-04 01:19:55,863] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0155318e-28 8.5530548e-25 7.0778246e-22 1.0000000e+00 2.7251841e-24
 6.9068854e-21 2.9769330e-21], sum to 1.0000
[2019-04-04 01:19:55,868] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3239
[2019-04-04 01:19:55,882] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 68.0, 0.0, 0.0, 26.0, 24.97751142749461, 0.3429029210103531, 0.0, 1.0, 40934.31084290647], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3558600.0000, 
sim time next is 3559200.0000, 
raw observation next is [-4.666666666666666, 67.0, 0.0, 0.0, 26.0, 24.97029632341839, 0.3352025618720413, 0.0, 1.0, 40884.1116685058], 
processed observation next is [0.0, 0.17391304347826086, 0.33333333333333337, 0.67, 0.0, 0.0, 0.6666666666666666, 0.5808580269515323, 0.6117341872906804, 0.0, 1.0, 0.1946862460405038], 
reward next is 0.8053, 
noisyNet noise sample is [array([0.91495], dtype=float32), 2.0657241]. 
=============================================
[2019-04-04 01:19:59,199] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0788961e-24 5.1839103e-18 3.5607709e-16 1.0000000e+00 4.0797734e-20
 6.4898060e-13 3.7930370e-17], sum to 1.0000
[2019-04-04 01:19:59,199] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1325
[2019-04-04 01:19:59,227] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 75.0, 0.0, 0.0, 26.0, 25.42216109158245, 0.3925196854791252, 0.0, 1.0, 40841.07364047496], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3810000.0000, 
sim time next is 3810600.0000, 
raw observation next is [-4.0, 74.0, 0.0, 0.0, 26.0, 25.39480045768187, 0.3764057171508406, 0.0, 1.0, 55819.45573188274], 
processed observation next is [1.0, 0.08695652173913043, 0.3518005540166205, 0.74, 0.0, 0.0, 0.6666666666666666, 0.6162333714734892, 0.6254685723836135, 0.0, 1.0, 0.2658069320565845], 
reward next is 0.7342, 
noisyNet noise sample is [array([1.8016884], dtype=float32), -0.490334]. 
=============================================
[2019-04-04 01:20:05,223] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.5910404e-25 1.1698629e-16 1.0381743e-16 1.0000000e+00 7.6830468e-16
 2.3939870e-13 1.4216214e-17], sum to 1.0000
[2019-04-04 01:20:05,223] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6647
[2019-04-04 01:20:05,291] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.5, 50.5, 247.0, 48.0, 26.0, 25.24784483988324, 0.4870229381508324, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4541400.0000, 
sim time next is 4542000.0000, 
raw observation next is [2.666666666666667, 50.0, 249.8333333333333, 58.83333333333333, 26.0, 25.70309747438343, 0.5368024864095607, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5364727608494922, 0.5, 0.8327777777777776, 0.06500920810313075, 0.6666666666666666, 0.6419247895319525, 0.6789341621365202, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.300843], dtype=float32), -0.22263029]. 
=============================================
[2019-04-04 01:20:05,324] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[88.89733 ]
 [88.66448 ]
 [87.79156 ]
 [87.46106 ]
 [87.303246]], R is [[88.73822784]
 [88.85084534]
 [88.02788544]
 [87.98567963]
 [88.10582733]].
[2019-04-04 01:20:30,407] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.0004763e-34 1.9639681e-26 1.0478823e-24 1.0000000e+00 7.3034724e-30
 2.9832605e-24 3.8063730e-24], sum to 1.0000
[2019-04-04 01:20:30,407] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0456
[2019-04-04 01:20:30,416] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.333333333333333, 49.0, 83.16666666666666, 674.5, 26.0, 25.50916649209599, 0.484463627238676, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3685200.0000, 
sim time next is 3685800.0000, 
raw observation next is [5.166666666666667, 49.5, 79.33333333333334, 644.0, 26.0, 25.50079793509725, 0.4811161827962174, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.6057248384118191, 0.495, 0.2644444444444445, 0.7116022099447514, 0.6666666666666666, 0.6250664945914375, 0.6603720609320725, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5105581], dtype=float32), 0.2394796]. 
=============================================
[2019-04-04 01:20:58,915] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.5661949e-28 5.5136063e-21 5.2603228e-19 1.0000000e+00 4.6120961e-23
 9.3307011e-16 8.1995721e-19], sum to 1.0000
[2019-04-04 01:20:58,915] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8167
[2019-04-04 01:20:58,940] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.166666666666667, 51.5, 0.0, 0.0, 26.0, 25.91245550194355, 0.5884403907797537, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3520200.0000, 
sim time next is 3520800.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 26.047263098094, 0.5910786341754977, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.518005540166205, 0.52, 0.0, 0.0, 0.6666666666666666, 0.6706052581745, 0.6970262113918325, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.43058366], dtype=float32), 1.5179669]. 
=============================================
[2019-04-04 01:20:59,897] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.9301896e-27 1.7615707e-19 3.9242901e-19 1.0000000e+00 7.7182785e-21
 9.6843233e-10 1.3784595e-19], sum to 1.0000
[2019-04-04 01:20:59,897] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6478
[2019-04-04 01:21:00,002] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.5, 79.5, 135.0, 0.0, 26.0, 24.35241375108433, 0.38963219412645, 1.0, 1.0, 196918.6660207978], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4714200.0000, 
sim time next is 4714800.0000, 
raw observation next is [1.666666666666667, 77.33333333333334, 145.1666666666667, 0.9999999999999998, 26.0, 25.09705824846674, 0.5008140421113328, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5087719298245615, 0.7733333333333334, 0.48388888888888903, 0.0011049723756906074, 0.6666666666666666, 0.5914215207055618, 0.666938014037111, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9081561], dtype=float32), 0.43971545]. 
=============================================
[2019-04-04 01:21:09,571] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:21:09,571] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:21:09,578] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run20
[2019-04-04 01:21:14,577] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.7755769e-26 2.2982950e-17 5.7475765e-17 1.0000000e+00 1.3904858e-18
 3.5945842e-19 7.9009620e-18], sum to 1.0000
[2019-04-04 01:21:14,578] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7304
[2019-04-04 01:21:14,621] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.2, 46.0, 78.5, 146.0, 26.0, 27.43533925544794, 0.8502776278003483, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4640400.0000, 
sim time next is 4641000.0000, 
raw observation next is [5.000000000000001, 46.5, 65.66666666666666, 145.6666666666667, 26.0, 27.45660755216126, 0.8558786192500064, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.6011080332409973, 0.465, 0.21888888888888886, 0.16095764272559857, 0.6666666666666666, 0.7880506293467716, 0.7852928730833355, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5962941], dtype=float32), -0.0562221]. 
=============================================
[2019-04-04 01:21:14,672] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[85.79167 ]
 [86.075386]
 [86.32664 ]
 [86.58243 ]
 [86.805725]], R is [[85.62656403]
 [85.77030182]
 [85.91259766]
 [86.05347443]
 [86.19293976]].
[2019-04-04 01:21:20,701] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1136598e-28 2.7454428e-20 2.2221584e-19 1.0000000e+00 1.4145852e-22
 6.3431032e-19 7.6514102e-21], sum to 1.0000
[2019-04-04 01:21:20,701] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9165
[2019-04-04 01:21:20,780] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 71.0, 136.6666666666667, 283.6666666666666, 26.0, 26.07009877400824, 0.5170520238805114, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4611000.0000, 
sim time next is 4611600.0000, 
raw observation next is [-2.0, 71.0, 143.5, 340.0, 26.0, 26.0928857188016, 0.5235039272878574, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.40720221606648205, 0.71, 0.47833333333333333, 0.3756906077348066, 0.6666666666666666, 0.6744071432334667, 0.6745013090959525, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.08923719], dtype=float32), 0.12830964]. 
=============================================
[2019-04-04 01:21:35,522] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.4667047e-27 7.6734749e-22 1.1205220e-18 1.0000000e+00 4.0052287e-21
 1.3862013e-18 3.0549114e-19], sum to 1.0000
[2019-04-04 01:21:35,522] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1733
[2019-04-04 01:21:35,551] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 26.0, 25.39738424140091, 0.3438284723772014, 0.0, 1.0, 37956.49189091205], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4247400.0000, 
sim time next is 4248000.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 26.0, 25.47041839081218, 0.341873770492351, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.17391304347826086, 0.5457063711911359, 0.45, 0.0, 0.0, 0.6666666666666666, 0.6225348659010151, 0.6139579234974503, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.24460638], dtype=float32), -0.20308849]. 
=============================================
[2019-04-04 01:21:35,631] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[86.743866]
 [86.7547  ]
 [86.765976]
 [86.79706 ]
 [86.897255]], R is [[86.76211548]
 [86.71374512]
 [86.62402344]
 [86.5138092 ]
 [86.48743439]].
[2019-04-04 01:21:36,897] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.2297319e-27 6.5602801e-18 6.8190363e-18 1.0000000e+00 7.0497004e-20
 2.2668850e-12 3.6025205e-18], sum to 1.0000
[2019-04-04 01:21:36,897] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5796
[2019-04-04 01:21:36,912] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.3, 84.0, 142.5, 131.5, 26.0, 26.17501863482667, 0.5985868231425848, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4438800.0000, 
sim time next is 4439400.0000, 
raw observation next is [1.25, 84.33333333333333, 150.0, 97.99999999999999, 26.0, 26.16230816871725, 0.5924468941502902, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.497229916897507, 0.8433333333333333, 0.5, 0.10828729281767954, 0.6666666666666666, 0.6801923473931041, 0.6974822980500966, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5679302], dtype=float32), 0.19580212]. 
=============================================
[2019-04-04 01:21:40,439] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:21:40,440] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:21:40,443] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run20
[2019-04-04 01:21:44,747] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:21:44,748] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:21:44,769] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run20
[2019-04-04 01:21:45,735] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.1584140e-28 6.2461762e-24 4.8541931e-20 1.0000000e+00 2.3503122e-25
 3.3282031e-19 9.2086437e-22], sum to 1.0000
[2019-04-04 01:21:45,736] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3711
[2019-04-04 01:21:45,850] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.1666666666666667, 39.16666666666666, 0.0, 0.0, 26.0, 25.38287488961076, 0.34876330634148, 0.0, 1.0, 52746.11016070691], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4921800.0000, 
sim time next is 4922400.0000, 
raw observation next is [0.3333333333333333, 39.33333333333334, 0.0, 0.0, 26.0, 25.3695625616643, 0.3502312978502928, 0.0, 1.0, 50634.92232331035], 
processed observation next is [0.0, 1.0, 0.4718374884579871, 0.3933333333333334, 0.0, 0.0, 0.6666666666666666, 0.6141302134720249, 0.6167437659500976, 0.0, 1.0, 0.24111867773004927], 
reward next is 0.7589, 
noisyNet noise sample is [array([0.861212], dtype=float32), -0.7174155]. 
=============================================
[2019-04-04 01:21:52,934] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.7413711e-28 8.1360211e-24 3.8720730e-23 1.0000000e+00 8.0088408e-25
 5.0581393e-22 2.2092592e-20], sum to 1.0000
[2019-04-04 01:21:52,934] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6611
[2019-04-04 01:21:52,983] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.5, 57.5, 0.0, 0.0, 26.0, 25.44659885994127, 0.38376046284242, 0.0, 1.0, 18762.02291428322], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4836600.0000, 
sim time next is 4837200.0000, 
raw observation next is [-1.666666666666667, 58.33333333333334, 0.0, 0.0, 26.0, 25.40642491038673, 0.3765602112669865, 0.0, 1.0, 40982.17884510356], 
processed observation next is [0.0, 1.0, 0.4164358264081256, 0.5833333333333335, 0.0, 0.0, 0.6666666666666666, 0.6172020758655608, 0.6255200704223288, 0.0, 1.0, 0.19515323259573125], 
reward next is 0.8048, 
noisyNet noise sample is [array([-0.6910921], dtype=float32), -1.0051228]. 
=============================================
[2019-04-04 01:22:04,335] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:22:04,335] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:22:04,339] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run20
[2019-04-04 01:22:06,949] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.9305435e-33 2.7791351e-24 1.7406391e-23 1.0000000e+00 2.4288616e-26
 2.1669784e-24 9.8345218e-25], sum to 1.0000
[2019-04-04 01:22:06,963] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0037
[2019-04-04 01:22:06,974] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [11.0, 27.0, 101.0, 663.0, 26.0, 25.71866980057283, 0.4640595189472256, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3663000.0000, 
sim time next is 3663600.0000, 
raw observation next is [11.0, 27.33333333333333, 102.6666666666667, 679.6666666666666, 26.0, 25.69872895359791, 0.4653614037455568, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.7673130193905818, 0.27333333333333326, 0.3422222222222223, 0.751012891344383, 0.6666666666666666, 0.6415607461331593, 0.6551204679151855, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.27479392], dtype=float32), -0.54281646]. 
=============================================
[2019-04-04 01:22:10,935] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:22:10,935] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:22:10,938] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run20
[2019-04-04 01:22:11,528] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.5887608e-28 5.0423967e-23 2.0923548e-20 1.0000000e+00 1.0909608e-23
 1.2993332e-17 4.9524644e-20], sum to 1.0000
[2019-04-04 01:22:11,529] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3933
[2019-04-04 01:22:11,551] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-12.0, 63.0, 0.0, 0.0, 26.0, 24.27764875659684, 0.1446066767259603, 0.0, 1.0, 43599.81311811237], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3987000.0000, 
sim time next is 3987600.0000, 
raw observation next is [-12.0, 63.0, 0.0, 0.0, 26.0, 24.24840627776409, 0.1402732208850739, 0.0, 1.0, 43609.98490722045], 
processed observation next is [1.0, 0.13043478260869565, 0.13019390581717452, 0.63, 0.0, 0.0, 0.6666666666666666, 0.5207005231470075, 0.5467577402950247, 0.0, 1.0, 0.20766659479628788], 
reward next is 0.7923, 
noisyNet noise sample is [array([0.5115173], dtype=float32), -1.0628984]. 
=============================================
[2019-04-04 01:22:12,176] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:22:12,176] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:22:12,182] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run20
[2019-04-04 01:22:12,509] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.4729546e-28 3.1280367e-19 1.0029171e-18 1.0000000e+00 1.5404593e-20
 3.3599993e-15 2.7934578e-20], sum to 1.0000
[2019-04-04 01:22:12,509] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2926
[2019-04-04 01:22:12,576] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.3333333333333333, 58.66666666666666, 140.6666666666667, 681.0, 26.0, 26.4962597843862, 0.6442700392035007, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4615800.0000, 
sim time next is 4616400.0000, 
raw observation next is [0.6666666666666666, 57.33333333333334, 134.8333333333333, 724.0, 26.0, 26.55351915785292, 0.6613712479410242, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4810710987996307, 0.5733333333333335, 0.4494444444444443, 0.8, 0.6666666666666666, 0.7127932631544102, 0.720457082647008, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.646614], dtype=float32), -0.29253593]. 
=============================================
[2019-04-04 01:22:20,463] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:22:20,473] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:22:20,487] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run20
[2019-04-04 01:22:26,998] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.28176415e-23 7.96816070e-17 9.48638718e-17 1.00000000e+00
 2.87765382e-17 2.18550107e-15 1.68407471e-17], sum to 1.0000
[2019-04-04 01:22:26,999] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5119
[2019-04-04 01:22:27,010] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.833333333333333, 49.66666666666667, 0.0, 0.0, 26.0, 26.3180486800249, 0.7000707854686196, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4644600.0000, 
sim time next is 4645200.0000, 
raw observation next is [3.666666666666667, 50.33333333333334, 0.0, 0.0, 26.0, 26.38594522775912, 0.7075537596633508, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.564173591874423, 0.5033333333333334, 0.0, 0.0, 0.6666666666666666, 0.6988287689799266, 0.7358512532211169, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.20398597], dtype=float32), -1.3439422]. 
=============================================
[2019-04-04 01:22:27,018] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.5621774e-27 2.0914053e-20 7.4595892e-19 1.0000000e+00 5.6147651e-21
 5.2753712e-20 1.2812730e-19], sum to 1.0000
[2019-04-04 01:22:27,018] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2250
[2019-04-04 01:22:27,031] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.3333333333333333, 50.0, 0.0, 0.0, 26.0, 25.58831513019904, 0.4273685091937147, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5020800.0000, 
sim time next is 5021400.0000, 
raw observation next is [-0.6666666666666667, 52.5, 0.0, 0.0, 26.0, 25.53114108279354, 0.4078390198758606, 0.0, 1.0, 20263.08438090337], 
processed observation next is [1.0, 0.08695652173913043, 0.44413665743305636, 0.525, 0.0, 0.0, 0.6666666666666666, 0.6275950902327949, 0.6359463399586202, 0.0, 1.0, 0.09649087800430176], 
reward next is 0.9035, 
noisyNet noise sample is [array([-0.9279603], dtype=float32), 0.6784485]. 
=============================================
[2019-04-04 01:22:28,208] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.8497130e-31 3.7381821e-23 7.8488301e-21 1.0000000e+00 2.2127610e-26
 1.8773633e-21 7.1181652e-23], sum to 1.0000
[2019-04-04 01:22:28,208] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1467
[2019-04-04 01:22:28,287] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 57.0, 0.0, 0.0, 26.0, 25.89626280729216, 0.5633649579824205, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4659000.0000, 
sim time next is 4659600.0000, 
raw observation next is [2.0, 57.00000000000001, 0.0, 0.0, 26.0, 25.83097895836622, 0.5353054169650552, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.518005540166205, 0.5700000000000001, 0.0, 0.0, 0.6666666666666666, 0.6525815798638517, 0.6784351389883517, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.12881279], dtype=float32), 0.6433271]. 
=============================================
[2019-04-04 01:22:35,749] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:22:35,749] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:22:35,752] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run20
[2019-04-04 01:22:37,750] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.8049184e-22 5.5658324e-18 1.2408451e-13 1.0000000e+00 6.2850619e-17
 6.2897944e-11 7.0627968e-16], sum to 1.0000
[2019-04-04 01:22:37,758] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1319
[2019-04-04 01:22:37,775] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.900000000000002, 78.0, 0.0, 0.0, 23.0, 20.79904747719718, -0.7272366191205771, 0.0, 1.0, 46337.96970093639], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 195600.0000, 
sim time next is 196200.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 23.0, 20.7953570851673, -0.7337717303773929, 0.0, 1.0, 46337.0544081337], 
processed observation next is [1.0, 0.2608695652173913, 0.21606648199445982, 0.78, 0.0, 0.0, 0.4166666666666667, 0.23294642376394167, 0.25540942320753574, 0.0, 1.0, 0.2206526400387319], 
reward next is 0.7793, 
noisyNet noise sample is [array([2.2238705], dtype=float32), 0.31815532]. 
=============================================
[2019-04-04 01:22:41,105] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:22:41,105] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:22:41,108] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run20
[2019-04-04 01:22:42,623] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.2504234e-26 5.4608065e-17 6.3976475e-15 1.0000000e+00 2.1273823e-19
 1.1709195e-11 4.7886309e-18], sum to 1.0000
[2019-04-04 01:22:42,624] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3357
[2019-04-04 01:22:42,629] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [8.75, 19.0, 0.0, 0.0, 26.0, 26.82762060686223, 0.7916430149526003, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5089800.0000, 
sim time next is 5090400.0000, 
raw observation next is [8.7, 19.0, 0.0, 0.0, 26.0, 26.77383955398867, 0.7796239202815882, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.703601108033241, 0.19, 0.0, 0.0, 0.6666666666666666, 0.7311532961657224, 0.7598746400938627, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.03926528], dtype=float32), -0.99745446]. 
=============================================
[2019-04-04 01:22:44,343] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:22:44,343] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:22:44,347] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run20
[2019-04-04 01:22:48,115] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.2644280e-22 2.1379444e-14 7.3492128e-15 9.9999857e-01 1.8160296e-13
 1.3752340e-06 5.4689580e-14], sum to 1.0000
[2019-04-04 01:22:48,115] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7049
[2019-04-04 01:22:48,227] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.4166666666666667, 55.83333333333333, 115.3333333333333, 559.0, 25.0, 24.90889418991557, 0.1597457242079567, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 735000.0000, 
sim time next is 735600.0000, 
raw observation next is [-0.2333333333333334, 54.66666666666667, 123.1666666666667, 504.0, 25.0, 24.87104032048376, 0.153281041008187, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.456140350877193, 0.5466666666666667, 0.4105555555555557, 0.5569060773480663, 0.5833333333333334, 0.5725866933736468, 0.5510936803360623, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.33113545], dtype=float32), 0.432786]. 
=============================================
[2019-04-04 01:22:59,165] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:22:59,165] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:22:59,168] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run20
[2019-04-04 01:23:02,037] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.6328964e-20 4.5613021e-16 3.0195529e-14 1.0000000e+00 4.5225808e-15
 6.4735182e-11 1.2653596e-13], sum to 1.0000
[2019-04-04 01:23:02,037] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1831
[2019-04-04 01:23:02,102] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-9.100000000000001, 37.33333333333334, 0.0, 0.0, 23.0, 22.86644170337967, -0.4006209812558669, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 408000.0000, 
sim time next is 408600.0000, 
raw observation next is [-9.2, 38.0, 0.0, 0.0, 23.0, 22.7068530308923, -0.3765933941460791, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.20775623268698065, 0.38, 0.0, 0.0, 0.4166666666666667, 0.3922377525743584, 0.3744688686179736, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7747907], dtype=float32), -0.590515]. 
=============================================
[2019-04-04 01:23:04,833] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.1296336e-32 3.2166635e-26 4.2906236e-23 1.0000000e+00 9.2822656e-26
 8.7200992e-22 1.2656954e-24], sum to 1.0000
[2019-04-04 01:23:04,839] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4546
[2019-04-04 01:23:04,859] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.9, 69.66666666666667, 0.0, 0.0, 26.0, 24.92457056972982, 0.2889698093983748, 0.0, 1.0, 55950.34537021228], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4302600.0000, 
sim time next is 4303200.0000, 
raw observation next is [5.8, 70.33333333333334, 0.0, 0.0, 26.0, 24.88592487332432, 0.2970581490684241, 0.0, 1.0, 196792.842404876], 
processed observation next is [0.0, 0.8260869565217391, 0.6232686980609419, 0.7033333333333335, 0.0, 0.0, 0.6666666666666666, 0.5738270727770267, 0.599019383022808, 0.0, 1.0, 0.9371087733565523], 
reward next is 0.0629, 
noisyNet noise sample is [array([0.6996569], dtype=float32), -1.8867662]. 
=============================================
[2019-04-04 01:23:07,145] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:23:07,145] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:23:07,149] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run20
[2019-04-04 01:23:12,938] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.7422393e-21 5.7265695e-16 2.3458544e-15 1.0000000e+00 4.1403831e-14
 2.8435307e-10 4.4945062e-14], sum to 1.0000
[2019-04-04 01:23:12,939] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3626
[2019-04-04 01:23:12,983] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.6, 91.33333333333334, 0.0, 0.0, 23.0, 22.59019465974006, -0.3384356545555903, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 543000.0000, 
sim time next is 543600.0000, 
raw observation next is [0.5, 92.0, 0.0, 0.0, 23.0, 22.48557159957101, -0.3586999867426041, 0.0, 1.0, 45603.82334441595], 
processed observation next is [0.0, 0.30434782608695654, 0.4764542936288089, 0.92, 0.0, 0.0, 0.4166666666666667, 0.37379763329758414, 0.3804333377524653, 0.0, 1.0, 0.21716106354483786], 
reward next is 0.7828, 
noisyNet noise sample is [array([-0.47428545], dtype=float32), -1.7282305]. 
=============================================
[2019-04-04 01:23:14,654] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.1602339e-28 1.0498708e-24 4.1534823e-21 1.0000000e+00 2.8674964e-24
 2.5669521e-21 1.2185199e-20], sum to 1.0000
[2019-04-04 01:23:14,654] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0553
[2019-04-04 01:23:14,680] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.333333333333333, 61.66666666666667, 0.0, 0.0, 26.0, 24.60190460111177, 0.1812964499686152, 0.0, 1.0, 39491.69615424814], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4861200.0000, 
sim time next is 4861800.0000, 
raw observation next is [-3.5, 62.5, 0.0, 0.0, 26.0, 24.57619951156721, 0.1766617403886875, 0.0, 1.0, 39481.0942427219], 
processed observation next is [0.0, 0.2608695652173913, 0.36565096952908593, 0.625, 0.0, 0.0, 0.6666666666666666, 0.5480166259639342, 0.5588872467962291, 0.0, 1.0, 0.18800521067962808], 
reward next is 0.8120, 
noisyNet noise sample is [array([1.558903], dtype=float32), -1.9450464]. 
=============================================
[2019-04-04 01:23:23,929] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:23:23,929] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:23:23,932] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run20
[2019-04-04 01:23:30,314] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.4320479e-18 2.2814125e-12 1.9973410e-11 9.9994886e-01 1.8093308e-11
 5.1119991e-05 1.2152729e-13], sum to 1.0000
[2019-04-04 01:23:30,314] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9928
[2019-04-04 01:23:30,372] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.2, 66.16666666666667, 145.0, 0.0, 24.0, 23.70768650448325, -0.1788126274506392, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 215400.0000, 
sim time next is 216000.0000, 
raw observation next is [-5.0, 65.0, 141.0, 0.0, 24.0, 23.62175183198216, -0.1817606011761596, 1.0, 1.0, 96791.88997622542], 
processed observation next is [1.0, 0.5217391304347826, 0.32409972299168976, 0.65, 0.47, 0.0, 0.5, 0.4684793193318466, 0.4394131329412801, 1.0, 1.0, 0.4609137617915496], 
reward next is 0.5391, 
noisyNet noise sample is [array([1.2765709], dtype=float32), -1.0293974]. 
=============================================
[2019-04-04 01:23:30,396] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[89.26374 ]
 [89.405624]
 [89.616554]
 [89.633   ]
 [89.662926]], R is [[88.93412781]
 [89.04478455]
 [89.15433502]
 [89.26279449]
 [89.37017059]].
[2019-04-04 01:23:31,200] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:23:31,200] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:23:31,203] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run20
[2019-04-04 01:23:31,578] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.2500399e-29 1.0085872e-19 2.9382158e-19 1.0000000e+00 4.1461964e-23
 7.0958242e-16 3.7048951e-21], sum to 1.0000
[2019-04-04 01:23:31,582] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0549
[2019-04-04 01:23:31,620] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [8.666666666666668, 25.33333333333334, 75.33333333333333, 663.1666666666667, 26.0, 27.92154295607872, 0.9511176534397247, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4983600.0000, 
sim time next is 4984200.0000, 
raw observation next is [8.5, 25.5, 72.0, 641.0, 26.0, 28.0150313221705, 0.8218208028351995, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.698060941828255, 0.255, 0.24, 0.7082872928176795, 0.6666666666666666, 0.8345859435142083, 0.7739402676117332, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.7745137], dtype=float32), -1.0617871]. 
=============================================
[2019-04-04 01:23:39,219] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:23:39,219] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:23:39,223] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run20
[2019-04-04 01:23:43,137] A3C_AGENT_WORKER-Thread-4 INFO:Evaluating...
[2019-04-04 01:23:43,140] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 01:23:43,141] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:23:43,143] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run27
[2019-04-04 01:23:43,140] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 01:23:43,168] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 01:23:43,171] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:23:43,171] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:23:43,177] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run27
[2019-04-04 01:23:43,178] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run27
[2019-04-04 01:24:28,648] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.77442855], dtype=float32), 0.64938945]
[2019-04-04 01:24:28,648] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-3.9037219285, 75.378532855, 87.14736896, 0.0, 23.0, 22.11006746058877, -0.333960276854347, 0.0, 1.0, 88025.27282161174]
[2019-04-04 01:24:28,648] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 01:24:28,649] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.7271864e-20 2.3048722e-16 3.4148018e-14 1.0000000e+00 4.7486183e-15
 7.1037780e-11 2.8642040e-14], sampled 0.4377424405560193
[2019-04-04 01:24:43,811] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.77442855], dtype=float32), 0.64938945]
[2019-04-04 01:24:43,811] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [5.866666666666667, 40.83333333333334, 170.6666666666667, 493.6666666666666, 22.0, 21.291202861963, -0.5592127572640744, 0.0, 1.0, 0.0]
[2019-04-04 01:24:43,811] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 01:24:43,811] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.1815296e-21 1.1129708e-15 1.2272657e-14 1.0000000e+00 5.6694287e-15
 1.0103145e-11 4.5709193e-15], sampled 0.3520083874453803
[2019-04-04 01:24:44,961] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.77442855], dtype=float32), 0.64938945]
[2019-04-04 01:24:44,961] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [4.7, 52.5, 109.0, 738.0, 22.0, 22.81780553902657, -0.2977116360294196, 1.0, 1.0, 0.0]
[2019-04-04 01:24:44,961] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 01:24:44,962] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [5.0624657e-21 5.6499330e-15 3.7393903e-14 1.0000000e+00 7.7259644e-14
 5.0955837e-11 1.4961077e-14], sampled 0.5077306049068534
[2019-04-04 01:25:08,233] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7045.9649 178573045.5780 -816.5909
[2019-04-04 01:25:32,985] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7410.5094 223372413.3674 -400.7728
[2019-04-04 01:25:37,850] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7332.8259 237009799.4450 -500.3303
[2019-04-04 01:25:38,881] A3C_AGENT_WORKER-Thread-4 INFO:Global step: 2600000, evaluation results [2600000.0, 7410.50942865554, 223372413.36737862, -400.7728478112823, 7045.964862173479, 178573045.57803705, -816.5908539456814, 7332.825880514881, 237009799.4449714, -500.33025200307435]
[2019-04-04 01:25:39,459] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.0274683e-25 9.8379661e-19 1.5986287e-16 1.0000000e+00 8.4692099e-21
 1.7586652e-17 8.5178275e-17], sum to 1.0000
[2019-04-04 01:25:39,477] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7550
[2019-04-04 01:25:39,551] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.8, 73.5, 0.0, 0.0, 26.0, 23.8836114043466, 0.03824763272470615, 0.0, 1.0, 41400.44489688517], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 787800.0000, 
sim time next is 788400.0000, 
raw observation next is [-7.8, 74.0, 0.0, 0.0, 26.0, 23.85486926635022, 0.04293321476111586, 0.0, 1.0, 41343.9105910639], 
processed observation next is [1.0, 0.13043478260869565, 0.24653739612188366, 0.74, 0.0, 0.0, 0.6666666666666666, 0.48790577219585174, 0.5143110715870386, 0.0, 1.0, 0.1968757647193519], 
reward next is 0.8031, 
noisyNet noise sample is [array([0.46759745], dtype=float32), 1.2549413]. 
=============================================
[2019-04-04 01:25:43,319] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.6451835e-24 3.4019990e-19 6.7945055e-16 1.0000000e+00 6.8544268e-18
 2.5949235e-13 1.5285328e-16], sum to 1.0000
[2019-04-04 01:25:43,319] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6293
[2019-04-04 01:25:43,397] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.383333333333333, 59.83333333333333, 148.3333333333333, 80.66666666666667, 24.0, 23.05720536453181, -0.1905217656097976, 0.0, 1.0, 18716.55709107697], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 654600.0000, 
sim time next is 655200.0000, 
raw observation next is [-1.2, 60.0, 131.5, 74.5, 24.0, 23.043677711994, -0.1940578395614364, 0.0, 1.0, 28757.50454932095], 
processed observation next is [0.0, 0.6086956521739131, 0.42936288088642666, 0.6, 0.43833333333333335, 0.08232044198895028, 0.5, 0.42030647599950005, 0.43531405347952123, 0.0, 1.0, 0.1369404978539093], 
reward next is 0.8631, 
noisyNet noise sample is [array([-0.552767], dtype=float32), 0.055192046]. 
=============================================
[2019-04-04 01:25:57,483] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:25:57,483] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:25:57,492] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run20
[2019-04-04 01:26:16,554] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.8427947e-26 4.4683329e-18 2.4774772e-17 1.0000000e+00 1.6599248e-20
 2.3211970e-15 7.2457170e-17], sum to 1.0000
[2019-04-04 01:26:16,554] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8311
[2019-04-04 01:26:16,607] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.08333333333333331, 95.16666666666667, 0.0, 0.0, 26.0, 25.27972151952173, 0.5034857458584687, 0.0, 1.0, 45651.84401638278], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1378200.0000, 
sim time next is 1378800.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 26.0, 25.26184611126273, 0.5057941383026697, 0.0, 1.0, 43000.52090840264], 
processed observation next is [1.0, 1.0, 0.46260387811634357, 0.95, 0.0, 0.0, 0.6666666666666666, 0.6051538426052275, 0.6685980461008899, 0.0, 1.0, 0.20476438527810784], 
reward next is 0.7952, 
noisyNet noise sample is [array([-0.26885244], dtype=float32), 0.77842116]. 
=============================================
[2019-04-04 01:26:18,017] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.7368596e-24 2.7334251e-14 2.2722262e-14 9.9999881e-01 2.8876380e-15
 1.1617979e-06 5.3829336e-16], sum to 1.0000
[2019-04-04 01:26:18,022] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2093
[2019-04-04 01:26:18,030] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [14.2, 77.33333333333333, 0.0, 0.0, 24.0, 23.78698789477529, 0.1143262402500264, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1046400.0000, 
sim time next is 1047000.0000, 
raw observation next is [14.3, 77.16666666666667, 0.0, 0.0, 24.0, 23.72789570672162, 0.1088492075345888, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.8587257617728533, 0.7716666666666667, 0.0, 0.0, 0.5, 0.47732464222680154, 0.5362830691781962, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.51472175], dtype=float32), -1.9905221]. 
=============================================
[2019-04-04 01:26:18,037] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[93.99   ]
 [93.87164]
 [93.80379]
 [93.86568]
 [93.55803]], R is [[94.25952148]
 [94.31692505]
 [94.37375641]
 [94.43002319]
 [94.4857254 ]].
[2019-04-04 01:26:28,851] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.1789489e-24 7.1029555e-21 1.5837076e-18 1.0000000e+00 1.4897077e-20
 7.9557644e-18 1.7793269e-18], sum to 1.0000
[2019-04-04 01:26:28,851] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2101
[2019-04-04 01:26:28,882] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.133333333333333, 85.66666666666667, 0.0, 0.0, 26.0, 24.05260332406825, 0.08802125045654952, 0.0, 1.0, 46631.14208406426], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1824000.0000, 
sim time next is 1824600.0000, 
raw observation next is [-6.166666666666667, 86.33333333333333, 0.0, 0.0, 26.0, 24.01418466997853, 0.08028742380448409, 0.0, 1.0, 46691.55686762241], 
processed observation next is [0.0, 0.08695652173913043, 0.2917820867959372, 0.8633333333333333, 0.0, 0.0, 0.6666666666666666, 0.5011820558315442, 0.5267624746014947, 0.0, 1.0, 0.22234074698867812], 
reward next is 0.7777, 
noisyNet noise sample is [array([-0.13244827], dtype=float32), 0.6488665]. 
=============================================
[2019-04-04 01:26:30,478] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.5866772e-26 1.8142420e-17 1.7566153e-15 1.0000000e+00 5.2064805e-19
 1.7270925e-09 6.7825865e-18], sum to 1.0000
[2019-04-04 01:26:30,482] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8002
[2019-04-04 01:26:30,579] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.6, 100.0, 22.66666666666666, 0.0, 26.0, 25.73891010456513, 0.4962128018779601, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1413600.0000, 
sim time next is 1414200.0000, 
raw observation next is [-0.6, 100.0, 27.33333333333333, 0.0, 26.0, 25.83070926165366, 0.5079372288954108, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.44598337950138506, 1.0, 0.0911111111111111, 0.0, 0.6666666666666666, 0.6525591051378049, 0.6693124096318036, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4699125], dtype=float32), -0.73875237]. 
=============================================
[2019-04-04 01:26:33,016] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.7809698e-26 2.3929077e-22 2.7186302e-18 1.0000000e+00 5.4082809e-22
 5.7895622e-17 1.4433085e-18], sum to 1.0000
[2019-04-04 01:26:33,016] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2996
[2019-04-04 01:26:33,129] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-12.63333333333333, 69.0, 0.0, 0.0, 26.0, 23.0439268294754, -0.09216983758510912, 1.0, 1.0, 165322.7587848859], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 286800.0000, 
sim time next is 287400.0000, 
raw observation next is [-12.71666666666667, 69.5, 9.999999999999998, 145.3333333333333, 26.0, 23.64247417251087, 0.0001026232373322703, 1.0, 1.0, 114132.4140187481], 
processed observation next is [1.0, 0.30434782608695654, 0.1103416435826407, 0.695, 0.033333333333333326, 0.16058931860036826, 0.6666666666666666, 0.47020618104257245, 0.5000342077457774, 1.0, 1.0, 0.5434876858035623], 
reward next is 0.4565, 
noisyNet noise sample is [array([-0.25215], dtype=float32), 0.30306268]. 
=============================================
[2019-04-04 01:26:36,435] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.8734396e-32 1.0717218e-24 8.6100571e-23 1.0000000e+00 3.2602311e-28
 9.9343723e-24 6.8669647e-25], sum to 1.0000
[2019-04-04 01:26:36,435] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2416
[2019-04-04 01:26:36,485] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.0, 89.0, 0.0, 0.0, 26.0, 24.81006031840936, 0.2468770580027466, 0.0, 1.0, 39926.32922077351], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 522000.0000, 
sim time next is 522600.0000, 
raw observation next is [4.883333333333334, 88.83333333333334, 0.0, 0.0, 26.0, 24.93457131936568, 0.2499028289617798, 0.0, 1.0, 39679.55158920719], 
processed observation next is [0.0, 0.043478260869565216, 0.597876269621422, 0.8883333333333334, 0.0, 0.0, 0.6666666666666666, 0.5778809432804733, 0.5833009429872599, 0.0, 1.0, 0.1889502456628914], 
reward next is 0.8110, 
noisyNet noise sample is [array([0.34132686], dtype=float32), -0.36015907]. 
=============================================
[2019-04-04 01:26:47,152] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.9697454e-26 3.6657626e-18 5.4116559e-18 1.0000000e+00 4.6974101e-17
 4.6405354e-12 3.5716077e-19], sum to 1.0000
[2019-04-04 01:26:47,153] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2834
[2019-04-04 01:26:47,163] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 92.5, 44.0, 0.0, 25.0, 24.74611058647129, 0.2719617880619041, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 1351800.0000, 
sim time next is 1352400.0000, 
raw observation next is [1.1, 92.66666666666667, 39.66666666666667, 0.0, 25.0, 24.73402257763308, 0.268945397289133, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.49307479224376743, 0.9266666666666667, 0.13222222222222224, 0.0, 0.5833333333333334, 0.5611685481360901, 0.5896484657630444, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.15187801], dtype=float32), -0.78590775]. 
=============================================
[2019-04-04 01:26:53,042] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.6464482e-28 5.9025203e-22 2.9384624e-19 1.0000000e+00 6.4390481e-23
 1.5146703e-18 1.3885980e-19], sum to 1.0000
[2019-04-04 01:26:53,043] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0466
[2019-04-04 01:26:53,067] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.800000000000001, 76.16666666666667, 0.0, 0.0, 26.0, 24.36362838545525, 0.1603004131224579, 0.0, 1.0, 44138.97227329509], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2250600.0000, 
sim time next is 2251200.0000, 
raw observation next is [-6.9, 77.33333333333334, 0.0, 0.0, 26.0, 24.30344273523501, 0.1536821313464242, 0.0, 1.0, 44091.59668166084], 
processed observation next is [1.0, 0.043478260869565216, 0.27146814404432135, 0.7733333333333334, 0.0, 0.0, 0.6666666666666666, 0.5252868946029174, 0.5512273771154748, 0.0, 1.0, 0.20995998419838496], 
reward next is 0.7900, 
noisyNet noise sample is [array([-0.80876154], dtype=float32), -0.9032417]. 
=============================================
[2019-04-04 01:26:55,051] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.0400031e-23 8.5504342e-20 3.3999658e-17 1.0000000e+00 9.5792519e-19
 3.7874163e-15 4.0763884e-17], sum to 1.0000
[2019-04-04 01:26:55,051] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7991
[2019-04-04 01:26:55,076] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.600000000000001, 85.5, 0.0, 0.0, 25.0, 24.16909669927178, 0.03922551876052213, 0.0, 1.0, 45932.4646148327], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 1887000.0000, 
sim time next is 1887600.0000, 
raw observation next is [-5.6, 85.0, 0.0, 0.0, 25.0, 24.14610038297143, 0.042083324893211, 0.0, 1.0, 45813.40382807631], 
processed observation next is [0.0, 0.8695652173913043, 0.30747922437673136, 0.85, 0.0, 0.0, 0.5833333333333334, 0.5121750319142858, 0.5140277749644037, 0.0, 1.0, 0.21815906584798245], 
reward next is 0.7818, 
noisyNet noise sample is [array([0.30756205], dtype=float32), -0.18560639]. 
=============================================
[2019-04-04 01:27:06,787] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.9356294e-27 4.3202634e-18 1.7125342e-17 1.0000000e+00 4.0930839e-21
 5.4841512e-15 1.8771299e-18], sum to 1.0000
[2019-04-04 01:27:06,787] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8252
[2019-04-04 01:27:06,819] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.25, 80.5, 0.0, 0.0, 25.0, 24.64864509808399, 0.2664910216368624, 0.0, 1.0, 56383.97371659584], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 1578600.0000, 
sim time next is 1579200.0000, 
raw observation next is [5.333333333333334, 80.0, 0.0, 0.0, 25.0, 24.60445581525376, 0.2654059304005792, 0.0, 1.0, 65286.55149109649], 
processed observation next is [1.0, 0.2608695652173913, 0.6103416435826409, 0.8, 0.0, 0.0, 0.5833333333333334, 0.5503713179378135, 0.5884686434668597, 0.0, 1.0, 0.3108883404337928], 
reward next is 0.6891, 
noisyNet noise sample is [array([0.55959517], dtype=float32), 0.6154027]. 
=============================================
[2019-04-04 01:27:06,875] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.7243481e-22 1.4287166e-15 3.0926363e-15 9.9999940e-01 3.6472685e-16
 5.7674788e-07 7.2362697e-16], sum to 1.0000
[2019-04-04 01:27:06,875] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3014
[2019-04-04 01:27:06,908] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.9, 68.0, 150.8333333333333, 237.5, 26.0, 26.05155382181366, 0.4637470354561111, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2215200.0000, 
sim time next is 2215800.0000, 
raw observation next is [-3.9, 68.0, 157.0, 285.0, 26.0, 26.0565773907846, 0.4682958740542764, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.3545706371191136, 0.68, 0.5233333333333333, 0.3149171270718232, 0.6666666666666666, 0.67138144923205, 0.6560986246847588, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4026212], dtype=float32), 1.7654465]. 
=============================================
[2019-04-04 01:27:14,849] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.5904683e-24 1.5371787e-19 9.1979242e-17 1.0000000e+00 3.3499186e-20
 1.0550411e-18 1.0509204e-17], sum to 1.0000
[2019-04-04 01:27:14,849] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7109
[2019-04-04 01:27:14,942] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 85.0, 65.0, 0.0, 26.0, 25.09942625567625, 0.340154082476797, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1762200.0000, 
sim time next is 1762800.0000, 
raw observation next is [-2.1, 85.66666666666667, 70.33333333333334, 0.0, 26.0, 24.99368263050409, 0.3311117848044758, 0.0, 1.0, 35778.95819124929], 
processed observation next is [0.0, 0.391304347826087, 0.404432132963989, 0.8566666666666667, 0.23444444444444448, 0.0, 0.6666666666666666, 0.5828068858753408, 0.6103705949348253, 0.0, 1.0, 0.17037599138690138], 
reward next is 0.8296, 
noisyNet noise sample is [array([0.55316657], dtype=float32), 0.09834527]. 
=============================================
[2019-04-04 01:27:19,334] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.7157029e-24 1.5776613e-16 5.2352843e-17 1.0000000e+00 2.9524670e-16
 2.2867213e-09 2.6484428e-16], sum to 1.0000
[2019-04-04 01:27:19,335] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4832
[2019-04-04 01:27:19,406] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 88.66666666666667, 0.0, 0.0, 26.0, 25.68264038822476, 0.5517691543439806, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1447800.0000, 
sim time next is 1448400.0000, 
raw observation next is [1.1, 89.33333333333334, 0.0, 0.0, 26.0, 25.83243416984326, 0.5669742885559454, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.49307479224376743, 0.8933333333333334, 0.0, 0.0, 0.6666666666666666, 0.6527028474869384, 0.6889914295186484, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.8482689], dtype=float32), -1.5931711]. 
=============================================
[2019-04-04 01:27:20,577] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.9149346e-27 1.1026488e-19 2.0231150e-17 1.0000000e+00 3.5246788e-21
 1.8169312e-15 2.1726289e-19], sum to 1.0000
[2019-04-04 01:27:20,577] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8433
[2019-04-04 01:27:20,591] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.633333333333333, 55.0, 0.0, 0.0, 26.0, 24.63802137587905, 0.1362859535972211, 0.0, 1.0, 39082.22783874816], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2529600.0000, 
sim time next is 2530200.0000, 
raw observation next is [-2.716666666666667, 54.5, 0.0, 0.0, 26.0, 24.80433922454669, 0.1362537162609285, 0.0, 1.0, 38862.79525537875], 
processed observation next is [1.0, 0.2608695652173913, 0.3873499538319483, 0.545, 0.0, 0.0, 0.6666666666666666, 0.5670282687122242, 0.5454179054203095, 0.0, 1.0, 0.18506092978751784], 
reward next is 0.8149, 
noisyNet noise sample is [array([-1.6109563], dtype=float32), -0.44212776]. 
=============================================
[2019-04-04 01:27:22,279] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.6522160e-22 1.8725268e-15 5.7243693e-14 1.0000000e+00 4.5268270e-15
 2.4293250e-11 4.6896623e-15], sum to 1.0000
[2019-04-04 01:27:22,280] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2066
[2019-04-04 01:27:22,316] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.2, 45.66666666666667, 57.16666666666667, 6.999999999999998, 26.0, 25.89801671226422, 0.4090403500240003, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2305200.0000, 
sim time next is 2305800.0000, 
raw observation next is [-0.3, 46.5, 41.0, 0.0, 26.0, 25.89211668391099, 0.3955312094042918, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.4542936288088643, 0.465, 0.13666666666666666, 0.0, 0.6666666666666666, 0.657676390325916, 0.6318437364680972, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3170207], dtype=float32), 1.085362]. 
=============================================
[2019-04-04 01:27:49,128] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0004095e-20 2.8398501e-14 9.0666407e-14 1.0000000e+00 2.0977049e-13
 1.1635010e-12 1.2518038e-13], sum to 1.0000
[2019-04-04 01:27:49,129] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9470
[2019-04-04 01:27:49,236] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.516666666666667, 42.66666666666667, 0.0, 0.0, 26.0, 24.97948947597691, 0.328533669978761, 0.0, 1.0, 76606.1839674612], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2577000.0000, 
sim time next is 2577600.0000, 
raw observation next is [-1.7, 44.0, 0.0, 0.0, 26.0, 24.94975610980962, 0.3357578613471406, 0.0, 1.0, 76429.74243011576], 
processed observation next is [1.0, 0.8695652173913043, 0.4155124653739613, 0.44, 0.0, 0.0, 0.6666666666666666, 0.579146342484135, 0.6119192871157135, 0.0, 1.0, 0.3639511544291227], 
reward next is 0.6360, 
noisyNet noise sample is [array([0.00884924], dtype=float32), -0.80418897]. 
=============================================
[2019-04-04 01:27:49,571] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.2275970e-22 2.4718251e-15 9.3524076e-15 1.0000000e+00 8.9833125e-17
 2.8836875e-09 2.3193914e-15], sum to 1.0000
[2019-04-04 01:27:49,571] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5486
[2019-04-04 01:27:49,667] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.833333333333334, 71.0, 114.5, 75.16666666666664, 26.0, 25.99347109601194, 0.398106816250549, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2197200.0000, 
sim time next is 2197800.0000, 
raw observation next is [-4.75, 71.0, 117.0, 0.0, 26.0, 25.96423681258869, 0.3852809931762344, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.3310249307479225, 0.71, 0.39, 0.0, 0.6666666666666666, 0.6636864010490576, 0.6284269977254114, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.21650426], dtype=float32), -1.048416]. 
=============================================
[2019-04-04 01:28:08,674] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.2462586e-28 2.6373445e-22 3.0523719e-19 1.0000000e+00 1.1951197e-21
 5.4582573e-20 3.8586315e-20], sum to 1.0000
[2019-04-04 01:28:08,675] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3023
[2019-04-04 01:28:08,725] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 65.0, 217.0, 154.0, 26.0, 24.96730298896011, 0.3430155248392688, 0.0, 1.0, 18744.52164880158], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2977200.0000, 
sim time next is 2977800.0000, 
raw observation next is [-3.0, 65.0, 230.0, 197.3333333333333, 26.0, 24.99277822624642, 0.3474874764056046, 0.0, 1.0, 18740.88438848251], 
processed observation next is [0.0, 0.4782608695652174, 0.3795013850415513, 0.65, 0.7666666666666667, 0.21804788213627987, 0.6666666666666666, 0.5827315188538682, 0.6158291588018682, 0.0, 1.0, 0.08924230661182148], 
reward next is 0.9108, 
noisyNet noise sample is [array([-1.3815135], dtype=float32), 1.0773432]. 
=============================================
[2019-04-04 01:28:26,664] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.9332847e-28 8.0174548e-19 1.7493379e-17 1.0000000e+00 1.7753841e-22
 9.3217174e-20 3.0870331e-19], sum to 1.0000
[2019-04-04 01:28:26,665] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2892
[2019-04-04 01:28:26,696] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 26.0, 25.26189952082892, 0.4751863826007876, 0.0, 1.0, 39574.99844574536], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1392000.0000, 
sim time next is 1392600.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 26.0, 25.2938616862673, 0.4812090515199374, 0.0, 1.0, 39518.91794130145], 
processed observation next is [1.0, 0.08695652173913043, 0.46260387811634357, 0.95, 0.0, 0.0, 0.6666666666666666, 0.6078218071889415, 0.6604030171733125, 0.0, 1.0, 0.18818532353000691], 
reward next is 0.8118, 
noisyNet noise sample is [array([0.46493348], dtype=float32), -0.71378034]. 
=============================================
[2019-04-04 01:28:33,496] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.1721224e-20 2.1945634e-13 4.7042092e-12 1.0000000e+00 1.5286984e-13
 9.6974512e-13 4.6900515e-13], sum to 1.0000
[2019-04-04 01:28:33,496] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9143
[2019-04-04 01:28:33,562] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.833333333333334, 81.33333333333334, 99.33333333333334, 496.8333333333333, 25.0, 24.79961130408461, 0.1120992027390126, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 1935600.0000, 
sim time next is 1936200.0000, 
raw observation next is [-7.566666666666666, 80.16666666666667, 113.6666666666667, 444.6666666666667, 25.0, 24.81967656745084, 0.1121578421695224, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.2530009233610342, 0.8016666666666667, 0.378888888888889, 0.4913443830570903, 0.5833333333333334, 0.5683063806209034, 0.5373859473898408, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.223628], dtype=float32), -0.10794306]. 
=============================================
[2019-04-04 01:28:46,881] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.8348103e-26 1.1165240e-18 1.3024242e-18 1.0000000e+00 1.5891715e-19
 2.3300153e-15 2.5303990e-19], sum to 1.0000
[2019-04-04 01:28:46,882] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8772
[2019-04-04 01:28:46,919] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 82.5, 0.0, 0.0, 25.0, 24.33706061697853, 0.1175667855916038, 0.0, 1.0, 58281.17103688825], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 2860200.0000, 
sim time next is 2860800.0000, 
raw observation next is [1.0, 83.66666666666666, 0.0, 0.0, 25.0, 24.34094462886712, 0.1165604380339952, 0.0, 1.0, 56707.85225563236], 
processed observation next is [1.0, 0.08695652173913043, 0.4903047091412743, 0.8366666666666666, 0.0, 0.0, 0.5833333333333334, 0.5284120524055934, 0.5388534793446651, 0.0, 1.0, 0.27003739169348745], 
reward next is 0.7300, 
noisyNet noise sample is [array([0.32352522], dtype=float32), -1.6131759]. 
=============================================
[2019-04-04 01:28:55,601] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.9889695e-33 1.5983524e-26 6.5253494e-22 1.0000000e+00 9.7009881e-31
 1.6104337e-26 4.0952777e-25], sum to 1.0000
[2019-04-04 01:28:55,601] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9443
[2019-04-04 01:28:55,652] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.6666666666666667, 41.0, 100.6666666666667, 780.1666666666667, 26.0, 25.12082418796162, 0.3625730046273433, 0.0, 1.0, 18705.3729190088], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3075600.0000, 
sim time next is 3076200.0000, 
raw observation next is [-0.5, 40.5, 99.0, 775.0, 26.0, 25.1130155924581, 0.3639017956638428, 0.0, 1.0, 18705.09942018974], 
processed observation next is [0.0, 0.6086956521739131, 0.44875346260387816, 0.405, 0.33, 0.856353591160221, 0.6666666666666666, 0.5927512993715084, 0.6213005985546143, 0.0, 1.0, 0.08907190200090354], 
reward next is 0.9109, 
noisyNet noise sample is [array([-0.16227071], dtype=float32), -0.48253897]. 
=============================================
[2019-04-04 01:29:04,956] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.6055324e-24 8.1916435e-16 1.8266467e-15 1.0000000e+00 3.0807527e-18
 3.2313464e-12 6.7071806e-16], sum to 1.0000
[2019-04-04 01:29:04,956] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9408
[2019-04-04 01:29:05,050] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 89.33333333333334, 80.16666666666667, 0.0, 26.0, 25.88682366603287, 0.5301210403920936, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1682400.0000, 
sim time next is 1683000.0000, 
raw observation next is [1.1, 88.0, 83.0, 0.0, 26.0, 25.87959328587789, 0.5282578904086409, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.49307479224376743, 0.88, 0.27666666666666667, 0.0, 0.6666666666666666, 0.6566327738231573, 0.676085963469547, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.56424934], dtype=float32), -0.8280637]. 
=============================================
[2019-04-04 01:29:05,093] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[87.51329 ]
 [87.5892  ]
 [87.64443 ]
 [87.69328 ]
 [87.660324]], R is [[87.56729889]
 [87.6916275 ]
 [87.81471252]
 [87.93656921]
 [88.0572052 ]].
[2019-04-04 01:29:05,684] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.4177069e-23 2.0791751e-16 1.2031644e-16 1.0000000e+00 2.5292449e-16
 1.1973109e-12 1.4108656e-16], sum to 1.0000
[2019-04-04 01:29:05,685] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7571
[2019-04-04 01:29:05,828] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.8999999999999999, 50.5, 0.0, 0.0, 26.0, 24.89602084070247, 0.3163081444184522, 1.0, 1.0, 134172.4825826986], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2309400.0000, 
sim time next is 2310000.0000, 
raw observation next is [-1.0, 51.0, 0.0, 0.0, 26.0, 25.14731190054016, 0.3949647644201439, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.4349030470914128, 0.51, 0.0, 0.0, 0.6666666666666666, 0.5956093250450133, 0.6316549214733813, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6640668], dtype=float32), -0.25657815]. 
=============================================
[2019-04-04 01:29:05,862] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[81.39616 ]
 [81.17027 ]
 [80.61193 ]
 [80.38004 ]
 [80.410225]], R is [[81.94583893]
 [81.4874649 ]
 [80.73458862]
 [80.55245209]
 [80.74692535]].
[2019-04-04 01:29:29,971] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0669918e-28 1.2218886e-22 1.5961385e-21 1.0000000e+00 9.2622549e-24
 1.9313822e-18 5.3906498e-22], sum to 1.0000
[2019-04-04 01:29:29,974] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3278
[2019-04-04 01:29:30,062] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.0, 64.0, 0.0, 0.0, 26.0, 23.95861293375093, 0.005259043121450716, 0.0, 1.0, 60570.80851710204], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2788800.0000, 
sim time next is 2789400.0000, 
raw observation next is [-7.0, 64.0, 0.0, 0.0, 26.0, 23.8305905016928, -0.01978426754394838, 0.0, 1.0, 62304.33830104289], 
processed observation next is [1.0, 0.2608695652173913, 0.2686980609418283, 0.64, 0.0, 0.0, 0.6666666666666666, 0.4858825418077333, 0.4934052441520172, 0.0, 1.0, 0.29668732524306135], 
reward next is 0.7033, 
noisyNet noise sample is [array([-0.69212973], dtype=float32), -0.6388521]. 
=============================================
[2019-04-04 01:29:43,332] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.4682580e-27 7.6795698e-21 6.0901545e-18 1.0000000e+00 1.7844087e-21
 6.0203959e-18 2.6205520e-18], sum to 1.0000
[2019-04-04 01:29:43,332] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4177
[2019-04-04 01:29:43,364] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.9, 77.0, 0.0, 0.0, 26.0, 23.68428170636485, -0.01090498341248527, 0.0, 1.0, 41897.59913177026], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2183400.0000, 
sim time next is 2184000.0000, 
raw observation next is [-5.8, 76.33333333333334, 0.0, 0.0, 26.0, 23.6741201474958, -0.01199345257291051, 0.0, 1.0, 41898.09496792268], 
processed observation next is [1.0, 0.2608695652173913, 0.30193905817174516, 0.7633333333333334, 0.0, 0.0, 0.6666666666666666, 0.4728433456246499, 0.49600218247569655, 0.0, 1.0, 0.19951473794248897], 
reward next is 0.8005, 
noisyNet noise sample is [array([0.2600667], dtype=float32), -0.78076166]. 
=============================================
[2019-04-04 01:29:43,492] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[85.93027 ]
 [85.79985 ]
 [85.6736  ]
 [85.567444]
 [85.44942 ]], R is [[85.98928833]
 [85.92988586]
 [85.87110138]
 [85.81290436]
 [85.75524139]].
[2019-04-04 01:30:06,559] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0710742e-20 1.3788371e-13 2.3600937e-12 1.0000000e+00 1.5546905e-15
 1.3415194e-10 2.0206566e-15], sum to 1.0000
[2019-04-04 01:30:06,565] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8125
[2019-04-04 01:30:06,679] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.616666666666667, 64.0, 198.3333333333333, 123.0, 26.0, 25.90841419576057, 0.4272142756056829, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2117400.0000, 
sim time next is 2118000.0000, 
raw observation next is [-6.533333333333334, 64.0, 174.6666666666667, 128.5, 26.0, 25.8722144773472, 0.4060296204399876, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.28162511542012925, 0.64, 0.5822222222222224, 0.1419889502762431, 0.6666666666666666, 0.6560178731122667, 0.6353432068133292, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.96582454], dtype=float32), -0.83627075]. 
=============================================
[2019-04-04 01:30:06,700] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[84.92575 ]
 [85.185936]
 [85.7289  ]
 [86.52497 ]
 [87.59675 ]], R is [[84.96859741]
 [85.11891174]
 [85.26772308]
 [85.41504669]
 [85.56089783]].
[2019-04-04 01:30:10,729] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3991889e-28 1.4355325e-19 1.1212056e-19 1.0000000e+00 8.2091321e-23
 7.3164873e-17 9.4422853e-21], sum to 1.0000
[2019-04-04 01:30:10,729] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1893
[2019-04-04 01:30:10,785] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.0, 35.0, 213.5, 429.0, 26.0, 25.62177107835738, 0.4552403745118627, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2811600.0000, 
sim time next is 2812200.0000, 
raw observation next is [4.333333333333334, 34.16666666666667, 225.3333333333333, 343.6666666666666, 26.0, 25.76401512469754, 0.4684081524548733, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.58264081255771, 0.34166666666666673, 0.751111111111111, 0.37974217311233877, 0.6666666666666666, 0.6470012603914617, 0.6561360508182911, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5514802], dtype=float32), -0.49717224]. 
=============================================
[2019-04-04 01:30:18,240] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.2909685e-25 1.4763981e-17 5.8295196e-17 1.0000000e+00 9.0048408e-21
 2.1274909e-16 1.7211858e-17], sum to 1.0000
[2019-04-04 01:30:18,240] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2078
[2019-04-04 01:30:18,342] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 71.0, 0.0, 0.0, 26.0, 25.27910383714996, 0.4192409571554909, 0.0, 1.0, 43703.47214629628], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3801600.0000, 
sim time next is 3802200.0000, 
raw observation next is [-3.166666666666667, 72.0, 0.0, 0.0, 26.0, 25.25895005538244, 0.412421802430365, 0.0, 1.0, 43739.04406818285], 
processed observation next is [1.0, 0.0, 0.3748845798707295, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6049125046152032, 0.637473934143455, 0.0, 1.0, 0.20828116222944212], 
reward next is 0.7917, 
noisyNet noise sample is [array([-0.03595472], dtype=float32), -1.1734854]. 
=============================================
[2019-04-04 01:30:35,453] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.7232574e-27 3.0393685e-20 1.4271208e-16 1.0000000e+00 1.0996988e-21
 3.8018008e-17 1.7330904e-18], sum to 1.0000
[2019-04-04 01:30:35,453] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5033
[2019-04-04 01:30:35,510] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 63.33333333333333, 0.0, 0.0, 26.0, 24.62814785363297, 0.1969956253492204, 0.0, 1.0, 42831.29885714022], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3393600.0000, 
sim time next is 3394200.0000, 
raw observation next is [-3.0, 64.16666666666667, 0.0, 0.0, 26.0, 24.58076440851266, 0.1867316435234334, 0.0, 1.0, 42799.6811830096], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.6416666666666667, 0.0, 0.0, 0.6666666666666666, 0.5483970340427217, 0.5622438811744778, 0.0, 1.0, 0.20380800563337903], 
reward next is 0.7962, 
noisyNet noise sample is [array([0.77451754], dtype=float32), -0.4947184]. 
=============================================
[2019-04-04 01:30:37,225] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.8444053e-28 1.0704568e-21 9.5545823e-19 1.0000000e+00 1.6244669e-23
 8.7600547e-19 2.9365433e-21], sum to 1.0000
[2019-04-04 01:30:37,228] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5331
[2019-04-04 01:30:37,268] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.6, 54.0, 221.5, 212.0, 26.0, 25.03048418839453, 0.3232263594615148, 0.0, 1.0, 18715.50001435356], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2379600.0000, 
sim time next is 2380200.0000, 
raw observation next is [-0.5, 53.66666666666666, 211.3333333333333, 141.3333333333333, 26.0, 25.03838325543219, 0.3090551513658841, 0.0, 1.0, 18713.75125390452], 
processed observation next is [0.0, 0.5652173913043478, 0.44875346260387816, 0.5366666666666666, 0.7044444444444443, 0.15616942909760584, 0.6666666666666666, 0.5865319379526825, 0.603018383788628, 0.0, 1.0, 0.08911310120906914], 
reward next is 0.9109, 
noisyNet noise sample is [array([1.1068611], dtype=float32), -1.2439271]. 
=============================================
[2019-04-04 01:30:38,128] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.8586906e-24 1.9188412e-16 1.8020070e-16 1.0000000e+00 2.9070324e-19
 4.4521912e-12 2.5292629e-17], sum to 1.0000
[2019-04-04 01:30:38,129] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5537
[2019-04-04 01:30:38,172] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 92.0, 15.0, 130.0, 25.0, 24.89195070639095, 0.3213245953334108, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 3224400.0000, 
sim time next is 3225000.0000, 
raw observation next is [-3.0, 92.0, 29.0, 178.0, 25.0, 24.810314731687, 0.3211514574339736, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.3795013850415513, 0.92, 0.09666666666666666, 0.19668508287292819, 0.5833333333333334, 0.5675262276405834, 0.6070504858113245, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.44197667], dtype=float32), -0.88162667]. 
=============================================
[2019-04-04 01:30:38,177] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[80.10867 ]
 [79.67144 ]
 [78.878174]
 [78.055046]
 [77.9607  ]], R is [[80.47053528]
 [80.66583252]
 [80.85917664]
 [81.05058289]
 [81.03885651]].
[2019-04-04 01:30:50,537] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.1822878e-30 7.1782132e-22 6.9318551e-21 1.0000000e+00 1.0813848e-24
 6.3836105e-21 1.4033581e-21], sum to 1.0000
[2019-04-04 01:30:50,537] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6941
[2019-04-04 01:30:50,590] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 44.0, 151.5, 724.0, 26.0, 25.92630364538001, 0.4416651956173743, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2808000.0000, 
sim time next is 2808600.0000, 
raw observation next is [2.333333333333333, 42.50000000000001, 160.3333333333333, 711.0, 26.0, 25.90774032735774, 0.4435445306632212, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.5272391505078486, 0.42500000000000004, 0.5344444444444443, 0.7856353591160221, 0.6666666666666666, 0.658978360613145, 0.6478481768877403, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3581527], dtype=float32), -0.2947077]. 
=============================================
[2019-04-04 01:31:02,927] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.5617673e-22 2.3456270e-18 3.0616710e-16 1.0000000e+00 3.9965374e-17
 6.8798895e-18 4.9057867e-16], sum to 1.0000
[2019-04-04 01:31:02,928] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5795
[2019-04-04 01:31:02,970] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.5, 52.0, 0.0, 0.0, 25.0, 24.32234228527701, 0.1492050939395078, 0.0, 1.0, 41585.23196121296], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 4163400.0000, 
sim time next is 4164000.0000, 
raw observation next is [-3.666666666666667, 52.66666666666667, 0.0, 0.0, 25.0, 24.29496663017142, 0.1430523349188972, 0.0, 1.0, 40696.0161710048], 
processed observation next is [0.0, 0.17391304347826086, 0.3610341643582641, 0.5266666666666667, 0.0, 0.0, 0.5833333333333334, 0.5245805525142849, 0.5476841116396324, 0.0, 1.0, 0.19379055319526095], 
reward next is 0.8062, 
noisyNet noise sample is [array([-0.3496528], dtype=float32), 0.6268797]. 
=============================================
[2019-04-04 01:31:02,973] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[79.50366 ]
 [79.52025 ]
 [79.50795 ]
 [79.474884]
 [79.37143 ]], R is [[79.47373962]
 [79.48097229]
 [79.47701263]
 [79.4535141 ]
 [79.40492249]].
[2019-04-04 01:31:11,224] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.85380612e-30 3.21365444e-21 3.61346044e-19 1.00000000e+00
 1.05708892e-23 8.31768322e-18 1.18243915e-23], sum to 1.0000
[2019-04-04 01:31:11,224] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2830
[2019-04-04 01:31:11,249] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.766666666666667, 81.33333333333334, 100.0, 193.3333333333333, 26.0, 25.49174395821391, 0.5190591842427866, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4436400.0000, 
sim time next is 4437000.0000, 
raw observation next is [1.65, 82.0, 120.0, 232.0, 26.0, 25.61999353550278, 0.5553356864427936, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.5083102493074793, 0.82, 0.4, 0.256353591160221, 0.6666666666666666, 0.6349994612918982, 0.6851118954809312, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.78268635], dtype=float32), 0.2858409]. 
=============================================
[2019-04-04 01:31:11,259] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[94.10848]
 [94.17742]
 [94.00669]
 [93.64105]
 [92.93658]], R is [[94.09907532]
 [94.15808868]
 [94.21650696]
 [94.2743454 ]
 [94.331604  ]].
[2019-04-04 01:31:13,527] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.8327401e-30 1.1022728e-22 8.0451079e-22 1.0000000e+00 1.2189073e-25
 1.0729711e-21 2.3409462e-23], sum to 1.0000
[2019-04-04 01:31:13,527] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7798
[2019-04-04 01:31:13,559] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.5, 50.0, 106.0, 752.0, 25.0, 25.39012089951808, 0.4582321060030028, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 3335400.0000, 
sim time next is 3336000.0000, 
raw observation next is [-3.333333333333333, 50.0, 102.8333333333333, 739.0, 25.0, 25.59983600357373, 0.4746921268856346, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.37026777469990774, 0.5, 0.3427777777777777, 0.8165745856353591, 0.5833333333333334, 0.6333196669644776, 0.6582307089618782, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.86026275], dtype=float32), 1.6171399]. 
=============================================
[2019-04-04 01:31:13,586] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[84.08928]
 [84.22469]
 [83.99804]
 [83.56076]
 [83.49619]], R is [[84.10257721]
 [84.2615509 ]
 [84.41893768]
 [83.73311615]
 [83.89578247]].
[2019-04-04 01:31:14,415] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.7076088e-28 3.5997300e-19 4.3426998e-19 1.0000000e+00 1.4491148e-22
 5.4498186e-17 2.8554514e-20], sum to 1.0000
[2019-04-04 01:31:14,415] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2636
[2019-04-04 01:31:14,454] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 66.0, 0.0, 0.0, 26.0, 25.22160672340683, 0.4759457200592929, 0.0, 1.0, 56891.04325906662], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3538800.0000, 
sim time next is 3539400.0000, 
raw observation next is [-1.166666666666667, 65.0, 0.0, 0.0, 26.0, 25.30383700375276, 0.4805550296042948, 0.0, 1.0, 47453.64691436609], 
processed observation next is [1.0, 1.0, 0.43028624192059095, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6086530836460634, 0.6601850098680982, 0.0, 1.0, 0.22596974721126709], 
reward next is 0.7740, 
noisyNet noise sample is [array([-0.91041756], dtype=float32), -0.20547608]. 
=============================================
[2019-04-04 01:31:16,478] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.66750401e-30 1.25134023e-25 1.86672195e-22 1.00000000e+00
 8.01356955e-27 1.05043677e-22 3.03715046e-23], sum to 1.0000
[2019-04-04 01:31:16,478] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8533
[2019-04-04 01:31:16,497] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 50.0, 0.0, 0.0, 26.0, 25.20206355716115, 0.3483112827943852, 0.0, 1.0, 39492.11860272479], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4161000.0000, 
sim time next is 4161600.0000, 
raw observation next is [-3.0, 50.0, 0.0, 0.0, 26.0, 25.19253726195078, 0.3460076466977984, 0.0, 1.0, 39485.51428599418], 
processed observation next is [0.0, 0.17391304347826086, 0.3795013850415513, 0.5, 0.0, 0.0, 0.6666666666666666, 0.599378105162565, 0.6153358822325995, 0.0, 1.0, 0.18802625850473417], 
reward next is 0.8120, 
noisyNet noise sample is [array([0.16210994], dtype=float32), 0.18408014]. 
=============================================
[2019-04-04 01:31:24,065] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.4122316e-26 5.7955448e-19 4.6727263e-18 1.0000000e+00 7.8689726e-22
 1.2246421e-14 2.3924734e-19], sum to 1.0000
[2019-04-04 01:31:24,065] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2378
[2019-04-04 01:31:24,110] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.3333333333333334, 42.0, 0.0, 0.0, 26.0, 25.38450309455774, 0.4460619039379703, 0.0, 1.0, 154894.2705193115], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4142400.0000, 
sim time next is 4143000.0000, 
raw observation next is [0.1666666666666666, 42.5, 0.0, 0.0, 26.0, 25.32322131251004, 0.4550180339928384, 0.0, 1.0, 97333.9619998713], 
processed observation next is [1.0, 0.9565217391304348, 0.4672206832871654, 0.425, 0.0, 0.0, 0.6666666666666666, 0.6102684427091699, 0.6516726779976129, 0.0, 1.0, 0.4634950571422443], 
reward next is 0.5365, 
noisyNet noise sample is [array([-0.29002023], dtype=float32), -1.0028596]. 
=============================================
[2019-04-04 01:31:24,120] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[80.49023 ]
 [80.162865]
 [80.09379 ]
 [80.149765]
 [80.14443 ]], R is [[80.41145325]
 [79.86974335]
 [79.59069824]
 [79.79479218]
 [79.99684143]].
[2019-04-04 01:31:26,569] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.2247970e-28 2.3063023e-19 7.2441546e-21 1.0000000e+00 7.2950965e-21
 2.4943367e-18 2.4740817e-21], sum to 1.0000
[2019-04-04 01:31:26,569] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8185
[2019-04-04 01:31:26,618] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.5, 93.0, 6.0, 41.0, 26.0, 25.77842455077545, 0.2573971221913416, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2914200.0000, 
sim time next is 2914800.0000, 
raw observation next is [1.333333333333333, 93.0, 0.0, 0.0, 26.0, 25.42314265648776, 0.3924619869948376, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.4995383194829178, 0.93, 0.0, 0.0, 0.6666666666666666, 0.6185952213739799, 0.6308206623316125, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3300039], dtype=float32), 0.9202337]. 
=============================================
[2019-04-04 01:31:27,915] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5384312e-30 9.6891284e-21 2.1060482e-20 1.0000000e+00 3.2693541e-24
 1.8228989e-18 6.9201206e-22], sum to 1.0000
[2019-04-04 01:31:27,916] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5767
[2019-04-04 01:31:27,970] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.583333333333334, 49.0, 160.3333333333333, 741.6666666666667, 26.0, 27.07778726050887, 0.7017313187373194, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4629000.0000, 
sim time next is 4629600.0000, 
raw observation next is [4.7, 49.0, 171.0, 706.0, 26.0, 26.61576101537805, 0.7597545918287292, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.592797783933518, 0.49, 0.57, 0.7801104972375691, 0.6666666666666666, 0.7179800846148376, 0.7532515306095764, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.9583479], dtype=float32), 1.7018069]. 
=============================================
[2019-04-04 01:31:28,514] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1850868e-28 7.0716017e-19 5.8953315e-18 1.0000000e+00 2.9671940e-21
 2.0554987e-18 1.7129432e-20], sum to 1.0000
[2019-04-04 01:31:28,514] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2348
[2019-04-04 01:31:28,532] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 49.0, 95.0, 734.0, 25.0, 25.82287827053198, 0.5178358592110152, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 3510000.0000, 
sim time next is 3510600.0000, 
raw observation next is [3.0, 49.0, 92.66666666666666, 718.3333333333333, 25.0, 25.91037939429874, 0.5354344322935366, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5457063711911359, 0.49, 0.3088888888888889, 0.7937384898710865, 0.5833333333333334, 0.6591982828582283, 0.6784781440978455, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.15333994], dtype=float32), -0.54319847]. 
=============================================
[2019-04-04 01:31:45,360] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.5383162e-27 5.5359489e-20 1.4017511e-16 1.0000000e+00 5.3184043e-22
 4.1356255e-17 2.6626918e-19], sum to 1.0000
[2019-04-04 01:31:45,396] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6799
[2019-04-04 01:31:45,410] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.82562588944677, 0.2876467363180348, 0.0, 1.0, 41138.59873195459], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3378600.0000, 
sim time next is 3379200.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.79747182757382, 0.287994475256888, 0.0, 1.0, 41126.18245550541], 
processed observation next is [1.0, 0.08695652173913043, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5664559856311516, 0.5959981584189626, 0.0, 1.0, 0.1958389640738353], 
reward next is 0.8042, 
noisyNet noise sample is [array([-0.07631707], dtype=float32), 0.55140764]. 
=============================================
[2019-04-04 01:31:53,458] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:31:53,458] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:31:53,461] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run21
[2019-04-04 01:32:01,636] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4909758e-27 2.3926910e-20 1.4859519e-18 1.0000000e+00 4.3656790e-23
 7.8367594e-17 2.6828282e-19], sum to 1.0000
[2019-04-04 01:32:01,637] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9338
[2019-04-04 01:32:01,698] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.833333333333333, 49.33333333333334, 0.0, 0.0, 26.0, 24.90349351180355, 0.2005296575958488, 0.0, 1.0, 38750.74869006513], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4950600.0000, 
sim time next is 4951200.0000, 
raw observation next is [-2.666666666666667, 48.66666666666667, 0.0, 0.0, 26.0, 24.87444345407534, 0.2415114556237716, 1.0, 1.0, 8336.977859270868], 
processed observation next is [1.0, 0.30434782608695654, 0.38873499538319484, 0.4866666666666667, 0.0, 0.0, 0.6666666666666666, 0.5728702878396117, 0.5805038185412572, 1.0, 1.0, 0.039699894567956515], 
reward next is 0.9603, 
noisyNet noise sample is [array([1.160015], dtype=float32), 1.2867048]. 
=============================================
[2019-04-04 01:32:02,238] A3C_AGENT_WORKER-Thread-2 INFO:Local step 170000, global step 2698664: loss 1.1954
[2019-04-04 01:32:02,239] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 170000, global step 2698664: learning rate 0.0005
[2019-04-04 01:32:02,789] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.3502162e-28 1.3004192e-17 1.4117001e-16 1.0000000e+00 1.2808032e-20
 1.1104209e-17 2.7764188e-19], sum to 1.0000
[2019-04-04 01:32:02,792] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1194
[2019-04-04 01:32:02,800] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [10.06666666666667, 59.33333333333334, 0.0, 0.0, 26.0, 26.81931318850354, 0.8357670932640312, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4396200.0000, 
sim time next is 4396800.0000, 
raw observation next is [9.933333333333334, 59.66666666666667, 0.0, 0.0, 26.0, 26.75662947708746, 0.8242323093946856, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.7377654662973223, 0.5966666666666667, 0.0, 0.0, 0.6666666666666666, 0.7297191230906218, 0.774744103131562, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5415183], dtype=float32), 2.052433]. 
=============================================
[2019-04-04 01:32:04,839] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-04-04 01:32:04,839] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 01:32:04,840] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:32:04,840] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 01:32:04,840] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 01:32:04,840] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:32:04,841] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:32:04,851] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run28
[2019-04-04 01:32:04,851] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run28
[2019-04-04 01:32:04,894] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run28
[2019-04-04 01:32:20,534] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.8499182], dtype=float32), 0.7884266]
[2019-04-04 01:32:20,534] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [0.9000000000000001, 89.33333333333334, 0.0, 0.0, 19.0, 18.63550005332982, -1.197974181994865, 0.0, 1.0, 51905.99074170427]
[2019-04-04 01:32:20,534] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 01:32:20,535] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [6.0358581e-15 1.3459630e-11 4.1308942e-10 1.0000000e+00 2.4267248e-09
 4.5460428e-08 8.0419965e-10], sampled 0.446921391357744
[2019-04-04 01:32:33,053] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.8499182], dtype=float32), 0.7884266]
[2019-04-04 01:32:33,053] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [0.0, 95.0, 93.0, 0.0, 19.0, 19.83669125593092, -0.9381894014351393, 1.0, 1.0, 0.0]
[2019-04-04 01:32:33,053] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 01:32:33,054] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.1321177e-11 3.5401428e-08 9.3267140e-08 9.9968874e-01 8.7242217e-05
 2.2377692e-04 1.4516758e-07], sampled 0.5658737363737335
[2019-04-04 01:32:58,050] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.8499182], dtype=float32), 0.7884266]
[2019-04-04 01:32:58,051] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-0.7, 66.0, 0.0, 0.0, 19.0, 18.75057543192015, -1.216529135624336, 0.0, 1.0, 0.0]
[2019-04-04 01:32:58,051] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 01:32:58,052] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.47406501e-14 1.28274535e-11 4.73662665e-10 1.00000000e+00
 5.58772761e-09 3.39006725e-08 1.00052389e-09], sampled 0.3114378101202038
[2019-04-04 01:33:17,538] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5565.8460 144499437.7245 -2172.8077
[2019-04-04 01:33:28,527] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 4995.5949 172072548.2054 -2849.9013
[2019-04-04 01:33:33,245] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5079.4222 185928468.5697 -2764.1870
[2019-04-04 01:33:34,267] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 2700000, evaluation results [2700000.0, 4995.594892684556, 172072548.2054224, -2849.901282589651, 5565.845956151186, 144499437.72453028, -2172.8077109675396, 5079.422220918253, 185928468.56967267, -2764.1869987705086]
[2019-04-04 01:33:37,765] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:33:37,765] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:33:37,769] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run21
[2019-04-04 01:33:39,174] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.9847426e-25 3.8770068e-17 1.6714911e-18 1.0000000e+00 2.4914564e-17
 1.1508620e-10 1.1946656e-18], sum to 1.0000
[2019-04-04 01:33:39,175] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3448
[2019-04-04 01:33:39,198] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 50.66666666666666, 147.0, 7.999999999999998, 25.0, 25.16626614490148, 0.1920767657274607, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 4538400.0000, 
sim time next is 4539000.0000, 
raw observation next is [2.0, 51.33333333333334, 167.0, 16.0, 25.0, 24.47539323374467, 0.1876770726069354, 1.0, 1.0, 31567.51426128947], 
processed observation next is [1.0, 0.5217391304347826, 0.518005540166205, 0.5133333333333334, 0.5566666666666666, 0.017679558011049725, 0.5833333333333334, 0.5396161028120557, 0.5625590242023119, 1.0, 1.0, 0.1503214964823308], 
reward next is 0.8497, 
noisyNet noise sample is [array([-2.3865879], dtype=float32), 0.9538509]. 
=============================================
[2019-04-04 01:33:39,206] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[88.99997 ]
 [88.36775 ]
 [88.40549 ]
 [88.58559 ]
 [88.815735]], R is [[89.59153748]
 [89.69562531]
 [89.79866791]
 [89.90068054]
 [90.00167084]].
[2019-04-04 01:33:39,787] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:33:39,787] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:33:39,795] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run21
[2019-04-04 01:33:46,742] A3C_AGENT_WORKER-Thread-19 INFO:Local step 170000, global step 2705092: loss 3.6966
[2019-04-04 01:33:46,743] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 170000, global step 2705092: learning rate 0.0005
[2019-04-04 01:33:46,826] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.0572629e-27 3.7808117e-19 1.5011239e-18 1.0000000e+00 3.7502838e-21
 2.1567445e-16 3.0986545e-19], sum to 1.0000
[2019-04-04 01:33:46,826] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6626
[2019-04-04 01:33:46,869] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:33:46,870] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:33:46,874] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.333333333333333, 50.33333333333333, 102.1666666666667, 705.8333333333334, 26.0, 26.12451886669933, 0.5478546574760085, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3922800.0000, 
sim time next is 3923400.0000, 
raw observation next is [-7.166666666666667, 49.66666666666667, 104.3333333333333, 719.6666666666666, 26.0, 26.20723230170282, 0.5578768491593603, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.26408125577100644, 0.4966666666666667, 0.3477777777777777, 0.7952117863720073, 0.6666666666666666, 0.6839360251419017, 0.6859589497197868, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3469256], dtype=float32), -2.4786618]. 
=============================================
[2019-04-04 01:33:46,879] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run21
[2019-04-04 01:33:48,011] A3C_AGENT_WORKER-Thread-17 INFO:Local step 170000, global step 2705631: loss 0.5640
[2019-04-04 01:33:48,011] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 170000, global step 2705631: learning rate 0.0005
[2019-04-04 01:33:52,393] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.7277655e-23 2.8880695e-16 6.7564599e-16 1.0000000e+00 6.2536575e-18
 3.4040346e-13 1.5151275e-17], sum to 1.0000
[2019-04-04 01:33:52,394] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6919
[2019-04-04 01:33:52,406] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.1666666666666666, 77.0, 19.33333333333334, 18.66666666666667, 26.0, 25.55123282512904, 0.4862958432893925, 1.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4729800.0000, 
sim time next is 4730400.0000, 
raw observation next is [0.0, 78.0, 0.0, 0.0, 26.0, 25.47333697474144, 0.4759708518715093, 1.0, 1.0, 19994.67417120794], 
processed observation next is [1.0, 0.782608695652174, 0.46260387811634357, 0.78, 0.0, 0.0, 0.6666666666666666, 0.6227780812284532, 0.6586569506238364, 1.0, 1.0, 0.09521273414860923], 
reward next is 0.9048, 
noisyNet noise sample is [array([0.47217405], dtype=float32), 0.6467651]. 
=============================================
[2019-04-04 01:33:54,868] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.5072516e-16 1.8880741e-11 3.0328895e-12 1.0000000e+00 3.8666595e-10
 1.0609186e-08 1.0705235e-11], sum to 1.0000
[2019-04-04 01:33:54,868] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5564
[2019-04-04 01:33:54,875] A3C_AGENT_WORKER-Thread-11 INFO:Local step 170000, global step 2708474: loss 0.6178
[2019-04-04 01:33:54,889] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 170000, global step 2708476: learning rate 0.0005
[2019-04-04 01:33:54,894] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.3, 71.0, 0.0, 0.0, 21.5, 21.63182912463232, -0.6064998630502006, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.5], 
sim time this is 147600.0000, 
sim time next is 148200.0000, 
raw observation next is [-7.3, 69.33333333333334, 0.0, 0.0, 21.5, 21.62279077488649, -0.6143281579542652, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.26038781163434904, 0.6933333333333335, 0.0, 0.0, 0.2916666666666667, 0.3018992312405408, 0.29522394734857826, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.6104939], dtype=float32), 0.020074794]. 
=============================================
[2019-04-04 01:33:55,242] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:33:55,242] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:33:55,277] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run21
[2019-04-04 01:33:56,321] A3C_AGENT_WORKER-Thread-2 INFO:Local step 170500, global step 2709092: loss 0.0048
[2019-04-04 01:33:56,335] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 170500, global step 2709096: learning rate 0.0005
[2019-04-04 01:34:00,047] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:34:00,047] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:34:00,053] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run21
[2019-04-04 01:34:03,577] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.7506993e-31 6.3323948e-26 4.4471142e-24 1.0000000e+00 8.1274783e-27
 9.7676126e-22 4.4988340e-23], sum to 1.0000
[2019-04-04 01:34:03,578] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3593
[2019-04-04 01:34:03,631] A3C_AGENT_WORKER-Thread-20 INFO:Local step 170000, global step 2711929: loss 0.3575
[2019-04-04 01:34:03,632] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 170000, global step 2711929: learning rate 0.0005
[2019-04-04 01:34:03,641] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.166666666666667, 46.5, 97.33333333333334, 545.3333333333333, 26.0, 25.61950657215838, 0.4423004978841542, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4179000.0000, 
sim time next is 4179600.0000, 
raw observation next is [-4.0, 45.0, 100.0, 574.0, 26.0, 25.6485749321441, 0.4382580272687553, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.3518005540166205, 0.45, 0.3333333333333333, 0.6342541436464089, 0.6666666666666666, 0.6373812443453417, 0.6460860090895851, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.37837678], dtype=float32), -0.31021374]. 
=============================================
[2019-04-04 01:34:06,017] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:34:06,017] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:34:06,020] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run21
[2019-04-04 01:34:08,145] A3C_AGENT_WORKER-Thread-16 INFO:Local step 170000, global step 2713789: loss 0.3366
[2019-04-04 01:34:08,146] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 170000, global step 2713789: learning rate 0.0005
[2019-04-04 01:34:09,724] A3C_AGENT_WORKER-Thread-19 INFO:Local step 170500, global step 2714398: loss 0.1039
[2019-04-04 01:34:09,725] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 170500, global step 2714398: learning rate 0.0005
[2019-04-04 01:34:10,402] A3C_AGENT_WORKER-Thread-17 INFO:Local step 170500, global step 2714693: loss 0.0130
[2019-04-04 01:34:10,403] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 170500, global step 2714693: learning rate 0.0005
[2019-04-04 01:34:12,079] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:34:12,079] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:34:12,083] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run21
[2019-04-04 01:34:12,356] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.5061843e-24 1.3371245e-19 2.9604713e-18 1.0000000e+00 6.8563884e-18
 2.3462291e-16 4.7915885e-19], sum to 1.0000
[2019-04-04 01:34:12,364] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2479
[2019-04-04 01:34:12,376] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.2, 96.0, 0.0, 0.0, 19.0, 18.97235217935731, -1.07157199898556, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 5400.0000, 
sim time next is 6000.0000, 
raw observation next is [7.2, 96.0, 0.0, 0.0, 19.0, 18.97235468660214, -1.072640072431482, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.043478260869565216, 0.662049861495845, 0.96, 0.0, 0.0, 0.08333333333333333, 0.08102955721684513, 0.14245330918950602, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2170985], dtype=float32), -0.68003696]. 
=============================================
[2019-04-04 01:34:12,408] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[71.38444]
 [72.58675]
 [73.2047 ]
 [74.2489 ]
 [74.38716]], R is [[70.8399353 ]
 [71.13153839]
 [71.42022705]
 [71.70602417]
 [71.9889679 ]].
[2019-04-04 01:34:12,989] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.1376453e-31 8.0665144e-25 9.5024702e-24 1.0000000e+00 2.8050979e-27
 7.6268656e-23 5.5634351e-25], sum to 1.0000
[2019-04-04 01:34:12,990] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8573
[2019-04-04 01:34:13,003] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 26.0, 25.35417754230103, 0.3305525311334556, 0.0, 1.0, 41402.84626371069], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4251600.0000, 
sim time next is 4252200.0000, 
raw observation next is [3.0, 45.66666666666666, 0.0, 0.0, 26.0, 25.39400295543158, 0.33074559581388, 0.0, 1.0, 28618.8186536752], 
processed observation next is [0.0, 0.21739130434782608, 0.5457063711911359, 0.45666666666666655, 0.0, 0.0, 0.6666666666666666, 0.6161669129526318, 0.61024853193796, 0.0, 1.0, 0.13628008882702478], 
reward next is 0.8637, 
noisyNet noise sample is [array([1.125466], dtype=float32), -1.8252473]. 
=============================================
[2019-04-04 01:34:13,801] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:34:13,801] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:34:13,834] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run21
[2019-04-04 01:34:13,991] A3C_AGENT_WORKER-Thread-18 INFO:Local step 170000, global step 2716118: loss 0.2706
[2019-04-04 01:34:14,000] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 170000, global step 2716118: learning rate 0.0005
[2019-04-04 01:34:14,373] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:34:14,373] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:34:14,381] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run21
[2019-04-04 01:34:15,147] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1918029e-25 8.5807090e-16 3.4269752e-15 1.0000000e+00 1.3274196e-16
 1.4715600e-13 2.3658206e-17], sum to 1.0000
[2019-04-04 01:34:15,147] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2008
[2019-04-04 01:34:15,166] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.666666666666667, 50.0, 121.6666666666667, 837.3333333333334, 25.0, 25.96377046757393, 0.5631441684705641, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 4621200.0000, 
sim time next is 4621800.0000, 
raw observation next is [2.833333333333333, 49.5, 121.3333333333333, 841.6666666666666, 25.0, 26.16049267262009, 0.4289710845491785, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.541089566020314, 0.495, 0.40444444444444433, 0.9300184162062615, 0.5833333333333334, 0.6800410560516742, 0.6429903615163929, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.36768904], dtype=float32), 0.23103893]. 
=============================================
[2019-04-04 01:34:16,650] A3C_AGENT_WORKER-Thread-11 INFO:Local step 170500, global step 2716926: loss 0.0067
[2019-04-04 01:34:16,652] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 170500, global step 2716926: learning rate 0.0005
[2019-04-04 01:34:20,486] A3C_AGENT_WORKER-Thread-6 INFO:Local step 170000, global step 2718274: loss 0.2984
[2019-04-04 01:34:20,488] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 170000, global step 2718274: learning rate 0.0005
[2019-04-04 01:34:21,058] A3C_AGENT_WORKER-Thread-2 INFO:Local step 171000, global step 2718504: loss 0.0345
[2019-04-04 01:34:21,059] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 171000, global step 2718504: learning rate 0.0005
[2019-04-04 01:34:21,252] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:34:21,253] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:34:21,256] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run21
[2019-04-04 01:34:22,231] A3C_AGENT_WORKER-Thread-10 INFO:Local step 170000, global step 2718943: loss 0.1729
[2019-04-04 01:34:22,232] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 170000, global step 2718943: learning rate 0.0005
[2019-04-04 01:34:23,044] A3C_AGENT_WORKER-Thread-3 INFO:Local step 170000, global step 2719269: loss 0.1407
[2019-04-04 01:34:23,045] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 170000, global step 2719269: learning rate 0.0005
[2019-04-04 01:34:23,543] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.5496794e-25 1.9754562e-19 6.8961543e-18 1.0000000e+00 1.4237752e-18
 3.1117413e-13 6.4139644e-17], sum to 1.0000
[2019-04-04 01:34:23,546] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5422
[2019-04-04 01:34:23,570] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.199999999999999, 69.16666666666667, 0.0, 0.0, 24.0, 19.72947356217885, -0.9195761860700108, 0.0, 1.0, 48456.02512140956], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 111000.0000, 
sim time next is 111600.0000, 
raw observation next is [-7.3, 68.0, 0.0, 0.0, 24.0, 19.75763856235233, -0.9238010420856684, 0.0, 1.0, 48459.64511094425], 
processed observation next is [1.0, 0.30434782608695654, 0.26038781163434904, 0.68, 0.0, 0.0, 0.5, 0.1464698801960275, 0.19206631930477722, 0.0, 1.0, 0.23076021481402023], 
reward next is 0.7692, 
noisyNet noise sample is [array([-1.8947775], dtype=float32), -0.003875878]. 
=============================================
[2019-04-04 01:34:26,313] A3C_AGENT_WORKER-Thread-20 INFO:Local step 170500, global step 2720546: loss 0.5392
[2019-04-04 01:34:26,318] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 170500, global step 2720546: learning rate 0.0005
[2019-04-04 01:34:29,568] A3C_AGENT_WORKER-Thread-12 INFO:Local step 170000, global step 2721656: loss 0.1169
[2019-04-04 01:34:29,569] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 170000, global step 2721656: learning rate 0.0005
[2019-04-04 01:34:30,544] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.4732080e-23 2.3696592e-17 2.8572088e-14 1.0000000e+00 1.6935385e-17
 7.5255754e-14 2.6365380e-16], sum to 1.0000
[2019-04-04 01:34:30,553] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1794
[2019-04-04 01:34:30,567] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 24.0, 21.91971247142586, -0.4687728359785504, 0.0, 1.0, 45180.640072443], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 184200.0000, 
sim time next is 184800.0000, 
raw observation next is [-8.900000000000002, 78.0, 0.0, 0.0, 24.0, 21.88180656030924, -0.4748813288448002, 0.0, 1.0, 45212.03866571413], 
processed observation next is [1.0, 0.13043478260869565, 0.2160664819944598, 0.78, 0.0, 0.0, 0.5, 0.32348388002577, 0.3417062237183999, 0.0, 1.0, 0.21529542221768633], 
reward next is 0.7847, 
noisyNet noise sample is [array([-0.5538125], dtype=float32), -0.53442675]. 
=============================================
[2019-04-04 01:34:32,178] A3C_AGENT_WORKER-Thread-17 INFO:Local step 171000, global step 2722753: loss 0.3209
[2019-04-04 01:34:32,179] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 171000, global step 2722753: learning rate 0.0005
[2019-04-04 01:34:33,138] A3C_AGENT_WORKER-Thread-16 INFO:Local step 170500, global step 2723083: loss 0.0288
[2019-04-04 01:34:33,139] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 170500, global step 2723083: learning rate 0.0005
[2019-04-04 01:34:33,344] A3C_AGENT_WORKER-Thread-19 INFO:Local step 171000, global step 2723162: loss 0.0020
[2019-04-04 01:34:33,347] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 171000, global step 2723163: learning rate 0.0005
[2019-04-04 01:34:34,340] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:34:34,342] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:34:34,351] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run21
[2019-04-04 01:34:36,756] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.6029944e-20 3.4047275e-14 1.2260316e-12 1.0000000e+00 1.8954248e-14
 1.2468815e-08 1.5582347e-12], sum to 1.0000
[2019-04-04 01:34:36,756] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8719
[2019-04-04 01:34:36,794] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.55, 75.5, 0.0, 0.0, 21.5, 21.12954798639863, -0.7095795031220283, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.5], 
sim time this is 707400.0000, 
sim time next is 708000.0000, 
raw observation next is [-2.466666666666667, 75.66666666666667, 0.0, 0.0, 21.5, 21.12979668320379, -0.7212847461568206, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.39427516158818104, 0.7566666666666667, 0.0, 0.0, 0.2916666666666667, 0.26081639026698245, 0.2595717512810598, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0388633], dtype=float32), -1.1263614]. 
=============================================
[2019-04-04 01:34:36,817] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[77.758575]
 [77.84244 ]
 [77.691345]
 [77.514435]
 [77.38184 ]], R is [[78.00196838]
 [78.22194672]
 [78.43972778]
 [78.24848938]
 [78.03367615]].
[2019-04-04 01:34:38,078] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.7020647e-21 1.7382557e-17 1.7480632e-14 1.0000000e+00 2.7911199e-16
 3.8425188e-12 3.7641691e-15], sum to 1.0000
[2019-04-04 01:34:38,079] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4301
[2019-04-04 01:34:38,119] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.9, 65.0, 117.5, 25.5, 22.0, 21.12963608330029, -0.6331623782749618, 0.0, 1.0, 31964.48345372417], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 640800.0000, 
sim time next is 641400.0000, 
raw observation next is [-3.816666666666666, 65.0, 111.6666666666667, 17.0, 22.0, 21.16541570361459, -0.6301386321778385, 0.0, 1.0, 18720.34331156223], 
processed observation next is [0.0, 0.43478260869565216, 0.3568790397045245, 0.65, 0.37222222222222234, 0.01878453038674033, 0.3333333333333333, 0.26378464196788237, 0.28995378927405385, 0.0, 1.0, 0.08914449195982013], 
reward next is 0.9109, 
noisyNet noise sample is [array([-1.0783921], dtype=float32), 2.0261831]. 
=============================================
[2019-04-04 01:34:38,468] A3C_AGENT_WORKER-Thread-18 INFO:Local step 170500, global step 2724852: loss 0.0195
[2019-04-04 01:34:38,471] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 170500, global step 2724852: learning rate 0.0005
[2019-04-04 01:34:38,839] A3C_AGENT_WORKER-Thread-11 INFO:Local step 171000, global step 2724977: loss 0.4613
[2019-04-04 01:34:38,840] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 171000, global step 2724977: learning rate 0.0005
[2019-04-04 01:34:40,695] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.8018347e-22 4.9668507e-17 1.4427876e-15 1.0000000e+00 3.4426574e-16
 1.7605616e-13 6.0794366e-16], sum to 1.0000
[2019-04-04 01:34:40,695] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9740
[2019-04-04 01:34:40,726] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.883333333333334, 82.66666666666667, 0.0, 0.0, 23.0, 22.42548212651076, -0.3397543324454497, 0.0, 1.0, 34710.63981961329], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 532200.0000, 
sim time next is 532800.0000, 
raw observation next is [2.7, 82.0, 0.0, 0.0, 23.0, 22.42573073611578, -0.3411026343129681, 0.0, 1.0, 37239.23105498478], 
processed observation next is [0.0, 0.17391304347826086, 0.5373961218836566, 0.82, 0.0, 0.0, 0.4166666666666667, 0.3688108946763151, 0.3862991218956773, 0.0, 1.0, 0.17732967169040373], 
reward next is 0.8227, 
noisyNet noise sample is [array([0.7836592], dtype=float32), -0.3124333]. 
=============================================
[2019-04-04 01:34:42,548] A3C_AGENT_WORKER-Thread-14 INFO:Local step 170000, global step 2726428: loss 0.1380
[2019-04-04 01:34:42,549] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 170000, global step 2726429: learning rate 0.0005
[2019-04-04 01:34:42,690] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:34:42,691] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:34:42,697] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run21
[2019-04-04 01:34:43,480] A3C_AGENT_WORKER-Thread-2 INFO:Local step 171500, global step 2726735: loss 0.3977
[2019-04-04 01:34:43,480] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 171500, global step 2726735: learning rate 0.0005
[2019-04-04 01:34:44,597] A3C_AGENT_WORKER-Thread-6 INFO:Local step 170500, global step 2727137: loss 0.0079
[2019-04-04 01:34:44,598] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 170500, global step 2727137: learning rate 0.0005
[2019-04-04 01:34:45,379] A3C_AGENT_WORKER-Thread-10 INFO:Local step 170500, global step 2727433: loss 0.0438
[2019-04-04 01:34:45,380] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 170500, global step 2727433: learning rate 0.0005
[2019-04-04 01:34:48,674] A3C_AGENT_WORKER-Thread-3 INFO:Local step 170500, global step 2728618: loss 0.0402
[2019-04-04 01:34:48,675] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 170500, global step 2728619: learning rate 0.0005
[2019-04-04 01:34:48,836] A3C_AGENT_WORKER-Thread-20 INFO:Local step 171000, global step 2728701: loss 0.2925
[2019-04-04 01:34:48,837] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 171000, global step 2728701: learning rate 0.0005
[2019-04-04 01:34:54,079] A3C_AGENT_WORKER-Thread-15 INFO:Local step 170000, global step 2729716: loss 0.1728
[2019-04-04 01:34:54,080] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 170000, global step 2729716: learning rate 0.0005
[2019-04-04 01:34:54,922] A3C_AGENT_WORKER-Thread-12 INFO:Local step 170500, global step 2729862: loss 0.0094
[2019-04-04 01:34:54,935] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 170500, global step 2729862: learning rate 0.0005
[2019-04-04 01:35:00,097] A3C_AGENT_WORKER-Thread-19 INFO:Local step 171500, global step 2730883: loss 0.0977
[2019-04-04 01:35:00,109] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 171500, global step 2730883: learning rate 0.0005
[2019-04-04 01:35:01,377] A3C_AGENT_WORKER-Thread-17 INFO:Local step 171500, global step 2731179: loss 0.0383
[2019-04-04 01:35:01,379] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 171500, global step 2731179: learning rate 0.0005
[2019-04-04 01:35:03,805] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2817671e-26 1.3644668e-16 1.0882131e-16 1.0000000e+00 8.5769411e-17
 6.7255284e-15 1.6018563e-18], sum to 1.0000
[2019-04-04 01:35:03,806] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3829
[2019-04-04 01:35:03,816] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.7, 83.0, 0.0, 0.0, 24.5, 24.11215371208788, 0.1027853513481162, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.5], 
sim time this is 964800.0000, 
sim time next is 965400.0000, 
raw observation next is [7.883333333333334, 83.0, 0.0, 0.0, 24.5, 24.0786744417532, 0.08862696743418876, 0.0, 1.0, 18738.02358594562], 
processed observation next is [1.0, 0.17391304347826086, 0.6809787626962143, 0.83, 0.0, 0.0, 0.5416666666666666, 0.5065562034794334, 0.529542322478063, 0.0, 1.0, 0.08922868374259818], 
reward next is 0.9108, 
noisyNet noise sample is [array([-0.76296026], dtype=float32), -0.57221025]. 
=============================================
[2019-04-04 01:35:04,820] A3C_AGENT_WORKER-Thread-16 INFO:Local step 171000, global step 2731771: loss 0.0877
[2019-04-04 01:35:04,820] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 171000, global step 2731771: learning rate 0.0005
[2019-04-04 01:35:04,841] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:35:04,841] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:35:04,844] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run21
[2019-04-04 01:35:07,041] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.9339281e-20 2.4977601e-14 4.7587169e-12 1.0000000e+00 1.6179474e-13
 8.1249868e-11 7.2381066e-13], sum to 1.0000
[2019-04-04 01:35:07,043] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0298
[2019-04-04 01:35:07,157] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-11.33333333333333, 52.33333333333334, 0.0, 0.0, 23.0, 21.90527163464541, -0.4846107767426092, 0.0, 1.0, 46833.19374139448], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 427200.0000, 
sim time next is 427800.0000, 
raw observation next is [-11.51666666666667, 53.16666666666666, 0.0, 0.0, 23.0, 21.86338886826941, -0.4941133677855534, 0.0, 1.0, 46790.04199845012], 
processed observation next is [1.0, 0.9565217391304348, 0.14358264081255764, 0.5316666666666666, 0.0, 0.0, 0.4166666666666667, 0.32194907235578424, 0.3352955440714822, 0.0, 1.0, 0.22280972380214342], 
reward next is 0.7772, 
noisyNet noise sample is [array([0.17611606], dtype=float32), 0.30722168]. 
=============================================
[2019-04-04 01:35:08,308] A3C_AGENT_WORKER-Thread-2 INFO:Local step 172000, global step 2732444: loss 72.2271
[2019-04-04 01:35:08,356] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 172000, global step 2732444: learning rate 0.0005
[2019-04-04 01:35:09,844] A3C_AGENT_WORKER-Thread-11 INFO:Local step 171500, global step 2732734: loss 0.0125
[2019-04-04 01:35:09,859] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 171500, global step 2732734: learning rate 0.0005
[2019-04-04 01:35:10,566] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0811858e-26 7.7602557e-22 1.0197445e-19 1.0000000e+00 2.1068827e-21
 8.4474024e-20 3.4525159e-19], sum to 1.0000
[2019-04-04 01:35:10,566] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0739
[2019-04-04 01:35:10,614] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 66.0, 0.0, 0.0, 24.0, 22.58147436546016, -0.298158369222496, 0.0, 1.0, 44579.08673025454], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 627600.0000, 
sim time next is 628200.0000, 
raw observation next is [-4.5, 66.5, 0.0, 0.0, 24.0, 22.58137733933936, -0.2994201966683577, 0.0, 1.0, 44542.5028273895], 
processed observation next is [0.0, 0.2608695652173913, 0.3379501385041552, 0.665, 0.0, 0.0, 0.5, 0.3817814449449468, 0.40019326777721415, 0.0, 1.0, 0.21210715632090238], 
reward next is 0.7879, 
noisyNet noise sample is [array([-0.12770243], dtype=float32), 0.54648966]. 
=============================================
[2019-04-04 01:35:13,997] A3C_AGENT_WORKER-Thread-18 INFO:Local step 171000, global step 2733501: loss 0.2308
[2019-04-04 01:35:13,997] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 171000, global step 2733501: learning rate 0.0005
[2019-04-04 01:35:17,166] A3C_AGENT_WORKER-Thread-5 INFO:Local step 170000, global step 2734136: loss 0.2911
[2019-04-04 01:35:17,213] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 170000, global step 2734136: learning rate 0.0005
[2019-04-04 01:35:17,634] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:35:17,634] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:35:17,694] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run21
[2019-04-04 01:35:20,783] A3C_AGENT_WORKER-Thread-14 INFO:Local step 170500, global step 2734898: loss 0.1004
[2019-04-04 01:35:20,815] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 170500, global step 2734898: learning rate 0.0005
[2019-04-04 01:35:23,478] A3C_AGENT_WORKER-Thread-10 INFO:Local step 171000, global step 2735434: loss 0.5255
[2019-04-04 01:35:23,479] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 171000, global step 2735434: learning rate 0.0005
[2019-04-04 01:35:23,979] A3C_AGENT_WORKER-Thread-6 INFO:Local step 171000, global step 2735533: loss 0.2427
[2019-04-04 01:35:23,980] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 171000, global step 2735533: learning rate 0.0005
[2019-04-04 01:35:25,420] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0649897e-23 6.8379737e-18 7.2929206e-15 1.0000000e+00 3.2500297e-17
 8.4844871e-14 4.4719161e-17], sum to 1.0000
[2019-04-04 01:35:25,420] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0951
[2019-04-04 01:35:25,474] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.2, 80.0, 135.3333333333333, 528.6666666666666, 23.0, 22.13279108071058, -0.329666923565705, 0.0, 1.0, 37796.27531658598], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 565800.0000, 
sim time next is 566400.0000, 
raw observation next is [-1.2, 80.0, 136.6666666666667, 561.8333333333334, 23.0, 22.11827974627403, -0.3195709373222363, 0.0, 1.0, 40522.60297861055], 
processed observation next is [0.0, 0.5652173913043478, 0.42936288088642666, 0.8, 0.4555555555555557, 0.6208103130755065, 0.4166666666666667, 0.3431899788561692, 0.3934763542259212, 0.0, 1.0, 0.19296477608862167], 
reward next is 0.8070, 
noisyNet noise sample is [array([-0.00716163], dtype=float32), 1.5349851]. 
=============================================
[2019-04-04 01:35:28,348] A3C_AGENT_WORKER-Thread-20 INFO:Local step 171500, global step 2736535: loss 0.0023
[2019-04-04 01:35:28,390] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 171500, global step 2736535: learning rate 0.0005
[2019-04-04 01:35:29,181] A3C_AGENT_WORKER-Thread-13 INFO:Local step 170000, global step 2736733: loss 0.2565
[2019-04-04 01:35:29,217] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 170000, global step 2736733: learning rate 0.0005
[2019-04-04 01:35:30,353] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2843487e-30 8.0226951e-17 5.1900907e-23 1.0000000e+00 3.9748896e-23
 1.5346607e-17 4.9302353e-22], sum to 1.0000
[2019-04-04 01:35:30,353] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7601
[2019-04-04 01:35:30,453] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [16.1, 78.66666666666667, 0.0, 0.0, 24.5, 23.20816601520026, 0.05107944607231346, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [24.5], 
sim time this is 1210800.0000, 
sim time next is 1211400.0000, 
raw observation next is [16.1, 79.0, 0.0, 0.0, 24.5, 23.1855261196852, 0.05158992933611498, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.0, 0.9085872576177286, 0.79, 0.0, 0.0, 0.5416666666666666, 0.4321271766404333, 0.5171966431120384, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.61856645], dtype=float32), -1.5957237]. 
=============================================
[2019-04-04 01:35:31,071] A3C_AGENT_WORKER-Thread-3 INFO:Local step 171000, global step 2737156: loss 0.2399
[2019-04-04 01:35:31,072] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 171000, global step 2737156: learning rate 0.0005
[2019-04-04 01:35:33,916] A3C_AGENT_WORKER-Thread-17 INFO:Local step 172000, global step 2737790: loss 62.7897
[2019-04-04 01:35:33,920] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 172000, global step 2737790: learning rate 0.0005
[2019-04-04 01:35:33,992] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.1293700e-23 9.1258933e-17 1.8076381e-13 1.0000000e+00 2.1381911e-15
 3.2696917e-16 5.9926284e-17], sum to 1.0000
[2019-04-04 01:35:34,013] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9437
[2019-04-04 01:35:34,143] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-9.866666666666667, 44.33333333333334, 31.66666666666666, 279.5, 24.0, 24.12530310639163, -0.01801460430859462, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 318000.0000, 
sim time next is 318600.0000, 
raw observation next is [-10.05, 45.5, 24.0, 240.0, 24.0, 24.03448125275311, -0.1504279967163176, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.18421052631578946, 0.455, 0.08, 0.26519337016574585, 0.5, 0.5028734377294258, 0.44985733442789416, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2164652], dtype=float32), -1.5997809]. 
=============================================
[2019-04-04 01:35:34,203] A3C_AGENT_WORKER-Thread-19 INFO:Local step 172000, global step 2737851: loss 77.2813
[2019-04-04 01:35:34,204] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 172000, global step 2737851: learning rate 0.0005
[2019-04-04 01:35:36,289] A3C_AGENT_WORKER-Thread-12 INFO:Local step 171000, global step 2738308: loss 0.6826
[2019-04-04 01:35:36,289] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 171000, global step 2738308: learning rate 0.0005
[2019-04-04 01:35:37,923] A3C_AGENT_WORKER-Thread-15 INFO:Local step 170500, global step 2738654: loss 0.0251
[2019-04-04 01:35:37,943] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 170500, global step 2738654: learning rate 0.0005
[2019-04-04 01:35:40,111] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.7624677e-19 1.1752468e-11 9.7440252e-13 1.0000000e+00 1.9206274e-09
 2.1147637e-08 4.9689575e-12], sum to 1.0000
[2019-04-04 01:35:40,119] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0780
[2019-04-04 01:35:40,267] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.483333333333333, 83.5, 0.0, 0.0, 24.0, 23.01128216788544, -0.1854764181411634, 1.0, 1.0, 92815.17603456399], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 849000.0000, 
sim time next is 849600.0000, 
raw observation next is [-3.4, 83.0, 0.0, 0.0, 24.0, 22.97193629847535, -0.1761066929069408, 0.0, 1.0, 89029.49294177399], 
processed observation next is [1.0, 0.8695652173913043, 0.368421052631579, 0.83, 0.0, 0.0, 0.5, 0.41432802487294573, 0.44129776903101975, 0.0, 1.0, 0.42394996638939997], 
reward next is 0.5761, 
noisyNet noise sample is [array([1.8178946], dtype=float32), -0.41669342]. 
=============================================
[2019-04-04 01:35:41,825] A3C_AGENT_WORKER-Thread-11 INFO:Local step 172000, global step 2739481: loss 56.2115
[2019-04-04 01:35:41,890] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 172000, global step 2739481: learning rate 0.0005
[2019-04-04 01:35:43,049] A3C_AGENT_WORKER-Thread-2 INFO:Local step 172500, global step 2739743: loss 0.0272
[2019-04-04 01:35:43,050] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 172500, global step 2739743: learning rate 0.0005
[2019-04-04 01:35:44,242] A3C_AGENT_WORKER-Thread-16 INFO:Local step 171500, global step 2740055: loss 0.0226
[2019-04-04 01:35:44,259] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 171500, global step 2740055: learning rate 0.0005
[2019-04-04 01:35:50,445] A3C_AGENT_WORKER-Thread-18 INFO:Local step 171500, global step 2741591: loss 0.2467
[2019-04-04 01:35:50,459] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 171500, global step 2741591: learning rate 0.0005
[2019-04-04 01:35:51,645] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:35:51,645] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:35:51,648] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run21
[2019-04-04 01:35:51,734] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.4095295e-31 3.3332488e-18 3.8911613e-22 1.0000000e+00 4.6713980e-22
 9.0490115e-23 8.3946704e-24], sum to 1.0000
[2019-04-04 01:35:51,734] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0303
[2019-04-04 01:35:51,806] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [17.7, 67.0, 0.0, 0.0, 25.0, 23.59808969613962, 0.1361734118594798, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 1193400.0000, 
sim time next is 1194000.0000, 
raw observation next is [17.7, 67.0, 0.0, 0.0, 25.0, 23.57721407286502, 0.1320758199508336, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.8260869565217391, 0.9529085872576178, 0.67, 0.0, 0.0, 0.5833333333333334, 0.4647678394054182, 0.5440252733169445, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4196061], dtype=float32), -0.7985118]. 
=============================================
[2019-04-04 01:35:51,835] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[84.70292]
 [84.64739]
 [84.59728]
 [84.56836]
 [84.53552]], R is [[84.94299316]
 [85.09356689]
 [85.24263   ]
 [85.39020538]
 [85.53630066]].
[2019-04-04 01:35:55,951] A3C_AGENT_WORKER-Thread-5 INFO:Local step 170500, global step 2742831: loss 0.0192
[2019-04-04 01:35:55,957] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 170500, global step 2742831: learning rate 0.0005
[2019-04-04 01:35:56,300] A3C_AGENT_WORKER-Thread-20 INFO:Local step 172000, global step 2742908: loss 53.5702
[2019-04-04 01:35:56,302] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 172000, global step 2742908: learning rate 0.0005
[2019-04-04 01:35:59,041] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.9824202e-24 1.9768548e-15 1.9749413e-14 1.0000000e+00 4.1985433e-16
 2.4634205e-14 1.4835802e-16], sum to 1.0000
[2019-04-04 01:35:59,048] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8769
[2019-04-04 01:35:59,094] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.2, 95.0, 0.0, 0.0, 25.0, 24.42664894198931, 0.2175561361169848, 0.0, 1.0, 18765.58316138923], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 1481400.0000, 
sim time next is 1482000.0000, 
raw observation next is [2.2, 95.33333333333334, 0.0, 0.0, 25.0, 24.45632712011716, 0.2106213249535197, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.5235457063711911, 0.9533333333333335, 0.0, 0.0, 0.5833333333333334, 0.5380272600097632, 0.5702071083178399, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.544882], dtype=float32), -0.10436146]. 
=============================================
[2019-04-04 01:35:59,180] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[91.937614]
 [91.89077 ]
 [91.84503 ]
 [91.76373 ]
 [91.763596]], R is [[92.05024719]
 [92.04038239]
 [91.91100311]
 [91.73232269]
 [91.49320221]].
[2019-04-04 01:35:59,511] A3C_AGENT_WORKER-Thread-10 INFO:Local step 171500, global step 2743654: loss 0.0591
[2019-04-04 01:35:59,512] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 171500, global step 2743654: learning rate 0.0005
[2019-04-04 01:36:00,319] A3C_AGENT_WORKER-Thread-6 INFO:Local step 171500, global step 2743857: loss 0.0061
[2019-04-04 01:36:00,332] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 171500, global step 2743859: learning rate 0.0005
[2019-04-04 01:36:01,077] A3C_AGENT_WORKER-Thread-14 INFO:Local step 171000, global step 2744055: loss 0.0177
[2019-04-04 01:36:01,078] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 171000, global step 2744055: learning rate 0.0005
[2019-04-04 01:36:02,760] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.7379119e-28 1.8896991e-22 1.6101937e-19 1.0000000e+00 2.8132708e-22
 7.5281403e-19 3.5336986e-21], sum to 1.0000
[2019-04-04 01:36:02,792] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3119
[2019-04-04 01:36:02,847] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.8, 56.0, 81.33333333333333, 53.0, 26.0, 24.87210403321595, 0.2205130324348731, 0.0, 1.0, 44989.45105198729], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 657600.0000, 
sim time next is 658200.0000, 
raw observation next is [-0.7, 55.0, 81.66666666666667, 50.0, 26.0, 24.87052130414131, 0.2217948787815326, 0.0, 1.0, 40492.2362979577], 
processed observation next is [0.0, 0.6086956521739131, 0.443213296398892, 0.55, 0.27222222222222225, 0.055248618784530384, 0.6666666666666666, 0.5725434420117758, 0.5739316262605109, 0.0, 1.0, 0.19282017284741762], 
reward next is 0.8072, 
noisyNet noise sample is [array([0.6728381], dtype=float32), -0.36245352]. 
=============================================
[2019-04-04 01:36:03,184] A3C_AGENT_WORKER-Thread-4 INFO:Local step 170000, global step 2744568: loss 0.2577
[2019-04-04 01:36:03,186] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 170000, global step 2744568: learning rate 0.0005
[2019-04-04 01:36:03,493] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.0293239e-26 1.4081665e-16 1.6369128e-17 1.0000000e+00 1.4516843e-19
 3.0561897e-14 8.4255914e-18], sum to 1.0000
[2019-04-04 01:36:03,493] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0991
[2019-04-04 01:36:03,548] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 92.0, 5.999999999999998, 0.0, 25.0, 24.44037933547084, 0.2618249343704353, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 1324200.0000, 
sim time next is 1324800.0000, 
raw observation next is [1.1, 92.0, 9.0, 0.0, 25.0, 24.34133845745598, 0.2688380359155632, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.49307479224376743, 0.92, 0.03, 0.0, 0.5833333333333334, 0.5284448714546649, 0.5896126786385211, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.25316194], dtype=float32), -2.4289732]. 
=============================================
[2019-04-04 01:36:05,090] A3C_AGENT_WORKER-Thread-17 INFO:Local step 172500, global step 2745109: loss 3.2387
[2019-04-04 01:36:05,091] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 172500, global step 2745109: learning rate 0.0005
[2019-04-04 01:36:05,325] A3C_AGENT_WORKER-Thread-19 INFO:Local step 172500, global step 2745177: loss 0.0642
[2019-04-04 01:36:05,326] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 172500, global step 2745177: learning rate 0.0005
[2019-04-04 01:36:07,607] A3C_AGENT_WORKER-Thread-3 INFO:Local step 171500, global step 2745746: loss 0.0659
[2019-04-04 01:36:07,634] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 171500, global step 2745752: learning rate 0.0005
[2019-04-04 01:36:08,666] A3C_AGENT_WORKER-Thread-13 INFO:Local step 170500, global step 2746028: loss 0.0158
[2019-04-04 01:36:08,669] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 170500, global step 2746028: learning rate 0.0005
[2019-04-04 01:36:09,854] A3C_AGENT_WORKER-Thread-16 INFO:Local step 172000, global step 2746345: loss 54.9975
[2019-04-04 01:36:09,856] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 172000, global step 2746345: learning rate 0.0005
[2019-04-04 01:36:12,740] A3C_AGENT_WORKER-Thread-11 INFO:Local step 172500, global step 2747125: loss 0.1811
[2019-04-04 01:36:12,742] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 172500, global step 2747126: learning rate 0.0005
[2019-04-04 01:36:12,885] A3C_AGENT_WORKER-Thread-12 INFO:Local step 171500, global step 2747158: loss 0.0516
[2019-04-04 01:36:12,889] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 171500, global step 2747160: learning rate 0.0005
[2019-04-04 01:36:13,876] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5054639e-27 2.9737950e-17 1.4128275e-14 1.0000000e+00 5.6050978e-19
 7.1502582e-10 2.7538339e-20], sum to 1.0000
[2019-04-04 01:36:13,876] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6087
[2019-04-04 01:36:13,928] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.2, 82.66666666666667, 0.0, 0.0, 25.0, 24.57305297209922, 0.3415731587561447, 0.0, 1.0, 96673.6248048827], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 1638600.0000, 
sim time next is 1639200.0000, 
raw observation next is [7.200000000000001, 83.33333333333334, 0.0, 0.0, 25.0, 24.5991514935405, 0.3459627287825624, 0.0, 1.0, 46202.18778724877], 
processed observation next is [1.0, 1.0, 0.662049861495845, 0.8333333333333335, 0.0, 0.0, 0.5833333333333334, 0.549929291128375, 0.6153209095941875, 0.0, 1.0, 0.22001041803451793], 
reward next is 0.7800, 
noisyNet noise sample is [array([-0.7602049], dtype=float32), 0.0051376736]. 
=============================================
[2019-04-04 01:36:14,283] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.7042806e-28 1.2203559e-11 3.1414072e-18 1.0000000e+00 6.3163768e-18
 1.9957501e-17 1.3001253e-19], sum to 1.0000
[2019-04-04 01:36:14,283] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0264
[2019-04-04 01:36:14,346] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [16.36666666666667, 52.5, 0.0, 0.0, 26.0, 25.15201228977008, 0.582455153159473, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1101000.0000, 
sim time next is 1101600.0000, 
raw observation next is [16.1, 53.0, 0.0, 0.0, 26.0, 25.5290066224692, 0.590673146549974, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.9085872576177286, 0.53, 0.0, 0.0, 0.6666666666666666, 0.6274172185391, 0.6968910488499914, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.42901972], dtype=float32), 0.28778595]. 
=============================================
[2019-04-04 01:36:14,979] A3C_AGENT_WORKER-Thread-15 INFO:Local step 171000, global step 2747648: loss 0.0690
[2019-04-04 01:36:14,982] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 171000, global step 2747648: learning rate 0.0005
[2019-04-04 01:36:15,512] A3C_AGENT_WORKER-Thread-2 INFO:Local step 173000, global step 2747759: loss 0.1641
[2019-04-04 01:36:15,513] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 173000, global step 2747759: learning rate 0.0005
[2019-04-04 01:36:16,776] A3C_AGENT_WORKER-Thread-18 INFO:Local step 172000, global step 2748039: loss 46.1139
[2019-04-04 01:36:16,777] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 172000, global step 2748039: learning rate 0.0005
[2019-04-04 01:36:20,999] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.6026078e-22 6.6263308e-17 2.8759571e-14 1.0000000e+00 2.3225054e-17
 4.3185419e-14 6.9043511e-15], sum to 1.0000
[2019-04-04 01:36:21,000] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9324
[2019-04-04 01:36:21,020] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.566666666666666, 71.66666666666667, 0.0, 0.0, 24.0, 23.05371431007271, -0.2233139973202344, 0.0, 1.0, 42004.76477428994], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 693600.0000, 
sim time next is 694200.0000, 
raw observation next is [-3.483333333333333, 71.83333333333333, 0.0, 0.0, 24.0, 23.10268666493967, -0.2238021485252561, 0.0, 1.0, 41976.14515238746], 
processed observation next is [1.0, 0.0, 0.3661126500461681, 0.7183333333333333, 0.0, 0.0, 0.5, 0.42522388874497263, 0.4253992838249146, 0.0, 1.0, 0.19988640548755932], 
reward next is 0.8001, 
noisyNet noise sample is [array([1.7213522], dtype=float32), 0.3996275]. 
=============================================
[2019-04-04 01:36:26,846] A3C_AGENT_WORKER-Thread-10 INFO:Local step 172000, global step 2750618: loss 38.3962
[2019-04-04 01:36:26,846] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 172000, global step 2750618: learning rate 0.0005
[2019-04-04 01:36:27,432] A3C_AGENT_WORKER-Thread-6 INFO:Local step 172000, global step 2750751: loss 39.6964
[2019-04-04 01:36:27,433] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 172000, global step 2750751: learning rate 0.0005
[2019-04-04 01:36:27,855] A3C_AGENT_WORKER-Thread-20 INFO:Local step 172500, global step 2750845: loss 0.2634
[2019-04-04 01:36:27,855] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 172500, global step 2750845: learning rate 0.0005
[2019-04-04 01:36:31,428] A3C_AGENT_WORKER-Thread-5 INFO:Local step 171000, global step 2751772: loss 0.5905
[2019-04-04 01:36:31,429] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 171000, global step 2751772: learning rate 0.0005
[2019-04-04 01:36:32,666] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.3103153e-36 5.5863235e-23 1.3761324e-26 1.0000000e+00 3.6260925e-28
 1.9348052e-24 5.7375360e-27], sum to 1.0000
[2019-04-04 01:36:32,667] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0395
[2019-04-04 01:36:32,674] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [16.1, 81.0, 0.0, 0.0, 26.0, 24.09552696508099, 0.2686654579494479, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1214400.0000, 
sim time next is 1215000.0000, 
raw observation next is [16.1, 81.5, 0.0, 0.0, 26.0, 24.06835059520585, 0.2638107172175481, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.043478260869565216, 0.9085872576177286, 0.815, 0.0, 0.0, 0.6666666666666666, 0.5056958829338208, 0.5879369057391827, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.61635184], dtype=float32), -0.7750416]. 
=============================================
[2019-04-04 01:36:32,707] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[94.82994 ]
 [94.745514]
 [94.644745]
 [94.62619 ]
 [94.60718 ]], R is [[94.98897552]
 [95.03908539]
 [95.08869171]
 [95.13780212]
 [95.18642426]].
[2019-04-04 01:36:35,117] A3C_AGENT_WORKER-Thread-14 INFO:Local step 171500, global step 2752710: loss 0.0463
[2019-04-04 01:36:35,117] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 171500, global step 2752710: learning rate 0.0005
[2019-04-04 01:36:35,432] A3C_AGENT_WORKER-Thread-3 INFO:Local step 172000, global step 2752781: loss 31.9152
[2019-04-04 01:36:35,432] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 172000, global step 2752781: learning rate 0.0005
[2019-04-04 01:36:37,429] A3C_AGENT_WORKER-Thread-17 INFO:Local step 173000, global step 2753263: loss 2.2005
[2019-04-04 01:36:37,429] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 173000, global step 2753263: learning rate 0.0005
[2019-04-04 01:36:37,925] A3C_AGENT_WORKER-Thread-19 INFO:Local step 173000, global step 2753363: loss 0.6285
[2019-04-04 01:36:37,925] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 173000, global step 2753363: learning rate 0.0005
[2019-04-04 01:36:38,146] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.1375820e-20 1.8432896e-14 2.7511425e-15 1.0000000e+00 3.5462969e-14
 2.3208818e-12 2.2027428e-15], sum to 1.0000
[2019-04-04 01:36:38,147] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6075
[2019-04-04 01:36:38,230] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.05, 79.0, 147.0, 0.0, 26.0, 25.61368596486293, 0.3358322008335741, 1.0, 1.0, 24354.68303755163], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2028600.0000, 
sim time next is 2029200.0000, 
raw observation next is [-4.866666666666667, 77.66666666666667, 148.5, 0.0, 26.0, 25.64383102360745, 0.3408959377405817, 1.0, 1.0, 23530.73275377002], 
processed observation next is [1.0, 0.4782608695652174, 0.3277931671283472, 0.7766666666666667, 0.495, 0.0, 0.6666666666666666, 0.6369859186339543, 0.6136319792468606, 1.0, 1.0, 0.1120511083512858], 
reward next is 0.8879, 
noisyNet noise sample is [array([-0.5675742], dtype=float32), 1.4496741]. 
=============================================
[2019-04-04 01:36:38,431] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.6994081e-25 9.2167900e-16 4.7583148e-17 1.0000000e+00 9.1534414e-16
 8.9963191e-15 2.8124409e-17], sum to 1.0000
[2019-04-04 01:36:38,462] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0002
[2019-04-04 01:36:38,533] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.6, 95.5, 12.0, 0.0, 26.0, 25.49598228747788, 0.3784500164899878, 1.0, 1.0, 23785.4453133012], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1356600.0000, 
sim time next is 1357200.0000, 
raw observation next is [0.5, 96.0, 9.0, 0.0, 26.0, 25.03567309417328, 0.4242087996681243, 1.0, 1.0, 136831.7081331148], 
processed observation next is [1.0, 0.7391304347826086, 0.4764542936288089, 0.96, 0.03, 0.0, 0.6666666666666666, 0.5863060911811067, 0.6414029332227081, 1.0, 1.0, 0.6515795625386419], 
reward next is 0.3484, 
noisyNet noise sample is [array([-0.79595375], dtype=float32), -0.1520884]. 
=============================================
[2019-04-04 01:36:40,607] A3C_AGENT_WORKER-Thread-16 INFO:Local step 172500, global step 2753979: loss 0.0026
[2019-04-04 01:36:40,607] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 172500, global step 2753979: learning rate 0.0005
[2019-04-04 01:36:40,716] A3C_AGENT_WORKER-Thread-4 INFO:Local step 170500, global step 2754004: loss 0.0762
[2019-04-04 01:36:40,716] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 170500, global step 2754004: learning rate 0.0005
[2019-04-04 01:36:40,789] A3C_AGENT_WORKER-Thread-12 INFO:Local step 172000, global step 2754024: loss 29.5074
[2019-04-04 01:36:40,789] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 172000, global step 2754024: learning rate 0.0005
[2019-04-04 01:36:43,277] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.8832819e-23 9.0565423e-15 1.1288998e-13 1.0000000e+00 1.4425188e-14
 2.2822591e-13 2.7762961e-16], sum to 1.0000
[2019-04-04 01:36:43,279] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3234
[2019-04-04 01:36:43,383] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.8, 100.0, 42.16666666666667, 0.0, 26.0, 25.84374649699216, 0.4996663171873764, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1502400.0000, 
sim time next is 1503000.0000, 
raw observation next is [1.9, 100.0, 47.0, 0.0, 26.0, 25.93183991830191, 0.513053240955229, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.515235457063712, 1.0, 0.15666666666666668, 0.0, 0.6666666666666666, 0.6609866598584926, 0.6710177469850763, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0217746], dtype=float32), 1.9612775]. 
=============================================
[2019-04-04 01:36:43,395] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[90.17589 ]
 [90.345474]
 [90.40599 ]
 [90.58592 ]
 [90.53308 ]], R is [[90.2146759 ]
 [90.31253052]
 [90.40940857]
 [90.50531769]
 [90.6002655 ]].
[2019-04-04 01:36:44,487] A3C_AGENT_WORKER-Thread-11 INFO:Local step 173000, global step 2754994: loss 0.9955
[2019-04-04 01:36:44,488] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 173000, global step 2754994: learning rate 0.0005
[2019-04-04 01:36:45,542] A3C_AGENT_WORKER-Thread-13 INFO:Local step 171000, global step 2755265: loss 0.0207
[2019-04-04 01:36:45,542] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 171000, global step 2755265: learning rate 0.0005
[2019-04-04 01:36:48,037] A3C_AGENT_WORKER-Thread-18 INFO:Local step 172500, global step 2755812: loss 0.4868
[2019-04-04 01:36:48,082] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 172500, global step 2755812: learning rate 0.0005
[2019-04-04 01:36:48,564] A3C_AGENT_WORKER-Thread-15 INFO:Local step 171500, global step 2755912: loss 0.0184
[2019-04-04 01:36:48,564] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 171500, global step 2755912: learning rate 0.0005
[2019-04-04 01:36:53,246] A3C_AGENT_WORKER-Thread-2 INFO:Local step 173500, global step 2757094: loss 4.1895
[2019-04-04 01:36:53,246] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 173500, global step 2757094: learning rate 0.0005
[2019-04-04 01:36:57,720] A3C_AGENT_WORKER-Thread-6 INFO:Local step 172500, global step 2758287: loss 0.0172
[2019-04-04 01:36:57,724] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 172500, global step 2758287: learning rate 0.0005
[2019-04-04 01:36:57,977] A3C_AGENT_WORKER-Thread-10 INFO:Local step 172500, global step 2758365: loss 0.0183
[2019-04-04 01:36:57,979] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 172500, global step 2758365: learning rate 0.0005
[2019-04-04 01:36:59,849] A3C_AGENT_WORKER-Thread-20 INFO:Local step 173000, global step 2758853: loss 0.0167
[2019-04-04 01:36:59,849] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 173000, global step 2758853: learning rate 0.0005
[2019-04-04 01:37:00,248] A3C_AGENT_WORKER-Thread-14 INFO:Local step 172000, global step 2758968: loss 35.5424
[2019-04-04 01:37:00,250] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 172000, global step 2758968: learning rate 0.0005
[2019-04-04 01:37:01,578] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.1882367e-28 8.2775116e-19 1.9590152e-18 1.0000000e+00 1.3252211e-20
 3.2192502e-13 8.7357945e-19], sum to 1.0000
[2019-04-04 01:37:01,579] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9225
[2019-04-04 01:37:01,611] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [13.8, 100.0, 80.0, 0.0, 26.0, 24.74320123099161, 0.4432538942203149, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1261200.0000, 
sim time next is 1261800.0000, 
raw observation next is [13.8, 100.0, 77.0, 0.0, 26.0, 24.72914997257053, 0.4460196743176557, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.844875346260388, 1.0, 0.25666666666666665, 0.0, 0.6666666666666666, 0.5607624977142107, 0.6486732247725518, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.17173034], dtype=float32), 0.7467812]. 
=============================================
[2019-04-04 01:37:03,890] A3C_AGENT_WORKER-Thread-5 INFO:Local step 171500, global step 2759961: loss 0.0014
[2019-04-04 01:37:03,897] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 171500, global step 2759961: learning rate 0.0005
[2019-04-04 01:37:04,558] A3C_AGENT_WORKER-Thread-3 INFO:Local step 172500, global step 2760172: loss 0.1546
[2019-04-04 01:37:04,559] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 172500, global step 2760172: learning rate 0.0005
[2019-04-04 01:37:09,521] A3C_AGENT_WORKER-Thread-12 INFO:Local step 172500, global step 2761555: loss 0.9819
[2019-04-04 01:37:09,544] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 172500, global step 2761555: learning rate 0.0005
[2019-04-04 01:37:10,017] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.1363553e-22 2.5033933e-13 1.0808920e-12 9.9998820e-01 4.5833956e-14
 1.1836929e-05 3.6960913e-14], sum to 1.0000
[2019-04-04 01:37:10,020] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7871
[2019-04-04 01:37:10,102] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.3, 79.0, 128.0, 392.5, 26.0, 25.64916913459968, 0.3199663700925102, 1.0, 1.0, 22429.65080986836], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1936800.0000, 
sim time next is 1937400.0000, 
raw observation next is [-7.016666666666667, 78.33333333333334, 142.3333333333333, 340.3333333333333, 26.0, 25.71807648308519, 0.3308999395518892, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.2682363804247461, 0.7833333333333334, 0.4744444444444443, 0.3760589318600368, 0.6666666666666666, 0.6431730402570993, 0.6102999798506298, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.61581635], dtype=float32), -2.1514204]. 
=============================================
[2019-04-04 01:37:10,877] A3C_AGENT_WORKER-Thread-16 INFO:Local step 173000, global step 2761894: loss 0.0296
[2019-04-04 01:37:10,878] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 173000, global step 2761895: learning rate 0.0005
[2019-04-04 01:37:11,940] A3C_AGENT_WORKER-Thread-15 INFO:Local step 172000, global step 2762150: loss 34.2419
[2019-04-04 01:37:11,949] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 172000, global step 2762150: learning rate 0.0005
[2019-04-04 01:37:12,671] A3C_AGENT_WORKER-Thread-17 INFO:Local step 173500, global step 2762349: loss 2.7178
[2019-04-04 01:37:12,672] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 173500, global step 2762349: learning rate 0.0005
[2019-04-04 01:37:14,491] A3C_AGENT_WORKER-Thread-19 INFO:Local step 173500, global step 2762891: loss 5.4456
[2019-04-04 01:37:14,492] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 173500, global step 2762891: learning rate 0.0005
[2019-04-04 01:37:16,737] A3C_AGENT_WORKER-Thread-4 INFO:Local step 171000, global step 2763506: loss 0.0339
[2019-04-04 01:37:16,737] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 171000, global step 2763506: learning rate 0.0005
[2019-04-04 01:37:17,171] A3C_AGENT_WORKER-Thread-18 INFO:Local step 173000, global step 2763628: loss 0.0573
[2019-04-04 01:37:17,172] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 173000, global step 2763628: learning rate 0.0005
[2019-04-04 01:37:17,742] A3C_AGENT_WORKER-Thread-13 INFO:Local step 171500, global step 2763770: loss 0.3450
[2019-04-04 01:37:17,745] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 171500, global step 2763770: learning rate 0.0005
[2019-04-04 01:37:19,082] A3C_AGENT_WORKER-Thread-11 INFO:Local step 173500, global step 2764097: loss 4.6007
[2019-04-04 01:37:19,083] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 173500, global step 2764097: learning rate 0.0005
[2019-04-04 01:37:20,212] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.34620262e-21 2.70665565e-12 7.67374792e-12 1.00000000e+00
 2.13601245e-13 1.34922935e-08 8.53600705e-14], sum to 1.0000
[2019-04-04 01:37:20,212] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4146
[2019-04-04 01:37:20,273] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 92.0, 109.5, 0.0, 26.0, 25.67606716671467, 0.5056870137680125, 1.0, 1.0, 64954.96268288029], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1342800.0000, 
sim time next is 1343400.0000, 
raw observation next is [1.1, 92.0, 108.3333333333333, 0.0, 26.0, 25.60849622641311, 0.51355668782045, 1.0, 1.0, 39563.30077955168], 
processed observation next is [1.0, 0.5652173913043478, 0.49307479224376743, 0.92, 0.361111111111111, 0.0, 0.6666666666666666, 0.6340413522010925, 0.6711855626068166, 1.0, 1.0, 0.18839667037881752], 
reward next is 0.8116, 
noisyNet noise sample is [array([0.03253927], dtype=float32), 0.22353436]. 
=============================================
[2019-04-04 01:37:20,606] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.9197256e-25 6.8658069e-19 2.0331175e-18 1.0000000e+00 1.2299762e-19
 2.1734759e-14 2.3279516e-17], sum to 1.0000
[2019-04-04 01:37:20,606] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9133
[2019-04-04 01:37:20,677] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.199999999999999, 78.83333333333334, 0.0, 0.0, 25.0, 23.70498706585725, -0.01005983462360099, 0.0, 1.0, 43122.26992713223], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 2164200.0000, 
sim time next is 2164800.0000, 
raw observation next is [-7.100000000000001, 78.66666666666667, 0.0, 0.0, 25.0, 23.63911500317555, -0.01550113058300553, 0.0, 1.0, 43154.04407028201], 
processed observation next is [1.0, 0.043478260869565216, 0.26592797783933514, 0.7866666666666667, 0.0, 0.0, 0.5833333333333334, 0.46992625026462925, 0.4948329564723315, 0.0, 1.0, 0.20549544795372388], 
reward next is 0.7945, 
noisyNet noise sample is [array([2.1889644], dtype=float32), -0.057655513]. 
=============================================
[2019-04-04 01:37:25,160] A3C_AGENT_WORKER-Thread-2 INFO:Local step 174000, global step 2765682: loss 1.0943
[2019-04-04 01:37:25,160] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 174000, global step 2765682: learning rate 0.0005
[2019-04-04 01:37:26,503] A3C_AGENT_WORKER-Thread-10 INFO:Local step 173000, global step 2766036: loss 0.0335
[2019-04-04 01:37:26,504] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 173000, global step 2766036: learning rate 0.0005
[2019-04-04 01:37:26,560] A3C_AGENT_WORKER-Thread-5 INFO:Local step 172000, global step 2766055: loss 28.6144
[2019-04-04 01:37:26,562] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 172000, global step 2766055: learning rate 0.0005
[2019-04-04 01:37:26,590] A3C_AGENT_WORKER-Thread-6 INFO:Local step 173000, global step 2766061: loss 0.0257
[2019-04-04 01:37:26,590] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 173000, global step 2766061: learning rate 0.0005
[2019-04-04 01:37:28,690] A3C_AGENT_WORKER-Thread-14 INFO:Local step 172500, global step 2766581: loss 0.6095
[2019-04-04 01:37:28,722] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 172500, global step 2766581: learning rate 0.0005
[2019-04-04 01:37:30,297] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.3760691e-22 3.4158925e-16 7.2947820e-12 1.0000000e+00 1.8320810e-17
 4.4631460e-14 6.9255591e-15], sum to 1.0000
[2019-04-04 01:37:30,297] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2698
[2019-04-04 01:37:30,330] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.3, 82.0, 0.0, 0.0, 25.0, 23.53713620130669, -0.05029863088305581, 0.0, 1.0, 44521.78573135402], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 2253600.0000, 
sim time next is 2254200.0000, 
raw observation next is [-7.383333333333333, 82.66666666666667, 0.0, 0.0, 25.0, 23.48810567495687, -0.05194876730229684, 0.0, 1.0, 44515.97054506568], 
processed observation next is [1.0, 0.08695652173913043, 0.25807940904893817, 0.8266666666666667, 0.0, 0.0, 0.5833333333333334, 0.4573421395797392, 0.4826837442325677, 0.0, 1.0, 0.21198081211936037], 
reward next is 0.7880, 
noisyNet noise sample is [array([0.356271], dtype=float32), 0.19238444]. 
=============================================
[2019-04-04 01:37:33,081] A3C_AGENT_WORKER-Thread-3 INFO:Local step 173000, global step 2767749: loss 0.0206
[2019-04-04 01:37:33,083] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 173000, global step 2767750: learning rate 0.0005
[2019-04-04 01:37:33,756] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.0087036e-26 8.8634843e-19 5.8383297e-18 1.0000000e+00 6.2172785e-21
 6.8974803e-15 2.8555305e-18], sum to 1.0000
[2019-04-04 01:37:33,756] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7473
[2019-04-04 01:37:33,778] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.7, 83.0, 0.0, 0.0, 26.0, 24.7511073636537, 0.2431885079642866, 0.0, 1.0, 42986.02915982071], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1983000.0000, 
sim time next is 1983600.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.69525575634322, 0.2333281766145034, 0.0, 1.0, 42998.76593918493], 
processed observation next is [1.0, 1.0, 0.30747922437673136, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5579379796952683, 0.5777760588715012, 0.0, 1.0, 0.204756028281833], 
reward next is 0.7952, 
noisyNet noise sample is [array([0.37633345], dtype=float32), -0.755055]. 
=============================================
[2019-04-04 01:37:34,351] A3C_AGENT_WORKER-Thread-20 INFO:Local step 173500, global step 2768102: loss 2.6596
[2019-04-04 01:37:34,352] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 173500, global step 2768102: learning rate 0.0005
[2019-04-04 01:37:34,663] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.6939808e-23 6.5993576e-19 4.2493453e-17 1.0000000e+00 1.7015155e-18
 3.6744587e-17 5.1028041e-17], sum to 1.0000
[2019-04-04 01:37:34,663] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6009
[2019-04-04 01:37:34,733] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.75, 71.0, 120.0, 0.0, 26.0, 24.96569919699981, 0.2507739051492055, 0.0, 1.0, 52907.75387675758], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1859400.0000, 
sim time next is 1860000.0000, 
raw observation next is [-4.666666666666667, 71.0, 128.3333333333333, 6.666666666666665, 26.0, 24.95515418801462, 0.2551514991680672, 0.0, 1.0, 54039.57423824208], 
processed observation next is [0.0, 0.5217391304347826, 0.3333333333333333, 0.71, 0.42777777777777765, 0.00736648250460405, 0.6666666666666666, 0.5795961823345518, 0.5850504997226891, 0.0, 1.0, 0.25733130589639086], 
reward next is 0.7427, 
noisyNet noise sample is [array([1.0657672], dtype=float32), -2.134053]. 
=============================================
[2019-04-04 01:37:34,777] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[77.53868 ]
 [77.64996 ]
 [77.72005 ]
 [77.812225]
 [78.06514 ]], R is [[77.85643005]
 [77.82592773]
 [77.83717346]
 [77.9018631 ]
 [77.97555542]].
[2019-04-04 01:37:37,036] A3C_AGENT_WORKER-Thread-12 INFO:Local step 173000, global step 2768883: loss 0.0791
[2019-04-04 01:37:37,036] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 173000, global step 2768883: learning rate 0.0005
[2019-04-04 01:37:39,184] A3C_AGENT_WORKER-Thread-15 INFO:Local step 172500, global step 2769390: loss 0.2440
[2019-04-04 01:37:39,219] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 172500, global step 2769390: learning rate 0.0005
[2019-04-04 01:37:40,502] A3C_AGENT_WORKER-Thread-13 INFO:Local step 172000, global step 2769732: loss 28.5295
[2019-04-04 01:37:40,506] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 172000, global step 2769734: learning rate 0.0005
[2019-04-04 01:37:43,650] A3C_AGENT_WORKER-Thread-17 INFO:Local step 174000, global step 2770505: loss 1.5668
[2019-04-04 01:37:43,650] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 174000, global step 2770505: learning rate 0.0005
[2019-04-04 01:37:45,555] A3C_AGENT_WORKER-Thread-16 INFO:Local step 173500, global step 2771028: loss 1.3596
[2019-04-04 01:37:45,557] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 173500, global step 2771028: learning rate 0.0005
[2019-04-04 01:37:46,800] A3C_AGENT_WORKER-Thread-19 INFO:Local step 174000, global step 2771358: loss 1.7582
[2019-04-04 01:37:46,823] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 174000, global step 2771358: learning rate 0.0005
[2019-04-04 01:37:48,234] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.2263414e-22 6.1697816e-16 9.5938361e-13 1.0000000e+00 8.1377218e-18
 1.6994305e-10 3.5354759e-15], sum to 1.0000
[2019-04-04 01:37:48,234] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7801
[2019-04-04 01:37:48,278] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.6, 78.83333333333333, 0.0, 0.0, 26.0, 25.23709106126795, 0.3783459511898908, 0.0, 1.0, 46125.64633016608], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1975800.0000, 
sim time next is 1976400.0000, 
raw observation next is [-5.6, 78.0, 0.0, 0.0, 26.0, 25.22189249855941, 0.379295680426524, 0.0, 1.0, 44447.41032925551], 
processed observation next is [1.0, 0.9130434782608695, 0.30747922437673136, 0.78, 0.0, 0.0, 0.6666666666666666, 0.6018243748799508, 0.626431893475508, 0.0, 1.0, 0.2116543349012167], 
reward next is 0.7883, 
noisyNet noise sample is [array([1.0448693], dtype=float32), -0.8514606]. 
=============================================
[2019-04-04 01:37:48,313] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.3406618e-24 6.1588037e-15 8.6628711e-15 1.0000000e+00 8.1151232e-18
 4.5660425e-10 7.0637397e-16], sum to 1.0000
[2019-04-04 01:37:48,322] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9327
[2019-04-04 01:37:48,346] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.2, 92.33333333333334, 0.0, 0.0, 26.0, 25.35881744228369, 0.4782902382297047, 0.0, 1.0, 42631.3829263104], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1476600.0000, 
sim time next is 1477200.0000, 
raw observation next is [2.2, 92.66666666666667, 0.0, 0.0, 26.0, 25.43120933628573, 0.4857369095755653, 0.0, 1.0, 18764.93507323838], 
processed observation next is [1.0, 0.08695652173913043, 0.5235457063711911, 0.9266666666666667, 0.0, 0.0, 0.6666666666666666, 0.6192674446904775, 0.6619123031918551, 0.0, 1.0, 0.08935683368208752], 
reward next is 0.9106, 
noisyNet noise sample is [array([1.1237613], dtype=float32), 0.9654555]. 
=============================================
[2019-04-04 01:37:48,678] A3C_AGENT_WORKER-Thread-4 INFO:Local step 171500, global step 2771847: loss 0.0092
[2019-04-04 01:37:48,684] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 171500, global step 2771849: learning rate 0.0005
[2019-04-04 01:37:50,011] A3C_AGENT_WORKER-Thread-11 INFO:Local step 174000, global step 2772234: loss 1.7748
[2019-04-04 01:37:50,012] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 174000, global step 2772234: learning rate 0.0005
[2019-04-04 01:37:51,871] A3C_AGENT_WORKER-Thread-18 INFO:Local step 173500, global step 2772654: loss 2.1874
[2019-04-04 01:37:51,871] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 173500, global step 2772654: learning rate 0.0005
[2019-04-04 01:37:53,672] A3C_AGENT_WORKER-Thread-5 INFO:Local step 172500, global step 2773235: loss 0.2195
[2019-04-04 01:37:53,678] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 172500, global step 2773235: learning rate 0.0005
[2019-04-04 01:37:55,240] A3C_AGENT_WORKER-Thread-2 INFO:Local step 174500, global step 2773720: loss 1.2145
[2019-04-04 01:37:55,240] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 174500, global step 2773720: learning rate 0.0005
[2019-04-04 01:37:56,177] A3C_AGENT_WORKER-Thread-14 INFO:Local step 173000, global step 2773966: loss 0.0708
[2019-04-04 01:37:56,179] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 173000, global step 2773966: learning rate 0.0005
[2019-04-04 01:37:58,728] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.7979156e-27 4.4367554e-19 7.7635582e-18 1.0000000e+00 2.6900812e-21
 5.1154386e-15 6.9814075e-19], sum to 1.0000
[2019-04-04 01:37:58,728] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3374
[2019-04-04 01:37:58,793] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 91.0, 0.0, 0.0, 26.0, 24.75115991553656, 0.2467389671570298, 0.0, 1.0, 42578.2482185349], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2076000.0000, 
sim time next is 2076600.0000, 
raw observation next is [-4.5, 91.0, 0.0, 0.0, 26.0, 24.70827020598099, 0.2409946543707616, 0.0, 1.0, 42653.82908975758], 
processed observation next is [1.0, 0.0, 0.3379501385041552, 0.91, 0.0, 0.0, 0.6666666666666666, 0.5590225171650826, 0.5803315514569205, 0.0, 1.0, 0.20311347185598846], 
reward next is 0.7969, 
noisyNet noise sample is [array([-0.59770525], dtype=float32), 0.73958766]. 
=============================================
[2019-04-04 01:38:00,582] A3C_AGENT_WORKER-Thread-6 INFO:Local step 173500, global step 2775167: loss 2.4430
[2019-04-04 01:38:00,582] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 173500, global step 2775167: learning rate 0.0005
[2019-04-04 01:38:00,814] A3C_AGENT_WORKER-Thread-10 INFO:Local step 173500, global step 2775227: loss 2.2119
[2019-04-04 01:38:00,815] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 173500, global step 2775227: learning rate 0.0005
[2019-04-04 01:38:02,046] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.0599823e-27 1.5076777e-18 1.7703579e-16 1.0000000e+00 2.9352709e-18
 4.0291981e-20 4.6125452e-19], sum to 1.0000
[2019-04-04 01:38:02,047] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7946
[2019-04-04 01:38:02,089] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.4, 26.0, 75.0, 63.33333333333334, 26.0, 25.58841772933478, 0.3377729626062238, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2823600.0000, 
sim time next is 2824200.0000, 
raw observation next is [6.3, 26.5, 71.0, 76.0, 26.0, 24.80754432832789, 0.3045050208852564, 1.0, 1.0, 177992.1451532395], 
processed observation next is [1.0, 0.6956521739130435, 0.6371191135734073, 0.265, 0.23666666666666666, 0.08397790055248619, 0.6666666666666666, 0.5672953606939908, 0.6015016736284188, 1.0, 1.0, 0.8475816435868547], 
reward next is 0.1524, 
noisyNet noise sample is [array([0.76476806], dtype=float32), -1.5857121]. 
=============================================
[2019-04-04 01:38:06,344] A3C_AGENT_WORKER-Thread-20 INFO:Local step 174000, global step 2776812: loss 1.7997
[2019-04-04 01:38:06,345] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 174000, global step 2776812: learning rate 0.0005
[2019-04-04 01:38:06,636] A3C_AGENT_WORKER-Thread-3 INFO:Local step 173500, global step 2776876: loss 2.2781
[2019-04-04 01:38:06,637] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 173500, global step 2776876: learning rate 0.0005
[2019-04-04 01:38:06,716] A3C_AGENT_WORKER-Thread-15 INFO:Local step 173000, global step 2776900: loss 0.0378
[2019-04-04 01:38:06,721] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 173000, global step 2776900: learning rate 0.0005
[2019-04-04 01:38:06,935] A3C_AGENT_WORKER-Thread-13 INFO:Local step 172500, global step 2776969: loss 0.2724
[2019-04-04 01:38:06,954] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 172500, global step 2776969: learning rate 0.0005
[2019-04-04 01:38:07,939] A3C_AGENT_WORKER-Thread-4 INFO:Local step 172000, global step 2777177: loss 30.9809
[2019-04-04 01:38:07,939] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 172000, global step 2777177: learning rate 0.0005
[2019-04-04 01:38:11,169] A3C_AGENT_WORKER-Thread-12 INFO:Local step 173500, global step 2777966: loss 2.6735
[2019-04-04 01:38:11,170] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 173500, global step 2777966: learning rate 0.0005
[2019-04-04 01:38:11,907] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.7623611e-24 2.1031509e-19 7.2605712e-18 1.0000000e+00 8.5852712e-21
 1.9694613e-17 9.9172503e-18], sum to 1.0000
[2019-04-04 01:38:11,907] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0619
[2019-04-04 01:38:11,927] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.2, 35.0, 0.0, 0.0, 26.0, 25.27443574647821, 0.2786464669734355, 0.0, 1.0, 40066.28945459457], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2496600.0000, 
sim time next is 2497200.0000, 
raw observation next is [-1.2, 34.33333333333334, 0.0, 0.0, 26.0, 25.32088175803266, 0.2765943581533092, 0.0, 1.0, 40085.68606100273], 
processed observation next is [0.0, 0.9130434782608695, 0.42936288088642666, 0.34333333333333343, 0.0, 0.0, 0.6666666666666666, 0.6100734798360549, 0.5921981193844364, 0.0, 1.0, 0.19088421933810823], 
reward next is 0.8091, 
noisyNet noise sample is [array([-0.1558158], dtype=float32), 0.65066856]. 
=============================================
[2019-04-04 01:38:12,769] A3C_AGENT_WORKER-Thread-17 INFO:Local step 174500, global step 2778470: loss 0.9846
[2019-04-04 01:38:12,770] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 174500, global step 2778470: learning rate 0.0005
[2019-04-04 01:38:15,630] A3C_AGENT_WORKER-Thread-19 INFO:Local step 174500, global step 2779207: loss 0.5918
[2019-04-04 01:38:15,635] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 174500, global step 2779207: learning rate 0.0005
[2019-04-04 01:38:15,905] A3C_AGENT_WORKER-Thread-16 INFO:Local step 174000, global step 2779276: loss 2.3684
[2019-04-04 01:38:15,906] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 174000, global step 2779276: learning rate 0.0005
[2019-04-04 01:38:19,421] A3C_AGENT_WORKER-Thread-11 INFO:Local step 174500, global step 2780074: loss 0.6376
[2019-04-04 01:38:19,422] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 174500, global step 2780074: learning rate 0.0005
[2019-04-04 01:38:21,154] A3C_AGENT_WORKER-Thread-5 INFO:Local step 173000, global step 2780534: loss 0.0048
[2019-04-04 01:38:21,155] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 173000, global step 2780534: learning rate 0.0005
[2019-04-04 01:38:25,023] A3C_AGENT_WORKER-Thread-18 INFO:Local step 174000, global step 2781482: loss 2.9225
[2019-04-04 01:38:25,026] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 174000, global step 2781482: learning rate 0.0005
[2019-04-04 01:38:25,797] A3C_AGENT_WORKER-Thread-2 INFO:Local step 175000, global step 2781644: loss 0.2792
[2019-04-04 01:38:25,853] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 175000, global step 2781644: learning rate 0.0005
[2019-04-04 01:38:31,252] A3C_AGENT_WORKER-Thread-14 INFO:Local step 173500, global step 2782939: loss 1.9377
[2019-04-04 01:38:31,253] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 173500, global step 2782939: learning rate 0.0005
[2019-04-04 01:38:32,629] A3C_AGENT_WORKER-Thread-6 INFO:Local step 174000, global step 2783315: loss 2.5880
[2019-04-04 01:38:32,631] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 174000, global step 2783315: learning rate 0.0005
[2019-04-04 01:38:32,860] A3C_AGENT_WORKER-Thread-10 INFO:Local step 174000, global step 2783375: loss 2.5011
[2019-04-04 01:38:32,872] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 174000, global step 2783375: learning rate 0.0005
[2019-04-04 01:38:35,224] A3C_AGENT_WORKER-Thread-13 INFO:Local step 173000, global step 2783992: loss 0.0115
[2019-04-04 01:38:35,225] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 173000, global step 2783992: learning rate 0.0005
[2019-04-04 01:38:36,239] A3C_AGENT_WORKER-Thread-4 INFO:Local step 172500, global step 2784229: loss 0.3330
[2019-04-04 01:38:36,240] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 172500, global step 2784229: learning rate 0.0005
[2019-04-04 01:38:38,232] A3C_AGENT_WORKER-Thread-20 INFO:Local step 174500, global step 2784760: loss 0.2156
[2019-04-04 01:38:38,293] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 174500, global step 2784760: learning rate 0.0005
[2019-04-04 01:38:40,192] A3C_AGENT_WORKER-Thread-3 INFO:Local step 174000, global step 2785311: loss 2.4613
[2019-04-04 01:38:40,193] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 174000, global step 2785311: learning rate 0.0005
[2019-04-04 01:38:42,834] A3C_AGENT_WORKER-Thread-12 INFO:Local step 174000, global step 2786033: loss 2.1555
[2019-04-04 01:38:42,869] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 174000, global step 2786034: learning rate 0.0005
[2019-04-04 01:38:43,354] A3C_AGENT_WORKER-Thread-17 INFO:Local step 175000, global step 2786190: loss 0.1758
[2019-04-04 01:38:43,376] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 175000, global step 2786190: learning rate 0.0005
[2019-04-04 01:38:43,808] A3C_AGENT_WORKER-Thread-15 INFO:Local step 173500, global step 2786323: loss 1.3249
[2019-04-04 01:38:43,809] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 173500, global step 2786323: learning rate 0.0005
[2019-04-04 01:38:45,240] A3C_AGENT_WORKER-Thread-19 INFO:Local step 175000, global step 2786664: loss 0.1504
[2019-04-04 01:38:45,240] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 175000, global step 2786664: learning rate 0.0005
[2019-04-04 01:38:47,392] A3C_AGENT_WORKER-Thread-16 INFO:Local step 174500, global step 2787264: loss 0.2221
[2019-04-04 01:38:47,394] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 174500, global step 2787265: learning rate 0.0005
[2019-04-04 01:38:48,963] A3C_AGENT_WORKER-Thread-11 INFO:Local step 175000, global step 2787723: loss 0.2726
[2019-04-04 01:38:48,982] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 175000, global step 2787723: learning rate 0.0005
[2019-04-04 01:38:49,758] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.7609766e-22 7.4083331e-15 4.9339242e-14 1.0000000e+00 5.3427408e-16
 3.0340425e-11 4.5427092e-14], sum to 1.0000
[2019-04-04 01:38:49,781] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4607
[2019-04-04 01:38:49,882] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.416666666666667, 81.0, 0.0, 0.0, 26.0, 24.86439877865594, 0.3399822820042357, 0.0, 1.0, 176786.3404150267], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1972200.0000, 
sim time next is 1972800.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.96398663873467, 0.3666944950818868, 0.0, 1.0, 55192.08228049615], 
processed observation next is [1.0, 0.8695652173913043, 0.30747922437673136, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5803322198945559, 0.622231498360629, 0.0, 1.0, 0.26281943943093405], 
reward next is 0.7372, 
noisyNet noise sample is [array([0.70039153], dtype=float32), 0.7061266]. 
=============================================
[2019-04-04 01:38:51,413] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5749361e-25 1.0666757e-18 5.5045627e-16 1.0000000e+00 3.5035040e-18
 2.6230628e-14 8.0336848e-18], sum to 1.0000
[2019-04-04 01:38:51,413] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3642
[2019-04-04 01:38:51,435] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.5, 100.0, 0.0, 0.0, 25.0, 24.41171571328234, 0.1158972036193831, 0.0, 1.0, 46716.89987822677], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 3112200.0000, 
sim time next is 3112800.0000, 
raw observation next is [0.6666666666666666, 100.0, 0.0, 0.0, 25.0, 24.39374691808696, 0.115731932364179, 0.0, 1.0, 51790.98557797032], 
processed observation next is [1.0, 0.0, 0.4810710987996307, 1.0, 0.0, 0.0, 0.5833333333333334, 0.5328122431739134, 0.5385773107880597, 0.0, 1.0, 0.2466237408474777], 
reward next is 0.7534, 
noisyNet noise sample is [array([0.5172053], dtype=float32), -0.9966795]. 
=============================================
[2019-04-04 01:38:53,011] A3C_AGENT_WORKER-Thread-2 INFO:Local step 175500, global step 2788868: loss 0.9360
[2019-04-04 01:38:53,012] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 175500, global step 2788868: learning rate 0.0005
[2019-04-04 01:38:53,659] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.3241896e-22 1.3000783e-15 7.7598356e-16 1.0000000e+00 2.0436235e-17
 5.1764613e-14 1.6976605e-15], sum to 1.0000
[2019-04-04 01:38:53,659] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3720
[2019-04-04 01:38:53,693] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 55.0, 0.0, 0.0, 26.0, 25.35985072637109, 0.4622608444211059, 1.0, 1.0, 25288.52304221615], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3354000.0000, 
sim time next is 3354600.0000, 
raw observation next is [-3.0, 55.0, 0.0, 0.0, 26.0, 25.27339039753069, 0.4484058728536411, 1.0, 1.0, 25226.24096120045], 
processed observation next is [1.0, 0.8260869565217391, 0.3795013850415513, 0.55, 0.0, 0.0, 0.6666666666666666, 0.6061158664608909, 0.649468624284547, 1.0, 1.0, 0.12012495695809737], 
reward next is 0.8799, 
noisyNet noise sample is [array([0.8285809], dtype=float32), -0.7885574]. 
=============================================
[2019-04-04 01:38:55,873] A3C_AGENT_WORKER-Thread-5 INFO:Local step 173500, global step 2789698: loss 0.5793
[2019-04-04 01:38:55,873] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 173500, global step 2789698: learning rate 0.0005
[2019-04-04 01:38:56,250] A3C_AGENT_WORKER-Thread-18 INFO:Local step 174500, global step 2789810: loss 0.2393
[2019-04-04 01:38:56,250] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 174500, global step 2789810: learning rate 0.0005
[2019-04-04 01:39:00,732] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.3965151e-27 9.0222780e-20 3.4584924e-16 1.0000000e+00 2.7705863e-21
 4.7362910e-18 1.1537776e-19], sum to 1.0000
[2019-04-04 01:39:00,732] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4544
[2019-04-04 01:39:00,787] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.8, 56.0, 105.5, 760.5, 26.0, 26.50753111077019, 0.6300089093226916, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2728800.0000, 
sim time next is 2729400.0000, 
raw observation next is [-4.666666666666667, 55.66666666666667, 104.3333333333333, 751.3333333333334, 26.0, 26.59196735891696, 0.642533121229471, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3333333333333333, 0.5566666666666668, 0.3477777777777777, 0.8302025782688767, 0.6666666666666666, 0.7159972799097467, 0.7141777070764904, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5738374], dtype=float32), 0.7885847]. 
=============================================
[2019-04-04 01:39:00,976] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.9683895e-28 4.5247264e-20 1.0027325e-20 1.0000000e+00 1.8549535e-21
 3.5967632e-20 2.2942283e-19], sum to 1.0000
[2019-04-04 01:39:00,976] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2460
[2019-04-04 01:39:01,027] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.266666666666667, 54.66666666666667, 99.33333333333333, 713.1666666666667, 26.0, 26.75429702290992, 0.6644100936124937, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2731200.0000, 
sim time next is 2731800.0000, 
raw observation next is [-4.133333333333333, 54.33333333333333, 96.66666666666667, 693.3333333333334, 26.0, 26.7719012492729, 0.5559292616545976, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.34810710987996313, 0.5433333333333333, 0.32222222222222224, 0.7661141804788214, 0.6666666666666666, 0.7309917707727417, 0.6853097538848658, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.10634001], dtype=float32), -0.52285004]. 
=============================================
[2019-04-04 01:39:01,856] A3C_AGENT_WORKER-Thread-6 INFO:Local step 174500, global step 2791415: loss 0.1666
[2019-04-04 01:39:01,856] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 174500, global step 2791415: learning rate 0.0005
[2019-04-04 01:39:01,981] A3C_AGENT_WORKER-Thread-14 INFO:Local step 174000, global step 2791458: loss 2.1797
[2019-04-04 01:39:01,986] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 174000, global step 2791461: learning rate 0.0005
[2019-04-04 01:39:02,595] A3C_AGENT_WORKER-Thread-10 INFO:Local step 174500, global step 2791625: loss 0.1083
[2019-04-04 01:39:02,597] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 174500, global step 2791625: learning rate 0.0005
[2019-04-04 01:39:02,949] A3C_AGENT_WORKER-Thread-4 INFO:Local step 173000, global step 2791715: loss 0.1633
[2019-04-04 01:39:02,960] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 173000, global step 2791720: learning rate 0.0005
[2019-04-04 01:39:06,137] A3C_AGENT_WORKER-Thread-20 INFO:Local step 175000, global step 2792632: loss 0.0917
[2019-04-04 01:39:06,157] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 175000, global step 2792639: learning rate 0.0005
[2019-04-04 01:39:08,582] A3C_AGENT_WORKER-Thread-13 INFO:Local step 173500, global step 2793279: loss 1.6723
[2019-04-04 01:39:08,583] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 173500, global step 2793279: learning rate 0.0005
[2019-04-04 01:39:08,657] A3C_AGENT_WORKER-Thread-17 INFO:Local step 175500, global step 2793296: loss 0.7236
[2019-04-04 01:39:08,657] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 175500, global step 2793296: learning rate 0.0005
[2019-04-04 01:39:09,147] A3C_AGENT_WORKER-Thread-3 INFO:Local step 174500, global step 2793454: loss 0.4861
[2019-04-04 01:39:09,148] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 174500, global step 2793454: learning rate 0.0005
[2019-04-04 01:39:10,203] A3C_AGENT_WORKER-Thread-19 INFO:Local step 175500, global step 2793783: loss 0.5072
[2019-04-04 01:39:10,225] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 175500, global step 2793783: learning rate 0.0005
[2019-04-04 01:39:11,148] A3C_AGENT_WORKER-Thread-12 INFO:Local step 174500, global step 2794018: loss 0.1730
[2019-04-04 01:39:11,148] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 174500, global step 2794018: learning rate 0.0005
[2019-04-04 01:39:13,698] A3C_AGENT_WORKER-Thread-15 INFO:Local step 174000, global step 2794754: loss 2.2937
[2019-04-04 01:39:13,699] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 174000, global step 2794754: learning rate 0.0005
[2019-04-04 01:39:13,776] A3C_AGENT_WORKER-Thread-11 INFO:Local step 175500, global step 2794776: loss 0.3248
[2019-04-04 01:39:13,781] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 175500, global step 2794777: learning rate 0.0005
[2019-04-04 01:39:16,207] A3C_AGENT_WORKER-Thread-16 INFO:Local step 175000, global step 2795469: loss 0.0824
[2019-04-04 01:39:16,225] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 175000, global step 2795469: learning rate 0.0005
[2019-04-04 01:39:18,374] A3C_AGENT_WORKER-Thread-2 INFO:Local step 176000, global step 2796059: loss 0.4074
[2019-04-04 01:39:18,375] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 176000, global step 2796059: learning rate 0.0005
[2019-04-04 01:39:23,804] A3C_AGENT_WORKER-Thread-18 INFO:Local step 175000, global step 2797690: loss 0.0928
[2019-04-04 01:39:23,805] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 175000, global step 2797690: learning rate 0.0005
[2019-04-04 01:39:24,467] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.8501716e-26 5.9706972e-18 1.9655445e-16 1.0000000e+00 3.6419679e-19
 1.0523237e-11 6.6945169e-19], sum to 1.0000
[2019-04-04 01:39:24,472] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3560
[2019-04-04 01:39:24,536] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 49.0, 75.0, 606.0, 25.0, 25.5314398393169, 0.4945060311916523, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 3513600.0000, 
sim time next is 3514200.0000, 
raw observation next is [3.0, 49.0, 70.66666666666666, 579.0, 25.0, 25.81309509782466, 0.5198777187812825, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.5457063711911359, 0.49, 0.23555555555555552, 0.6397790055248619, 0.5833333333333334, 0.651091258152055, 0.6732925729270942, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2773726], dtype=float32), -0.40311053]. 
=============================================
[2019-04-04 01:39:25,771] A3C_AGENT_WORKER-Thread-5 INFO:Local step 174000, global step 2798243: loss 2.1914
[2019-04-04 01:39:25,782] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 174000, global step 2798247: learning rate 0.0005
[2019-04-04 01:39:29,396] A3C_AGENT_WORKER-Thread-6 INFO:Local step 175000, global step 2799363: loss 0.0337
[2019-04-04 01:39:29,397] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 175000, global step 2799363: learning rate 0.0005
[2019-04-04 01:39:29,731] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.2120773e-24 7.3561152e-17 9.0151749e-16 1.0000000e+00 1.5931169e-19
 6.1998963e-13 1.9116885e-16], sum to 1.0000
[2019-04-04 01:39:29,731] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4365
[2019-04-04 01:39:29,789] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.8, 58.5, 110.3333333333333, 791.6666666666666, 26.0, 26.0749918923977, 0.5505286537863534, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2725800.0000, 
sim time next is 2726400.0000, 
raw observation next is [-5.6, 58.0, 109.6666666666667, 789.8333333333334, 26.0, 26.14421208327686, 0.5689347575551656, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.30747922437673136, 0.58, 0.3655555555555557, 0.872744014732965, 0.6666666666666666, 0.6786843402730716, 0.6896449191850552, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.105699], dtype=float32), -0.78651583]. 
=============================================
[2019-04-04 01:39:30,506] A3C_AGENT_WORKER-Thread-10 INFO:Local step 175000, global step 2799657: loss 0.1991
[2019-04-04 01:39:30,506] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 175000, global step 2799657: learning rate 0.0005
[2019-04-04 01:39:30,799] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.3604972e-21 3.9361292e-14 4.9196272e-12 1.0000000e+00 2.0391217e-14
 1.2617170e-11 1.7457608e-14], sum to 1.0000
[2019-04-04 01:39:30,800] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2791
[2019-04-04 01:39:30,829] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.45, 63.5, 152.0, 275.0, 26.0, 25.78726630252763, 0.366279709255815, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2629800.0000, 
sim time next is 2630400.0000, 
raw observation next is [-4.266666666666667, 63.0, 164.0, 257.6666666666667, 26.0, 25.75071202867253, 0.3652355624936834, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.3444136657433057, 0.63, 0.5466666666666666, 0.2847145488029466, 0.6666666666666666, 0.6458926690560443, 0.6217451874978944, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2006853], dtype=float32), -2.001508]. 
=============================================
[2019-04-04 01:39:31,757] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-04 01:39:31,765] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 01:39:31,765] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:39:31,767] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run29
[2019-04-04 01:39:31,817] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 01:39:31,819] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:39:31,821] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 01:39:31,824] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:39:31,827] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run29
[2019-04-04 01:39:31,883] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run29
[2019-04-04 01:41:17,740] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.0059766], dtype=float32), 0.9201055]
[2019-04-04 01:41:17,740] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-1.7, 39.0, 0.0, 0.0, 22.5, 21.89918091091924, -0.4866720393652253, 0.0, 1.0, 90514.95685285532]
[2019-04-04 01:41:17,741] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 01:41:17,741] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [6.4190030e-18 1.7814123e-14 5.9974091e-13 1.0000000e+00 1.2577734e-12
 1.5698338e-11 7.4230222e-13], sampled 0.7777683091686812
[2019-04-04 01:41:29,029] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.0059766], dtype=float32), 0.9201055]
[2019-04-04 01:41:29,029] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-1.602708204, 91.62952656499999, 0.0, 0.0, 23.0, 22.47972873861165, -0.2874453160580404, 0.0, 1.0, 78385.90320433384]
[2019-04-04 01:41:29,029] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 01:41:29,030] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.4897844e-17 1.3003407e-12 2.5315632e-11 1.0000000e+00 1.0063046e-11
 7.7728695e-09 1.1126079e-11], sampled 0.9291097446357198
[2019-04-04 01:41:51,215] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7487.3338 204260002.8640 209.2459
[2019-04-04 01:42:02,833] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7417.9284 219507055.5100 -601.3519
[2019-04-04 01:42:06,664] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 6973.3641 232307307.7924 -736.2445
[2019-04-04 01:42:07,689] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 2800000, evaluation results [2800000.0, 7417.928418635287, 219507055.5099788, -601.3518572071965, 7487.333808092095, 204260002.86399695, 209.24592224412334, 6973.364093644762, 232307307.7923974, -736.2445103511777]
[2019-04-04 01:42:08,075] A3C_AGENT_WORKER-Thread-14 INFO:Local step 174500, global step 2800134: loss 0.1895
[2019-04-04 01:42:08,077] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 174500, global step 2800134: learning rate 0.0005
[2019-04-04 01:42:08,193] A3C_AGENT_WORKER-Thread-20 INFO:Local step 175500, global step 2800173: loss 0.1943
[2019-04-04 01:42:08,197] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 175500, global step 2800173: learning rate 0.0005
[2019-04-04 01:42:09,032] A3C_AGENT_WORKER-Thread-17 INFO:Local step 176000, global step 2800473: loss 0.4645
[2019-04-04 01:42:09,033] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 176000, global step 2800473: learning rate 0.0005
[2019-04-04 01:42:10,343] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.0897763e-21 2.6631036e-15 5.4698685e-12 1.0000000e+00 4.0822221e-14
 3.0662354e-12 9.9131098e-14], sum to 1.0000
[2019-04-04 01:42:10,344] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9387
[2019-04-04 01:42:10,360] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.83087765583175, 0.2872383287112171, 0.0, 1.0, 41104.36271413106], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3379800.0000, 
sim time next is 3380400.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.83961419999779, 0.2829561760758867, 0.0, 1.0, 41085.82598418969], 
processed observation next is [1.0, 0.13043478260869565, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5699678499998159, 0.594318725358629, 0.0, 1.0, 0.19564679040090327], 
reward next is 0.8044, 
noisyNet noise sample is [array([-0.56390667], dtype=float32), -0.64913344]. 
=============================================
[2019-04-04 01:42:10,582] A3C_AGENT_WORKER-Thread-19 INFO:Local step 176000, global step 2801113: loss 0.5741
[2019-04-04 01:42:10,583] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 176000, global step 2801114: learning rate 0.0005
[2019-04-04 01:42:11,685] A3C_AGENT_WORKER-Thread-3 INFO:Local step 175000, global step 2801648: loss 0.0567
[2019-04-04 01:42:11,686] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 175000, global step 2801648: learning rate 0.0005
[2019-04-04 01:42:11,753] A3C_AGENT_WORKER-Thread-4 INFO:Local step 173500, global step 2801677: loss 0.3551
[2019-04-04 01:42:11,754] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 173500, global step 2801678: learning rate 0.0005
[2019-04-04 01:42:11,949] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.6667981e-26 3.1632609e-23 7.8961330e-20 1.0000000e+00 3.3845321e-22
 2.6343308e-20 2.6796037e-21], sum to 1.0000
[2019-04-04 01:42:11,950] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3922
[2019-04-04 01:42:11,980] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.4, 61.0, 0.0, 0.0, 26.0, 23.59412419285939, -0.06688910230300098, 0.0, 1.0, 44321.29452892929], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2433600.0000, 
sim time next is 2434200.0000, 
raw observation next is [-8.4, 61.0, 0.0, 0.0, 26.0, 23.55085934786635, -0.07578099382454333, 0.0, 1.0, 44367.29067000026], 
processed observation next is [0.0, 0.17391304347826086, 0.2299168975069252, 0.61, 0.0, 0.0, 0.6666666666666666, 0.46257161232219585, 0.4747396687251522, 0.0, 1.0, 0.21127281271428694], 
reward next is 0.7887, 
noisyNet noise sample is [array([0.62657976], dtype=float32), 1.4555901]. 
=============================================
[2019-04-04 01:42:12,448] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.7331973e-29 4.6433069e-23 7.6277094e-23 1.0000000e+00 1.9914233e-24
 5.1964552e-21 2.5409677e-22], sum to 1.0000
[2019-04-04 01:42:12,448] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4096
[2019-04-04 01:42:12,479] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.5, 54.5, 64.0, 522.0, 25.0, 24.55098660431807, 0.2484324502974175, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 3688200.0000, 
sim time next is 3688800.0000, 
raw observation next is [4.333333333333334, 56.0, 55.83333333333334, 462.5, 25.0, 24.53065202828761, 0.2372677177089485, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.58264081255771, 0.56, 0.18611111111111114, 0.511049723756906, 0.5833333333333334, 0.5442210023573008, 0.5790892392363162, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.21204919], dtype=float32), 0.8616041]. 
=============================================
[2019-04-04 01:42:12,723] A3C_AGENT_WORKER-Thread-13 INFO:Local step 174000, global step 2802096: loss 2.3214
[2019-04-04 01:42:12,731] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 174000, global step 2802101: learning rate 0.0005
[2019-04-04 01:42:12,912] A3C_AGENT_WORKER-Thread-11 INFO:Local step 176000, global step 2802171: loss 0.5533
[2019-04-04 01:42:12,918] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 176000, global step 2802172: learning rate 0.0005
[2019-04-04 01:42:13,132] A3C_AGENT_WORKER-Thread-12 INFO:Local step 175000, global step 2802249: loss 0.0264
[2019-04-04 01:42:13,135] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 175000, global step 2802249: learning rate 0.0005
[2019-04-04 01:42:15,200] A3C_AGENT_WORKER-Thread-16 INFO:Local step 175500, global step 2803093: loss 0.0966
[2019-04-04 01:42:15,205] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 175500, global step 2803098: learning rate 0.0005
[2019-04-04 01:42:15,561] A3C_AGENT_WORKER-Thread-2 INFO:Local step 176500, global step 2803240: loss 0.2057
[2019-04-04 01:42:15,563] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 176500, global step 2803241: learning rate 0.0005
[2019-04-04 01:42:16,047] A3C_AGENT_WORKER-Thread-15 INFO:Local step 174500, global step 2803435: loss 0.2121
[2019-04-04 01:42:16,047] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 174500, global step 2803435: learning rate 0.0005
[2019-04-04 01:42:20,766] A3C_AGENT_WORKER-Thread-18 INFO:Local step 175500, global step 2805465: loss 0.1339
[2019-04-04 01:42:20,766] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 175500, global step 2805465: learning rate 0.0005
[2019-04-04 01:42:24,619] A3C_AGENT_WORKER-Thread-5 INFO:Local step 174500, global step 2807008: loss 1.0081
[2019-04-04 01:42:24,621] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 174500, global step 2807008: learning rate 0.0005
[2019-04-04 01:42:25,018] A3C_AGENT_WORKER-Thread-6 INFO:Local step 175500, global step 2807175: loss 0.3110
[2019-04-04 01:42:25,036] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 175500, global step 2807175: learning rate 0.0005
[2019-04-04 01:42:25,835] A3C_AGENT_WORKER-Thread-17 INFO:Local step 176500, global step 2807503: loss 0.4137
[2019-04-04 01:42:25,836] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 176500, global step 2807503: learning rate 0.0005
[2019-04-04 01:42:26,254] A3C_AGENT_WORKER-Thread-10 INFO:Local step 175500, global step 2807690: loss 0.1450
[2019-04-04 01:42:26,255] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 175500, global step 2807690: learning rate 0.0005
[2019-04-04 01:42:26,429] A3C_AGENT_WORKER-Thread-20 INFO:Local step 176000, global step 2807760: loss 0.3075
[2019-04-04 01:42:26,430] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 176000, global step 2807761: learning rate 0.0005
[2019-04-04 01:42:27,328] A3C_AGENT_WORKER-Thread-19 INFO:Local step 176500, global step 2808177: loss 1.0239
[2019-04-04 01:42:27,329] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 176500, global step 2808177: learning rate 0.0005
[2019-04-04 01:42:27,863] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.2966610e-22 1.5603230e-15 1.3445016e-13 1.0000000e+00 1.2090859e-16
 3.5581062e-14 2.7378254e-15], sum to 1.0000
[2019-04-04 01:42:27,864] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1602
[2019-04-04 01:42:27,874] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.0, 77.0, 0.0, 0.0, 26.0, 25.05022784050661, 0.4001276478424027, 0.0, 1.0, 43632.35424808804], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3285000.0000, 
sim time next is 3285600.0000, 
raw observation next is [-7.0, 74.66666666666667, 0.0, 0.0, 26.0, 25.04858805345034, 0.4026624990598606, 0.0, 1.0, 43670.74327265855], 
processed observation next is [1.0, 0.0, 0.2686980609418283, 0.7466666666666667, 0.0, 0.0, 0.6666666666666666, 0.5873823377875285, 0.6342208330199536, 0.0, 1.0, 0.20795592034599308], 
reward next is 0.7920, 
noisyNet noise sample is [array([-0.39771256], dtype=float32), 0.18401504]. 
=============================================
[2019-04-04 01:42:28,606] A3C_AGENT_WORKER-Thread-14 INFO:Local step 175000, global step 2808674: loss 0.0566
[2019-04-04 01:42:28,610] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 175000, global step 2808675: learning rate 0.0005
[2019-04-04 01:42:29,893] A3C_AGENT_WORKER-Thread-11 INFO:Local step 176500, global step 2809178: loss 0.4807
[2019-04-04 01:42:29,898] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 176500, global step 2809178: learning rate 0.0005
[2019-04-04 01:42:30,473] A3C_AGENT_WORKER-Thread-3 INFO:Local step 175500, global step 2809423: loss 0.0637
[2019-04-04 01:42:30,474] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 175500, global step 2809424: learning rate 0.0005
[2019-04-04 01:42:32,111] A3C_AGENT_WORKER-Thread-12 INFO:Local step 175500, global step 2810024: loss 0.0626
[2019-04-04 01:42:32,120] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 175500, global step 2810026: learning rate 0.0005
[2019-04-04 01:42:33,242] A3C_AGENT_WORKER-Thread-16 INFO:Local step 176000, global step 2810541: loss 0.5095
[2019-04-04 01:42:33,244] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 176000, global step 2810541: learning rate 0.0005
[2019-04-04 01:42:34,326] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.2764917e-27 4.3244527e-25 5.5721307e-22 1.0000000e+00 1.7685966e-23
 3.0587182e-23 1.6238505e-19], sum to 1.0000
[2019-04-04 01:42:34,329] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5851
[2019-04-04 01:42:34,354] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.733333333333334, 60.33333333333334, 0.0, 0.0, 26.0, 23.25945265983755, -0.1441298189418079, 0.0, 1.0, 44214.54167558936], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2439600.0000, 
sim time next is 2440200.0000, 
raw observation next is [-8.816666666666666, 60.16666666666666, 0.0, 0.0, 26.0, 23.2220526039507, -0.1502629828797678, 0.0, 1.0, 44172.13969593932], 
processed observation next is [0.0, 0.21739130434782608, 0.21837488457987075, 0.6016666666666666, 0.0, 0.0, 0.6666666666666666, 0.43517105032922504, 0.4499123390400774, 0.0, 1.0, 0.2103435223616158], 
reward next is 0.7897, 
noisyNet noise sample is [array([-0.57657087], dtype=float32), 0.9334805]. 
=============================================
[2019-04-04 01:42:34,626] A3C_AGENT_WORKER-Thread-13 INFO:Local step 174500, global step 2811138: loss 0.4974
[2019-04-04 01:42:34,634] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 174500, global step 2811141: learning rate 0.0005
[2019-04-04 01:42:34,769] A3C_AGENT_WORKER-Thread-4 INFO:Local step 174000, global step 2811204: loss 1.5940
[2019-04-04 01:42:34,769] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 174000, global step 2811204: learning rate 0.0005
[2019-04-04 01:42:35,080] A3C_AGENT_WORKER-Thread-2 INFO:Local step 177000, global step 2811325: loss 0.0189
[2019-04-04 01:42:35,082] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 177000, global step 2811327: learning rate 0.0005
[2019-04-04 01:42:37,422] A3C_AGENT_WORKER-Thread-15 INFO:Local step 175000, global step 2812347: loss 0.0897
[2019-04-04 01:42:37,424] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 175000, global step 2812347: learning rate 0.0005
[2019-04-04 01:42:39,012] A3C_AGENT_WORKER-Thread-18 INFO:Local step 176000, global step 2813076: loss 0.5443
[2019-04-04 01:42:39,013] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 176000, global step 2813076: learning rate 0.0005
[2019-04-04 01:42:42,904] A3C_AGENT_WORKER-Thread-6 INFO:Local step 176000, global step 2814851: loss 0.2844
[2019-04-04 01:42:42,906] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 176000, global step 2814852: learning rate 0.0005
[2019-04-04 01:42:43,461] A3C_AGENT_WORKER-Thread-20 INFO:Local step 176500, global step 2815109: loss 0.0490
[2019-04-04 01:42:43,462] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 176500, global step 2815109: learning rate 0.0005
[2019-04-04 01:42:44,450] A3C_AGENT_WORKER-Thread-10 INFO:Local step 176000, global step 2815539: loss 0.3006
[2019-04-04 01:42:44,450] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 176000, global step 2815539: learning rate 0.0005
[2019-04-04 01:42:44,562] A3C_AGENT_WORKER-Thread-17 INFO:Local step 177000, global step 2815600: loss 0.0190
[2019-04-04 01:42:44,564] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 177000, global step 2815600: learning rate 0.0005
[2019-04-04 01:42:45,150] A3C_AGENT_WORKER-Thread-19 INFO:Local step 177000, global step 2815907: loss 0.0446
[2019-04-04 01:42:45,152] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 177000, global step 2815908: learning rate 0.0005
[2019-04-04 01:42:45,395] A3C_AGENT_WORKER-Thread-5 INFO:Local step 175000, global step 2816028: loss 0.0423
[2019-04-04 01:42:45,396] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 175000, global step 2816029: learning rate 0.0005
[2019-04-04 01:42:47,272] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.4752781e-23 2.5882773e-15 9.9942569e-13 1.0000000e+00 9.0189196e-17
 3.0530932e-12 7.3087957e-15], sum to 1.0000
[2019-04-04 01:42:47,274] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6029
[2019-04-04 01:42:47,303] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.166666666666667, 100.0, 0.0, 0.0, 26.0, 25.35709358389626, 0.5051920201706669, 0.0, 1.0, 40667.08648996945], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3215400.0000, 
sim time next is 3216000.0000, 
raw observation next is [-2.333333333333333, 100.0, 0.0, 0.0, 26.0, 25.35175339098315, 0.497183931175763, 0.0, 1.0, 40591.31940447319], 
processed observation next is [1.0, 0.21739130434782608, 0.3979686057248385, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6126461159152624, 0.6657279770585877, 0.0, 1.0, 0.19329199716415807], 
reward next is 0.8067, 
noisyNet noise sample is [array([-1.0780438], dtype=float32), -1.539183]. 
=============================================
[2019-04-04 01:42:47,324] A3C_AGENT_WORKER-Thread-14 INFO:Local step 175500, global step 2816882: loss 0.0632
[2019-04-04 01:42:47,326] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[78.8459  ]
 [78.982086]
 [79.091736]
 [79.20315 ]
 [79.262764]], R is [[78.72533417]
 [78.74443054]
 [78.76245117]
 [78.77825165]
 [78.7875061 ]].
[2019-04-04 01:42:47,327] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 175500, global step 2816882: learning rate 0.0005
[2019-04-04 01:42:47,512] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.1874196e-21 7.2871299e-13 9.6380380e-13 1.0000000e+00 2.9709834e-13
 6.4543836e-11 8.8623282e-13], sum to 1.0000
[2019-04-04 01:42:47,514] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6053
[2019-04-04 01:42:47,576] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 24.97093429898765, 0.4543329406531622, 0.0, 1.0, 109566.0563669344], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4477800.0000, 
sim time next is 4478400.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.00722180746986, 0.462973062055416, 1.0, 1.0, 45942.96420672511], 
processed observation next is [1.0, 0.8695652173913043, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5839351506224885, 0.654324354018472, 1.0, 1.0, 0.21877602003202434], 
reward next is 0.7812, 
noisyNet noise sample is [array([0.3427178], dtype=float32), -0.22583985]. 
=============================================
[2019-04-04 01:42:47,876] A3C_AGENT_WORKER-Thread-3 INFO:Local step 176000, global step 2817115: loss 0.3703
[2019-04-04 01:42:47,876] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 176000, global step 2817115: learning rate 0.0005
[2019-04-04 01:42:47,909] A3C_AGENT_WORKER-Thread-11 INFO:Local step 177000, global step 2817132: loss 0.0096
[2019-04-04 01:42:47,909] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 177000, global step 2817132: learning rate 0.0005
[2019-04-04 01:42:49,536] A3C_AGENT_WORKER-Thread-12 INFO:Local step 176000, global step 2817905: loss 0.2666
[2019-04-04 01:42:49,537] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 176000, global step 2817906: learning rate 0.0005
[2019-04-04 01:42:49,966] A3C_AGENT_WORKER-Thread-16 INFO:Local step 176500, global step 2818113: loss 0.0907
[2019-04-04 01:42:49,971] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 176500, global step 2818113: learning rate 0.0005
[2019-04-04 01:42:51,480] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.5826583e-25 4.3571361e-19 2.1584636e-17 1.0000000e+00 1.0575186e-19
 1.6940984e-12 5.8113102e-18], sum to 1.0000
[2019-04-04 01:42:51,480] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0081
[2019-04-04 01:42:51,509] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.0, 49.0, 0.0, 0.0, 26.0, 25.40329084599594, 0.4933882949559056, 0.0, 1.0, 66135.42516809114], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3967200.0000, 
sim time next is 3967800.0000, 
raw observation next is [-8.166666666666668, 49.66666666666667, 0.0, 0.0, 26.0, 25.38875867906184, 0.4458925939011708, 0.0, 1.0, 60789.27645499849], 
processed observation next is [1.0, 0.9565217391304348, 0.2363804247460757, 0.4966666666666667, 0.0, 0.0, 0.6666666666666666, 0.6157298899218201, 0.6486308646337237, 0.0, 1.0, 0.2894727450238023], 
reward next is 0.7105, 
noisyNet noise sample is [array([2.4827497], dtype=float32), -0.47285163]. 
=============================================
[2019-04-04 01:42:51,633] A3C_AGENT_WORKER-Thread-2 INFO:Local step 177500, global step 2818890: loss 1.3656
[2019-04-04 01:42:51,635] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 177500, global step 2818890: learning rate 0.0005
[2019-04-04 01:42:55,497] A3C_AGENT_WORKER-Thread-13 INFO:Local step 175000, global step 2820689: loss 0.0500
[2019-04-04 01:42:55,499] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 175000, global step 2820689: learning rate 0.0005
[2019-04-04 01:42:55,606] A3C_AGENT_WORKER-Thread-18 INFO:Local step 176500, global step 2820741: loss 0.0319
[2019-04-04 01:42:55,606] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 176500, global step 2820741: learning rate 0.0005
[2019-04-04 01:42:55,753] A3C_AGENT_WORKER-Thread-15 INFO:Local step 175500, global step 2820817: loss 0.0803
[2019-04-04 01:42:55,754] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 175500, global step 2820817: learning rate 0.0005
[2019-04-04 01:42:56,279] A3C_AGENT_WORKER-Thread-4 INFO:Local step 174500, global step 2821035: loss 1.9041
[2019-04-04 01:42:56,280] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 174500, global step 2821035: learning rate 0.0005
[2019-04-04 01:42:59,698] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.83088081e-30 1.63029237e-20 2.73254947e-20 1.00000000e+00
 1.05264697e-22 6.35765914e-20 3.46228522e-21], sum to 1.0000
[2019-04-04 01:42:59,699] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9603
[2019-04-04 01:42:59,711] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.833333333333333, 48.83333333333334, 111.0, 777.3333333333333, 26.0, 26.61407856783318, 0.6261317393258568, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3408600.0000, 
sim time next is 3409200.0000, 
raw observation next is [3.0, 49.0, 112.0, 784.0, 26.0, 26.62998875192516, 0.6194436457584476, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.5457063711911359, 0.49, 0.37333333333333335, 0.8662983425414365, 0.6666666666666666, 0.7191657293270968, 0.7064812152528158, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6629845], dtype=float32), 0.3346613]. 
=============================================
[2019-04-04 01:42:59,878] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.0176710e-21 1.3114463e-13 3.3799560e-12 1.0000000e+00 7.7551758e-12
 1.1591715e-14 5.1642400e-15], sum to 1.0000
[2019-04-04 01:42:59,879] A3C_AGENT_WORKER-Thread-6 INFO:Local step 176500, global step 2822571: loss 0.1917
[2019-04-04 01:42:59,882] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 176500, global step 2822572: learning rate 0.0005
[2019-04-04 01:42:59,883] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0889
[2019-04-04 01:42:59,891] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 51.0, 119.5, 0.0, 25.0, 25.45787623733736, 0.3377002662472551, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 4534800.0000, 
sim time next is 4535400.0000, 
raw observation next is [2.0, 49.5, 121.0, 0.0, 25.0, 25.40726839583847, 0.3388561110622006, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.518005540166205, 0.495, 0.4033333333333333, 0.0, 0.5833333333333334, 0.6172723663198726, 0.6129520370207335, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.7072269], dtype=float32), 0.550276]. 
=============================================
[2019-04-04 01:43:00,028] A3C_AGENT_WORKER-Thread-17 INFO:Local step 177500, global step 2822638: loss 2.5267
[2019-04-04 01:43:00,028] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 177500, global step 2822638: learning rate 0.0005
[2019-04-04 01:43:00,640] A3C_AGENT_WORKER-Thread-19 INFO:Local step 177500, global step 2822922: loss 1.1402
[2019-04-04 01:43:00,643] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 177500, global step 2822922: learning rate 0.0005
[2019-04-04 01:43:00,924] A3C_AGENT_WORKER-Thread-10 INFO:Local step 176500, global step 2823067: loss 0.0748
[2019-04-04 01:43:00,925] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 176500, global step 2823067: learning rate 0.0005
[2019-04-04 01:43:01,134] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4979546e-26 5.1039282e-19 1.7050687e-19 1.0000000e+00 4.2830750e-20
 2.4628998e-17 2.6066936e-18], sum to 1.0000
[2019-04-04 01:43:01,134] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9113
[2019-04-04 01:43:01,152] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.4175252156104, 0.455219089062636, 0.0, 1.0, 18765.13430203387], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3896400.0000, 
sim time next is 3897000.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.69467993263127, 0.4419597717709146, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.40720221606648205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6412233277192726, 0.6473199239236381, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9375755], dtype=float32), 0.06343187]. 
=============================================
[2019-04-04 01:43:01,167] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[86.45449 ]
 [86.449104]
 [86.361916]
 [86.27651 ]
 [86.12282 ]], R is [[86.48085785]
 [86.52669525]
 [86.45729065]
 [86.35929108]
 [86.19873047]].
[2019-04-04 01:43:01,616] A3C_AGENT_WORKER-Thread-20 INFO:Local step 177000, global step 2823363: loss 0.0090
[2019-04-04 01:43:01,617] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 177000, global step 2823363: learning rate 0.0005
[2019-04-04 01:43:02,986] A3C_AGENT_WORKER-Thread-11 INFO:Local step 177500, global step 2823968: loss 0.4852
[2019-04-04 01:43:02,994] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 177500, global step 2823969: learning rate 0.0005
[2019-04-04 01:43:03,363] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1650287e-27 1.6352228e-23 6.9821921e-23 1.0000000e+00 9.3998324e-23
 2.8972444e-20 1.0535037e-20], sum to 1.0000
[2019-04-04 01:43:03,364] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9986
[2019-04-04 01:43:03,396] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.08649825634355, 0.3437641305355191, 0.0, 1.0, 198831.9754348916], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3612000.0000, 
sim time next is 3612600.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.07041331068512, 0.3685653486576476, 0.0, 1.0, 165499.45588227], 
processed observation next is [0.0, 0.8260869565217391, 0.4349030470914128, 0.42, 0.0, 0.0, 0.6666666666666666, 0.5892011092237599, 0.6228551162192159, 0.0, 1.0, 0.7880926470584286], 
reward next is 0.2119, 
noisyNet noise sample is [array([1.218875], dtype=float32), 0.9734971]. 
=============================================
[2019-04-04 01:43:04,020] A3C_AGENT_WORKER-Thread-5 INFO:Local step 175500, global step 2824462: loss 0.0599
[2019-04-04 01:43:04,021] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 175500, global step 2824462: learning rate 0.0005
[2019-04-04 01:43:04,546] A3C_AGENT_WORKER-Thread-3 INFO:Local step 176500, global step 2824694: loss 0.0450
[2019-04-04 01:43:04,547] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 176500, global step 2824694: learning rate 0.0005
[2019-04-04 01:43:05,354] A3C_AGENT_WORKER-Thread-14 INFO:Local step 176000, global step 2825003: loss 0.2476
[2019-04-04 01:43:05,356] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 176000, global step 2825003: learning rate 0.0005
[2019-04-04 01:43:06,786] A3C_AGENT_WORKER-Thread-12 INFO:Local step 176500, global step 2825713: loss 0.2011
[2019-04-04 01:43:06,788] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 176500, global step 2825713: learning rate 0.0005
[2019-04-04 01:43:08,403] A3C_AGENT_WORKER-Thread-16 INFO:Local step 177000, global step 2826482: loss 0.0169
[2019-04-04 01:43:08,404] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 177000, global step 2826482: learning rate 0.0005
[2019-04-04 01:43:08,667] A3C_AGENT_WORKER-Thread-2 INFO:Local step 178000, global step 2826621: loss 0.4292
[2019-04-04 01:43:08,671] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 178000, global step 2826622: learning rate 0.0005
[2019-04-04 01:43:08,798] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.2068868e-25 2.4376629e-17 1.8907423e-17 1.0000000e+00 1.4433807e-17
 3.2708070e-13 9.4019003e-18], sum to 1.0000
[2019-04-04 01:43:08,799] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2429
[2019-04-04 01:43:08,814] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 57.0, 0.0, 0.0, 25.0, 24.80105484564152, 0.3337289645277318, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 4660200.0000, 
sim time next is 4660800.0000, 
raw observation next is [2.0, 57.0, 0.0, 0.0, 25.0, 24.71784693565624, 0.3164502644121573, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.518005540166205, 0.57, 0.0, 0.0, 0.5833333333333334, 0.5598205779713533, 0.6054834214707191, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.49046806], dtype=float32), 0.48680353]. 
=============================================
[2019-04-04 01:43:10,714] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.0962249e-25 2.9182575e-17 1.8701773e-16 1.0000000e+00 3.3528368e-18
 4.2645652e-15 1.6976717e-17], sum to 1.0000
[2019-04-04 01:43:10,714] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6513
[2019-04-04 01:43:10,759] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 92.0, 89.0, 0.0, 25.0, 25.14041454093121, 0.2924577984208705, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 4698000.0000, 
sim time next is 4698600.0000, 
raw observation next is [0.0, 92.0, 97.66666666666667, 0.0, 25.0, 25.14350968127934, 0.303544360103676, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.46260387811634357, 0.92, 0.3255555555555556, 0.0, 0.5833333333333334, 0.5952924734399451, 0.601181453367892, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3793122], dtype=float32), -1.0467439]. 
=============================================
[2019-04-04 01:43:13,620] A3C_AGENT_WORKER-Thread-15 INFO:Local step 176000, global step 2828836: loss 0.1675
[2019-04-04 01:43:13,622] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 176000, global step 2828837: learning rate 0.0005
[2019-04-04 01:43:14,090] A3C_AGENT_WORKER-Thread-18 INFO:Local step 177000, global step 2829045: loss 0.0428
[2019-04-04 01:43:14,090] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 177000, global step 2829045: learning rate 0.0005
[2019-04-04 01:43:14,259] A3C_AGENT_WORKER-Thread-13 INFO:Local step 175500, global step 2829136: loss 0.0827
[2019-04-04 01:43:14,260] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 175500, global step 2829137: learning rate 0.0005
[2019-04-04 01:43:16,393] A3C_AGENT_WORKER-Thread-4 INFO:Local step 175000, global step 2830141: loss 0.0114
[2019-04-04 01:43:16,395] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 175000, global step 2830142: learning rate 0.0005
[2019-04-04 01:43:16,503] A3C_AGENT_WORKER-Thread-19 INFO:Local step 178000, global step 2830200: loss 0.2848
[2019-04-04 01:43:16,505] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 178000, global step 2830200: learning rate 0.0005
[2019-04-04 01:43:16,635] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.8806380e-27 8.7672534e-24 6.6629728e-20 1.0000000e+00 4.0895524e-23
 9.3214725e-19 1.4809197e-20], sum to 1.0000
[2019-04-04 01:43:16,635] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1632
[2019-04-04 01:43:16,659] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.166666666666667, 65.83333333333334, 0.0, 0.0, 26.0, 24.85057522790968, 0.304199631720096, 0.0, 1.0, 40876.20026670017], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3561000.0000, 
sim time next is 3561600.0000, 
raw observation next is [-5.333333333333334, 66.66666666666667, 0.0, 0.0, 26.0, 24.79768532645382, 0.2940214234654812, 0.0, 1.0, 40881.87856423299], 
processed observation next is [0.0, 0.21739130434782608, 0.31486611265004616, 0.6666666666666667, 0.0, 0.0, 0.6666666666666666, 0.566473777204485, 0.5980071411551604, 0.0, 1.0, 0.1946756122106333], 
reward next is 0.8053, 
noisyNet noise sample is [array([-1.3779529], dtype=float32), 2.1836367]. 
=============================================
[2019-04-04 01:43:17,084] A3C_AGENT_WORKER-Thread-17 INFO:Local step 178000, global step 2830509: loss 0.0363
[2019-04-04 01:43:17,085] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 178000, global step 2830510: learning rate 0.0005
[2019-04-04 01:43:17,752] A3C_AGENT_WORKER-Thread-20 INFO:Local step 177500, global step 2830825: loss 1.1600
[2019-04-04 01:43:17,756] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 177500, global step 2830825: learning rate 0.0005
[2019-04-04 01:43:18,619] A3C_AGENT_WORKER-Thread-6 INFO:Local step 177000, global step 2831229: loss 0.0298
[2019-04-04 01:43:18,630] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 177000, global step 2831229: learning rate 0.0005
[2019-04-04 01:43:18,960] A3C_AGENT_WORKER-Thread-10 INFO:Local step 177000, global step 2831395: loss 0.0493
[2019-04-04 01:43:18,963] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 177000, global step 2831395: learning rate 0.0005
[2019-04-04 01:43:18,974] A3C_AGENT_WORKER-Thread-11 INFO:Local step 178000, global step 2831404: loss 0.1201
[2019-04-04 01:43:18,976] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 178000, global step 2831404: learning rate 0.0005
[2019-04-04 01:43:21,131] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.1449730e-25 7.1691702e-18 6.1760394e-18 1.0000000e+00 1.1461535e-19
 9.8483070e-16 5.1831195e-18], sum to 1.0000
[2019-04-04 01:43:21,132] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4714
[2019-04-04 01:43:21,150] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 75.0, 0.0, 0.0, 26.0, 25.45920731835644, 0.4585770648872567, 0.0, 1.0, 37277.81320864828], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3795600.0000, 
sim time next is 3796200.0000, 
raw observation next is [-3.0, 74.0, 0.0, 0.0, 26.0, 25.40345809216114, 0.4508740858451317, 0.0, 1.0, 66324.5507412274], 
processed observation next is [1.0, 0.9565217391304348, 0.3795013850415513, 0.74, 0.0, 0.0, 0.6666666666666666, 0.6169548410134285, 0.6502913619483772, 0.0, 1.0, 0.31583119400584475], 
reward next is 0.6842, 
noisyNet noise sample is [array([0.96638435], dtype=float32), -0.8656581]. 
=============================================
[2019-04-04 01:43:21,514] A3C_AGENT_WORKER-Thread-14 INFO:Local step 176500, global step 2832576: loss 0.0505
[2019-04-04 01:43:21,514] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 176500, global step 2832576: learning rate 0.0005
[2019-04-04 01:43:21,579] A3C_AGENT_WORKER-Thread-5 INFO:Local step 176000, global step 2832609: loss 0.3741
[2019-04-04 01:43:21,581] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 176000, global step 2832609: learning rate 0.0005
[2019-04-04 01:43:22,659] A3C_AGENT_WORKER-Thread-3 INFO:Local step 177000, global step 2833170: loss 0.0601
[2019-04-04 01:43:22,665] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 177000, global step 2833173: learning rate 0.0005
[2019-04-04 01:43:23,155] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.17954267e-28 1.47759596e-20 4.30607609e-20 1.00000000e+00
 1.43279418e-23 1.01271625e-19 7.25510583e-21], sum to 1.0000
[2019-04-04 01:43:23,156] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9715
[2019-04-04 01:43:23,168] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.9413073e-27 1.6580533e-18 1.4289886e-17 1.0000000e+00 5.7886895e-21
 5.4703167e-16 2.5686199e-19], sum to 1.0000
[2019-04-04 01:43:23,178] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3942
[2019-04-04 01:43:23,197] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.916666666666667, 70.16666666666667, 0.0, 0.0, 26.0, 25.54622751686317, 0.3740660213407023, 0.0, 1.0, 26673.55633888102], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4333800.0000, 
sim time next is 4334400.0000, 
raw observation next is [3.9, 70.0, 0.0, 0.0, 26.0, 25.46648114346235, 0.3951459769558254, 0.0, 1.0, 75420.04390469164], 
processed observation next is [1.0, 0.17391304347826086, 0.5706371191135734, 0.7, 0.0, 0.0, 0.6666666666666666, 0.6222067619551958, 0.6317153256519418, 0.0, 1.0, 0.35914306621281733], 
reward next is 0.6409, 
noisyNet noise sample is [array([-1.0491995], dtype=float32), 0.9500979]. 
=============================================
[2019-04-04 01:43:23,201] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.0, 77.0, 48.0, 298.0, 26.0, 25.3905815118131, 0.3765085870977503, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3830400.0000, 
sim time next is 3831000.0000, 
raw observation next is [-4.833333333333334, 76.0, 62.33333333333334, 347.6666666666667, 26.0, 25.35224708801257, 0.3803635747961504, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.32871652816251157, 0.76, 0.2077777777777778, 0.38416206261510133, 0.6666666666666666, 0.612687257334381, 0.6267878582653835, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6064825], dtype=float32), 0.46884373]. 
=============================================
[2019-04-04 01:43:23,221] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[85.38053]
 [85.7023 ]
 [86.0712 ]
 [86.08244]
 [84.37891]], R is [[85.30848694]
 [85.45540619]
 [85.60085297]
 [85.74484253]
 [85.79798126]].
[2019-04-04 01:43:23,279] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.8695963e-27 9.8137897e-19 1.6737571e-17 1.0000000e+00 2.6665516e-21
 2.2775371e-20 1.4168772e-19], sum to 1.0000
[2019-04-04 01:43:23,280] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4265
[2019-04-04 01:43:23,296] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.833333333333333, 70.0, 0.0, 0.0, 26.0, 25.53736291220892, 0.5133656499443174, 0.0, 1.0, 58446.41139193042], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4428600.0000, 
sim time next is 4429200.0000, 
raw observation next is [2.666666666666667, 72.0, 0.0, 0.0, 26.0, 25.52352212528042, 0.5106716438271729, 0.0, 1.0, 50345.8703828853], 
processed observation next is [1.0, 0.2608695652173913, 0.5364727608494922, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6269601771067016, 0.6702238812757243, 0.0, 1.0, 0.23974223991850144], 
reward next is 0.7603, 
noisyNet noise sample is [array([-0.85256743], dtype=float32), 0.5361075]. 
=============================================
[2019-04-04 01:43:23,618] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:43:23,618] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:43:23,630] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run22
[2019-04-04 01:43:24,119] A3C_AGENT_WORKER-Thread-16 INFO:Local step 177500, global step 2833883: loss 1.6214
[2019-04-04 01:43:24,120] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 177500, global step 2833883: learning rate 0.0005
[2019-04-04 01:43:24,608] A3C_AGENT_WORKER-Thread-12 INFO:Local step 177000, global step 2834142: loss 0.0809
[2019-04-04 01:43:24,618] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 177000, global step 2834142: learning rate 0.0005
[2019-04-04 01:43:30,061] A3C_AGENT_WORKER-Thread-15 INFO:Local step 176500, global step 2836572: loss 0.0806
[2019-04-04 01:43:30,063] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 176500, global step 2836573: learning rate 0.0005
[2019-04-04 01:43:30,378] A3C_AGENT_WORKER-Thread-18 INFO:Local step 177500, global step 2836719: loss 1.2490
[2019-04-04 01:43:30,386] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 177500, global step 2836719: learning rate 0.0005
[2019-04-04 01:43:30,633] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:43:30,633] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:43:30,660] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.9214126e-26 7.5613897e-17 1.7227952e-17 1.0000000e+00 1.5894451e-18
 1.9188412e-16 8.6528128e-19], sum to 1.0000
[2019-04-04 01:43:30,662] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1154
[2019-04-04 01:43:30,672] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [8.95, 19.0, 0.0, 0.0, 25.5, 26.77447217202178, 0.7677812646218803, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.5], 
sim time this is 5087400.0000, 
sim time next is 5088000.0000, 
raw observation next is [8.9, 19.0, 0.0, 0.0, 25.5, 26.71387586427328, 0.7552990665109099, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.7091412742382273, 0.19, 0.0, 0.0, 0.625, 0.7261563220227734, 0.7517663555036367, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.3480098], dtype=float32), 0.7237287]. 
=============================================
[2019-04-04 01:43:30,682] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[82.90043 ]
 [82.72377 ]
 [82.667336]
 [82.44363 ]
 [82.26539 ]], R is [[83.2430954 ]
 [83.41066742]
 [83.57656097]
 [83.74079895]
 [83.90338898]].
[2019-04-04 01:43:30,686] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run22
[2019-04-04 01:43:31,674] A3C_AGENT_WORKER-Thread-13 INFO:Local step 176000, global step 2837290: loss 0.2300
[2019-04-04 01:43:31,675] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 176000, global step 2837290: learning rate 0.0005
[2019-04-04 01:43:32,141] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:43:32,142] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:43:32,188] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run22
[2019-04-04 01:43:34,069] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:43:34,069] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:43:34,086] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run22
[2019-04-04 01:43:34,204] A3C_AGENT_WORKER-Thread-4 INFO:Local step 175500, global step 2838321: loss 0.1929
[2019-04-04 01:43:34,208] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 175500, global step 2838321: learning rate 0.0005
[2019-04-04 01:43:34,347] A3C_AGENT_WORKER-Thread-6 INFO:Local step 177500, global step 2838378: loss 1.6634
[2019-04-04 01:43:34,348] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 177500, global step 2838378: learning rate 0.0005
[2019-04-04 01:43:34,489] A3C_AGENT_WORKER-Thread-20 INFO:Local step 178000, global step 2838428: loss 0.0232
[2019-04-04 01:43:34,502] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 178000, global step 2838432: learning rate 0.0005
[2019-04-04 01:43:35,066] A3C_AGENT_WORKER-Thread-10 INFO:Local step 177500, global step 2838641: loss 1.6182
[2019-04-04 01:43:35,067] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 177500, global step 2838641: learning rate 0.0005
[2019-04-04 01:43:37,957] A3C_AGENT_WORKER-Thread-5 INFO:Local step 176500, global step 2839689: loss 0.0362
[2019-04-04 01:43:37,961] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 176500, global step 2839689: learning rate 0.0005
[2019-04-04 01:43:38,606] A3C_AGENT_WORKER-Thread-3 INFO:Local step 177500, global step 2839937: loss 1.5144
[2019-04-04 01:43:38,608] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 177500, global step 2839939: learning rate 0.0005
[2019-04-04 01:43:39,540] A3C_AGENT_WORKER-Thread-14 INFO:Local step 177000, global step 2840369: loss 0.0986
[2019-04-04 01:43:39,543] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 177000, global step 2840370: learning rate 0.0005
[2019-04-04 01:43:41,051] A3C_AGENT_WORKER-Thread-12 INFO:Local step 177500, global step 2841146: loss 1.4370
[2019-04-04 01:43:41,051] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 177500, global step 2841146: learning rate 0.0005
[2019-04-04 01:43:41,659] A3C_AGENT_WORKER-Thread-16 INFO:Local step 178000, global step 2841463: loss 0.0954
[2019-04-04 01:43:41,675] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 178000, global step 2841463: learning rate 0.0005
[2019-04-04 01:43:42,902] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.6922181e-25 1.3747751e-16 2.8416880e-17 1.0000000e+00 6.1580896e-18
 1.2480817e-14 3.3790820e-18], sum to 1.0000
[2019-04-04 01:43:42,906] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9236
[2019-04-04 01:43:42,921] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 92.0, 161.5, 3.0, 26.0, 26.44149421909702, 0.5779708743123246, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4701600.0000, 
sim time next is 4702200.0000, 
raw observation next is [0.0, 92.0, 177.0, 4.0, 26.0, 26.42746790577748, 0.5802231209237436, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.46260387811634357, 0.92, 0.59, 0.004419889502762431, 0.6666666666666666, 0.7022889921481233, 0.6934077069745812, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.72210306], dtype=float32), 0.17654015]. 
=============================================
[2019-04-04 01:43:45,896] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.89236813e-24 1.41804335e-14 2.58908360e-14 1.00000000e+00
 3.30301318e-16 2.33724845e-11 2.15818489e-16], sum to 1.0000
[2019-04-04 01:43:45,896] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0877
[2019-04-04 01:43:45,903] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.000000000000001, 46.5, 65.66666666666666, 145.6666666666667, 26.0, 27.4569752693398, 0.8559852173056796, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4641000.0000, 
sim time next is 4641600.0000, 
raw observation next is [4.800000000000001, 47.0, 52.83333333333333, 145.3333333333333, 26.0, 27.51193324808084, 0.8216554661579587, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.5955678670360112, 0.47, 0.1761111111111111, 0.16058931860036826, 0.6666666666666666, 0.7926611040067367, 0.7738851553859862, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0127478], dtype=float32), -0.047177542]. 
=============================================
[2019-04-04 01:43:47,272] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.6062876e-29 1.7809862e-23 1.2462924e-22 1.0000000e+00 2.0688035e-23
 7.1882604e-17 3.4389606e-21], sum to 1.0000
[2019-04-04 01:43:47,272] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7567
[2019-04-04 01:43:47,284] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.26887192962126, 0.3479226899915217, 0.0, 1.0, 43379.60370336282], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4840800.0000, 
sim time next is 4841400.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.24818714776788, 0.3440244116192244, 0.0, 1.0, 40590.30281043029], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.6, 0.0, 0.0, 0.6666666666666666, 0.6040155956473233, 0.6146748038730748, 0.0, 1.0, 0.19328715624014423], 
reward next is 0.8067, 
noisyNet noise sample is [array([1.3373647], dtype=float32), 1.3916585]. 
=============================================
[2019-04-04 01:43:47,405] A3C_AGENT_WORKER-Thread-15 INFO:Local step 177000, global step 2844137: loss 0.0572
[2019-04-04 01:43:47,406] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 177000, global step 2844139: learning rate 0.0005
[2019-04-04 01:43:47,565] A3C_AGENT_WORKER-Thread-18 INFO:Local step 178000, global step 2844210: loss 0.5645
[2019-04-04 01:43:47,567] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 178000, global step 2844210: learning rate 0.0005
[2019-04-04 01:43:48,182] A3C_AGENT_WORKER-Thread-13 INFO:Local step 176500, global step 2844455: loss 0.0182
[2019-04-04 01:43:48,183] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 176500, global step 2844455: learning rate 0.0005
[2019-04-04 01:43:48,667] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:43:48,668] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:43:48,681] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run22
[2019-04-04 01:43:49,515] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.7230927e-30 5.3274144e-24 1.5610029e-21 1.0000000e+00 7.2368719e-26
 3.4254286e-25 1.6744098e-23], sum to 1.0000
[2019-04-04 01:43:49,518] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7438
[2019-04-04 01:43:49,530] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 37.0, 114.0, 732.0, 26.0, 25.18209756772873, 0.445745401075121, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4806600.0000, 
sim time next is 4807200.0000, 
raw observation next is [3.0, 37.0, 105.5, 729.5, 26.0, 25.18736620666907, 0.4444491498171452, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.5457063711911359, 0.37, 0.3516666666666667, 0.8060773480662984, 0.6666666666666666, 0.5989471838890891, 0.6481497166057151, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5578455], dtype=float32), 1.3712617]. 
=============================================
[2019-04-04 01:43:51,368] A3C_AGENT_WORKER-Thread-4 INFO:Local step 176000, global step 2845766: loss 0.4158
[2019-04-04 01:43:51,373] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 176000, global step 2845768: learning rate 0.0005
[2019-04-04 01:43:51,578] A3C_AGENT_WORKER-Thread-6 INFO:Local step 178000, global step 2845847: loss 0.0405
[2019-04-04 01:43:51,579] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 178000, global step 2845847: learning rate 0.0005
[2019-04-04 01:43:51,809] A3C_AGENT_WORKER-Thread-10 INFO:Local step 178000, global step 2845939: loss 0.0343
[2019-04-04 01:43:51,809] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 178000, global step 2845939: learning rate 0.0005
[2019-04-04 01:43:54,496] A3C_AGENT_WORKER-Thread-3 INFO:Local step 178000, global step 2847016: loss 0.0895
[2019-04-04 01:43:54,503] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 178000, global step 2847017: learning rate 0.0005
[2019-04-04 01:43:54,602] A3C_AGENT_WORKER-Thread-14 INFO:Local step 177500, global step 2847069: loss 2.9479
[2019-04-04 01:43:54,603] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 177500, global step 2847069: learning rate 0.0005
[2019-04-04 01:43:55,723] A3C_AGENT_WORKER-Thread-5 INFO:Local step 177000, global step 2847603: loss 0.0256
[2019-04-04 01:43:55,725] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 177000, global step 2847605: learning rate 0.0005
[2019-04-04 01:43:55,953] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:43:55,954] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:43:55,994] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run22
[2019-04-04 01:43:57,588] A3C_AGENT_WORKER-Thread-12 INFO:Local step 178000, global step 2848410: loss 0.1085
[2019-04-04 01:43:57,594] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 178000, global step 2848410: learning rate 0.0005
[2019-04-04 01:44:02,278] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:44:02,278] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:44:02,282] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run22
[2019-04-04 01:44:02,702] A3C_AGENT_WORKER-Thread-15 INFO:Local step 177500, global step 2850469: loss 2.4250
[2019-04-04 01:44:02,704] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 177500, global step 2850469: learning rate 0.0005
[2019-04-04 01:44:05,480] A3C_AGENT_WORKER-Thread-13 INFO:Local step 177000, global step 2851704: loss 0.0798
[2019-04-04 01:44:05,481] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 177000, global step 2851704: learning rate 0.0005
[2019-04-04 01:44:05,673] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:44:05,673] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:44:05,676] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run22
[2019-04-04 01:44:06,494] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:44:06,495] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:44:06,502] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run22
[2019-04-04 01:44:07,218] A3C_AGENT_WORKER-Thread-4 INFO:Local step 176500, global step 2852370: loss 0.0499
[2019-04-04 01:44:07,218] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 176500, global step 2852370: learning rate 0.0005
[2019-04-04 01:44:07,788] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.6121124e-25 5.4216207e-16 2.5620380e-16 1.0000000e+00 4.2409404e-17
 2.5385813e-13 8.2954436e-17], sum to 1.0000
[2019-04-04 01:44:07,788] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7744
[2019-04-04 01:44:07,828] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.666666666666667, 45.0, 109.5, 670.5, 26.0, 26.53008858310938, 0.62476560289641, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5044800.0000, 
sim time next is 5045400.0000, 
raw observation next is [2.0, 44.0, 112.0, 698.0, 26.0, 26.61357856676746, 0.6505463037715654, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.518005540166205, 0.44, 0.37333333333333335, 0.7712707182320442, 0.6666666666666666, 0.7177982138972882, 0.7168487679238552, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3043172], dtype=float32), -0.82746917]. 
=============================================
[2019-04-04 01:44:08,450] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:44:08,451] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:44:08,459] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run22
[2019-04-04 01:44:09,587] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.9268309e-26 7.6752514e-17 6.7907157e-18 1.0000000e+00 5.5588729e-18
 1.6737031e-13 5.8903411e-18], sum to 1.0000
[2019-04-04 01:44:09,587] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4188
[2019-04-04 01:44:09,683] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.3333333333333334, 94.66666666666666, 10.5, 0.0, 26.0, 25.80559649359563, 0.4845819426806181, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4693200.0000, 
sim time next is 4693800.0000, 
raw observation next is [-0.1666666666666666, 93.33333333333334, 21.0, 0.0, 26.0, 25.81263417277624, 0.4789798286805797, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.4579870729455217, 0.9333333333333335, 0.07, 0.0, 0.6666666666666666, 0.6510528477313532, 0.6596599428935266, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.0356981], dtype=float32), -1.1769252]. 
=============================================
[2019-04-04 01:44:12,152] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:44:12,153] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:44:12,156] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run22
[2019-04-04 01:44:12,278] A3C_AGENT_WORKER-Thread-14 INFO:Local step 178000, global step 2853578: loss 0.2532
[2019-04-04 01:44:12,278] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 178000, global step 2853578: learning rate 0.0005
[2019-04-04 01:44:13,869] A3C_AGENT_WORKER-Thread-5 INFO:Local step 177500, global step 2853875: loss 0.9946
[2019-04-04 01:44:13,917] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 177500, global step 2853875: learning rate 0.0005
[2019-04-04 01:44:26,241] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.7630013e-24 6.1445700e-19 1.3613364e-17 1.0000000e+00 1.6818654e-18
 4.6532888e-16 3.1469725e-18], sum to 1.0000
[2019-04-04 01:44:26,242] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0225
[2019-04-04 01:44:26,306] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.9333333333333333, 80.66666666666667, 123.1666666666667, 352.5, 24.0, 23.1068147849942, -0.1173154000519415, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 562800.0000, 
sim time next is 563400.0000, 
raw observation next is [-1.0, 80.5, 130.0, 396.0, 24.0, 23.0953625648267, -0.1202667056414565, 0.0, 1.0, 18711.3926820103], 
processed observation next is [0.0, 0.5217391304347826, 0.4349030470914128, 0.805, 0.43333333333333335, 0.4375690607734807, 0.5, 0.4246135470688917, 0.45991109811951447, 0.0, 1.0, 0.08910186991433476], 
reward next is 0.9109, 
noisyNet noise sample is [array([0.2697702], dtype=float32), -0.32210454]. 
=============================================
[2019-04-04 01:44:26,846] A3C_AGENT_WORKER-Thread-15 INFO:Local step 178000, global step 2856792: loss 0.1342
[2019-04-04 01:44:26,865] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 178000, global step 2856792: learning rate 0.0005
[2019-04-04 01:44:33,371] A3C_AGENT_WORKER-Thread-13 INFO:Local step 177500, global step 2858158: loss 0.6167
[2019-04-04 01:44:33,371] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 177500, global step 2858158: learning rate 0.0005
[2019-04-04 01:44:37,453] A3C_AGENT_WORKER-Thread-4 INFO:Local step 177000, global step 2859077: loss 0.0305
[2019-04-04 01:44:37,491] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 177000, global step 2859083: learning rate 0.0005
[2019-04-04 01:44:40,771] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:44:40,771] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:44:40,813] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run22
[2019-04-04 01:44:47,157] A3C_AGENT_WORKER-Thread-5 INFO:Local step 178000, global step 2861161: loss 0.4152
[2019-04-04 01:44:47,158] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 178000, global step 2861161: learning rate 0.0005
[2019-04-04 01:44:48,719] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.0699080e-25 5.1562376e-19 2.5750964e-18 1.0000000e+00 7.9449252e-21
 8.0134578e-17 7.1387377e-19], sum to 1.0000
[2019-04-04 01:44:48,719] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7741
[2019-04-04 01:44:48,776] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-9.5, 40.0, 0.0, 0.0, 26.0, 25.46384939307015, 0.3228904865695024, 1.0, 1.0, 46750.11787049963], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 412200.0000, 
sim time next is 412800.0000, 
raw observation next is [-9.5, 40.0, 0.0, 0.0, 26.0, 25.40153563448074, 0.3096920996243903, 1.0, 1.0, 46843.60824200758], 
processed observation next is [1.0, 0.782608695652174, 0.1994459833795014, 0.4, 0.0, 0.0, 0.6666666666666666, 0.6167946362067284, 0.6032306998747968, 1.0, 1.0, 0.22306480115241706], 
reward next is 0.7769, 
noisyNet noise sample is [array([0.93359643], dtype=float32), -0.32735592]. 
=============================================
[2019-04-04 01:44:53,255] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.6852096e-18 1.7171037e-13 6.2352276e-12 1.0000000e+00 4.0810884e-09
 3.4219089e-10 4.3976281e-12], sum to 1.0000
[2019-04-04 01:44:53,255] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6923
[2019-04-04 01:44:53,285] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.8, 86.0, 0.0, 0.0, 19.0, 18.12393092355549, -1.214291001420683, 0.0, 1.0, 22972.33916739317], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 68400.0000, 
sim time next is 69000.0000, 
raw observation next is [3.616666666666667, 86.5, 0.0, 0.0, 19.0, 18.12486053898423, -1.212126460703748, 0.0, 1.0, 25808.24657558489], 
processed observation next is [0.0, 0.8260869565217391, 0.5627885503231764, 0.865, 0.0, 0.0, 0.08333333333333333, 0.010405044915352471, 0.09595784643208398, 0.0, 1.0, 0.12289641226468995], 
reward next is 0.8771, 
noisyNet noise sample is [array([-0.15009944], dtype=float32), 0.13695918]. 
=============================================
[2019-04-04 01:44:53,292] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[82.492424]
 [82.54017 ]
 [82.6016  ]
 [82.63482 ]
 [82.66677 ]], R is [[82.46624756]
 [82.53219604]
 [82.60681152]
 [82.69026947]
 [82.77441406]].
[2019-04-04 01:44:53,449] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:44:53,449] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:44:53,452] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run22
[2019-04-04 01:45:01,673] A3C_AGENT_WORKER-Thread-13 INFO:Local step 178000, global step 2865046: loss 0.0643
[2019-04-04 01:45:01,675] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 178000, global step 2865046: learning rate 0.0005
[2019-04-04 01:45:03,592] A3C_AGENT_WORKER-Thread-4 INFO:Local step 177500, global step 2865620: loss 1.7995
[2019-04-04 01:45:03,596] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 177500, global step 2865620: learning rate 0.0005
[2019-04-04 01:45:08,157] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:45:08,157] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:45:08,214] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run22
[2019-04-04 01:45:09,290] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0836784e-21 2.7351080e-10 1.6186056e-12 1.0000000e+00 6.4065010e-11
 6.4404844e-12 5.7764424e-15], sum to 1.0000
[2019-04-04 01:45:09,290] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1508
[2019-04-04 01:45:09,314] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [13.26666666666667, 84.33333333333334, 120.5, 0.0, 24.0, 24.35579212319234, 0.1973979460033484, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 998400.0000, 
sim time next is 999000.0000, 
raw observation next is [13.55, 83.5, 119.0, 0.0, 24.0, 24.50162587667013, 0.2214836001430289, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.8379501385041552, 0.835, 0.39666666666666667, 0.0, 0.5, 0.5418021563891774, 0.5738278667143429, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0063285], dtype=float32), 0.12989819]. 
=============================================
[2019-04-04 01:45:09,341] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[94.46747 ]
 [94.32368 ]
 [94.16992 ]
 [94.06297 ]
 [93.771835]], R is [[94.6884613 ]
 [94.74157715]
 [94.79415894]
 [94.84622192]
 [94.89775848]].
[2019-04-04 01:45:15,451] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.5450627e-21 1.3039064e-13 2.6668793e-12 1.0000000e+00 1.7986033e-13
 9.1732538e-10 1.2059094e-13], sum to 1.0000
[2019-04-04 01:45:15,456] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7250
[2019-04-04 01:45:15,525] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 25.0, 23.91740720883533, 0.2245916893822107, 0.0, 1.0, 101282.357252202], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 1366800.0000, 
sim time next is 1367400.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 25.0, 23.91407302749122, 0.2406958281609783, 1.0, 1.0, 65847.30681167294], 
processed observation next is [1.0, 0.8260869565217391, 0.4764542936288089, 0.96, 0.0, 0.0, 0.5833333333333334, 0.4928394189576017, 0.5802319427203261, 1.0, 1.0, 0.31355860386510925], 
reward next is 0.6864, 
noisyNet noise sample is [array([-0.7864644], dtype=float32), 0.57303405]. 
=============================================
[2019-04-04 01:45:19,974] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:45:19,974] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:45:19,979] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run22
[2019-04-04 01:45:24,333] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.7808445e-28 2.6316705e-19 5.0752765e-17 1.0000000e+00 7.0984289e-22
 4.5985678e-16 6.6552791e-19], sum to 1.0000
[2019-04-04 01:45:24,333] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6859
[2019-04-04 01:45:24,352] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.7, 79.0, 0.0, 0.0, 26.0, 24.72182800698441, 0.2077477818026821, 0.0, 1.0, 39386.32794305947], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 871200.0000, 
sim time next is 871800.0000, 
raw observation next is [-1.7, 79.00000000000001, 0.0, 0.0, 26.0, 24.7538011967003, 0.2162565145430164, 0.0, 1.0, 39361.22658599812], 
processed observation next is [1.0, 0.08695652173913043, 0.4155124653739613, 0.7900000000000001, 0.0, 0.0, 0.6666666666666666, 0.5628167663916918, 0.5720855048476722, 0.0, 1.0, 0.18743441231427677], 
reward next is 0.8126, 
noisyNet noise sample is [array([-1.0965363], dtype=float32), -0.9125406]. 
=============================================
[2019-04-04 01:45:26,800] A3C_AGENT_WORKER-Thread-4 INFO:Local step 178000, global step 2872976: loss 0.5519
[2019-04-04 01:45:26,805] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 178000, global step 2872976: learning rate 0.0005
[2019-04-04 01:45:33,595] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.46492281e-19 1.28995805e-12 1.09853515e-10 1.00000000e+00
 3.07243647e-15 4.18188456e-10 7.25449865e-13], sum to 1.0000
[2019-04-04 01:45:33,595] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4677
[2019-04-04 01:45:33,695] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.483333333333334, 78.0, 49.0, 197.3333333333333, 24.0, 23.68317931413421, -0.1914277250196447, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 204600.0000, 
sim time next is 205200.0000, 
raw observation next is [-8.4, 78.0, 56.5, 148.0, 24.0, 23.60423108124524, -0.1947361729849882, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.2299168975069252, 0.78, 0.18833333333333332, 0.16353591160220995, 0.5, 0.46701925677043654, 0.4350879423383373, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.45617145], dtype=float32), 1.4771284]. 
=============================================
[2019-04-04 01:45:39,665] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.8832354e-17 2.2577737e-10 2.3865135e-10 9.9993312e-01 3.3996448e-11
 6.6848545e-05 7.1058638e-11], sum to 1.0000
[2019-04-04 01:45:39,665] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7937
[2019-04-04 01:45:39,707] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.100000000000001, 58.5, 0.0, 0.0, 23.0, 22.12889934031141, -0.3527211616144483, 1.0, 1.0, 53019.39163398495], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 763800.0000, 
sim time next is 764400.0000, 
raw observation next is [-5.2, 59.0, 0.0, 0.0, 23.0, 22.16867289618982, -0.3410448615928315, 0.0, 1.0, 166922.3590492966], 
processed observation next is [1.0, 0.8695652173913043, 0.31855955678670367, 0.59, 0.0, 0.0, 0.4166666666666667, 0.3473894080158182, 0.3863183794690562, 0.0, 1.0, 0.7948683764252219], 
reward next is 0.2051, 
noisyNet noise sample is [array([-0.6552406], dtype=float32), -1.0859308]. 
=============================================
[2019-04-04 01:45:43,749] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0701141e-17 4.5828102e-11 1.8690030e-10 9.9999821e-01 1.4581013e-10
 1.8153410e-06 1.1628672e-11], sum to 1.0000
[2019-04-04 01:45:43,750] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8376
[2019-04-04 01:45:43,821] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.9, 84.83333333333334, 55.66666666666666, 0.0, 23.0, 23.22220798404812, -0.2532312139292354, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 831000.0000, 
sim time next is 831600.0000, 
raw observation next is [-3.9, 86.0, 54.0, 0.0, 23.0, 23.2570380556342, -0.2530404093164182, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.3545706371191136, 0.86, 0.18, 0.0, 0.4166666666666667, 0.4380865046361834, 0.41565319689452723, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.980647], dtype=float32), 1.5440258]. 
=============================================
[2019-04-04 01:45:45,576] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.2487705e-28 1.9546417e-17 5.1442751e-18 1.0000000e+00 2.0256989e-20
 3.9043004e-17 6.2208611e-21], sum to 1.0000
[2019-04-04 01:45:45,576] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3584
[2019-04-04 01:45:45,588] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [8.8, 83.0, 0.0, 0.0, 26.0, 25.53384971884403, 0.4520334620376263, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 969600.0000, 
sim time next is 970200.0000, 
raw observation next is [8.8, 83.0, 0.0, 0.0, 26.0, 25.51132568823916, 0.4385977307006219, 0.0, 1.0, 18751.21586873674], 
processed observation next is [1.0, 0.21739130434782608, 0.7063711911357342, 0.83, 0.0, 0.0, 0.6666666666666666, 0.6259438073532634, 0.646199243566874, 0.0, 1.0, 0.08929150413684163], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.2317706], dtype=float32), 0.6930511]. 
=============================================
[2019-04-04 01:45:49,676] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:45:49,676] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:45:49,686] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run22
[2019-04-04 01:45:51,799] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.1770515e-26 1.1989865e-15 9.6902794e-20 1.0000000e+00 1.3459897e-17
 5.0332720e-14 3.2501908e-17], sum to 1.0000
[2019-04-04 01:45:51,799] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4000
[2019-04-04 01:45:51,839] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.0, 100.0, 0.0, 0.0, 26.0, 24.72122371301067, 0.2727344656891041, 0.0, 1.0, 62858.09670269697], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 936600.0000, 
sim time next is 937200.0000, 
raw observation next is [5.0, 100.0, 0.0, 0.0, 26.0, 24.64465869635087, 0.2780435235731147, 0.0, 1.0, 199213.5096351467], 
processed observation next is [1.0, 0.8695652173913043, 0.6011080332409973, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5537215580292392, 0.5926811745243715, 0.0, 1.0, 0.9486357601673653], 
reward next is 0.0514, 
noisyNet noise sample is [array([-0.71810776], dtype=float32), 1.8235245]. 
=============================================
[2019-04-04 01:45:52,159] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.1863871e-17 3.2641139e-09 4.2772005e-11 1.0000000e+00 7.0975008e-09
 2.3990211e-08 3.3332653e-12], sum to 1.0000
[2019-04-04 01:45:52,159] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6105
[2019-04-04 01:45:52,175] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.4, 93.0, 48.0, 0.0, 23.0, 22.98513282540167, -0.2388203269200115, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 920400.0000, 
sim time next is 921000.0000, 
raw observation next is [4.4, 93.0, 42.00000000000001, 0.0, 23.0, 23.12734021750313, -0.2227481933071431, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5844875346260389, 0.93, 0.14, 0.0, 0.4166666666666667, 0.42727835145859405, 0.4257506022309523, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.55253893], dtype=float32), 1.1405088]. 
=============================================
[2019-04-04 01:45:52,187] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[87.584885]
 [87.66283 ]
 [87.59036 ]
 [87.43389 ]
 [87.5081  ]], R is [[87.599823  ]
 [87.72382355]
 [87.84658813]
 [87.96812439]
 [88.08844757]].
[2019-04-04 01:46:03,149] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.01385654e-19 6.45899214e-14 6.62954283e-13 1.00000000e+00
 2.29514218e-16 3.58098100e-11 1.97811277e-13], sum to 1.0000
[2019-04-04 01:46:03,149] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6858
[2019-04-04 01:46:03,201] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.616666666666667, 66.5, 0.0, 0.0, 23.0, 22.31664746644785, -0.3394833231695602, 0.0, 1.0, 45243.28841857256], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 773400.0000, 
sim time next is 774000.0000, 
raw observation next is [-6.7, 67.0, 0.0, 0.0, 23.0, 22.31000081160557, -0.3420951911124024, 0.0, 1.0, 44993.98494205136], 
processed observation next is [1.0, 1.0, 0.2770083102493075, 0.67, 0.0, 0.0, 0.4166666666666667, 0.3591667343004641, 0.3859682696291992, 0.0, 1.0, 0.21425707115262552], 
reward next is 0.7857, 
noisyNet noise sample is [array([1.8212452], dtype=float32), -0.9972792]. 
=============================================
[2019-04-04 01:46:03,321] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[79.806755]
 [79.70096 ]
 [79.50157 ]
 [79.35725 ]
 [79.16859 ]], R is [[79.8904953 ]
 [79.87614441]
 [79.86051178]
 [79.84261322]
 [79.81896973]].
[2019-04-04 01:46:04,392] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.7472337e-21 1.3232654e-10 9.6312118e-11 1.0000000e+00 2.5937917e-12
 3.2386569e-09 1.4178919e-14], sum to 1.0000
[2019-04-04 01:46:04,392] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7312
[2019-04-04 01:46:04,433] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [12.2, 83.0, 35.0, 96.5, 23.0, 23.34876716699695, 0.007660481356336554, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 1068000.0000, 
sim time next is 1068600.0000, 
raw observation next is [12.2, 83.0, 48.0, 124.0, 23.0, 23.5499650385139, 0.02202497584308842, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.8005540166204987, 0.83, 0.16, 0.13701657458563535, 0.4166666666666667, 0.462497086542825, 0.5073416586143628, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7174033], dtype=float32), -0.08894647]. 
=============================================
[2019-04-04 01:46:11,184] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.1550055e-17 1.2788315e-09 1.3371731e-09 9.9999690e-01 3.6331466e-11
 3.1273287e-06 2.5293207e-11], sum to 1.0000
[2019-04-04 01:46:11,184] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7490
[2019-04-04 01:46:11,247] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.166666666666667, 66.33333333333334, 37.33333333333333, 0.0, 25.0, 24.60176691216343, 0.02115214148865106, 1.0, 1.0, 37292.20810122467], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 1959600.0000, 
sim time next is 1960200.0000, 
raw observation next is [-3.35, 68.5, 30.0, 0.0, 25.0, 24.34420941647584, 0.08542186165951894, 1.0, 1.0, 37375.50968095523], 
processed observation next is [1.0, 0.6956521739130435, 0.3698060941828255, 0.685, 0.1, 0.0, 0.5833333333333334, 0.5286841180396534, 0.5284739538865063, 1.0, 1.0, 0.17797861752835822], 
reward next is 0.8220, 
noisyNet noise sample is [array([-0.13689187], dtype=float32), 2.8042533]. 
=============================================
[2019-04-04 01:46:13,106] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.39228789e-21 5.32670609e-11 3.38378002e-12 9.99301791e-01
 1.02935584e-13 6.98140473e-04 2.01828138e-13], sum to 1.0000
[2019-04-04 01:46:13,107] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0206
[2019-04-04 01:46:13,157] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 92.0, 67.66666666666667, 0.0, 26.0, 25.81808974130258, 0.5207654678989352, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1435200.0000, 
sim time next is 1435800.0000, 
raw observation next is [1.1, 92.0, 63.33333333333334, 0.0, 26.0, 25.96689634703058, 0.5360132208621883, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.49307479224376743, 0.92, 0.21111111111111114, 0.0, 0.6666666666666666, 0.6639080289192151, 0.6786710736207294, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8610593], dtype=float32), -0.516519]. 
=============================================
[2019-04-04 01:46:14,585] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.63929625e-17 4.94862241e-11 1.08497225e-10 9.99982238e-01
 1.53636395e-10 1.77210331e-05 1.92460423e-10], sum to 1.0000
[2019-04-04 01:46:14,589] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2251
[2019-04-04 01:46:14,669] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.9, 64.66666666666667, 0.0, 0.0, 25.0, 23.92261923157227, -0.04175195011340885, 1.0, 1.0, 144201.3217891731], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 155400.0000, 
sim time next is 156000.0000, 
raw observation next is [-8.0, 65.33333333333334, 0.0, 0.0, 25.0, 23.85087309882073, -0.02766959509019135, 1.0, 1.0, 155248.4385732805], 
processed observation next is [1.0, 0.8260869565217391, 0.24099722991689754, 0.6533333333333334, 0.0, 0.0, 0.5833333333333334, 0.4875727582350609, 0.4907768016366029, 1.0, 1.0, 0.7392782789203833], 
reward next is 0.2607, 
noisyNet noise sample is [array([-0.29359147], dtype=float32), -0.958285]. 
=============================================
[2019-04-04 01:46:14,672] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[78.937546]
 [79.22586 ]
 [78.964325]
 [78.66759 ]
 [78.391235]], R is [[78.33947754]
 [77.86940765]
 [77.99720764]
 [78.21723938]
 [78.43506622]].
[2019-04-04 01:46:24,484] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 1.0834075e-25 4.6207791e-30 1.0000000e+00 1.8079344e-32
 3.7652051e-29 4.2931623e-30], sum to 1.0000
[2019-04-04 01:46:24,484] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3450
[2019-04-04 01:46:24,595] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [15.5, 93.0, 0.0, 0.0, 26.0, 23.24691709822005, 0.0668966809693325, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1224000.0000, 
sim time next is 1224600.0000, 
raw observation next is [15.41666666666667, 93.5, 0.0, 0.0, 26.0, 23.22146082018715, 0.06192626017975126, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.17391304347826086, 0.8896583564173594, 0.935, 0.0, 0.0, 0.6666666666666666, 0.43512173501559587, 0.5206420867265837, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.1335897], dtype=float32), 0.28289285]. 
=============================================
[2019-04-04 01:46:26,819] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.2054210e-23 1.7487802e-14 8.3536278e-15 1.0000000e+00 7.2192856e-17
 3.2450811e-13 1.0342851e-15], sum to 1.0000
[2019-04-04 01:46:26,819] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9944
[2019-04-04 01:46:26,844] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.2, 92.0, 0.0, 0.0, 26.0, 25.4093440783185, 0.5498181506677717, 0.0, 1.0, 54410.08324464162], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1310400.0000, 
sim time next is 1311000.0000, 
raw observation next is [2.1, 92.0, 0.0, 0.0, 26.0, 25.41554478692548, 0.5533236191231191, 0.0, 1.0, 42843.59169972379], 
processed observation next is [1.0, 0.17391304347826086, 0.5207756232686982, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6179620655771233, 0.6844412063743731, 0.0, 1.0, 0.20401710333201803], 
reward next is 0.7960, 
noisyNet noise sample is [array([-0.04799864], dtype=float32), -0.6475449]. 
=============================================
[2019-04-04 01:46:26,879] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[88.2447  ]
 [88.4291  ]
 [88.61903 ]
 [88.594315]
 [88.7106  ]], R is [[88.03157043]
 [87.89215851]
 [87.81791687]
 [87.74532318]
 [87.51172638]].
[2019-04-04 01:46:43,557] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.9182214e-28 5.8438627e-23 1.5101974e-20 1.0000000e+00 2.1351188e-22
 1.3881903e-19 1.9568108e-19], sum to 1.0000
[2019-04-04 01:46:43,557] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0124
[2019-04-04 01:46:43,790] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.4, 77.66666666666667, 12.66666666666667, 0.0, 26.0, 23.39978583923563, -0.09316784360177412, 0.0, 1.0, 44008.49498306862], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 634200.0000, 
sim time next is 634800.0000, 
raw observation next is [-4.300000000000001, 76.33333333333334, 15.83333333333333, 0.0, 26.0, 23.37819705559059, -0.02003747197887701, 0.0, 1.0, 202372.6258221027], 
processed observation next is [0.0, 0.34782608695652173, 0.34349030470914127, 0.7633333333333334, 0.05277777777777777, 0.0, 0.6666666666666666, 0.4481830879658825, 0.4933208426737077, 0.0, 1.0, 0.9636791705814415], 
reward next is 0.0363, 
noisyNet noise sample is [array([0.9595779], dtype=float32), -0.2638695]. 
=============================================
[2019-04-04 01:46:45,736] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.1215552e-21 2.3451378e-12 1.4484803e-13 1.0000000e+00 2.8593352e-14
 8.6388025e-10 1.1786662e-14], sum to 1.0000
[2019-04-04 01:46:45,736] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4005
[2019-04-04 01:46:45,831] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.08333333333333333, 94.5, 94.0, 0.0, 25.0, 24.90828370896837, 0.2762076529677003, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 1426200.0000, 
sim time next is 1426800.0000, 
raw observation next is [0.1666666666666667, 94.0, 95.0, 0.0, 25.0, 24.92265035400496, 0.2676913380638216, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.4672206832871654, 0.94, 0.31666666666666665, 0.0, 0.5833333333333334, 0.5768875295004134, 0.5892304460212738, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.49858257], dtype=float32), -1.5059164]. 
=============================================
[2019-04-04 01:46:51,772] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.80499072e-25 1.51781909e-15 1.07226965e-13 1.00000000e+00
 6.86992755e-17 3.27687638e-13 2.61829514e-17], sum to 1.0000
[2019-04-04 01:46:51,772] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6095
[2019-04-04 01:46:51,794] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.1, 100.0, 55.66666666666666, 0.0, 26.0, 26.09641170411997, 0.5321125103467512, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1504200.0000, 
sim time next is 1504800.0000, 
raw observation next is [2.2, 100.0, 60.0, 0.0, 26.0, 26.13122187682271, 0.5234728375172332, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.5235457063711911, 1.0, 0.2, 0.0, 0.6666666666666666, 0.6776018230685592, 0.6744909458390778, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7669678], dtype=float32), 0.11441765]. 
=============================================
[2019-04-04 01:46:54,489] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.5829493e-24 1.1088401e-09 1.3497941e-14 1.0000000e+00 1.1226122e-14
 6.1410632e-09 2.0959359e-18], sum to 1.0000
[2019-04-04 01:46:54,490] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9523
[2019-04-04 01:46:54,532] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [14.8, 57.5, 0.0, 0.0, 24.0, 24.84536337096816, 0.3487386953952815, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1105800.0000, 
sim time next is 1106400.0000, 
raw observation next is [14.6, 58.00000000000001, 0.0, 0.0, 24.0, 24.70500143335276, 0.3428040262984594, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.8670360110803325, 0.5800000000000001, 0.0, 0.0, 0.5, 0.5587501194460632, 0.6142680087661532, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.12955126], dtype=float32), -0.7699896]. 
=============================================
[2019-04-04 01:47:15,507] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0733518e-21 2.7959674e-13 2.3966443e-14 9.9999988e-01 2.5054826e-12
 1.0192139e-07 7.8223327e-17], sum to 1.0000
[2019-04-04 01:47:15,508] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3247
[2019-04-04 01:47:15,536] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.366666666666667, 92.0, 63.83333333333333, 0.0, 25.0, 24.96398832104996, 0.3066991677570761, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 1678800.0000, 
sim time next is 1679400.0000, 
raw observation next is [1.3, 92.0, 66.0, 0.0, 25.0, 24.96095877943734, 0.2934251721310442, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.49861495844875353, 0.92, 0.22, 0.0, 0.5833333333333334, 0.5800798982864451, 0.5978083907103481, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0575994], dtype=float32), -0.18982264]. 
=============================================
[2019-04-04 01:47:17,177] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.2313729e-24 3.6162323e-19 5.3500014e-16 1.0000000e+00 3.0327207e-20
 6.0674988e-18 2.1465903e-17], sum to 1.0000
[2019-04-04 01:47:17,177] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6525
[2019-04-04 01:47:17,200] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.1, 83.0, 0.0, 0.0, 25.0, 24.1267976457596, 0.06331530559184241, 0.0, 1.0, 43459.97016840813], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 599400.0000, 
sim time next is 600000.0000, 
raw observation next is [-3.2, 83.0, 0.0, 0.0, 25.0, 24.10483963552659, 0.06125023239838878, 0.0, 1.0, 43383.44575224492], 
processed observation next is [0.0, 0.9565217391304348, 0.37396121883656513, 0.83, 0.0, 0.0, 0.5833333333333334, 0.5087366362938827, 0.5204167441327963, 0.0, 1.0, 0.20658783691545202], 
reward next is 0.7934, 
noisyNet noise sample is [array([-1.4846352], dtype=float32), 1.5116758]. 
=============================================
[2019-04-04 01:47:17,275] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[78.29864]
 [78.40384]
 [78.48565]
 [78.47085]
 [78.42792]], R is [[78.24695587]
 [78.25753021]
 [78.26764679]
 [78.27740479]
 [78.2868576 ]].
[2019-04-04 01:47:17,591] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.2595200e-25 1.4857437e-16 4.5039867e-15 1.0000000e+00 9.5761420e-20
 2.4908984e-15 1.4192704e-16], sum to 1.0000
[2019-04-04 01:47:17,591] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5736
[2019-04-04 01:47:17,635] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.01310683646407, 0.3985100714304783, 0.0, 1.0, 38624.42898909673], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1407000.0000, 
sim time next is 1407600.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.02210632178261, 0.4057744239568999, 0.0, 1.0, 38642.37659376024], 
processed observation next is [1.0, 0.30434782608695654, 0.44598337950138506, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5851755268152173, 0.6352581413189666, 0.0, 1.0, 0.18401131711314397], 
reward next is 0.8160, 
noisyNet noise sample is [array([-0.94859076], dtype=float32), 1.0582917]. 
=============================================
[2019-04-04 01:47:21,826] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-04 01:47:21,838] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 01:47:21,839] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:47:21,847] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run30
[2019-04-04 01:47:21,891] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 01:47:21,892] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:47:21,894] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run30
[2019-04-04 01:47:21,923] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 01:47:21,924] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:47:21,926] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run30
[2019-04-04 01:47:46,397] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.0786794], dtype=float32), 0.79171985]
[2019-04-04 01:47:46,398] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-13.96734358, 94.84062613, 0.0, 634.6325157, 21.0, 20.81743056699472, -0.827533236471602, 1.0, 1.0, 0.0]
[2019-04-04 01:47:46,398] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 01:47:46,400] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.3648664e-15 2.2232886e-13 4.9709698e-11 1.0000000e+00 3.0037837e-11
 1.1299685e-09 9.4061134e-11], sampled 0.8044572657244005
[2019-04-04 01:48:35,036] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.0786794], dtype=float32), 0.79171985]
[2019-04-04 01:48:35,036] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-7.980496535666667, 65.91727811333334, 0.0, 0.0, 21.0, 20.74487205990769, -0.7244815183223464, 0.0, 1.0, 0.0]
[2019-04-04 01:48:35,036] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 01:48:35,037] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [6.2652956e-15 6.5721696e-13 2.0845103e-10 1.0000000e+00 2.6224065e-11
 1.1535789e-09 4.5255433e-10], sampled 0.5328955658445718
[2019-04-04 01:49:31,265] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6418.2516 168261256.7960 -1280.4566
[2019-04-04 01:49:43,993] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5868.5149 194341075.0005 -1743.9347
[2019-04-04 01:49:49,875] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5800.5529 209637679.8746 -1656.0129
[2019-04-04 01:49:50,899] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 2900000, evaluation results [2900000.0, 5868.514860474169, 194341075.000452, -1743.9346995002295, 6418.251572757656, 168261256.79601592, -1280.456582474896, 5800.552850694566, 209637679.87456894, -1656.0129005925726]
[2019-04-04 01:49:55,408] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.2120365e-25 9.0366157e-18 2.2107283e-18 1.0000000e+00 6.5700907e-20
 4.3303159e-16 4.7116595e-17], sum to 1.0000
[2019-04-04 01:49:55,409] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0386
[2019-04-04 01:49:55,443] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.616666666666667, 77.5, 0.0, 0.0, 26.0, 24.50984040071291, 0.2092546388439444, 0.0, 1.0, 44163.1463636696], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2245800.0000, 
sim time next is 2246400.0000, 
raw observation next is [-6.7, 78.0, 0.0, 0.0, 26.0, 24.46015385659563, 0.1990501769843839, 0.0, 1.0, 44229.33255330897], 
processed observation next is [1.0, 0.0, 0.2770083102493075, 0.78, 0.0, 0.0, 0.6666666666666666, 0.5383461547163023, 0.5663500589947946, 0.0, 1.0, 0.21061586930147128], 
reward next is 0.7894, 
noisyNet noise sample is [array([0.72924876], dtype=float32), -0.52024555]. 
=============================================
[2019-04-04 01:49:56,432] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.7646839e-29 8.3133462e-16 7.6129212e-21 1.0000000e+00 2.6032016e-21
 6.8054750e-17 1.2675775e-20], sum to 1.0000
[2019-04-04 01:49:56,433] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1651
[2019-04-04 01:49:56,451] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [18.71666666666667, 63.33333333333333, 46.0, 0.0, 24.0, 23.62390131620393, 0.135523769394373, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1181400.0000, 
sim time next is 1182000.0000, 
raw observation next is [18.63333333333333, 63.66666666666667, 37.5, 0.0, 24.0, 23.61081317576148, 0.132027444094543, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.9787626962142197, 0.6366666666666667, 0.125, 0.0, 0.5, 0.4675677646467899, 0.5440091480315143, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.39535812], dtype=float32), -0.85063875]. 
=============================================
[2019-04-04 01:49:56,477] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[88.76072 ]
 [88.82589 ]
 [88.849945]
 [88.89043 ]
 [88.938095]], R is [[88.77521515]
 [88.88746643]
 [88.99859619]
 [89.10861206]
 [89.2175293 ]].
[2019-04-04 01:50:00,134] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.1251878e-23 6.6396943e-16 6.0155064e-18 1.0000000e+00 4.7080349e-15
 2.7249160e-15 1.4649877e-16], sum to 1.0000
[2019-04-04 01:50:00,135] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4712
[2019-04-04 01:50:00,211] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.7333333333333335, 43.33333333333334, 135.1666666666667, 45.0, 26.0, 26.0364642339397, 0.4700970679101999, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2301600.0000, 
sim time next is 2302200.0000, 
raw observation next is [0.55, 43.5, 138.0, 42.0, 26.0, 25.37302229500731, 0.4131565800988786, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.4778393351800555, 0.435, 0.46, 0.04640883977900553, 0.6666666666666666, 0.6144185245839425, 0.6377188600329595, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.06689466], dtype=float32), -0.267823]. 
=============================================
[2019-04-04 01:50:01,158] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.5654969e-24 3.1573529e-16 6.6638252e-16 1.0000000e+00 1.1954652e-15
 1.3244382e-12 2.2103247e-17], sum to 1.0000
[2019-04-04 01:50:01,159] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7547
[2019-04-04 01:50:01,203] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.266666666666667, 76.33333333333334, 9.166666666666666, 1.666666666666667, 26.0, 25.62238700460529, 0.3501945643243299, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1963200.0000, 
sim time next is 1963800.0000, 
raw observation next is [-4.45, 77.0, 0.0, 0.0, 26.0, 25.70733190290515, 0.3211588057728923, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.3393351800554017, 0.77, 0.0, 0.0, 0.6666666666666666, 0.6422776585754292, 0.6070529352576307, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.49042434], dtype=float32), -0.26623183]. 
=============================================
[2019-04-04 01:50:04,205] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.5034656e-21 1.9587556e-16 1.2848198e-14 1.0000000e+00 1.0630528e-17
 1.4231414e-12 1.0732206e-14], sum to 1.0000
[2019-04-04 01:50:04,238] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9863
[2019-04-04 01:50:04,256] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-12.26666666666667, 80.66666666666666, 0.0, 0.0, 25.0, 23.41218248536444, -0.0644827361735117, 0.0, 1.0, 44984.34457179842], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 2688000.0000, 
sim time next is 2688600.0000, 
raw observation next is [-12.58333333333333, 81.83333333333334, 0.0, 0.0, 25.0, 23.31952504209235, -0.07691171785852421, 0.0, 1.0, 45006.40115883028], 
processed observation next is [1.0, 0.08695652173913043, 0.11403508771929832, 0.8183333333333335, 0.0, 0.0, 0.5833333333333334, 0.44329375350769595, 0.47436276071382527, 0.0, 1.0, 0.21431619599442991], 
reward next is 0.7857, 
noisyNet noise sample is [array([-0.55908245], dtype=float32), 1.2935296]. 
=============================================
[2019-04-04 01:50:09,818] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.1762829e-21 8.2149653e-14 1.7892443e-14 1.0000000e+00 1.5676907e-12
 2.5799137e-11 1.2344072e-14], sum to 1.0000
[2019-04-04 01:50:09,818] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3070
[2019-04-04 01:50:09,869] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.166666666666667, 66.33333333333334, 37.33333333333333, 0.0, 25.0, 24.6024602304992, 0.02129538736742517, 1.0, 1.0, 37291.04880812518], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 1959600.0000, 
sim time next is 1960200.0000, 
raw observation next is [-3.35, 68.5, 30.0, 0.0, 25.0, 24.34487098302128, 0.08556667594561111, 1.0, 1.0, 37374.64031299305], 
processed observation next is [1.0, 0.6956521739130435, 0.3698060941828255, 0.685, 0.1, 0.0, 0.5833333333333334, 0.5287392485851067, 0.5285222253152037, 1.0, 1.0, 0.17797447768091929], 
reward next is 0.8220, 
noisyNet noise sample is [array([0.57075775], dtype=float32), 0.21573871]. 
=============================================
[2019-04-04 01:50:17,356] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.3308715e-20 2.0695678e-13 1.1461810e-13 1.0000000e+00 7.4767482e-13
 7.1492654e-13 2.7337046e-14], sum to 1.0000
[2019-04-04 01:50:17,356] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0225
[2019-04-04 01:50:17,421] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.9, 82.0, 38.5, 0.0, 25.0, 24.92184838466297, 0.06770916480687711, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 2046000.0000, 
sim time next is 2046600.0000, 
raw observation next is [-3.9, 82.0, 32.0, 0.0, 25.0, 24.35551321678345, 0.09423506436434659, 1.0, 1.0, 72482.63942391188], 
processed observation next is [1.0, 0.6956521739130435, 0.3545706371191136, 0.82, 0.10666666666666667, 0.0, 0.5833333333333334, 0.5296261013986209, 0.5314116881214489, 1.0, 1.0, 0.34515542582815184], 
reward next is 0.6548, 
noisyNet noise sample is [array([-0.5380903], dtype=float32), 1.1094786]. 
=============================================
[2019-04-04 01:50:19,998] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.3271505e-24 3.0783710e-16 1.2029304e-16 1.0000000e+00 5.9316243e-19
 1.8827910e-15 3.8071397e-18], sum to 1.0000
[2019-04-04 01:50:20,002] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9079
[2019-04-04 01:50:20,050] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.3166666666666667, 35.16666666666667, 0.0, 0.0, 26.0, 25.16286894030107, 0.3555454474687125, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2571000.0000, 
sim time next is 2571600.0000, 
raw observation next is [0.1333333333333334, 35.33333333333334, 0.0, 0.0, 26.0, 25.22448428545307, 0.3480359984308336, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.46629732225300097, 0.35333333333333344, 0.0, 0.0, 0.6666666666666666, 0.6020403571210892, 0.6160119994769445, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8364424], dtype=float32), 0.739736]. 
=============================================
[2019-04-04 01:50:24,996] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.7135326e-23 1.6237452e-19 4.3427559e-16 1.0000000e+00 3.1395504e-18
 1.9311603e-19 4.9833810e-17], sum to 1.0000
[2019-04-04 01:50:24,997] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5166
[2019-04-04 01:50:25,037] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.600000000000001, 85.5, 0.0, 0.0, 24.0, 23.34852060465749, -0.1509565142423319, 0.0, 1.0, 46959.08307095129], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1887000.0000, 
sim time next is 1887600.0000, 
raw observation next is [-5.6, 85.0, 0.0, 0.0, 24.0, 23.34279565371643, -0.1477312816336327, 0.0, 1.0, 46381.7015327495], 
processed observation next is [0.0, 0.8695652173913043, 0.30747922437673136, 0.85, 0.0, 0.0, 0.5, 0.4452329711430358, 0.4507562394554558, 0.0, 1.0, 0.22086524539404526], 
reward next is 0.7791, 
noisyNet noise sample is [array([-1.3861847], dtype=float32), -0.3319668]. 
=============================================
[2019-04-04 01:50:30,489] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.7849047e-22 4.6633314e-15 1.9064168e-15 1.0000000e+00 9.0092660e-15
 4.0641285e-14 1.8476700e-16], sum to 1.0000
[2019-04-04 01:50:30,489] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3485
[2019-04-04 01:50:30,510] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 44.0, 89.5, 21.0, 26.0, 25.85919710498329, 0.4170235597732633, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2304000.0000, 
sim time next is 2304600.0000, 
raw observation next is [-0.09999999999999999, 44.83333333333334, 73.33333333333333, 14.0, 26.0, 25.88182138256206, 0.4143796342634612, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.4598337950138504, 0.4483333333333334, 0.24444444444444444, 0.015469613259668509, 0.6666666666666666, 0.6568184485468382, 0.638126544754487, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.9596027], dtype=float32), 0.13528906]. 
=============================================
[2019-04-04 01:50:35,966] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.6767088e-21 7.0763187e-13 2.2187135e-13 1.0000000e+00 7.7115912e-15
 3.2267320e-09 4.6560569e-13], sum to 1.0000
[2019-04-04 01:50:35,966] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9670
[2019-04-04 01:50:36,025] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 25.0, 24.02599687973484, 0.2560427138334519, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 1368600.0000, 
sim time next is 1369200.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 25.0, 24.0594907977673, 0.2673745134725273, 0.0, 1.0, 199158.6561503943], 
processed observation next is [1.0, 0.8695652173913043, 0.4764542936288089, 0.96, 0.0, 0.0, 0.5833333333333334, 0.5049575664806083, 0.5891248378241758, 0.0, 1.0, 0.9483745530971157], 
reward next is 0.0516, 
noisyNet noise sample is [array([0.43977195], dtype=float32), 1.2151773]. 
=============================================
[2019-04-04 01:50:41,317] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.8647070e-25 1.5348415e-18 6.9915952e-17 1.0000000e+00 1.1585845e-20
 5.1522960e-15 2.7681502e-18], sum to 1.0000
[2019-04-04 01:50:41,320] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7723
[2019-04-04 01:50:41,334] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.3, 63.0, 0.0, 0.0, 25.0, 24.20653941570971, 0.1008621287567449, 0.0, 1.0, 39264.73904356328], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 2335200.0000, 
sim time next is 2335800.0000, 
raw observation next is [-2.3, 62.5, 0.0, 0.0, 25.0, 24.17600580278707, 0.09463809662787907, 0.0, 1.0, 39261.30939707235], 
processed observation next is [0.0, 0.0, 0.3988919667590028, 0.625, 0.0, 0.0, 0.5833333333333334, 0.5146671502322558, 0.531546032209293, 0.0, 1.0, 0.186958616176535], 
reward next is 0.8130, 
noisyNet noise sample is [array([0.5064628], dtype=float32), 0.27798513]. 
=============================================
[2019-04-04 01:50:46,011] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.2521987e-26 2.5858023e-21 2.2039152e-19 1.0000000e+00 6.7970928e-25
 8.2231944e-18 2.5923827e-19], sum to 1.0000
[2019-04-04 01:50:46,017] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1387
[2019-04-04 01:50:46,039] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.0, 81.50000000000001, 0.0, 0.0, 26.0, 24.68501803452208, 0.2254913764201233, 0.0, 1.0, 45534.35625686232], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1811400.0000, 
sim time next is 1812000.0000, 
raw observation next is [-5.0, 81.0, 0.0, 0.0, 26.0, 24.65321540049093, 0.218811630492749, 0.0, 1.0, 45499.1505252065], 
processed observation next is [0.0, 1.0, 0.32409972299168976, 0.81, 0.0, 0.0, 0.6666666666666666, 0.5544346167075774, 0.5729372101642497, 0.0, 1.0, 0.21666262154860239], 
reward next is 0.7833, 
noisyNet noise sample is [array([-0.36617583], dtype=float32), 1.3205596]. 
=============================================
[2019-04-04 01:50:46,043] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[81.46851]
 [81.44572]
 [81.43215]
 [81.41394]
 [81.36375]], R is [[81.47745514]
 [81.44585419]
 [81.4143219 ]
 [81.38283539]
 [81.35139465]].
[2019-04-04 01:51:00,277] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3833912e-25 4.3646743e-18 1.4286288e-17 1.0000000e+00 2.4235578e-19
 2.6674245e-15 3.8849547e-18], sum to 1.0000
[2019-04-04 01:51:00,281] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4690
[2019-04-04 01:51:00,339] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.8, 56.0, 93.5, 25.5, 26.0, 25.59855633290804, 0.2685648240474927, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2538000.0000, 
sim time next is 2538600.0000, 
raw observation next is [-2.533333333333334, 54.83333333333334, 107.6666666666667, 28.0, 26.0, 25.64312816729587, 0.2739149487561248, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.39242843951985223, 0.5483333333333335, 0.358888888888889, 0.030939226519337018, 0.6666666666666666, 0.6369273472746558, 0.5913049829187083, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.790203], dtype=float32), -2.068052]. 
=============================================
[2019-04-04 01:51:21,576] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.2452521e-17 7.5479692e-11 2.7667349e-11 1.0000000e+00 3.1850894e-10
 3.6437098e-09 1.5162134e-11], sum to 1.0000
[2019-04-04 01:51:21,577] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7202
[2019-04-04 01:51:21,598] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.5, 50.0, 88.33333333333333, 137.6666666666667, 24.0, 23.94694347643815, 0.03089066449446594, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2650800.0000, 
sim time next is 2651400.0000, 
raw observation next is [0.5, 50.0, 75.0, 124.0, 24.0, 24.20629446047959, 0.05281800840933376, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.4764542936288089, 0.5, 0.25, 0.13701657458563535, 0.5, 0.5171912050399659, 0.5176060028031112, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7740796], dtype=float32), -0.26715678]. 
=============================================
[2019-04-04 01:51:36,220] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.5557247e-29 6.2121246e-24 1.1251726e-20 1.0000000e+00 4.9203479e-24
 4.4618170e-22 1.8998337e-21], sum to 1.0000
[2019-04-04 01:51:36,222] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6866
[2019-04-04 01:51:36,266] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.5, 80.33333333333334, 0.0, 0.0, 26.0, 25.00572448105483, 0.2829227993249031, 0.0, 1.0, 54006.30858623674], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3088200.0000, 
sim time next is 3088800.0000, 
raw observation next is [-0.6, 82.0, 0.0, 0.0, 26.0, 24.9913359514203, 0.2837033095454464, 0.0, 1.0, 50908.00733189168], 
processed observation next is [0.0, 0.782608695652174, 0.44598337950138506, 0.82, 0.0, 0.0, 0.6666666666666666, 0.582611329285025, 0.5945677698484821, 0.0, 1.0, 0.24241908253281752], 
reward next is 0.7576, 
noisyNet noise sample is [array([-0.05599967], dtype=float32), -0.9752289]. 
=============================================
[2019-04-04 01:51:44,694] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5946961e-21 7.7934206e-15 2.3088126e-14 1.0000000e+00 3.6732693e-16
 2.2859437e-14 9.0029446e-15], sum to 1.0000
[2019-04-04 01:51:44,694] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9968
[2019-04-04 01:51:44,711] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.0, 40.5, 0.0, 0.0, 24.0, 23.36559669619447, -0.1077165980518145, 0.0, 1.0, 49884.24983915404], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4074600.0000, 
sim time next is 4075200.0000, 
raw observation next is [-5.0, 41.0, 0.0, 0.0, 24.0, 23.39084007350703, -0.1063715729220116, 0.0, 1.0, 34675.88011823085], 
processed observation next is [1.0, 0.17391304347826086, 0.32409972299168976, 0.41, 0.0, 0.0, 0.5, 0.4492366727922524, 0.46454280902599615, 0.0, 1.0, 0.16512323865824216], 
reward next is 0.8349, 
noisyNet noise sample is [array([0.8903995], dtype=float32), 0.13141485]. 
=============================================
[2019-04-04 01:51:45,570] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.1172682e-28 5.9919624e-18 6.8584547e-18 1.0000000e+00 2.1093317e-17
 1.0800601e-19 1.1447421e-19], sum to 1.0000
[2019-04-04 01:51:45,573] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3278
[2019-04-04 01:51:45,584] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.733333333333333, 99.33333333333334, 79.66666666666666, 649.0, 26.0, 27.14135211320076, 0.9365140612650942, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3166800.0000, 
sim time next is 3167400.0000, 
raw observation next is [6.666666666666666, 99.16666666666666, 75.33333333333334, 619.0, 26.0, 27.37123219830424, 0.9629200242951406, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.6472760849492153, 0.9916666666666666, 0.2511111111111111, 0.6839779005524862, 0.6666666666666666, 0.7809360165253535, 0.8209733414317135, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1809725], dtype=float32), -1.1573036]. 
=============================================
[2019-04-04 01:51:46,386] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.8093498e-27 1.1883641e-21 2.4350396e-19 1.0000000e+00 3.7280041e-21
 3.4466416e-22 1.0585352e-20], sum to 1.0000
[2019-04-04 01:51:46,388] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2065
[2019-04-04 01:51:46,452] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 53.0, 146.0, 92.0, 25.0, 24.38412321968796, 0.1301137296811534, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 4266000.0000, 
sim time next is 4266600.0000, 
raw observation next is [3.166666666666667, 53.16666666666667, 158.0, 105.0, 25.0, 24.45776706558305, 0.156758903028647, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.5503231763619576, 0.5316666666666667, 0.5266666666666666, 0.11602209944751381, 0.5833333333333334, 0.5381472554652541, 0.5522529676762157, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.7284402], dtype=float32), 0.35372272]. 
=============================================
[2019-04-04 01:51:48,080] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.2629671e-29 3.4022553e-21 1.5893700e-21 1.0000000e+00 2.3807047e-24
 5.1750876e-22 7.7851918e-22], sum to 1.0000
[2019-04-04 01:51:48,080] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1629
[2019-04-04 01:51:48,106] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.5, 27.5, 114.0, 830.0, 26.0, 26.1269480983131, 0.57598767523652, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4023000.0000, 
sim time next is 4023600.0000, 
raw observation next is [-3.333333333333333, 27.0, 112.3333333333333, 824.0, 26.0, 26.18373003967509, 0.48678275151846, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.37026777469990774, 0.27, 0.37444444444444436, 0.9104972375690608, 0.6666666666666666, 0.6819775033062575, 0.66226091717282, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.04410351], dtype=float32), -0.14165343]. 
=============================================
[2019-04-04 01:51:49,114] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4906256e-24 2.7267211e-18 3.7907061e-18 1.0000000e+00 8.6470295e-21
 1.0920789e-19 6.0105947e-19], sum to 1.0000
[2019-04-04 01:51:49,123] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4178
[2019-04-04 01:51:49,152] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.333333333333333, 27.66666666666667, 0.0, 0.0, 26.0, 25.70640548724652, 0.49751575502474, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4040400.0000, 
sim time next is 4041000.0000, 
raw observation next is [-3.5, 28.5, 0.0, 0.0, 26.0, 25.62206356244321, 0.4876733620851264, 1.0, 1.0, 91837.88511475825], 
processed observation next is [1.0, 0.782608695652174, 0.36565096952908593, 0.285, 0.0, 0.0, 0.6666666666666666, 0.6351719635369341, 0.6625577873617088, 1.0, 1.0, 0.43732326245122977], 
reward next is 0.5627, 
noisyNet noise sample is [array([-0.09004453], dtype=float32), 0.23697777]. 
=============================================
[2019-04-04 01:51:49,159] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[78.706345]
 [77.97919 ]
 [77.41018 ]
 [78.403336]
 [78.36169 ]], R is [[78.75437927]
 [78.96683502]
 [78.50076294]
 [78.71575928]
 [78.92860413]].
[2019-04-04 01:52:08,867] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.7362502e-27 1.6866679e-22 8.2408974e-20 1.0000000e+00 2.7160824e-24
 2.4891918e-20 1.4863586e-20], sum to 1.0000
[2019-04-04 01:52:08,868] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8918
[2019-04-04 01:52:08,877] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 43.0, 0.0, 0.0, 26.0, 25.48051188177284, 0.4161210903038187, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4219800.0000, 
sim time next is 4220400.0000, 
raw observation next is [1.0, 43.0, 0.0, 0.0, 26.0, 25.56279697104168, 0.413571499357333, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.4903047091412743, 0.43, 0.0, 0.0, 0.6666666666666666, 0.6302330809201401, 0.6378571664524443, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.46438786], dtype=float32), 0.74601376]. 
=============================================
[2019-04-04 01:52:15,508] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.5342897e-24 1.4354369e-15 1.0448535e-15 1.0000000e+00 1.2066254e-16
 1.2776523e-14 1.9163686e-16], sum to 1.0000
[2019-04-04 01:52:15,509] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2320
[2019-04-04 01:52:15,524] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 71.0, 0.0, 0.0, 26.0, 25.28541885807778, 0.3955125053056083, 0.0, 1.0, 40847.73372863406], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4515000.0000, 
sim time next is 4515600.0000, 
raw observation next is [-1.0, 71.0, 0.0, 0.0, 26.0, 25.26711412785518, 0.3881661642501461, 0.0, 1.0, 40781.56077541743], 
processed observation next is [1.0, 0.2608695652173913, 0.4349030470914128, 0.71, 0.0, 0.0, 0.6666666666666666, 0.6055928439879317, 0.6293887214167154, 0.0, 1.0, 0.1941979084543687], 
reward next is 0.8058, 
noisyNet noise sample is [array([0.5453544], dtype=float32), -0.20649679]. 
=============================================
[2019-04-04 01:52:16,186] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.3276806e-21 3.8363925e-14 5.9177848e-14 1.0000000e+00 1.4367222e-14
 3.9101789e-14 1.6947949e-14], sum to 1.0000
[2019-04-04 01:52:16,187] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8704
[2019-04-04 01:52:16,201] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 62.0, 0.0, 0.0, 25.0, 24.68366251132993, 0.2827163350909336, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 4674600.0000, 
sim time next is 4675200.0000, 
raw observation next is [2.0, 62.0, 0.0, 0.0, 25.0, 24.69744771395903, 0.2730810372873529, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.518005540166205, 0.62, 0.0, 0.0, 0.5833333333333334, 0.5581206428299191, 0.5910270124291176, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6656398], dtype=float32), -0.03799341]. 
=============================================
[2019-04-04 01:52:27,666] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.6882087e-23 4.1256061e-16 1.4866743e-15 1.0000000e+00 5.0937438e-16
 2.5409937e-13 9.7816254e-16], sum to 1.0000
[2019-04-04 01:52:27,667] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2596
[2019-04-04 01:52:27,685] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.333333333333334, 74.0, 0.0, 0.0, 26.0, 25.21237116950056, 0.3529811953658646, 0.0, 1.0, 41021.13369410952], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3906600.0000, 
sim time next is 3907200.0000, 
raw observation next is [-4.666666666666667, 71.0, 0.0, 0.0, 26.0, 25.21458663043011, 0.3416076709659886, 0.0, 1.0, 41100.83018131205], 
processed observation next is [1.0, 0.21739130434782608, 0.3333333333333333, 0.71, 0.0, 0.0, 0.6666666666666666, 0.6012155525358424, 0.6138692236553295, 0.0, 1.0, 0.1957182389586288], 
reward next is 0.8043, 
noisyNet noise sample is [array([1.0427066], dtype=float32), -0.94838625]. 
=============================================
[2019-04-04 01:52:30,974] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:52:30,974] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:52:30,990] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run23
[2019-04-04 01:52:32,107] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.11950339e-24 3.42881551e-17 1.49464019e-16 1.00000000e+00
 2.19551545e-19 1.26693115e-14 8.21814990e-17], sum to 1.0000
[2019-04-04 01:52:32,108] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4803
[2019-04-04 01:52:32,132] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.24855475651254, 0.3021900443591986, 0.0, 1.0, 43220.44027479935], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3733200.0000, 
sim time next is 3733800.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.1916799506516, 0.2939665555191229, 0.0, 1.0, 41994.25759131282], 
processed observation next is [1.0, 0.21739130434782608, 0.3795013850415513, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5993066625543001, 0.5979888518397076, 0.0, 1.0, 0.1999726551967277], 
reward next is 0.8000, 
noisyNet noise sample is [array([1.0961232], dtype=float32), 0.57295775]. 
=============================================
[2019-04-04 01:52:33,491] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.8210339e-26 3.0861281e-18 2.0194063e-17 1.0000000e+00 7.1072109e-21
 2.9121396e-18 1.0282645e-18], sum to 1.0000
[2019-04-04 01:52:33,492] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7317
[2019-04-04 01:52:33,511] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 26.0, 25.31559033904994, 0.4710819922354983, 0.0, 1.0, 45632.8703525081], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3279600.0000, 
sim time next is 3280200.0000, 
raw observation next is [-6.166666666666666, 90.66666666666667, 0.0, 0.0, 26.0, 25.2983997711923, 0.4621458983764364, 0.0, 1.0, 43911.00298132953], 
processed observation next is [1.0, 1.0, 0.29178208679593726, 0.9066666666666667, 0.0, 0.0, 0.6666666666666666, 0.6081999809326918, 0.6540486327921454, 0.0, 1.0, 0.2091000141968073], 
reward next is 0.7909, 
noisyNet noise sample is [array([0.42687005], dtype=float32), 2.5597696]. 
=============================================
[2019-04-04 01:52:35,396] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.7657507e-28 3.2972500e-22 3.4717812e-21 1.0000000e+00 1.2050624e-22
 1.5956721e-18 2.5826774e-21], sum to 1.0000
[2019-04-04 01:52:35,397] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3945
[2019-04-04 01:52:35,468] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-11.0, 82.66666666666666, 29.99999999999999, 194.6666666666666, 26.0, 25.31658494416898, 0.3994111251066989, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3311400.0000, 
sim time next is 3312000.0000, 
raw observation next is [-11.0, 84.0, 44.0, 245.0, 26.0, 25.51231250087619, 0.4123572197650753, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.15789473684210528, 0.84, 0.14666666666666667, 0.27071823204419887, 0.6666666666666666, 0.6260260417396827, 0.6374524065883584, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.549055], dtype=float32), 0.3175312]. 
=============================================
[2019-04-04 01:52:35,481] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[96.99557 ]
 [97.278694]
 [96.83645 ]
 [95.49746 ]
 [94.6764  ]], R is [[96.69754028]
 [96.73056793]
 [96.36022186]
 [95.42930603]
 [94.51132965]].
[2019-04-04 01:52:36,905] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.18851972e-26 1.66642889e-19 5.94989319e-18 1.00000000e+00
 3.16261424e-20 5.43426385e-19 1.06040697e-19], sum to 1.0000
[2019-04-04 01:52:36,906] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8489
[2019-04-04 01:52:36,924] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.333333333333334, 73.0, 0.0, 0.0, 26.0, 24.87544094197296, 0.2784534763080829, 0.0, 1.0, 43832.74935757958], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3820800.0000, 
sim time next is 3821400.0000, 
raw observation next is [-4.5, 74.0, 0.0, 0.0, 26.0, 24.87044606434342, 0.2739676130874759, 0.0, 1.0, 43767.16063440148], 
processed observation next is [1.0, 0.21739130434782608, 0.3379501385041552, 0.74, 0.0, 0.0, 0.6666666666666666, 0.5725371720286182, 0.5913225376958253, 0.0, 1.0, 0.20841505064000704], 
reward next is 0.7916, 
noisyNet noise sample is [array([-0.7590113], dtype=float32), 1.8571951]. 
=============================================
[2019-04-04 01:52:38,191] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0068389e-25 8.1739096e-17 5.3906431e-17 1.0000000e+00 1.5226649e-18
 2.2365339e-16 1.2458726e-17], sum to 1.0000
[2019-04-04 01:52:38,192] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6297
[2019-04-04 01:52:38,211] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.833333333333333, 60.0, 107.0, 743.3333333333334, 26.0, 26.46885500427902, 0.6022075900484819, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3838200.0000, 
sim time next is 3838800.0000, 
raw observation next is [-1.666666666666667, 60.00000000000001, 108.5, 759.1666666666667, 26.0, 26.51467700681176, 0.6134583543973852, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4164358264081256, 0.6000000000000001, 0.3616666666666667, 0.8388581952117865, 0.6666666666666666, 0.7095564172343133, 0.7044861181324618, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2361599], dtype=float32), -0.27288818]. 
=============================================
[2019-04-04 01:52:39,137] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:52:39,138] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:52:39,148] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run23
[2019-04-04 01:52:47,774] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:52:47,774] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:52:47,789] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run23
[2019-04-04 01:52:49,623] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.8256638e-20 2.7319859e-13 1.6341687e-12 1.0000000e+00 4.2712998e-14
 1.2817592e-08 3.1077893e-14], sum to 1.0000
[2019-04-04 01:52:49,624] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0202
[2019-04-04 01:52:49,640] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.8, 73.0, 0.0, 0.0, 25.0, 24.46411020229809, 0.2201386874192612, 0.0, 1.0, 19336.95708892919], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 4501800.0000, 
sim time next is 4502400.0000, 
raw observation next is [-0.8666666666666667, 73.0, 0.0, 0.0, 25.0, 24.4201366151973, 0.2242473791363802, 0.0, 1.0, 54557.57817268305], 
processed observation next is [1.0, 0.08695652173913043, 0.4385964912280702, 0.73, 0.0, 0.0, 0.5833333333333334, 0.535011384599775, 0.5747491263787934, 0.0, 1.0, 0.2597979912984907], 
reward next is 0.7402, 
noisyNet noise sample is [array([0.11515147], dtype=float32), 0.10079985]. 
=============================================
[2019-04-04 01:52:52,091] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.7039588e-23 9.7089525e-15 2.9630407e-15 1.0000000e+00 1.8519685e-15
 2.0354797e-15 3.2505522e-16], sum to 1.0000
[2019-04-04 01:52:52,092] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4318
[2019-04-04 01:52:52,103] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 25.62599082303072, 0.4866393130976121, 1.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4558200.0000, 
sim time next is 4558800.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 25.61632785058731, 0.4790591505115341, 1.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.782608695652174, 0.518005540166205, 0.52, 0.0, 0.0, 0.6666666666666666, 0.6346939875489426, 0.6596863835038447, 1.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.8310746], dtype=float32), 0.44833103]. 
=============================================
[2019-04-04 01:52:54,984] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.0927465e-27 2.6239079e-22 3.7870022e-20 1.0000000e+00 3.8288604e-23
 1.0425774e-20 8.4565550e-21], sum to 1.0000
[2019-04-04 01:52:54,984] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2093
[2019-04-04 01:52:55,080] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-12.55, 68.5, 0.0, 0.0, 26.0, 21.58868923344984, -0.4497724590592689, 1.0, 1.0, 203381.1641666486], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 286200.0000, 
sim time next is 286800.0000, 
raw observation next is [-12.63333333333333, 69.0, 0.0, 0.0, 26.0, 22.18827398544775, -0.3288317737546267, 1.0, 1.0, 167652.2696467387], 
processed observation next is [1.0, 0.30434782608695654, 0.11265004616805181, 0.69, 0.0, 0.0, 0.6666666666666666, 0.3490228321206459, 0.3903894087484578, 1.0, 1.0, 0.7983441411749462], 
reward next is 0.2017, 
noisyNet noise sample is [array([-1.1011741], dtype=float32), 0.5460503]. 
=============================================
[2019-04-04 01:52:56,915] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:52:56,915] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:52:56,932] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run23
[2019-04-04 01:53:06,692] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.4772999e-24 2.1513498e-20 1.4811175e-20 1.0000000e+00 1.9944237e-19
 1.2944707e-16 1.0098312e-18], sum to 1.0000
[2019-04-04 01:53:06,692] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5360
[2019-04-04 01:53:06,728] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 24.0, 23.3826712120028, -0.07430514142340652, 0.0, 1.0, 61853.71918981288], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4841400.0000, 
sim time next is 4842000.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 24.0, 23.41887988328071, -0.07062457594948746, 0.0, 1.0, 29682.94251145381], 
processed observation next is [0.0, 0.043478260869565216, 0.40720221606648205, 0.6, 0.0, 0.0, 0.5, 0.4515733236067258, 0.47645847468350416, 0.0, 1.0, 0.14134734529263718], 
reward next is 0.8587, 
noisyNet noise sample is [array([-0.39525247], dtype=float32), 0.14043638]. 
=============================================
[2019-04-04 01:53:06,753] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[87.98457 ]
 [87.851616]
 [87.746254]
 [87.589226]
 [87.521454]], R is [[88.14321136]
 [87.96723938]
 [87.6991806 ]
 [87.40992737]
 [87.21368408]].
[2019-04-04 01:53:08,468] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.1752749e-26 2.8218491e-21 4.1351910e-21 1.0000000e+00 5.3161935e-22
 2.2395698e-19 6.1833883e-20], sum to 1.0000
[2019-04-04 01:53:08,475] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6192
[2019-04-04 01:53:08,517] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.6666666666666666, 39.66666666666666, 0.0, 0.0, 26.0, 25.39237821236648, 0.3498826159197927, 0.0, 1.0, 29926.09682732787], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4923600.0000, 
sim time next is 4924200.0000, 
raw observation next is [0.8333333333333334, 39.83333333333334, 0.0, 0.0, 26.0, 25.38386384236454, 0.3499585118749254, 0.0, 1.0, 37916.05400215604], 
processed observation next is [0.0, 1.0, 0.4856879039704525, 0.39833333333333343, 0.0, 0.0, 0.6666666666666666, 0.6153219868637118, 0.6166528372916418, 0.0, 1.0, 0.18055263810550495], 
reward next is 0.8194, 
noisyNet noise sample is [array([1.3859473], dtype=float32), -1.3533247]. 
=============================================
[2019-04-04 01:53:08,629] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.5264863e-31 4.2372062e-24 1.0940871e-22 1.0000000e+00 1.7766968e-24
 2.3070456e-23 1.8882364e-22], sum to 1.0000
[2019-04-04 01:53:08,630] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9818
[2019-04-04 01:53:08,659] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.166666666666667, 64.83333333333334, 0.0, 0.0, 26.0, 25.26700145839721, 0.3383526409870271, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4299000.0000, 
sim time next is 4299600.0000, 
raw observation next is [6.133333333333334, 65.66666666666667, 0.0, 0.0, 26.0, 25.20133215407799, 0.3238154930013123, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.782608695652174, 0.6325023084025855, 0.6566666666666667, 0.0, 0.0, 0.6666666666666666, 0.6001110128398324, 0.6079384976671042, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3423977], dtype=float32), 0.41950473]. 
=============================================
[2019-04-04 01:53:11,052] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.5106825e-26 6.5754784e-21 1.3298308e-18 1.0000000e+00 1.3133642e-20
 1.2446654e-18 1.4295978e-19], sum to 1.0000
[2019-04-04 01:53:11,052] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6000
[2019-04-04 01:53:11,061] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 45.0, 175.1666666666667, 414.0, 24.0, 23.26699899110556, -0.05615680834448145, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4893600.0000, 
sim time next is 4894200.0000, 
raw observation next is [3.0, 45.0, 163.0, 422.0, 24.0, 23.28192146959997, -0.05496109902757376, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.5457063711911359, 0.45, 0.5433333333333333, 0.4662983425414365, 0.5, 0.440160122466664, 0.4816796336574754, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.55919045], dtype=float32), -0.8435669]. 
=============================================
[2019-04-04 01:53:13,013] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.8625238e-24 3.5807827e-15 2.2161662e-14 1.0000000e+00 4.2943330e-19
 1.0842239e-14 3.5759621e-16], sum to 1.0000
[2019-04-04 01:53:13,014] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2513
[2019-04-04 01:53:13,067] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.333333333333333, 61.66666666666667, 0.0, 0.0, 26.0, 25.40734999852638, 0.468099523283926, 0.0, 1.0, 22153.62455086072], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3885600.0000, 
sim time next is 3886200.0000, 
raw observation next is [-1.5, 62.5, 0.0, 0.0, 26.0, 25.4274737130127, 0.462329331276337, 0.0, 1.0, 18762.43996801267], 
processed observation next is [1.0, 1.0, 0.4210526315789474, 0.625, 0.0, 0.0, 0.6666666666666666, 0.6189561427510583, 0.6541097770921124, 0.0, 1.0, 0.08934495222863176], 
reward next is 0.9107, 
noisyNet noise sample is [array([0.21724252], dtype=float32), -0.9875982]. 
=============================================
[2019-04-04 01:53:13,127] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.8454690e-27 3.8113023e-22 1.4438357e-20 1.0000000e+00 1.1471353e-23
 2.2852629e-22 9.4532777e-21], sum to 1.0000
[2019-04-04 01:53:13,127] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5755
[2019-04-04 01:53:13,137] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 47.0, 0.0, 0.0, 26.0, 25.32679970870775, 0.4316319105947135, 0.0, 1.0, 45691.42513875259], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4824000.0000, 
sim time next is 4824600.0000, 
raw observation next is [0.8333333333333334, 47.66666666666667, 0.0, 0.0, 26.0, 25.42652468886662, 0.4384657580855094, 0.0, 1.0, 18764.58029247532], 
processed observation next is [0.0, 0.8695652173913043, 0.4856879039704525, 0.47666666666666674, 0.0, 0.0, 0.6666666666666666, 0.6188770574055518, 0.6461552526951698, 0.0, 1.0, 0.08935514424988247], 
reward next is 0.9106, 
noisyNet noise sample is [array([-2.3713574], dtype=float32), -1.1041272]. 
=============================================
[2019-04-04 01:53:13,643] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.08437860e-26 2.95872590e-15 2.17330849e-19 1.00000000e+00
 1.13331155e-17 3.37739723e-16 4.54573383e-18], sum to 1.0000
[2019-04-04 01:53:13,644] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1155
[2019-04-04 01:53:13,695] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [13.8, 31.0, 120.0, 828.0, 26.0, 27.85404122948508, 0.9435905673857952, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4361400.0000, 
sim time next is 4362000.0000, 
raw observation next is [14.2, 30.0, 119.6666666666667, 832.1666666666666, 26.0, 27.95388221276729, 0.969983632853264, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.8559556786703602, 0.3, 0.398888888888889, 0.9195211786372007, 0.6666666666666666, 0.8294901843972742, 0.8233278776177547, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7985557], dtype=float32), -3.1363976]. 
=============================================
[2019-04-04 01:53:13,709] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[84.409904]
 [84.80236 ]
 [85.19574 ]
 [85.65692 ]
 [86.21881 ]], R is [[84.13729858]
 [84.29592896]
 [84.45297241]
 [84.60844421]
 [84.76235962]].
[2019-04-04 01:53:18,451] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.7662506e-22 1.4188579e-10 2.9926309e-12 1.0000000e+00 1.8085046e-12
 2.1032719e-11 4.3236190e-14], sum to 1.0000
[2019-04-04 01:53:18,452] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8536
[2019-04-04 01:53:18,470] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [12.6, 34.0, 117.5, 804.0, 24.0, 25.8258947735858, 0.4323953882674538, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4359600.0000, 
sim time next is 4360200.0000, 
raw observation next is [13.0, 33.0, 118.3333333333333, 812.0, 24.0, 25.8493666423968, 0.4470945600766956, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.8227146814404434, 0.33, 0.3944444444444443, 0.8972375690607735, 0.5, 0.6541138868663999, 0.6490315200255652, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1317784], dtype=float32), -0.4227765]. 
=============================================
[2019-04-04 01:53:18,835] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.00207454e-20 2.39745490e-09 8.87244445e-12 9.99999762e-01
 1.59304536e-13 2.64653551e-07 6.99072487e-14], sum to 1.0000
[2019-04-04 01:53:18,835] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6373
[2019-04-04 01:53:18,867] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [14.66666666666667, 28.83333333333334, 117.0, 849.3333333333334, 24.0, 26.29757779921974, 0.4769929798990158, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4366200.0000, 
sim time next is 4366800.0000, 
raw observation next is [14.6, 29.0, 116.5, 847.5, 24.0, 25.75639864148298, 0.506159661193487, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.8670360110803325, 0.29, 0.3883333333333333, 0.93646408839779, 0.5, 0.6463665534569151, 0.6687198870644956, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6794186], dtype=float32), -1.097506]. 
=============================================
[2019-04-04 01:53:22,833] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:53:22,833] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:53:22,836] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run23
[2019-04-04 01:53:26,760] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.3480883e-22 5.5943223e-15 1.0865369e-13 1.0000000e+00 4.7608067e-15
 3.6944827e-12 7.1803168e-16], sum to 1.0000
[2019-04-04 01:53:26,760] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7840
[2019-04-04 01:53:26,782] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 24.0, 23.7571863168056, 0.06021965095035838, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4473000.0000, 
sim time next is 4473600.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 24.0, 23.66124834968267, 0.03403559219297989, 0.0, 1.0, 84642.07696413767], 
processed observation next is [1.0, 0.782608695652174, 0.46260387811634357, 0.72, 0.0, 0.0, 0.5, 0.4717706958068891, 0.5113451973976599, 0.0, 1.0, 0.4030575093530365], 
reward next is 0.5969, 
noisyNet noise sample is [array([-0.21642137], dtype=float32), 0.7940539]. 
=============================================
[2019-04-04 01:53:26,829] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.0048440e-20 1.7971622e-14 1.3759899e-12 1.0000000e+00 3.3844296e-14
 2.9902757e-11 7.4613160e-14], sum to 1.0000
[2019-04-04 01:53:26,830] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9429
[2019-04-04 01:53:26,883] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 23.0, 22.21615963042769, -0.4528734010434761, 1.0, 1.0, 43650.23767483239], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 239400.0000, 
sim time next is 240000.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 23.0, 22.05742030029332, -0.4641711783732678, 1.0, 1.0, 124636.3356294433], 
processed observation next is [1.0, 0.782608695652174, 0.368421052631579, 0.65, 0.0, 0.0, 0.4166666666666667, 0.33811835835777665, 0.3452762738755774, 1.0, 1.0, 0.5935063601402062], 
reward next is 0.4065, 
noisyNet noise sample is [array([2.6417384], dtype=float32), -0.63157284]. 
=============================================
[2019-04-04 01:53:26,889] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[90.860405]
 [90.83288 ]
 [90.76747 ]
 [90.6718  ]
 [90.50611 ]], R is [[90.2906723 ]
 [90.17990875]
 [90.12398529]
 [90.07442474]
 [90.03901672]].
[2019-04-04 01:53:27,924] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:53:27,924] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:53:27,929] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run23
[2019-04-04 01:53:31,925] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.26874246e-29 8.79109748e-24 1.00058025e-22 1.00000000e+00
 2.18931162e-25 3.49023200e-25 1.97159867e-22], sum to 1.0000
[2019-04-04 01:53:31,925] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3008
[2019-04-04 01:53:31,968] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.8333333333333334, 54.33333333333334, 0.0, 0.0, 26.0, 25.39978905065709, 0.4004495453028598, 0.0, 1.0, 42904.89311387072], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4830600.0000, 
sim time next is 4831200.0000, 
raw observation next is [-1.0, 55.0, 0.0, 0.0, 26.0, 25.3893636048319, 0.4003631019380038, 0.0, 1.0, 46058.69579924512], 
processed observation next is [0.0, 0.9565217391304348, 0.4349030470914128, 0.55, 0.0, 0.0, 0.6666666666666666, 0.6157803004026583, 0.6334543673126679, 0.0, 1.0, 0.2193271228535482], 
reward next is 0.7807, 
noisyNet noise sample is [array([1.2998097], dtype=float32), -1.6349049]. 
=============================================
[2019-04-04 01:53:34,178] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.6196920e-29 1.1715983e-19 2.0101889e-19 1.0000000e+00 3.5332673e-21
 2.1693835e-18 6.2435892e-21], sum to 1.0000
[2019-04-04 01:53:34,178] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1297
[2019-04-04 01:53:34,185] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [8.0, 26.0, 106.1666666666667, 811.5, 26.0, 27.63406075465125, 0.8712352155718452, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4977600.0000, 
sim time next is 4978200.0000, 
raw observation next is [8.0, 26.0, 103.3333333333333, 804.0, 26.0, 27.69961493185066, 0.885702575717743, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.6842105263157896, 0.26, 0.34444444444444433, 0.8883977900552487, 0.6666666666666666, 0.8083012443208885, 0.7952341919059144, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.06789297], dtype=float32), 1.1600617]. 
=============================================
[2019-04-04 01:53:38,353] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:53:38,353] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:53:38,359] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run23
[2019-04-04 01:53:40,627] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:53:40,628] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:53:40,658] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run23
[2019-04-04 01:53:46,357] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:53:46,357] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:53:46,360] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run23
[2019-04-04 01:53:52,712] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.1454217e-24 5.0019720e-16 1.5956937e-14 1.0000000e+00 3.6033432e-19
 5.6539648e-15 5.8146202e-16], sum to 1.0000
[2019-04-04 01:53:52,748] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3353
[2019-04-04 01:53:52,794] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.0, 69.0, 0.0, 0.0, 26.0, 24.43769424854639, 0.1762145862992612, 0.0, 1.0, 42196.82536126586], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 775800.0000, 
sim time next is 776400.0000, 
raw observation next is [-7.1, 69.66666666666666, 0.0, 0.0, 26.0, 24.40391658894673, 0.1679372626033945, 0.0, 1.0, 42081.30522088631], 
processed observation next is [1.0, 1.0, 0.2659279778393352, 0.6966666666666665, 0.0, 0.0, 0.6666666666666666, 0.5336597157455607, 0.5559790875344649, 0.0, 1.0, 0.20038716771850623], 
reward next is 0.7996, 
noisyNet noise sample is [array([1.0311407], dtype=float32), -0.15534157]. 
=============================================
[2019-04-04 01:53:57,741] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:53:57,741] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:53:57,745] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run23
[2019-04-04 01:53:58,545] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:53:58,546] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:53:58,566] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run23
[2019-04-04 01:54:05,022] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:54:05,022] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:54:05,044] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run23
[2019-04-04 01:54:16,416] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.7945144e-19 2.0405178e-11 3.2949202e-13 1.0000000e+00 5.3240773e-11
 4.0135714e-10 5.8187564e-13], sum to 1.0000
[2019-04-04 01:54:16,416] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3032
[2019-04-04 01:54:16,455] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.3, 61.5, 43.33333333333334, 0.0, 25.0, 24.41746048409038, 0.04185887766652729, 1.0, 1.0, 35775.68426087745], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 229800.0000, 
sim time next is 230400.0000, 
raw observation next is [-3.4, 62.0, 37.0, 0.0, 25.0, 24.50536383406875, 0.04891280802563421, 1.0, 1.0, 34218.85134357696], 
processed observation next is [1.0, 0.6956521739130435, 0.368421052631579, 0.62, 0.12333333333333334, 0.0, 0.5833333333333334, 0.5421136528390624, 0.5163042693418781, 1.0, 1.0, 0.16294691115989027], 
reward next is 0.8371, 
noisyNet noise sample is [array([-0.259836], dtype=float32), -3.1979384]. 
=============================================
[2019-04-04 01:54:33,969] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.4119171e-20 3.5858128e-15 9.5202112e-13 1.0000000e+00 5.3600367e-14
 8.5295264e-11 4.7519460e-14], sum to 1.0000
[2019-04-04 01:54:33,970] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3539
[2019-04-04 01:54:34,011] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-10.96666666666667, 68.0, 0.0, 0.0, 23.0, 21.24820231781015, -0.618597455872808, 0.0, 1.0, 48979.41990601266], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 278400.0000, 
sim time next is 279000.0000, 
raw observation next is [-11.15, 68.5, 0.0, 0.0, 23.0, 21.19680791141212, -0.6233571302607347, 0.0, 1.0, 49030.78177892559], 
processed observation next is [1.0, 0.21739130434782608, 0.15373961218836565, 0.685, 0.0, 0.0, 0.4166666666666667, 0.26640065928434326, 0.29221428991308845, 0.0, 1.0, 0.233479913232979], 
reward next is 0.7665, 
noisyNet noise sample is [array([-0.72734505], dtype=float32), 0.7365486]. 
=============================================
[2019-04-04 01:54:34,078] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[81.78215]
 [81.82642]
 [81.86043]
 [81.93979]
 [82.04658]], R is [[81.70534515]
 [81.65505219]
 [81.60555267]
 [81.55702209]
 [81.50960541]].
[2019-04-04 01:54:34,633] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:54:34,633] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:54:34,638] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run23
[2019-04-04 01:54:39,972] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:54:39,972] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:54:39,991] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run23
[2019-04-04 01:54:43,100] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-04 01:54:43,132] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 01:54:43,133] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:54:43,142] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 01:54:43,142] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run31
[2019-04-04 01:54:43,161] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:54:43,283] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run31
[2019-04-04 01:54:43,146] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 01:54:43,308] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:54:43,340] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run31
[2019-04-04 01:55:16,135] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.2188962], dtype=float32), 0.6759383]
[2019-04-04 01:55:16,135] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-3.661274482, 77.35720886, 43.59318241, 47.597286695, 19.0, 18.88316718664653, -1.137877584680766, 0.0, 1.0, 0.0]
[2019-04-04 01:55:16,136] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 01:55:16,137] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [4.8313139e-16 2.1596130e-13 7.5761914e-12 1.0000000e+00 6.3519462e-10
 9.7255470e-10 6.9424494e-11], sampled 0.0004279278858192459
[2019-04-04 01:55:41,134] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.2188962], dtype=float32), 0.6759383]
[2019-04-04 01:55:41,135] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [3.3, 84.0, 0.0, 0.0, 19.0, 19.04046816049012, -0.9981742221606437, 0.0, 1.0, 0.0]
[2019-04-04 01:55:41,135] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 01:55:41,136] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.3575148e-16 4.9726473e-11 2.0961347e-11 9.9999976e-01 5.2957638e-09
 2.3382715e-07 1.6638571e-10], sampled 0.7641552899523099
[2019-04-04 01:55:55,133] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.2188962], dtype=float32), 0.6759383]
[2019-04-04 01:55:55,133] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-1.0, 60.0, 222.0, 255.5, 19.0, 18.70754192484543, -1.111321760632591, 0.0, 1.0, 0.0]
[2019-04-04 01:55:55,133] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 01:55:55,134] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.0070932e-16 6.9469322e-13 5.4774588e-12 1.0000000e+00 9.8019559e-10
 5.9881282e-09 2.8374992e-11], sampled 0.9128452935188796
[2019-04-04 01:56:50,430] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.2188962], dtype=float32), 0.6759383]
[2019-04-04 01:56:50,430] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-2.419425894, 75.59576469, 0.0, 0.0, 19.0, 18.66569578766375, -1.173609775417154, 0.0, 1.0, 18724.21614130312]
[2019-04-04 01:56:50,430] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 01:56:50,431] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [5.09934322e-16 3.46640278e-13 1.26247285e-11 1.00000000e+00
 1.97697392e-09 8.48725978e-09 1.16198079e-10], sampled 0.004138126141037479
[2019-04-04 01:56:54,542] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5664.9494 149105495.9476 -1995.0993
[2019-04-04 01:57:09,543] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 4987.9791 172584929.6817 -2849.4376
[2019-04-04 01:57:18,898] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5093.2978 185907989.1130 -2751.5680
[2019-04-04 01:57:19,932] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 3000000, evaluation results [3000000.0, 4987.97911248897, 172584929.68165275, -2849.4376347302846, 5664.949360737056, 149105495.94759172, -1995.0993173683883, 5093.297768158066, 185907989.11304876, -2751.5680005164486]
[2019-04-04 01:57:30,434] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.1464597e-15 4.1361407e-11 2.7198421e-11 9.9999952e-01 2.2770428e-11
 4.4857245e-07 3.8605924e-10], sum to 1.0000
[2019-04-04 01:57:30,434] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5202
[2019-04-04 01:57:30,478] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-12.8, 74.66666666666666, 0.0, 0.0, 23.0, 22.49208044437829, -0.3693116506327894, 0.0, 1.0, 22184.13644435977], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 333600.0000, 
sim time next is 334200.0000, 
raw observation next is [-12.8, 75.83333333333334, 0.0, 0.0, 23.0, 22.31313261257007, -0.3880981094755045, 0.0, 1.0, 102858.6455904221], 
processed observation next is [1.0, 0.8695652173913043, 0.1080332409972299, 0.7583333333333334, 0.0, 0.0, 0.4166666666666667, 0.3594277177141724, 0.3706339635081652, 0.0, 1.0, 0.4898030742401052], 
reward next is 0.5102, 
noisyNet noise sample is [array([-0.5727758], dtype=float32), -1.2505838]. 
=============================================
[2019-04-04 01:57:33,366] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.4450594e-23 1.1277537e-15 6.4567315e-16 1.0000000e+00 6.5168056e-18
 2.4767276e-12 1.2490479e-15], sum to 1.0000
[2019-04-04 01:57:33,369] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7170
[2019-04-04 01:57:33,430] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.2, 72.0, 0.0, 0.0, 25.0, 24.41157314841698, 0.1612522967359842, 0.0, 1.0, 47750.58560501139], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 4598400.0000, 
sim time next is 4599000.0000, 
raw observation next is [-2.3, 72.5, 0.0, 0.0, 25.0, 24.34373943512686, 0.1587179162178251, 0.0, 1.0, 63851.32746599089], 
processed observation next is [1.0, 0.21739130434782608, 0.3988919667590028, 0.725, 0.0, 0.0, 0.5833333333333334, 0.5286449529272383, 0.5529059720726084, 0.0, 1.0, 0.3040539403142423], 
reward next is 0.6959, 
noisyNet noise sample is [array([0.40652484], dtype=float32), -1.7209873]. 
=============================================
[2019-04-04 01:57:33,440] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[82.530594]
 [82.69503 ]
 [82.73635 ]
 [82.69618 ]
 [82.72945 ]], R is [[82.28613281]
 [82.23588562]
 [82.32420349]
 [82.41162872]
 [82.46839142]].
[2019-04-04 01:57:57,255] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.8462554e-22 2.3868408e-18 9.2162935e-17 1.0000000e+00 6.7606332e-17
 3.3597943e-15 8.0603338e-17], sum to 1.0000
[2019-04-04 01:57:57,255] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0587
[2019-04-04 01:57:57,313] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.9, 71.0, 77.0, 25.5, 23.0, 22.3577963802404, -0.3941117175537963, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 637200.0000, 
sim time next is 637800.0000, 
raw observation next is [-3.9, 70.0, 96.33333333333334, 34.00000000000001, 23.0, 22.31271122104047, -0.4033254981632327, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.3545706371191136, 0.7, 0.3211111111111111, 0.03756906077348067, 0.4166666666666667, 0.35939260175337245, 0.3655581672789225, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2975444], dtype=float32), 0.010693837]. 
=============================================
[2019-04-04 01:58:01,863] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.9163888e-29 4.3689576e-17 4.0509483e-18 1.0000000e+00 5.4401103e-20
 1.0985165e-21 2.1610473e-20], sum to 1.0000
[2019-04-04 01:58:01,864] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5618
[2019-04-04 01:58:01,913] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [12.0, 19.0, 98.5, 757.3333333333334, 26.0, 28.70036122505883, 1.020137701893631, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5066400.0000, 
sim time next is 5067000.0000, 
raw observation next is [12.0, 19.0, 96.0, 745.0, 26.0, 28.27596598914598, 1.077933104447243, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.7950138504155125, 0.19, 0.32, 0.8232044198895028, 0.6666666666666666, 0.8563304990954984, 0.8593110348157477, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6185225], dtype=float32), 0.6653338]. 
=============================================
[2019-04-04 01:58:01,919] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[92.699745]
 [92.94726 ]
 [93.23741 ]
 [93.353035]
 [93.55211 ]], R is [[92.70709229]
 [92.78002167]
 [92.85221863]
 [92.92369843]
 [92.99446106]].
[2019-04-04 01:58:05,537] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:58:05,537] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:58:05,540] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run23
[2019-04-04 01:58:07,359] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.8716231e-22 3.4080978e-15 4.8294788e-13 1.0000000e+00 1.4438757e-18
 1.4428339e-12 1.1939574e-14], sum to 1.0000
[2019-04-04 01:58:07,359] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8892
[2019-04-04 01:58:07,411] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-13.95, 63.0, 71.0, 729.0, 26.0, 25.87876992234206, 0.3456043167290439, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 383400.0000, 
sim time next is 384000.0000, 
raw observation next is [-13.76666666666667, 62.0, 68.83333333333334, 734.8333333333333, 26.0, 25.9221188819606, 0.3474354712285492, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.08125577100646345, 0.62, 0.22944444444444448, 0.8119705340699815, 0.6666666666666666, 0.6601765734967167, 0.6158118237428497, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.40706986], dtype=float32), -0.4889174]. 
=============================================
[2019-04-04 01:58:07,419] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[73.68408 ]
 [74.3121  ]
 [74.68469 ]
 [75.34721 ]
 [76.125275]], R is [[73.53139496]
 [73.79608154]
 [74.05812073]
 [74.07019806]
 [74.07891083]].
[2019-04-04 01:58:14,582] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.5776670e-22 1.1885168e-16 7.7896421e-16 1.0000000e+00 3.5712317e-16
 4.0984928e-12 5.1354011e-15], sum to 1.0000
[2019-04-04 01:58:14,585] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2764
[2019-04-04 01:58:14,619] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 69.0, 0.0, 0.0, 23.0, 22.42356863383673, -0.3758236378802294, 0.0, 1.0, 26475.14124186404], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 683400.0000, 
sim time next is 684000.0000, 
raw observation next is [-3.4, 69.0, 0.0, 0.0, 23.0, 22.46151180264981, -0.3807594141105985, 0.0, 1.0, 18758.39757567987], 
processed observation next is [0.0, 0.9565217391304348, 0.368421052631579, 0.69, 0.0, 0.0, 0.4166666666666667, 0.3717926502208175, 0.37308019529646713, 0.0, 1.0, 0.08932570274133272], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.11738133], dtype=float32), -0.3081877]. 
=============================================
[2019-04-04 01:58:14,633] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[81.83713]
 [81.80596]
 [81.81945]
 [81.84641]
 [81.86451]], R is [[81.89348602]
 [81.9484787 ]
 [81.93015289]
 [81.90398407]
 [81.85736084]].
[2019-04-04 01:58:18,650] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.0729354e-14 1.2966091e-06 4.9405639e-07 9.9698466e-01 3.6508891e-06
 3.0099279e-03 3.6713768e-08], sum to 1.0000
[2019-04-04 01:58:18,670] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2154
[2019-04-04 01:58:18,729] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.5, 50.0, 110.0, 611.0, 22.0, 22.43549345283369, -0.4095860951530519, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 738000.0000, 
sim time next is 738600.0000, 
raw observation next is [0.5, 49.16666666666667, 103.0, 665.0, 22.0, 22.37219776563196, -0.4294262657877416, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.4764542936288089, 0.4916666666666667, 0.3433333333333333, 0.7348066298342542, 0.3333333333333333, 0.3643498138026633, 0.3568579114040861, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8106452], dtype=float32), -1.3279487]. 
=============================================
[2019-04-04 01:58:23,666] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:58:23,666] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:58:23,670] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run23
[2019-04-04 01:58:31,027] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.5890849e-18 1.2607844e-09 1.0026243e-10 9.9999952e-01 5.6023818e-11
 4.2105017e-07 5.5312539e-11], sum to 1.0000
[2019-04-04 01:58:31,027] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9390
[2019-04-04 01:58:31,119] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.9, 84.0, 0.0, 0.0, 25.0, 24.33364377237301, 0.03889326784451993, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 844200.0000, 
sim time next is 844800.0000, 
raw observation next is [-3.899999999999999, 84.66666666666666, 0.0, 0.0, 25.0, 24.08985678457181, 0.01456507386805841, 1.0, 1.0, 35182.59574033691], 
processed observation next is [1.0, 0.782608695652174, 0.35457063711911363, 0.8466666666666666, 0.0, 0.0, 0.5833333333333334, 0.5074880653809842, 0.5048550246226862, 1.0, 1.0, 0.16753617019208053], 
reward next is 0.8325, 
noisyNet noise sample is [array([-0.35071042], dtype=float32), 1.0611452]. 
=============================================
[2019-04-04 01:58:31,728] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.4338455e-21 7.6799233e-13 4.7396172e-14 1.0000000e+00 4.1564534e-14
 1.1534402e-09 7.2519463e-15], sum to 1.0000
[2019-04-04 01:58:31,729] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7726
[2019-04-04 01:58:31,816] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.566666666666666, 84.0, 0.0, 0.0, 25.0, 24.03961093592996, 0.01557620793542772, 1.0, 1.0, 28268.66617114241], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 848400.0000, 
sim time next is 849000.0000, 
raw observation next is [-3.483333333333333, 83.5, 0.0, 0.0, 25.0, 23.94264520056895, 0.01247342492099834, 1.0, 1.0, 86423.29925428181], 
processed observation next is [1.0, 0.8260869565217391, 0.3661126500461681, 0.835, 0.0, 0.0, 0.5833333333333334, 0.4952204333807459, 0.5041578083069994, 1.0, 1.0, 0.41153952025848484], 
reward next is 0.5885, 
noisyNet noise sample is [array([0.8155682], dtype=float32), 1.2126416]. 
=============================================
[2019-04-04 01:58:31,851] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[81.86455]
 [81.58541]
 [81.17445]
 [82.16305]
 [81.80369]], R is [[81.84750366]
 [81.89441681]
 [82.07546997]
 [82.25471497]
 [82.33559418]].
[2019-04-04 01:58:32,067] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.1730581e-18 5.3782053e-12 1.0451339e-11 9.9999928e-01 1.0994561e-11
 7.7104039e-07 4.4815068e-11], sum to 1.0000
[2019-04-04 01:58:32,068] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4429
[2019-04-04 01:58:32,095] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.900000000000002, 78.0, 0.0, 0.0, 23.0, 21.17575298368728, -0.6409936633669694, 0.0, 1.0, 45726.95212483649], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 188400.0000, 
sim time next is 189000.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 23.0, 21.12239615686377, -0.6515119155252329, 0.0, 1.0, 45797.4486429579], 
processed observation next is [1.0, 0.17391304347826086, 0.21606648199445982, 0.78, 0.0, 0.0, 0.4166666666666667, 0.2601996797386474, 0.28282936149158905, 0.0, 1.0, 0.21808308877598997], 
reward next is 0.7819, 
noisyNet noise sample is [array([-0.08891492], dtype=float32), 0.32373267]. 
=============================================
[2019-04-04 01:58:32,103] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[81.1514  ]
 [81.11383 ]
 [81.06907 ]
 [81.002556]
 [80.95139 ]], R is [[81.17838287]
 [81.14884949]
 [81.11977386]
 [81.09100342]
 [81.06260681]].
[2019-04-04 01:58:33,506] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.2120144e-22 5.8058623e-17 9.6350362e-18 1.0000000e+00 7.8758394e-17
 1.3314368e-15 8.0307517e-16], sum to 1.0000
[2019-04-04 01:58:33,516] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7286
[2019-04-04 01:58:33,542] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.733333333333333, 86.33333333333333, 0.0, 0.0, 24.0, 23.07984894701907, -0.1922928626105646, 0.0, 1.0, 43342.63975813262], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 607200.0000, 
sim time next is 607800.0000, 
raw observation next is [-3.816666666666666, 86.16666666666667, 0.0, 0.0, 24.0, 23.06060560558225, -0.1980567514758881, 0.0, 1.0, 43299.17658433934], 
processed observation next is [0.0, 0.0, 0.3568790397045245, 0.8616666666666667, 0.0, 0.0, 0.5, 0.4217171337985208, 0.4339810828413706, 0.0, 1.0, 0.20618655516352066], 
reward next is 0.7938, 
noisyNet noise sample is [array([0.24809974], dtype=float32), 0.10217161]. 
=============================================
[2019-04-04 01:58:37,834] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.1682116e-23 4.9210768e-14 3.3825822e-15 1.0000000e+00 1.8398149e-15
 1.0225260e-11 2.2499088e-16], sum to 1.0000
[2019-04-04 01:58:37,846] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1714
[2019-04-04 01:58:37,861] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.3, 92.0, 0.0, 0.0, 26.0, 25.30260135335313, 0.5386103495843181, 0.0, 1.0, 49186.55975666803], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1303200.0000, 
sim time next is 1303800.0000, 
raw observation next is [3.2, 92.0, 0.0, 0.0, 26.0, 25.31616553355507, 0.5481168483693905, 0.0, 1.0, 42783.43599829531], 
processed observation next is [1.0, 0.08695652173913043, 0.551246537396122, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6096804611295891, 0.6827056161231302, 0.0, 1.0, 0.20373064761093004], 
reward next is 0.7963, 
noisyNet noise sample is [array([-0.05126982], dtype=float32), -0.4974137]. 
=============================================
[2019-04-04 01:58:47,037] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.9322729e-29 2.2146824e-22 6.6437493e-23 1.0000000e+00 3.0103096e-23
 3.9423775e-23 6.1247862e-22], sum to 1.0000
[2019-04-04 01:58:47,039] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6180
[2019-04-04 01:58:47,060] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.7, 83.66666666666667, 0.0, 0.0, 26.0, 24.81892797997124, 0.3477655729847735, 0.0, 1.0, 43636.93176907478], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1746600.0000, 
sim time next is 1747200.0000, 
raw observation next is [-0.8, 84.33333333333334, 0.0, 0.0, 26.0, 24.80190847312978, 0.3518266480116815, 0.0, 1.0, 43681.47759127629], 
processed observation next is [0.0, 0.21739130434782608, 0.4404432132963989, 0.8433333333333334, 0.0, 0.0, 0.6666666666666666, 0.5668257060941485, 0.6172755493372272, 0.0, 1.0, 0.20800703614893473], 
reward next is 0.7920, 
noisyNet noise sample is [array([-1.5421706], dtype=float32), -0.15749615]. 
=============================================
[2019-04-04 01:58:48,215] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.4568103e-24 1.4972518e-11 5.5766371e-15 1.0000000e+00 1.9349011e-13
 3.8074197e-16 7.8638650e-16], sum to 1.0000
[2019-04-04 01:58:48,215] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7043
[2019-04-04 01:58:48,234] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [14.4, 75.0, 114.0, 0.0, 26.0, 27.17875902791471, 0.8532384743182836, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1074600.0000, 
sim time next is 1075200.0000, 
raw observation next is [14.76666666666667, 73.33333333333334, 137.3333333333333, 35.83333333333333, 26.0, 27.223870102954, 0.8710091972993096, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.8716528162511544, 0.7333333333333334, 0.4577777777777776, 0.03959484346224677, 0.6666666666666666, 0.7686558419128332, 0.7903363990997699, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4453611], dtype=float32), -0.3764343]. 
=============================================
[2019-04-04 01:58:49,192] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2458593e-23 7.0830556e-17 6.9124082e-17 1.0000000e+00 1.5410074e-18
 4.4294566e-16 4.5170586e-17], sum to 1.0000
[2019-04-04 01:58:49,197] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8220
[2019-04-04 01:58:49,212] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.716666666666667, 71.0, 0.0, 0.0, 26.0, 24.13279675407848, 0.08653511682849764, 0.0, 1.0, 41546.58943113503], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 784200.0000, 
sim time next is 784800.0000, 
raw observation next is [-7.8, 71.0, 0.0, 0.0, 26.0, 24.07290564997324, 0.07790506657102829, 0.0, 1.0, 41546.06766886778], 
processed observation next is [1.0, 0.08695652173913043, 0.24653739612188366, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5060754708311034, 0.5259683555236762, 0.0, 1.0, 0.19783841747079894], 
reward next is 0.8022, 
noisyNet noise sample is [array([-0.9916737], dtype=float32), -0.037580144]. 
=============================================
[2019-04-04 01:58:50,208] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.39149825e-27 9.08095172e-18 8.01308264e-19 1.00000000e+00
 1.39782421e-19 1.93115488e-17 1.21890634e-19], sum to 1.0000
[2019-04-04 01:58:50,209] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5381
[2019-04-04 01:58:50,224] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.2, 96.0, 0.0, 0.0, 26.0, 25.30560273170148, 0.4377859484706703, 0.0, 1.0, 36875.02065121313], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1488600.0000, 
sim time next is 1489200.0000, 
raw observation next is [2.2, 96.0, 0.0, 0.0, 26.0, 25.28617409701132, 0.4352398944587867, 0.0, 1.0, 36903.66721857347], 
processed observation next is [1.0, 0.21739130434782608, 0.5235457063711911, 0.96, 0.0, 0.0, 0.6666666666666666, 0.6071811747509432, 0.6450799648195956, 0.0, 1.0, 0.17573174865987368], 
reward next is 0.8243, 
noisyNet noise sample is [array([-0.46463758], dtype=float32), -1.2357098]. 
=============================================
[2019-04-04 01:58:51,005] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.25507835e-33 7.81734379e-19 2.92503313e-25 1.00000000e+00
 4.65058796e-25 6.55729296e-21 3.78059647e-26], sum to 1.0000
[2019-04-04 01:58:51,005] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8154
[2019-04-04 01:58:51,069] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [17.7, 67.0, 0.0, 0.0, 25.0, 23.89771246282336, 0.2130465547428929, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 1196400.0000, 
sim time next is 1197000.0000, 
raw observation next is [17.7, 67.0, 0.0, 0.0, 25.0, 23.89643616227551, 0.2103791125859441, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.9529085872576178, 0.67, 0.0, 0.0, 0.5833333333333334, 0.49136968018962585, 0.5701263708619814, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2371475], dtype=float32), 0.19794519]. 
=============================================
[2019-04-04 01:58:51,104] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[92.092026]
 [92.03856 ]
 [91.98035 ]
 [91.91275 ]
 [91.83508 ]], R is [[92.24219513]
 [92.31977081]
 [92.39657593]
 [92.47261047]
 [92.54788208]].
[2019-04-04 01:58:54,108] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.4394266e-35 3.2560293e-21 7.0032014e-26 1.0000000e+00 1.8430911e-27
 3.3223645e-21 1.8077743e-26], sum to 1.0000
[2019-04-04 01:58:54,111] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6275
[2019-04-04 01:58:54,116] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [17.7, 67.0, 0.0, 0.0, 26.0, 24.5463650167997, 0.3637465192222183, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1198800.0000, 
sim time next is 1199400.0000, 
raw observation next is [17.51666666666667, 68.33333333333334, 0.0, 0.0, 26.0, 24.52722403592147, 0.3581012451871021, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.9130434782608695, 0.9478301015697139, 0.6833333333333335, 0.0, 0.0, 0.6666666666666666, 0.5439353363267893, 0.6193670817290341, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2971676], dtype=float32), 0.0436429]. 
=============================================
[2019-04-04 01:59:01,760] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.64734967e-23 2.65480507e-16 1.40996787e-15 1.00000000e+00
 1.05254126e-17 5.38246491e-17 2.98499302e-15], sum to 1.0000
[2019-04-04 01:59:01,792] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9542
[2019-04-04 01:59:01,818] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.199999999999999, 78.83333333333334, 0.0, 0.0, 26.0, 24.40718749937652, 0.1656097899041565, 0.0, 1.0, 42524.6326742007], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2164200.0000, 
sim time next is 2164800.0000, 
raw observation next is [-7.100000000000001, 78.66666666666667, 0.0, 0.0, 26.0, 24.33637985019749, 0.1590713465853799, 0.0, 1.0, 42567.09067344868], 
processed observation next is [1.0, 0.043478260869565216, 0.26592797783933514, 0.7866666666666667, 0.0, 0.0, 0.6666666666666666, 0.528031654183124, 0.5530237821951266, 0.0, 1.0, 0.20270043177832703], 
reward next is 0.7973, 
noisyNet noise sample is [array([0.37730697], dtype=float32), 1.0145262]. 
=============================================
[2019-04-04 01:59:03,189] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.8391561e-22 3.8310966e-15 7.8961152e-17 1.0000000e+00 1.7913589e-13
 1.1606025e-11 2.7899598e-16], sum to 1.0000
[2019-04-04 01:59:03,189] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1463
[2019-04-04 01:59:03,247] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.4, 68.0, 129.0, 0.0, 26.0, 25.56271252009812, 0.4640604372362354, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2125200.0000, 
sim time next is 2125800.0000, 
raw observation next is [-5.3, 68.0, 125.0, 0.0, 26.0, 26.01880507044075, 0.4889885830675813, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.31578947368421056, 0.68, 0.4166666666666667, 0.0, 0.6666666666666666, 0.6682337558700624, 0.6629961943558604, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.196214], dtype=float32), 0.7971061]. 
=============================================
[2019-04-04 01:59:07,104] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.1298047e-24 6.2854663e-18 1.5390445e-19 1.0000000e+00 9.6082736e-19
 1.1981467e-20 4.1393640e-18], sum to 1.0000
[2019-04-04 01:59:07,108] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8904
[2019-04-04 01:59:07,195] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 70.0, 8.333333333333332, 0.0, 26.0, 25.34206964703381, 0.3221902392722799, 1.0, 1.0, 33453.2764860149], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2222400.0000, 
sim time next is 2223000.0000, 
raw observation next is [-4.5, 69.5, 0.0, 0.0, 26.0, 24.62665377233346, 0.2785354772468231, 1.0, 1.0, 198788.7789598495], 
processed observation next is [1.0, 0.7391304347826086, 0.3379501385041552, 0.695, 0.0, 0.0, 0.6666666666666666, 0.5522211476944549, 0.5928451590822744, 1.0, 1.0, 0.9466132331421405], 
reward next is 0.0534, 
noisyNet noise sample is [array([1.464912], dtype=float32), -1.277255]. 
=============================================
[2019-04-04 01:59:07,201] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[84.50226 ]
 [84.262955]
 [84.372986]
 [84.43404 ]
 [84.58258 ]], R is [[83.47370148]
 [83.47966003]
 [83.4720993 ]
 [83.42577362]
 [83.30119324]].
[2019-04-04 01:59:12,176] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4996091e-23 4.9211886e-18 6.4239760e-18 1.0000000e+00 1.8630630e-17
 3.8449616e-15 5.4807995e-17], sum to 1.0000
[2019-04-04 01:59:12,176] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9169
[2019-04-04 01:59:12,238] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.566666666666666, 69.33333333333333, 0.0, 0.0, 26.0, 25.78762808161932, 0.4491913237667416, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2227200.0000, 
sim time next is 2227800.0000, 
raw observation next is [-4.583333333333333, 69.66666666666667, 0.0, 0.0, 26.0, 25.6833646792943, 0.4335519066834135, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.3356417359187443, 0.6966666666666668, 0.0, 0.0, 0.6666666666666666, 0.6402803899411916, 0.6445173022278045, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6302221], dtype=float32), -0.06367073]. 
=============================================
[2019-04-04 01:59:14,902] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.2169896e-22 5.3987101e-17 1.3872647e-14 1.0000000e+00 2.3453432e-16
 4.0909246e-16 9.0134893e-16], sum to 1.0000
[2019-04-04 01:59:14,903] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0461
[2019-04-04 01:59:14,920] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.5, 90.16666666666667, 0.0, 0.0, 26.0, 24.32776849350118, 0.1562978606697327, 0.0, 1.0, 43832.32867584075], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2087400.0000, 
sim time next is 2088000.0000, 
raw observation next is [-5.6, 91.0, 0.0, 0.0, 26.0, 24.40939305278589, 0.1644952090442758, 0.0, 1.0, 43577.2676762854], 
processed observation next is [1.0, 0.17391304347826086, 0.30747922437673136, 0.91, 0.0, 0.0, 0.6666666666666666, 0.5341160877321576, 0.5548317363480919, 0.0, 1.0, 0.20751079845850193], 
reward next is 0.7925, 
noisyNet noise sample is [array([-0.7264838], dtype=float32), 1.2259055]. 
=============================================
[2019-04-04 01:59:14,930] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[81.03234 ]
 [80.955154]
 [80.90061 ]
 [80.88706 ]
 [80.87466 ]], R is [[81.0402832 ]
 [81.02115631]
 [81.00402069]
 [80.98742676]
 [80.97134399]].
[2019-04-04 01:59:15,597] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.44401822e-34 6.51016474e-20 1.12347506e-26 1.00000000e+00
 2.18491360e-26 1.05835936e-24 2.50818202e-25], sum to 1.0000
[2019-04-04 01:59:15,598] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7287
[2019-04-04 01:59:15,655] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [15.0, 96.0, 9.333333333333332, 0.0, 26.0, 23.45158317565032, 0.1388040629052505, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1237800.0000, 
sim time next is 1238400.0000, 
raw observation next is [15.0, 96.0, 14.0, 0.0, 26.0, 23.43655664161823, 0.1343560563817825, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.8781163434903049, 0.96, 0.04666666666666667, 0.0, 0.6666666666666666, 0.45304638680151904, 0.5447853521272609, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3548084], dtype=float32), 1.266142]. 
=============================================
[2019-04-04 01:59:16,357] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.3611046e-25 2.8695127e-22 8.5478583e-20 1.0000000e+00 8.4198868e-22
 1.0589471e-20 2.0305044e-19], sum to 1.0000
[2019-04-04 01:59:16,357] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4420
[2019-04-04 01:59:16,381] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.64542412175214, 0.2109096148831529, 0.0, 1.0, 39421.17738344803], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2343000.0000, 
sim time next is 2343600.0000, 
raw observation next is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.60577129956849, 0.2033110431944842, 0.0, 1.0, 39533.46371453848], 
processed observation next is [0.0, 0.13043478260869565, 0.3988919667590028, 0.62, 0.0, 0.0, 0.6666666666666666, 0.5504809416307076, 0.5677703477314947, 0.0, 1.0, 0.1882545891168499], 
reward next is 0.8117, 
noisyNet noise sample is [array([1.1681023], dtype=float32), 0.18943284]. 
=============================================
[2019-04-04 01:59:28,231] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1303252e-22 3.6381336e-20 1.5968535e-18 1.0000000e+00 1.9026289e-18
 5.0714392e-19 1.1666674e-16], sum to 1.0000
[2019-04-04 01:59:28,231] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3271
[2019-04-04 01:59:28,299] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.9, 82.0, 0.0, 0.0, 25.0, 24.05118335506208, 0.1052058965390787, 0.0, 1.0, 44863.81116417337], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 1790400.0000, 
sim time next is 1791000.0000, 
raw observation next is [-3.9, 82.0, 0.0, 0.0, 25.0, 24.04796791756637, 0.1042046799781101, 0.0, 1.0, 45516.09235768962], 
processed observation next is [0.0, 0.7391304347826086, 0.3545706371191136, 0.82, 0.0, 0.0, 0.5833333333333334, 0.5039973264638643, 0.5347348933260366, 0.0, 1.0, 0.21674329694137912], 
reward next is 0.7833, 
noisyNet noise sample is [array([0.21040247], dtype=float32), 1.6693742]. 
=============================================
[2019-04-04 01:59:28,305] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[78.65707 ]
 [78.81341 ]
 [78.88798 ]
 [79.055954]
 [79.185356]], R is [[78.87957764]
 [78.87714386]
 [78.88101959]
 [78.89368439]
 [78.92294312]].
[2019-04-04 01:59:31,526] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.5232806e-28 1.9280520e-21 7.3134625e-20 1.0000000e+00 2.0023805e-21
 3.9686988e-20 1.7572217e-19], sum to 1.0000
[2019-04-04 01:59:31,526] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8685
[2019-04-04 01:59:31,615] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-9.5, 91.0, 9.999999999999998, 19.33333333333334, 26.0, 24.40697843971402, 0.1884606588428806, 1.0, 1.0, 152805.736690337], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2274000.0000, 
sim time next is 2274600.0000, 
raw observation next is [-9.5, 91.0, 17.0, 18.66666666666667, 26.0, 24.8542918192442, 0.2269405527562184, 1.0, 1.0, 33813.53073534323], 
processed observation next is [1.0, 0.30434782608695654, 0.1994459833795014, 0.91, 0.056666666666666664, 0.02062615101289135, 0.6666666666666666, 0.5711909849370166, 0.5756468509187395, 1.0, 1.0, 0.16101681302544396], 
reward next is 0.8390, 
noisyNet noise sample is [array([-1.9516019], dtype=float32), -0.09370765]. 
=============================================
[2019-04-04 01:59:40,309] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.2850132e-23 3.9839898e-12 9.3256125e-16 1.0000000e+00 5.1845589e-12
 2.1889616e-13 7.5461464e-17], sum to 1.0000
[2019-04-04 01:59:40,314] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9148
[2019-04-04 01:59:40,346] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [13.8, 49.0, 111.5, 0.0, 26.0, 26.10964628268074, 0.7117255443131648, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1609200.0000, 
sim time next is 1609800.0000, 
raw observation next is [13.71666666666667, 49.33333333333334, 100.3333333333333, 0.0, 26.0, 26.48199124144454, 0.759579905478912, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.8425669436749772, 0.4933333333333334, 0.3344444444444443, 0.0, 0.6666666666666666, 0.7068326034537117, 0.753193301826304, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3953879], dtype=float32), -0.8007524]. 
=============================================
[2019-04-04 01:59:44,220] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.6215448e-35 3.0150145e-20 1.4476863e-26 1.0000000e+00 8.0653288e-29
 2.1574631e-21 1.1184634e-26], sum to 1.0000
[2019-04-04 01:59:44,221] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8215
[2019-04-04 01:59:44,233] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [15.5, 93.0, 0.0, 0.0, 26.0, 23.37657452632953, 0.09969087488279949, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1222800.0000, 
sim time next is 1223400.0000, 
raw observation next is [15.5, 93.0, 0.0, 0.0, 26.0, 23.36315973241733, 0.09640198795526243, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.13043478260869565, 0.8919667590027703, 0.93, 0.0, 0.0, 0.6666666666666666, 0.44692997770144416, 0.5321339959850875, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5886595], dtype=float32), 0.16605529]. 
=============================================
[2019-04-04 01:59:48,427] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6041446e-24 1.3211996e-18 8.7445379e-17 1.0000000e+00 7.8578176e-21
 5.4388503e-18 4.8475547e-18], sum to 1.0000
[2019-04-04 01:59:48,428] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4321
[2019-04-04 01:59:48,474] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 26.0, 25.23764216761126, 0.4520607846470288, 0.0, 1.0, 42941.43571296612], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1731600.0000, 
sim time next is 1732200.0000, 
raw observation next is [0.45, 91.83333333333334, 0.0, 0.0, 26.0, 25.22265155715953, 0.4484029389980515, 0.0, 1.0, 42930.43642968672], 
processed observation next is [0.0, 0.043478260869565216, 0.47506925207756234, 0.9183333333333334, 0.0, 0.0, 0.6666666666666666, 0.6018876297632941, 0.6494676463326838, 0.0, 1.0, 0.20443064966517485], 
reward next is 0.7956, 
noisyNet noise sample is [array([-0.37736014], dtype=float32), 0.9735511]. 
=============================================
[2019-04-04 01:59:55,880] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.18945411e-17 1.13678112e-11 1.26295632e-12 1.00000000e+00
 1.08511575e-10 5.32501732e-09 2.41093365e-12], sum to 1.0000
[2019-04-04 01:59:55,880] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4950
[2019-04-04 01:59:55,951] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 65.0, 56.0, 0.0, 25.0, 25.1515091926091, 0.2605899479898402, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 2131200.0000, 
sim time next is 2131800.0000, 
raw observation next is [-4.5, 65.5, 46.0, 0.0, 25.0, 25.21478621153084, 0.2614190528153003, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.3379501385041552, 0.655, 0.15333333333333332, 0.0, 0.5833333333333334, 0.6012321842942366, 0.5871396842717668, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.49567023], dtype=float32), 0.42611083]. 
=============================================
[2019-04-04 01:59:56,659] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.14761855e-35 5.22585337e-22 2.61724598e-25 1.00000000e+00
 3.53417312e-27 1.67702678e-20 1.63113883e-27], sum to 1.0000
[2019-04-04 01:59:56,659] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3041
[2019-04-04 01:59:56,705] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [15.0, 96.0, 9.333333333333332, 0.0, 26.0, 22.89459169892496, 0.002387695489998176, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1237800.0000, 
sim time next is 1238400.0000, 
raw observation next is [15.0, 96.0, 14.0, 0.0, 26.0, 22.87982015049373, -0.002049657718365747, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.8781163434903049, 0.96, 0.04666666666666667, 0.0, 0.6666666666666666, 0.406651679207811, 0.49931678076054475, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3522421], dtype=float32), -0.92403]. 
=============================================
[2019-04-04 02:00:06,012] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.2589627e-25 1.9175627e-14 1.1943689e-19 1.0000000e+00 2.6364460e-17
 1.7209489e-14 1.9117021e-17], sum to 1.0000
[2019-04-04 02:00:06,012] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3226
[2019-04-04 02:00:06,047] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.866666666666667, 80.0, 0.0, 0.0, 26.0, 25.69097788500945, 0.5590961894022426, 0.0, 1.0, 33635.50402158597], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1550400.0000, 
sim time next is 1551000.0000, 
raw observation next is [5.683333333333334, 81.0, 0.0, 0.0, 26.0, 25.58718684972305, 0.5499596143852924, 0.0, 1.0, 84581.27346847337], 
processed observation next is [1.0, 0.9565217391304348, 0.6200369344413666, 0.81, 0.0, 0.0, 0.6666666666666666, 0.6322655708102541, 0.6833198714617641, 0.0, 1.0, 0.4027679688974923], 
reward next is 0.5972, 
noisyNet noise sample is [array([0.6711196], dtype=float32), 1.7758679]. 
=============================================
[2019-04-04 02:00:06,058] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[95.42007]
 [95.20075]
 [94.93832]
 [94.8865 ]
 [94.92408]], R is [[95.16046906]
 [95.0486908 ]
 [95.09820557]
 [95.14722443]
 [95.195755  ]].
[2019-04-04 02:00:06,777] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3094987e-25 3.0156126e-20 1.4240080e-19 1.0000000e+00 1.1393563e-21
 3.7552694e-19 4.3515428e-17], sum to 1.0000
[2019-04-04 02:00:06,778] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8118
[2019-04-04 02:00:06,835] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 69.0, 51.0, 59.99999999999999, 26.0, 24.87415838690399, 0.2868335380524897, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2364000.0000, 
sim time next is 2364600.0000, 
raw observation next is [-3.4, 69.0, 64.99999999999999, 120.0, 26.0, 25.20003486853917, 0.3143963787703304, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.368421052631579, 0.69, 0.21666666666666662, 0.13259668508287292, 0.6666666666666666, 0.6000029057115975, 0.6047987929234434, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.22997865], dtype=float32), 0.136012]. 
=============================================
[2019-04-04 02:00:12,023] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.1822562e-27 4.4792187e-19 2.4129479e-20 1.0000000e+00 6.9557545e-19
 3.6437207e-18 5.2791184e-19], sum to 1.0000
[2019-04-04 02:00:12,024] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3706
[2019-04-04 02:00:12,084] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 64.0, 90.0, 172.5, 26.0, 25.65813922169716, 0.3735552940672924, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2794800.0000, 
sim time next is 2795400.0000, 
raw observation next is [-6.0, 64.0, 108.0, 207.0, 26.0, 25.80593576355875, 0.3885181985437362, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.296398891966759, 0.64, 0.36, 0.2287292817679558, 0.6666666666666666, 0.6504946469632292, 0.6295060661812454, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0496769], dtype=float32), -0.59696996]. 
=============================================
[2019-04-04 02:00:12,949] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.5798080e-20 1.3473254e-13 2.1759023e-14 1.0000000e+00 2.3908358e-15
 7.5610736e-13 5.6578298e-14], sum to 1.0000
[2019-04-04 02:00:12,949] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3202
[2019-04-04 02:00:12,981] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.3, 65.0, 0.0, 0.0, 25.0, 24.22845943253765, 0.1251914598275152, 0.0, 1.0, 39414.44724910602], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 2332800.0000, 
sim time next is 2333400.0000, 
raw observation next is [-2.3, 64.5, 0.0, 0.0, 25.0, 24.2822646530788, 0.1221583067623283, 0.0, 1.0, 39335.44806598167], 
processed observation next is [0.0, 0.0, 0.3988919667590028, 0.645, 0.0, 0.0, 0.5833333333333334, 0.5235220544232334, 0.5407194355874427, 0.0, 1.0, 0.18731165745705558], 
reward next is 0.8127, 
noisyNet noise sample is [array([-0.04310877], dtype=float32), 0.13206305]. 
=============================================
[2019-04-04 02:00:16,260] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.9933553e-26 4.8354554e-19 2.2129355e-21 1.0000000e+00 3.5733520e-19
 8.0243741e-18 2.7364626e-19], sum to 1.0000
[2019-04-04 02:00:16,260] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2437
[2019-04-04 02:00:16,322] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.616666666666667, 55.66666666666667, 0.0, 0.0, 26.0, 24.9455503666379, 0.3611385520096936, 1.0, 1.0, 41542.26779398629], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2317800.0000, 
sim time next is 2318400.0000, 
raw observation next is [-1.7, 56.0, 0.0, 0.0, 26.0, 25.02004263911792, 0.3640143742357831, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.4155124653739613, 0.56, 0.0, 0.0, 0.6666666666666666, 0.5850035532598268, 0.6213381247452611, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5838245], dtype=float32), -0.41526586]. 
=============================================
[2019-04-04 02:00:18,768] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.4146241e-28 1.9218880e-24 2.8862214e-22 1.0000000e+00 6.2270246e-24
 1.7940852e-23 2.1560717e-22], sum to 1.0000
[2019-04-04 02:00:18,770] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8427
[2019-04-04 02:00:18,806] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.8, 34.33333333333334, 0.0, 0.0, 25.0, 24.32835577484642, 0.05794888909055427, 0.0, 1.0, 42111.49392765845], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 2500800.0000, 
sim time next is 2501400.0000, 
raw observation next is [-0.7, 34.66666666666666, 0.0, 0.0, 25.0, 24.33752599935328, 0.05859635989919409, 0.0, 1.0, 41355.7965052567], 
processed observation next is [0.0, 0.9565217391304348, 0.443213296398892, 0.34666666666666657, 0.0, 0.0, 0.5833333333333334, 0.5281271666127733, 0.519532119966398, 0.0, 1.0, 0.1969323643107462], 
reward next is 0.8031, 
noisyNet noise sample is [array([0.2824174], dtype=float32), -0.16729812]. 
=============================================
[2019-04-04 02:00:49,802] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.9555862e-25 6.8745972e-16 2.3530220e-17 1.0000000e+00 1.8649258e-17
 1.4781895e-18 1.7712486e-17], sum to 1.0000
[2019-04-04 02:00:49,802] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5536
[2019-04-04 02:00:49,841] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.666666666666667, 36.5, 201.6666666666667, 514.3333333333334, 25.0, 24.75665638861731, 0.2317245265762565, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 2811000.0000, 
sim time next is 2811600.0000, 
raw observation next is [4.0, 35.0, 213.5, 429.0, 25.0, 24.84990667035049, 0.2167405077040806, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5734072022160666, 0.35, 0.7116666666666667, 0.4740331491712707, 0.5833333333333334, 0.5708255558625407, 0.5722468359013603, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.55737317], dtype=float32), 0.47958142]. 
=============================================
[2019-04-04 02:00:58,354] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.0516235e-22 2.6891776e-14 7.9080220e-17 1.0000000e+00 2.4674538e-15
 1.3082172e-15 1.2201364e-16], sum to 1.0000
[2019-04-04 02:00:58,355] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9754
[2019-04-04 02:00:58,385] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.5, 45.66666666666667, 205.8333333333333, 142.6666666666667, 26.0, 25.93184623880664, 0.4650844291268831, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2644800.0000, 
sim time next is 2645400.0000, 
raw observation next is [0.5, 46.33333333333334, 195.6666666666667, 155.3333333333333, 26.0, 25.26189664350427, 0.4067736895288014, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.4764542936288089, 0.46333333333333343, 0.6522222222222224, 0.17163904235727434, 0.6666666666666666, 0.6051580536253557, 0.6355912298429338, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.0853264], dtype=float32), -0.57948864]. 
=============================================
[2019-04-04 02:01:04,110] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0990239e-28 5.7557619e-24 1.1025425e-21 1.0000000e+00 4.7012629e-25
 3.1523488e-22 1.4612390e-21], sum to 1.0000
[2019-04-04 02:01:04,110] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7598
[2019-04-04 02:01:04,133] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.166666666666667, 82.83333333333333, 0.0, 0.0, 26.0, 24.30778521550132, 0.1972415786442331, 0.0, 1.0, 42805.36896220028], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2956200.0000, 
sim time next is 2956800.0000, 
raw observation next is [-3.333333333333333, 81.66666666666667, 0.0, 0.0, 26.0, 24.28868947834403, 0.1902970881082983, 0.0, 1.0, 42742.86915054311], 
processed observation next is [0.0, 0.21739130434782608, 0.37026777469990774, 0.8166666666666668, 0.0, 0.0, 0.6666666666666666, 0.5240574565286691, 0.5634323627027661, 0.0, 1.0, 0.2035374721454434], 
reward next is 0.7965, 
noisyNet noise sample is [array([0.24165829], dtype=float32), 0.66384375]. 
=============================================
[2019-04-04 02:01:09,887] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.6315257e-22 9.2822386e-13 9.0457003e-15 1.0000000e+00 7.5109369e-17
 1.0455200e-10 4.1031116e-14], sum to 1.0000
[2019-04-04 02:01:09,887] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7435
[2019-04-04 02:01:09,900] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.3333333333333333, 100.0, 0.0, 0.0, 26.0, 25.37810122374033, 0.5652728156259504, 0.0, 1.0, 61477.66684240185], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3205200.0000, 
sim time next is 3205800.0000, 
raw observation next is [-0.5, 100.0, 0.0, 0.0, 26.0, 25.47192588204118, 0.5806552254650426, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.44875346260387816, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6226604901700984, 0.6935517418216808, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.17016917], dtype=float32), 0.7136759]. 
=============================================
[2019-04-04 02:01:12,818] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.7729412e-22 4.5980438e-15 2.7550810e-15 1.0000000e+00 1.2569237e-17
 6.0988587e-15 1.8666290e-15], sum to 1.0000
[2019-04-04 02:01:12,819] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2726
[2019-04-04 02:01:12,838] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.0, 71.0, 0.0, 0.0, 26.0, 25.21184635120525, 0.4076151460485129, 0.0, 1.0, 45726.67213184173], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3366000.0000, 
sim time next is 3366600.0000, 
raw observation next is [-5.166666666666667, 72.0, 0.0, 0.0, 26.0, 25.17506689052341, 0.3996794711596801, 0.0, 1.0, 42964.41842749076], 
processed observation next is [1.0, 1.0, 0.31948291782086796, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5979222408769509, 0.63322649038656, 0.0, 1.0, 0.20459246870233697], 
reward next is 0.7954, 
noisyNet noise sample is [array([1.0468184], dtype=float32), -0.5024739]. 
=============================================
[2019-04-04 02:01:13,438] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.4770936e-28 1.0418453e-21 6.9248549e-20 1.0000000e+00 1.7137184e-23
 3.1042575e-22 3.8185371e-21], sum to 1.0000
[2019-04-04 02:01:13,442] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9368
[2019-04-04 02:01:13,464] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.833333333333334, 48.33333333333334, 0.0, 0.0, 26.0, 25.43179529458619, 0.4947437843137849, 0.0, 1.0, 65356.9893539507], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3966600.0000, 
sim time next is 3967200.0000, 
raw observation next is [-8.0, 49.0, 0.0, 0.0, 26.0, 25.40412687528584, 0.4935137333661784, 0.0, 1.0, 66469.90964386618], 
processed observation next is [1.0, 0.9565217391304348, 0.24099722991689754, 0.49, 0.0, 0.0, 0.6666666666666666, 0.6170105729404867, 0.6645045777887261, 0.0, 1.0, 0.31652337925650564], 
reward next is 0.6835, 
noisyNet noise sample is [array([-0.07207157], dtype=float32), 0.16541024]. 
=============================================
[2019-04-04 02:01:18,460] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.6829164e-24 2.6719260e-15 7.2806865e-16 1.0000000e+00 8.5179941e-16
 2.2692373e-13 5.5011440e-15], sum to 1.0000
[2019-04-04 02:01:18,461] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2946
[2019-04-04 02:01:18,483] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 77.83333333333334, 0.0, 0.0, 26.0, 25.43851858974375, 0.4267609960898474, 0.0, 1.0, 76676.66660035065], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3463800.0000, 
sim time next is 3464400.0000, 
raw observation next is [1.0, 76.66666666666667, 0.0, 0.0, 26.0, 25.40022484968677, 0.432552562632212, 0.0, 1.0, 77685.80177370834], 
processed observation next is [1.0, 0.08695652173913043, 0.4903047091412743, 0.7666666666666667, 0.0, 0.0, 0.6666666666666666, 0.616685404140564, 0.6441841875440707, 0.0, 1.0, 0.36993238939861117], 
reward next is 0.6301, 
noisyNet noise sample is [array([-0.8541319], dtype=float32), 0.32617626]. 
=============================================
[2019-04-04 02:01:41,482] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.9956715e-29 1.4652844e-23 1.9895716e-23 1.0000000e+00 6.4032581e-24
 9.7791005e-21 4.2695313e-22], sum to 1.0000
[2019-04-04 02:01:41,496] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3101
[2019-04-04 02:01:41,508] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.833333333333333, 47.83333333333333, 0.0, 0.0, 26.0, 25.41375571244374, 0.3641303149718509, 0.0, 1.0, 36283.40673527771], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4233000.0000, 
sim time next is 4233600.0000, 
raw observation next is [2.0, 48.0, 0.0, 0.0, 26.0, 25.42743128234252, 0.3628828566858481, 0.0, 1.0, 30641.03609891238], 
processed observation next is [0.0, 0.0, 0.518005540166205, 0.48, 0.0, 0.0, 0.6666666666666666, 0.6189526068618768, 0.620960952228616, 0.0, 1.0, 0.14590969570910658], 
reward next is 0.8541, 
noisyNet noise sample is [array([-0.05356431], dtype=float32), 0.30762312]. 
=============================================
[2019-04-04 02:01:42,684] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.9135713e-23 6.3249129e-14 4.2507273e-15 1.0000000e+00 3.4868545e-16
 1.9458846e-14 9.8776165e-16], sum to 1.0000
[2019-04-04 02:01:42,684] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8931
[2019-04-04 02:01:42,722] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.5, 100.0, 0.0, 0.0, 26.0, 25.72655120419344, 0.5773608833995897, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3202200.0000, 
sim time next is 3202800.0000, 
raw observation next is [0.3333333333333334, 100.0, 0.0, 0.0, 26.0, 25.64116936482147, 0.5514518330325914, 0.0, 1.0, 21926.45383433456], 
processed observation next is [1.0, 0.043478260869565216, 0.4718374884579871, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6367641137351224, 0.6838172776775305, 0.0, 1.0, 0.10441168492540266], 
reward next is 0.8956, 
noisyNet noise sample is [array([-0.11698812], dtype=float32), -0.19503298]. 
=============================================
[2019-04-04 02:01:43,448] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.2752735e-25 4.8924565e-18 8.6326728e-18 1.0000000e+00 1.0012003e-19
 1.0352003e-16 3.5317233e-18], sum to 1.0000
[2019-04-04 02:01:43,449] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3223
[2019-04-04 02:01:43,476] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 56.66666666666667, 0.0, 0.0, 25.0, 24.62574144880063, 0.249762032015372, 0.0, 1.0, 52859.9270093612], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 3882000.0000, 
sim time next is 3882600.0000, 
raw observation next is [-1.0, 57.5, 0.0, 0.0, 25.0, 24.50962722945524, 0.2432615514373603, 0.0, 1.0, 100961.896413432], 
processed observation next is [1.0, 0.9565217391304348, 0.4349030470914128, 0.575, 0.0, 0.0, 0.5833333333333334, 0.5424689357879368, 0.5810871838124535, 0.0, 1.0, 0.4807709353020571], 
reward next is 0.5192, 
noisyNet noise sample is [array([-1.3072687], dtype=float32), -0.60136646]. 
=============================================
[2019-04-04 02:01:51,983] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.6746223e-27 1.0229009e-18 3.8995778e-19 1.0000000e+00 7.4924921e-21
 1.4395474e-18 4.6317268e-20], sum to 1.0000
[2019-04-04 02:01:51,983] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8631
[2019-04-04 02:01:52,011] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 55.33333333333334, 0.0, 0.0, 26.0, 25.57413269457686, 0.5490677564828129, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4663200.0000, 
sim time next is 4663800.0000, 
raw observation next is [2.0, 54.5, 0.0, 0.0, 26.0, 25.63898553754083, 0.5413585299909941, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.518005540166205, 0.545, 0.0, 0.0, 0.6666666666666666, 0.6365821281284024, 0.6804528433303313, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.8910489], dtype=float32), -2.2985506]. 
=============================================
[2019-04-04 02:01:58,440] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4964924e-26 1.8198496e-17 5.1915295e-18 1.0000000e+00 8.5825530e-21
 2.9624613e-17 5.5353156e-20], sum to 1.0000
[2019-04-04 02:01:58,442] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7385
[2019-04-04 02:01:58,475] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 61.0, 0.0, 0.0, 26.0, 25.42767133070242, 0.4480316208311133, 0.0, 1.0, 131147.4985439373], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4573800.0000, 
sim time next is 4574400.0000, 
raw observation next is [1.0, 61.0, 0.0, 0.0, 26.0, 25.29474195619381, 0.4516449188815327, 0.0, 1.0, 112359.0970287582], 
processed observation next is [1.0, 0.9565217391304348, 0.4903047091412743, 0.61, 0.0, 0.0, 0.6666666666666666, 0.6078951630161509, 0.6505483062938442, 0.0, 1.0, 0.5350433191845628], 
reward next is 0.4650, 
noisyNet noise sample is [array([0.8932076], dtype=float32), 0.7012789]. 
=============================================
[2019-04-04 02:01:59,175] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0220040e-25 2.7348179e-16 6.6778870e-19 1.0000000e+00 6.6826561e-18
 3.7465879e-17 6.4838723e-19], sum to 1.0000
[2019-04-04 02:01:59,176] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5302
[2019-04-04 02:01:59,208] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.6666666666666667, 72.83333333333334, 74.00000000000001, 44.00000000000001, 26.0, 25.43738115246401, 0.4017440790052225, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4522200.0000, 
sim time next is 4522800.0000, 
raw observation next is [-0.5333333333333334, 72.66666666666667, 92.5, 55.0, 26.0, 25.34087020829562, 0.4544397903469679, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.44783010156971376, 0.7266666666666667, 0.30833333333333335, 0.06077348066298342, 0.6666666666666666, 0.611739184024635, 0.651479930115656, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2408879], dtype=float32), -1.3529751]. 
=============================================
[2019-04-04 02:02:00,419] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.7969369e-29 9.4785710e-25 1.9167795e-22 1.0000000e+00 1.3549745e-24
 3.0840725e-23 1.2632511e-22], sum to 1.0000
[2019-04-04 02:02:00,419] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7532
[2019-04-04 02:02:00,469] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.333333333333333, 41.66666666666667, 105.3333333333333, 631.3333333333333, 25.0, 24.31046384763934, 0.1623103891305283, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 4180800.0000, 
sim time next is 4181400.0000, 
raw observation next is [-3.0, 40.0, 108.0, 660.0, 25.0, 24.32107603700731, 0.1608046644850888, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.3795013850415513, 0.4, 0.36, 0.7292817679558011, 0.5833333333333334, 0.5267563364172757, 0.5536015548283629, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.41579187], dtype=float32), 1.7720073]. 
=============================================
[2019-04-04 02:02:02,543] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.247999e-28 9.590655e-20 9.107165e-21 1.000000e+00 3.574231e-23
 4.229818e-20 3.200393e-21], sum to 1.0000
[2019-04-04 02:02:02,543] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3816
[2019-04-04 02:02:02,553] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 46.33333333333334, 118.5, 833.1666666666667, 26.0, 26.21260344145368, 0.5924621034514187, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3933600.0000, 
sim time next is 3934200.0000, 
raw observation next is [-6.0, 45.66666666666666, 118.0, 831.3333333333334, 26.0, 26.16722248611214, 0.595397268354774, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.296398891966759, 0.45666666666666655, 0.3933333333333333, 0.9186003683241253, 0.6666666666666666, 0.6806018738426785, 0.698465756118258, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6115845], dtype=float32), -1.3593627]. 
=============================================
[2019-04-04 02:02:03,418] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-04 02:02:03,419] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 02:02:03,420] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 02:02:03,420] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:02:03,420] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:02:03,420] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 02:02:03,421] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:02:03,426] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run32
[2019-04-04 02:02:03,448] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run32
[2019-04-04 02:02:03,466] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run32
[2019-04-04 02:02:38,881] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.2529688], dtype=float32), 0.5366058]
[2019-04-04 02:02:38,882] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [1.8311060005, 94.69005735666667, 22.52719959, 0.0, 20.0, 19.39511772484043, -1.065172889341175, 1.0, 1.0, 46602.86675069864]
[2019-04-04 02:02:38,882] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 02:02:38,883] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.6469094e-12 4.8882217e-08 3.3352057e-09 9.9930465e-01 6.9063366e-04
 4.6318687e-06 3.9458254e-08], sampled 0.2184292225311235
[2019-04-04 02:02:47,866] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.2529688], dtype=float32), 0.5366058]
[2019-04-04 02:02:47,866] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [2.833333333333333, 82.0, 0.0, 0.0, 20.5, 19.83603193027461, -0.8921494225843967, 0.0, 1.0, 18680.41167023054]
[2019-04-04 02:02:47,866] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 02:02:47,867] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.6429443e-16 6.1980078e-12 3.8683523e-12 1.0000000e+00 6.7352501e-10
 3.6333742e-10 3.2898809e-11], sampled 0.24998146838509927
[2019-04-04 02:03:07,912] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.2529688], dtype=float32), 0.5366058]
[2019-04-04 02:03:07,912] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [0.0, 56.0, 191.0, 280.3333333333333, 21.5, 20.84855904879348, -0.6091059087096474, 0.0, 1.0, 18681.41294449824]
[2019-04-04 02:03:07,913] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 02:03:07,914] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [7.2404360e-18 3.0247538e-13 6.8942253e-13 1.0000000e+00 1.0221243e-11
 1.6220091e-11 1.7459957e-12], sampled 0.482428770983501
[2019-04-04 02:04:06,534] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6536.9778 170222069.6682 -1207.3700
[2019-04-04 02:04:33,737] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 6773.2973 203249759.0845 -1468.4243
[2019-04-04 02:04:40,177] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5774.0093 205822838.2432 -1818.4580
[2019-04-04 02:04:41,234] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 3100000, evaluation results [3100000.0, 6773.297283372003, 203249759.08451438, -1468.424348808929, 6536.977760897018, 170222069.66822293, -1207.3699523546084, 5774.009304908485, 205822838.24316844, -1818.4579968349185]
[2019-04-04 02:05:02,262] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:05:02,263] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:05:02,279] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run24
[2019-04-04 02:05:02,595] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.7338522e-24 2.4288802e-11 7.0918463e-16 1.0000000e+00 2.4366255e-14
 9.1739608e-16 4.8682731e-18], sum to 1.0000
[2019-04-04 02:05:02,595] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3983
[2019-04-04 02:05:02,673] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.0, 100.0, 111.0, 775.5, 26.0, 27.03811818645765, 0.7432047144697735, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3150000.0000, 
sim time next is 3150600.0000, 
raw observation next is [7.166666666666667, 98.83333333333334, 112.0, 785.3333333333334, 26.0, 27.0760306319703, 0.75737998897129, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.6611265004616806, 0.9883333333333334, 0.37333333333333335, 0.8677716390423573, 0.6666666666666666, 0.7563358859975251, 0.7524599963237634, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.83635336], dtype=float32), -0.5798648]. 
=============================================
[2019-04-04 02:05:11,197] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:05:11,197] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:05:11,207] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run24
[2019-04-04 02:05:27,430] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2211611e-23 6.5234745e-17 2.8647528e-16 1.0000000e+00 3.8079734e-15
 2.4633885e-17 1.9590171e-16], sum to 1.0000
[2019-04-04 02:05:27,430] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4306
[2019-04-04 02:05:27,493] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 25.0, 24.49798320993453, 0.2443272417818532, 0.0, 1.0, 22152.86791522432], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 4473600.0000, 
sim time next is 4474200.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 25.0, 24.3173199612182, 0.2280560229901992, 0.0, 1.0, 21937.60913250617], 
processed observation next is [1.0, 0.782608695652174, 0.46260387811634357, 0.72, 0.0, 0.0, 0.5833333333333334, 0.5264433301015167, 0.5760186743300664, 0.0, 1.0, 0.10446480539288654], 
reward next is 0.8955, 
noisyNet noise sample is [array([-1.3417479], dtype=float32), -0.7671833]. 
=============================================
[2019-04-04 02:05:35,176] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:05:35,176] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:05:35,180] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run24
[2019-04-04 02:05:40,278] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:05:40,278] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:05:40,313] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run24
[2019-04-04 02:05:45,894] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.01594697e-27 1.12290120e-18 6.92826319e-20 1.00000000e+00
 1.01528696e-22 2.73179276e-17 3.68145423e-20], sum to 1.0000
[2019-04-04 02:05:45,897] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2090
[2019-04-04 02:05:45,934] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 57.0, 0.0, 0.0, 26.0, 25.93458712388942, 0.6262603492148673, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4658400.0000, 
sim time next is 4659000.0000, 
raw observation next is [2.0, 57.0, 0.0, 0.0, 26.0, 25.89551557420434, 0.5633203696890312, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.518005540166205, 0.57, 0.0, 0.0, 0.6666666666666666, 0.6579596311836949, 0.6877734565630105, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9437852], dtype=float32), -0.21113135]. 
=============================================
[2019-04-04 02:05:45,961] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[87.683266]
 [87.437546]
 [87.11401 ]
 [86.79295 ]
 [86.92422 ]], R is [[87.7769928 ]
 [87.89922333]
 [88.02023315]
 [88.14002991]
 [88.25862885]].
[2019-04-04 02:05:51,869] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.3317869e-28 1.6912802e-23 1.0026578e-24 1.0000000e+00 6.2336251e-26
 8.2635858e-22 1.4958791e-22], sum to 1.0000
[2019-04-04 02:05:51,869] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8281
[2019-04-04 02:05:51,900] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 39.0, 0.0, 0.0, 26.0, 25.40524355532926, 0.3508154131531461, 0.0, 1.0, 47811.27348567296], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4921200.0000, 
sim time next is 4921800.0000, 
raw observation next is [0.1666666666666667, 39.16666666666666, 0.0, 0.0, 26.0, 25.38300693777585, 0.3488144766100822, 0.0, 1.0, 52737.42784691903], 
processed observation next is [0.0, 1.0, 0.4672206832871654, 0.39166666666666655, 0.0, 0.0, 0.6666666666666666, 0.6152505781479874, 0.6162714922033607, 0.0, 1.0, 0.25113060879485255], 
reward next is 0.7489, 
noisyNet noise sample is [array([1.0209962], dtype=float32), -0.057845794]. 
=============================================
[2019-04-04 02:06:01,207] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.3689205e-30 2.8966212e-23 1.3395220e-22 1.0000000e+00 1.1076206e-24
 3.6518520e-23 2.8757246e-23], sum to 1.0000
[2019-04-04 02:06:01,208] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4143
[2019-04-04 02:06:01,217] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 37.0, 148.0, 742.0, 25.0, 24.26303874965978, 0.2317564994871084, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 4804200.0000, 
sim time next is 4804800.0000, 
raw observation next is [3.0, 37.0, 139.5, 739.5, 25.0, 24.26853140163046, 0.2302254955654099, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.5457063711911359, 0.37, 0.465, 0.8171270718232044, 0.5833333333333334, 0.5223776168025385, 0.5767418318551366, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2200246], dtype=float32), 0.53691894]. 
=============================================
[2019-04-04 02:06:05,346] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.2952875e-32 8.0532591e-26 2.1350096e-25 1.0000000e+00 1.0509672e-27
 9.7994176e-28 1.8949163e-24], sum to 1.0000
[2019-04-04 02:06:05,346] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0919
[2019-04-04 02:06:05,376] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 35.5, 82.0, 549.0, 26.0, 25.17694820547879, 0.4100582238010348, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4811400.0000, 
sim time next is 4812000.0000, 
raw observation next is [3.0, 35.0, 73.83333333333334, 488.3333333333333, 26.0, 25.17458941420186, 0.4003062401565565, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.5457063711911359, 0.35, 0.24611111111111114, 0.5395948434622467, 0.6666666666666666, 0.5978824511834885, 0.6334354133855188, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.00744284], dtype=float32), -1.6214716]. 
=============================================
[2019-04-04 02:06:05,422] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[84.78653 ]
 [84.85402 ]
 [84.918526]
 [84.940445]
 [84.96669 ]], R is [[84.51766968]
 [84.67249298]
 [84.82576752]
 [84.97750854]
 [85.12773132]].
[2019-04-04 02:06:08,765] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:06:08,765] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:06:08,784] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run24
[2019-04-04 02:06:13,977] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:06:13,977] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:06:13,993] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run24
[2019-04-04 02:06:15,042] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:06:15,042] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:06:15,050] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run24
[2019-04-04 02:06:20,207] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:06:20,207] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:06:20,236] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run24
[2019-04-04 02:06:23,479] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.9153869e-16 1.9960531e-11 7.7655660e-12 1.0000000e+00 1.5338615e-10
 1.5208789e-08 2.4385877e-10], sum to 1.0000
[2019-04-04 02:06:23,480] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8275
[2019-04-04 02:06:23,515] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-14.91666666666667, 69.0, 0.0, 0.0, 23.0, 21.34309978150435, -0.61889868235773, 0.0, 1.0, 49682.29764489207], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 352200.0000, 
sim time next is 352800.0000, 
raw observation next is [-15.0, 69.0, 0.0, 0.0, 23.0, 21.34597919016992, -0.629343516383665, 0.0, 1.0, 49854.35415719918], 
processed observation next is [1.0, 0.08695652173913043, 0.04709141274238226, 0.69, 0.0, 0.0, 0.4166666666666667, 0.27883159918082673, 0.2902188278721117, 0.0, 1.0, 0.23740168646285323], 
reward next is 0.7626, 
noisyNet noise sample is [array([-0.28159142], dtype=float32), -0.07606324]. 
=============================================
[2019-04-04 02:06:23,530] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:06:23,531] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:06:23,544] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run24
[2019-04-04 02:06:24,545] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:06:24,545] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:06:24,551] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run24
[2019-04-04 02:06:26,570] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:06:26,570] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:06:26,572] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run24
[2019-04-04 02:06:34,675] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.2492290e-17 4.2610829e-08 5.8394872e-10 9.9999964e-01 2.1912608e-07
 1.2975897e-07 6.6368598e-11], sum to 1.0000
[2019-04-04 02:06:34,676] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8582
[2019-04-04 02:06:34,694] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 72.0, 123.5, 5.5, 26.0, 26.16204309770681, 0.5469098334181823, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4723200.0000, 
sim time next is 4723800.0000, 
raw observation next is [1.0, 72.0, 115.6666666666667, 7.333333333333334, 26.0, 26.17579651053349, 0.547564092957279, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.4903047091412743, 0.72, 0.38555555555555565, 0.008103130755064457, 0.6666666666666666, 0.6813163758777909, 0.682521364319093, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.243498], dtype=float32), -0.33362925]. 
=============================================
[2019-04-04 02:06:37,362] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:06:37,376] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:06:37,379] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run24
[2019-04-04 02:06:37,778] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.8599287e-19 7.0551637e-12 4.9619111e-12 1.0000000e+00 6.2905556e-13
 7.9609928e-09 5.6685044e-13], sum to 1.0000
[2019-04-04 02:06:37,778] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0389
[2019-04-04 02:06:37,873] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.733333333333334, 78.0, 28.33333333333334, 249.6666666666667, 24.0, 23.47103148440124, -0.176034672395831, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 202800.0000, 
sim time next is 203400.0000, 
raw observation next is [-8.65, 78.0, 34.0, 296.0, 24.0, 23.58569460866467, -0.1600991353866736, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.22299168975069253, 0.78, 0.11333333333333333, 0.3270718232044199, 0.5, 0.4654745507220559, 0.44663362153777547, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3783555], dtype=float32), 1.595132]. 
=============================================
[2019-04-04 02:06:39,459] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.0437384e-19 1.5301218e-13 4.4091683e-12 1.0000000e+00 1.3832613e-11
 2.1069599e-09 4.7294243e-13], sum to 1.0000
[2019-04-04 02:06:39,461] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3783
[2019-04-04 02:06:39,546] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 70.83333333333333, 143.3333333333333, 0.0, 24.0, 23.80096569047289, -0.1489298484707787, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 213000.0000, 
sim time next is 213600.0000, 
raw observation next is [-5.800000000000001, 69.66666666666667, 148.1666666666667, 0.0, 24.0, 23.80585386475745, -0.1565077554156363, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.30193905817174516, 0.6966666666666668, 0.49388888888888904, 0.0, 0.5, 0.48382115539645404, 0.4478307481947879, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.77489966], dtype=float32), -0.6989148]. 
=============================================
[2019-04-04 02:06:46,193] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.0137563e-28 5.9444001e-19 2.5800888e-20 1.0000000e+00 8.0250312e-22
 1.2748278e-19 5.4200561e-21], sum to 1.0000
[2019-04-04 02:06:46,193] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7711
[2019-04-04 02:06:46,227] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.0, 26.33333333333333, 122.1666666666667, 848.3333333333334, 26.0, 26.90878250438841, 0.6818391728226301, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4966800.0000, 
sim time next is 4967400.0000, 
raw observation next is [5.5, 25.66666666666667, 122.3333333333333, 851.6666666666666, 26.0, 26.98872982944966, 0.6890056485149555, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.6149584487534627, 0.2566666666666667, 0.4077777777777777, 0.9410681399631675, 0.6666666666666666, 0.749060819120805, 0.7296685495049852, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.001076], dtype=float32), -0.66306704]. 
=============================================
[2019-04-04 02:06:58,640] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:06:58,640] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:06:58,644] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run24
[2019-04-04 02:07:03,803] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2564289e-26 1.5379024e-16 1.2067819e-16 1.0000000e+00 9.1661114e-19
 8.7973064e-16 3.5172412e-19], sum to 1.0000
[2019-04-04 02:07:03,804] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5298
[2019-04-04 02:07:03,844] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 41.0, 114.0, 753.5, 26.0, 26.96858947614495, 0.7189835981592191, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5047200.0000, 
sim time next is 5047800.0000, 
raw observation next is [3.333333333333333, 40.16666666666667, 114.6666666666667, 772.0, 26.0, 27.03494127444265, 0.740532477539836, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.5549399815327793, 0.4016666666666667, 0.38222222222222235, 0.8530386740331491, 0.6666666666666666, 0.7529117728702209, 0.7468441591799454, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.41881466], dtype=float32), 1.2802124]. 
=============================================
[2019-04-04 02:07:03,906] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1966497e-18 1.4450805e-13 1.3961819e-13 1.0000000e+00 1.5019304e-12
 7.1590837e-11 5.0180073e-12], sum to 1.0000
[2019-04-04 02:07:03,906] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2036
[2019-04-04 02:07:03,929] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 23.0, 21.03932459759837, -0.6806590524409248, 0.0, 1.0, 46154.60337039026], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 196200.0000, 
sim time next is 196800.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 23.0, 21.00075759667542, -0.6839143858704628, 0.0, 1.0, 46160.15166834382], 
processed observation next is [1.0, 0.2608695652173913, 0.21606648199445982, 0.78, 0.0, 0.0, 0.4166666666666667, 0.250063133056285, 0.27202853804317906, 0.0, 1.0, 0.21981024603973245], 
reward next is 0.7802, 
noisyNet noise sample is [array([-1.1066266], dtype=float32), 0.16218896]. 
=============================================
[2019-04-04 02:07:08,433] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:07:08,433] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:07:08,436] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run24
[2019-04-04 02:07:10,135] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.5130111e-23 7.1570413e-17 1.3413816e-17 1.0000000e+00 1.8348595e-15
 9.0203393e-15 3.6968839e-17], sum to 1.0000
[2019-04-04 02:07:10,135] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2087
[2019-04-04 02:07:10,183] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.9, 84.83333333333334, 55.66666666666666, 0.0, 25.5, 25.803076007932, 0.3347358397360368, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.5], 
sim time this is 831000.0000, 
sim time next is 831600.0000, 
raw observation next is [-3.9, 86.0, 54.0, 0.0, 25.5, 25.77458879939646, 0.2300390805388362, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.3545706371191136, 0.86, 0.18, 0.0, 0.625, 0.6478823999497051, 0.5766796935129453, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.1351513], dtype=float32), -1.3533063]. 
=============================================
[2019-04-04 02:07:16,212] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.2585181e-19 1.2991865e-14 7.3012058e-14 1.0000000e+00 2.1538269e-15
 2.6600694e-10 1.6068729e-13], sum to 1.0000
[2019-04-04 02:07:16,212] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8188
[2019-04-04 02:07:16,230] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.8, 83.0, 0.0, 0.0, 24.0, 23.37345501985725, -0.1126484905412061, 0.0, 1.0, 48747.44620148902], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 597000.0000, 
sim time next is 597600.0000, 
raw observation next is [-2.8, 83.0, 0.0, 0.0, 24.0, 23.36185101235284, -0.1146590992575427, 0.0, 1.0, 47738.31681223087], 
processed observation next is [0.0, 0.9565217391304348, 0.38504155124653744, 0.83, 0.0, 0.0, 0.5, 0.44682091769607, 0.4617803002474858, 0.0, 1.0, 0.22732531815348034], 
reward next is 0.7727, 
noisyNet noise sample is [array([0.5267766], dtype=float32), -1.079576]. 
=============================================
[2019-04-04 02:07:21,161] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.1848236e-24 5.1022346e-19 9.5771695e-19 1.0000000e+00 9.7025223e-20
 1.8637444e-18 5.9139397e-17], sum to 1.0000
[2019-04-04 02:07:21,165] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9266
[2019-04-04 02:07:21,206] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.8, 87.0, 0.0, 0.0, 26.0, 24.97744541989386, 0.2886836342185783, 0.0, 1.0, 32861.29546450658], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 580200.0000, 
sim time next is 580800.0000, 
raw observation next is [-1.9, 87.0, 0.0, 0.0, 26.0, 24.95752770837903, 0.2836246859385762, 0.0, 1.0, 48345.07047203682], 
processed observation next is [0.0, 0.7391304347826086, 0.4099722991689751, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5797939756982524, 0.5945415619795255, 0.0, 1.0, 0.23021462129541345], 
reward next is 0.7698, 
noisyNet noise sample is [array([1.0139657], dtype=float32), -0.45315707]. 
=============================================
[2019-04-04 02:07:23,906] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.8084625e-19 2.0110878e-10 2.1182380e-13 1.0000000e+00 2.0465465e-12
 7.5524081e-10 9.7572850e-13], sum to 1.0000
[2019-04-04 02:07:23,907] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0257
[2019-04-04 02:07:23,914] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 72.0, 147.0, 0.0, 26.0, 25.99979859419356, 0.5363805425796829, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4721400.0000, 
sim time next is 4722000.0000, 
raw observation next is [1.0, 72.0, 139.1666666666667, 1.833333333333333, 26.0, 26.07344833355433, 0.5454160371551068, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.4903047091412743, 0.72, 0.4638888888888891, 0.002025782688766114, 0.6666666666666666, 0.6727873611295276, 0.6818053457183689, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.91755134], dtype=float32), 0.55894053]. 
=============================================
[2019-04-04 02:07:23,926] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[87.73765 ]
 [87.76046 ]
 [87.726036]
 [87.53109 ]
 [87.25317 ]], R is [[87.78272247]
 [87.9048996 ]
 [88.02584839]
 [88.08628845]
 [88.10614014]].
[2019-04-04 02:07:34,819] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.9572870e-19 9.4127031e-14 2.3276533e-14 1.0000000e+00 2.8410542e-13
 3.4935119e-12 2.1875759e-13], sum to 1.0000
[2019-04-04 02:07:34,819] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7602
[2019-04-04 02:07:34,846] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.516666666666667, 82.50000000000001, 0.0, 0.0, 23.0, 22.42512310760807, -0.3380311947505589, 0.0, 1.0, 43231.87045828949], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 533400.0000, 
sim time next is 534000.0000, 
raw observation next is [2.333333333333333, 83.0, 0.0, 0.0, 23.0, 22.43693032031414, -0.3378455629237898, 0.0, 1.0, 35316.98814211797], 
processed observation next is [0.0, 0.17391304347826086, 0.5272391505078486, 0.83, 0.0, 0.0, 0.4166666666666667, 0.36974419335951164, 0.38738481235873673, 0.0, 1.0, 0.16817613401008555], 
reward next is 0.8318, 
noisyNet noise sample is [array([1.8406765], dtype=float32), -0.55558634]. 
=============================================
[2019-04-04 02:07:34,871] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[82.2934  ]
 [82.444756]
 [82.63567 ]
 [82.83958 ]
 [83.02946 ]], R is [[82.21581268]
 [82.18778992]
 [82.18557739]
 [82.2049408 ]
 [82.22723389]].
[2019-04-04 02:07:36,660] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.2899811e-32 2.9474885e-19 7.3021888e-23 1.0000000e+00 7.3504191e-25
 4.1759086e-24 3.2404471e-25], sum to 1.0000
[2019-04-04 02:07:36,665] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8493
[2019-04-04 02:07:36,687] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [15.33333333333333, 94.0, 0.0, 0.0, 26.0, 23.61215950756455, 0.1713135729559166, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1225200.0000, 
sim time next is 1225800.0000, 
raw observation next is [15.25, 94.5, 0.0, 0.0, 26.0, 23.59566563072174, 0.1668010328278102, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.17391304347826086, 0.8850415512465375, 0.945, 0.0, 0.0, 0.6666666666666666, 0.46630546922681165, 0.5556003442759367, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.8348675], dtype=float32), 0.15025413]. 
=============================================
[2019-04-04 02:07:37,819] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.8717628e-26 7.7741081e-18 8.7617206e-20 1.0000000e+00 3.1206335e-20
 3.9325811e-17 4.6888480e-19], sum to 1.0000
[2019-04-04 02:07:37,820] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9795
[2019-04-04 02:07:37,884] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.2, 96.0, 0.0, 0.0, 26.0, 24.69242633965994, 0.4458269605610992, 0.0, 1.0, 30607.70002849129], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1278000.0000, 
sim time next is 1278600.0000, 
raw observation next is [7.016666666666667, 96.0, 0.0, 0.0, 26.0, 24.70140085406574, 0.4467818135828867, 0.0, 1.0, 27427.07149581432], 
processed observation next is [0.0, 0.8260869565217391, 0.656971375807941, 0.96, 0.0, 0.0, 0.6666666666666666, 0.5584500711721448, 0.6489272711942956, 0.0, 1.0, 0.13060510236102058], 
reward next is 0.8694, 
noisyNet noise sample is [array([-1.1464766], dtype=float32), 0.9558012]. 
=============================================
[2019-04-04 02:07:38,297] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:07:38,297] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:07:38,304] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run24
[2019-04-04 02:07:45,861] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.2948375e-21 7.8690343e-14 1.8854720e-15 1.0000000e+00 4.1444285e-14
 2.3404222e-13 2.0437748e-14], sum to 1.0000
[2019-04-04 02:07:45,865] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4690
[2019-04-04 02:07:45,888] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 84.0, 77.0, 0.0, 24.0, 24.15104715855363, -0.04046912628565255, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 901800.0000, 
sim time next is 902400.0000, 
raw observation next is [1.1, 84.0, 80.33333333333334, 0.0, 24.0, 24.09577386689556, -0.0431491799520685, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.49307479224376743, 0.84, 0.26777777777777784, 0.0, 0.5, 0.50798115557463, 0.48561694001597716, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.720725], dtype=float32), 1.6572733]. 
=============================================
[2019-04-04 02:07:47,598] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.4348990e-20 1.9706039e-13 7.0263340e-15 1.0000000e+00 1.2035971e-14
 7.7973836e-13 5.4889861e-15], sum to 1.0000
[2019-04-04 02:07:47,599] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2565
[2019-04-04 02:07:47,664] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.9, 79.0, 75.0, 0.0, 26.0, 26.31455670202186, 0.4597509814025882, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 828000.0000, 
sim time next is 828600.0000, 
raw observation next is [-3.9, 80.16666666666667, 69.66666666666666, 0.0, 26.0, 26.29003420585317, 0.4538696571460736, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3545706371191136, 0.8016666666666667, 0.2322222222222222, 0.0, 0.6666666666666666, 0.6908361838210976, 0.6512898857153578, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2116332], dtype=float32), -0.56037116]. 
=============================================
[2019-04-04 02:07:47,950] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:07:47,951] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:07:47,962] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run24
[2019-04-04 02:07:55,975] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.4734986e-24 4.5580540e-14 4.9291548e-18 1.0000000e+00 2.5392603e-16
 3.5500055e-14 4.8840889e-16], sum to 1.0000
[2019-04-04 02:07:55,976] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0189
[2019-04-04 02:07:55,988] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.366666666666667, 73.33333333333334, 0.0, 0.0, 26.0, 25.36000023253837, 0.5332449487966376, 0.0, 1.0, 196217.9094192961], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1542000.0000, 
sim time next is 1542600.0000, 
raw observation next is [7.45, 73.5, 0.0, 0.0, 26.0, 25.2889015452947, 0.5512503470103215, 0.0, 1.0, 196665.9549784136], 
processed observation next is [1.0, 0.8695652173913043, 0.6689750692520776, 0.735, 0.0, 0.0, 0.6666666666666666, 0.6074084621078916, 0.6837501156701071, 0.0, 1.0, 0.9365045475162552], 
reward next is 0.0635, 
noisyNet noise sample is [array([-0.60192233], dtype=float32), 1.4847767]. 
=============================================
[2019-04-04 02:07:56,338] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.1907730e-22 7.4691901e-14 4.5138526e-18 1.0000000e+00 1.7770099e-15
 6.9979466e-13 3.4520752e-15], sum to 1.0000
[2019-04-04 02:07:56,338] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7377
[2019-04-04 02:07:56,352] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.7, 80.5, 0.0, 0.0, 23.0, 22.58501519658003, -0.2301185589983425, 0.0, 1.0, 37754.70474033371], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 961800.0000, 
sim time next is 962400.0000, 
raw observation next is [7.7, 81.0, 0.0, 0.0, 23.0, 22.57360994312911, -0.218776087937848, 0.0, 1.0, 36897.22377712149], 
processed observation next is [1.0, 0.13043478260869565, 0.6759002770083103, 0.81, 0.0, 0.0, 0.4166666666666667, 0.38113416192742583, 0.42707463735405066, 0.0, 1.0, 0.17570106560534043], 
reward next is 0.8243, 
noisyNet noise sample is [array([1.3541992], dtype=float32), 0.8677098]. 
=============================================
[2019-04-04 02:07:59,096] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.7112142e-26 1.5052920e-19 5.9563137e-20 1.0000000e+00 1.8337824e-22
 3.3142758e-21 9.7996817e-20], sum to 1.0000
[2019-04-04 02:07:59,099] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2782
[2019-04-04 02:07:59,117] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.7, 98.66666666666666, 0.0, 0.0, 26.0, 25.40595400813864, 0.5806829950348217, 0.0, 1.0, 21194.14530122985], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1284000.0000, 
sim time next is 1284600.0000, 
raw observation next is [5.600000000000001, 99.33333333333334, 0.0, 0.0, 26.0, 25.45904850112069, 0.580081471552283, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.6177285318559558, 0.9933333333333334, 0.0, 0.0, 0.6666666666666666, 0.6215873750933909, 0.6933604905174277, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.13995236], dtype=float32), -0.34776154]. 
=============================================
[2019-04-04 02:08:23,761] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.8700752e-26 3.0792503e-17 1.7137153e-19 1.0000000e+00 8.3858496e-21
 4.9895656e-16 1.9430425e-19], sum to 1.0000
[2019-04-04 02:08:23,762] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5367
[2019-04-04 02:08:23,800] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.35, 92.0, 0.0, 0.0, 26.0, 25.35749252318352, 0.4782611250046575, 0.0, 1.0, 82172.61165732612], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1463400.0000, 
sim time next is 1464000.0000, 
raw observation next is [1.433333333333333, 92.0, 0.0, 0.0, 26.0, 25.30794689062104, 0.4796278918530416, 0.0, 1.0, 58693.29088416603], 
processed observation next is [1.0, 0.9565217391304348, 0.502308402585411, 0.92, 0.0, 0.0, 0.6666666666666666, 0.60899557421842, 0.6598759639510139, 0.0, 1.0, 0.27949186135317156], 
reward next is 0.7205, 
noisyNet noise sample is [array([-2.22729], dtype=float32), -0.56686395]. 
=============================================
[2019-04-04 02:08:23,808] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[92.92196]
 [92.74911]
 [92.53653]
 [92.29486]
 [92.16664]], R is [[93.04212189]
 [92.72040558]
 [92.50341797]
 [92.3240509 ]
 [92.20050049]].
[2019-04-04 02:08:25,741] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.3694267e-25 9.5703773e-19 6.0527901e-20 1.0000000e+00 3.4892177e-19
 1.9629284e-17 9.5253229e-19], sum to 1.0000
[2019-04-04 02:08:25,741] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8541
[2019-04-04 02:08:25,783] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.133333333333333, 87.33333333333334, 0.0, 0.0, 24.0, 23.36944329101587, -0.1157895159434391, 0.0, 1.0, 42797.83655469809], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 526800.0000, 
sim time next is 527400.0000, 
raw observation next is [4.05, 87.0, 0.0, 0.0, 24.0, 23.38424768411259, -0.1144049526972754, 0.0, 1.0, 34859.03258814794], 
processed observation next is [0.0, 0.08695652173913043, 0.5747922437673131, 0.87, 0.0, 0.0, 0.5, 0.4486873070093826, 0.4618650157675748, 0.0, 1.0, 0.16599539327689494], 
reward next is 0.8340, 
noisyNet noise sample is [array([-0.36832994], dtype=float32), 0.54253393]. 
=============================================
[2019-04-04 02:08:26,737] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.5459755e-21 3.0429717e-12 1.0178194e-14 1.0000000e+00 8.3212869e-14
 4.7527987e-10 4.4045218e-15], sum to 1.0000
[2019-04-04 02:08:26,737] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2946
[2019-04-04 02:08:26,766] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.7, 100.0, 37.33333333333334, 0.0, 26.0, 25.85765039031254, 0.4925774420071647, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1501800.0000, 
sim time next is 1502400.0000, 
raw observation next is [1.8, 100.0, 42.16666666666667, 0.0, 26.0, 25.86279567514272, 0.5054840467475219, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.5124653739612189, 1.0, 0.14055555555555557, 0.0, 0.6666666666666666, 0.6552329729285601, 0.668494682249174, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8567829], dtype=float32), -1.0341467]. 
=============================================
[2019-04-04 02:08:27,790] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.1475971e-25 6.2175554e-16 6.0394417e-18 1.0000000e+00 6.1221589e-18
 5.1378679e-16 7.5181859e-18], sum to 1.0000
[2019-04-04 02:08:27,791] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2091
[2019-04-04 02:08:27,807] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.2, 94.0, 0.0, 0.0, 25.0, 24.46518010656395, 0.2285266477424274, 0.0, 1.0, 57306.24114011285], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 1479600.0000, 
sim time next is 1480200.0000, 
raw observation next is [2.2, 94.33333333333334, 0.0, 0.0, 25.0, 24.39853248541711, 0.2271022989378156, 0.0, 1.0, 80382.7259504808], 
processed observation next is [1.0, 0.13043478260869565, 0.5235457063711911, 0.9433333333333335, 0.0, 0.0, 0.5833333333333334, 0.5332110404514259, 0.5757007663126051, 0.0, 1.0, 0.38277488547848], 
reward next is 0.6172, 
noisyNet noise sample is [array([0.7324798], dtype=float32), 0.5917387]. 
=============================================
[2019-04-04 02:08:28,744] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.8035293e-23 8.1351810e-17 1.2515745e-17 1.0000000e+00 3.8874286e-19
 1.8026670e-16 1.7701292e-15], sum to 1.0000
[2019-04-04 02:08:28,744] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4986
[2019-04-04 02:08:28,761] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.8, 72.33333333333334, 0.0, 0.0, 26.0, 25.16437717667366, 0.3264728079207216, 0.0, 1.0, 44971.19677206533], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2240400.0000, 
sim time next is 2241000.0000, 
raw observation next is [-5.9, 73.0, 0.0, 0.0, 26.0, 25.03862161999971, 0.3089199721377364, 0.0, 1.0, 44493.04987291373], 
processed observation next is [1.0, 0.9565217391304348, 0.2991689750692521, 0.73, 0.0, 0.0, 0.6666666666666666, 0.5865518016666424, 0.6029733240459122, 0.0, 1.0, 0.21187166606149396], 
reward next is 0.7881, 
noisyNet noise sample is [array([-1.6878463], dtype=float32), 2.125836]. 
=============================================
[2019-04-04 02:08:28,764] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[78.54734 ]
 [78.60136 ]
 [78.57585 ]
 [78.442665]
 [78.36498 ]], R is [[78.62495422]
 [78.6245575 ]
 [78.62473297]
 [78.62633514]
 [78.62734222]].
[2019-04-04 02:08:32,004] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2268570e-24 1.3204155e-15 1.9797140e-16 1.0000000e+00 6.5377212e-18
 3.6350080e-15 1.8286876e-17], sum to 1.0000
[2019-04-04 02:08:32,005] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4483
[2019-04-04 02:08:32,024] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.0, 81.50000000000001, 0.0, 0.0, 25.0, 24.71088634685571, 0.3484981549230509, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 1559400.0000, 
sim time next is 1560000.0000, 
raw observation next is [5.0, 81.0, 0.0, 0.0, 25.0, 24.7629358522637, 0.3474207014704838, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.6011080332409973, 0.81, 0.0, 0.0, 0.5833333333333334, 0.5635779876886415, 0.6158069004901613, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6413689], dtype=float32), -0.08412948]. 
=============================================
[2019-04-04 02:08:32,030] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[92.40828]
 [92.44782]
 [92.57088]
 [92.59916]
 [92.47813]], R is [[92.3557663 ]
 [92.43221283]
 [92.50788879]
 [92.49359894]
 [92.45620728]].
[2019-04-04 02:08:37,435] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.8857878e-25 5.5996638e-21 4.8121200e-20 1.0000000e+00 1.4149630e-22
 2.1694134e-21 4.1278215e-18], sum to 1.0000
[2019-04-04 02:08:37,436] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4811
[2019-04-04 02:08:37,468] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 83.0, 0.0, 0.0, 26.0, 25.12511857384198, 0.3301090301542864, 0.0, 1.0, 47430.06979155265], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1800000.0000, 
sim time next is 1800600.0000, 
raw observation next is [-4.583333333333333, 83.5, 0.0, 0.0, 26.0, 25.11125577106174, 0.3245919339243338, 0.0, 1.0, 46772.044272568], 
processed observation next is [0.0, 0.8695652173913043, 0.3356417359187443, 0.835, 0.0, 0.0, 0.6666666666666666, 0.5926046475884782, 0.6081973113081113, 0.0, 1.0, 0.22272402034556188], 
reward next is 0.7773, 
noisyNet noise sample is [array([1.1036575], dtype=float32), 1.3961234]. 
=============================================
[2019-04-04 02:08:41,680] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4497270e-24 7.5873788e-21 1.3404896e-20 1.0000000e+00 5.1159255e-23
 1.2430224e-20 2.9544679e-19], sum to 1.0000
[2019-04-04 02:08:41,680] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7765
[2019-04-04 02:08:41,693] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.133333333333334, 43.33333333333334, 0.0, 0.0, 26.0, 25.03540025296462, 0.245889039043744, 0.0, 1.0, 43025.30379586819], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2410800.0000, 
sim time next is 2411400.0000, 
raw observation next is [-4.316666666666666, 43.66666666666667, 0.0, 0.0, 26.0, 24.98795181696698, 0.2369958999456165, 0.0, 1.0, 43019.08349236519], 
processed observation next is [0.0, 0.9130434782608695, 0.34302862419205915, 0.4366666666666667, 0.0, 0.0, 0.6666666666666666, 0.5823293180805816, 0.5789986333152055, 0.0, 1.0, 0.20485277853507233], 
reward next is 0.7951, 
noisyNet noise sample is [array([-0.44873124], dtype=float32), -0.25761935]. 
=============================================
[2019-04-04 02:08:42,960] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.5795037e-20 1.0372999e-15 7.9761630e-16 1.0000000e+00 1.1206911e-14
 1.3441126e-12 8.4004791e-14], sum to 1.0000
[2019-04-04 02:08:42,960] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6853
[2019-04-04 02:08:42,971] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 91.0, 0.0, 0.0, 23.0, 22.48293270365287, -0.2245835194416854, 0.0, 1.0, 47438.2800143592], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 1738800.0000, 
sim time next is 1739400.0000, 
raw observation next is [-0.09999999999999999, 90.33333333333334, 0.0, 0.0, 23.0, 22.46823466816624, -0.2249607074982666, 0.0, 1.0, 51281.97580789066], 
processed observation next is [0.0, 0.13043478260869565, 0.4598337950138504, 0.9033333333333334, 0.0, 0.0, 0.4166666666666667, 0.37235288901385327, 0.42501309750057775, 0.0, 1.0, 0.24419988479947932], 
reward next is 0.7558, 
noisyNet noise sample is [array([1.133088], dtype=float32), 0.61215746]. 
=============================================
[2019-04-04 02:08:47,323] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.9194013e-25 4.8075673e-21 7.5428829e-20 1.0000000e+00 1.8994187e-23
 4.8099151e-21 1.3677642e-18], sum to 1.0000
[2019-04-04 02:08:47,323] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3483
[2019-04-04 02:08:47,352] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.916666666666667, 41.5, 0.0, 0.0, 26.0, 24.79448397288931, 0.1944933743134597, 0.0, 1.0, 43037.66713512431], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2415000.0000, 
sim time next is 2415600.0000, 
raw observation next is [-5.0, 41.0, 0.0, 0.0, 26.0, 24.7468772493741, 0.1853877234688391, 0.0, 1.0, 43062.23377450046], 
processed observation next is [0.0, 1.0, 0.32409972299168976, 0.41, 0.0, 0.0, 0.6666666666666666, 0.5622397707811752, 0.5617959078229463, 0.0, 1.0, 0.20505825606904982], 
reward next is 0.7949, 
noisyNet noise sample is [array([-0.0861244], dtype=float32), 1.2929158]. 
=============================================
[2019-04-04 02:08:52,563] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.5801416e-22 6.1298395e-13 7.9680660e-14 1.0000000e+00 3.5578110e-16
 2.1037694e-11 6.8506991e-15], sum to 1.0000
[2019-04-04 02:08:52,563] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1709
[2019-04-04 02:08:52,592] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 25.0, 24.39068121231429, 0.3033038457694295, 0.0, 1.0, 46157.52031476059], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 1383600.0000, 
sim time next is 1384200.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 25.0, 24.53321784623708, 0.3051713256680712, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.46260387811634357, 0.95, 0.0, 0.0, 0.5833333333333334, 0.5444348205197566, 0.6017237752226904, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.7382545], dtype=float32), 0.5070887]. 
=============================================
[2019-04-04 02:08:55,406] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.7438053e-23 7.7446611e-17 4.0068436e-16 1.0000000e+00 1.6365749e-18
 1.5312576e-13 6.7336112e-16], sum to 1.0000
[2019-04-04 02:08:55,407] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9859
[2019-04-04 02:08:55,432] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-9.5, 91.0, 0.0, 0.0, 25.0, 22.59500170237354, -0.3222337790298572, 0.0, 1.0, 44895.65592277602], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 1926000.0000, 
sim time next is 1926600.0000, 
raw observation next is [-9.5, 91.00000000000001, 0.0, 0.0, 25.0, 22.58046030436826, -0.3268568422260884, 0.0, 1.0, 44887.08491187033], 
processed observation next is [1.0, 0.30434782608695654, 0.1994459833795014, 0.9100000000000001, 0.0, 0.0, 0.5833333333333334, 0.38170502536402157, 0.3910477192579705, 0.0, 1.0, 0.21374802338985874], 
reward next is 0.7863, 
noisyNet noise sample is [array([0.5217561], dtype=float32), -0.5071099]. 
=============================================
[2019-04-04 02:08:57,126] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.5506778e-20 4.7320821e-11 1.2523692e-14 1.0000000e+00 6.9654484e-11
 1.3176138e-10 6.2856025e-13], sum to 1.0000
[2019-04-04 02:08:57,127] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6107
[2019-04-04 02:08:57,144] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.5, 92.0, 59.5, 0.0, 24.0, 24.02764237022436, 0.0861883358247379, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1677600.0000, 
sim time next is 1678200.0000, 
raw observation next is [1.433333333333333, 92.0, 61.66666666666667, 0.0, 24.0, 24.06193345375165, 0.08185870493398328, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.502308402585411, 0.92, 0.20555555555555557, 0.0, 0.5, 0.5051611211459708, 0.5272862349779944, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.39768547], dtype=float32), 0.4634549]. 
=============================================
[2019-04-04 02:09:04,819] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.2560556e-22 2.0995603e-11 1.0168177e-15 1.0000000e+00 2.0571159e-13
 5.7634597e-10 3.0282415e-16], sum to 1.0000
[2019-04-04 02:09:04,821] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8780
[2019-04-04 02:09:04,829] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [12.28333333333333, 86.0, 125.3333333333333, 0.0, 24.0, 24.74195798907385, 0.2176388869402927, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 994200.0000, 
sim time next is 994800.0000, 
raw observation next is [12.36666666666667, 86.0, 126.6666666666667, 0.0, 24.0, 24.78174630278943, 0.2191931339575365, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.8051708217913206, 0.86, 0.42222222222222233, 0.0, 0.5, 0.5651455252324524, 0.5730643779858455, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.592815], dtype=float32), 0.25500244]. 
=============================================
[2019-04-04 02:09:06,021] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.46691178e-23 1.04325056e-11 6.07627729e-18 1.00000000e+00
 6.19118933e-16 1.58637374e-11 6.41474727e-17], sum to 1.0000
[2019-04-04 02:09:06,048] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8021
[2019-04-04 02:09:06,055] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [8.833333333333334, 63.33333333333333, 202.6666666666667, 114.8333333333333, 25.0, 25.71998054832024, 0.4791965728905618, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 1593600.0000, 
sim time next is 1594200.0000, 
raw observation next is [9.116666666666667, 62.16666666666666, 205.3333333333333, 141.6666666666667, 25.0, 25.76517864788396, 0.4853435422999113, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.7151431209602955, 0.6216666666666666, 0.6844444444444443, 0.15653775322283614, 0.5833333333333334, 0.6470982206569967, 0.6617811807666371, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0024437], dtype=float32), 1.0673922]. 
=============================================
[2019-04-04 02:09:06,745] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.1073790e-29 1.7034520e-24 1.3955950e-22 1.0000000e+00 5.1918646e-26
 4.1293188e-25 2.6849962e-21], sum to 1.0000
[2019-04-04 02:09:06,746] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3539
[2019-04-04 02:09:06,808] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.166666666666666, 45.33333333333333, 63.5, 683.6666666666667, 26.0, 25.26730538794525, 0.2517483606985316, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2454000.0000, 
sim time next is 2454600.0000, 
raw observation next is [-5.883333333333333, 44.16666666666667, 66.0, 702.3333333333333, 26.0, 25.22048885103042, 0.2464672793303625, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.2996306555863343, 0.4416666666666667, 0.22, 0.7760589318600367, 0.6666666666666666, 0.6017074042525351, 0.5821557597767875, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5507256], dtype=float32), 0.45825446]. 
=============================================
[2019-04-04 02:09:06,873] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.16597876e-22 1.26591903e-14 2.04688582e-16 1.00000000e+00
 2.25467846e-15 6.98167788e-14 1.13435135e-14], sum to 1.0000
[2019-04-04 02:09:06,878] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5302
[2019-04-04 02:09:06,959] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.2, 82.5, 104.0, 0.0, 26.0, 26.36788061136959, 0.4846222370210504, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2039400.0000, 
sim time next is 2040000.0000, 
raw observation next is [-4.3, 83.66666666666666, 98.5, 0.0, 26.0, 26.35391202703134, 0.4759517791579772, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.34349030470914127, 0.8366666666666666, 0.3283333333333333, 0.0, 0.6666666666666666, 0.6961593355859449, 0.658650593052659, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6120866], dtype=float32), -0.8413653]. 
=============================================
[2019-04-04 02:09:06,973] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[89.76057 ]
 [90.01061 ]
 [90.28106 ]
 [90.491776]
 [90.68563 ]], R is [[89.63414764]
 [89.73780823]
 [89.84043121]
 [89.94202423]
 [90.04260254]].
[2019-04-04 02:09:09,752] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.13905695e-20 2.61788966e-14 2.40734440e-16 1.00000000e+00
 9.89647969e-15 1.07996717e-12 6.31882047e-15], sum to 1.0000
[2019-04-04 02:09:09,753] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8641
[2019-04-04 02:09:09,790] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.9, 68.0, 135.5, 0.0, 25.0, 24.8021662824826, 0.1513356253972369, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 2203200.0000, 
sim time next is 2203800.0000, 
raw observation next is [-3.816666666666666, 67.5, 133.0, 0.0, 25.0, 24.77042279003843, 0.1524585906106828, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.3568790397045245, 0.675, 0.44333333333333336, 0.0, 0.5833333333333334, 0.5642018991698693, 0.5508195302035609, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.345417], dtype=float32), 0.703381]. 
=============================================
[2019-04-04 02:09:26,147] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.87721664e-23 7.48262870e-16 1.16615972e-17 1.00000000e+00
 1.32503109e-17 1.36175395e-14 3.11066841e-16], sum to 1.0000
[2019-04-04 02:09:26,147] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4926
[2019-04-04 02:09:26,196] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 81.5, 0.0, 0.0, 26.0, 25.00068428312075, 0.4417230806805607, 0.0, 1.0, 101777.5853236516], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2925000.0000, 
sim time next is 2925600.0000, 
raw observation next is [-1.0, 82.66666666666667, 0.0, 0.0, 26.0, 25.09541394798921, 0.468633560184585, 0.0, 1.0, 64655.99868372991], 
processed observation next is [1.0, 0.8695652173913043, 0.4349030470914128, 0.8266666666666667, 0.0, 0.0, 0.6666666666666666, 0.5912844956657676, 0.656211186728195, 0.0, 1.0, 0.30788570801776144], 
reward next is 0.6921, 
noisyNet noise sample is [array([0.3435491], dtype=float32), -1.0412465]. 
=============================================
[2019-04-04 02:09:31,692] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.4782870e-27 1.8920680e-19 1.9349716e-20 1.0000000e+00 3.6455070e-22
 3.1983582e-19 3.0168899e-20], sum to 1.0000
[2019-04-04 02:09:31,692] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2606
[2019-04-04 02:09:31,717] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.3, 57.00000000000001, 0.0, 0.0, 26.0, 24.78756304058506, 0.1452047387760297, 0.0, 1.0, 38360.85482662677], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2524800.0000, 
sim time next is 2525400.0000, 
raw observation next is [-2.3, 57.0, 0.0, 0.0, 26.0, 24.81463639233305, 0.1452311376578454, 0.0, 1.0, 38373.85504999014], 
processed observation next is [1.0, 0.21739130434782608, 0.3988919667590028, 0.57, 0.0, 0.0, 0.6666666666666666, 0.5678863660277541, 0.5484103792192818, 0.0, 1.0, 0.18273264309519113], 
reward next is 0.8173, 
noisyNet noise sample is [array([-1.0559952], dtype=float32), 0.18267684]. 
=============================================
[2019-04-04 02:09:34,485] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.2609489e-25 1.2565161e-17 7.0315481e-19 1.0000000e+00 2.1964119e-19
 6.5321401e-17 1.2800962e-18], sum to 1.0000
[2019-04-04 02:09:34,486] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5231
[2019-04-04 02:09:34,540] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.233333333333333, 33.0, 0.0, 0.0, 26.0, 25.23265256674421, 0.3087461203891619, 1.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2569200.0000, 
sim time next is 2569800.0000, 
raw observation next is [0.8666666666666667, 34.0, 0.0, 0.0, 26.0, 24.80370935107069, 0.3259206438791613, 1.0, 1.0, 196403.6692060178], 
processed observation next is [1.0, 0.7391304347826086, 0.4866112650046169, 0.34, 0.0, 0.0, 0.6666666666666666, 0.5669757792558908, 0.6086402146263871, 1.0, 1.0, 0.9352555676477038], 
reward next is 0.0647, 
noisyNet noise sample is [array([0.31354445], dtype=float32), -0.78766835]. 
=============================================
[2019-04-04 02:09:38,018] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.5683122e-22 1.0371155e-12 1.0865508e-14 1.0000000e+00 2.2423613e-15
 2.0315091e-14 2.5786883e-15], sum to 1.0000
[2019-04-04 02:09:38,020] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9113
[2019-04-04 02:09:38,033] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.616666666666667, 74.33333333333334, 0.0, 0.0, 24.0, 23.85036896612363, 0.1622702655483995, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1627800.0000, 
sim time next is 1628400.0000, 
raw observation next is [7.533333333333333, 74.66666666666667, 0.0, 0.0, 24.0, 23.75582825276109, 0.1464686012278922, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.6712834718374886, 0.7466666666666667, 0.0, 0.0, 0.5, 0.47965235439675763, 0.5488228670759641, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.87986326], dtype=float32), 0.51213497]. 
=============================================
[2019-04-04 02:09:43,295] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.3358758e-20 3.8040231e-12 3.8736809e-13 1.0000000e+00 1.0639510e-14
 2.0990168e-10 5.3872103e-13], sum to 1.0000
[2019-04-04 02:09:43,296] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2570
[2019-04-04 02:09:43,308] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 24.0, 23.74865348540905, 0.1198252202576049, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1719000.0000, 
sim time next is 1719600.0000, 
raw observation next is [0.5, 92.0, 0.0, 0.0, 24.0, 23.750880406077, 0.112084151657925, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.4764542936288089, 0.92, 0.0, 0.0, 0.5, 0.47924003383974983, 0.537361383885975, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.28105786], dtype=float32), 0.021129142]. 
=============================================
[2019-04-04 02:09:57,119] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4885212e-19 2.6436939e-12 1.7987277e-12 1.0000000e+00 2.0446325e-14
 4.2274906e-11 2.9038328e-13], sum to 1.0000
[2019-04-04 02:09:57,123] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4205
[2019-04-04 02:09:57,143] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.333333333333333, 71.0, 0.0, 0.0, 25.0, 24.02685323704263, 0.09545615101624016, 0.0, 1.0, 45219.46118269103], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 2677200.0000, 
sim time next is 2677800.0000, 
raw observation next is [-6.666666666666667, 71.5, 0.0, 0.0, 25.0, 23.99841736315347, 0.08399693338208647, 0.0, 1.0, 45183.96181840872], 
processed observation next is [1.0, 1.0, 0.27793167128347185, 0.715, 0.0, 0.0, 0.5833333333333334, 0.49986811359612265, 0.5279989777940288, 0.0, 1.0, 0.21516172294480343], 
reward next is 0.7848, 
noisyNet noise sample is [array([-2.7244432], dtype=float32), -1.5203327]. 
=============================================
[2019-04-04 02:09:58,076] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.4792370e-26 3.7244228e-13 1.3598630e-16 1.0000000e+00 3.5714189e-18
 4.0928507e-13 7.3423984e-19], sum to 1.0000
[2019-04-04 02:09:58,079] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9276
[2019-04-04 02:09:58,128] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.333333333333333, 100.0, 69.33333333333334, 340.3333333333334, 26.0, 25.66670543587803, 0.4156350836246672, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3140400.0000, 
sim time next is 3141000.0000, 
raw observation next is [6.5, 100.0, 83.0, 392.0, 26.0, 25.76876742092901, 0.4473775538738547, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.6426592797783934, 1.0, 0.27666666666666667, 0.4331491712707182, 0.6666666666666666, 0.6473972850774175, 0.6491258512912849, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2538922], dtype=float32), -1.0474025]. 
=============================================
[2019-04-04 02:09:58,142] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[102.857025]
 [102.77092 ]
 [102.75582 ]
 [102.57237 ]
 [102.20746 ]], R is [[103.06342316]
 [103.03279114]
 [103.00246429]
 [102.97244263]
 [102.94271851]].
[2019-04-04 02:09:59,658] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-04 02:09:59,660] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 02:09:59,660] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 02:09:59,661] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:09:59,661] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 02:09:59,661] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:09:59,662] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:09:59,665] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run33
[2019-04-04 02:09:59,701] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run33
[2019-04-04 02:09:59,733] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run33
[2019-04-04 02:10:09,523] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.2530038], dtype=float32), 0.38802892]
[2019-04-04 02:10:09,523] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-4.560534103, 59.75687645000001, 0.0, 0.0, 19.0, 18.69068918155409, -1.219203081452773, 0.0, 1.0, 20522.537629308]
[2019-04-04 02:10:09,523] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 02:10:09,524] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [3.3723599e-10 5.0186544e-09 2.6276867e-08 9.9998820e-01 5.9553090e-06
 4.3744626e-06 1.5092889e-06], sampled 0.5266965272755509
[2019-04-04 02:10:31,602] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.2530038], dtype=float32), 0.38802892]
[2019-04-04 02:10:31,602] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [4.7, 82.5, 0.0, 0.0, 23.0, 22.87414861157576, -0.08511917405280663, 0.0, 1.0, 0.0]
[2019-04-04 02:10:31,603] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 02:10:31,603] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [5.1537743e-21 8.9758540e-15 1.0856968e-15 1.0000000e+00 2.9480230e-15
 9.1390234e-14 9.7889836e-15], sampled 0.48946140387181225
[2019-04-04 02:11:51,564] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.2530038], dtype=float32), 0.38802892]
[2019-04-04 02:11:51,564] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [13.8, 52.0, 238.0, 404.0, 23.0, 25.27885173333597, 0.4148108312114114, 1.0, 1.0, 0.0]
[2019-04-04 02:11:51,564] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 02:11:51,565] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [5.4298917e-19 2.6054367e-10 1.8250082e-13 1.0000000e+00 1.6293486e-11
 8.5233140e-11 3.4901999e-13], sampled 0.4164548670546939
[2019-04-04 02:11:52,177] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6020.4457 159720000.3472 -1602.7687
[2019-04-04 02:12:20,716] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5342.0726 196401191.2142 -2285.0467
[2019-04-04 02:12:29,235] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 6820.9852 212696352.1977 -1001.1228
[2019-04-04 02:12:30,270] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 3200000, evaluation results [3200000.0, 6820.985220785089, 212696352.19770882, -1001.1227790751208, 6020.445683203959, 159720000.34721655, -1602.7686943747935, 5342.07260918522, 196401191.21416193, -2285.0466947281557]
[2019-04-04 02:12:30,923] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.4293267e-24 4.1583875e-18 2.7762097e-17 1.0000000e+00 3.1559257e-20
 1.2250006e-16 2.4604770e-17], sum to 1.0000
[2019-04-04 02:12:30,924] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9069
[2019-04-04 02:12:31,006] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-15.0, 83.0, 26.66666666666666, 110.0, 26.0, 24.38072724728248, 0.1697748668013268, 1.0, 1.0, 106131.5776453966], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2706600.0000, 
sim time next is 2707200.0000, 
raw observation next is [-15.0, 83.0, 40.0, 165.0, 26.0, 24.70344038219487, 0.238618517885165, 1.0, 1.0, 87745.88658465476], 
processed observation next is [1.0, 0.34782608695652173, 0.04709141274238226, 0.83, 0.13333333333333333, 0.18232044198895028, 0.6666666666666666, 0.5586200318495725, 0.5795395059617217, 1.0, 1.0, 0.41783755516502263], 
reward next is 0.5822, 
noisyNet noise sample is [array([1.4802151], dtype=float32), -0.2531172]. 
=============================================
[2019-04-04 02:12:32,239] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.9237974e-19 1.1032806e-09 1.1030769e-12 9.9999940e-01 7.3267937e-12
 6.2502244e-07 6.5014886e-12], sum to 1.0000
[2019-04-04 02:12:32,239] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4978
[2019-04-04 02:12:32,262] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 100.0, 90.0, 0.0, 25.0, 25.17028884290275, 0.2642477323096525, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 2903400.0000, 
sim time next is 2904000.0000, 
raw observation next is [2.0, 100.0, 89.16666666666666, 0.0, 25.0, 25.14450881232547, 0.2558230903272298, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.518005540166205, 1.0, 0.29722222222222217, 0.0, 0.5833333333333334, 0.5953757343604558, 0.5852743634424099, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.43620846], dtype=float32), -0.7934822]. 
=============================================
[2019-04-04 02:12:32,278] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[85.61596 ]
 [85.8273  ]
 [86.10291 ]
 [86.433495]
 [86.8854  ]], R is [[85.7793045 ]
 [85.92150879]
 [86.06229401]
 [86.20167542]
 [86.33966064]].
[2019-04-04 02:12:33,051] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.2782754e-27 5.3089587e-22 2.2755908e-21 1.0000000e+00 1.9893885e-24
 6.3754089e-21 2.9745392e-20], sum to 1.0000
[2019-04-04 02:12:33,051] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3130
[2019-04-04 02:12:33,065] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 71.0, 0.0, 0.0, 26.0, 25.24273117287087, 0.4253746382314129, 0.0, 1.0, 41140.11955173072], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3549600.0000, 
sim time next is 3550200.0000, 
raw observation next is [-3.0, 70.0, 0.0, 0.0, 26.0, 25.2827417925014, 0.4222950267349035, 0.0, 1.0, 40967.05968354834], 
processed observation next is [0.0, 0.08695652173913043, 0.3795013850415513, 0.7, 0.0, 0.0, 0.6666666666666666, 0.6068951493751168, 0.6407650089116345, 0.0, 1.0, 0.1950812365883254], 
reward next is 0.8049, 
noisyNet noise sample is [array([-0.11809608], dtype=float32), -0.29004884]. 
=============================================
[2019-04-04 02:12:48,239] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.4539628e-18 4.1303641e-11 5.4416552e-13 1.0000000e+00 7.2379928e-14
 3.9454953e-10 2.8192906e-12], sum to 1.0000
[2019-04-04 02:12:48,242] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0584
[2019-04-04 02:12:48,295] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.2, 64.0, 0.0, 0.0, 24.0, 23.10496852557665, -0.051308899046177, 0.0, 1.0, 193723.819774724], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2665800.0000, 
sim time next is 2666400.0000, 
raw observation next is [-1.2, 64.33333333333333, 0.0, 0.0, 24.0, 23.16595572258819, -0.01330325703798154, 0.0, 1.0, 110269.263566175], 
processed observation next is [1.0, 0.8695652173913043, 0.42936288088642666, 0.6433333333333333, 0.0, 0.0, 0.5, 0.4304963102156825, 0.49556558098733944, 0.0, 1.0, 0.5250917312675], 
reward next is 0.4749, 
noisyNet noise sample is [array([0.79943967], dtype=float32), -1.4819052]. 
=============================================
[2019-04-04 02:13:04,729] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.4473879e-23 1.8776967e-17 2.5105866e-16 1.0000000e+00 1.6687070e-18
 2.5983571e-12 6.0203134e-15], sum to 1.0000
[2019-04-04 02:13:04,729] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9049
[2019-04-04 02:13:04,768] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.8333333333333334, 100.0, 0.0, 0.0, 25.0, 24.45949023549663, 0.125121355296288, 0.0, 1.0, 40751.0435074185], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 3103800.0000, 
sim time next is 3104400.0000, 
raw observation next is [-0.6666666666666667, 100.0, 0.0, 0.0, 25.0, 24.42813287159473, 0.1242182625251923, 0.0, 1.0, 56338.19120592901], 
processed observation next is [0.0, 0.9565217391304348, 0.44413665743305636, 1.0, 0.0, 0.0, 0.5833333333333334, 0.5356777392995609, 0.5414060875083974, 0.0, 1.0, 0.26827710098061436], 
reward next is 0.7317, 
noisyNet noise sample is [array([-1.9590675], dtype=float32), 0.21639924]. 
=============================================
[2019-04-04 02:13:30,411] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.4934850e-24 3.7110737e-15 1.3687006e-15 1.0000000e+00 3.5001090e-19
 3.0006410e-14 2.1877520e-16], sum to 1.0000
[2019-04-04 02:13:30,415] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3761
[2019-04-04 02:13:30,443] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.35435069103215, 0.4351401301619142, 0.0, 1.0, 39855.11053761634], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3891000.0000, 
sim time next is 3891600.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.36214972247507, 0.4367959548576844, 0.0, 1.0, 39754.27025877546], 
processed observation next is [1.0, 0.043478260869565216, 0.40720221606648205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6135124768729225, 0.6455986516192281, 0.0, 1.0, 0.18930604885131172], 
reward next is 0.8107, 
noisyNet noise sample is [array([-0.76452094], dtype=float32), -0.46422118]. 
=============================================
[2019-04-04 02:13:33,433] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.4410908e-20 2.1133421e-12 2.0511587e-13 9.9999905e-01 7.3978985e-13
 9.3399763e-07 1.8308044e-13], sum to 1.0000
[2019-04-04 02:13:33,434] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3027
[2019-04-04 02:13:33,491] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.65, 34.5, 218.0, 22.0, 24.0, 24.20308700677866, -0.05890038908520926, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2550600.0000, 
sim time next is 2551200.0000, 
raw observation next is [1.833333333333333, 33.0, 210.1666666666667, 98.66666666666666, 24.0, 24.13423979446835, -0.05408558007069247, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.5133887349953832, 0.33, 0.7005555555555557, 0.10902394106813995, 0.5, 0.5111866495390291, 0.48197147330976914, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9150002], dtype=float32), -0.072520405]. 
=============================================
[2019-04-04 02:13:37,887] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.5257419e-28 6.2764720e-21 3.3127195e-22 1.0000000e+00 6.1089798e-25
 8.5680943e-21 4.3854984e-20], sum to 1.0000
[2019-04-04 02:13:37,888] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8087
[2019-04-04 02:13:37,945] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.7, 71.0, 0.0, 0.0, 26.0, 24.87611955214988, 0.322237079107746, 0.0, 1.0, 198256.9512896638], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4303800.0000, 
sim time next is 4304400.0000, 
raw observation next is [5.6, 71.66666666666667, 0.0, 0.0, 26.0, 24.9215383445282, 0.3711120917992605, 0.0, 1.0, 179164.8321797554], 
processed observation next is [0.0, 0.8260869565217391, 0.6177285318559557, 0.7166666666666667, 0.0, 0.0, 0.6666666666666666, 0.5767948620440168, 0.6237040305997535, 0.0, 1.0, 0.8531658675226448], 
reward next is 0.1468, 
noisyNet noise sample is [array([-0.55318457], dtype=float32), -0.8592629]. 
=============================================
[2019-04-04 02:13:48,609] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.5977032e-26 2.1321556e-15 3.3211295e-17 1.0000000e+00 1.3162029e-20
 7.9363006e-19 1.5310639e-18], sum to 1.0000
[2019-04-04 02:13:48,609] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3793
[2019-04-04 02:13:48,631] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 60.0, 71.66666666666666, 596.3333333333333, 25.0, 26.2570171875138, 0.4035931135115685, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 3773400.0000, 
sim time next is 3774000.0000, 
raw observation next is [0.0, 60.00000000000001, 67.83333333333333, 567.6666666666666, 25.0, 26.20886528672865, 0.532145184281627, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.46260387811634357, 0.6000000000000001, 0.2261111111111111, 0.627255985267035, 0.5833333333333334, 0.6840721072273874, 0.6773817280938758, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1795228], dtype=float32), 0.50636786]. 
=============================================
[2019-04-04 02:13:48,652] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[80.304085]
 [80.54488 ]
 [80.761406]
 [81.011284]
 [81.326   ]], R is [[80.43666077]
 [80.6322937 ]
 [80.82597351]
 [81.01771545]
 [81.20754242]].
[2019-04-04 02:13:59,467] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.35685969e-23 5.27583009e-14 8.85346619e-15 1.00000000e+00
 1.22832298e-16 1.70070867e-13 1.07482135e-16], sum to 1.0000
[2019-04-04 02:13:59,467] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9424
[2019-04-04 02:13:59,542] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 92.0, 71.00000000000001, 322.0000000000001, 25.0, 24.78673554873723, 0.3377646731518464, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 3226800.0000, 
sim time next is 3227400.0000, 
raw observation next is [-3.0, 92.0, 85.0, 370.0, 25.0, 24.85193827228364, 0.3724495679746485, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.3795013850415513, 0.92, 0.2833333333333333, 0.4088397790055249, 0.5833333333333334, 0.5709948560236366, 0.6241498559915495, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.42409813], dtype=float32), 0.58191967]. 
=============================================
[2019-04-04 02:14:11,750] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4323952e-23 4.8010966e-14 2.7803552e-16 1.0000000e+00 4.9183547e-18
 6.7765554e-12 5.2276438e-16], sum to 1.0000
[2019-04-04 02:14:11,750] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6728
[2019-04-04 02:14:11,806] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 71.0, 82.00000000000001, 114.0, 26.0, 25.47422268943061, 0.4033807930705027, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4608600.0000, 
sim time next is 4609200.0000, 
raw observation next is [-2.0, 71.0, 102.5, 142.5, 26.0, 25.44243602049803, 0.43904696451326, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.40720221606648205, 0.71, 0.3416666666666667, 0.1574585635359116, 0.6666666666666666, 0.6202030017081691, 0.6463489881710867, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.31229663], dtype=float32), -0.22342642]. 
=============================================
[2019-04-04 02:14:32,336] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.0308124e-20 3.4163267e-10 3.7582402e-13 1.0000000e+00 1.2947134e-15
 4.7001560e-08 6.8068386e-13], sum to 1.0000
[2019-04-04 02:14:32,336] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6053
[2019-04-04 02:14:32,347] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.333333333333333, 95.0, 109.3333333333333, 804.8333333333334, 24.0, 24.65514554753062, 0.318273024369863, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3246000.0000, 
sim time next is 3246600.0000, 
raw observation next is [-3.666666666666667, 97.5, 107.6666666666667, 797.6666666666666, 24.0, 24.76821317801083, 0.3326283193914403, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.3610341643582641, 0.975, 0.358888888888889, 0.8813996316758748, 0.5, 0.5640177648342357, 0.6108761064638134, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9690627], dtype=float32), -0.14208348]. 
=============================================
[2019-04-04 02:14:43,286] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:14:43,287] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:14:43,296] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run25
[2019-04-04 02:14:47,888] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.6142306e-20 8.8398544e-10 1.9888988e-13 1.0000000e+00 5.1498817e-14
 2.2850068e-10 3.4325196e-13], sum to 1.0000
[2019-04-04 02:14:47,889] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5087
[2019-04-04 02:14:48,004] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [11.66666666666667, 52.66666666666667, 0.0, 0.0, 25.0, 26.94036496032398, 0.7934772728127042, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 4389600.0000, 
sim time next is 4390200.0000, 
raw observation next is [11.5, 54.0, 0.0, 0.0, 25.0, 26.82235324608963, 0.7796210508537752, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.7811634349030472, 0.54, 0.0, 0.0, 0.5833333333333334, 0.7351961038408025, 0.7598736836179251, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6062529], dtype=float32), 0.82512206]. 
=============================================
[2019-04-04 02:14:53,185] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:14:53,185] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:14:53,205] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run25
[2019-04-04 02:14:54,630] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0410145e-22 3.9467762e-13 6.3428258e-15 1.0000000e+00 1.1972394e-17
 1.9228138e-11 6.1547172e-15], sum to 1.0000
[2019-04-04 02:14:54,630] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0679
[2019-04-04 02:14:54,650] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.3, 63.33333333333333, 0.0, 0.0, 25.0, 24.50559412745393, 0.2226377450216386, 0.0, 1.0, 40513.13465978382], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 4583400.0000, 
sim time next is 4584000.0000, 
raw observation next is [0.2000000000000001, 63.66666666666667, 0.0, 0.0, 25.0, 24.51115752288958, 0.2234228988707843, 0.0, 1.0, 32494.14228341008], 
processed observation next is [1.0, 0.043478260869565216, 0.46814404432132967, 0.6366666666666667, 0.0, 0.0, 0.5833333333333334, 0.5425964602407983, 0.5744742996235948, 0.0, 1.0, 0.15473401087338134], 
reward next is 0.8453, 
noisyNet noise sample is [array([1.7317405], dtype=float32), -0.397242]. 
=============================================
[2019-04-04 02:14:54,665] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[82.89422 ]
 [82.84884 ]
 [82.865074]
 [82.853096]
 [82.882225]], R is [[83.0063858 ]
 [82.98340607]
 [82.98747253]
 [83.06834412]
 [83.10037994]].
[2019-04-04 02:14:59,332] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.4232607e-20 8.8503562e-12 2.0233006e-13 1.0000000e+00 1.0892893e-15
 8.2301166e-10 6.8698594e-14], sum to 1.0000
[2019-04-04 02:14:59,332] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4482
[2019-04-04 02:14:59,414] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.9, 71.0, 0.0, 0.0, 25.0, 24.36014440279747, 0.183905508652051, 0.0, 1.0, 79123.66349275406], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 4512600.0000, 
sim time next is 4513200.0000, 
raw observation next is [-0.9333333333333333, 71.0, 0.0, 0.0, 25.0, 24.34055776259584, 0.1897975121720142, 0.0, 1.0, 58164.32694360066], 
processed observation next is [1.0, 0.21739130434782608, 0.4367497691597415, 0.71, 0.0, 0.0, 0.5833333333333334, 0.5283798135496532, 0.5632658373906714, 0.0, 1.0, 0.2769729854457174], 
reward next is 0.7230, 
noisyNet noise sample is [array([0.27698725], dtype=float32), 1.0018756]. 
=============================================
[2019-04-04 02:15:11,902] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:15:11,902] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:15:11,912] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run25
[2019-04-04 02:15:15,365] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.6217370e-35 4.8728743e-28 1.4292001e-26 1.0000000e+00 2.7231570e-32
 2.0805847e-26 4.7832596e-27], sum to 1.0000
[2019-04-04 02:15:15,378] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2071
[2019-04-04 02:15:15,395] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 37.0, 131.0, 737.0, 26.0, 25.14039325815724, 0.436751553159286, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4805400.0000, 
sim time next is 4806000.0000, 
raw observation next is [3.0, 37.0, 122.5, 734.5, 26.0, 25.13444626495843, 0.4438311086940617, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.5457063711911359, 0.37, 0.4083333333333333, 0.8116022099447514, 0.6666666666666666, 0.5945371887465359, 0.6479437028980205, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7327415], dtype=float32), 0.03908592]. 
=============================================
[2019-04-04 02:15:15,398] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[94.83676 ]
 [95.048325]
 [95.25877 ]
 [95.37034 ]
 [95.43964 ]], R is [[94.80901337]
 [94.86092377]
 [94.91231537]
 [94.9631958 ]
 [95.01356506]].
[2019-04-04 02:15:15,737] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:15:15,737] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:15:15,754] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run25
[2019-04-04 02:15:16,927] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3261178e-25 8.0188702e-17 4.1107249e-18 1.0000000e+00 3.8706131e-22
 1.0794980e-15 1.2442429e-18], sum to 1.0000
[2019-04-04 02:15:16,927] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1333
[2019-04-04 02:15:16,960] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 61.0, 0.0, 0.0, 26.0, 25.72123904589993, 0.5432734206173301, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4572000.0000, 
sim time next is 4572600.0000, 
raw observation next is [1.0, 61.0, 0.0, 0.0, 26.0, 25.67945770932876, 0.4813370050160075, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.4903047091412743, 0.61, 0.0, 0.0, 0.6666666666666666, 0.6399548091107299, 0.6604456683386691, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.44165936], dtype=float32), -1.8692447]. 
=============================================
[2019-04-04 02:15:21,052] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.0597500e-32 3.3569731e-24 6.0743355e-24 1.0000000e+00 4.7103749e-28
 2.8920141e-25 5.8304106e-24], sum to 1.0000
[2019-04-04 02:15:21,086] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9538
[2019-04-04 02:15:21,137] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.166666666666667, 30.66666666666666, 118.3333333333333, 838.6666666666667, 25.0, 24.14278551406826, 0.1787889957290295, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 4191000.0000, 
sim time next is 4191600.0000, 
raw observation next is [1.333333333333333, 31.33333333333334, 118.1666666666667, 842.8333333333334, 25.0, 24.13622413416584, 0.1819780739470435, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.4995383194829178, 0.3133333333333334, 0.393888888888889, 0.9313075506445673, 0.5833333333333334, 0.5113520111804867, 0.5606593579823479, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0825922], dtype=float32), -0.2564196]. 
=============================================
[2019-04-04 02:15:32,773] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.3109732e-15 7.2764561e-10 4.3461786e-11 9.9999940e-01 2.6342033e-09
 5.8677585e-07 9.4889308e-10], sum to 1.0000
[2019-04-04 02:15:32,773] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6274
[2019-04-04 02:15:32,833] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.766666666666667, 88.0, 0.0, 0.0, 19.0, 18.09001381954286, -1.217874517284549, 0.0, 1.0, 36945.60095407941], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 63600.0000, 
sim time next is 64200.0000, 
raw observation next is [4.583333333333334, 88.5, 0.0, 0.0, 19.0, 18.09422163612955, -1.215501821534817, 0.0, 1.0, 30132.57596079186], 
processed observation next is [0.0, 0.7391304347826086, 0.5895660203139428, 0.885, 0.0, 0.0, 0.08333333333333333, 0.007851803010795836, 0.094832726155061, 0.0, 1.0, 0.1434884569561517], 
reward next is 0.8565, 
noisyNet noise sample is [array([0.78070784], dtype=float32), 0.46433502]. 
=============================================
[2019-04-04 02:15:39,551] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.5564913e-28 8.3318896e-23 9.2477894e-22 1.0000000e+00 4.0472288e-24
 9.8335563e-23 2.8175038e-21], sum to 1.0000
[2019-04-04 02:15:39,551] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0606
[2019-04-04 02:15:39,609] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.8333333333333334, 47.66666666666667, 0.0, 0.0, 25.0, 24.61852641192699, 0.2364100759716599, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 4824600.0000, 
sim time next is 4825200.0000, 
raw observation next is [0.6666666666666667, 48.33333333333334, 0.0, 0.0, 25.0, 24.66130751689369, 0.2302924047355469, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.4810710987996307, 0.48333333333333345, 0.0, 0.0, 0.5833333333333334, 0.5551089597411408, 0.576764134911849, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.88051397], dtype=float32), -0.8020327]. 
=============================================
[2019-04-04 02:15:44,852] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.21420745e-23 6.72891232e-16 2.86477618e-15 1.00000000e+00
 9.58396790e-19 1.44455498e-12 6.26359292e-17], sum to 1.0000
[2019-04-04 02:15:44,852] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4832
[2019-04-04 02:15:44,925] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 49.33333333333334, 0.0, 0.0, 25.0, 24.53063818043885, 0.1947361491052425, 0.0, 1.0, 18749.68423035898], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 5029800.0000, 
sim time next is 5030400.0000, 
raw observation next is [-1.0, 48.66666666666667, 0.0, 0.0, 25.0, 24.61327459502613, 0.1916501596852249, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.4349030470914128, 0.4866666666666667, 0.0, 0.0, 0.5833333333333334, 0.5511062162521775, 0.5638833865617416, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.05084236], dtype=float32), 0.85609674]. 
=============================================
[2019-04-04 02:15:54,627] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:15:54,627] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:15:54,650] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run25
[2019-04-04 02:15:54,981] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:15:54,981] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:15:54,984] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run25
[2019-04-04 02:16:11,238] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:16:11,239] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:16:11,244] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run25
[2019-04-04 02:16:11,745] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:16:11,746] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:16:11,749] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run25
[2019-04-04 02:16:11,899] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:16:11,899] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:16:11,925] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run25
[2019-04-04 02:16:13,827] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:16:13,827] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:16:13,831] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run25
[2019-04-04 02:16:15,162] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.5526223e-25 1.2454627e-19 7.4251237e-19 1.0000000e+00 4.9384046e-22
 1.4885391e-14 5.9045591e-18], sum to 1.0000
[2019-04-04 02:16:15,162] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5240
[2019-04-04 02:16:15,197] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 48.0, 0.0, 0.0, 24.0, 23.53405789628127, -0.07156660898296023, 0.0, 1.0, 30293.03599016174], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4233600.0000, 
sim time next is 4234200.0000, 
raw observation next is [2.0, 48.0, 0.0, 0.0, 24.0, 23.53641128225753, -0.07366245856586105, 0.0, 1.0, 31116.36687506663], 
processed observation next is [0.0, 0.0, 0.518005540166205, 0.48, 0.0, 0.0, 0.5, 0.46136760685479405, 0.475445847144713, 0.0, 1.0, 0.14817317559555537], 
reward next is 0.8518, 
noisyNet noise sample is [array([-0.43361032], dtype=float32), -0.03929157]. 
=============================================
[2019-04-04 02:16:18,470] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.4761322e-27 4.2986459e-18 4.4870706e-18 1.0000000e+00 5.9222774e-22
 8.1679292e-16 6.2276815e-19], sum to 1.0000
[2019-04-04 02:16:18,470] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4254
[2019-04-04 02:16:18,478] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.666666666666667, 52.5, 120.6666666666667, 830.3333333333334, 24.0, 23.3606839647909, -0.03064160246019275, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4276200.0000, 
sim time next is 4276800.0000, 
raw observation next is [7.0, 52.0, 120.5, 834.5, 24.0, 23.36878849417441, -0.02475766370754477, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.6565096952908588, 0.52, 0.40166666666666667, 0.9220994475138121, 0.5, 0.44739904118120083, 0.4917474454308184, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.49978405], dtype=float32), 0.70024395]. 
=============================================
[2019-04-04 02:16:20,841] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:16:20,841] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:16:20,858] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run25
[2019-04-04 02:16:21,253] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:16:21,253] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:16:21,260] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run25
[2019-04-04 02:16:29,388] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:16:29,388] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:16:29,392] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run25
[2019-04-04 02:16:32,259] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.1030297e-27 1.2523147e-17 3.8629632e-19 1.0000000e+00 6.5296289e-23
 2.3892015e-17 2.4182108e-19], sum to 1.0000
[2019-04-04 02:16:32,259] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2673
[2019-04-04 02:16:32,286] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.3, 46.0, 85.0, 31.0, 26.0, 25.36991884543146, 0.3566390542194262, 1.0, 1.0, 42941.393061731], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 747000.0000, 
sim time next is 747600.0000, 
raw observation next is [-0.4, 45.66666666666667, 82.16666666666667, 26.33333333333334, 26.0, 25.4870483067108, 0.3701764708963751, 1.0, 1.0, 29051.84954485226], 
processed observation next is [1.0, 0.6521739130434783, 0.45152354570637127, 0.4566666666666667, 0.2738888888888889, 0.02909760589318601, 0.6666666666666666, 0.6239206922259001, 0.6233921569654584, 1.0, 1.0, 0.13834214068977266], 
reward next is 0.8617, 
noisyNet noise sample is [array([-0.7583612], dtype=float32), -0.68308634]. 
=============================================
[2019-04-04 02:16:34,963] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1889911e-16 2.3676314e-10 2.3705776e-10 9.9999857e-01 1.9518481e-10
 1.3964404e-06 2.9630652e-11], sum to 1.0000
[2019-04-04 02:16:34,963] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0432
[2019-04-04 02:16:35,010] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.9, 81.83333333333334, 186.0, 20.66666666666666, 23.0, 22.68884337751138, -0.3369952679428356, 1.0, 1.0, 51752.2058700913], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 126600.0000, 
sim time next is 127200.0000, 
raw observation next is [-8.0, 77.66666666666667, 185.0, 16.83333333333333, 23.0, 22.74910729939254, -0.3285190546225359, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.24099722991689754, 0.7766666666666667, 0.6166666666666667, 0.018600368324125226, 0.4166666666666667, 0.39575894161604513, 0.39049364845915474, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.0462834], dtype=float32), -0.34388703]. 
=============================================
[2019-04-04 02:16:41,393] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.03705604e-19 9.43233512e-12 1.68946506e-11 9.99999642e-01
 1.00964316e-16 3.44698890e-07 3.54322799e-13], sum to 1.0000
[2019-04-04 02:16:41,394] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3581
[2019-04-04 02:16:41,459] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-14.68333333333333, 70.0, 49.66666666666666, 735.0, 26.0, 25.52535475500021, 0.261556337585759, 1.0, 1.0, 52927.42107943253], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 381000.0000, 
sim time next is 381600.0000, 
raw observation next is [-14.5, 66.0, 55.0, 733.5, 26.0, 25.53067461820993, 0.2891932447149366, 1.0, 1.0, 52628.70127305402], 
processed observation next is [1.0, 0.43478260869565216, 0.06094182825484763, 0.66, 0.18333333333333332, 0.8104972375690608, 0.6666666666666666, 0.6275562181841607, 0.5963977482383122, 1.0, 1.0, 0.25061286320501913], 
reward next is 0.7494, 
noisyNet noise sample is [array([-0.18499048], dtype=float32), 0.91569227]. 
=============================================
[2019-04-04 02:16:44,997] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0178260e-16 2.8762645e-10 6.7521700e-12 9.9999988e-01 4.7566457e-09
 1.6324640e-07 6.1744353e-11], sum to 1.0000
[2019-04-04 02:16:45,024] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5615
[2019-04-04 02:16:45,070] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 23.0, 22.0554974256069, -0.4181312076559955, 1.0, 1.0, 170938.0510278098], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 235800.0000, 
sim time next is 236400.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 23.0, 22.19462410146482, -0.4333093645406816, 1.0, 1.0, 21632.05049181826], 
processed observation next is [1.0, 0.7391304347826086, 0.368421052631579, 0.65, 0.0, 0.0, 0.4166666666666667, 0.3495520084554018, 0.3555635451531061, 1.0, 1.0, 0.10300976424675362], 
reward next is 0.8970, 
noisyNet noise sample is [array([-0.38255614], dtype=float32), -0.31494144]. 
=============================================
[2019-04-04 02:16:48,473] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:16:48,473] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:16:48,479] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run25
[2019-04-04 02:16:49,237] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.6460595e-24 4.4693169e-20 8.9604505e-17 1.0000000e+00 7.2208683e-21
 7.7344749e-17 2.5917230e-17], sum to 1.0000
[2019-04-04 02:16:49,237] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6332
[2019-04-04 02:16:49,264] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-15.7, 73.83333333333334, 0.0, 0.0, 26.0, 22.04742450857896, -0.4130348221243309, 0.0, 1.0, 48777.50806576235], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 364200.0000, 
sim time next is 364800.0000, 
raw observation next is [-15.8, 74.66666666666667, 0.0, 0.0, 26.0, 22.04236766737864, -0.4290252772985839, 0.0, 1.0, 48744.29902031168], 
processed observation next is [1.0, 0.21739130434782608, 0.024930747922437636, 0.7466666666666667, 0.0, 0.0, 0.6666666666666666, 0.33686397228155346, 0.3569915742338054, 0.0, 1.0, 0.2321157096205318], 
reward next is 0.7679, 
noisyNet noise sample is [array([-1.7930303], dtype=float32), -0.81587976]. 
=============================================
[2019-04-04 02:16:50,346] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.4421434e-23 1.9503665e-17 2.5384732e-18 1.0000000e+00 2.8510150e-17
 2.6773989e-12 1.8983090e-17], sum to 1.0000
[2019-04-04 02:16:50,347] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4830
[2019-04-04 02:16:50,382] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.3666666666666667, 36.0, 62.33333333333334, 0.0, 24.0, 24.02394120094964, -0.2178851621485696, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 487200.0000, 
sim time next is 487800.0000, 
raw observation next is [0.55, 35.5, 56.0, 0.0, 24.0, 23.60690406623415, -0.1995168847828859, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.4778393351800555, 0.355, 0.18666666666666668, 0.0, 0.5, 0.46724200551951256, 0.43349437173903804, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.65716], dtype=float32), -0.43527937]. 
=============================================
[2019-04-04 02:16:51,495] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.2456617e-27 5.7884404e-23 4.4540117e-22 1.0000000e+00 1.9960789e-23
 1.1391731e-22 1.0954037e-20], sum to 1.0000
[2019-04-04 02:16:51,495] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1003
[2019-04-04 02:16:51,540] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.166666666666667, 55.83333333333334, 0.0, 0.0, 24.0, 23.48250045948282, -0.05055802441931423, 0.0, 1.0, 41897.72601218349], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4835400.0000, 
sim time next is 4836000.0000, 
raw observation next is [-1.333333333333333, 56.66666666666667, 0.0, 0.0, 24.0, 23.50211511230229, -0.04606674502805302, 0.0, 1.0, 28282.72620899841], 
processed observation next is [0.0, 1.0, 0.42566943674976926, 0.5666666666666668, 0.0, 0.0, 0.5, 0.45850959269185737, 0.48464441832398236, 0.0, 1.0, 0.13467964861427814], 
reward next is 0.8653, 
noisyNet noise sample is [array([0.01149838], dtype=float32), -0.018098295]. 
=============================================
[2019-04-04 02:16:51,575] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[80.35198 ]
 [80.41258 ]
 [80.46594 ]
 [80.49683 ]
 [80.485016]], R is [[80.37570953]
 [80.37243652]
 [80.35451508]
 [80.33363342]
 [80.32617188]].
[2019-04-04 02:16:58,328] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.1463817e-27 1.7159640e-21 5.3395054e-22 1.0000000e+00 3.3808508e-24
 1.9405802e-22 2.0132345e-20], sum to 1.0000
[2019-04-04 02:16:58,328] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7938
[2019-04-04 02:16:58,404] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.8, 81.0, 109.5, 265.5, 26.0, 24.83096924766562, 0.303649178667379, 0.0, 1.0, 42579.72122723794], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 561600.0000, 
sim time next is 562200.0000, 
raw observation next is [-0.8666666666666667, 80.83333333333333, 116.3333333333333, 309.0000000000001, 26.0, 24.90929785673605, 0.3113141377472463, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.4385964912280702, 0.8083333333333332, 0.38777777777777767, 0.3414364640883979, 0.6666666666666666, 0.5757748213946708, 0.6037713792490821, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4529747], dtype=float32), 0.79323405]. 
=============================================
[2019-04-04 02:17:03,767] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.6167292e-31 2.0702441e-27 4.0066112e-25 1.0000000e+00 2.9620023e-29
 4.5251635e-25 1.5979089e-24], sum to 1.0000
[2019-04-04 02:17:03,768] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8880
[2019-04-04 02:17:03,882] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.300000000000001, 76.33333333333334, 15.83333333333333, 0.0, 26.0, 23.37406917010713, -0.02005312944122931, 0.0, 1.0, 202372.5358147246], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 634800.0000, 
sim time next is 635400.0000, 
raw observation next is [-4.2, 75.0, 19.0, 0.0, 26.0, 23.71221868612996, 0.08035853395790804, 0.0, 1.0, 203171.6033023518], 
processed observation next is [0.0, 0.34782608695652173, 0.34626038781163443, 0.75, 0.06333333333333334, 0.0, 0.6666666666666666, 0.4760182238441635, 0.5267861779859694, 0.0, 1.0, 0.9674838252492943], 
reward next is 0.0325, 
noisyNet noise sample is [array([0.04608577], dtype=float32), -0.13862418]. 
=============================================
[2019-04-04 02:17:04,775] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.8610699e-25 6.4669942e-21 4.2076739e-20 1.0000000e+00 4.4596582e-21
 3.8764496e-22 2.5212936e-19], sum to 1.0000
[2019-04-04 02:17:04,776] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7702
[2019-04-04 02:17:04,822] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.6, 54.00000000000001, 82.66666666666667, 41.0, 24.0, 23.02180713197139, -0.2080901844099048, 0.0, 1.0, 25659.43908765785], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 660000.0000, 
sim time next is 660600.0000, 
raw observation next is [-0.6, 54.0, 83.0, 38.0, 24.0, 23.01054553833225, -0.2109811906549995, 0.0, 1.0, 36426.2260378732], 
processed observation next is [0.0, 0.6521739130434783, 0.44598337950138506, 0.54, 0.27666666666666667, 0.041988950276243095, 0.5, 0.4175454615276874, 0.42967293644833354, 0.0, 1.0, 0.17345821922796764], 
reward next is 0.8265, 
noisyNet noise sample is [array([0.5085436], dtype=float32), 0.58328086]. 
=============================================
[2019-04-04 02:17:06,553] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.0121182e-27 2.1250058e-19 9.9539231e-21 1.0000000e+00 1.7373505e-21
 3.6022200e-17 1.0002040e-19], sum to 1.0000
[2019-04-04 02:17:06,553] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6959
[2019-04-04 02:17:06,594] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [8.3, 25.0, 0.0, 0.0, 24.0, 24.37595678541653, 0.196756124574409, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 5095200.0000, 
sim time next is 5095800.0000, 
raw observation next is [8.25, 27.5, 0.0, 0.0, 24.0, 24.29853221011694, 0.1802803891638122, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.6911357340720222, 0.275, 0.0, 0.0, 0.5, 0.5248776841764116, 0.5600934630546041, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.52270573], dtype=float32), -0.01848411]. 
=============================================
[2019-04-04 02:17:07,508] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.54336479e-28 1.80719059e-16 5.81408213e-18 1.00000000e+00
 1.01559945e-17 6.25976864e-14 3.71666046e-20], sum to 1.0000
[2019-04-04 02:17:07,508] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4042
[2019-04-04 02:17:07,540] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [12.2, 83.0, 22.0, 69.0, 24.0, 24.32888401133641, 0.2654118940800278, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1067400.0000, 
sim time next is 1068000.0000, 
raw observation next is [12.2, 83.0, 35.0, 96.5, 24.0, 24.60311858934142, 0.2972004062749887, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.8005540166204987, 0.83, 0.11666666666666667, 0.10662983425414364, 0.5, 0.5502598824451184, 0.5990668020916629, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.1807555], dtype=float32), -1.2597669]. 
=============================================
[2019-04-04 02:17:07,550] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[102.10811 ]
 [102.07881 ]
 [101.99191 ]
 [101.88196 ]
 [101.827225]], R is [[102.28365326]
 [102.26081848]
 [102.23821259]
 [102.21582794]
 [102.19367218]].
[2019-04-04 02:17:07,796] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:17:07,815] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:17:07,824] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run25
[2019-04-04 02:17:14,850] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.1082175e-25 3.1540410e-17 1.7253537e-17 1.0000000e+00 9.0691287e-22
 1.4126219e-15 5.3422924e-16], sum to 1.0000
[2019-04-04 02:17:14,851] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7635
[2019-04-04 02:17:14,868] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.04797503361057, 0.4074140816777951, 0.0, 1.0, 38621.08112888264], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1407000.0000, 
sim time next is 1407600.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.05706639752101, 0.4147032010249063, 0.0, 1.0, 38636.69866104406], 
processed observation next is [1.0, 0.30434782608695654, 0.44598337950138506, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5880888664600841, 0.6382344003416355, 0.0, 1.0, 0.18398427933830505], 
reward next is 0.8160, 
noisyNet noise sample is [array([-1.230178], dtype=float32), 0.4544009]. 
=============================================
[2019-04-04 02:17:20,780] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:17:20,780] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:17:20,785] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run25
[2019-04-04 02:17:31,508] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.2870109e-24 3.6982541e-16 2.2858094e-17 1.0000000e+00 1.8822209e-18
 7.1470119e-14 2.5367237e-16], sum to 1.0000
[2019-04-04 02:17:31,509] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8172
[2019-04-04 02:17:31,536] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.266666666666667, 79.66666666666667, 97.5, 700.1666666666667, 25.0, 24.89430798668451, 0.3636358548007686, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 1514400.0000, 
sim time next is 1515000.0000, 
raw observation next is [6.733333333333333, 76.33333333333333, 95.0, 700.3333333333334, 25.0, 24.13703272113093, 0.3322088148031646, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.649122807017544, 0.7633333333333333, 0.31666666666666665, 0.7738489871086557, 0.5833333333333334, 0.5114193934275774, 0.6107362716010548, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.08472452], dtype=float32), -0.5648506]. 
=============================================
[2019-04-04 02:17:31,562] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[87.93111 ]
 [88.315796]
 [88.69767 ]
 [89.04905 ]
 [89.182846]], R is [[88.02747345]
 [88.14720154]
 [88.26573181]
 [88.3830719 ]
 [88.49924469]].
[2019-04-04 02:17:38,875] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.9104801e-20 1.2323148e-12 3.5624542e-13 1.0000000e+00 7.1978690e-12
 5.1260418e-09 5.7688733e-13], sum to 1.0000
[2019-04-04 02:17:38,875] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0169
[2019-04-04 02:17:38,935] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.5, 100.0, 0.0, 0.0, 23.0, 22.15048058648544, -0.3338144167582498, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 933000.0000, 
sim time next is 933600.0000, 
raw observation next is [4.600000000000001, 100.0, 0.0, 0.0, 23.0, 22.23734110228855, -0.3438680576746083, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.5900277008310251, 1.0, 0.0, 0.0, 0.4166666666666667, 0.3531117585240458, 0.38537731410846393, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3974729], dtype=float32), -2.279362]. 
=============================================
[2019-04-04 02:17:43,002] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.5678011e-31 1.1125689e-20 4.3480968e-22 1.0000000e+00 2.5861436e-24
 7.2210998e-18 2.8426117e-23], sum to 1.0000
[2019-04-04 02:17:43,003] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9873
[2019-04-04 02:17:43,015] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 26.0, 25.61655662226586, 0.584411245873003, 0.0, 1.0, 49453.04594326274], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1036800.0000, 
sim time next is 1037400.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 26.0, 25.63751672207347, 0.6089761787296363, 0.0, 1.0, 21918.09122432311], 
processed observation next is [1.0, 0.0, 0.8614958448753465, 0.75, 0.0, 0.0, 0.6666666666666666, 0.636459726839456, 0.7029920595765454, 0.0, 1.0, 0.10437186297296719], 
reward next is 0.8956, 
noisyNet noise sample is [array([-0.10153715], dtype=float32), 0.03125066]. 
=============================================
[2019-04-04 02:17:49,902] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.7868833e-27 2.1955072e-19 3.1278885e-22 1.0000000e+00 8.5216777e-21
 3.3969578e-17 1.0526968e-19], sum to 1.0000
[2019-04-04 02:17:49,902] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6807
[2019-04-04 02:17:49,924] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 95.0, 67.66666666666667, 0.0, 26.0, 25.91576453480182, 0.4876832152726414, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1419600.0000, 
sim time next is 1420200.0000, 
raw observation next is [0.0, 95.0, 72.0, 0.0, 26.0, 25.86766781380079, 0.480927134697175, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.46260387811634357, 0.95, 0.24, 0.0, 0.6666666666666666, 0.6556389844833991, 0.6603090448990584, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.361562], dtype=float32), 2.2369745]. 
=============================================
[2019-04-04 02:17:54,587] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.1114651e-30 3.4807591e-21 2.1033472e-23 1.0000000e+00 3.5107044e-24
 1.6606541e-19 2.4787837e-23], sum to 1.0000
[2019-04-04 02:17:54,587] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8988
[2019-04-04 02:17:54,607] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [13.9, 99.33333333333334, 99.0, 0.0, 26.0, 24.75607244670998, 0.4169344675598632, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1255800.0000, 
sim time next is 1256400.0000, 
raw observation next is [13.8, 100.0, 98.0, 0.0, 26.0, 24.72264610881583, 0.4122264114510538, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.844875346260388, 1.0, 0.32666666666666666, 0.0, 0.6666666666666666, 0.5602205090679858, 0.6374088038170179, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.13495739], dtype=float32), 0.40063712]. 
=============================================
[2019-04-04 02:17:56,003] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.4727360e-24 3.7529322e-13 1.3375596e-17 1.0000000e+00 4.7859352e-15
 2.9449299e-09 1.4059067e-16], sum to 1.0000
[2019-04-04 02:17:56,003] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8025
[2019-04-04 02:17:56,021] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [12.2, 83.0, 15.0, 48.33333333333334, 24.0, 23.96798108018255, 0.1655935185553003, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1066200.0000, 
sim time next is 1066800.0000, 
raw observation next is [12.2, 83.0, 18.5, 58.66666666666666, 24.0, 23.98919902310772, 0.1977556399648052, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.8005540166204987, 0.83, 0.06166666666666667, 0.06482504604051564, 0.5, 0.49909991859230995, 0.5659185466549351, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7510439], dtype=float32), -0.26624975]. 
=============================================
[2019-04-04 02:17:58,625] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.9813751e-32 2.6420035e-17 8.1527009e-25 1.0000000e+00 4.5947567e-26
 4.6444915e-24 8.6962340e-25], sum to 1.0000
[2019-04-04 02:17:58,625] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2550
[2019-04-04 02:17:58,641] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [15.5, 93.0, 0.0, 0.0, 26.0, 23.85030295145033, 0.219360850379984, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1223400.0000, 
sim time next is 1224000.0000, 
raw observation next is [15.5, 93.0, 0.0, 0.0, 26.0, 23.83156769748032, 0.2145969252712088, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.17391304347826086, 0.8919667590027703, 0.93, 0.0, 0.0, 0.6666666666666666, 0.4859639747900267, 0.5715323084237363, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0980815], dtype=float32), 0.5616211]. 
=============================================
[2019-04-04 02:17:58,695] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[98.5116  ]
 [98.4945  ]
 [98.462524]
 [98.438705]
 [98.414955]], R is [[98.54360962]
 [98.55817413]
 [98.57259369]
 [98.58686829]
 [98.60099792]].
[2019-04-04 02:17:59,766] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-04 02:17:59,772] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 02:17:59,773] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:17:59,775] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run34
[2019-04-04 02:17:59,794] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 02:17:59,796] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:17:59,796] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 02:17:59,798] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:17:59,802] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run34
[2019-04-04 02:17:59,833] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run34
[2019-04-04 02:18:28,717] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.9066859], dtype=float32), 0.26457766]
[2019-04-04 02:18:28,718] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-1.6, 66.0, 78.5, 137.0, 19.0, 18.51006945192473, -1.24310786597427, 0.0, 1.0, 18694.52158258835]
[2019-04-04 02:18:28,718] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 02:18:28,718] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.2922752e-16 2.1563107e-14 8.8696503e-13 1.0000000e+00 2.4903333e-11
 8.3579636e-11 1.1843243e-11], sampled 0.9142761121988058
[2019-04-04 02:19:07,596] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.9066859], dtype=float32), 0.26457766]
[2019-04-04 02:19:07,596] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-4.0, 77.0, 0.0, 0.0, 19.0, 18.63424726878573, -1.125652701804339, 0.0, 1.0, 115102.8472184616]
[2019-04-04 02:19:07,597] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 02:19:07,598] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [6.9197759e-15 1.3230293e-12 2.3894321e-11 1.0000000e+00 2.8152472e-10
 4.1392014e-09 4.1405654e-10], sampled 0.7443321592941535
[2019-04-04 02:19:11,201] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5565.8460 144499437.7245 -2172.8077
[2019-04-04 02:19:30,151] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 4969.8568 171424657.3513 -2880.0027
[2019-04-04 02:19:39,646] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5066.5999 185048419.1411 -2785.1128
[2019-04-04 02:19:40,682] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 3300000, evaluation results [3300000.0, 4969.856789961999, 171424657.3512828, -2880.002680936151, 5565.845956151186, 144499437.72453028, -2172.8077109675396, 5066.599925907876, 185048419.14109835, -2785.112797972731]
[2019-04-04 02:19:45,444] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.32866145e-30 2.27268178e-16 2.49746279e-23 1.00000000e+00
 6.87034818e-24 3.04133820e-14 5.41024134e-22], sum to 1.0000
[2019-04-04 02:19:45,444] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3099
[2019-04-04 02:19:45,448] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [16.43333333333334, 76.0, 0.0, 0.0, 24.0, 23.04266242236653, 0.00860394881347847, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1207200.0000, 
sim time next is 1207800.0000, 
raw observation next is [16.35, 76.5, 0.0, 0.0, 24.0, 23.03731796068913, 0.004740081232211525, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 1.0, 0.9155124653739612, 0.765, 0.0, 0.0, 0.5, 0.4197764967240942, 0.5015800270774039, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.12361896], dtype=float32), 0.6810399]. 
=============================================
[2019-04-04 02:19:45,850] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.6757719e-27 2.2159811e-17 1.0980489e-18 1.0000000e+00 1.5964316e-20
 6.0463176e-16 1.2163848e-18], sum to 1.0000
[2019-04-04 02:19:45,851] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7582
[2019-04-04 02:19:45,917] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.699999999999999, 84.33333333333334, 0.0, 0.0, 26.0, 26.07758834970149, 0.7172479103201201, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1633800.0000, 
sim time next is 1634400.0000, 
raw observation next is [6.6, 86.0, 0.0, 0.0, 26.0, 26.06849095202917, 0.7104396933554774, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.6454293628808865, 0.86, 0.0, 0.0, 0.6666666666666666, 0.6723742460024308, 0.7368132311184925, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.99063826], dtype=float32), -1.2687666]. 
=============================================
[2019-04-04 02:19:55,012] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.9205835e-24 2.0407644e-17 8.8787507e-18 1.0000000e+00 8.6913398e-22
 1.3173063e-15 1.1194423e-16], sum to 1.0000
[2019-04-04 02:19:55,016] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7619
[2019-04-04 02:19:55,051] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.3, 80.5, 0.0, 0.0, 26.0, 24.3984967247669, 0.2015856695176504, 0.0, 1.0, 42465.70877826943], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2161800.0000, 
sim time next is 2162400.0000, 
raw observation next is [-7.3, 80.0, 0.0, 0.0, 26.0, 24.47472864761495, 0.1965977163689807, 0.0, 1.0, 42438.38350495022], 
processed observation next is [1.0, 0.0, 0.26038781163434904, 0.8, 0.0, 0.0, 0.6666666666666666, 0.539560720634579, 0.5655325721229936, 0.0, 1.0, 0.20208754049976296], 
reward next is 0.7979, 
noisyNet noise sample is [array([0.8488475], dtype=float32), 0.4591544]. 
=============================================
[2019-04-04 02:20:22,134] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.8950869e-27 1.4072990e-20 8.6225648e-19 1.0000000e+00 4.5816567e-22
 4.4096932e-18 5.2398318e-20], sum to 1.0000
[2019-04-04 02:20:22,134] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7572
[2019-04-04 02:20:22,219] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.8, 50.33333333333334, 0.0, 0.0, 26.0, 24.89614421021635, 0.1573628455311399, 0.0, 1.0, 38390.31231722903], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2520600.0000, 
sim time next is 2521200.0000, 
raw observation next is [-1.9, 51.66666666666667, 0.0, 0.0, 26.0, 24.84487954120067, 0.1529710162076445, 0.0, 1.0, 38414.40256544557], 
processed observation next is [1.0, 0.17391304347826086, 0.4099722991689751, 0.5166666666666667, 0.0, 0.0, 0.6666666666666666, 0.5704066284333891, 0.5509903387358815, 0.0, 1.0, 0.18292572650212174], 
reward next is 0.8171, 
noisyNet noise sample is [array([-2.1716566], dtype=float32), 0.4098422]. 
=============================================
[2019-04-04 02:20:38,623] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.4613616e-21 6.7136192e-12 3.8619932e-14 1.0000000e+00 1.2813753e-12
 9.3544027e-12 1.2811094e-15], sum to 1.0000
[2019-04-04 02:20:38,623] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7538
[2019-04-04 02:20:38,631] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [12.75, 81.5, 0.0, 0.0, 23.0, 23.05115513212474, -0.05470332091024427, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 1063800.0000, 
sim time next is 1064400.0000, 
raw observation next is [12.56666666666667, 82.0, 0.0, 0.0, 23.0, 23.07270724408173, -0.06262294439453132, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.8107109879963068, 0.82, 0.0, 0.0, 0.4166666666666667, 0.4227256036734775, 0.47912568520182286, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4218594], dtype=float32), 0.29703313]. 
=============================================
[2019-04-04 02:20:46,509] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.94912589e-23 2.33415969e-16 1.20357236e-17 1.00000000e+00
 1.62087372e-19 1.00605132e-11 4.78746430e-17], sum to 1.0000
[2019-04-04 02:20:46,511] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8219
[2019-04-04 02:20:46,560] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 91.0, 0.0, 0.0, 25.0, 23.87700333628367, 0.06651651722699052, 0.0, 1.0, 43759.66837741698], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 2074800.0000, 
sim time next is 2075400.0000, 
raw observation next is [-4.5, 91.0, 0.0, 0.0, 25.0, 23.97331682599041, 0.06642828337519623, 0.0, 1.0, 43484.37127019024], 
processed observation next is [1.0, 0.0, 0.3379501385041552, 0.91, 0.0, 0.0, 0.5833333333333334, 0.49777640216586744, 0.5221427611250654, 0.0, 1.0, 0.20706843461995353], 
reward next is 0.7929, 
noisyNet noise sample is [array([0.89144605], dtype=float32), -0.11997804]. 
=============================================
[2019-04-04 02:21:01,277] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.5285965e-24 7.3014798e-19 3.5927080e-18 1.0000000e+00 1.4234486e-19
 1.2899088e-12 3.8561946e-17], sum to 1.0000
[2019-04-04 02:21:01,279] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0228
[2019-04-04 02:21:01,308] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.700000000000001, 78.0, 0.0, 0.0, 26.0, 24.40858056179385, 0.1737372825479641, 0.0, 1.0, 42530.78579266041], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2167800.0000, 
sim time next is 2168400.0000, 
raw observation next is [-6.700000000000001, 78.0, 0.0, 0.0, 26.0, 24.52698593230454, 0.1729116291169707, 0.0, 1.0, 42478.45373511202], 
processed observation next is [1.0, 0.08695652173913043, 0.2770083102493075, 0.78, 0.0, 0.0, 0.6666666666666666, 0.5439154943587118, 0.5576372097056569, 0.0, 1.0, 0.20227835111958106], 
reward next is 0.7977, 
noisyNet noise sample is [array([1.0110227], dtype=float32), -2.1683173]. 
=============================================
[2019-04-04 02:21:10,329] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2873307e-23 5.5507865e-15 1.5285685e-15 1.0000000e+00 5.1028797e-18
 3.2319840e-14 1.3136375e-16], sum to 1.0000
[2019-04-04 02:21:10,329] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5431
[2019-04-04 02:21:10,362] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 95.33333333333334, 60.16666666666667, 51.83333333333333, 26.0, 25.58478061275201, 0.4462651066226315, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2911200.0000, 
sim time next is 2911800.0000, 
raw observation next is [2.0, 94.16666666666666, 49.33333333333334, 49.66666666666667, 26.0, 25.7628263040425, 0.4623176190070954, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.518005540166205, 0.9416666666666665, 0.16444444444444448, 0.05488029465930019, 0.6666666666666666, 0.6469021920035415, 0.6541058730023651, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6974836], dtype=float32), -0.14813079]. 
=============================================
[2019-04-04 02:21:39,648] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.6560716e-25 4.3338624e-19 6.6686463e-19 1.0000000e+00 5.6047474e-22
 1.4348153e-19 5.8270295e-17], sum to 1.0000
[2019-04-04 02:21:39,652] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0538
[2019-04-04 02:21:39,713] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.816666666666666, 81.33333333333334, 0.0, 0.0, 26.0, 23.6162226369834, -0.08283602968611457, 0.0, 1.0, 45291.68592412822], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1918200.0000, 
sim time next is 1918800.0000, 
raw observation next is [-8.9, 82.0, 0.0, 0.0, 26.0, 23.64798938437124, -0.08964832350699359, 0.0, 1.0, 45241.4760234663], 
processed observation next is [1.0, 0.21739130434782608, 0.21606648199445982, 0.82, 0.0, 0.0, 0.6666666666666666, 0.4706657820309366, 0.4701172254976688, 0.0, 1.0, 0.21543560011174429], 
reward next is 0.7846, 
noisyNet noise sample is [array([-0.63628983], dtype=float32), -1.5259598]. 
=============================================
[2019-04-04 02:22:06,695] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.2839381e-22 3.5968620e-11 4.0765653e-14 9.9999988e-01 2.6064572e-16
 1.5648894e-07 1.8553669e-14], sum to 1.0000
[2019-04-04 02:22:06,695] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1650
[2019-04-04 02:22:06,752] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.833333333333333, 100.0, 0.0, 0.0, 26.0, 25.74558900754807, 0.7232534172184595, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3186600.0000, 
sim time next is 3187200.0000, 
raw observation next is [2.666666666666667, 100.0, 0.0, 0.0, 26.0, 25.80062914887329, 0.7208340439853943, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.5364727608494922, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6500524290727743, 0.7402780146617981, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3853944], dtype=float32), 0.60141826]. 
=============================================
[2019-04-04 02:22:13,601] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.4399173e-23 1.5022540e-16 7.6286310e-18 1.0000000e+00 1.2861124e-17
 5.9795274e-15 2.5421556e-18], sum to 1.0000
[2019-04-04 02:22:13,601] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2788
[2019-04-04 02:22:13,609] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.5, 25.5, 79.0, 50.66666666666667, 25.0, 24.72418367714764, 0.1587463980512842, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 2823000.0000, 
sim time next is 2823600.0000, 
raw observation next is [6.4, 26.0, 75.0, 63.33333333333334, 25.0, 24.88298714692728, 0.163359791057536, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.6398891966759004, 0.26, 0.25, 0.0699815837937385, 0.5833333333333334, 0.57358226224394, 0.5544532636858454, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.401491], dtype=float32), 0.25110215]. 
=============================================
[2019-04-04 02:22:23,155] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.2477698e-26 3.2739786e-20 3.3375713e-20 1.0000000e+00 4.1739976e-24
 6.0271287e-18 4.8809152e-19], sum to 1.0000
[2019-04-04 02:22:23,155] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2294
[2019-04-04 02:22:23,300] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 65.0, 243.0, 240.6666666666667, 26.0, 24.98308096688696, 0.3449882267427517, 0.0, 1.0, 33515.15907698083], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2978400.0000, 
sim time next is 2979000.0000, 
raw observation next is [-3.0, 65.0, 256.0, 284.0, 26.0, 24.96447596776485, 0.3548781826984213, 0.0, 1.0, 49131.83328605622], 
processed observation next is [0.0, 0.4782608695652174, 0.3795013850415513, 0.65, 0.8533333333333334, 0.3138121546961326, 0.6666666666666666, 0.5803729973137376, 0.6182927275661404, 0.0, 1.0, 0.233961110885982], 
reward next is 0.7660, 
noisyNet noise sample is [array([-0.22821367], dtype=float32), 0.3675579]. 
=============================================
[2019-04-04 02:22:23,318] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[88.39539 ]
 [88.488075]
 [88.52511 ]
 [88.436714]
 [88.14396 ]], R is [[88.23395538]
 [88.1920166 ]
 [88.22085571]
 [88.24938965]
 [88.27762604]].
[2019-04-04 02:22:24,096] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.7669531e-30 4.4794954e-25 8.8273617e-23 1.0000000e+00 5.7496243e-29
 3.2439663e-21 4.6743369e-23], sum to 1.0000
[2019-04-04 02:22:24,097] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0651
[2019-04-04 02:22:24,212] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.3, 50.0, 50.5, 540.5, 26.0, 24.92126487712456, 0.2356796982397268, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2451600.0000, 
sim time next is 2452200.0000, 
raw observation next is [-7.016666666666667, 48.83333333333334, 54.0, 582.0, 26.0, 25.21391947626088, 0.2521373236485439, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.2682363804247461, 0.48833333333333345, 0.18, 0.6430939226519337, 0.6666666666666666, 0.6011599563550734, 0.5840457745495146, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0479383], dtype=float32), -1.7703156]. 
=============================================
[2019-04-04 02:22:28,742] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.7081105e-21 1.0288112e-12 8.5197250e-14 1.0000000e+00 1.8335562e-17
 3.7171433e-12 3.3393088e-13], sum to 1.0000
[2019-04-04 02:22:28,742] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4376
[2019-04-04 02:22:28,771] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 25.0, 24.60499603615547, 0.3915615927037917, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 3531600.0000, 
sim time next is 3532200.0000, 
raw observation next is [-0.1666666666666667, 73.0, 0.0, 0.0, 25.0, 24.83879582810139, 0.4072451051169517, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.4579870729455217, 0.73, 0.0, 0.0, 0.5833333333333334, 0.5698996523417824, 0.6357483683723172, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.9144342], dtype=float32), -0.63195044]. 
=============================================
[2019-04-04 02:22:39,223] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.6867567e-26 1.0317885e-19 8.8401807e-20 1.0000000e+00 1.9917673e-22
 6.5822459e-16 4.0164018e-19], sum to 1.0000
[2019-04-04 02:22:39,223] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0994
[2019-04-04 02:22:39,255] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.166666666666667, 62.16666666666667, 0.0, 0.0, 26.0, 25.72347269151781, 0.4454757751918333, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3703800.0000, 
sim time next is 3704400.0000, 
raw observation next is [2.0, 62.0, 0.0, 0.0, 26.0, 25.67534724644636, 0.4376466565995781, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.9130434782608695, 0.518005540166205, 0.62, 0.0, 0.0, 0.6666666666666666, 0.6396122705371967, 0.645882218866526, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5336896], dtype=float32), 0.05166891]. 
=============================================
[2019-04-04 02:22:40,185] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0207338e-25 2.2547845e-20 1.3848046e-20 1.0000000e+00 5.1583350e-22
 1.0861740e-21 6.7470712e-20], sum to 1.0000
[2019-04-04 02:22:40,185] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8329
[2019-04-04 02:22:40,274] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 25.0, 24.17582365556967, 0.08347276692950233, 0.0, 1.0, 38659.25131967622], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 3024000.0000, 
sim time next is 3024600.0000, 
raw observation next is [-4.166666666666667, 66.0, 0.0, 0.0, 25.0, 24.16032037143231, 0.07959330273233077, 0.0, 1.0, 38584.49330512944], 
processed observation next is [0.0, 0.0, 0.3471837488457987, 0.66, 0.0, 0.0, 0.5833333333333334, 0.5133600309526924, 0.5265311009107769, 0.0, 1.0, 0.1837356824053783], 
reward next is 0.8163, 
noisyNet noise sample is [array([0.639043], dtype=float32), 0.44229743]. 
=============================================
[2019-04-04 02:23:07,859] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.6811887e-21 3.0382682e-12 5.5524386e-15 1.0000000e+00 6.8474527e-17
 5.0629355e-11 1.5607651e-13], sum to 1.0000
[2019-04-04 02:23:07,859] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4198
[2019-04-04 02:23:07,888] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 58.33333333333334, 0.0, 0.0, 25.0, 24.43370837675146, 0.2470919802160783, 0.0, 1.0, 106901.2747769853], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 3883200.0000, 
sim time next is 3883800.0000, 
raw observation next is [-1.0, 59.16666666666666, 0.0, 0.0, 25.0, 24.41803912493379, 0.2546775678093876, 0.0, 1.0, 76332.64314548037], 
processed observation next is [1.0, 0.9565217391304348, 0.4349030470914128, 0.5916666666666666, 0.0, 0.0, 0.5833333333333334, 0.5348365937444827, 0.5848925226031292, 0.0, 1.0, 0.36348877688323983], 
reward next is 0.6365, 
noisyNet noise sample is [array([0.46991855], dtype=float32), -0.32840618]. 
=============================================
[2019-04-04 02:23:19,440] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.0299510e-23 2.1084300e-14 1.2652135e-15 1.0000000e+00 4.9376072e-17
 4.5388684e-12 8.3260094e-16], sum to 1.0000
[2019-04-04 02:23:19,440] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1380
[2019-04-04 02:23:19,456] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 93.0, 0.0, 0.0, 26.0, 25.37977119856581, 0.4023866047783777, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2916000.0000, 
sim time next is 2916600.0000, 
raw observation next is [0.6666666666666667, 92.83333333333333, 0.0, 0.0, 26.0, 25.24677360737012, 0.3937641293254325, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.4810710987996307, 0.9283333333333332, 0.0, 0.0, 0.6666666666666666, 0.6038978006141766, 0.6312547097751442, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-3.3926024], dtype=float32), -0.51037604]. 
=============================================
[2019-04-04 02:23:33,851] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.9900210e-29 1.6400177e-19 4.0341812e-20 1.0000000e+00 3.3927257e-23
 3.3350132e-20 1.3919378e-21], sum to 1.0000
[2019-04-04 02:23:33,852] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2829
[2019-04-04 02:23:33,917] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 47.66666666666666, 116.3333333333333, 815.1666666666667, 26.0, 25.38775139407254, 0.5158575954794298, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3415200.0000, 
sim time next is 3415800.0000, 
raw observation next is [3.0, 48.33333333333334, 115.6666666666667, 813.3333333333334, 26.0, 25.66542268573087, 0.5104324485253714, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.5457063711911359, 0.48333333333333345, 0.38555555555555565, 0.8987108655616943, 0.6666666666666666, 0.6387852238109059, 0.6701441495084571, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0301368], dtype=float32), -0.33423364]. 
=============================================
[2019-04-04 02:23:34,301] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.4102936e-33 6.6711709e-25 6.3108904e-24 1.0000000e+00 1.6340027e-28
 1.1266258e-25 1.0135562e-24], sum to 1.0000
[2019-04-04 02:23:34,301] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6419
[2019-04-04 02:23:34,310] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.5, 42.5, 113.0, 818.0, 26.0, 25.29170803107861, 0.4550776071936456, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3677400.0000, 
sim time next is 3678000.0000, 
raw observation next is [5.666666666666667, 42.66666666666666, 111.5, 811.0, 26.0, 25.29562456359479, 0.4572522396888783, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.6195752539242845, 0.4266666666666666, 0.37166666666666665, 0.8961325966850828, 0.6666666666666666, 0.6079687136328991, 0.6524174132296261, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.23129657], dtype=float32), 1.3631825]. 
=============================================
[2019-04-04 02:23:34,351] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[86.95544 ]
 [86.57098 ]
 [86.09348 ]
 [85.51512 ]
 [84.931496]], R is [[87.33318329]
 [87.45985413]
 [87.58525848]
 [87.70940399]
 [87.83231354]].
[2019-04-04 02:23:55,905] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.8469223e-29 4.8704662e-25 1.3416751e-22 1.0000000e+00 7.1848249e-27
 5.4199943e-21 1.9117400e-23], sum to 1.0000
[2019-04-04 02:23:55,923] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9553
[2019-04-04 02:23:55,952] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.0, 71.0, 0.0, 0.0, 26.0, 24.51234156470041, 0.1692354898238438, 0.0, 1.0, 38262.31163287497], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3030000.0000, 
sim time next is 3030600.0000, 
raw observation next is [-5.0, 71.0, 0.0, 0.0, 26.0, 24.46807408153463, 0.1602453395050535, 0.0, 1.0, 38373.36232657857], 
processed observation next is [0.0, 0.043478260869565216, 0.32409972299168976, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5390061734612193, 0.5534151131683512, 0.0, 1.0, 0.1827302967932313], 
reward next is 0.8173, 
noisyNet noise sample is [array([-0.98820174], dtype=float32), 0.06950116]. 
=============================================
[2019-04-04 02:24:02,003] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.3280462e-31 2.0816178e-25 2.1637569e-24 1.0000000e+00 3.5555642e-28
 1.9678152e-26 1.4432105e-24], sum to 1.0000
[2019-04-04 02:24:02,003] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8596
[2019-04-04 02:24:02,087] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.13562238869988, 0.363342247542967, 0.0, 1.0, 18704.7043245645], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3607800.0000, 
sim time next is 3608400.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.12728810818255, 0.3548551005197975, 0.0, 1.0, 18704.48826934167], 
processed observation next is [0.0, 0.782608695652174, 0.4349030470914128, 0.42, 0.0, 0.0, 0.6666666666666666, 0.5939406756818792, 0.6182850335065991, 0.0, 1.0, 0.08906899175876985], 
reward next is 0.9109, 
noisyNet noise sample is [array([1.0248482], dtype=float32), 0.41610926]. 
=============================================
[2019-04-04 02:24:09,935] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.0958995e-22 1.5212485e-14 1.2179972e-16 1.0000000e+00 1.8700899e-18
 1.1270872e-11 2.0784782e-15], sum to 1.0000
[2019-04-04 02:24:09,936] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7940
[2019-04-04 02:24:09,959] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.833333333333334, 49.0, 108.6666666666667, 747.3333333333334, 25.0, 25.59455352081751, 0.395853454858709, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 3924600.0000, 
sim time next is 3925200.0000, 
raw observation next is [-6.666666666666667, 49.0, 110.8333333333333, 761.1666666666667, 25.0, 25.6139980782982, 0.4035794880000977, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.27793167128347185, 0.49, 0.36944444444444435, 0.8410681399631676, 0.5833333333333334, 0.6344998398581833, 0.6345264960000326, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7577689], dtype=float32), -1.7500744]. 
=============================================
[2019-04-04 02:24:12,587] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.7788764e-22 3.5959207e-12 1.3186639e-15 1.0000000e+00 9.1681533e-16
 2.0876953e-12 9.7123974e-15], sum to 1.0000
[2019-04-04 02:24:12,587] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6364
[2019-04-04 02:24:12,615] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [10.46666666666667, 58.66666666666666, 0.0, 0.0, 26.0, 26.96467147591734, 0.8687889909137486, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4394400.0000, 
sim time next is 4395000.0000, 
raw observation next is [10.33333333333333, 58.83333333333334, 0.0, 0.0, 26.0, 26.9281019263427, 0.8568508926034503, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.7488457987072946, 0.5883333333333334, 0.0, 0.0, 0.6666666666666666, 0.7440084938618915, 0.7856169642011501, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4925668], dtype=float32), 0.41109523]. 
=============================================
[2019-04-04 02:24:12,618] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[80.120575]
 [80.190155]
 [80.679504]
 [81.230576]
 [80.75547 ]], R is [[80.27194214]
 [80.46922302]
 [80.66452789]
 [80.85787964]
 [81.04930115]].
[2019-04-04 02:24:13,174] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.0846009e-22 3.4607066e-14 6.1280331e-15 1.0000000e+00 6.4159677e-18
 8.9666236e-13 3.7887912e-14], sum to 1.0000
[2019-04-04 02:24:13,175] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7774
[2019-04-04 02:24:13,215] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 61.0, 0.0, 0.0, 25.0, 24.8064294103762, 0.2619598762907114, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 4572600.0000, 
sim time next is 4573200.0000, 
raw observation next is [1.0, 61.0, 0.0, 0.0, 25.0, 24.74010833493893, 0.2319316584437996, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.4903047091412743, 0.61, 0.0, 0.0, 0.5833333333333334, 0.5616756945782443, 0.5773105528145999, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4300923], dtype=float32), 0.9630342]. 
=============================================
[2019-04-04 02:24:14,650] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.2172711e-22 2.2621121e-16 1.1310755e-15 1.0000000e+00 3.4587263e-18
 3.7663850e-13 1.7219306e-14], sum to 1.0000
[2019-04-04 02:24:14,651] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8426
[2019-04-04 02:24:14,683] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-12.0, 63.0, 0.0, 0.0, 25.0, 23.7578799514086, 0.03123350578826606, 0.0, 1.0, 44421.79246121606], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 3982200.0000, 
sim time next is 3982800.0000, 
raw observation next is [-12.0, 63.00000000000001, 0.0, 0.0, 25.0, 23.69214019717535, 0.02621732183165358, 0.0, 1.0, 44404.97427043357], 
processed observation next is [1.0, 0.08695652173913043, 0.13019390581717452, 0.6300000000000001, 0.0, 0.0, 0.5833333333333334, 0.474345016431279, 0.5087391072772178, 0.0, 1.0, 0.21145225843063606], 
reward next is 0.7885, 
noisyNet noise sample is [array([-0.2646675], dtype=float32), 2.2842867]. 
=============================================
[2019-04-04 02:24:24,748] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.23043395e-30 1.43661284e-25 1.08422723e-23 1.00000000e+00
 3.70491047e-28 2.44329723e-23 7.13343882e-24], sum to 1.0000
[2019-04-04 02:24:24,749] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0472
[2019-04-04 02:24:24,855] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.833333333333334, 52.5, 61.33333333333334, 325.3333333333334, 26.0, 24.54119581095986, 0.2405845543772399, 0.0, 1.0, 39750.0607744546], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4176600.0000, 
sim time next is 4177200.0000, 
raw observation next is [-4.666666666666667, 51.00000000000001, 76.66666666666667, 406.6666666666667, 26.0, 24.60824072388738, 0.3272983664172431, 0.0, 1.0, 201847.932780909], 
processed observation next is [0.0, 0.34782608695652173, 0.3333333333333333, 0.5100000000000001, 0.2555555555555556, 0.44935543278084716, 0.6666666666666666, 0.550686726990615, 0.6090994554724144, 0.0, 1.0, 0.9611806322900429], 
reward next is 0.0388, 
noisyNet noise sample is [array([-0.2196406], dtype=float32), -0.05722247]. 
=============================================
[2019-04-04 02:24:28,464] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.9028111e-23 3.8142849e-14 1.4781290e-16 1.0000000e+00 2.2539366e-17
 2.5622289e-12 5.5957309e-15], sum to 1.0000
[2019-04-04 02:24:28,469] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2068
[2019-04-04 02:24:28,488] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 43.0, 0.0, 0.0, 25.0, 24.40938948014139, 0.2500449752672032, 0.0, 1.0, 64036.86211738238], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 4143600.0000, 
sim time next is 4144200.0000, 
raw observation next is [-0.1666666666666667, 42.83333333333334, 0.0, 0.0, 25.0, 24.44941682385143, 0.2531665336434117, 0.0, 1.0, 25370.05324888024], 
processed observation next is [1.0, 1.0, 0.4579870729455217, 0.42833333333333345, 0.0, 0.0, 0.5833333333333334, 0.5374514019876191, 0.5843888445478039, 0.0, 1.0, 0.12080977737562018], 
reward next is 0.8792, 
noisyNet noise sample is [array([-1.7214835], dtype=float32), 0.72471035]. 
=============================================
[2019-04-04 02:24:30,703] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.7662309e-26 5.4068398e-21 1.8254787e-20 1.0000000e+00 4.2291642e-24
 5.9729070e-18 6.5640819e-19], sum to 1.0000
[2019-04-04 02:24:30,705] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9133
[2019-04-04 02:24:30,720] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.666666666666667, 58.33333333333334, 0.0, 0.0, 26.0, 25.40633113876535, 0.3765063405566116, 0.0, 1.0, 40975.21078849763], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4837200.0000, 
sim time next is 4837800.0000, 
raw observation next is [-1.833333333333333, 59.16666666666666, 0.0, 0.0, 26.0, 25.38680147538033, 0.3836504382481745, 0.0, 1.0, 50563.54120581362], 
processed observation next is [0.0, 1.0, 0.41181902123730385, 0.5916666666666666, 0.0, 0.0, 0.6666666666666666, 0.6155667896150275, 0.6278834794160582, 0.0, 1.0, 0.24077876764673153], 
reward next is 0.7592, 
noisyNet noise sample is [array([0.49010885], dtype=float32), -0.31217477]. 
=============================================
[2019-04-04 02:24:41,474] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.1936240e-29 8.4146402e-25 1.2594925e-23 1.0000000e+00 1.1289533e-25
 6.4949985e-23 8.3303363e-22], sum to 1.0000
[2019-04-04 02:24:41,475] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4363
[2019-04-04 02:24:41,503] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.5, 48.0, 0.0, 0.0, 26.0, 25.25902669455928, 0.3807279355264934, 0.0, 1.0, 39270.17506555223], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4156200.0000, 
sim time next is 4156800.0000, 
raw observation next is [-2.666666666666667, 48.66666666666666, 0.0, 0.0, 26.0, 25.23215073131762, 0.3738732060642508, 0.0, 1.0, 39322.76847301539], 
processed observation next is [0.0, 0.08695652173913043, 0.38873499538319484, 0.4866666666666666, 0.0, 0.0, 0.6666666666666666, 0.6026792276098017, 0.6246244020214169, 0.0, 1.0, 0.18725127844293044], 
reward next is 0.8127, 
noisyNet noise sample is [array([0.32577088], dtype=float32), 1.1846509]. 
=============================================
[2019-04-04 02:24:43,858] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:24:43,860] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:24:43,894] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run26
[2019-04-04 02:24:53,100] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:24:53,101] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:24:53,105] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run26
[2019-04-04 02:24:54,530] A3C_AGENT_WORKER-Thread-2 INFO:Local step 212500, global step 3381078: loss 0.0151
[2019-04-04 02:24:54,532] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 212500, global step 3381078: learning rate 0.0005
[2019-04-04 02:24:56,805] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:24:56,805] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:24:56,809] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run26
[2019-04-04 02:25:04,255] A3C_AGENT_WORKER-Thread-17 INFO:Local step 212500, global step 3384036: loss 0.0090
[2019-04-04 02:25:04,283] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 212500, global step 3384037: learning rate 0.0005
[2019-04-04 02:25:07,394] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.9180852e-19 4.3395929e-10 5.6933868e-13 9.9999690e-01 1.3532965e-13
 3.1516402e-06 1.8415744e-12], sum to 1.0000
[2019-04-04 02:25:07,395] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0835
[2019-04-04 02:25:07,483] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 78.0, 0.0, 0.0, 25.0, 24.03080982308479, 0.1943338503745193, 0.0, 1.0, 89798.39776130537], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 4737600.0000, 
sim time next is 4738200.0000, 
raw observation next is [-1.166666666666667, 79.16666666666667, 0.0, 0.0, 25.0, 24.02115343876807, 0.2050876286123879, 0.0, 1.0, 67769.94716508003], 
processed observation next is [1.0, 0.8695652173913043, 0.43028624192059095, 0.7916666666666667, 0.0, 0.0, 0.5833333333333334, 0.5017627865640059, 0.568362542870796, 0.0, 1.0, 0.32271403411942867], 
reward next is 0.6773, 
noisyNet noise sample is [array([-0.29997456], dtype=float32), 0.6950271]. 
=============================================
[2019-04-04 02:25:07,873] A3C_AGENT_WORKER-Thread-19 INFO:Local step 212500, global step 3385253: loss -1.0654
[2019-04-04 02:25:07,889] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 212500, global step 3385253: learning rate 0.0005
[2019-04-04 02:25:08,435] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.03522119e-22 3.88156118e-10 8.46054929e-14 9.99999404e-01
 2.13517286e-16 5.47295258e-07 1.06773626e-13], sum to 1.0000
[2019-04-04 02:25:08,453] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4618
[2019-04-04 02:25:08,462] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [8.350000000000001, 62.16666666666667, 0.0, 0.0, 26.0, 25.77586727021641, 0.5855824968933102, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4403400.0000, 
sim time next is 4404000.0000, 
raw observation next is [8.2, 62.33333333333334, 0.0, 0.0, 26.0, 25.69782828703362, 0.5829887994850883, 0.0, 1.0, 196217.9094192961], 
processed observation next is [1.0, 1.0, 0.6897506925207757, 0.6233333333333334, 0.0, 0.0, 0.6666666666666666, 0.6414856905861349, 0.6943295998283627, 0.0, 1.0, 0.9343709972347434], 
reward next is 0.0656, 
noisyNet noise sample is [array([-0.97921723], dtype=float32), 0.9462905]. 
=============================================
[2019-04-04 02:25:08,466] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[85.26815]
 [85.1643 ]
 [84.98633]
 [84.8518 ]
 [84.67616]], R is [[84.24404907]
 [84.40161133]
 [84.5575943 ]
 [84.71202087]
 [84.86489868]].
[2019-04-04 02:25:12,071] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:25:12,072] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:25:12,077] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run26
[2019-04-04 02:25:24,365] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.7417550e-23 2.5319068e-14 4.8593374e-15 1.0000000e+00 6.2620874e-17
 6.3570026e-13 1.4565853e-15], sum to 1.0000
[2019-04-04 02:25:24,365] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6012
[2019-04-04 02:25:24,403] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.1, 64.0, 0.0, 0.0, 26.0, 25.43772817581679, 0.4439018334978964, 0.0, 1.0, 25058.53367822241], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4584600.0000, 
sim time next is 4585200.0000, 
raw observation next is [2.775557561562891e-17, 64.33333333333333, 0.0, 0.0, 26.0, 25.40427463300461, 0.4483374957507899, 0.0, 1.0, 46277.22456775677], 
processed observation next is [1.0, 0.043478260869565216, 0.46260387811634357, 0.6433333333333333, 0.0, 0.0, 0.6666666666666666, 0.6170228860837176, 0.64944583191693, 0.0, 1.0, 0.220367736036937], 
reward next is 0.7796, 
noisyNet noise sample is [array([1.4905403], dtype=float32), 1.6483972]. 
=============================================
[2019-04-04 02:25:25,481] A3C_AGENT_WORKER-Thread-11 INFO:Local step 212500, global step 3390094: loss 0.0893
[2019-04-04 02:25:25,481] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 212500, global step 3390094: learning rate 0.0005
[2019-04-04 02:25:28,902] A3C_AGENT_WORKER-Thread-2 INFO:Local step 213000, global step 3391056: loss 0.0242
[2019-04-04 02:25:28,904] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 213000, global step 3391057: learning rate 0.0005
[2019-04-04 02:25:34,730] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [9.4166553e-25 2.5846157e-16 2.8534428e-16 1.0000000e+00 1.5654300e-18
 1.5096123e-13 1.3693684e-16], sum to 1.0000
[2019-04-04 02:25:34,730] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8737
[2019-04-04 02:25:34,764] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.333333333333333, 49.16666666666667, 0.0, 0.0, 26.0, 25.35316518912567, 0.3649166333906997, 0.0, 1.0, 61558.54033260729], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5033400.0000, 
sim time next is 5034000.0000, 
raw observation next is [-1.666666666666667, 52.33333333333334, 0.0, 0.0, 26.0, 25.33592232757877, 0.3581883437672531, 0.0, 1.0, 46071.79864273935], 
processed observation next is [1.0, 0.2608695652173913, 0.4164358264081256, 0.5233333333333334, 0.0, 0.0, 0.6666666666666666, 0.6113268606315643, 0.6193961145890844, 0.0, 1.0, 0.21938951734637785], 
reward next is 0.7806, 
noisyNet noise sample is [array([0.14462481], dtype=float32), 0.9584698]. 
=============================================
[2019-04-04 02:25:34,787] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[84.06379 ]
 [83.97758 ]
 [84.01841 ]
 [84.021706]
 [84.10175 ]], R is [[83.9630661 ]
 [83.83029938]
 [83.60125732]
 [83.56443787]
 [83.58611298]].
[2019-04-04 02:25:36,849] A3C_AGENT_WORKER-Thread-17 INFO:Local step 213000, global step 3394155: loss 0.0411
[2019-04-04 02:25:36,852] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 213000, global step 3394156: learning rate 0.0005
[2019-04-04 02:25:36,945] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:25:36,945] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:25:36,951] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run26
[2019-04-04 02:25:37,181] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:25:37,182] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:25:37,186] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run26
[2019-04-04 02:25:37,281] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.6112550e-27 3.8998517e-16 4.0927798e-17 1.0000000e+00 6.0817408e-21
 1.7161843e-16 3.7865256e-20], sum to 1.0000
[2019-04-04 02:25:37,284] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7170
[2019-04-04 02:25:37,305] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [12.0, 17.0, 56.0, 438.5, 26.0, 29.16177294860935, 1.202293919974713, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5072400.0000, 
sim time next is 5073000.0000, 
raw observation next is [11.83333333333333, 17.0, 49.33333333333333, 389.6666666666666, 26.0, 28.86694454549286, 1.175853961763354, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.7903970452446908, 0.17, 0.16444444444444442, 0.4305709023941067, 0.6666666666666666, 0.905578712124405, 0.8919513205877848, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1611483], dtype=float32), 0.9000086]. 
=============================================
[2019-04-04 02:25:37,328] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[83.98343 ]
 [84.19737 ]
 [84.6158  ]
 [85.079735]
 [85.28327 ]], R is [[84.1280365 ]
 [84.28675842]
 [84.44389343]
 [84.59945679]
 [84.75346375]].
[2019-04-04 02:25:39,244] A3C_AGENT_WORKER-Thread-19 INFO:Local step 213000, global step 3394912: loss 0.0177
[2019-04-04 02:25:39,244] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 213000, global step 3394912: learning rate 0.0005
[2019-04-04 02:25:39,828] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:25:39,828] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:25:39,843] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run26
[2019-04-04 02:25:41,133] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:25:41,133] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:25:41,136] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run26
[2019-04-04 02:25:44,545] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:25:44,545] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:25:44,562] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run26
[2019-04-04 02:25:46,767] A3C_AGENT_WORKER-Thread-20 INFO:Local step 212500, global step 3397026: loss 0.1091
[2019-04-04 02:25:46,770] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 212500, global step 3397026: learning rate 0.0005
[2019-04-04 02:25:47,247] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.3093012e-18 4.4504509e-12 2.2422460e-11 1.0000000e+00 1.8490849e-11
 3.7797063e-09 5.8476084e-12], sum to 1.0000
[2019-04-04 02:25:47,247] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0893
[2019-04-04 02:25:47,312] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [8.0, 86.0, 87.0, 0.0, 22.0, 20.77688972842103, -0.6282086828540097, 0.0, 1.0, 32151.10158650197], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 48600.0000, 
sim time next is 49200.0000, 
raw observation next is [7.9, 86.0, 85.66666666666666, 0.0, 22.0, 20.78124934340185, -0.6246186889036877, 0.0, 1.0, 26727.76202504698], 
processed observation next is [0.0, 0.5652173913043478, 0.6814404432132966, 0.86, 0.2855555555555555, 0.0, 0.3333333333333333, 0.23177077861682083, 0.29179377036543747, 0.0, 1.0, 0.12727505726212848], 
reward next is 0.8727, 
noisyNet noise sample is [array([0.21566343], dtype=float32), -0.7242789]. 
=============================================
[2019-04-04 02:25:48,235] A3C_AGENT_WORKER-Thread-10 INFO:Local step 212500, global step 3397535: loss 0.1631
[2019-04-04 02:25:48,237] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 212500, global step 3397536: learning rate 0.0005
[2019-04-04 02:25:49,615] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.9139353e-19 4.0959418e-09 4.5951732e-12 9.9831676e-01 3.5660914e-12
 1.6832382e-03 7.8888762e-12], sum to 1.0000
[2019-04-04 02:25:49,621] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9927
[2019-04-04 02:25:49,632] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.466666666666667, 63.33333333333333, 0.0, 0.0, 26.0, 25.77811917622902, 0.6408556172412149, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4407000.0000, 
sim time next is 4407600.0000, 
raw observation next is [7.333333333333334, 63.66666666666667, 0.0, 0.0, 26.0, 25.81153660722484, 0.6356338498407194, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.6657433056325024, 0.6366666666666667, 0.0, 0.0, 0.6666666666666666, 0.6509613839354035, 0.7118779499469065, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.76978505], dtype=float32), 1.2617342]. 
=============================================
[2019-04-04 02:25:50,242] A3C_AGENT_WORKER-Thread-6 INFO:Local step 212500, global step 3398249: loss 0.0214
[2019-04-04 02:25:50,262] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 212500, global step 3398249: learning rate 0.0005
[2019-04-04 02:25:51,968] A3C_AGENT_WORKER-Thread-15 INFO:Local step 212500, global step 3398928: loss 0.2055
[2019-04-04 02:25:51,968] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 212500, global step 3398928: learning rate 0.0005
[2019-04-04 02:25:52,372] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:25:52,372] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:25:52,389] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run26
[2019-04-04 02:25:53,084] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:25:53,085] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:25:53,088] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run26
[2019-04-04 02:25:54,426] A3C_AGENT_WORKER-Thread-11 INFO:Local step 213000, global step 3399693: loss 0.0061
[2019-04-04 02:25:54,446] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 213000, global step 3399693: learning rate 0.0005
[2019-04-04 02:25:54,565] A3C_AGENT_WORKER-Thread-18 INFO:Local step 212500, global step 3399725: loss 0.0383
[2019-04-04 02:25:54,566] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 212500, global step 3399725: learning rate 0.0005
[2019-04-04 02:25:55,617] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-04 02:25:55,619] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 02:25:55,619] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:25:55,621] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run35
[2019-04-04 02:25:55,639] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 02:25:55,641] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 02:25:55,642] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:25:55,642] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:25:55,646] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run35
[2019-04-04 02:25:55,675] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run35
[2019-04-04 02:26:13,219] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.7004609], dtype=float32), 0.19534795]
[2019-04-04 02:26:13,219] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [0.6, 77.16666666666667, 0.0, 0.0, 21.0, 20.22929154403034, -0.8989339692960248, 1.0, 1.0, 0.0]
[2019-04-04 02:26:13,219] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 02:26:13,220] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.8975570e-18 5.0591388e-16 1.0511345e-14 1.0000000e+00 1.1767619e-13
 5.6633170e-13 6.2551974e-14], sampled 0.2748646051171646
[2019-04-04 02:26:22,683] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.7004609], dtype=float32), 0.19534795]
[2019-04-04 02:26:22,683] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-0.45, 73.5, 99.0, 328.0, 23.0, 22.43190136357735, -0.4434977992048135, 1.0, 1.0, 0.0]
[2019-04-04 02:26:22,683] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 02:26:22,684] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [2.6349453e-19 2.2522596e-14 1.7448762e-13 1.0000000e+00 5.5281788e-14
 4.8088755e-11 1.0912556e-13], sampled 0.46162978327341875
[2019-04-04 02:26:24,938] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.7004609], dtype=float32), 0.19534795]
[2019-04-04 02:26:24,938] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [16.36666666666667, 66.33333333333334, 37.5, 178.8333333333333, 23.0, 21.86115826205305, -0.3335830965161839, 1.0, 0.0, 0.0]
[2019-04-04 02:26:24,938] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 02:26:24,939] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [3.0511930e-21 7.3404474e-09 4.8564305e-14 1.0000000e+00 8.2078075e-14
 5.8165739e-08 2.4861613e-15], sampled 0.9015284718304878
[2019-04-04 02:27:20,537] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.7004609], dtype=float32), 0.19534795]
[2019-04-04 02:27:20,537] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-2.346570883, 83.86379106000001, 75.8819059, 841.1392079, 24.0, 24.26573430974558, 0.1756028846630167, 1.0, 1.0, 0.0]
[2019-04-04 02:27:20,537] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 02:27:20,538] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [8.4641437e-20 4.3075102e-14 4.3686747e-13 1.0000000e+00 9.7069849e-16
 3.8003697e-11 9.0707572e-14], sampled 0.8698527161833892
[2019-04-04 02:27:26,781] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7475.9453 198876828.4025 -74.6587
[2019-04-04 02:27:42,757] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 6912.6959 218923171.1264 -670.8758
[2019-04-04 02:27:53,189] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 6800.5496 240568787.3949 -448.8398
[2019-04-04 02:27:54,216] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 3400000, evaluation results [3400000.0, 6912.69592546996, 218923171.12642926, -670.875804698099, 7475.945308960262, 198876828.40253136, -74.65871750704994, 6800.54961811814, 240568787.394856, -448.83979904597516]
[2019-04-04 02:27:54,221] A3C_AGENT_WORKER-Thread-2 INFO:Local step 213500, global step 3400004: loss 0.0877
[2019-04-04 02:27:54,223] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 213500, global step 3400005: learning rate 0.0005
[2019-04-04 02:27:54,484] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.8757685e-30 2.8570747e-20 1.1871136e-21 1.0000000e+00 8.9433733e-25
 2.0222853e-21 5.4833128e-23], sum to 1.0000
[2019-04-04 02:27:54,485] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4030
[2019-04-04 02:27:54,514] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [10.66666666666667, 20.83333333333334, 115.6666666666667, 846.3333333333333, 26.0, 26.95950699557597, 0.8919529970666144, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5061000.0000, 
sim time next is 5061600.0000, 
raw observation next is [11.0, 20.0, 114.5, 839.5, 26.0, 27.52172374193226, 0.952843296610809, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.7673130193905818, 0.2, 0.38166666666666665, 0.9276243093922651, 0.6666666666666666, 0.793476978494355, 0.817614432203603, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4588351], dtype=float32), 0.021020316]. 
=============================================
[2019-04-04 02:27:55,318] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1578207e-19 1.1389677e-13 3.7843009e-13 1.0000000e+00 2.0439453e-15
 1.9696852e-11 1.4879864e-13], sum to 1.0000
[2019-04-04 02:27:55,321] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6514
[2019-04-04 02:27:55,414] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.55, 62.5, 0.0, 0.0, 26.0, 25.11080707006847, 0.2638761110182782, 1.0, 1.0, 40798.58241423962], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 153000.0000, 
sim time next is 153600.0000, 
raw observation next is [-7.633333333333333, 63.0, 0.0, 0.0, 26.0, 24.77607091660445, 0.2602896260628618, 1.0, 1.0, 201536.1173643175], 
processed observation next is [1.0, 0.782608695652174, 0.2511542012927055, 0.63, 0.0, 0.0, 0.6666666666666666, 0.5646725763837042, 0.5867632086876206, 1.0, 1.0, 0.9596957969729405], 
reward next is 0.0403, 
noisyNet noise sample is [array([1.241908], dtype=float32), -0.14433895]. 
=============================================
[2019-04-04 02:27:56,416] A3C_AGENT_WORKER-Thread-3 INFO:Local step 212500, global step 3400932: loss 0.0039
[2019-04-04 02:27:56,418] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 212500, global step 3400932: learning rate 0.0005
[2019-04-04 02:27:57,120] A3C_AGENT_WORKER-Thread-16 INFO:Local step 212500, global step 3401206: loss 0.5634
[2019-04-04 02:27:57,120] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 212500, global step 3401206: learning rate 0.0005
[2019-04-04 02:27:57,249] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:27:57,249] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:27:57,264] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run26
[2019-04-04 02:27:58,532] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:27:58,532] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:27:58,536] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run26
[2019-04-04 02:27:58,897] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.7074174e-22 5.7143929e-16 2.8790388e-16 1.0000000e+00 4.1307177e-19
 7.1425935e-15 2.9832156e-16], sum to 1.0000
[2019-04-04 02:27:58,897] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8589
[2019-04-04 02:27:58,953] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-10.0, 42.0, 0.0, 0.0, 26.0, 25.08207321341089, 0.2852855381957513, 0.0, 1.0, 76861.65417502825], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 417600.0000, 
sim time next is 418200.0000, 
raw observation next is [-10.1, 42.83333333333334, 0.0, 0.0, 26.0, 25.08256114244645, 0.2875222236541271, 0.0, 1.0, 68884.64781139257], 
processed observation next is [1.0, 0.8695652173913043, 0.18282548476454297, 0.42833333333333345, 0.0, 0.0, 0.6666666666666666, 0.5902134285372043, 0.5958407412180423, 0.0, 1.0, 0.32802213243520273], 
reward next is 0.6720, 
noisyNet noise sample is [array([-0.8181224], dtype=float32), -0.62337]. 
=============================================
[2019-04-04 02:28:03,308] A3C_AGENT_WORKER-Thread-17 INFO:Local step 213500, global step 3402954: loss 0.0625
[2019-04-04 02:28:03,346] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 213500, global step 3402963: learning rate 0.0005
[2019-04-04 02:28:05,234] A3C_AGENT_WORKER-Thread-19 INFO:Local step 213500, global step 3403372: loss 0.0394
[2019-04-04 02:28:05,234] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 213500, global step 3403372: learning rate 0.0005
[2019-04-04 02:28:09,571] A3C_AGENT_WORKER-Thread-12 INFO:Local step 212500, global step 3404487: loss 0.0019
[2019-04-04 02:28:09,572] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 212500, global step 3404487: learning rate 0.0005
[2019-04-04 02:28:10,316] A3C_AGENT_WORKER-Thread-5 INFO:Local step 212500, global step 3404665: loss 0.2131
[2019-04-04 02:28:10,319] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 212500, global step 3404665: learning rate 0.0005
[2019-04-04 02:28:10,737] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.4634153e-15 2.0938873e-10 1.4647050e-10 9.9996984e-01 2.7642741e-10
 3.0103296e-05 2.5621544e-10], sum to 1.0000
[2019-04-04 02:28:10,737] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8448
[2019-04-04 02:28:10,774] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.516666666666667, 88.33333333333334, 0.0, 0.0, 19.0, 19.06311616401108, -1.008884135590593, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 72600.0000, 
sim time next is 73200.0000, 
raw observation next is [2.333333333333333, 87.66666666666667, 0.0, 0.0, 19.0, 19.17600977017722, -1.005573889710252, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.5272391505078486, 0.8766666666666667, 0.0, 0.0, 0.08333333333333333, 0.09800081418143503, 0.16480870342991602, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.26850355], dtype=float32), 0.78548807]. 
=============================================
[2019-04-04 02:28:13,932] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2702142e-22 1.5714926e-16 2.5548157e-16 1.0000000e+00 1.1686619e-18
 3.1315428e-14 6.3209595e-17], sum to 1.0000
[2019-04-04 02:28:13,932] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4311
[2019-04-04 02:28:13,972] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.9, 54.5, 0.0, 0.0, 26.0, 25.37007766405684, 0.3345443274113306, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 757800.0000, 
sim time next is 758400.0000, 
raw observation next is [-3.899999999999999, 54.0, 0.0, 0.0, 26.0, 25.16564230569823, 0.3111987522115817, 1.0, 1.0, 18721.71335090541], 
processed observation next is [1.0, 0.782608695652174, 0.35457063711911363, 0.54, 0.0, 0.0, 0.6666666666666666, 0.5971368588081859, 0.6037329174038606, 1.0, 1.0, 0.08915101595669243], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.4008273], dtype=float32), -1.208499]. 
=============================================
[2019-04-04 02:28:14,994] A3C_AGENT_WORKER-Thread-20 INFO:Local step 213000, global step 3405813: loss 0.0012
[2019-04-04 02:28:14,995] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 213000, global step 3405813: learning rate 0.0005
[2019-04-04 02:28:17,699] A3C_AGENT_WORKER-Thread-10 INFO:Local step 213000, global step 3406477: loss 0.0047
[2019-04-04 02:28:17,699] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 213000, global step 3406477: learning rate 0.0005
[2019-04-04 02:28:18,048] A3C_AGENT_WORKER-Thread-6 INFO:Local step 213000, global step 3406585: loss 0.0101
[2019-04-04 02:28:18,049] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 213000, global step 3406585: learning rate 0.0005
[2019-04-04 02:28:19,134] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0959852e-19 6.1305410e-13 1.3083016e-12 1.0000000e+00 2.9186947e-15
 5.2679628e-08 1.3829618e-13], sum to 1.0000
[2019-04-04 02:28:19,134] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5122
[2019-04-04 02:28:19,191] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.5, 78.0, 0.0, 0.0, 25.0, 24.20733663099691, 0.1967033436913647, 1.0, 1.0, 43904.15894684679], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 4732200.0000, 
sim time next is 4732800.0000, 
raw observation next is [-0.6666666666666666, 78.0, 0.0, 0.0, 25.0, 24.16108509779231, 0.1895903084560304, 1.0, 1.0, 61931.48190444989], 
processed observation next is [1.0, 0.782608695652174, 0.44413665743305636, 0.78, 0.0, 0.0, 0.5833333333333334, 0.5134237581493591, 0.5631967694853435, 1.0, 1.0, 0.2949118185926185], 
reward next is 0.7051, 
noisyNet noise sample is [array([-0.15749604], dtype=float32), 0.2568424]. 
=============================================
[2019-04-04 02:28:19,805] A3C_AGENT_WORKER-Thread-15 INFO:Local step 213000, global step 3407059: loss 0.0193
[2019-04-04 02:28:19,806] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 213000, global step 3407059: learning rate 0.0005
[2019-04-04 02:28:21,765] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.9204873e-25 2.5603145e-19 1.9972646e-18 1.0000000e+00 1.0298725e-21
 1.9492797e-18 1.1251780e-18], sum to 1.0000
[2019-04-04 02:28:21,765] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2954
[2019-04-04 02:28:21,791] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 25.0, 24.20403253539391, 0.1744355114924766, 0.0, 1.0, 41739.70643800297], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 4756200.0000, 
sim time next is 4756800.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 25.0, 24.22566071293426, 0.1765515889061735, 0.0, 1.0, 41622.40641530391], 
processed observation next is [0.0, 0.043478260869565216, 0.3518005540166205, 0.71, 0.0, 0.0, 0.5833333333333334, 0.5188050594111884, 0.5588505296353912, 0.0, 1.0, 0.198201935310971], 
reward next is 0.8018, 
noisyNet noise sample is [array([0.29087397], dtype=float32), -0.40283155]. 
=============================================
[2019-04-04 02:28:22,395] A3C_AGENT_WORKER-Thread-2 INFO:Local step 214000, global step 3407625: loss 0.0251
[2019-04-04 02:28:22,396] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 214000, global step 3407625: learning rate 0.0005
[2019-04-04 02:28:22,625] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:28:22,625] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:28:22,628] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run26
[2019-04-04 02:28:25,418] A3C_AGENT_WORKER-Thread-11 INFO:Local step 213500, global step 3408256: loss 0.2905
[2019-04-04 02:28:25,419] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 213500, global step 3408256: learning rate 0.0005
[2019-04-04 02:28:26,361] A3C_AGENT_WORKER-Thread-18 INFO:Local step 213000, global step 3408495: loss 0.0247
[2019-04-04 02:28:26,409] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 213000, global step 3408495: learning rate 0.0005
[2019-04-04 02:28:28,817] A3C_AGENT_WORKER-Thread-3 INFO:Local step 213000, global step 3409202: loss 0.0261
[2019-04-04 02:28:28,820] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 213000, global step 3409202: learning rate 0.0005
[2019-04-04 02:28:30,070] A3C_AGENT_WORKER-Thread-16 INFO:Local step 213000, global step 3409515: loss 0.0009
[2019-04-04 02:28:30,072] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 213000, global step 3409515: learning rate 0.0005
[2019-04-04 02:28:33,581] A3C_AGENT_WORKER-Thread-14 INFO:Local step 212500, global step 3410448: loss 0.0926
[2019-04-04 02:28:33,585] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 212500, global step 3410448: learning rate 0.0005
[2019-04-04 02:28:34,363] A3C_AGENT_WORKER-Thread-17 INFO:Local step 214000, global step 3410701: loss 0.1368
[2019-04-04 02:28:34,364] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 214000, global step 3410701: learning rate 0.0005
[2019-04-04 02:28:35,853] A3C_AGENT_WORKER-Thread-19 INFO:Local step 214000, global step 3411182: loss 0.2029
[2019-04-04 02:28:35,856] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 214000, global step 3411183: learning rate 0.0005
[2019-04-04 02:28:38,402] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.5708953e-22 6.4315349e-16 5.6683228e-16 1.0000000e+00 1.1701919e-18
 4.8063534e-15 3.7503506e-16], sum to 1.0000
[2019-04-04 02:28:38,405] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1689
[2019-04-04 02:28:38,448] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-11.53333333333333, 54.33333333333334, 0.0, 0.0, 24.0, 22.36139485666813, -0.366179145429287, 0.0, 1.0, 46347.96152451537], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 433200.0000, 
sim time next is 433800.0000, 
raw observation next is [-11.45, 54.5, 0.0, 0.0, 24.0, 22.35278607977016, -0.3761213325147573, 0.0, 1.0, 46391.37385070253], 
processed observation next is [1.0, 0.0, 0.14542936288088645, 0.545, 0.0, 0.0, 0.5, 0.3627321733141799, 0.3746262224950809, 0.0, 1.0, 0.2209113040509644], 
reward next is 0.7791, 
noisyNet noise sample is [array([-1.3250614], dtype=float32), -0.29715782]. 
=============================================
[2019-04-04 02:28:40,366] A3C_AGENT_WORKER-Thread-12 INFO:Local step 213000, global step 3412484: loss 0.0004
[2019-04-04 02:28:40,368] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 213000, global step 3412484: learning rate 0.0005
[2019-04-04 02:28:42,677] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:28:42,677] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:28:42,680] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run26
[2019-04-04 02:28:42,911] A3C_AGENT_WORKER-Thread-5 INFO:Local step 213000, global step 3413102: loss 0.0008
[2019-04-04 02:28:42,913] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 213000, global step 3413102: learning rate 0.0005
[2019-04-04 02:28:43,664] A3C_AGENT_WORKER-Thread-2 INFO:Local step 214500, global step 3413299: loss 0.1347
[2019-04-04 02:28:43,664] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 214500, global step 3413299: learning rate 0.0005
[2019-04-04 02:28:46,448] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.5385685e-22 5.5111394e-16 2.8920621e-15 1.0000000e+00 1.1629301e-17
 9.7212288e-13 3.9715109e-15], sum to 1.0000
[2019-04-04 02:28:46,448] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7299
[2019-04-04 02:28:46,496] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 25.0, 21.82247093820311, -0.478486492212341, 0.0, 1.0, 45654.10905311209], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 194400.0000, 
sim time next is 195000.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 25.0, 21.79557243129442, -0.4881238471888635, 0.0, 1.0, 45647.1912566832], 
processed observation next is [1.0, 0.2608695652173913, 0.21606648199445982, 0.78, 0.0, 0.0, 0.5833333333333334, 0.3162977026078684, 0.33729205093704556, 0.0, 1.0, 0.21736757741277715], 
reward next is 0.7826, 
noisyNet noise sample is [array([-0.9704109], dtype=float32), -0.5411563]. 
=============================================
[2019-04-04 02:28:46,501] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[83.47832]
 [83.41727]
 [83.37428]
 [83.35103]
 [83.33641]], R is [[83.46749115]
 [83.41542053]
 [83.36392975]
 [83.31314087]
 [83.26306915]].
[2019-04-04 02:28:48,923] A3C_AGENT_WORKER-Thread-20 INFO:Local step 213500, global step 3414770: loss 0.1103
[2019-04-04 02:28:48,926] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 213500, global step 3414771: learning rate 0.0005
[2019-04-04 02:28:50,076] A3C_AGENT_WORKER-Thread-10 INFO:Local step 213500, global step 3415039: loss 0.1374
[2019-04-04 02:28:50,090] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 213500, global step 3415039: learning rate 0.0005
[2019-04-04 02:28:50,989] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:28:50,989] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:28:51,087] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run26
[2019-04-04 02:28:51,485] A3C_AGENT_WORKER-Thread-6 INFO:Local step 213500, global step 3415444: loss 0.0487
[2019-04-04 02:28:51,485] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 213500, global step 3415444: learning rate 0.0005
[2019-04-04 02:28:51,612] A3C_AGENT_WORKER-Thread-15 INFO:Local step 213500, global step 3415486: loss 0.0252
[2019-04-04 02:28:51,616] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 213500, global step 3415486: learning rate 0.0005
[2019-04-04 02:28:53,756] A3C_AGENT_WORKER-Thread-4 INFO:Local step 212500, global step 3416079: loss 0.1303
[2019-04-04 02:28:53,756] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 212500, global step 3416079: learning rate 0.0005
[2019-04-04 02:28:54,153] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1064856e-22 4.7417508e-18 8.4194910e-17 1.0000000e+00 5.2950287e-20
 2.8118632e-16 1.4914450e-16], sum to 1.0000
[2019-04-04 02:28:54,154] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0359
[2019-04-04 02:28:54,214] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.633333333333333, 87.0, 0.0, 0.0, 24.0, 23.06951183910926, -0.1635978280069225, 0.0, 1.0, 45995.23945472202], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 585600.0000, 
sim time next is 586200.0000, 
raw observation next is [-2.716666666666667, 87.0, 0.0, 0.0, 24.0, 23.05117416818293, -0.1639777696708545, 0.0, 1.0, 53574.89049489972], 
processed observation next is [0.0, 0.782608695652174, 0.3873499538319483, 0.87, 0.0, 0.0, 0.5, 0.4209311806819107, 0.4453407434430485, 0.0, 1.0, 0.25511852616618913], 
reward next is 0.7449, 
noisyNet noise sample is [array([1.0944527], dtype=float32), -0.65510094]. 
=============================================
[2019-04-04 02:28:54,478] A3C_AGENT_WORKER-Thread-17 INFO:Local step 214500, global step 3416277: loss 0.0187
[2019-04-04 02:28:54,483] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 214500, global step 3416277: learning rate 0.0005
[2019-04-04 02:28:55,012] A3C_AGENT_WORKER-Thread-11 INFO:Local step 214000, global step 3416441: loss 0.2057
[2019-04-04 02:28:55,013] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 214000, global step 3416441: learning rate 0.0005
[2019-04-04 02:28:56,493] A3C_AGENT_WORKER-Thread-19 INFO:Local step 214500, global step 3416856: loss 0.0063
[2019-04-04 02:28:56,494] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 214500, global step 3416856: learning rate 0.0005
[2019-04-04 02:28:58,676] A3C_AGENT_WORKER-Thread-18 INFO:Local step 213500, global step 3417409: loss 0.0026
[2019-04-04 02:28:58,682] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 213500, global step 3417409: learning rate 0.0005
[2019-04-04 02:29:01,688] A3C_AGENT_WORKER-Thread-13 INFO:Local step 212500, global step 3418256: loss 0.1621
[2019-04-04 02:29:01,689] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 212500, global step 3418256: learning rate 0.0005
[2019-04-04 02:29:01,839] A3C_AGENT_WORKER-Thread-16 INFO:Local step 213500, global step 3418289: loss 0.0023
[2019-04-04 02:29:01,840] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 213500, global step 3418289: learning rate 0.0005
[2019-04-04 02:29:02,148] A3C_AGENT_WORKER-Thread-3 INFO:Local step 213500, global step 3418383: loss 0.0033
[2019-04-04 02:29:02,148] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 213500, global step 3418383: learning rate 0.0005
[2019-04-04 02:29:06,473] A3C_AGENT_WORKER-Thread-14 INFO:Local step 213000, global step 3419500: loss 0.0034
[2019-04-04 02:29:06,521] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 213000, global step 3419500: learning rate 0.0005
[2019-04-04 02:29:07,124] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.5337552e-27 3.2121798e-16 1.7507925e-18 1.0000000e+00 2.3614903e-19
 5.8932271e-15 3.4511876e-18], sum to 1.0000
[2019-04-04 02:29:07,130] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6201
[2019-04-04 02:29:07,216] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [11.63333333333333, 52.66666666666667, 85.33333333333333, 103.0, 26.0, 27.17482817555931, 0.8144278969409191, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1527600.0000, 
sim time next is 1528200.0000, 
raw observation next is [11.35, 54.0, 87.0, 28.0, 26.0, 27.2506737819104, 0.7003981493708454, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.7770083102493075, 0.54, 0.29, 0.030939226519337018, 0.6666666666666666, 0.7708894818258667, 0.7334660497902817, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.22116134], dtype=float32), 0.1260067]. 
=============================================
[2019-04-04 02:29:08,600] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.8960035e-22 3.8616304e-15 7.0136906e-16 1.0000000e+00 6.8391214e-18
 8.5846139e-14 6.5030547e-16], sum to 1.0000
[2019-04-04 02:29:08,600] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1710
[2019-04-04 02:29:08,659] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.35, 55.0, 0.0, 0.0, 26.0, 25.62059203796855, 0.3069945211611427, 1.0, 1.0, 74722.28834048413], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 754200.0000, 
sim time next is 754800.0000, 
raw observation next is [-3.533333333333333, 55.33333333333333, 0.0, 0.0, 26.0, 24.92182910040079, 0.2785861108570353, 1.0, 1.0, 197843.2966875662], 
processed observation next is [1.0, 0.7391304347826086, 0.36472760849492153, 0.5533333333333332, 0.0, 0.0, 0.6666666666666666, 0.5768190917000657, 0.5928620369523451, 1.0, 1.0, 0.9421109366074581], 
reward next is 0.0579, 
noisyNet noise sample is [array([0.13406114], dtype=float32), 0.17833596]. 
=============================================
[2019-04-04 02:29:10,241] A3C_AGENT_WORKER-Thread-2 INFO:Local step 215000, global step 3420340: loss 0.2846
[2019-04-04 02:29:10,273] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 215000, global step 3420340: learning rate 0.0005
[2019-04-04 02:29:12,102] A3C_AGENT_WORKER-Thread-12 INFO:Local step 213500, global step 3420801: loss 0.0135
[2019-04-04 02:29:12,103] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 213500, global step 3420801: learning rate 0.0005
[2019-04-04 02:29:14,865] A3C_AGENT_WORKER-Thread-5 INFO:Local step 213500, global step 3421421: loss 0.0059
[2019-04-04 02:29:14,865] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 213500, global step 3421421: learning rate 0.0005
[2019-04-04 02:29:18,294] A3C_AGENT_WORKER-Thread-11 INFO:Local step 214500, global step 3422376: loss 0.0037
[2019-04-04 02:29:18,315] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 214500, global step 3422376: learning rate 0.0005
[2019-04-04 02:29:21,911] A3C_AGENT_WORKER-Thread-15 INFO:Local step 214000, global step 3423228: loss 1.1228
[2019-04-04 02:29:21,913] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 214000, global step 3423228: learning rate 0.0005
[2019-04-04 02:29:21,940] A3C_AGENT_WORKER-Thread-20 INFO:Local step 214000, global step 3423239: loss 0.4525
[2019-04-04 02:29:21,946] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 214000, global step 3423239: learning rate 0.0005
[2019-04-04 02:29:22,342] A3C_AGENT_WORKER-Thread-10 INFO:Local step 214000, global step 3423347: loss 0.1963
[2019-04-04 02:29:22,343] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 214000, global step 3423347: learning rate 0.0005
[2019-04-04 02:29:23,321] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.7741467e-24 2.6424208e-14 6.1680847e-17 1.0000000e+00 2.7247053e-17
 1.6873604e-10 3.8125191e-17], sum to 1.0000
[2019-04-04 02:29:23,321] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1554
[2019-04-04 02:29:23,334] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.15, 81.0, 0.0, 0.0, 25.0, 24.56331723820027, 0.208511567121386, 0.0, 1.0, 25904.51200408822], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 959400.0000, 
sim time next is 960000.0000, 
raw observation next is [7.333333333333333, 80.66666666666666, 0.0, 0.0, 25.0, 24.51879734813649, 0.2054998317394636, 0.0, 1.0, 51407.82764190659], 
processed observation next is [1.0, 0.08695652173913043, 0.6657433056325024, 0.8066666666666665, 0.0, 0.0, 0.5833333333333334, 0.5432331123447076, 0.5684999439131545, 0.0, 1.0, 0.24479917924717423], 
reward next is 0.7552, 
noisyNet noise sample is [array([0.9239696], dtype=float32), -0.6677001]. 
=============================================
[2019-04-04 02:29:23,337] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[92.00365 ]
 [91.68936 ]
 [91.406204]
 [91.12691 ]
 [90.91283 ]], R is [[92.08888245]
 [92.04463959]
 [92.03495789]
 [92.11460876]
 [92.19346619]].
[2019-04-04 02:29:23,337] A3C_AGENT_WORKER-Thread-6 INFO:Local step 214000, global step 3423615: loss 0.7861
[2019-04-04 02:29:23,338] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 214000, global step 3423615: learning rate 0.0005
[2019-04-04 02:29:23,611] A3C_AGENT_WORKER-Thread-17 INFO:Local step 215000, global step 3423677: loss 1.2322
[2019-04-04 02:29:23,657] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 215000, global step 3423684: learning rate 0.0005
[2019-04-04 02:29:25,004] A3C_AGENT_WORKER-Thread-19 INFO:Local step 215000, global step 3423990: loss 1.1895
[2019-04-04 02:29:25,005] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 215000, global step 3423990: learning rate 0.0005
[2019-04-04 02:29:28,902] A3C_AGENT_WORKER-Thread-4 INFO:Local step 213000, global step 3425088: loss 0.0002
[2019-04-04 02:29:28,904] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 213000, global step 3425088: learning rate 0.0005
[2019-04-04 02:29:30,382] A3C_AGENT_WORKER-Thread-18 INFO:Local step 214000, global step 3425552: loss 0.2059
[2019-04-04 02:29:30,402] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 214000, global step 3425552: learning rate 0.0005
[2019-04-04 02:29:33,183] A3C_AGENT_WORKER-Thread-3 INFO:Local step 214000, global step 3426346: loss 0.1252
[2019-04-04 02:29:33,185] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 214000, global step 3426346: learning rate 0.0005
[2019-04-04 02:29:33,239] A3C_AGENT_WORKER-Thread-16 INFO:Local step 214000, global step 3426367: loss 0.1484
[2019-04-04 02:29:33,243] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 214000, global step 3426367: learning rate 0.0005
[2019-04-04 02:29:37,696] A3C_AGENT_WORKER-Thread-13 INFO:Local step 213000, global step 3427654: loss 0.0028
[2019-04-04 02:29:37,697] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 213000, global step 3427654: learning rate 0.0005
[2019-04-04 02:29:40,639] A3C_AGENT_WORKER-Thread-2 INFO:Local step 215500, global step 3428468: loss 0.0431
[2019-04-04 02:29:40,640] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 215500, global step 3428468: learning rate 0.0005
[2019-04-04 02:29:41,686] A3C_AGENT_WORKER-Thread-14 INFO:Local step 213500, global step 3428677: loss 0.0131
[2019-04-04 02:29:41,689] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 213500, global step 3428677: learning rate 0.0005
[2019-04-04 02:29:48,840] A3C_AGENT_WORKER-Thread-12 INFO:Local step 214000, global step 3429757: loss 0.0120
[2019-04-04 02:29:48,855] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 214000, global step 3429757: learning rate 0.0005
[2019-04-04 02:29:50,422] A3C_AGENT_WORKER-Thread-15 INFO:Local step 214500, global step 3430098: loss 0.0921
[2019-04-04 02:29:50,424] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 214500, global step 3430098: learning rate 0.0005
[2019-04-04 02:29:50,554] A3C_AGENT_WORKER-Thread-5 INFO:Local step 214000, global step 3430115: loss 0.0175
[2019-04-04 02:29:50,554] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 214000, global step 3430115: learning rate 0.0005
[2019-04-04 02:29:51,898] A3C_AGENT_WORKER-Thread-10 INFO:Local step 214500, global step 3430375: loss 0.0172
[2019-04-04 02:29:51,928] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 214500, global step 3430375: learning rate 0.0005
[2019-04-04 02:29:53,031] A3C_AGENT_WORKER-Thread-20 INFO:Local step 214500, global step 3430580: loss 0.0531
[2019-04-04 02:29:53,070] A3C_AGENT_WORKER-Thread-11 INFO:Local step 215000, global step 3430588: loss 0.5154
[2019-04-04 02:29:53,084] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 215000, global step 3430588: learning rate 0.0005
[2019-04-04 02:29:53,143] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 214500, global step 3430580: learning rate 0.0005
[2019-04-04 02:29:53,203] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.6127683e-29 1.0382490e-17 3.4572208e-19 1.0000000e+00 6.9551607e-22
 1.3947525e-13 1.0220266e-19], sum to 1.0000
[2019-04-04 02:29:53,204] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3044
[2019-04-04 02:29:53,290] A3C_AGENT_WORKER-Thread-6 INFO:Local step 214500, global step 3430643: loss 0.0058
[2019-04-04 02:29:53,307] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [9.600000000000001, 89.66666666666667, 0.0, 0.0, 26.0, 24.95481392341969, 0.2814630624005847, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 978000.0000, 
sim time next is 978600.0000, 
raw observation next is [9.5, 91.33333333333333, 8.999999999999998, 0.0, 26.0, 24.95162894231441, 0.2823806005545924, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.7257617728531857, 0.9133333333333333, 0.029999999999999995, 0.0, 0.6666666666666666, 0.5793024118595342, 0.5941268668515308, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8628043], dtype=float32), 0.35897383]. 
=============================================
[2019-04-04 02:29:53,321] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 214500, global step 3430643: learning rate 0.0005
[2019-04-04 02:29:59,445] A3C_AGENT_WORKER-Thread-19 INFO:Local step 215500, global step 3431963: loss 0.0599
[2019-04-04 02:29:59,446] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 215500, global step 3431963: learning rate 0.0005
[2019-04-04 02:30:00,501] A3C_AGENT_WORKER-Thread-17 INFO:Local step 215500, global step 3432190: loss 0.0487
[2019-04-04 02:30:00,501] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 215500, global step 3432190: learning rate 0.0005
[2019-04-04 02:30:04,049] A3C_AGENT_WORKER-Thread-18 INFO:Local step 214500, global step 3432784: loss 0.0600
[2019-04-04 02:30:04,077] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 214500, global step 3432784: learning rate 0.0005
[2019-04-04 02:30:05,032] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.2551225e-21 6.7589672e-14 6.2582665e-17 1.0000000e+00 9.0570554e-16
 1.2917419e-10 7.6471735e-15], sum to 1.0000
[2019-04-04 02:30:05,032] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2111
[2019-04-04 02:30:05,100] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 93.0, 31.0, 0.0, 26.0, 25.70727360526157, 0.5120786454265976, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1353600.0000, 
sim time next is 1354200.0000, 
raw observation next is [1.0, 93.5, 26.66666666666666, 0.0, 26.0, 25.70926003480033, 0.506010361055706, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.4903047091412743, 0.935, 0.08888888888888886, 0.0, 0.6666666666666666, 0.6424383362333609, 0.668670120351902, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.21572591], dtype=float32), 1.540658]. 
=============================================
[2019-04-04 02:30:08,470] A3C_AGENT_WORKER-Thread-16 INFO:Local step 214500, global step 3433594: loss 0.2240
[2019-04-04 02:30:08,525] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 214500, global step 3433594: learning rate 0.0005
[2019-04-04 02:30:08,985] A3C_AGENT_WORKER-Thread-3 INFO:Local step 214500, global step 3433685: loss 0.0468
[2019-04-04 02:30:08,986] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 214500, global step 3433685: learning rate 0.0005
[2019-04-04 02:30:09,791] A3C_AGENT_WORKER-Thread-4 INFO:Local step 213500, global step 3433849: loss 0.2708
[2019-04-04 02:30:09,795] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 213500, global step 3433849: learning rate 0.0005
[2019-04-04 02:30:24,118] A3C_AGENT_WORKER-Thread-13 INFO:Local step 213500, global step 3436621: loss 0.1314
[2019-04-04 02:30:24,119] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 213500, global step 3436621: learning rate 0.0005
[2019-04-04 02:30:26,001] A3C_AGENT_WORKER-Thread-14 INFO:Local step 214000, global step 3436960: loss 0.3452
[2019-04-04 02:30:26,002] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 214000, global step 3436960: learning rate 0.0005
[2019-04-04 02:30:26,182] A3C_AGENT_WORKER-Thread-12 INFO:Local step 214500, global step 3436997: loss 0.0585
[2019-04-04 02:30:26,183] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 214500, global step 3436997: learning rate 0.0005
[2019-04-04 02:30:28,381] A3C_AGENT_WORKER-Thread-2 INFO:Local step 216000, global step 3437430: loss 0.1245
[2019-04-04 02:30:28,390] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 216000, global step 3437430: learning rate 0.0005
[2019-04-04 02:30:28,708] A3C_AGENT_WORKER-Thread-5 INFO:Local step 214500, global step 3437489: loss 0.0498
[2019-04-04 02:30:28,724] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 214500, global step 3437489: learning rate 0.0005
[2019-04-04 02:30:29,225] A3C_AGENT_WORKER-Thread-15 INFO:Local step 215000, global step 3437583: loss 0.5377
[2019-04-04 02:30:29,227] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 215000, global step 3437583: learning rate 0.0005
[2019-04-04 02:30:32,011] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.0587087e-25 9.6263439e-15 1.0572086e-18 1.0000000e+00 5.8853326e-18
 7.9543587e-13 1.5485081e-19], sum to 1.0000
[2019-04-04 02:30:32,012] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5912
[2019-04-04 02:30:32,041] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [15.31666666666667, 76.0, 46.33333333333334, 0.0, 25.0, 25.71888703185296, 0.494399102641398, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 1007400.0000, 
sim time next is 1008000.0000, 
raw observation next is [15.5, 75.0, 41.0, 0.0, 25.0, 25.82664112711702, 0.5019948786042062, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.8919667590027703, 0.75, 0.13666666666666666, 0.0, 0.5833333333333334, 0.6522200939264184, 0.6673316262014021, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8270094], dtype=float32), -1.9045111]. 
=============================================
[2019-04-04 02:30:32,130] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[103.37001 ]
 [103.255806]
 [103.34436 ]
 [103.75679 ]
 [103.31681 ]], R is [[103.37227631]
 [103.33855438]
 [103.30516815]
 [103.27211761]
 [103.23939514]].
[2019-04-04 02:30:32,350] A3C_AGENT_WORKER-Thread-20 INFO:Local step 215000, global step 3438187: loss 0.6879
[2019-04-04 02:30:32,380] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 215000, global step 3438187: learning rate 0.0005
[2019-04-04 02:30:33,130] A3C_AGENT_WORKER-Thread-10 INFO:Local step 215000, global step 3438377: loss 0.7371
[2019-04-04 02:30:33,149] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 215000, global step 3438377: learning rate 0.0005
[2019-04-04 02:30:33,356] A3C_AGENT_WORKER-Thread-6 INFO:Local step 215000, global step 3438416: loss 1.0526
[2019-04-04 02:30:33,357] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 215000, global step 3438416: learning rate 0.0005
[2019-04-04 02:30:36,119] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.8264423e-27 6.7479303e-17 1.8515276e-18 1.0000000e+00 1.7770063e-19
 7.7206867e-15 2.2664620e-19], sum to 1.0000
[2019-04-04 02:30:36,119] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2237
[2019-04-04 02:30:36,188] A3C_AGENT_WORKER-Thread-11 INFO:Local step 215500, global step 3438976: loss 0.0995
[2019-04-04 02:30:36,188] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 215500, global step 3438976: learning rate 0.0005
[2019-04-04 02:30:36,221] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.266666666666667, 81.00000000000001, 25.0, 25.0, 26.0, 25.32705335380628, 0.4790638217660375, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1584600.0000, 
sim time next is 1585200.0000, 
raw observation next is [5.533333333333333, 80.0, 31.0, 30.0, 26.0, 25.46054619700581, 0.5352517905240833, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.6158818097876271, 0.8, 0.10333333333333333, 0.03314917127071823, 0.6666666666666666, 0.6217121830838176, 0.6784172635080278, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2951435], dtype=float32), 1.0170294]. 
=============================================
[2019-04-04 02:30:40,362] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.2011049e-19 1.7907475e-13 1.9493403e-13 1.0000000e+00 9.4546358e-16
 6.5910411e-12 4.5291783e-14], sum to 1.0000
[2019-04-04 02:30:40,370] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0481
[2019-04-04 02:30:40,410] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.533333333333334, 64.0, 174.6666666666667, 128.5, 26.0, 25.87221288486973, 0.4060289803522468, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2118000.0000, 
sim time next is 2118600.0000, 
raw observation next is [-6.45, 64.0, 151.0, 134.0, 26.0, 25.74976734085036, 0.3664461782892346, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.28393351800554023, 0.64, 0.5033333333333333, 0.14806629834254142, 0.6666666666666666, 0.6458139450708634, 0.6221487260964115, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5346204], dtype=float32), -1.5928433]. 
=============================================
[2019-04-04 02:30:40,421] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.07811296e-19 1.15200709e-12 2.07117591e-12 9.99999881e-01
 2.43601729e-17 1.43933022e-07 3.88254180e-13], sum to 1.0000
[2019-04-04 02:30:40,449] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3966
[2019-04-04 02:30:40,511] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.0, 68.0, 0.0, 0.0, 26.0, 25.24385032347193, 0.4157777561062066, 0.0, 1.0, 47027.14335235977], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2235600.0000, 
sim time next is 2236200.0000, 
raw observation next is [-5.100000000000001, 68.5, 0.0, 0.0, 26.0, 25.2833253767671, 0.413866204097092, 0.0, 1.0, 45955.49361235161], 
processed observation next is [1.0, 0.9130434782608695, 0.32132963988919666, 0.685, 0.0, 0.0, 0.6666666666666666, 0.6069437813972582, 0.6379554013656973, 0.0, 1.0, 0.21883568386834099], 
reward next is 0.7812, 
noisyNet noise sample is [array([0.00037744], dtype=float32), -1.0177404]. 
=============================================
[2019-04-04 02:30:44,112] A3C_AGENT_WORKER-Thread-18 INFO:Local step 215000, global step 3440492: loss 0.0027
[2019-04-04 02:30:44,136] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 215000, global step 3440492: learning rate 0.0005
[2019-04-04 02:30:44,424] A3C_AGENT_WORKER-Thread-19 INFO:Local step 216000, global step 3440597: loss 0.3335
[2019-04-04 02:30:44,424] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 216000, global step 3440597: learning rate 0.0005
[2019-04-04 02:30:46,947] A3C_AGENT_WORKER-Thread-17 INFO:Local step 216000, global step 3441144: loss 0.5807
[2019-04-04 02:30:46,949] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 216000, global step 3441144: learning rate 0.0005
[2019-04-04 02:30:48,147] A3C_AGENT_WORKER-Thread-3 INFO:Local step 215000, global step 3441380: loss 0.0079
[2019-04-04 02:30:48,158] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 215000, global step 3441380: learning rate 0.0005
[2019-04-04 02:30:48,794] A3C_AGENT_WORKER-Thread-16 INFO:Local step 215000, global step 3441521: loss 0.0432
[2019-04-04 02:30:48,797] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 215000, global step 3441521: learning rate 0.0005
[2019-04-04 02:30:51,770] A3C_AGENT_WORKER-Thread-4 INFO:Local step 214000, global step 3442065: loss 0.6833
[2019-04-04 02:30:51,782] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 214000, global step 3442065: learning rate 0.0005
[2019-04-04 02:30:53,796] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.8512821e-26 4.3858688e-19 2.2561204e-18 1.0000000e+00 4.8825627e-23
 3.4228806e-18 4.4876332e-19], sum to 1.0000
[2019-04-04 02:30:53,797] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4815
[2019-04-04 02:30:53,830] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 26.0, 25.37090208895953, 0.4633445922058204, 0.0, 1.0, 43854.12245912698], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1731600.0000, 
sim time next is 1732200.0000, 
raw observation next is [0.45, 91.83333333333334, 0.0, 0.0, 26.0, 25.35762904072758, 0.4593957664405368, 0.0, 1.0, 43509.44269027506], 
processed observation next is [0.0, 0.043478260869565216, 0.47506925207756234, 0.9183333333333334, 0.0, 0.0, 0.6666666666666666, 0.6131357533939651, 0.6531319221468456, 0.0, 1.0, 0.2071878223346431], 
reward next is 0.7928, 
noisyNet noise sample is [array([1.375716], dtype=float32), -0.4129331]. 
=============================================
[2019-04-04 02:31:00,855] A3C_AGENT_WORKER-Thread-14 INFO:Local step 214500, global step 3444047: loss 0.0213
[2019-04-04 02:31:00,857] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 214500, global step 3444047: learning rate 0.0005
[2019-04-04 02:31:05,036] A3C_AGENT_WORKER-Thread-12 INFO:Local step 215000, global step 3444862: loss 0.0380
[2019-04-04 02:31:05,036] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 215000, global step 3444862: learning rate 0.0005
[2019-04-04 02:31:06,178] A3C_AGENT_WORKER-Thread-5 INFO:Local step 215000, global step 3445112: loss 0.0023
[2019-04-04 02:31:06,181] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 215000, global step 3445112: learning rate 0.0005
[2019-04-04 02:31:06,662] A3C_AGENT_WORKER-Thread-13 INFO:Local step 214000, global step 3445228: loss 0.0344
[2019-04-04 02:31:06,667] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 214000, global step 3445228: learning rate 0.0005
[2019-04-04 02:31:08,341] A3C_AGENT_WORKER-Thread-15 INFO:Local step 215500, global step 3445603: loss 0.0815
[2019-04-04 02:31:08,341] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 215500, global step 3445603: learning rate 0.0005
[2019-04-04 02:31:08,916] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.0311334e-25 1.1077675e-18 1.3814246e-17 1.0000000e+00 1.8885616e-21
 1.7006202e-14 4.7701980e-18], sum to 1.0000
[2019-04-04 02:31:08,916] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9734
[2019-04-04 02:31:09,068] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.6, 91.0, 0.0, 0.0, 26.0, 24.40952190864765, 0.1644477243344842, 0.0, 1.0, 43577.15579868975], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2088000.0000, 
sim time next is 2088600.0000, 
raw observation next is [-5.7, 90.33333333333334, 0.0, 0.0, 26.0, 24.49285936546093, 0.145290712672328, 0.0, 1.0, 43692.19492733606], 
processed observation next is [1.0, 0.17391304347826086, 0.30470914127423826, 0.9033333333333334, 0.0, 0.0, 0.6666666666666666, 0.5410716137884108, 0.5484302375574427, 0.0, 1.0, 0.20805807108255267], 
reward next is 0.7919, 
noisyNet noise sample is [array([0.7053872], dtype=float32), -0.020625995]. 
=============================================
[2019-04-04 02:31:10,379] A3C_AGENT_WORKER-Thread-20 INFO:Local step 215500, global step 3445998: loss 0.0607
[2019-04-04 02:31:10,379] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 215500, global step 3445998: learning rate 0.0005
[2019-04-04 02:31:10,899] A3C_AGENT_WORKER-Thread-2 INFO:Local step 216500, global step 3446092: loss 0.9529
[2019-04-04 02:31:10,930] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 216500, global step 3446092: learning rate 0.0005
[2019-04-04 02:31:12,911] A3C_AGENT_WORKER-Thread-10 INFO:Local step 215500, global step 3446475: loss 0.0862
[2019-04-04 02:31:12,912] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 215500, global step 3446475: learning rate 0.0005
[2019-04-04 02:31:13,249] A3C_AGENT_WORKER-Thread-6 INFO:Local step 215500, global step 3446535: loss 0.0530
[2019-04-04 02:31:13,251] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 215500, global step 3446535: learning rate 0.0005
[2019-04-04 02:31:19,208] A3C_AGENT_WORKER-Thread-11 INFO:Local step 216000, global step 3447954: loss 0.3966
[2019-04-04 02:31:19,208] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 216000, global step 3447954: learning rate 0.0005
[2019-04-04 02:31:21,304] A3C_AGENT_WORKER-Thread-18 INFO:Local step 215500, global step 3448476: loss 0.1195
[2019-04-04 02:31:21,305] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 215500, global step 3448476: learning rate 0.0005
[2019-04-04 02:31:22,659] A3C_AGENT_WORKER-Thread-4 INFO:Local step 214500, global step 3448778: loss 0.0282
[2019-04-04 02:31:22,667] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 214500, global step 3448778: learning rate 0.0005
[2019-04-04 02:31:23,923] A3C_AGENT_WORKER-Thread-19 INFO:Local step 216500, global step 3449058: loss 0.6375
[2019-04-04 02:31:23,923] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 216500, global step 3449058: learning rate 0.0005
[2019-04-04 02:31:25,371] A3C_AGENT_WORKER-Thread-3 INFO:Local step 215500, global step 3449340: loss 0.0352
[2019-04-04 02:31:25,372] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 215500, global step 3449340: learning rate 0.0005
[2019-04-04 02:31:25,657] A3C_AGENT_WORKER-Thread-16 INFO:Local step 215500, global step 3449396: loss 0.0327
[2019-04-04 02:31:25,657] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 215500, global step 3449396: learning rate 0.0005
[2019-04-04 02:31:26,698] A3C_AGENT_WORKER-Thread-17 INFO:Local step 216500, global step 3449654: loss 1.4855
[2019-04-04 02:31:26,703] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 216500, global step 3449654: learning rate 0.0005
[2019-04-04 02:31:29,907] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.8102005e-24 9.6415855e-17 3.0219409e-16 1.0000000e+00 3.9311503e-20
 1.7736221e-13 2.6971265e-16], sum to 1.0000
[2019-04-04 02:31:29,920] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0178
[2019-04-04 02:31:29,956] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.25486566269682, 0.08730620466493821, 0.0, 1.0, 41265.19636902229], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2001600.0000, 
sim time next is 2002200.0000, 
raw observation next is [-5.7, 83.66666666666667, 0.0, 0.0, 26.0, 24.17813606013566, 0.07367615155031441, 0.0, 1.0, 41285.25432128397], 
processed observation next is [1.0, 0.17391304347826086, 0.30470914127423826, 0.8366666666666667, 0.0, 0.0, 0.6666666666666666, 0.5148446716779717, 0.5245587171834382, 0.0, 1.0, 0.19659644914897126], 
reward next is 0.8034, 
noisyNet noise sample is [array([0.4371982], dtype=float32), -0.24349767]. 
=============================================
[2019-04-04 02:31:36,034] A3C_AGENT_WORKER-Thread-14 INFO:Local step 215000, global step 3451617: loss 0.2109
[2019-04-04 02:31:36,035] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 215000, global step 3451617: learning rate 0.0005
[2019-04-04 02:31:36,645] A3C_AGENT_WORKER-Thread-13 INFO:Local step 214500, global step 3451776: loss 0.0124
[2019-04-04 02:31:36,675] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 214500, global step 3451776: learning rate 0.0005
[2019-04-04 02:31:39,966] A3C_AGENT_WORKER-Thread-12 INFO:Local step 215500, global step 3452611: loss 0.0498
[2019-04-04 02:31:39,967] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 215500, global step 3452611: learning rate 0.0005
[2019-04-04 02:31:40,738] A3C_AGENT_WORKER-Thread-5 INFO:Local step 215500, global step 3452777: loss 0.0478
[2019-04-04 02:31:40,739] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 215500, global step 3452777: learning rate 0.0005
[2019-04-04 02:31:46,093] A3C_AGENT_WORKER-Thread-2 INFO:Local step 217000, global step 3454037: loss 0.1406
[2019-04-04 02:31:46,093] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 217000, global step 3454037: learning rate 0.0005
[2019-04-04 02:31:46,807] A3C_AGENT_WORKER-Thread-15 INFO:Local step 216000, global step 3454221: loss 0.1102
[2019-04-04 02:31:46,807] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 216000, global step 3454221: learning rate 0.0005
[2019-04-04 02:31:48,152] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.2174382e-25 2.9636465e-18 7.6403389e-18 1.0000000e+00 2.1062480e-21
 7.8510400e-15 1.0305343e-18], sum to 1.0000
[2019-04-04 02:31:48,177] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6171
[2019-04-04 02:31:48,208] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.2, 75.0, 0.0, 0.0, 26.0, 24.00033700997597, 0.05419444269276367, 0.0, 1.0, 41986.2517425405], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2178000.0000, 
sim time next is 2178600.0000, 
raw observation next is [-6.2, 75.66666666666667, 0.0, 0.0, 26.0, 23.97390675494345, 0.04263774936649808, 0.0, 1.0, 41961.0574000061], 
processed observation next is [1.0, 0.21739130434782608, 0.2908587257617729, 0.7566666666666667, 0.0, 0.0, 0.6666666666666666, 0.4978255629119541, 0.514212583122166, 0.0, 1.0, 0.1998145590476481], 
reward next is 0.8002, 
noisyNet noise sample is [array([-1.2832952], dtype=float32), -0.12894024]. 
=============================================
[2019-04-04 02:31:49,025] A3C_AGENT_WORKER-Thread-20 INFO:Local step 216000, global step 3454760: loss 0.0738
[2019-04-04 02:31:49,025] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 216000, global step 3454760: learning rate 0.0005
[2019-04-04 02:31:49,351] A3C_AGENT_WORKER-Thread-10 INFO:Local step 216000, global step 3454838: loss 0.0795
[2019-04-04 02:31:49,354] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 216000, global step 3454838: learning rate 0.0005
[2019-04-04 02:31:50,958] A3C_AGENT_WORKER-Thread-6 INFO:Local step 216000, global step 3455263: loss 0.1275
[2019-04-04 02:31:50,959] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 216000, global step 3455263: learning rate 0.0005
[2019-04-04 02:31:53,369] A3C_AGENT_WORKER-Thread-4 INFO:Local step 215000, global step 3455900: loss 0.2397
[2019-04-04 02:31:53,369] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 215000, global step 3455900: learning rate 0.0005
[2019-04-04 02:31:54,427] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.0939637e-20 1.4225173e-12 3.3817984e-13 1.0000000e+00 9.3538212e-18
 1.7188440e-09 2.7990571e-13], sum to 1.0000
[2019-04-04 02:31:54,427] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9752
[2019-04-04 02:31:54,468] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.7, 83.66666666666667, 0.0, 0.0, 26.0, 24.43814142252876, 0.1797417364490311, 0.0, 1.0, 42537.70223498326], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1987800.0000, 
sim time next is 1988400.0000, 
raw observation next is [-5.8, 84.33333333333334, 0.0, 0.0, 26.0, 24.40868139363476, 0.1722819421370506, 0.0, 1.0, 42428.98899152636], 
processed observation next is [1.0, 0.0, 0.30193905817174516, 0.8433333333333334, 0.0, 0.0, 0.6666666666666666, 0.5340567828028968, 0.5574273140456835, 0.0, 1.0, 0.20204280472155411], 
reward next is 0.7980, 
noisyNet noise sample is [array([-0.50772125], dtype=float32), 2.690265]. 
=============================================
[2019-04-04 02:31:54,790] A3C_AGENT_WORKER-Thread-11 INFO:Local step 216500, global step 3456219: loss 1.1727
[2019-04-04 02:31:54,792] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 216500, global step 3456220: learning rate 0.0005
[2019-04-04 02:31:57,668] A3C_AGENT_WORKER-Thread-18 INFO:Local step 216000, global step 3456963: loss 0.4891
[2019-04-04 02:31:57,696] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 216000, global step 3456963: learning rate 0.0005
[2019-04-04 02:31:58,024] A3C_AGENT_WORKER-Thread-19 INFO:Local step 217000, global step 3457064: loss 0.2467
[2019-04-04 02:31:58,025] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 217000, global step 3457064: learning rate 0.0005
[2019-04-04 02:31:58,562] A3C_AGENT_WORKER-Thread-17 INFO:Local step 217000, global step 3457243: loss 0.0385
[2019-04-04 02:31:58,568] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 217000, global step 3457244: learning rate 0.0005
[2019-04-04 02:32:01,063] A3C_AGENT_WORKER-Thread-16 INFO:Local step 216000, global step 3457994: loss 0.0575
[2019-04-04 02:32:01,071] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 216000, global step 3457994: learning rate 0.0005
[2019-04-04 02:32:01,286] A3C_AGENT_WORKER-Thread-3 INFO:Local step 216000, global step 3458058: loss 0.1544
[2019-04-04 02:32:01,287] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 216000, global step 3458059: learning rate 0.0005
[2019-04-04 02:32:03,901] A3C_AGENT_WORKER-Thread-13 INFO:Local step 215000, global step 3458800: loss 0.7899
[2019-04-04 02:32:03,901] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 215000, global step 3458800: learning rate 0.0005
[2019-04-04 02:32:04,793] A3C_AGENT_WORKER-Thread-14 INFO:Local step 215500, global step 3459054: loss 0.1068
[2019-04-04 02:32:04,794] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 215500, global step 3459054: learning rate 0.0005
[2019-04-04 02:32:10,104] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.2525068e-27 3.4359426e-22 2.9299416e-21 1.0000000e+00 5.4756962e-26
 1.9796324e-22 5.4758981e-21], sum to 1.0000
[2019-04-04 02:32:10,104] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1054
[2019-04-04 02:32:10,170] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.4, 45.66666666666667, 66.83333333333334, 51.0, 26.0, 24.9477928595448, 0.2844010698006282, 0.0, 1.0, 45854.43764566031], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2392800.0000, 
sim time next is 2393400.0000, 
raw observation next is [-0.5, 45.33333333333333, 54.66666666666667, 44.0, 26.0, 24.95038218512543, 0.2838834520208295, 0.0, 1.0, 39689.39996656022], 
processed observation next is [0.0, 0.6956521739130435, 0.44875346260387816, 0.4533333333333333, 0.18222222222222223, 0.04861878453038674, 0.6666666666666666, 0.5791985154271192, 0.5946278173402765, 0.0, 1.0, 0.1889971426979058], 
reward next is 0.8110, 
noisyNet noise sample is [array([-0.44066247], dtype=float32), -0.06988449]. 
=============================================
[2019-04-04 02:32:12,208] A3C_AGENT_WORKER-Thread-12 INFO:Local step 216000, global step 3461311: loss 0.2350
[2019-04-04 02:32:12,214] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 216000, global step 3461311: learning rate 0.0005
[2019-04-04 02:32:12,546] A3C_AGENT_WORKER-Thread-5 INFO:Local step 216000, global step 3461405: loss 0.2750
[2019-04-04 02:32:12,547] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 216000, global step 3461405: learning rate 0.0005
[2019-04-04 02:32:13,549] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.7159362e-25 5.5712253e-19 7.2818455e-18 1.0000000e+00 6.7506202e-21
 1.0243388e-16 2.2678718e-19], sum to 1.0000
[2019-04-04 02:32:13,549] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3815
[2019-04-04 02:32:13,626] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.333333333333333, 49.00000000000001, 227.8333333333333, 69.83333333333333, 26.0, 25.27904956977459, 0.2397273468347839, 1.0, 1.0, 7477.255146904752], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2294400.0000, 
sim time next is 2295000.0000, 
raw observation next is [-1.15, 48.0, 221.0, 69.0, 26.0, 25.02549320998833, 0.3268141053832397, 1.0, 1.0, 120848.194350736], 
processed observation next is [1.0, 0.5652173913043478, 0.4307479224376732, 0.48, 0.7366666666666667, 0.07624309392265194, 0.6666666666666666, 0.5854577674990274, 0.6089380351277466, 1.0, 1.0, 0.5754675921463619], 
reward next is 0.4245, 
noisyNet noise sample is [array([-1.3899305], dtype=float32), 0.57471114]. 
=============================================
[2019-04-04 02:32:13,632] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[85.82199 ]
 [85.907234]
 [85.96039 ]
 [85.98802 ]
 [86.196815]], R is [[85.48152924]
 [85.59111023]
 [85.73519897]
 [85.78881073]
 [85.84188843]].
[2019-04-04 02:32:13,796] A3C_AGENT_WORKER-Thread-2 INFO:Local step 217500, global step 3461816: loss 0.5653
[2019-04-04 02:32:13,797] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 217500, global step 3461816: learning rate 0.0005
[2019-04-04 02:32:15,064] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.9975572e-22 6.1997577e-12 4.3589614e-13 1.0000000e+00 1.0409099e-17
 1.4227412e-10 2.4822583e-14], sum to 1.0000
[2019-04-04 02:32:15,065] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9258
[2019-04-04 02:32:15,086] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.3333333333333334, 93.0, 0.0, 0.0, 26.0, 25.42304503792591, 0.489682448126391, 0.0, 1.0, 72185.22582043632], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1722000.0000, 
sim time next is 1722600.0000, 
raw observation next is [0.25, 93.5, 0.0, 0.0, 26.0, 25.34920092170216, 0.4841628098600617, 0.0, 1.0, 83390.36245805369], 
processed observation next is [1.0, 0.9565217391304348, 0.46952908587257625, 0.935, 0.0, 0.0, 0.6666666666666666, 0.6124334101418466, 0.6613876032866872, 0.0, 1.0, 0.39709696408596995], 
reward next is 0.6029, 
noisyNet noise sample is [array([-0.6222369], dtype=float32), 1.6676804]. 
=============================================
[2019-04-04 02:32:16,027] A3C_AGENT_WORKER-Thread-15 INFO:Local step 216500, global step 3462497: loss 1.6402
[2019-04-04 02:32:16,028] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 216500, global step 3462497: learning rate 0.0005
[2019-04-04 02:32:16,760] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.8941725e-26 1.3684988e-20 9.0766880e-19 1.0000000e+00 3.6262263e-23
 6.9334027e-17 1.5896999e-18], sum to 1.0000
[2019-04-04 02:32:16,763] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1315
[2019-04-04 02:32:16,778] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.8, 84.33333333333334, 0.0, 0.0, 26.0, 24.80822350362242, 0.3536227557271498, 0.0, 1.0, 43676.92025387779], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1747200.0000, 
sim time next is 1747800.0000, 
raw observation next is [-0.8999999999999999, 85.0, 0.0, 0.0, 26.0, 24.84913247818278, 0.3506256973543227, 0.0, 1.0, 43706.05917814816], 
processed observation next is [0.0, 0.21739130434782608, 0.43767313019390586, 0.85, 0.0, 0.0, 0.6666666666666666, 0.5707610398485651, 0.6168752324514409, 0.0, 1.0, 0.20812409132451506], 
reward next is 0.7919, 
noisyNet noise sample is [array([0.49903876], dtype=float32), 0.05398225]. 
=============================================
[2019-04-04 02:32:18,138] A3C_AGENT_WORKER-Thread-20 INFO:Local step 216500, global step 3463059: loss 1.8105
[2019-04-04 02:32:18,138] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 216500, global step 3463059: learning rate 0.0005
[2019-04-04 02:32:18,215] A3C_AGENT_WORKER-Thread-4 INFO:Local step 215500, global step 3463080: loss 0.0710
[2019-04-04 02:32:18,216] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 215500, global step 3463080: learning rate 0.0005
[2019-04-04 02:32:18,910] A3C_AGENT_WORKER-Thread-10 INFO:Local step 216500, global step 3463271: loss 1.5979
[2019-04-04 02:32:18,914] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 216500, global step 3463271: learning rate 0.0005
[2019-04-04 02:32:19,457] A3C_AGENT_WORKER-Thread-6 INFO:Local step 216500, global step 3463404: loss 1.6611
[2019-04-04 02:32:19,458] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 216500, global step 3463404: learning rate 0.0005
[2019-04-04 02:32:20,551] A3C_AGENT_WORKER-Thread-11 INFO:Local step 217000, global step 3463680: loss 0.3280
[2019-04-04 02:32:20,552] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 217000, global step 3463680: learning rate 0.0005
[2019-04-04 02:32:22,420] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.6104203e-28 3.3490431e-24 2.4351022e-21 1.0000000e+00 2.0484454e-26
 1.7353710e-20 7.1859392e-22], sum to 1.0000
[2019-04-04 02:32:22,420] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2798
[2019-04-04 02:32:22,436] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 23.99033921800791, 0.04835845274286262, 0.0, 1.0, 40194.09861806368], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3040200.0000, 
sim time next is 3040800.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 23.96261471191745, 0.04197952979841935, 0.0, 1.0, 40209.96407238945], 
processed observation next is [0.0, 0.17391304347826086, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.49688455932645414, 0.5139931765994731, 0.0, 1.0, 0.1914760193923307], 
reward next is 0.8085, 
noisyNet noise sample is [array([-0.16591573], dtype=float32), 1.3943192]. 
=============================================
[2019-04-04 02:32:24,307] A3C_AGENT_WORKER-Thread-19 INFO:Local step 217500, global step 3464836: loss 1.3580
[2019-04-04 02:32:24,308] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 217500, global step 3464836: learning rate 0.0005
[2019-04-04 02:32:24,332] A3C_AGENT_WORKER-Thread-17 INFO:Local step 217500, global step 3464844: loss 1.6639
[2019-04-04 02:32:24,361] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 217500, global step 3464844: learning rate 0.0005
[2019-04-04 02:32:26,492] A3C_AGENT_WORKER-Thread-18 INFO:Local step 216500, global step 3465445: loss 0.5658
[2019-04-04 02:32:26,496] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 216500, global step 3465446: learning rate 0.0005
[2019-04-04 02:32:29,153] A3C_AGENT_WORKER-Thread-13 INFO:Local step 215500, global step 3466120: loss 0.0538
[2019-04-04 02:32:29,154] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 215500, global step 3466120: learning rate 0.0005
[2019-04-04 02:32:29,828] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.6092505e-29 2.8204136e-24 5.9272272e-22 1.0000000e+00 6.5275736e-27
 2.2678064e-23 2.7151941e-22], sum to 1.0000
[2019-04-04 02:32:29,829] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8252
[2019-04-04 02:32:29,890] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.7833333333333333, 30.33333333333334, 0.0, 0.0, 26.0, 25.29002500095567, 0.286810930873466, 0.0, 1.0, 40744.65320715756], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2491800.0000, 
sim time next is 2492400.0000, 
raw observation next is [-0.8666666666666667, 31.66666666666667, 0.0, 0.0, 26.0, 25.30749762717304, 0.2865287411457433, 0.0, 1.0, 40354.53007513627], 
processed observation next is [0.0, 0.8695652173913043, 0.4385964912280702, 0.3166666666666667, 0.0, 0.0, 0.6666666666666666, 0.6089581355977532, 0.5955095803819145, 0.0, 1.0, 0.1921644289292203], 
reward next is 0.8078, 
noisyNet noise sample is [array([-0.4123373], dtype=float32), 0.1463777]. 
=============================================
[2019-04-04 02:32:30,021] A3C_AGENT_WORKER-Thread-3 INFO:Local step 216500, global step 3466335: loss 0.4945
[2019-04-04 02:32:30,033] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 216500, global step 3466338: learning rate 0.0005
[2019-04-04 02:32:30,326] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.1260928e-24 8.7475837e-15 3.1304646e-16 1.0000000e+00 1.1251437e-18
 5.7681139e-13 4.2865469e-17], sum to 1.0000
[2019-04-04 02:32:30,326] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0512
[2019-04-04 02:32:30,344] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.333333333333333, 100.0, 0.0, 0.0, 26.0, 25.34354071125251, 0.3325219013536434, 0.0, 1.0, 39112.53375580363], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3118800.0000, 
sim time next is 3119400.0000, 
raw observation next is [1.5, 100.0, 0.0, 0.0, 26.0, 25.41877433799898, 0.3466523458138596, 0.0, 1.0, 18765.85886897519], 
processed observation next is [1.0, 0.08695652173913043, 0.5041551246537397, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6182311948332483, 0.6155507819379532, 0.0, 1.0, 0.08936123270940566], 
reward next is 0.9106, 
noisyNet noise sample is [array([0.24683544], dtype=float32), -1.829291]. 
=============================================
[2019-04-04 02:32:30,412] A3C_AGENT_WORKER-Thread-16 INFO:Local step 216500, global step 3466454: loss 0.3671
[2019-04-04 02:32:30,412] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 216500, global step 3466454: learning rate 0.0005
[2019-04-04 02:32:31,343] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.6479127e-27 3.0556511e-23 7.0269791e-22 1.0000000e+00 5.9101267e-25
 2.1798834e-20 4.7434260e-21], sum to 1.0000
[2019-04-04 02:32:31,345] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6084
[2019-04-04 02:32:31,360] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 66.33333333333333, 0.0, 0.0, 26.0, 24.17197149363326, 0.1079778779743068, 0.0, 1.0, 41139.4876422486], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2353200.0000, 
sim time next is 2353800.0000, 
raw observation next is [-2.9, 65.66666666666667, 0.0, 0.0, 26.0, 24.1495060806397, 0.1030033110610633, 0.0, 1.0, 41175.42569398272], 
processed observation next is [0.0, 0.21739130434782608, 0.38227146814404434, 0.6566666666666667, 0.0, 0.0, 0.6666666666666666, 0.5124588400533083, 0.5343344370203544, 0.0, 1.0, 0.19607345568563198], 
reward next is 0.8039, 
noisyNet noise sample is [array([1.1967345], dtype=float32), 0.11920919]. 
=============================================
[2019-04-04 02:32:36,740] A3C_AGENT_WORKER-Thread-14 INFO:Local step 216000, global step 3468073: loss 0.1573
[2019-04-04 02:32:36,753] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 216000, global step 3468073: learning rate 0.0005
[2019-04-04 02:32:40,237] A3C_AGENT_WORKER-Thread-2 INFO:Local step 218000, global step 3468999: loss 0.2967
[2019-04-04 02:32:40,238] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 218000, global step 3468999: learning rate 0.0005
[2019-04-04 02:32:43,059] A3C_AGENT_WORKER-Thread-12 INFO:Local step 216500, global step 3469684: loss 0.0393
[2019-04-04 02:32:43,061] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 216500, global step 3469684: learning rate 0.0005
[2019-04-04 02:32:43,140] A3C_AGENT_WORKER-Thread-5 INFO:Local step 216500, global step 3469703: loss 0.0387
[2019-04-04 02:32:43,141] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 216500, global step 3469703: learning rate 0.0005
[2019-04-04 02:32:44,737] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.2666878e-23 5.5729982e-16 9.5142298e-15 1.0000000e+00 4.3511756e-18
 1.3593860e-12 2.0430409e-15], sum to 1.0000
[2019-04-04 02:32:44,737] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3819
[2019-04-04 02:32:44,773] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.600000000000001, 83.0, 0.0, 0.0, 26.0, 24.32602109180834, 0.1217319216628415, 0.0, 1.0, 41423.11900702921], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1998600.0000, 
sim time next is 1999200.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.34400640272548, 0.1265855403720567, 0.0, 1.0, 41354.56147684997], 
processed observation next is [1.0, 0.13043478260869565, 0.30747922437673136, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5286672002271233, 0.5421951801240189, 0.0, 1.0, 0.1969264832230951], 
reward next is 0.8031, 
noisyNet noise sample is [array([-0.35498935], dtype=float32), -1.1506308]. 
=============================================
[2019-04-04 02:32:45,966] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.4830590e-25 7.9346959e-19 8.3673971e-17 1.0000000e+00 3.9854572e-23
 1.8242233e-15 1.5555484e-18], sum to 1.0000
[2019-04-04 02:32:45,970] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1195
[2019-04-04 02:32:46,061] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.333333333333334, 62.33333333333334, 112.8333333333333, 796.0, 26.0, 25.94367508509608, 0.4666043431643716, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2722800.0000, 
sim time next is 2723400.0000, 
raw observation next is [-7.0, 61.5, 113.0, 799.0, 26.0, 25.86480095204576, 0.3619430005033122, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.2686980609418283, 0.615, 0.37666666666666665, 0.8828729281767956, 0.6666666666666666, 0.6554000793371465, 0.6206476668344374, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.47030646], dtype=float32), 0.39539948]. 
=============================================
[2019-04-04 02:32:46,246] A3C_AGENT_WORKER-Thread-15 INFO:Local step 217000, global step 3470451: loss 0.0674
[2019-04-04 02:32:46,260] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 217000, global step 3470451: learning rate 0.0005
[2019-04-04 02:32:48,539] A3C_AGENT_WORKER-Thread-10 INFO:Local step 217000, global step 3471052: loss 0.1176
[2019-04-04 02:32:48,546] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 217000, global step 3471052: learning rate 0.0005
[2019-04-04 02:32:48,600] A3C_AGENT_WORKER-Thread-20 INFO:Local step 217000, global step 3471067: loss 0.1847
[2019-04-04 02:32:48,603] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 217000, global step 3471067: learning rate 0.0005
[2019-04-04 02:32:49,994] A3C_AGENT_WORKER-Thread-6 INFO:Local step 217000, global step 3471444: loss 0.2574
[2019-04-04 02:32:49,995] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 217000, global step 3471444: learning rate 0.0005
[2019-04-04 02:32:50,674] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.6198024e-20 1.9223756e-13 5.3458143e-14 1.0000000e+00 1.5184561e-15
 8.4453590e-12 8.7233118e-14], sum to 1.0000
[2019-04-04 02:32:50,675] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8822
[2019-04-04 02:32:50,746] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.9, 82.0, 51.5, 0.0, 26.0, 25.65759113856756, 0.4101208041491242, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2044800.0000, 
sim time next is 2045400.0000, 
raw observation next is [-3.9, 82.00000000000001, 45.0, 0.0, 26.0, 25.86657889637593, 0.4128562313890938, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.3545706371191136, 0.8200000000000002, 0.15, 0.0, 0.6666666666666666, 0.6555482413646608, 0.6376187437963646, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.12389121], dtype=float32), 1.5931277]. 
=============================================
[2019-04-04 02:32:50,781] A3C_AGENT_WORKER-Thread-11 INFO:Local step 217500, global step 3471643: loss 0.8821
[2019-04-04 02:32:50,782] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 217500, global step 3471643: learning rate 0.0005
[2019-04-04 02:32:51,628] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.1279022e-26 1.2216721e-18 3.0707193e-19 1.0000000e+00 2.0460823e-23
 4.4135681e-16 4.9178457e-19], sum to 1.0000
[2019-04-04 02:32:51,629] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1576
[2019-04-04 02:32:51,639] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 46.0, 73.5, 587.5, 26.0, 26.0379324929033, 0.6476104257643437, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3340800.0000, 
sim time next is 3341400.0000, 
raw observation next is [-2.0, 46.66666666666667, 69.0, 558.6666666666667, 26.0, 26.38678948898249, 0.6766238918408979, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.40720221606648205, 0.46666666666666673, 0.23, 0.6173112338858197, 0.6666666666666666, 0.6988991240818742, 0.7255412972802993, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8675435], dtype=float32), 1.9960726]. 
=============================================
[2019-04-04 02:32:52,143] A3C_AGENT_WORKER-Thread-4 INFO:Local step 216000, global step 3472011: loss 0.4143
[2019-04-04 02:32:52,144] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 216000, global step 3472011: learning rate 0.0005
[2019-04-04 02:32:53,088] A3C_AGENT_WORKER-Thread-19 INFO:Local step 218000, global step 3472313: loss 0.4514
[2019-04-04 02:32:53,088] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 218000, global step 3472313: learning rate 0.0005
[2019-04-04 02:32:53,494] A3C_AGENT_WORKER-Thread-17 INFO:Local step 218000, global step 3472424: loss 0.5524
[2019-04-04 02:32:53,494] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 218000, global step 3472424: learning rate 0.0005
[2019-04-04 02:32:56,177] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.6236977e-26 6.3072690e-20 8.5270844e-19 1.0000000e+00 6.5754250e-22
 6.9045510e-18 9.2649998e-19], sum to 1.0000
[2019-04-04 02:32:56,178] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0919
[2019-04-04 02:32:56,198] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 70.0, 0.0, 0.0, 26.0, 25.28209108748464, 0.4221046372690494, 0.0, 1.0, 40966.22965000493], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3550200.0000, 
sim time next is 3550800.0000, 
raw observation next is [-3.0, 69.0, 0.0, 0.0, 26.0, 25.26695935289648, 0.4136463673805488, 0.0, 1.0, 40858.66678045761], 
processed observation next is [0.0, 0.08695652173913043, 0.3795013850415513, 0.69, 0.0, 0.0, 0.6666666666666666, 0.6055799460747068, 0.6378821224601829, 0.0, 1.0, 0.194565079906941], 
reward next is 0.8054, 
noisyNet noise sample is [array([-0.51965725], dtype=float32), 0.6656386]. 
=============================================
[2019-04-04 02:32:57,239] A3C_AGENT_WORKER-Thread-18 INFO:Local step 217000, global step 3473469: loss 0.2469
[2019-04-04 02:32:57,242] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 217000, global step 3473469: learning rate 0.0005
[2019-04-04 02:32:59,764] A3C_AGENT_WORKER-Thread-16 INFO:Local step 217000, global step 3474063: loss 0.1009
[2019-04-04 02:32:59,767] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 217000, global step 3474063: learning rate 0.0005
[2019-04-04 02:33:01,268] A3C_AGENT_WORKER-Thread-3 INFO:Local step 217000, global step 3474415: loss 0.1703
[2019-04-04 02:33:01,271] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 217000, global step 3474415: learning rate 0.0005
[2019-04-04 02:33:03,904] A3C_AGENT_WORKER-Thread-13 INFO:Local step 216000, global step 3475105: loss 0.3233
[2019-04-04 02:33:03,906] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 216000, global step 3475105: learning rate 0.0005
[2019-04-04 02:33:08,011] A3C_AGENT_WORKER-Thread-2 INFO:Local step 218500, global step 3476082: loss 4.2881
[2019-04-04 02:33:08,032] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 218500, global step 3476087: learning rate 0.0005
[2019-04-04 02:33:10,036] A3C_AGENT_WORKER-Thread-14 INFO:Local step 216500, global step 3476481: loss 0.1573
[2019-04-04 02:33:10,036] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 216500, global step 3476481: learning rate 0.0005
[2019-04-04 02:33:12,348] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1769449e-22 1.3755219e-13 2.2721330e-14 9.9999976e-01 1.4940348e-17
 2.7214620e-07 3.0030494e-15], sum to 1.0000
[2019-04-04 02:33:12,349] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8101
[2019-04-04 02:33:12,422] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.333333333333333, 97.66666666666667, 0.0, 0.0, 26.0, 24.77496768998368, 0.2199137495026439, 0.0, 1.0, 55431.74823053519], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2874000.0000, 
sim time next is 2874600.0000, 
raw observation next is [1.5, 96.5, 0.0, 0.0, 26.0, 24.73171244957057, 0.2261933623845539, 0.0, 1.0, 55379.37739625446], 
processed observation next is [1.0, 0.2608695652173913, 0.5041551246537397, 0.965, 0.0, 0.0, 0.6666666666666666, 0.5609760374642141, 0.5753977874615179, 0.0, 1.0, 0.263711320934545], 
reward next is 0.7363, 
noisyNet noise sample is [array([-1.4998796], dtype=float32), -1.6106907]. 
=============================================
[2019-04-04 02:33:14,716] A3C_AGENT_WORKER-Thread-5 INFO:Local step 217000, global step 3477598: loss 0.0603
[2019-04-04 02:33:14,723] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 217000, global step 3477602: learning rate 0.0005
[2019-04-04 02:33:15,174] A3C_AGENT_WORKER-Thread-12 INFO:Local step 217000, global step 3477724: loss 0.0137
[2019-04-04 02:33:15,176] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 217000, global step 3477724: learning rate 0.0005
[2019-04-04 02:33:16,615] A3C_AGENT_WORKER-Thread-15 INFO:Local step 217500, global step 3478047: loss 2.2091
[2019-04-04 02:33:16,616] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 217500, global step 3478047: learning rate 0.0005
[2019-04-04 02:33:21,836] A3C_AGENT_WORKER-Thread-10 INFO:Local step 217500, global step 3479070: loss 1.7518
[2019-04-04 02:33:21,837] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 217500, global step 3479070: learning rate 0.0005
[2019-04-04 02:33:22,399] A3C_AGENT_WORKER-Thread-11 INFO:Local step 218000, global step 3479172: loss 0.7878
[2019-04-04 02:33:22,399] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 218000, global step 3479172: learning rate 0.0005
[2019-04-04 02:33:23,039] A3C_AGENT_WORKER-Thread-20 INFO:Local step 217500, global step 3479340: loss 0.9433
[2019-04-04 02:33:23,043] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 217500, global step 3479340: learning rate 0.0005
[2019-04-04 02:33:24,022] A3C_AGENT_WORKER-Thread-17 INFO:Local step 218500, global step 3479549: loss 4.9818
[2019-04-04 02:33:24,024] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 218500, global step 3479549: learning rate 0.0005
[2019-04-04 02:33:24,929] A3C_AGENT_WORKER-Thread-6 INFO:Local step 217500, global step 3479734: loss 1.3073
[2019-04-04 02:33:24,932] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 217500, global step 3479735: learning rate 0.0005
[2019-04-04 02:33:25,542] A3C_AGENT_WORKER-Thread-19 INFO:Local step 218500, global step 3479887: loss 4.8796
[2019-04-04 02:33:25,543] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 218500, global step 3479887: learning rate 0.0005
[2019-04-04 02:33:27,488] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.9324401e-35 4.8219472e-29 3.1061820e-26 1.0000000e+00 1.3676606e-32
 1.5398455e-31 3.3184023e-27], sum to 1.0000
[2019-04-04 02:33:27,489] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3835
[2019-04-04 02:33:27,536] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.1666666666666667, 39.16666666666666, 89.0, 707.0, 26.0, 25.12508615499956, 0.3648708069368586, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3078600.0000, 
sim time next is 3079200.0000, 
raw observation next is [0.3333333333333333, 39.33333333333334, 86.5, 690.0, 26.0, 25.12353441416526, 0.3634159125137463, 0.0, 1.0, 18697.72079975967], 
processed observation next is [0.0, 0.6521739130434783, 0.4718374884579871, 0.3933333333333334, 0.28833333333333333, 0.7624309392265194, 0.6666666666666666, 0.593627867847105, 0.6211386375045821, 0.0, 1.0, 0.08903676571314129], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.04512109], dtype=float32), -1.196109]. 
=============================================
[2019-04-04 02:33:29,266] A3C_AGENT_WORKER-Thread-4 INFO:Local step 216500, global step 3480689: loss 0.3347
[2019-04-04 02:33:29,268] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 216500, global step 3480689: learning rate 0.0005
[2019-04-04 02:33:34,722] A3C_AGENT_WORKER-Thread-18 INFO:Local step 217500, global step 3481941: loss 0.7932
[2019-04-04 02:33:34,722] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 217500, global step 3481941: learning rate 0.0005
[2019-04-04 02:33:36,837] A3C_AGENT_WORKER-Thread-16 INFO:Local step 217500, global step 3482442: loss 0.7387
[2019-04-04 02:33:36,842] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 217500, global step 3482442: learning rate 0.0005
[2019-04-04 02:33:37,597] A3C_AGENT_WORKER-Thread-3 INFO:Local step 217500, global step 3482612: loss 0.9199
[2019-04-04 02:33:37,598] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 217500, global step 3482612: learning rate 0.0005
[2019-04-04 02:33:40,925] A3C_AGENT_WORKER-Thread-2 INFO:Local step 219000, global step 3483292: loss 5.0128
[2019-04-04 02:33:40,925] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 219000, global step 3483292: learning rate 0.0005
[2019-04-04 02:33:42,332] A3C_AGENT_WORKER-Thread-13 INFO:Local step 216500, global step 3483566: loss 0.1785
[2019-04-04 02:33:42,405] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 216500, global step 3483566: learning rate 0.0005
[2019-04-04 02:33:45,571] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.6879573e-25 3.4309779e-15 1.8993438e-18 1.0000000e+00 7.8402238e-18
 9.9450732e-11 1.1527406e-17], sum to 1.0000
[2019-04-04 02:33:45,571] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7131
[2019-04-04 02:33:45,608] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.0, 100.0, 108.0, 746.0, 26.0, 26.88917636749629, 0.6987839693234538, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3148200.0000, 
sim time next is 3148800.0000, 
raw observation next is [7.0, 100.0, 109.0, 755.8333333333334, 26.0, 26.95834022258996, 0.7166567993752285, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.6565096952908588, 1.0, 0.36333333333333334, 0.8351749539594844, 0.6666666666666666, 0.7465283518824967, 0.7388855997917428, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3500775], dtype=float32), 0.26521036]. 
=============================================
[2019-04-04 02:33:46,485] A3C_AGENT_WORKER-Thread-14 INFO:Local step 217000, global step 3484465: loss 0.1421
[2019-04-04 02:33:46,485] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 217000, global step 3484465: learning rate 0.0005
[2019-04-04 02:33:51,653] A3C_AGENT_WORKER-Thread-15 INFO:Local step 218000, global step 3485635: loss 0.1480
[2019-04-04 02:33:51,654] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 218000, global step 3485635: learning rate 0.0005
[2019-04-04 02:33:53,491] A3C_AGENT_WORKER-Thread-12 INFO:Local step 217500, global step 3486000: loss 1.7433
[2019-04-04 02:33:53,491] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 217500, global step 3486000: learning rate 0.0005
[2019-04-04 02:33:53,977] A3C_AGENT_WORKER-Thread-5 INFO:Local step 217500, global step 3486125: loss 1.4077
[2019-04-04 02:33:53,977] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 217500, global step 3486125: learning rate 0.0005
[2019-04-04 02:33:57,173] A3C_AGENT_WORKER-Thread-11 INFO:Local step 218500, global step 3486842: loss 5.1177
[2019-04-04 02:33:57,190] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 218500, global step 3486842: learning rate 0.0005
[2019-04-04 02:33:57,252] A3C_AGENT_WORKER-Thread-10 INFO:Local step 218000, global step 3486862: loss 0.2035
[2019-04-04 02:33:57,253] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 218000, global step 3486862: learning rate 0.0005
[2019-04-04 02:33:57,944] A3C_AGENT_WORKER-Thread-20 INFO:Local step 218000, global step 3487022: loss 0.1986
[2019-04-04 02:33:57,949] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 218000, global step 3487026: learning rate 0.0005
[2019-04-04 02:33:59,068] A3C_AGENT_WORKER-Thread-17 INFO:Local step 219000, global step 3487317: loss 3.7425
[2019-04-04 02:33:59,070] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 219000, global step 3487317: learning rate 0.0005
[2019-04-04 02:33:59,800] A3C_AGENT_WORKER-Thread-6 INFO:Local step 218000, global step 3487484: loss 0.3177
[2019-04-04 02:33:59,824] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 218000, global step 3487484: learning rate 0.0005
[2019-04-04 02:34:00,831] A3C_AGENT_WORKER-Thread-19 INFO:Local step 219000, global step 3487681: loss 4.3014
[2019-04-04 02:34:00,831] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 219000, global step 3487681: learning rate 0.0005
[2019-04-04 02:34:07,569] A3C_AGENT_WORKER-Thread-4 INFO:Local step 217000, global step 3489068: loss 0.0307
[2019-04-04 02:34:07,571] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 217000, global step 3489068: learning rate 0.0005
[2019-04-04 02:34:10,868] A3C_AGENT_WORKER-Thread-18 INFO:Local step 218000, global step 3489862: loss 0.1639
[2019-04-04 02:34:10,869] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 218000, global step 3489862: learning rate 0.0005
[2019-04-04 02:34:13,543] A3C_AGENT_WORKER-Thread-16 INFO:Local step 218000, global step 3490450: loss 0.1052
[2019-04-04 02:34:13,545] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 218000, global step 3490450: learning rate 0.0005
[2019-04-04 02:34:14,346] A3C_AGENT_WORKER-Thread-3 INFO:Local step 218000, global step 3490643: loss 0.2407
[2019-04-04 02:34:14,352] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 218000, global step 3490643: learning rate 0.0005
[2019-04-04 02:34:15,182] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.7447107e-27 3.6539896e-20 3.0021359e-21 1.0000000e+00 4.2754942e-23
 8.4175874e-19 4.7176029e-21], sum to 1.0000
[2019-04-04 02:34:15,182] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3247
[2019-04-04 02:34:15,255] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 52.0, 115.5, 814.5, 26.0, 26.19180531902213, 0.6377331266204719, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3502800.0000, 
sim time next is 3503400.0000, 
raw observation next is [2.166666666666667, 51.5, 115.3333333333333, 811.6666666666666, 26.0, 26.24511090452853, 0.654898266471735, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5226223453370269, 0.515, 0.3844444444444443, 0.8968692449355432, 0.6666666666666666, 0.6870925753773776, 0.7182994221572451, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.2020037], dtype=float32), 0.95073456]. 
=============================================
[2019-04-04 02:34:15,312] A3C_AGENT_WORKER-Thread-2 INFO:Local step 219500, global step 3490904: loss 0.0385
[2019-04-04 02:34:15,313] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 219500, global step 3490904: learning rate 0.0005
[2019-04-04 02:34:20,069] A3C_AGENT_WORKER-Thread-13 INFO:Local step 217000, global step 3492000: loss 0.0124
[2019-04-04 02:34:20,077] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 217000, global step 3492000: learning rate 0.0005
[2019-04-04 02:34:25,232] A3C_AGENT_WORKER-Thread-14 INFO:Local step 217500, global step 3493163: loss 1.5640
[2019-04-04 02:34:25,232] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 217500, global step 3493163: learning rate 0.0005
[2019-04-04 02:34:26,878] A3C_AGENT_WORKER-Thread-15 INFO:Local step 218500, global step 3493531: loss 6.2507
[2019-04-04 02:34:26,879] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 218500, global step 3493531: learning rate 0.0005
[2019-04-04 02:34:28,799] A3C_AGENT_WORKER-Thread-12 INFO:Local step 218000, global step 3494040: loss 0.9840
[2019-04-04 02:34:28,808] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 218000, global step 3494043: learning rate 0.0005
[2019-04-04 02:34:29,540] A3C_AGENT_WORKER-Thread-5 INFO:Local step 218000, global step 3494229: loss 0.7472
[2019-04-04 02:34:29,540] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 218000, global step 3494229: learning rate 0.0005
[2019-04-04 02:34:30,577] A3C_AGENT_WORKER-Thread-10 INFO:Local step 218500, global step 3494476: loss 3.5485
[2019-04-04 02:34:30,577] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 218500, global step 3494476: learning rate 0.0005
[2019-04-04 02:34:31,501] A3C_AGENT_WORKER-Thread-11 INFO:Local step 219000, global step 3494728: loss 1.1011
[2019-04-04 02:34:31,509] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 219000, global step 3494732: learning rate 0.0005
[2019-04-04 02:34:33,186] A3C_AGENT_WORKER-Thread-20 INFO:Local step 218500, global step 3495217: loss 4.4502
[2019-04-04 02:34:33,213] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 218500, global step 3495217: learning rate 0.0005
[2019-04-04 02:34:33,656] A3C_AGENT_WORKER-Thread-17 INFO:Local step 219500, global step 3495324: loss 0.0484
[2019-04-04 02:34:33,662] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 219500, global step 3495324: learning rate 0.0005
[2019-04-04 02:34:34,056] A3C_AGENT_WORKER-Thread-19 INFO:Local step 219500, global step 3495434: loss 0.0409
[2019-04-04 02:34:34,082] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 219500, global step 3495434: learning rate 0.0005
[2019-04-04 02:34:34,587] A3C_AGENT_WORKER-Thread-6 INFO:Local step 218500, global step 3495553: loss 4.1270
[2019-04-04 02:34:34,591] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 218500, global step 3495553: learning rate 0.0005
[2019-04-04 02:34:40,741] A3C_AGENT_WORKER-Thread-18 INFO:Local step 218500, global step 3497368: loss 4.8189
[2019-04-04 02:34:40,744] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 218500, global step 3497368: learning rate 0.0005
[2019-04-04 02:34:41,990] A3C_AGENT_WORKER-Thread-4 INFO:Local step 217500, global step 3497704: loss 1.1375
[2019-04-04 02:34:41,995] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 217500, global step 3497704: learning rate 0.0005
[2019-04-04 02:34:44,387] A3C_AGENT_WORKER-Thread-3 INFO:Local step 218500, global step 3498375: loss 3.6563
[2019-04-04 02:34:44,390] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 218500, global step 3498376: learning rate 0.0005
[2019-04-04 02:34:45,098] A3C_AGENT_WORKER-Thread-2 INFO:Local step 220000, global step 3498565: loss 1.0773
[2019-04-04 02:34:45,107] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 220000, global step 3498565: learning rate 0.0005
[2019-04-04 02:34:45,670] A3C_AGENT_WORKER-Thread-16 INFO:Local step 218500, global step 3498712: loss 3.5287
[2019-04-04 02:34:45,672] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 218500, global step 3498712: learning rate 0.0005
[2019-04-04 02:34:50,507] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-04 02:34:50,513] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 02:34:50,513] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:34:50,517] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 02:34:50,521] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:34:50,522] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run36
[2019-04-04 02:34:50,520] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run36
[2019-04-04 02:34:50,624] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 02:34:50,625] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:34:50,700] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run36
[2019-04-04 02:35:28,614] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.7436336], dtype=float32), 0.078377865]
[2019-04-04 02:35:28,614] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-1.160437517666667, 82.65479101, 31.53663675583333, 0.0, 20.0, 19.5186375089588, -0.954178544901103, 0.0, 1.0, 0.0]
[2019-04-04 02:35:28,614] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 02:35:28,615] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [4.7741170e-16 1.2704253e-14 1.1073104e-12 1.0000000e+00 7.1097520e-12
 2.1316964e-11 1.6806207e-11], sampled 0.2759376870197111
[2019-04-04 02:35:31,168] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.7436336], dtype=float32), 0.078377865]
[2019-04-04 02:35:31,168] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [2.816666666666667, 76.33333333333334, 37.00000000000001, 104.0, 20.0, 19.53136377586777, -0.8975632938801867, 1.0, 1.0, 0.0]
[2019-04-04 02:35:31,168] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 02:35:31,169] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [2.6429220e-16 5.4078287e-14 1.0112780e-12 1.0000000e+00 6.2209404e-11
 8.0227193e-11 9.1427118e-12], sampled 0.020930097499714084
[2019-04-04 02:35:55,380] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.7436336], dtype=float32), 0.078377865]
[2019-04-04 02:35:55,381] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [2.666666666666667, 100.0, 0.0, 0.0, 20.0, 19.67365503803424, -0.9664025211399082, 0.0, 1.0, 18724.11285344516]
[2019-04-04 02:35:55,381] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 02:35:55,381] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.3888212e-15 2.1894513e-12 2.5875028e-11 1.0000000e+00 5.5762900e-10
 8.3976657e-09 2.5152580e-10], sampled 0.97739282041273
[2019-04-04 02:36:09,540] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5776.1109 156560672.1847 -1760.7610
[2019-04-04 02:36:19,278] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5275.9049 181489267.1220 -2429.2178
[2019-04-04 02:36:25,999] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5268.2788 194208590.8254 -2349.3598
[2019-04-04 02:36:27,022] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 3500000, evaluation results [3500000.0, 5275.904862089485, 181489267.12202036, -2429.2177547133747, 5776.1109410754025, 156560672.18473327, -1760.7609880341015, 5268.278799288253, 194208590.82538438, -2349.359764412906]
[2019-04-04 02:36:28,933] A3C_AGENT_WORKER-Thread-13 INFO:Local step 217500, global step 3500833: loss 0.8135
[2019-04-04 02:36:28,934] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 217500, global step 3500834: learning rate 0.0005
[2019-04-04 02:36:30,270] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.1592813e-23 1.7192688e-18 4.1292841e-19 1.0000000e+00 8.2877449e-20
 7.1567923e-18 7.3439748e-17], sum to 1.0000
[2019-04-04 02:36:30,271] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2450
[2019-04-04 02:36:30,301] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 55.0, 0.0, 0.0, 26.0, 25.45341870912014, 0.4767427306323082, 0.0, 1.0, 25681.63140449749], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3353400.0000, 
sim time next is 3354000.0000, 
raw observation next is [-3.0, 55.0, 0.0, 0.0, 26.0, 25.35784632952823, 0.4618049543014839, 1.0, 1.0, 25281.22227793451], 
processed observation next is [1.0, 0.8260869565217391, 0.3795013850415513, 0.55, 0.0, 0.0, 0.6666666666666666, 0.6131538607940191, 0.6539349847671613, 1.0, 1.0, 0.12038677275206909], 
reward next is 0.8796, 
noisyNet noise sample is [array([0.02599284], dtype=float32), 0.6026689]. 
=============================================
[2019-04-04 02:36:30,322] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[76.442825]
 [77.024124]
 [77.95011 ]
 [77.72381 ]
 [77.46446 ]], R is [[77.48744965]
 [77.59028625]
 [77.68502045]
 [77.75746918]
 [77.77721405]].
[2019-04-04 02:36:30,331] A3C_AGENT_WORKER-Thread-15 INFO:Local step 219000, global step 3501500: loss 4.5699
[2019-04-04 02:36:30,332] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 219000, global step 3501500: learning rate 0.0005
[2019-04-04 02:36:30,777] A3C_AGENT_WORKER-Thread-14 INFO:Local step 218000, global step 3501706: loss 0.1158
[2019-04-04 02:36:30,778] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 218000, global step 3501706: learning rate 0.0005
[2019-04-04 02:36:31,305] A3C_AGENT_WORKER-Thread-12 INFO:Local step 218500, global step 3501914: loss 4.3111
[2019-04-04 02:36:31,307] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 218500, global step 3501914: learning rate 0.0005
[2019-04-04 02:36:31,767] A3C_AGENT_WORKER-Thread-5 INFO:Local step 218500, global step 3502132: loss 4.6597
[2019-04-04 02:36:31,775] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 218500, global step 3502133: learning rate 0.0005
[2019-04-04 02:36:32,171] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0089469e-23 9.0145078e-19 2.6946864e-18 1.0000000e+00 1.2297739e-20
 2.5858004e-15 1.2637225e-16], sum to 1.0000
[2019-04-04 02:36:32,173] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3899
[2019-04-04 02:36:32,186] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-13.0, 66.0, 0.0, 0.0, 26.0, 23.69993658251408, 0.00791161531416291, 0.0, 1.0, 43787.59839893144], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3994200.0000, 
sim time next is 3994800.0000, 
raw observation next is [-13.0, 65.0, 0.0, 0.0, 26.0, 23.62303421110813, -0.00734162043356792, 0.0, 1.0, 43757.54527741557], 
processed observation next is [1.0, 0.21739130434782608, 0.10249307479224376, 0.65, 0.0, 0.0, 0.6666666666666666, 0.4685861842590109, 0.4975527931888107, 0.0, 1.0, 0.20836926322578844], 
reward next is 0.7916, 
noisyNet noise sample is [array([-1.2445422], dtype=float32), -0.24461931]. 
=============================================
[2019-04-04 02:36:32,724] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.4944464e-29 9.2742458e-23 4.4773865e-21 1.0000000e+00 1.1645191e-25
 7.6059099e-25 3.2006967e-22], sum to 1.0000
[2019-04-04 02:36:32,724] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6665
[2019-04-04 02:36:32,746] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.666666666666667, 38.0, 106.8333333333333, 794.0, 26.0, 26.86540029861344, 0.7575688521895126, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3939600.0000, 
sim time next is 3940200.0000, 
raw observation next is [-4.5, 38.0, 105.0, 788.0, 26.0, 26.89830605933716, 0.7606169897497898, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3379501385041552, 0.38, 0.35, 0.8707182320441988, 0.6666666666666666, 0.7415255049447632, 0.7535389965832633, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.56866497], dtype=float32), 0.80720484]. 
=============================================
[2019-04-04 02:36:32,891] A3C_AGENT_WORKER-Thread-10 INFO:Local step 219000, global step 3502652: loss 5.4020
[2019-04-04 02:36:32,892] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 219000, global step 3502652: learning rate 0.0005
[2019-04-04 02:36:32,982] A3C_AGENT_WORKER-Thread-17 INFO:Local step 220000, global step 3502693: loss 0.5821
[2019-04-04 02:36:32,983] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 220000, global step 3502694: learning rate 0.0005
[2019-04-04 02:36:33,299] A3C_AGENT_WORKER-Thread-11 INFO:Local step 219500, global step 3502849: loss 0.0038
[2019-04-04 02:36:33,301] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 219500, global step 3502849: learning rate 0.0005
[2019-04-04 02:36:34,128] A3C_AGENT_WORKER-Thread-20 INFO:Local step 219000, global step 3503191: loss 4.2823
[2019-04-04 02:36:34,170] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 219000, global step 3503194: learning rate 0.0005
[2019-04-04 02:36:34,360] A3C_AGENT_WORKER-Thread-19 INFO:Local step 220000, global step 3503297: loss 0.9263
[2019-04-04 02:36:34,366] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 220000, global step 3503303: learning rate 0.0005
[2019-04-04 02:36:35,362] A3C_AGENT_WORKER-Thread-6 INFO:Local step 219000, global step 3503800: loss 4.3938
[2019-04-04 02:36:35,371] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 219000, global step 3503802: learning rate 0.0005
[2019-04-04 02:36:38,671] A3C_AGENT_WORKER-Thread-18 INFO:Local step 219000, global step 3505226: loss 7.2041
[2019-04-04 02:36:38,673] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 219000, global step 3505226: learning rate 0.0005
[2019-04-04 02:36:40,148] A3C_AGENT_WORKER-Thread-4 INFO:Local step 218000, global step 3505928: loss 0.1120
[2019-04-04 02:36:40,149] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 218000, global step 3505930: learning rate 0.0005
[2019-04-04 02:36:40,616] A3C_AGENT_WORKER-Thread-3 INFO:Local step 219000, global step 3506155: loss 4.5120
[2019-04-04 02:36:40,617] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 219000, global step 3506155: learning rate 0.0005
[2019-04-04 02:36:41,271] A3C_AGENT_WORKER-Thread-2 INFO:Local step 220500, global step 3506440: loss 0.0027
[2019-04-04 02:36:41,271] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 220500, global step 3506440: learning rate 0.0005
[2019-04-04 02:36:41,690] A3C_AGENT_WORKER-Thread-16 INFO:Local step 219000, global step 3506614: loss 6.2200
[2019-04-04 02:36:41,690] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 219000, global step 3506614: learning rate 0.0005
[2019-04-04 02:36:47,212] A3C_AGENT_WORKER-Thread-13 INFO:Local step 218000, global step 3508978: loss 0.1682
[2019-04-04 02:36:47,213] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 218000, global step 3508978: learning rate 0.0005
[2019-04-04 02:36:48,122] A3C_AGENT_WORKER-Thread-12 INFO:Local step 219000, global step 3509381: loss 5.5607
[2019-04-04 02:36:48,124] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 219000, global step 3509381: learning rate 0.0005
[2019-04-04 02:36:48,301] A3C_AGENT_WORKER-Thread-15 INFO:Local step 219500, global step 3509480: loss 0.0369
[2019-04-04 02:36:48,305] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 219500, global step 3509480: learning rate 0.0005
[2019-04-04 02:36:48,442] A3C_AGENT_WORKER-Thread-14 INFO:Local step 218500, global step 3509551: loss 5.3143
[2019-04-04 02:36:48,458] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 218500, global step 3509553: learning rate 0.0005
[2019-04-04 02:36:49,521] A3C_AGENT_WORKER-Thread-11 INFO:Local step 220000, global step 3510067: loss 0.7116
[2019-04-04 02:36:49,522] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 220000, global step 3510068: learning rate 0.0005
[2019-04-04 02:36:49,637] A3C_AGENT_WORKER-Thread-5 INFO:Local step 219000, global step 3510124: loss 4.3875
[2019-04-04 02:36:49,639] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 219000, global step 3510125: learning rate 0.0005
[2019-04-04 02:36:50,300] A3C_AGENT_WORKER-Thread-17 INFO:Local step 220500, global step 3510464: loss 0.0819
[2019-04-04 02:36:50,302] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 220500, global step 3510464: learning rate 0.0005
[2019-04-04 02:36:50,543] A3C_AGENT_WORKER-Thread-10 INFO:Local step 219500, global step 3510594: loss 0.0298
[2019-04-04 02:36:50,545] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 219500, global step 3510595: learning rate 0.0005
[2019-04-04 02:36:51,237] A3C_AGENT_WORKER-Thread-19 INFO:Local step 220500, global step 3510924: loss 0.0028
[2019-04-04 02:36:51,238] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 220500, global step 3510924: learning rate 0.0005
[2019-04-04 02:36:51,612] A3C_AGENT_WORKER-Thread-20 INFO:Local step 219500, global step 3511106: loss 0.1269
[2019-04-04 02:36:51,613] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 219500, global step 3511106: learning rate 0.0005
[2019-04-04 02:36:53,340] A3C_AGENT_WORKER-Thread-6 INFO:Local step 219500, global step 3511858: loss 0.0357
[2019-04-04 02:36:53,342] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 219500, global step 3511858: learning rate 0.0005
[2019-04-04 02:36:55,049] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:36:55,049] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:36:55,053] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run27
[2019-04-04 02:36:55,231] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.2951849e-29 5.4029762e-24 8.0136164e-23 1.0000000e+00 7.1456562e-27
 2.7460579e-22 1.1807765e-22], sum to 1.0000
[2019-04-04 02:36:55,232] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7631
[2019-04-04 02:36:55,253] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 48.0, 0.0, 0.0, 26.0, 25.41230224237825, 0.3596135322400792, 0.0, 1.0, 41858.63093365571], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4234200.0000, 
sim time next is 4234800.0000, 
raw observation next is [2.0, 48.0, 0.0, 0.0, 26.0, 25.41852610237872, 0.3586530498686003, 0.0, 1.0, 37142.94762660154], 
processed observation next is [0.0, 0.0, 0.518005540166205, 0.48, 0.0, 0.0, 0.6666666666666666, 0.61821050853156, 0.6195510166228667, 0.0, 1.0, 0.17687117917429304], 
reward next is 0.8231, 
noisyNet noise sample is [array([0.7156823], dtype=float32), -0.4983247]. 
=============================================
[2019-04-04 02:36:56,578] A3C_AGENT_WORKER-Thread-18 INFO:Local step 219500, global step 3513460: loss 0.0268
[2019-04-04 02:36:56,584] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 219500, global step 3513464: learning rate 0.0005
[2019-04-04 02:36:57,390] A3C_AGENT_WORKER-Thread-4 INFO:Local step 218500, global step 3513822: loss 3.8632
[2019-04-04 02:36:57,391] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 218500, global step 3513822: learning rate 0.0005
[2019-04-04 02:36:58,704] A3C_AGENT_WORKER-Thread-3 INFO:Local step 219500, global step 3514475: loss 0.0275
[2019-04-04 02:36:58,706] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 219500, global step 3514475: learning rate 0.0005
[2019-04-04 02:36:59,077] A3C_AGENT_WORKER-Thread-16 INFO:Local step 219500, global step 3514663: loss 0.0446
[2019-04-04 02:36:59,078] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 219500, global step 3514663: learning rate 0.0005
[2019-04-04 02:37:01,356] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.9275319e-25 1.2552316e-18 1.2986345e-17 1.0000000e+00 8.7057496e-20
 6.9067482e-15 2.5168950e-17], sum to 1.0000
[2019-04-04 02:37:01,361] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3732
[2019-04-04 02:37:01,410] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 72.0, 32.99999999999999, 233.6666666666666, 26.0, 25.25806771462349, 0.3102196436188434, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3743400.0000, 
sim time next is 3744000.0000, 
raw observation next is [-4.0, 71.0, 47.0, 282.5, 26.0, 25.25284463894346, 0.3017852547692122, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.3518005540166205, 0.71, 0.15666666666666668, 0.31215469613259667, 0.6666666666666666, 0.6044037199119551, 0.6005950849230707, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9088909], dtype=float32), 0.85936207]. 
=============================================
[2019-04-04 02:37:01,422] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[90.14285 ]
 [90.12965 ]
 [89.58542 ]
 [87.88237 ]
 [86.561386]], R is [[88.82698059]
 [88.93871307]
 [89.04932404]
 [89.15882874]
 [89.26724243]].
[2019-04-04 02:37:03,688] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:37:03,689] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:37:03,707] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run27
[2019-04-04 02:37:03,749] A3C_AGENT_WORKER-Thread-15 INFO:Local step 220000, global step 3516825: loss 0.5163
[2019-04-04 02:37:03,751] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 220000, global step 3516825: learning rate 0.0005
[2019-04-04 02:37:04,569] A3C_AGENT_WORKER-Thread-13 INFO:Local step 218500, global step 3517211: loss 3.5424
[2019-04-04 02:37:04,572] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 218500, global step 3517211: learning rate 0.0005
[2019-04-04 02:37:05,093] A3C_AGENT_WORKER-Thread-14 INFO:Local step 219000, global step 3517474: loss 2.9303
[2019-04-04 02:37:05,106] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 219000, global step 3517474: learning rate 0.0005
[2019-04-04 02:37:05,135] A3C_AGENT_WORKER-Thread-12 INFO:Local step 219500, global step 3517498: loss 0.0375
[2019-04-04 02:37:05,135] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 219500, global step 3517498: learning rate 0.0005
[2019-04-04 02:37:05,519] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:37:05,520] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:37:05,529] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run27
[2019-04-04 02:37:06,080] A3C_AGENT_WORKER-Thread-10 INFO:Local step 220000, global step 3517940: loss 0.3632
[2019-04-04 02:37:06,081] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 220000, global step 3517940: learning rate 0.0005
[2019-04-04 02:37:06,822] A3C_AGENT_WORKER-Thread-11 INFO:Local step 220500, global step 3518263: loss 0.0209
[2019-04-04 02:37:06,824] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 220500, global step 3518265: learning rate 0.0005
[2019-04-04 02:37:06,877] A3C_AGENT_WORKER-Thread-5 INFO:Local step 219500, global step 3518292: loss 0.0329
[2019-04-04 02:37:06,881] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 219500, global step 3518293: learning rate 0.0005
[2019-04-04 02:37:07,464] A3C_AGENT_WORKER-Thread-20 INFO:Local step 220000, global step 3518527: loss 0.1028
[2019-04-04 02:37:07,465] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 220000, global step 3518527: learning rate 0.0005
[2019-04-04 02:37:08,727] A3C_AGENT_WORKER-Thread-6 INFO:Local step 220000, global step 3519027: loss 0.2972
[2019-04-04 02:37:08,728] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 220000, global step 3519027: learning rate 0.0005
[2019-04-04 02:37:10,109] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.1421956e-29 1.3117148e-23 1.3387615e-21 1.0000000e+00 2.0826652e-26
 8.5086520e-21 5.1371071e-22], sum to 1.0000
[2019-04-04 02:37:10,110] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8154
[2019-04-04 02:37:10,137] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 39.33333333333334, 0.0, 0.0, 26.0, 25.47692380901639, 0.4027134910848025, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4911000.0000, 
sim time next is 4911600.0000, 
raw observation next is [1.0, 38.66666666666667, 0.0, 0.0, 26.0, 25.53710703104585, 0.3992339680068489, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.4903047091412743, 0.3866666666666667, 0.0, 0.0, 0.6666666666666666, 0.6280922525871541, 0.6330779893356163, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.8296509], dtype=float32), -0.7265069]. 
=============================================
[2019-04-04 02:37:12,786] A3C_AGENT_WORKER-Thread-18 INFO:Local step 220000, global step 3520899: loss 0.2919
[2019-04-04 02:37:12,787] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 220000, global step 3520899: learning rate 0.0005
[2019-04-04 02:37:13,665] A3C_AGENT_WORKER-Thread-4 INFO:Local step 219000, global step 3521359: loss 5.5699
[2019-04-04 02:37:13,666] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 219000, global step 3521359: learning rate 0.0005
[2019-04-04 02:37:14,366] A3C_AGENT_WORKER-Thread-3 INFO:Local step 220000, global step 3521739: loss 0.0654
[2019-04-04 02:37:14,369] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 220000, global step 3521739: learning rate 0.0005
[2019-04-04 02:37:14,859] A3C_AGENT_WORKER-Thread-16 INFO:Local step 220000, global step 3521982: loss 0.2889
[2019-04-04 02:37:14,861] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 220000, global step 3521982: learning rate 0.0005
[2019-04-04 02:37:15,221] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.8359448e-30 1.5703956e-24 8.2054294e-25 1.0000000e+00 4.2981197e-26
 3.3008240e-22 2.0845431e-24], sum to 1.0000
[2019-04-04 02:37:15,230] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5523
[2019-04-04 02:37:15,237] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.666666666666667, 38.0, 0.0, 0.0, 26.0, 25.84935266176529, 0.5428609313663114, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4129800.0000, 
sim time next is 4130400.0000, 
raw observation next is [2.333333333333333, 39.0, 0.0, 0.0, 26.0, 25.80739915296864, 0.52400662577546, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.5272391505078486, 0.39, 0.0, 0.0, 0.6666666666666666, 0.6506165960807199, 0.6746688752584866, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.26982552], dtype=float32), -1.6528504]. 
=============================================
[2019-04-04 02:37:20,043] A3C_AGENT_WORKER-Thread-15 INFO:Local step 220500, global step 3524239: loss 0.0149
[2019-04-04 02:37:20,045] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 220500, global step 3524239: learning rate 0.0005
[2019-04-04 02:37:20,302] A3C_AGENT_WORKER-Thread-12 INFO:Local step 220000, global step 3524355: loss 0.3652
[2019-04-04 02:37:20,304] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 220000, global step 3524356: learning rate 0.0005
[2019-04-04 02:37:20,503] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:37:20,503] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:37:20,527] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run27
[2019-04-04 02:37:21,164] A3C_AGENT_WORKER-Thread-13 INFO:Local step 219000, global step 3524688: loss 4.4360
[2019-04-04 02:37:21,165] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 219000, global step 3524688: learning rate 0.0005
[2019-04-04 02:37:22,084] A3C_AGENT_WORKER-Thread-5 INFO:Local step 220000, global step 3525049: loss 0.2828
[2019-04-04 02:37:22,089] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 220000, global step 3525050: learning rate 0.0005
[2019-04-04 02:37:22,287] A3C_AGENT_WORKER-Thread-14 INFO:Local step 219500, global step 3525139: loss 0.0194
[2019-04-04 02:37:22,287] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 219500, global step 3525139: learning rate 0.0005
[2019-04-04 02:37:22,363] A3C_AGENT_WORKER-Thread-10 INFO:Local step 220500, global step 3525169: loss 0.0325
[2019-04-04 02:37:22,376] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 220500, global step 3525176: learning rate 0.0005
[2019-04-04 02:37:24,325] A3C_AGENT_WORKER-Thread-20 INFO:Local step 220500, global step 3525974: loss 0.0241
[2019-04-04 02:37:24,326] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 220500, global step 3525974: learning rate 0.0005
[2019-04-04 02:37:24,810] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.7955218e-27 2.9729368e-22 1.1631851e-21 1.0000000e+00 3.4163740e-24
 9.5541077e-19 1.1713436e-19], sum to 1.0000
[2019-04-04 02:37:24,814] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1670
[2019-04-04 02:37:24,855] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.22872654354087, 0.3396518660427648, 0.0, 1.0, 39679.84434396987], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4842000.0000, 
sim time next is 4842600.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.21177243179394, 0.3356649869244632, 0.0, 1.0, 39398.95791775776], 
processed observation next is [0.0, 0.043478260869565216, 0.40720221606648205, 0.6, 0.0, 0.0, 0.6666666666666666, 0.6009810359828283, 0.611888328974821, 0.0, 1.0, 0.187614085322656], 
reward next is 0.8124, 
noisyNet noise sample is [array([1.3449764], dtype=float32), -0.41881064]. 
=============================================
[2019-04-04 02:37:25,243] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6028131e-29 4.3124148e-23 3.9507048e-22 1.0000000e+00 6.9367048e-26
 5.9336779e-21 2.7734189e-22], sum to 1.0000
[2019-04-04 02:37:25,243] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8920
[2019-04-04 02:37:25,273] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.433333333333334, 75.33333333333334, 0.0, 0.0, 26.0, 25.60144662784692, 0.4125903309388585, 0.0, 1.0, 18736.84590438283], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4317600.0000, 
sim time next is 4318200.0000, 
raw observation next is [4.45, 75.5, 0.0, 0.0, 26.0, 25.56163109771477, 0.4037847621380671, 0.0, 1.0, 34333.25269494455], 
processed observation next is [0.0, 1.0, 0.5858725761772854, 0.755, 0.0, 0.0, 0.6666666666666666, 0.6301359248095642, 0.634594920712689, 0.0, 1.0, 0.16349167949973598], 
reward next is 0.8365, 
noisyNet noise sample is [array([2.7854822], dtype=float32), 0.6061832]. 
=============================================
[2019-04-04 02:37:25,912] A3C_AGENT_WORKER-Thread-6 INFO:Local step 220500, global step 3526564: loss 0.0004
[2019-04-04 02:37:25,915] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 220500, global step 3526564: learning rate 0.0005
[2019-04-04 02:37:27,157] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.4684243e-33 9.3201855e-28 7.5196389e-28 1.0000000e+00 6.6451196e-30
 5.5720288e-28 2.0698207e-24], sum to 1.0000
[2019-04-04 02:37:27,158] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6772
[2019-04-04 02:37:27,170] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.666666666666667, 36.0, 41.16666666666666, 245.6666666666667, 26.0, 25.09531740736227, 0.3837087809781666, 0.0, 1.0, 42987.13950701823], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4814400.0000, 
sim time next is 4815000.0000, 
raw observation next is [2.5, 37.0, 33.0, 185.0, 26.0, 25.09671363217901, 0.3755903756108692, 0.0, 1.0, 28356.69955865268], 
processed observation next is [0.0, 0.7391304347826086, 0.5318559556786704, 0.37, 0.11, 0.20441988950276244, 0.6666666666666666, 0.5913928026815842, 0.6251967918702898, 0.0, 1.0, 0.13503190266025086], 
reward next is 0.8650, 
noisyNet noise sample is [array([-2.0169685], dtype=float32), -1.0933154]. 
=============================================
[2019-04-04 02:37:27,174] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[83.8161  ]
 [84.01024 ]
 [84.197205]
 [84.43735 ]
 [84.75885 ]], R is [[83.64476013]
 [83.60360718]
 [83.64323425]
 [83.71784973]
 [83.88066864]].
[2019-04-04 02:37:29,367] A3C_AGENT_WORKER-Thread-18 INFO:Local step 220500, global step 3528209: loss 0.0001
[2019-04-04 02:37:29,368] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 220500, global step 3528209: learning rate 0.0005
[2019-04-04 02:37:30,924] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.5327325e-20 5.2420199e-13 1.2351326e-14 1.0000000e+00 8.5360071e-14
 1.5171587e-08 2.0364409e-13], sum to 1.0000
[2019-04-04 02:37:30,924] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4056
[2019-04-04 02:37:30,944] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.2, 84.66666666666667, 157.5, 64.5, 26.0, 26.14637014588746, 0.5962257628801777, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4440000.0000, 
sim time next is 4440600.0000, 
raw observation next is [1.15, 85.0, 165.0, 31.0, 26.0, 26.22302632292364, 0.6091689232913413, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.49445983379501385, 0.85, 0.55, 0.03425414364640884, 0.6666666666666666, 0.6852521935769701, 0.7030563077637805, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6645567], dtype=float32), 0.060813956]. 
=============================================
[2019-04-04 02:37:31,057] A3C_AGENT_WORKER-Thread-4 INFO:Local step 219500, global step 3528995: loss 0.0222
[2019-04-04 02:37:31,061] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 219500, global step 3528995: learning rate 0.0005
[2019-04-04 02:37:31,331] A3C_AGENT_WORKER-Thread-3 INFO:Local step 220500, global step 3529127: loss 0.0202
[2019-04-04 02:37:31,336] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 220500, global step 3529128: learning rate 0.0005
[2019-04-04 02:37:31,870] A3C_AGENT_WORKER-Thread-16 INFO:Local step 220500, global step 3529353: loss 0.0039
[2019-04-04 02:37:31,872] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 220500, global step 3529354: learning rate 0.0005
[2019-04-04 02:37:33,781] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:37:33,781] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:37:34,003] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run27
[2019-04-04 02:37:36,032] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:37:36,032] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:37:36,054] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run27
[2019-04-04 02:37:36,187] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.3982111e-27 2.3245991e-23 5.0394738e-23 1.0000000e+00 2.7659885e-24
 1.3497610e-21 8.9888134e-21], sum to 1.0000
[2019-04-04 02:37:36,187] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8600
[2019-04-04 02:37:36,202] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.166666666666667, 60.0, 0.0, 0.0, 26.0, 25.15255109398932, 0.3134367924321422, 0.0, 1.0, 39145.32610692392], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4846200.0000, 
sim time next is 4846800.0000, 
raw observation next is [-2.333333333333333, 60.00000000000001, 0.0, 0.0, 26.0, 25.12879259066721, 0.3075227766291255, 0.0, 1.0, 39139.98105264414], 
processed observation next is [0.0, 0.08695652173913043, 0.3979686057248385, 0.6000000000000001, 0.0, 0.0, 0.6666666666666666, 0.5940660492222675, 0.6025075922097085, 0.0, 1.0, 0.1863808621554483], 
reward next is 0.8136, 
noisyNet noise sample is [array([-0.99263036], dtype=float32), 1.1716801]. 
=============================================
[2019-04-04 02:37:36,784] A3C_AGENT_WORKER-Thread-12 INFO:Local step 220500, global step 3531443: loss 0.0146
[2019-04-04 02:37:36,786] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 220500, global step 3531443: learning rate 0.0005
[2019-04-04 02:37:37,571] A3C_AGENT_WORKER-Thread-14 INFO:Local step 220000, global step 3531824: loss 0.7686
[2019-04-04 02:37:37,571] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 220000, global step 3531824: learning rate 0.0005
[2019-04-04 02:37:37,857] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:37:37,857] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:37:37,860] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run27
[2019-04-04 02:37:37,891] A3C_AGENT_WORKER-Thread-13 INFO:Local step 219500, global step 3531939: loss 0.0564
[2019-04-04 02:37:37,898] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 219500, global step 3531941: learning rate 0.0005
[2019-04-04 02:37:38,269] A3C_AGENT_WORKER-Thread-5 INFO:Local step 220500, global step 3532076: loss 0.0001
[2019-04-04 02:37:38,278] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 220500, global step 3532076: learning rate 0.0005
[2019-04-04 02:37:39,209] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:37:39,209] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:37:39,212] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run27
[2019-04-04 02:37:40,403] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.2143075e-19 2.8935313e-14 5.6268754e-15 1.0000000e+00 4.2706398e-14
 1.3423013e-12 1.5607890e-13], sum to 1.0000
[2019-04-04 02:37:40,404] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9236
[2019-04-04 02:37:40,452] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.916666666666667, 65.0, 137.0, 0.0, 24.0, 23.6094643117688, -0.1511648674112146, 1.0, 1.0, 41518.0588801109], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 216600.0000, 
sim time next is 217200.0000, 
raw observation next is [-4.833333333333334, 65.0, 133.0, 0.0, 24.0, 23.67214004899122, -0.1488245919796751, 1.0, 1.0, 34046.94822177899], 
processed observation next is [1.0, 0.5217391304347826, 0.32871652816251157, 0.65, 0.44333333333333336, 0.0, 0.5, 0.47267833741593507, 0.45039180267344164, 1.0, 1.0, 0.16212832486561424], 
reward next is 0.8379, 
noisyNet noise sample is [array([-0.46580526], dtype=float32), 0.33921278]. 
=============================================
[2019-04-04 02:37:42,070] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.6721445e-23 9.7998885e-16 6.7086833e-18 1.0000000e+00 3.9984283e-17
 4.1850178e-15 3.2142128e-17], sum to 1.0000
[2019-04-04 02:37:42,070] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4744
[2019-04-04 02:37:42,094] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.6, 47.5, 40.0, 145.0, 26.0, 27.31586356595657, 0.774816066636303, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4642200.0000, 
sim time next is 4642800.0000, 
raw observation next is [4.4, 48.0, 33.33333333333334, 120.8333333333333, 26.0, 26.96479616778463, 0.6298779094461623, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.5844875346260389, 0.48, 0.11111111111111115, 0.1335174953959484, 0.6666666666666666, 0.7470663473153859, 0.7099593031487208, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.510841], dtype=float32), -1.3266703]. 
=============================================
[2019-04-04 02:37:42,699] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:37:42,699] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:37:42,726] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run27
[2019-04-04 02:37:46,089] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:37:46,089] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:37:46,093] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run27
[2019-04-04 02:37:47,785] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:37:47,785] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:37:47,788] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run27
[2019-04-04 02:37:48,374] A3C_AGENT_WORKER-Thread-4 INFO:Local step 220000, global step 3534820: loss 0.2203
[2019-04-04 02:37:48,425] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 220000, global step 3534820: learning rate 0.0005
[2019-04-04 02:37:53,344] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.9111033e-19 8.9314918e-12 1.1886973e-12 9.9999917e-01 8.3380094e-14
 7.7743459e-07 6.1538760e-12], sum to 1.0000
[2019-04-04 02:37:53,344] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2183
[2019-04-04 02:37:53,383] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.783333333333333, 95.33333333333333, 0.0, 0.0, 24.0, 23.24553625961701, -0.1544769185563035, 0.0, 1.0, 44234.93465943862], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 508200.0000, 
sim time next is 508800.0000, 
raw observation next is [1.966666666666667, 94.66666666666666, 0.0, 0.0, 24.0, 23.27046482323284, -0.1506567903225906, 0.0, 1.0, 43685.30579630787], 
processed observation next is [1.0, 0.9130434782608695, 0.5170821791320407, 0.9466666666666665, 0.0, 0.0, 0.5, 0.4392054019360699, 0.4497810698924698, 0.0, 1.0, 0.20802526569670413], 
reward next is 0.7920, 
noisyNet noise sample is [array([-1.2854636], dtype=float32), -0.8503262]. 
=============================================
[2019-04-04 02:37:56,575] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.0219887e-24 2.7597772e-19 9.5591440e-18 1.0000000e+00 8.6001286e-19
 5.7810465e-16 5.4494652e-17], sum to 1.0000
[2019-04-04 02:37:56,575] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6514
[2019-04-04 02:37:56,591] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.183333333333333, 87.5, 0.0, 0.0, 24.0, 23.30272402843773, -0.1287313730092668, 0.0, 1.0, 41708.56725784694], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 539400.0000, 
sim time next is 540000.0000, 
raw observation next is [1.1, 88.0, 0.0, 0.0, 24.0, 23.29297337977272, -0.1338209118220327, 0.0, 1.0, 41706.43160863993], 
processed observation next is [0.0, 0.2608695652173913, 0.49307479224376743, 0.88, 0.0, 0.0, 0.5, 0.4410811149810601, 0.4553930293926558, 0.0, 1.0, 0.19860205527923774], 
reward next is 0.8014, 
noisyNet noise sample is [array([-0.6102937], dtype=float32), -0.87371606]. 
=============================================
[2019-04-04 02:37:56,622] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[87.87456 ]
 [87.89715 ]
 [87.88579 ]
 [87.870605]
 [87.86127 ]], R is [[87.78714752]
 [87.71066284]
 [87.6348877 ]
 [87.55995178]
 [87.48594666]].
[2019-04-04 02:37:58,181] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:37:58,181] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:37:58,213] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run27
[2019-04-04 02:37:59,913] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:37:59,913] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:37:59,916] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run27
[2019-04-04 02:38:03,092] A3C_AGENT_WORKER-Thread-13 INFO:Local step 220000, global step 3537891: loss 0.6691
[2019-04-04 02:38:03,092] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 220000, global step 3537891: learning rate 0.0005
[2019-04-04 02:38:04,784] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.6076017e-18 5.6176392e-09 1.2442334e-11 9.9999833e-01 5.5744748e-13
 1.6718859e-06 3.5741993e-11], sum to 1.0000
[2019-04-04 02:38:04,784] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1534
[2019-04-04 02:38:04,873] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 61.0, 0.0, 0.0, 26.0, 25.41118826640461, 0.4726980087305113, 0.0, 1.0, 23053.14680048543], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4577400.0000, 
sim time next is 4578000.0000, 
raw observation next is [1.0, 61.0, 0.0, 0.0, 26.0, 25.41861357830286, 0.4718047663010452, 0.0, 1.0, 26074.36533051846], 
processed observation next is [1.0, 1.0, 0.4903047091412743, 0.61, 0.0, 0.0, 0.6666666666666666, 0.618217798191905, 0.6572682554336817, 0.0, 1.0, 0.12416364443104029], 
reward next is 0.8758, 
noisyNet noise sample is [array([-1.2511114], dtype=float32), 0.84718114]. 
=============================================
[2019-04-04 02:38:04,900] A3C_AGENT_WORKER-Thread-14 INFO:Local step 220500, global step 3538280: loss 0.0413
[2019-04-04 02:38:04,903] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 220500, global step 3538280: learning rate 0.0005
[2019-04-04 02:38:04,905] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[80.07652 ]
 [79.906784]
 [79.933   ]
 [80.05642 ]
 [79.95967 ]], R is [[80.33090973]
 [80.41782379]
 [80.49196625]
 [80.49107361]
 [80.45697784]].
[2019-04-04 02:38:05,831] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.7476611e-22 4.0935274e-18 1.1586128e-17 1.0000000e+00 3.2646408e-18
 7.0959092e-15 6.0571448e-16], sum to 1.0000
[2019-04-04 02:38:05,831] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3961
[2019-04-04 02:38:05,905] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.3, 75.0, 0.0, 0.0, 24.0, 22.9583343337875, -0.2259615967594983, 0.0, 1.0, 44412.7677521716], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 618000.0000, 
sim time next is 618600.0000, 
raw observation next is [-4.399999999999999, 75.0, 0.0, 0.0, 24.0, 22.9133278782713, -0.2339155816501013, 0.0, 1.0, 44622.56955137803], 
processed observation next is [0.0, 0.13043478260869565, 0.3407202216066483, 0.75, 0.0, 0.0, 0.5, 0.40944398985594166, 0.42202813944996626, 0.0, 1.0, 0.21248842643513347], 
reward next is 0.7875, 
noisyNet noise sample is [array([1.0917118], dtype=float32), 0.13109162]. 
=============================================
[2019-04-04 02:38:20,553] A3C_AGENT_WORKER-Thread-4 INFO:Local step 220500, global step 3541672: loss 0.0263
[2019-04-04 02:38:20,564] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 220500, global step 3541676: learning rate 0.0005
[2019-04-04 02:38:28,335] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.6897503e-23 1.4265695e-18 3.5898212e-16 1.0000000e+00 2.6441672e-20
 2.0206384e-14 2.2459042e-16], sum to 1.0000
[2019-04-04 02:38:28,335] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3921
[2019-04-04 02:38:28,448] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-15.8, 74.66666666666667, 0.0, 0.0, 26.0, 22.0495440298199, -0.4265253039723342, 0.0, 1.0, 48737.75857606447], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 364800.0000, 
sim time next is 365400.0000, 
raw observation next is [-15.9, 75.5, 0.0, 0.0, 26.0, 21.95866980061093, -0.4343190619095916, 0.0, 1.0, 48719.47563757372], 
processed observation next is [1.0, 0.21739130434782608, 0.02216066481994457, 0.755, 0.0, 0.0, 0.6666666666666666, 0.3298891500509109, 0.3552269793634695, 0.0, 1.0, 0.23199750303606534], 
reward next is 0.7680, 
noisyNet noise sample is [array([-0.42905205], dtype=float32), 1.1381717]. 
=============================================
[2019-04-04 02:38:28,602] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.6601584e-21 7.2345660e-15 8.0399207e-15 1.0000000e+00 2.8274688e-17
 5.6292621e-10 2.6342484e-14], sum to 1.0000
[2019-04-04 02:38:28,602] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1731
[2019-04-04 02:38:28,619] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.3, 72.33333333333333, 0.0, 0.0, 26.0, 23.76879741390125, 0.002620071122225435, 0.0, 1.0, 41579.49139404023], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 794400.0000, 
sim time next is 795000.0000, 
raw observation next is [-7.299999999999999, 71.66666666666667, 0.0, 0.0, 26.0, 23.82043047386867, -0.002503600415062784, 0.0, 1.0, 41633.23690591611], 
processed observation next is [1.0, 0.17391304347826086, 0.2603878116343491, 0.7166666666666667, 0.0, 0.0, 0.6666666666666666, 0.4850358728223891, 0.49916546652831245, 0.0, 1.0, 0.19825350907579098], 
reward next is 0.8017, 
noisyNet noise sample is [array([-0.08561574], dtype=float32), -2.124557]. 
=============================================
[2019-04-04 02:38:28,624] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[81.0067 ]
 [81.06058]
 [81.11421]
 [81.15883]
 [81.1496 ]], R is [[80.91548157]
 [80.90833282]
 [80.90161133]
 [80.8953476 ]
 [80.88947296]].
[2019-04-04 02:38:31,577] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:38:31,578] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:38:31,586] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run27
[2019-04-04 02:38:32,161] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0238499e-17 5.2841515e-13 5.3278944e-12 1.0000000e+00 1.2142825e-14
 1.8995192e-09 3.8060696e-12], sum to 1.0000
[2019-04-04 02:38:32,161] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2412
[2019-04-04 02:38:32,213] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-15.0, 69.0, 0.0, 0.0, 25.0, 22.27178775523015, -0.3890697431735888, 0.0, 1.0, 49601.73737351691], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 355200.0000, 
sim time next is 355800.0000, 
raw observation next is [-15.0, 69.0, 0.0, 0.0, 25.0, 22.16986666848805, -0.4090495010267994, 0.0, 1.0, 49698.65828535778], 
processed observation next is [1.0, 0.08695652173913043, 0.04709141274238226, 0.69, 0.0, 0.0, 0.5833333333333334, 0.34748888904067093, 0.3636501663244002, 0.0, 1.0, 0.23666027754932276], 
reward next is 0.7633, 
noisyNet noise sample is [array([-0.6200043], dtype=float32), -0.4916986]. 
=============================================
[2019-04-04 02:38:32,616] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.5261028e-25 4.7053146e-19 9.1194414e-20 1.0000000e+00 2.3875465e-22
 1.9196591e-18 5.0834674e-19], sum to 1.0000
[2019-04-04 02:38:32,616] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2006
[2019-04-04 02:38:32,679] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.333333333333333, 56.66666666666667, 0.0, 0.0, 26.0, 25.40198899084185, 0.3915288901670986, 0.0, 1.0, 25169.14894557757], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4836000.0000, 
sim time next is 4836600.0000, 
raw observation next is [-1.5, 57.5, 0.0, 0.0, 26.0, 25.44705196920454, 0.383801225065397, 0.0, 1.0, 18761.95932114371], 
processed observation next is [0.0, 1.0, 0.4210526315789474, 0.575, 0.0, 0.0, 0.6666666666666666, 0.6205876641003784, 0.6279337416884657, 0.0, 1.0, 0.08934266343401766], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.8178543], dtype=float32), 0.622322]. 
=============================================
[2019-04-04 02:38:34,697] A3C_AGENT_WORKER-Thread-13 INFO:Local step 220500, global step 3544611: loss 0.0328
[2019-04-04 02:38:34,705] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 220500, global step 3544611: learning rate 0.0005
[2019-04-04 02:38:44,544] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:38:44,561] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:38:44,564] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run27
[2019-04-04 02:38:58,175] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:38:58,175] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:38:58,179] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run27
[2019-04-04 02:39:03,113] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.2555198e-24 2.3391060e-09 1.8925146e-20 1.0000000e+00 2.6529829e-16
 3.6240412e-08 1.8806572e-17], sum to 1.0000
[2019-04-04 02:39:03,125] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1743
[2019-04-04 02:39:03,201] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [18.3, 65.0, 128.0, 0.0, 26.0, 25.0808045602352, 0.5168279620733016, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1175400.0000, 
sim time next is 1176000.0000, 
raw observation next is [18.3, 65.0, 120.0, 0.0, 26.0, 25.07540869063798, 0.5136179377607628, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.9695290858725764, 0.65, 0.4, 0.0, 0.6666666666666666, 0.5896173908864982, 0.6712059792535876, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.72913], dtype=float32), 0.98410714]. 
=============================================
[2019-04-04 02:39:03,228] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[79.83225 ]
 [79.899826]
 [79.95127 ]
 [79.99951 ]
 [80.049484]], R is [[79.88028717]
 [80.08148193]
 [80.28067017]
 [80.47786713]
 [80.67308807]].
[2019-04-04 02:39:04,977] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3214676e-24 1.0166300e-18 1.0104356e-19 1.0000000e+00 2.2627449e-21
 3.5107337e-15 1.6414024e-17], sum to 1.0000
[2019-04-04 02:39:04,977] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9356
[2019-04-04 02:39:05,022] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.9, 71.0, 0.0, 0.0, 26.0, 24.42814295183594, 0.1042373419969009, 0.0, 1.0, 41145.2854988681], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 691200.0000, 
sim time next is 691800.0000, 
raw observation next is [-3.816666666666666, 71.16666666666667, 0.0, 0.0, 26.0, 24.43354034619903, 0.1001105553439159, 0.0, 1.0, 41058.16128576428], 
processed observation next is [1.0, 0.0, 0.3568790397045245, 0.7116666666666667, 0.0, 0.0, 0.6666666666666666, 0.5361283621832525, 0.5333701851146386, 0.0, 1.0, 0.19551505374173467], 
reward next is 0.8045, 
noisyNet noise sample is [array([-0.6440151], dtype=float32), 1.554533]. 
=============================================
[2019-04-04 02:39:08,242] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.9096351e-25 3.8231626e-11 5.1591156e-17 9.9999619e-01 8.8000588e-16
 3.8691242e-06 7.3524081e-18], sum to 1.0000
[2019-04-04 02:39:08,242] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9101
[2019-04-04 02:39:08,292] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [14.4, 98.66666666666667, 92.16666666666667, 0.0, 26.0, 24.96458634217028, 0.4791318604884292, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1250400.0000, 
sim time next is 1251000.0000, 
raw observation next is [14.4, 98.0, 95.0, 0.0, 26.0, 25.00403679967106, 0.4857295374495447, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.8614958448753465, 0.98, 0.31666666666666665, 0.0, 0.6666666666666666, 0.5836697333059216, 0.6619098458165149, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.43801233], dtype=float32), -0.42670262]. 
=============================================
[2019-04-04 02:39:08,353] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[97.03598 ]
 [97.2052  ]
 [97.51259 ]
 [97.921906]
 [98.709785]], R is [[97.68432617]
 [97.70748138]
 [97.73040771]
 [97.75310516]
 [97.77557373]].
[2019-04-04 02:39:16,745] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.4611692e-17 5.4088700e-10 1.5366050e-10 9.9893230e-01 1.5666323e-12
 1.0677065e-03 1.0915709e-11], sum to 1.0000
[2019-04-04 02:39:16,745] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8581
[2019-04-04 02:39:16,889] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.3, 68.0, 0.0, 0.0, 26.0, 22.57092802885643, -0.1496650437123616, 1.0, 1.0, 203465.2837099041], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 113400.0000, 
sim time next is 114000.0000, 
raw observation next is [-7.3, 68.0, 0.0, 0.0, 26.0, 23.29002661861887, -0.05890047096849887, 0.0, 1.0, 161581.7417832723], 
processed observation next is [1.0, 0.30434782608695654, 0.26038781163434904, 0.68, 0.0, 0.0, 0.6666666666666666, 0.44083555155157256, 0.48036650967716704, 0.0, 1.0, 0.76943686563463], 
reward next is 0.2306, 
noisyNet noise sample is [array([-0.71420145], dtype=float32), 0.04738298]. 
=============================================
[2019-04-04 02:39:16,899] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[80.93366 ]
 [79.98351 ]
 [79.181145]
 [79.19435 ]
 [79.294304]], R is [[79.74761963]
 [78.98126221]
 [78.22806549]
 [78.22441101]
 [78.22102356]].
[2019-04-04 02:39:18,523] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.1743726e-17 4.1630536e-11 1.5637706e-11 9.9973315e-01 4.5264049e-11
 2.6686763e-04 2.8512526e-11], sum to 1.0000
[2019-04-04 02:39:18,523] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6155
[2019-04-04 02:39:18,604] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.8, 76.0, 141.6666666666667, 24.0, 26.0, 25.19677130643028, 0.2093889792720101, 1.0, 1.0, 43963.72231267995], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 123000.0000, 
sim time next is 123600.0000, 
raw observation next is [-7.8, 78.0, 165.8333333333333, 30.0, 26.0, 25.17156892277496, 0.2215114934027665, 1.0, 1.0, 58899.80021983662], 
processed observation next is [1.0, 0.43478260869565216, 0.24653739612188366, 0.78, 0.5527777777777776, 0.03314917127071823, 0.6666666666666666, 0.5976307435645799, 0.5738371644675888, 1.0, 1.0, 0.28047523914207917], 
reward next is 0.7195, 
noisyNet noise sample is [array([0.17596452], dtype=float32), 0.3206954]. 
=============================================
[2019-04-04 02:39:27,209] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.2878256e-15 3.6078077e-06 8.8962423e-08 2.5759375e-01 2.6038999e-10
 7.4240261e-01 8.4334228e-09], sum to 1.0000
[2019-04-04 02:39:27,209] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1137
[2019-04-04 02:39:27,246] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.633333333333333, 79.33333333333334, 0.0, 0.0, 26.0, 24.77034583547693, 0.2303397773685719, 0.0, 1.0, 40737.08897468485], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 861600.0000, 
sim time next is 862200.0000, 
raw observation next is [-2.55, 79.5, 0.0, 0.0, 26.0, 24.73861699103662, 0.2250545711827045, 0.0, 1.0, 40604.91907682428], 
processed observation next is [1.0, 1.0, 0.3919667590027701, 0.795, 0.0, 0.0, 0.6666666666666666, 0.5615514159197182, 0.5750181903942349, 0.0, 1.0, 0.19335675750868703], 
reward next is 0.8066, 
noisyNet noise sample is [array([0.27901533], dtype=float32), -0.0052404134]. 
=============================================
[2019-04-04 02:39:29,606] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.9954137e-19 2.8759017e-08 1.1232785e-11 6.8643376e-02 3.2072353e-12
 9.3135661e-01 1.2412566e-11], sum to 1.0000
[2019-04-04 02:39:29,630] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0570
[2019-04-04 02:39:29,653] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 26.0, 25.5928960301391, 0.5152483147736237, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1471800.0000, 
sim time next is 1472400.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 26.0, 25.67388392618349, 0.4888402555353044, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.5069252077562327, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6394903271819574, 0.6629467518451014, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.01973184], dtype=float32), 0.34030783]. 
=============================================
[2019-04-04 02:39:40,269] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.2492883e-18 4.8760516e-09 4.8011070e-12 9.4160634e-01 4.3259121e-09
 5.8393646e-02 1.4384749e-12], sum to 1.0000
[2019-04-04 02:39:40,270] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1335
[2019-04-04 02:39:40,340] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [9.600000000000001, 92.66666666666667, 22.5, 0.0, 26.0, 25.331148979639, 0.4823081669619119, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 980400.0000, 
sim time next is 981000.0000, 
raw observation next is [9.7, 92.5, 27.0, 0.0, 26.0, 25.70636050431543, 0.5032997620138621, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.7313019390581719, 0.925, 0.09, 0.0, 0.6666666666666666, 0.6421967086929525, 0.667766587337954, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4736613], dtype=float32), 2.4371495]. 
=============================================
[2019-04-04 02:39:40,357] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[96.49219 ]
 [96.42705 ]
 [96.336784]
 [96.19175 ]
 [95.89232 ]], R is [[96.4304657 ]
 [96.46616364]
 [96.50150299]
 [96.53649139]
 [96.57112885]].
[2019-04-04 02:40:01,519] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.9337660e-23 1.1157988e-13 3.1590636e-16 1.0000000e+00 3.6420431e-16
 4.0632009e-09 9.5495269e-17], sum to 1.0000
[2019-04-04 02:40:01,520] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1073
[2019-04-04 02:40:01,531] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [17.93333333333333, 65.66666666666666, 135.0, 0.0, 26.0, 25.30029113126204, 0.5280381701109369, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1161600.0000, 
sim time next is 1162200.0000, 
raw observation next is [18.11666666666667, 65.33333333333334, 140.0, 0.0, 26.0, 25.29617634648315, 0.5251032899762803, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.9644506001846724, 0.6533333333333334, 0.4666666666666667, 0.0, 0.6666666666666666, 0.6080146955402626, 0.6750344299920935, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.44414523], dtype=float32), -0.13023898]. 
=============================================
[2019-04-04 02:40:22,793] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.2928171e-13 6.1947358e-05 2.5752952e-08 4.8882710e-03 4.6075397e-06
 9.9504519e-01 5.2326881e-09], sum to 1.0000
[2019-04-04 02:40:22,793] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0329
[2019-04-04 02:40:22,845] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 90.0, 0.0, 0.0, 26.0, 25.77673882823092, 0.5408545192357638, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1449000.0000, 
sim time next is 1449600.0000, 
raw observation next is [1.1, 90.66666666666666, 0.0, 0.0, 26.0, 25.69996341650658, 0.5323082710360377, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.49307479224376743, 0.9066666666666666, 0.0, 0.0, 0.6666666666666666, 0.6416636180422151, 0.6774360903453459, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.02886236], dtype=float32), -0.8778716]. 
=============================================
[2019-04-04 02:40:33,464] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.78140835e-22 1.15770319e-16 1.06778409e-16 1.00000000e+00
 1.08271314e-18 3.48592716e-14 1.06315876e-16], sum to 1.0000
[2019-04-04 02:40:33,464] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6870
[2019-04-04 02:40:33,489] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.03333333333333333, 91.0, 0.0, 0.0, 26.0, 25.25756619028772, 0.4284939753996481, 0.0, 1.0, 42983.10196193241], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1738200.0000, 
sim time next is 1738800.0000, 
raw observation next is [0.0, 91.0, 0.0, 0.0, 26.0, 25.23762163042428, 0.4242110329052691, 0.0, 1.0, 43014.52344647329], 
processed observation next is [0.0, 0.13043478260869565, 0.46260387811634357, 0.91, 0.0, 0.0, 0.6666666666666666, 0.6031351358686899, 0.6414036776350897, 0.0, 1.0, 0.2048310640308252], 
reward next is 0.7952, 
noisyNet noise sample is [array([-0.09036296], dtype=float32), 0.82679975]. 
=============================================
[2019-04-04 02:40:38,869] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.5550965e-19 1.2275451e-09 4.1317603e-14 6.8147119e-09 5.8974639e-12
 1.0000000e+00 1.2280063e-13], sum to 1.0000
[2019-04-04 02:40:38,870] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2347
[2019-04-04 02:40:38,898] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.3333333333333333, 93.0, 0.0, 0.0, 26.0, 25.35560594955568, 0.4775909736853235, 0.0, 1.0, 43821.89293866869], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1726800.0000, 
sim time next is 1727400.0000, 
raw observation next is [0.4166666666666667, 92.5, 0.0, 0.0, 26.0, 25.35263943843519, 0.4754702314452099, 0.0, 1.0, 43416.31281429779], 
processed observation next is [1.0, 1.0, 0.47414589104339805, 0.925, 0.0, 0.0, 0.6666666666666666, 0.6127199532029325, 0.6584900771484034, 0.0, 1.0, 0.20674434673475137], 
reward next is 0.7933, 
noisyNet noise sample is [array([-0.3535767], dtype=float32), -0.2256247]. 
=============================================
[2019-04-04 02:40:40,291] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.6536893e-16 3.4658488e-08 4.6930465e-10 1.3379566e-03 6.4991276e-05
 9.9859697e-01 1.9842343e-09], sum to 1.0000
[2019-04-04 02:40:40,291] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0122
[2019-04-04 02:40:40,365] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.58333333333333, 80.5, 0.0, 0.0, 26.0, 25.75266702578879, 0.5775211840608523, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1014600.0000, 
sim time next is 1015200.0000, 
raw observation next is [14.4, 81.0, 0.0, 0.0, 26.0, 25.960873729725, 0.5830978297055147, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.8614958448753465, 0.81, 0.0, 0.0, 0.6666666666666666, 0.6634061441437501, 0.6943659432351715, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.35134104], dtype=float32), 0.4993729]. 
=============================================
[2019-04-04 02:40:42,745] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.5198307e-17 8.4759023e-08 4.6274599e-12 2.8655252e-06 1.5584077e-06
 9.9999547e-01 9.4470466e-11], sum to 1.0000
[2019-04-04 02:40:42,745] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5991
[2019-04-04 02:40:42,771] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.38333333333333, 53.5, 33.66666666666667, 24.66666666666667, 26.0, 26.99902482591621, 0.7643117300680381, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1615800.0000, 
sim time next is 1616400.0000, 
raw observation next is [12.2, 54.0, 25.5, 18.5, 26.0, 26.08142380159081, 0.7127916025607121, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.8005540166204987, 0.54, 0.085, 0.020441988950276244, 0.6666666666666666, 0.6734519834659007, 0.7375972008535707, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7038043], dtype=float32), 0.28971487]. 
=============================================
[2019-04-04 02:40:47,558] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.8708227e-16 2.6742029e-09 2.6280432e-12 2.5431191e-08 1.5679241e-08
 1.0000000e+00 3.8512523e-11], sum to 1.0000
[2019-04-04 02:40:47,558] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9095
[2019-04-04 02:40:47,565] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 84.0, 95.0, 0.0, 26.0, 25.86083958820203, 0.5223189093828844, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1684800.0000, 
sim time next is 1685400.0000, 
raw observation next is [1.1, 84.66666666666667, 99.0, 0.0, 26.0, 25.85287575120292, 0.5213801004790602, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.49307479224376743, 0.8466666666666667, 0.33, 0.0, 0.6666666666666666, 0.6544063126002433, 0.6737933668263535, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5057262], dtype=float32), 0.010288078]. 
=============================================
[2019-04-04 02:40:47,679] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.85702035e-19 1.16031208e-13 1.05523815e-12 9.99941468e-01
 1.09434895e-14 5.85134840e-05 1.03784830e-12], sum to 1.0000
[2019-04-04 02:40:47,680] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0723
[2019-04-04 02:40:47,739] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.716666666666667, 64.5, 133.0, 420.0, 26.0, 25.04582667897824, 0.2871926674837844, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2369400.0000, 
sim time next is 2370000.0000, 
raw observation next is [-2.633333333333333, 64.0, 136.0, 435.0, 26.0, 24.97762152193805, 0.2825334484976449, 0.0, 1.0, 44914.93223125712], 
processed observation next is [0.0, 0.43478260869565216, 0.38965835641735924, 0.64, 0.4533333333333333, 0.48066298342541436, 0.6666666666666666, 0.5814684601615042, 0.5941778161658816, 0.0, 1.0, 0.21388062967265295], 
reward next is 0.7861, 
noisyNet noise sample is [array([-0.5038616], dtype=float32), -0.9933325]. 
=============================================
[2019-04-04 02:40:47,748] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[79.819374]
 [79.75652 ]
 [79.72819 ]
 [79.681725]
 [79.87437 ]], R is [[79.83325195]
 [80.03491974]
 [80.23457336]
 [80.43222809]
 [80.6279068 ]].
[2019-04-04 02:40:51,141] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.6278559e-21 4.5576148e-16 6.1513576e-16 1.0000000e+00 5.4766586e-18
 1.4474619e-12 2.6510418e-15], sum to 1.0000
[2019-04-04 02:40:51,143] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5422
[2019-04-04 02:40:51,188] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.15, 44.5, 0.0, 0.0, 26.0, 24.95563323230313, 0.2655010926373467, 0.0, 1.0, 44683.35829419918], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2395800.0000, 
sim time next is 2396400.0000, 
raw observation next is [-1.333333333333333, 44.33333333333334, 0.0, 0.0, 26.0, 24.94738425245822, 0.2640558842121577, 0.0, 1.0, 48918.75707752301], 
processed observation next is [0.0, 0.7391304347826086, 0.42566943674976926, 0.4433333333333334, 0.0, 0.0, 0.6666666666666666, 0.5789486877048517, 0.5880186280707193, 0.0, 1.0, 0.2329464622739191], 
reward next is 0.7671, 
noisyNet noise sample is [array([-0.91724306], dtype=float32), -1.3784082]. 
=============================================
[2019-04-04 02:40:51,189] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.7558193e-21 3.1224620e-16 4.1469703e-16 1.0000000e+00 3.3904833e-18
 9.2299985e-13 1.8289963e-15], sum to 1.0000
[2019-04-04 02:40:51,190] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8744
[2019-04-04 02:40:51,247] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.333333333333333, 44.33333333333334, 0.0, 0.0, 26.0, 24.94738425245822, 0.2640558842121577, 0.0, 1.0, 48918.75707752301], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2396400.0000, 
sim time next is 2397000.0000, 
raw observation next is [-1.516666666666667, 44.16666666666667, 0.0, 0.0, 26.0, 24.94307891637311, 0.2640328841449751, 0.0, 1.0, 49319.78428762456], 
processed observation next is [0.0, 0.7391304347826086, 0.4205909510618652, 0.4416666666666667, 0.0, 0.0, 0.6666666666666666, 0.5785899096977593, 0.5880109613816583, 0.0, 1.0, 0.23485611565535505], 
reward next is 0.7651, 
noisyNet noise sample is [array([-0.91724306], dtype=float32), -1.3784082]. 
=============================================
[2019-04-04 02:40:51,252] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[76.829506]
 [76.47146 ]
 [76.82673 ]
 [76.97397 ]
 [77.21952 ]], R is [[77.07753754]
 [77.07381439]
 [77.09030151]
 [77.12853241]
 [77.18667603]].
[2019-04-04 02:40:55,059] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.3955595e-16 9.8582611e-08 2.7277454e-09 7.7101235e-05 1.9889493e-10
 9.9992275e-01 3.1974809e-09], sum to 1.0000
[2019-04-04 02:40:55,059] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1763
[2019-04-04 02:40:55,075] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.8, 56.5, 0.0, 0.0, 26.0, 25.35135706094108, 0.3796834254738884, 0.0, 1.0, 45747.74925180382], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2326200.0000, 
sim time next is 2326800.0000, 
raw observation next is [-1.9, 57.0, 0.0, 0.0, 26.0, 25.36943798808891, 0.3739087852438949, 0.0, 1.0, 41113.13175878854], 
processed observation next is [1.0, 0.9565217391304348, 0.4099722991689751, 0.57, 0.0, 0.0, 0.6666666666666666, 0.6141198323407426, 0.6246362617479649, 0.0, 1.0, 0.19577681789899307], 
reward next is 0.8042, 
noisyNet noise sample is [array([0.3302651], dtype=float32), 1.5635916]. 
=============================================
[2019-04-04 02:41:02,028] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.4616095e-15 1.1840137e-08 2.0600126e-09 9.7092199e-01 8.9204716e-10
 2.9077936e-02 2.0170315e-08], sum to 1.0000
[2019-04-04 02:41:02,028] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9434
[2019-04-04 02:41:02,061] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 85.66666666666667, 0.0, 0.0, 26.0, 24.23849551902836, 0.08510259321464513, 0.0, 1.0, 41093.67785516562], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2004000.0000, 
sim time next is 2004600.0000, 
raw observation next is [-6.1, 86.33333333333333, 0.0, 0.0, 26.0, 24.22580018316366, 0.08146891819960299, 0.0, 1.0, 41130.87457610225], 
processed observation next is [1.0, 0.17391304347826086, 0.29362880886426596, 0.8633333333333333, 0.0, 0.0, 0.6666666666666666, 0.518816681930305, 0.5271563060665343, 0.0, 1.0, 0.19586130750524883], 
reward next is 0.8041, 
noisyNet noise sample is [array([-1.9794995], dtype=float32), 0.63915503]. 
=============================================
[2019-04-04 02:41:02,447] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.1449485e-17 2.9045183e-14 2.6517944e-12 1.0000000e+00 1.4706950e-14
 2.4639248e-09 3.0013481e-12], sum to 1.0000
[2019-04-04 02:41:02,447] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5534
[2019-04-04 02:41:02,523] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.2, 81.66666666666667, 0.0, 0.0, 26.0, 23.82333509639025, 0.02586389765830923, 0.0, 1.0, 46926.04200714485], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1830000.0000, 
sim time next is 1830600.0000, 
raw observation next is [-6.2, 81.0, 0.0, 0.0, 26.0, 23.78591437551242, 0.01796891468132976, 0.0, 1.0, 46956.442115335], 
processed observation next is [0.0, 0.17391304347826086, 0.2908587257617729, 0.81, 0.0, 0.0, 0.6666666666666666, 0.4821595312927016, 0.50598963822711, 0.0, 1.0, 0.22360210531111904], 
reward next is 0.7764, 
noisyNet noise sample is [array([1.457781], dtype=float32), 0.3889017]. 
=============================================
[2019-04-04 02:41:11,850] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.6320010e-17 1.9438899e-10 2.6167302e-12 8.1170112e-04 8.1399089e-12
 9.9918824e-01 4.0127272e-12], sum to 1.0000
[2019-04-04 02:41:11,850] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6214
[2019-04-04 02:41:11,894] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.75, 86.0, 0.0, 0.0, 26.0, 24.62422322999462, 0.2017798418069976, 0.0, 1.0, 42796.47359217433], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2082600.0000, 
sim time next is 2083200.0000, 
raw observation next is [-4.833333333333333, 86.0, 0.0, 0.0, 26.0, 24.5916155987748, 0.1884640058884551, 0.0, 1.0, 42890.92740315911], 
processed observation next is [1.0, 0.08695652173913043, 0.3287165281625116, 0.86, 0.0, 0.0, 0.6666666666666666, 0.5493012998979001, 0.5628213352961516, 0.0, 1.0, 0.20424251144361483], 
reward next is 0.7958, 
noisyNet noise sample is [array([-0.24939904], dtype=float32), 0.08897351]. 
=============================================
[2019-04-04 02:41:15,838] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.9286923e-18 1.0486522e-11 1.8967064e-11 7.2075130e-07 5.2702799e-12
 9.9999928e-01 1.6704983e-11], sum to 1.0000
[2019-04-04 02:41:15,839] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8025
[2019-04-04 02:41:15,878] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.9, 85.0, 0.0, 0.0, 26.0, 24.20784064111516, 0.08951192837986355, 0.0, 1.0, 41328.05201869242], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2003400.0000, 
sim time next is 2004000.0000, 
raw observation next is [-6.0, 85.66666666666667, 0.0, 0.0, 26.0, 24.23849213668756, 0.08510166606350601, 0.0, 1.0, 41093.68063499916], 
processed observation next is [1.0, 0.17391304347826086, 0.296398891966759, 0.8566666666666667, 0.0, 0.0, 0.6666666666666666, 0.5198743447239634, 0.5283672220211687, 0.0, 1.0, 0.195684193499996], 
reward next is 0.8043, 
noisyNet noise sample is [array([-1.1260444], dtype=float32), 0.35194093]. 
=============================================
[2019-04-04 02:41:15,889] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[83.41706 ]
 [83.46267 ]
 [83.446526]
 [83.43026 ]
 [83.39802 ]], R is [[83.32239532]
 [83.29237366]
 [83.26135254]
 [83.23215485]
 [83.20334625]].
[2019-04-04 02:41:30,272] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.3826281e-17 4.4255804e-11 1.7718509e-11 8.3512859e-06 3.0439345e-11
 9.9999166e-01 5.3568776e-13], sum to 1.0000
[2019-04-04 02:41:30,273] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5738
[2019-04-04 02:41:30,350] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.2333333333333333, 52.66666666666667, 0.0, 0.0, 26.0, 25.69609740362456, 0.2770991124369804, 1.0, 1.0, 18683.53432778014], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2655600.0000, 
sim time next is 2656200.0000, 
raw observation next is [-0.4166666666666667, 53.33333333333333, 0.0, 0.0, 26.0, 25.4685427036805, 0.3940800957500605, 1.0, 1.0, 18754.89036581739], 
processed observation next is [1.0, 0.7391304347826086, 0.45106186518928904, 0.5333333333333333, 0.0, 0.0, 0.6666666666666666, 0.6223785586400418, 0.6313600319166869, 1.0, 1.0, 0.08930900174198758], 
reward next is 0.9107, 
noisyNet noise sample is [array([2.0737877], dtype=float32), 0.5406337]. 
=============================================
[2019-04-04 02:41:35,543] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.2094233e-16 2.1308443e-07 5.5864330e-10 8.5277961e-08 6.8891992e-10
 9.9999964e-01 3.5436407e-09], sum to 1.0000
[2019-04-04 02:41:35,543] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5725
[2019-04-04 02:41:35,569] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.3, 82.00000000000001, 0.0, 0.0, 26.0, 24.73045167926265, 0.2553783929034274, 0.0, 1.0, 42245.76319122949], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2157000.0000, 
sim time next is 2157600.0000, 
raw observation next is [-7.300000000000001, 82.0, 0.0, 0.0, 26.0, 24.66333769539632, 0.2428538947554917, 0.0, 1.0, 42283.81198785691], 
processed observation next is [1.0, 1.0, 0.26038781163434904, 0.82, 0.0, 0.0, 0.6666666666666666, 0.5552781412830265, 0.5809512982518306, 0.0, 1.0, 0.2013514856564615], 
reward next is 0.7986, 
noisyNet noise sample is [array([-0.7897021], dtype=float32), 0.37275928]. 
=============================================
[2019-04-04 02:41:40,187] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.9151097e-18 3.5948058e-10 2.3260156e-10 8.4454392e-08 4.3240268e-12
 9.9999988e-01 1.9318246e-10], sum to 1.0000
[2019-04-04 02:41:40,187] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0623
[2019-04-04 02:41:40,210] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 96.5, 0.0, 0.0, 26.0, 24.94753350122225, 0.2604033661499748, 0.0, 1.0, 55783.49439981297], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2871000.0000, 
sim time next is 2871600.0000, 
raw observation next is [1.0, 97.66666666666666, 0.0, 0.0, 26.0, 24.99109252593626, 0.2439826527202012, 0.0, 1.0, 55925.53392204629], 
processed observation next is [1.0, 0.21739130434782608, 0.4903047091412743, 0.9766666666666666, 0.0, 0.0, 0.6666666666666666, 0.5825910438280216, 0.5813275509067337, 0.0, 1.0, 0.26631206629545856], 
reward next is 0.7337, 
noisyNet noise sample is [array([-0.6931012], dtype=float32), 1.0889521]. 
=============================================
[2019-04-04 02:41:43,294] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.5113083e-16 6.7748368e-10 8.6589197e-10 6.9310557e-04 3.0978366e-11
 9.9930692e-01 3.6786760e-10], sum to 1.0000
[2019-04-04 02:41:43,311] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3794
[2019-04-04 02:41:43,332] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.616666666666667, 77.5, 0.0, 0.0, 26.0, 24.20688901358365, 0.09417029258679847, 0.0, 1.0, 42085.52259789556], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2175000.0000, 
sim time next is 2175600.0000, 
raw observation next is [-6.533333333333334, 77.0, 0.0, 0.0, 26.0, 24.18219685502402, 0.08735614313904598, 0.0, 1.0, 42046.74188464144], 
processed observation next is [1.0, 0.17391304347826086, 0.28162511542012925, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5151830712520017, 0.5291187143796819, 0.0, 1.0, 0.20022258040305446], 
reward next is 0.7998, 
noisyNet noise sample is [array([-0.5714387], dtype=float32), -0.9153318]. 
=============================================
[2019-04-04 02:41:51,421] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.6472880e-18 3.4995376e-10 2.6181856e-13 8.7903594e-13 3.6717237e-11
 1.0000000e+00 1.1248199e-14], sum to 1.0000
[2019-04-04 02:41:51,422] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6235
[2019-04-04 02:41:51,522] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 48.5, 155.0, 206.0, 26.0, 25.98575121577579, 0.4780922152104881, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2647800.0000, 
sim time next is 2648400.0000, 
raw observation next is [0.5, 49.0, 141.6666666666667, 192.3333333333333, 26.0, 26.03911101521076, 0.4767429112234405, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.4764542936288089, 0.49, 0.4722222222222224, 0.21252302025782682, 0.6666666666666666, 0.6699259179342301, 0.6589143037411468, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.358152], dtype=float32), -0.13037677]. 
=============================================
[2019-04-04 02:41:57,174] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.31057147e-20 1.48065018e-14 1.27934704e-14 3.13764422e-12
 4.02548861e-16 1.00000000e+00 2.07520734e-14], sum to 1.0000
[2019-04-04 02:41:57,174] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4946
[2019-04-04 02:41:57,219] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.333333333333334, 70.16666666666667, 0.0, 0.0, 26.0, 24.53426089591495, 0.2118626484713798, 0.0, 1.0, 44390.80681279445], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2682600.0000, 
sim time next is 2683200.0000, 
raw observation next is [-9.666666666666668, 71.33333333333334, 0.0, 0.0, 26.0, 24.46508906989193, 0.1931210651321813, 0.0, 1.0, 44405.56008641183], 
processed observation next is [1.0, 0.043478260869565216, 0.19482917820867957, 0.7133333333333334, 0.0, 0.0, 0.6666666666666666, 0.5387574224909942, 0.5643736883773938, 0.0, 1.0, 0.21145504803053253], 
reward next is 0.7885, 
noisyNet noise sample is [array([-0.04404144], dtype=float32), -0.0041849134]. 
=============================================
[2019-04-04 02:42:08,806] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-04 02:42:08,817] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 02:42:08,817] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:42:08,819] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run37
[2019-04-04 02:42:08,912] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 02:42:08,912] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:42:08,948] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run37
[2019-04-04 02:42:09,054] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 02:42:09,055] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:42:09,057] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run37
[2019-04-04 02:42:56,218] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.8686178], dtype=float32), 0.1770091]
[2019-04-04 02:42:56,218] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-11.45, 54.0, 0.0, 0.0, 26.0, 25.11303465778985, 0.1617729407616668, 0.0, 1.0, 35006.41587847498]
[2019-04-04 02:42:56,218] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 02:42:56,219] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.1228244e-17 3.1019154e-13 9.3152428e-13 9.9999976e-01 6.3875226e-16
 2.9040018e-07 2.6719803e-12], sampled 0.49209421939562203
[2019-04-04 02:43:32,157] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.8686178], dtype=float32), 0.1770091]
[2019-04-04 02:43:32,158] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [6.6, 76.0, 0.0, 0.0, 26.0, 26.11657382565011, 0.6917170860132349, 0.0, 1.0, 0.0]
[2019-04-04 02:43:32,158] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 02:43:32,158] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.3642745e-18 2.9570665e-10 3.1566939e-12 8.4836756e-05 3.3912971e-13
 9.9991512e-01 1.9839080e-12], sampled 0.5952546035681265
[2019-04-04 02:44:41,856] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7354.8669 239637953.1352 1604.9215
[2019-04-04 02:45:04,047] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7242.3598 263264444.8073 1558.2911
[2019-04-04 02:45:05,353] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7183.2402 275679548.5252 1232.9356
[2019-04-04 02:45:06,376] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 3600000, evaluation results [3600000.0, 7242.359786631755, 263264444.80732435, 1558.2910726216553, 7354.866889832542, 239637953.1351703, 1604.9214805558893, 7183.24024511809, 275679548.5252053, 1232.9355526826014]
[2019-04-04 02:45:06,420] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.2045133e-18 1.9192743e-11 2.3653605e-13 5.6890249e-11 2.4041855e-14
 1.0000000e+00 5.0396053e-13], sum to 1.0000
[2019-04-04 02:45:06,421] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6283
[2019-04-04 02:45:06,445] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.99251595610334, 0.3176863320669576, 0.0, 1.0, 45611.98220890501], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2761200.0000, 
sim time next is 2761800.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.93971716833421, 0.3069990665087364, 0.0, 1.0, 45299.70198850909], 
processed observation next is [1.0, 1.0, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5783097640278507, 0.6023330221695787, 0.0, 1.0, 0.21571286661194805], 
reward next is 0.7843, 
noisyNet noise sample is [array([-0.03956252], dtype=float32), 0.8312781]. 
=============================================
[2019-04-04 02:45:06,994] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.2089730e-18 1.7321123e-13 3.7375307e-12 9.9998605e-01 1.9165997e-14
 1.3918926e-05 2.6492955e-12], sum to 1.0000
[2019-04-04 02:45:06,999] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1443
[2019-04-04 02:45:07,032] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.66026710535385, 0.2155341076580096, 0.0, 1.0, 39417.56756909029], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2343000.0000, 
sim time next is 2343600.0000, 
raw observation next is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.62172712597677, 0.2080294335926761, 0.0, 1.0, 39528.80085049793], 
processed observation next is [0.0, 0.13043478260869565, 0.3988919667590028, 0.62, 0.0, 0.0, 0.6666666666666666, 0.5518105938313974, 0.569343144530892, 0.0, 1.0, 0.1882323850023711], 
reward next is 0.8118, 
noisyNet noise sample is [array([-0.51249474], dtype=float32), 0.6000964]. 
=============================================
[2019-04-04 02:45:09,154] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.67362227e-17 1.01732858e-11 1.08642245e-11 9.27860441e-04
 3.42124809e-12 9.99072075e-01 6.78750597e-11], sum to 1.0000
[2019-04-04 02:45:09,155] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3547
[2019-04-04 02:45:09,179] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.7, 43.0, 0.0, 0.0, 26.0, 24.94955266591035, 0.1792612685240754, 0.0, 1.0, 38638.72152500811], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2515800.0000, 
sim time next is 2516400.0000, 
raw observation next is [-1.7, 44.0, 0.0, 0.0, 26.0, 24.92659360843432, 0.1770075133453699, 0.0, 1.0, 38625.29216684512], 
processed observation next is [1.0, 0.13043478260869565, 0.4155124653739613, 0.44, 0.0, 0.0, 0.6666666666666666, 0.5772161340361933, 0.5590025044484567, 0.0, 1.0, 0.18392996269926248], 
reward next is 0.8161, 
noisyNet noise sample is [array([0.11255621], dtype=float32), -1.1051846]. 
=============================================
[2019-04-04 02:45:13,037] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.37134846e-16 1.03623186e-10 5.58662437e-13 1.98737689e-05
 2.48624506e-11 9.99980092e-01 1.29465813e-11], sum to 1.0000
[2019-04-04 02:45:13,037] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9122
[2019-04-04 02:45:13,069] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 48.33333333333334, 133.5, 43.0, 26.0, 25.81049147581781, 0.3013161141469633, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2542800.0000, 
sim time next is 2543400.0000, 
raw observation next is [-0.8999999999999999, 48.0, 133.0, 45.0, 26.0, 25.79455606610234, 0.3001667271904339, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.43767313019390586, 0.48, 0.44333333333333336, 0.049723756906077346, 0.6666666666666666, 0.6495463388418617, 0.6000555757301447, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.3669994], dtype=float32), -0.7657514]. 
=============================================
[2019-04-04 02:45:15,159] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.9289529e-17 4.2511644e-08 6.6125696e-13 2.7684782e-10 9.8472155e-09
 1.0000000e+00 3.0897479e-12], sum to 1.0000
[2019-04-04 02:45:15,159] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1085
[2019-04-04 02:45:15,205] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.166666666666667, 93.0, 86.33333333333334, 104.0, 26.0, 25.38219809465831, 0.316637024854639, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2883000.0000, 
sim time next is 2883600.0000, 
raw observation next is [1.0, 93.0, 77.0, 78.0, 26.0, 25.34188256609782, 0.3163244543524154, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4903047091412743, 0.93, 0.25666666666666665, 0.0861878453038674, 0.6666666666666666, 0.6118235471748182, 0.6054414847841385, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6320236], dtype=float32), 0.47364518]. 
=============================================
[2019-04-04 02:45:20,737] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.61596412e-21 5.44504119e-16 6.35016027e-15 9.99999881e-01
 1.06884765e-16 8.55205187e-08 4.23279855e-15], sum to 1.0000
[2019-04-04 02:45:20,762] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1397
[2019-04-04 02:45:20,779] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 77.0, 0.0, 0.0, 26.0, 23.60171524767719, 0.03520674279126273, 0.0, 1.0, 60774.74546677367], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2964600.0000, 
sim time next is 2965200.0000, 
raw observation next is [-4.0, 77.0, 14.0, 15.66666666666666, 26.0, 23.55738898212832, 0.03597228420203145, 0.0, 1.0, 60784.37768512526], 
processed observation next is [0.0, 0.30434782608695654, 0.3518005540166205, 0.77, 0.04666666666666667, 0.017311233885819514, 0.6666666666666666, 0.4631157485106933, 0.5119907614006771, 0.0, 1.0, 0.2894494175482155], 
reward next is 0.7106, 
noisyNet noise sample is [array([1.2954055], dtype=float32), -0.092796296]. 
=============================================
[2019-04-04 02:45:20,941] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.7493395e-21 2.6129550e-11 1.2026098e-15 5.7051256e-12 9.1866779e-15
 1.0000000e+00 1.6849481e-14], sum to 1.0000
[2019-04-04 02:45:20,942] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0492
[2019-04-04 02:45:20,963] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.0, 100.0, 0.0, 0.0, 26.0, 25.39519878827728, 0.3354433941695742, 0.0, 1.0, 40552.25082278661], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3135600.0000, 
sim time next is 3136200.0000, 
raw observation next is [6.0, 100.0, 0.0, 0.0, 26.0, 25.46365582226553, 0.332209124728522, 0.0, 1.0, 18759.19116434795], 
processed observation next is [1.0, 0.30434782608695654, 0.6288088642659281, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6219713185221275, 0.6107363749095073, 0.0, 1.0, 0.08932948173499024], 
reward next is 0.9107, 
noisyNet noise sample is [array([0.90897524], dtype=float32), -1.0653192]. 
=============================================
[2019-04-04 02:45:22,014] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.7042686e-21 5.3161772e-13 6.6458842e-14 1.2575089e-13 8.9272921e-16
 1.0000000e+00 9.5670381e-15], sum to 1.0000
[2019-04-04 02:45:22,014] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6949
[2019-04-04 02:45:22,072] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.583333333333333, 86.0, 0.0, 0.0, 26.0, 24.59249070702084, 0.2135685113612016, 0.0, 1.0, 42736.1184025244], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2081400.0000, 
sim time next is 2082000.0000, 
raw observation next is [-4.666666666666667, 86.0, 0.0, 0.0, 26.0, 24.63279632946669, 0.2098886633775661, 0.0, 1.0, 42742.63119795735], 
processed observation next is [1.0, 0.08695652173913043, 0.3333333333333333, 0.86, 0.0, 0.0, 0.6666666666666666, 0.5527330274555574, 0.569962887792522, 0.0, 1.0, 0.20353633903789214], 
reward next is 0.7965, 
noisyNet noise sample is [array([-0.60126156], dtype=float32), 0.1988472]. 
=============================================
[2019-04-04 02:45:22,090] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[82.36428 ]
 [82.37021 ]
 [82.371506]
 [82.36766 ]
 [82.403564]], R is [[82.27342987]
 [82.24719238]
 [82.22119141]
 [82.19551849]
 [82.17017365]].
[2019-04-04 02:45:26,731] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.1454646e-20 1.0430814e-11 2.4360539e-14 1.2806156e-08 1.2554390e-12
 1.0000000e+00 9.7995620e-14], sum to 1.0000
[2019-04-04 02:45:26,731] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3079
[2019-04-04 02:45:26,766] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.833333333333334, 25.0, 110.3333333333333, 0.0, 26.0, 25.14334918313188, 0.3645190876732088, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2818200.0000, 
sim time next is 2818800.0000, 
raw observation next is [7.0, 24.0, 106.5, 0.0, 26.0, 25.53130009760209, 0.3937259537616078, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.6565096952908588, 0.24, 0.355, 0.0, 0.6666666666666666, 0.627608341466841, 0.6312419845872026, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.03531652], dtype=float32), -1.6021076]. 
=============================================
[2019-04-04 02:45:33,927] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.8736862e-14 4.4546027e-08 6.6662943e-07 2.1363150e-02 4.1746491e-09
 9.7863621e-01 1.6607547e-08], sum to 1.0000
[2019-04-04 02:45:33,928] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8833
[2019-04-04 02:45:33,951] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-15.33333333333334, 83.0, 0.0, 0.0, 26.0, 23.23859105002247, -0.07444255396523386, 0.0, 1.0, 43693.12665221443], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2697600.0000, 
sim time next is 2698200.0000, 
raw observation next is [-15.5, 83.0, 0.0, 0.0, 26.0, 23.23169567493342, -0.08619747222811254, 0.0, 1.0, 43575.24927463688], 
processed observation next is [1.0, 0.21739130434782608, 0.033240997229916885, 0.83, 0.0, 0.0, 0.6666666666666666, 0.435974639577785, 0.4712675092572958, 0.0, 1.0, 0.2075011870220804], 
reward next is 0.7925, 
noisyNet noise sample is [array([0.29388264], dtype=float32), -0.04910739]. 
=============================================
[2019-04-04 02:45:44,732] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.8355049e-21 1.0091670e-14 2.5837409e-14 2.9758043e-09 9.0840150e-15
 1.0000000e+00 9.2578774e-16], sum to 1.0000
[2019-04-04 02:45:44,732] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3330
[2019-04-04 02:45:44,744] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 60.0, 104.0, 720.0, 26.0, 26.13614293927279, 0.5618819496311939, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3492000.0000, 
sim time next is 3492600.0000, 
raw observation next is [0.1666666666666667, 60.16666666666666, 105.6666666666667, 736.6666666666666, 26.0, 26.21649373712985, 0.5769514190605195, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4672206832871654, 0.6016666666666666, 0.3522222222222223, 0.8139963167587476, 0.6666666666666666, 0.6847078114274874, 0.6923171396868398, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.558151], dtype=float32), 0.9949919]. 
=============================================
[2019-04-04 02:45:45,843] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.6802726e-21 3.3738561e-16 2.1217297e-15 1.0000000e+00 4.4649592e-18
 1.5710889e-11 6.2055688e-15], sum to 1.0000
[2019-04-04 02:45:45,843] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7215
[2019-04-04 02:45:45,865] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 72.33333333333333, 0.0, 0.0, 26.0, 23.84153856871387, 0.009835793810922528, 0.0, 1.0, 40147.62459275247], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3044400.0000, 
sim time next is 3045000.0000, 
raw observation next is [-6.0, 71.16666666666667, 0.0, 0.0, 26.0, 23.81502607797616, 0.003348077199927824, 0.0, 1.0, 40163.66682092594], 
processed observation next is [0.0, 0.21739130434782608, 0.296398891966759, 0.7116666666666667, 0.0, 0.0, 0.6666666666666666, 0.48458550649801335, 0.5011160257333093, 0.0, 1.0, 0.1912555562901235], 
reward next is 0.8087, 
noisyNet noise sample is [array([-2.2533717], dtype=float32), 1.3328061]. 
=============================================
[2019-04-04 02:45:45,879] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[83.21315]
 [83.2746 ]
 [83.33139]
 [83.35728]
 [83.3799 ]], R is [[83.13787842]
 [83.11532593]
 [83.09299469]
 [83.07080841]
 [83.04876709]].
[2019-04-04 02:45:54,741] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.3893654e-27 2.7907218e-19 1.5805986e-19 1.0000000e+00 3.4011373e-22
 9.6369538e-13 2.1140087e-20], sum to 1.0000
[2019-04-04 02:45:54,742] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1647
[2019-04-04 02:45:54,771] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [11.33333333333333, 26.66666666666667, 109.3333333333333, 746.3333333333334, 26.0, 25.63766898111058, 0.4747208287792431, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3666000.0000, 
sim time next is 3666600.0000, 
raw observation next is [11.5, 26.0, 111.0, 763.0, 26.0, 25.6268168807368, 0.4811931251132, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.7811634349030472, 0.26, 0.37, 0.8430939226519337, 0.6666666666666666, 0.6355680733947334, 0.6603977083710667, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.13437355], dtype=float32), 0.14358082]. 
=============================================
[2019-04-04 02:45:57,738] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1303756e-17 2.4592378e-10 1.2885349e-11 6.2647318e-12 3.7290857e-13
 1.0000000e+00 1.1133992e-12], sum to 1.0000
[2019-04-04 02:45:57,739] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5328
[2019-04-04 02:45:57,755] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.833333333333334, 79.33333333333333, 0.0, 0.0, 26.0, 25.51608471674358, 0.5943773658112079, 0.0, 1.0, 18749.61899345254], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3271800.0000, 
sim time next is 3272400.0000, 
raw observation next is [-5.0, 81.0, 0.0, 0.0, 26.0, 25.66577889977249, 0.5949498240361965, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.32409972299168976, 0.81, 0.0, 0.0, 0.6666666666666666, 0.6388149083143743, 0.6983166080120654, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6320593], dtype=float32), -1.4625899]. 
=============================================
[2019-04-04 02:46:05,384] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.7244325e-17 1.7378894e-09 8.5463131e-13 4.9676446e-06 1.5198895e-10
 9.9999499e-01 4.7827250e-12], sum to 1.0000
[2019-04-04 02:46:05,384] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3416
[2019-04-04 02:46:05,392] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 49.5, 128.3333333333333, 178.6666666666667, 26.0, 26.04084903228582, 0.4620813255574658, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2649000.0000, 
sim time next is 2649600.0000, 
raw observation next is [0.5, 50.0, 115.0, 165.0, 26.0, 25.96425769734974, 0.4595506522570234, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.4764542936288089, 0.5, 0.38333333333333336, 0.18232044198895028, 0.6666666666666666, 0.6636881414458117, 0.6531835507523411, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.83941805], dtype=float32), 1.2226028]. 
=============================================
[2019-04-04 02:46:05,411] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.9119107e-20 1.1062033e-10 1.0880991e-12 7.6999118e-10 6.8974541e-15
 1.0000000e+00 1.7961145e-13], sum to 1.0000
[2019-04-04 02:46:05,411] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5802
[2019-04-04 02:46:05,420] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 75.66666666666667, 114.6666666666667, 821.0, 26.0, 26.69584467375143, 0.7607965562923745, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3241200.0000, 
sim time next is 3241800.0000, 
raw observation next is [-2.0, 78.0, 115.0, 823.0, 26.0, 26.65086037924895, 0.7526289687948685, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.40720221606648205, 0.78, 0.38333333333333336, 0.9093922651933701, 0.6666666666666666, 0.7209050316040791, 0.7508763229316228, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.8854389], dtype=float32), 0.0215355]. 
=============================================
[2019-04-04 02:46:16,681] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.9200545e-21 1.7684147e-15 8.3844666e-15 9.9999988e-01 2.1055932e-17
 7.1996269e-08 1.6887467e-15], sum to 1.0000
[2019-04-04 02:46:16,681] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0058
[2019-04-04 02:46:16,704] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 46.33333333333334, 0.0, 0.0, 26.0, 25.46190988151815, 0.386518758516765, 0.0, 1.0, 19538.69173641391], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4225800.0000, 
sim time next is 4226400.0000, 
raw observation next is [1.0, 47.0, 0.0, 0.0, 26.0, 25.46516121307063, 0.3813863253377943, 0.0, 1.0, 25202.09011191377], 
processed observation next is [0.0, 0.9565217391304348, 0.4903047091412743, 0.47, 0.0, 0.0, 0.6666666666666666, 0.6220967677558858, 0.6271287751125981, 0.0, 1.0, 0.1200099529138751], 
reward next is 0.8800, 
noisyNet noise sample is [array([-0.49021322], dtype=float32), -0.27936777]. 
=============================================
[2019-04-04 02:46:29,753] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.8417385e-15 5.1210367e-07 5.4596573e-08 5.8024132e-05 9.1002272e-09
 9.9994135e-01 7.2192496e-09], sum to 1.0000
[2019-04-04 02:46:29,755] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5527
[2019-04-04 02:46:29,774] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.333333333333333, 100.0, 0.0, 0.0, 26.0, 25.3513641037888, 0.4969760346863215, 0.0, 1.0, 40591.1362985544], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3216000.0000, 
sim time next is 3216600.0000, 
raw observation next is [-2.5, 100.0, 0.0, 0.0, 26.0, 25.32015545546831, 0.4943390393046808, 0.0, 1.0, 40579.4853772374], 
processed observation next is [1.0, 0.21739130434782608, 0.39335180055401664, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6100129546223592, 0.6647796797682269, 0.0, 1.0, 0.19323564465351142], 
reward next is 0.8068, 
noisyNet noise sample is [array([0.15906262], dtype=float32), -1.6001017]. 
=============================================
[2019-04-04 02:46:30,685] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.2453127e-24 3.3119352e-15 2.9998426e-16 5.8074659e-14 2.7370889e-19
 1.0000000e+00 2.7929583e-19], sum to 1.0000
[2019-04-04 02:46:30,685] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8232
[2019-04-04 02:46:30,692] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 71.0, 113.0, 795.5, 26.0, 26.42020753429777, 0.5675312024422713, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3754800.0000, 
sim time next is 3755400.0000, 
raw observation next is [-2.833333333333333, 70.0, 113.6666666666667, 804.3333333333334, 26.0, 26.40470712057821, 0.5734200561446295, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.3841181902123731, 0.7, 0.378888888888889, 0.8887661141804789, 0.6666666666666666, 0.7003922600481841, 0.6911400187148765, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.11148495], dtype=float32), -1.5686779]. 
=============================================
[2019-04-04 02:46:30,925] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.9460946e-23 6.1756923e-16 1.4966827e-16 1.6685839e-13 2.3531116e-17
 1.0000000e+00 2.0063748e-18], sum to 1.0000
[2019-04-04 02:46:30,928] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4375
[2019-04-04 02:46:30,958] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.333333333333333, 67.33333333333334, 99.33333333333333, 641.1666666666667, 26.0, 26.16006631052032, 0.5356273287536286, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3835200.0000, 
sim time next is 3835800.0000, 
raw observation next is [-3.0, 65.5, 101.0, 680.0, 26.0, 26.23474553452522, 0.5550113945084058, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.3795013850415513, 0.655, 0.33666666666666667, 0.7513812154696132, 0.6666666666666666, 0.6862287945437684, 0.6850037981694687, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4959363], dtype=float32), -0.7874586]. 
=============================================
[2019-04-04 02:46:35,924] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.8097898e-15 2.9816050e-09 6.2540412e-10 4.3825341e-05 2.9753560e-09
 9.9995613e-01 2.0519710e-09], sum to 1.0000
[2019-04-04 02:46:35,929] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6101
[2019-04-04 02:46:35,988] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.833333333333333, 48.5, 0.0, 0.0, 26.0, 25.6186479088732, 0.5402411271743889, 1.0, 1.0, 73930.21151451279], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3867000.0000, 
sim time next is 3867600.0000, 
raw observation next is [1.666666666666667, 49.0, 0.0, 0.0, 26.0, 24.97167215313427, 0.4799997117387072, 1.0, 1.0, 185098.6899814585], 
processed observation next is [1.0, 0.782608695652174, 0.5087719298245615, 0.49, 0.0, 0.0, 0.6666666666666666, 0.5809726794278559, 0.6599999039129024, 1.0, 1.0, 0.8814223332450405], 
reward next is 0.1186, 
noisyNet noise sample is [array([-1.4282271], dtype=float32), 1.1584736]. 
=============================================
[2019-04-04 02:46:39,272] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.42758038e-13 4.92507297e-08 8.06662683e-07 7.59765148e-01
 1.05279806e-10 2.40233794e-01 1.60120706e-07], sum to 1.0000
[2019-04-04 02:46:39,273] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8087
[2019-04-04 02:46:39,287] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.83333333333333, 68.0, 0.0, 0.0, 26.0, 23.92063440378896, 0.05628249023755193, 0.0, 1.0, 43786.67734118216], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3991800.0000, 
sim time next is 3992400.0000, 
raw observation next is [-13.0, 69.0, 0.0, 0.0, 26.0, 23.85531481102658, 0.04451379080414434, 0.0, 1.0, 43809.60175728693], 
processed observation next is [1.0, 0.21739130434782608, 0.10249307479224376, 0.69, 0.0, 0.0, 0.6666666666666666, 0.48794290091888176, 0.5148379302680481, 0.0, 1.0, 0.20861715122517585], 
reward next is 0.7914, 
noisyNet noise sample is [array([0.03995831], dtype=float32), -0.37659675]. 
=============================================
[2019-04-04 02:46:40,136] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.5341115e-24 1.9970153e-16 7.8194439e-16 1.0000000e+00 3.7530353e-19
 1.6649926e-08 5.8391153e-16], sum to 1.0000
[2019-04-04 02:46:40,139] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9951
[2019-04-04 02:46:40,157] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.0, 52.0, 131.3333333333333, 825.3333333333334, 26.0, 25.26751850174111, 0.427441396560885, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4279200.0000, 
sim time next is 4279800.0000, 
raw observation next is [7.0, 52.0, 142.6666666666667, 803.6666666666667, 26.0, 25.32566452258799, 0.4320526471327919, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.6565096952908588, 0.52, 0.47555555555555573, 0.8880294659300185, 0.6666666666666666, 0.6104720435489991, 0.644017549044264, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.04276301], dtype=float32), 0.73274434]. 
=============================================
[2019-04-04 02:46:41,383] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.7986354e-23 1.0166815e-16 6.9886353e-17 1.0000000e+00 2.9606137e-21
 3.5445191e-10 2.0505668e-16], sum to 1.0000
[2019-04-04 02:46:41,385] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5043
[2019-04-04 02:46:41,404] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.1666666666666666, 39.5, 94.0, 741.0, 26.0, 25.11516424333545, 0.3630874232346537, 0.0, 1.0, 18702.57367500813], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3077400.0000, 
sim time next is 3078000.0000, 
raw observation next is [0.0, 39.0, 91.5, 724.0, 26.0, 25.11685787937913, 0.3645607911942352, 0.0, 1.0, 18701.60594899947], 
processed observation next is [0.0, 0.6521739130434783, 0.46260387811634357, 0.39, 0.305, 0.8, 0.6666666666666666, 0.5930714899482608, 0.6215202637314118, 0.0, 1.0, 0.089055266423807], 
reward next is 0.9109, 
noisyNet noise sample is [array([0.9342595], dtype=float32), -0.04702576]. 
=============================================
[2019-04-04 02:46:41,408] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[84.305504]
 [84.35473 ]
 [84.44477 ]
 [84.44143 ]
 [84.46859 ]], R is [[84.36289215]
 [84.4302063 ]
 [84.49684143]
 [84.56280518]
 [84.62810516]].
[2019-04-04 02:46:42,041] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.8833699e-18 6.6228627e-11 2.8040452e-11 8.9644900e-08 1.0642376e-13
 9.9999988e-01 2.9301041e-13], sum to 1.0000
[2019-04-04 02:46:42,048] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3245
[2019-04-04 02:46:42,070] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 49.0, 118.8333333333333, 804.1666666666666, 26.0, 26.32302189911497, 0.6055448555105449, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3928800.0000, 
sim time next is 3929400.0000, 
raw observation next is [-6.0, 49.0, 120.0, 810.0, 26.0, 26.34783735417101, 0.6086354089591844, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.296398891966759, 0.49, 0.4, 0.8950276243093923, 0.6666666666666666, 0.6956531128475841, 0.7028784696530614, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.13479383], dtype=float32), -0.22849068]. 
=============================================
[2019-04-04 02:47:04,950] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.1365120e-22 4.4052587e-16 8.6310065e-15 1.0000000e+00 1.2653558e-19
 6.4961601e-09 3.4223896e-15], sum to 1.0000
[2019-04-04 02:47:04,950] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3978
[2019-04-04 02:47:04,977] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.3333333333333333, 31.66666666666667, 118.8333333333333, 826.1666666666666, 26.0, 25.11454300952961, 0.3875386298706013, 0.0, 1.0, 18699.00540380461], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4189200.0000, 
sim time next is 4189800.0000, 
raw observation next is [0.6666666666666667, 30.83333333333333, 118.6666666666667, 830.3333333333334, 26.0, 25.07011985223333, 0.3934659969280462, 0.0, 1.0, 21143.57945628644], 
processed observation next is [0.0, 0.4782608695652174, 0.4810710987996307, 0.3083333333333333, 0.39555555555555566, 0.9174953959484347, 0.6666666666666666, 0.5891766543527774, 0.6311553323093487, 0.0, 1.0, 0.10068371169660209], 
reward next is 0.8993, 
noisyNet noise sample is [array([-0.5816038], dtype=float32), -0.2773228]. 
=============================================
[2019-04-04 02:47:13,900] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:47:13,901] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:47:13,941] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run28
[2019-04-04 02:47:21,613] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.11005745e-20 5.51651823e-13 1.69708499e-13 4.44695752e-06
 8.70364571e-14 9.99995589e-01 1.05949775e-13], sum to 1.0000
[2019-04-04 02:47:21,630] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1088
[2019-04-04 02:47:21,643] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 80.0, 39.99999999999999, 77.33333333333331, 26.0, 25.59995230569341, 0.4964459396242053, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4434600.0000, 
sim time next is 4435200.0000, 
raw observation next is [2.0, 80.0, 60.0, 116.0, 26.0, 25.55674970821268, 0.4893511063212331, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.518005540166205, 0.8, 0.2, 0.1281767955801105, 0.6666666666666666, 0.6297291423510568, 0.663117035440411, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.47397733], dtype=float32), 0.8653124]. 
=============================================
[2019-04-04 02:47:27,349] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.4415163e-21 4.6832353e-14 9.1185036e-15 9.9999380e-01 4.2263005e-16
 6.2126760e-06 2.9909943e-14], sum to 1.0000
[2019-04-04 02:47:27,349] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4765
[2019-04-04 02:47:27,420] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.9, 86.0, 85.66666666666666, 0.0, 26.0, 24.35579838568722, 0.1230046088727312, 0.0, 1.0, 32219.64829762423], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 49200.0000, 
sim time next is 49800.0000, 
raw observation next is [7.8, 86.0, 84.33333333333334, 0.0, 26.0, 24.37715575319795, 0.1273951055859611, 0.0, 1.0, 27742.38194223356], 
processed observation next is [0.0, 0.5652173913043478, 0.6786703601108034, 0.86, 0.28111111111111114, 0.0, 0.6666666666666666, 0.5314296460998292, 0.5424650351953203, 0.0, 1.0, 0.13210658067730266], 
reward next is 0.8679, 
noisyNet noise sample is [array([-0.4547052], dtype=float32), 0.5741778]. 
=============================================
[2019-04-04 02:47:30,057] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:47:30,058] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:47:30,120] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run28
[2019-04-04 02:47:35,785] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:47:35,785] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:47:35,789] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run28
[2019-04-04 02:47:38,369] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3860635e-16 2.9871958e-10 6.2229152e-11 3.2157818e-04 1.5941048e-13
 9.9967849e-01 8.2430911e-12], sum to 1.0000
[2019-04-04 02:47:38,370] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2375
[2019-04-04 02:47:38,461] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 30.0, 0.0, 0.0, 26.0, 25.30302390946498, 0.4869856915331164, 0.0, 1.0, 169363.8217488125], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4048200.0000, 
sim time next is 4048800.0000, 
raw observation next is [-4.0, 29.66666666666666, 0.0, 0.0, 26.0, 25.34807564824036, 0.509298253711472, 0.0, 1.0, 88506.98401507176], 
processed observation next is [1.0, 0.8695652173913043, 0.3518005540166205, 0.29666666666666663, 0.0, 0.0, 0.6666666666666666, 0.6123396373533634, 0.6697660845704907, 0.0, 1.0, 0.42146182864319887], 
reward next is 0.5785, 
noisyNet noise sample is [array([-0.5489204], dtype=float32), 0.13899559]. 
=============================================
[2019-04-04 02:47:55,249] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.2228281e-20 1.4866769e-11 2.2978871e-13 4.7957367e-01 6.9413681e-13
 5.2042633e-01 2.8918120e-12], sum to 1.0000
[2019-04-04 02:47:55,250] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2598
[2019-04-04 02:47:55,268] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [11.16666666666667, 19.83333333333334, 113.3333333333333, 832.6666666666667, 26.0, 27.89082335511076, 0.9953126901708198, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5062200.0000, 
sim time next is 5062800.0000, 
raw observation next is [11.33333333333333, 19.66666666666667, 112.1666666666667, 825.8333333333333, 26.0, 28.10850529528904, 1.027302038158649, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.7765466297322253, 0.1966666666666667, 0.373888888888889, 0.9125230202578268, 0.6666666666666666, 0.8423754412740866, 0.8424340127195498, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1413248], dtype=float32), -1.3588647]. 
=============================================
[2019-04-04 02:48:01,036] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:48:01,036] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:48:01,040] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run28
[2019-04-04 02:48:03,311] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0163769e-20 1.9277142e-13 4.5630993e-14 7.7658505e-15 1.1517220e-15
 1.0000000e+00 3.3368670e-15], sum to 1.0000
[2019-04-04 02:48:03,311] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0437
[2019-04-04 02:48:03,423] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 72.0, 139.1666666666667, 1.833333333333333, 26.0, 26.07305182955591, 0.5454857688530806, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4722000.0000, 
sim time next is 4722600.0000, 
raw observation next is [1.0, 72.0, 131.3333333333333, 3.666666666666666, 26.0, 26.12736478429566, 0.5474166437422814, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.4903047091412743, 0.72, 0.4377777777777776, 0.004051565377532228, 0.6666666666666666, 0.6772803986913051, 0.6824722145807605, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.26443374], dtype=float32), 0.7366565]. 
=============================================
[2019-04-04 02:48:11,026] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.0821296e-18 3.8678671e-11 2.0260530e-14 2.2317831e-08 7.3869074e-13
 1.0000000e+00 1.5509515e-13], sum to 1.0000
[2019-04-04 02:48:11,026] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6990
[2019-04-04 02:48:11,138] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 92.0, 208.8333333333333, 6.0, 26.0, 26.46943652949851, 0.5967182330847245, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4704000.0000, 
sim time next is 4704600.0000, 
raw observation next is [0.0, 92.0, 209.6666666666667, 6.0, 26.0, 26.47827265952496, 0.5976067484080028, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.46260387811634357, 0.92, 0.698888888888889, 0.0066298342541436465, 0.6666666666666666, 0.7065227216270801, 0.6992022494693343, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.16999729], dtype=float32), -0.6211145]. 
=============================================
[2019-04-04 02:48:23,718] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.03131056e-30 4.80889657e-25 1.01453823e-25 1.00000000e+00
 6.01088190e-30 3.08498483e-19 2.02516174e-24], sum to 1.0000
[2019-04-04 02:48:23,718] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4128
[2019-04-04 02:48:23,745] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 35.0, 116.5, 798.0, 26.0, 25.2285983896157, 0.4062265092714809, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4186800.0000, 
sim time next is 4187400.0000, 
raw observation next is [-0.6666666666666667, 34.16666666666667, 117.3333333333333, 806.0, 26.0, 25.20668189320944, 0.4063859657952304, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.44413665743305636, 0.34166666666666673, 0.391111111111111, 0.8906077348066298, 0.6666666666666666, 0.6005568244341198, 0.6354619885984102, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6705337], dtype=float32), 0.5716398]. 
=============================================
[2019-04-04 02:48:27,145] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.1598265e-26 6.8710701e-20 2.3502285e-20 1.0000000e+00 2.1562680e-23
 8.0909942e-15 8.2174791e-20], sum to 1.0000
[2019-04-04 02:48:27,145] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5736
[2019-04-04 02:48:27,157] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 36.0, 0.0, 0.0, 26.0, 25.44776498538225, 0.3733026153430152, 0.0, 1.0, 19100.50669655699], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4917000.0000, 
sim time next is 4917600.0000, 
raw observation next is [1.0, 36.0, 0.0, 0.0, 26.0, 25.45163070667034, 0.3666976220917346, 0.0, 1.0, 24398.1534192721], 
processed observation next is [0.0, 0.9565217391304348, 0.4903047091412743, 0.36, 0.0, 0.0, 0.6666666666666666, 0.6209692255558616, 0.6222325406972449, 0.0, 1.0, 0.11618168294891476], 
reward next is 0.8838, 
noisyNet noise sample is [array([0.2320196], dtype=float32), -0.7183581]. 
=============================================
[2019-04-04 02:48:27,417] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.7484025e-20 1.9273051e-12 5.8270215e-14 9.3030763e-01 3.0017885e-14
 6.9692336e-02 2.5654720e-13], sum to 1.0000
[2019-04-04 02:48:27,417] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0570
[2019-04-04 02:48:27,448] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [12.0, 19.0, 96.0, 745.0, 26.0, 28.27586600583851, 1.077910957912978, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5067000.0000, 
sim time next is 5067600.0000, 
raw observation next is [12.0, 19.0, 92.66666666666666, 718.3333333333334, 26.0, 27.61893569418353, 1.042258196582489, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.7950138504155125, 0.19, 0.3088888888888889, 0.7937384898710866, 0.6666666666666666, 0.8015779745152942, 0.8474193988608297, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.13397406], dtype=float32), 0.66456914]. 
=============================================
[2019-04-04 02:48:32,445] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:48:32,445] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:48:32,448] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run28
[2019-04-04 02:48:33,213] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:48:33,213] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:48:33,238] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run28
[2019-04-04 02:48:35,981] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:48:35,981] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:48:36,008] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run28
[2019-04-04 02:48:39,966] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:48:39,966] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:48:39,970] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run28
[2019-04-04 02:48:40,821] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:48:40,821] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:48:40,825] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run28
[2019-04-04 02:48:46,195] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.9511252e-22 6.8127732e-15 4.5466849e-17 9.9999893e-01 5.9107977e-17
 1.1311938e-06 9.7035682e-16], sum to 1.0000
[2019-04-04 02:48:46,198] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7892
[2019-04-04 02:48:46,206] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [11.16666666666667, 17.0, 24.0, 194.6666666666667, 26.0, 28.50679151479489, 1.121696925230766, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5075400.0000, 
sim time next is 5076000.0000, 
raw observation next is [11.0, 17.0, 18.0, 146.0, 26.0, 28.56721921277094, 1.110453346709288, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.7673130193905818, 0.17, 0.06, 0.16132596685082873, 0.6666666666666666, 0.880601601064245, 0.8701511155697627, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.14561516], dtype=float32), -0.76596177]. 
=============================================
[2019-04-04 02:48:46,230] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[84.34602 ]
 [84.67888 ]
 [84.73644 ]
 [84.90361 ]
 [85.343506]], R is [[84.26659393]
 [84.42392731]
 [84.57968903]
 [84.73389435]
 [84.88655853]].
[2019-04-04 02:48:48,477] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:48:48,478] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:48:48,481] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run28
[2019-04-04 02:48:52,220] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:48:52,220] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:48:52,271] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run28
[2019-04-04 02:48:54,727] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.1375612e-18 1.7877469e-11 7.8231671e-11 9.9932742e-01 6.9976192e-16
 6.7261595e-04 8.3094712e-11], sum to 1.0000
[2019-04-04 02:48:54,727] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7738
[2019-04-04 02:48:54,754] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-13.9, 68.66666666666667, 0.0, 0.0, 26.0, 23.76130462495759, -0.005785369406120779, 0.0, 1.0, 47175.94336828189], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 343200.0000, 
sim time next is 343800.0000, 
raw observation next is [-13.9, 68.0, 0.0, 0.0, 26.0, 23.69535532602277, -0.02043810754663693, 0.0, 1.0, 47215.83388617179], 
processed observation next is [1.0, 1.0, 0.07756232686980608, 0.68, 0.0, 0.0, 0.6666666666666666, 0.4746129438352309, 0.4931872974844544, 0.0, 1.0, 0.22483730421986567], 
reward next is 0.7752, 
noisyNet noise sample is [array([0.6460845], dtype=float32), -1.4201758]. 
=============================================
[2019-04-04 02:48:55,173] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:48:55,173] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:48:55,176] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run28
[2019-04-04 02:48:56,117] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:48:56,117] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:48:56,120] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run28
[2019-04-04 02:49:09,878] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.0686951e-20 4.5712154e-13 8.5167415e-14 9.9997020e-01 2.5883656e-17
 2.9781577e-05 3.3110315e-13], sum to 1.0000
[2019-04-04 02:49:09,878] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5721
[2019-04-04 02:49:09,938] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.4, 71.0, 0.0, 0.0, 26.0, 23.8013489970957, 0.02090964112099787, 0.0, 1.0, 44749.67994982326], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 171600.0000, 
sim time next is 172200.0000, 
raw observation next is [-8.4, 71.0, 0.0, 0.0, 26.0, 23.76198366958531, 0.01082772663555527, 0.0, 1.0, 44698.65928916175], 
processed observation next is [1.0, 1.0, 0.2299168975069252, 0.71, 0.0, 0.0, 0.6666666666666666, 0.48016530579877575, 0.5036092422118518, 0.0, 1.0, 0.21285075851981786], 
reward next is 0.7871, 
noisyNet noise sample is [array([0.4761995], dtype=float32), 1.5074041]. 
=============================================
[2019-04-04 02:49:13,600] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.74849284e-15 2.05487738e-09 1.84289563e-11 9.86334801e-01
 1.56085550e-12 1.36651620e-02 1.00886716e-10], sum to 1.0000
[2019-04-04 02:49:13,600] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2533
[2019-04-04 02:49:13,691] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.9, 84.0, 29.0, 0.0, 26.0, 25.89977793864038, 0.2951505258209543, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 837000.0000, 
sim time next is 837600.0000, 
raw observation next is [-3.899999999999999, 84.66666666666666, 24.16666666666667, 0.0, 26.0, 25.41641131000322, 0.3116592915781236, 1.0, 1.0, 46852.63298401057], 
processed observation next is [1.0, 0.6956521739130435, 0.35457063711911363, 0.8466666666666666, 0.08055555555555557, 0.0, 0.6666666666666666, 0.6180342758336016, 0.6038864305260412, 1.0, 1.0, 0.22310777611433605], 
reward next is 0.7769, 
noisyNet noise sample is [array([0.38784355], dtype=float32), -0.6637822]. 
=============================================
[2019-04-04 02:49:29,413] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.0188137e-19 8.4270432e-13 9.8419569e-13 1.0000000e+00 1.5589584e-14
 1.8964069e-08 9.1775274e-13], sum to 1.0000
[2019-04-04 02:49:29,414] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2472
[2019-04-04 02:49:29,466] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.1666666666666666, 93.33333333333334, 21.0, 0.0, 26.0, 25.81263371946388, 0.4789806612118188, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4693800.0000, 
sim time next is 4694400.0000, 
raw observation next is [0.0, 92.0, 31.5, 0.0, 26.0, 25.78286313713301, 0.4677284742455532, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.46260387811634357, 0.92, 0.105, 0.0, 0.6666666666666666, 0.6485719280944174, 0.6559094914151844, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.19089629], dtype=float32), -1.2925218]. 
=============================================
[2019-04-04 02:49:35,946] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:49:35,946] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:49:35,949] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run28
[2019-04-04 02:49:44,243] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.6259969e-22 2.2808533e-17 2.1343587e-17 1.0000000e+00 5.2583590e-18
 5.2595817e-11 6.6533571e-17], sum to 1.0000
[2019-04-04 02:49:44,243] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0175
[2019-04-04 02:49:44,298] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 35.5, 32.0, 0.0, 26.0, 25.41276582824976, 0.271256833402064, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 490200.0000, 
sim time next is 490800.0000, 
raw observation next is [1.1, 37.0, 26.0, 0.0, 26.0, 25.56914972907998, 0.2830122931753931, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.49307479224376743, 0.37, 0.08666666666666667, 0.0, 0.6666666666666666, 0.6307624774233318, 0.5943374310584644, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.1844543], dtype=float32), 0.45431566]. 
=============================================
[2019-04-04 02:49:59,826] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.044889e-23 6.210915e-17 3.238693e-17 1.000000e+00 9.969841e-22
 1.977416e-12 9.041099e-18], sum to 1.0000
[2019-04-04 02:49:59,829] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3441
[2019-04-04 02:49:59,857] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.8, 85.0, 0.0, 0.0, 26.0, 25.10366473390657, 0.3053661524178422, 0.0, 1.0, 43076.03575381141], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 592200.0000, 
sim time next is 592800.0000, 
raw observation next is [-2.8, 84.33333333333333, 0.0, 0.0, 26.0, 25.12005343917796, 0.3007064231865242, 0.0, 1.0, 42885.51441213672], 
processed observation next is [0.0, 0.8695652173913043, 0.38504155124653744, 0.8433333333333333, 0.0, 0.0, 0.6666666666666666, 0.5933377865981632, 0.6002354743955081, 0.0, 1.0, 0.20421673529588913], 
reward next is 0.7958, 
noisyNet noise sample is [array([0.76177657], dtype=float32), -0.2012631]. 
=============================================
[2019-04-04 02:50:00,624] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:50:00,625] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:50:00,629] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run28
[2019-04-04 02:50:01,593] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.4977881e-22 1.4426776e-15 1.1868585e-16 1.0000000e+00 3.9705765e-20
 2.1499660e-10 4.8942723e-16], sum to 1.0000
[2019-04-04 02:50:01,593] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8512
[2019-04-04 02:50:01,639] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 36.0, 0.0, 0.0, 26.0, 25.75394590990392, 0.4816511355444481, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5005200.0000, 
sim time next is 5005800.0000, 
raw observation next is [3.0, 35.5, 0.0, 0.0, 26.0, 25.60708176902303, 0.4701991884759024, 0.0, 1.0, 115916.8910638279], 
processed observation next is [1.0, 0.9565217391304348, 0.5457063711911359, 0.355, 0.0, 0.0, 0.6666666666666666, 0.6339234807519191, 0.6567330628253009, 0.0, 1.0, 0.5519851955420376], 
reward next is 0.4480, 
noisyNet noise sample is [array([0.94116616], dtype=float32), 1.0734048]. 
=============================================
[2019-04-04 02:50:03,059] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.5367856e-23 5.9357244e-18 2.2082850e-17 1.0000000e+00 1.5056294e-21
 1.0188251e-15 1.7748276e-16], sum to 1.0000
[2019-04-04 02:50:03,060] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5971
[2019-04-04 02:50:03,141] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.2, 83.0, 104.5, 138.6666666666667, 26.0, 24.97356451011737, 0.322552353881104, 0.0, 1.0, 47328.49238651079], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 573600.0000, 
sim time next is 574200.0000, 
raw observation next is [-1.2, 83.0, 100.0, 73.0, 26.0, 24.96304562024421, 0.3238725673327311, 0.0, 1.0, 45307.37932213813], 
processed observation next is [0.0, 0.6521739130434783, 0.42936288088642666, 0.83, 0.3333333333333333, 0.08066298342541436, 0.6666666666666666, 0.5802538016870175, 0.6079575224442437, 0.0, 1.0, 0.2157494253435149], 
reward next is 0.7843, 
noisyNet noise sample is [array([-1.4385005], dtype=float32), 0.8975331]. 
=============================================
[2019-04-04 02:50:07,602] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.02244516e-20 9.31830940e-15 1.62566521e-13 9.99997973e-01
 4.92910723e-18 2.01336752e-06 9.18375060e-15], sum to 1.0000
[2019-04-04 02:50:07,631] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3638
[2019-04-04 02:50:07,726] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.9, 78.0, 17.0, 157.0, 26.0, 24.18465986005237, 0.07504305808049334, 1.0, 1.0, 90473.04257829412], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 201600.0000, 
sim time next is 202200.0000, 
raw observation next is [-8.816666666666666, 78.0, 22.66666666666667, 203.3333333333333, 26.0, 24.5342250740308, 0.1104637731284071, 1.0, 1.0, 83996.98721911132], 
processed observation next is [1.0, 0.34782608695652173, 0.21837488457987075, 0.78, 0.07555555555555557, 0.22467771639042353, 0.6666666666666666, 0.5445187561692334, 0.536821257709469, 1.0, 1.0, 0.3999856534243396], 
reward next is 0.6000, 
noisyNet noise sample is [array([-0.57771075], dtype=float32), -0.35614374]. 
=============================================
[2019-04-04 02:50:11,401] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:50:11,401] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:50:11,404] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run28
[2019-04-04 02:50:18,370] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1851092e-19 1.7362430e-13 6.2320614e-15 9.9999905e-01 1.1691436e-16
 1.0106795e-06 3.5221899e-13], sum to 1.0000
[2019-04-04 02:50:18,370] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8180
[2019-04-04 02:50:18,433] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.6, 57.0, 107.5, 614.0, 26.0, 25.84132958272414, 0.3888151440878131, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 734400.0000, 
sim time next is 735000.0000, 
raw observation next is [-0.4166666666666667, 55.83333333333333, 115.3333333333333, 559.0, 26.0, 25.83959091768709, 0.3823163383400265, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.45106186518928904, 0.5583333333333332, 0.3844444444444443, 0.6176795580110497, 0.6666666666666666, 0.6532992431405908, 0.6274387794466755, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9203001], dtype=float32), 1.3978002]. 
=============================================
[2019-04-04 02:50:18,445] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[85.25137 ]
 [85.788376]
 [86.50229 ]
 [87.314865]
 [87.55385 ]], R is [[84.95141602]
 [85.10190582]
 [85.25088501]
 [85.39837646]
 [85.54439545]].
[2019-04-04 02:50:28,883] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.08489348e-16 7.18675813e-11 1.16543563e-11 9.99174058e-01
 1.25520245e-14 8.25945870e-04 1.29830688e-10], sum to 1.0000
[2019-04-04 02:50:28,883] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3478
[2019-04-04 02:50:28,983] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.9, 84.0, 0.0, 0.0, 26.0, 24.7134063881187, 0.3384941626687361, 1.0, 1.0, 100178.9751611819], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 844200.0000, 
sim time next is 844800.0000, 
raw observation next is [-3.899999999999999, 84.66666666666666, 0.0, 0.0, 26.0, 25.1150231937282, 0.3732402451809517, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.35457063711911363, 0.8466666666666666, 0.0, 0.0, 0.6666666666666666, 0.5929185994773499, 0.6244134150603172, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.133269], dtype=float32), 1.0581998]. 
=============================================
[2019-04-04 02:50:31,028] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.4085612e-20 3.7453057e-15 5.4466367e-14 1.0000000e+00 1.7063580e-18
 1.2813192e-08 1.4010816e-14], sum to 1.0000
[2019-04-04 02:50:31,028] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5802
[2019-04-04 02:50:31,055] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.3, 71.0, 0.0, 0.0, 26.0, 23.79781971909524, -0.01175888455124242, 0.0, 1.0, 41715.71713353461], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 795600.0000, 
sim time next is 796200.0000, 
raw observation next is [-7.3, 71.0, 0.0, 0.0, 26.0, 23.75238387931326, -0.02406358240444314, 0.0, 1.0, 41825.33093210727], 
processed observation next is [1.0, 0.21739130434782608, 0.26038781163434904, 0.71, 0.0, 0.0, 0.6666666666666666, 0.47936532327610504, 0.49197880586518566, 0.0, 1.0, 0.19916824253384413], 
reward next is 0.8008, 
noisyNet noise sample is [array([0.96253175], dtype=float32), -1.0630188]. 
=============================================
[2019-04-04 02:50:47,285] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0451657e-20 3.7924024e-16 3.0453868e-16 1.0000000e+00 8.0444103e-20
 2.6523304e-12 2.5462649e-15], sum to 1.0000
[2019-04-04 02:50:47,286] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2206
[2019-04-04 02:50:47,314] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-14.91666666666667, 69.0, 0.0, 0.0, 26.0, 23.19635563923696, -0.1547342441358715, 0.0, 1.0, 48260.73399429623], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 352200.0000, 
sim time next is 352800.0000, 
raw observation next is [-15.0, 69.0, 0.0, 0.0, 26.0, 23.18394302466097, -0.1685865719017007, 0.0, 1.0, 48451.53429248174], 
processed observation next is [1.0, 0.08695652173913043, 0.04709141274238226, 0.69, 0.0, 0.0, 0.6666666666666666, 0.43199525205508077, 0.4438044760327664, 0.0, 1.0, 0.23072159186896066], 
reward next is 0.7693, 
noisyNet noise sample is [array([0.9036207], dtype=float32), 1.2713153]. 
=============================================
[2019-04-04 02:50:55,668] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-04-04 02:50:55,669] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 02:50:55,669] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:50:55,671] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 02:50:55,672] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:50:55,672] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 02:50:55,674] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:50:55,674] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run38
[2019-04-04 02:50:55,701] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run38
[2019-04-04 02:50:55,702] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run38
[2019-04-04 02:51:31,724] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.72084403], dtype=float32), 0.13164882]
[2019-04-04 02:51:31,724] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [8.816666666666666, 63.0, 0.0, 0.0, 19.0, 19.65653858895266, -1.091859188003502, 1.0, 1.0, 0.0]
[2019-04-04 02:51:31,725] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 02:51:31,726] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [8.1809991e-15 1.0644666e-14 1.4453507e-13 1.0000000e+00 9.4109720e-12
 5.2330168e-10 4.5673708e-12], sampled 0.6950169092783154
[2019-04-04 02:51:48,972] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.72084403], dtype=float32), 0.13164882]
[2019-04-04 02:51:48,972] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [11.05, 85.0, 0.0, 0.0, 20.0, 19.89795886622078, -0.8001902221535869, 0.0, 1.0, 0.0]
[2019-04-04 02:51:48,972] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 02:51:48,973] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [4.1124074e-16 1.6663323e-13 7.3939135e-13 1.0000000e+00 5.6848319e-13
 3.9608736e-08 4.5476470e-12], sampled 0.8290531058756699
[2019-04-04 02:52:15,164] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.72084403], dtype=float32), 0.13164882]
[2019-04-04 02:52:15,164] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-9.919671528, 89.67046021, 22.35287221, 261.52559385, 20.0, 19.90805064984601, -0.9694327044391503, 1.0, 1.0, 0.0]
[2019-04-04 02:52:15,165] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 02:52:15,166] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.0812965e-12 1.9513982e-12 3.4380546e-10 9.9999988e-01 8.4366202e-11
 1.6325963e-07 1.8978024e-09], sampled 0.09873246876698771
[2019-04-04 02:52:36,266] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.72084403], dtype=float32), 0.13164882]
[2019-04-04 02:52:36,267] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [3.4, 43.33333333333334, 66.0, 441.8333333333333, 20.0, 21.64283819885319, -0.5278640685704418, 1.0, 1.0, 0.0]
[2019-04-04 02:52:36,267] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 02:52:36,268] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [3.8903421e-16 4.7571557e-16 1.3003120e-14 1.0000000e+00 5.7486927e-14
 1.7670218e-12 3.2652283e-13], sampled 0.1527887337611351
[2019-04-04 02:52:49,007] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5776.1109 156560672.1847 -1760.7610
[2019-04-04 02:52:57,508] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5275.9049 181489267.1220 -2429.2178
[2019-04-04 02:53:04,349] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5268.2788 194208590.8254 -2349.3598
[2019-04-04 02:53:05,379] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 3700000, evaluation results [3700000.0, 5275.904862089485, 181489267.12202036, -2429.2177547133747, 5776.1109410754025, 156560672.18473327, -1760.7609880341015, 5268.278799288253, 194208590.82538438, -2349.359764412906]
[2019-04-04 02:53:16,833] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0512363e-18 1.1139230e-13 2.3377014e-12 9.9999976e-01 4.0693668e-16
 2.3550515e-07 1.1606940e-13], sum to 1.0000
[2019-04-04 02:53:16,833] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7662
[2019-04-04 02:53:16,848] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 26.0, 25.44619324715313, 0.564046727887256, 0.0, 1.0, 38918.92321529705], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1314000.0000, 
sim time next is 1314600.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 26.0, 25.56274508128456, 0.5626775581187916, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.5069252077562327, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6302287567737134, 0.6875591860395972, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5933826], dtype=float32), 1.4222363]. 
=============================================
[2019-04-04 02:53:23,343] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.4845031e-22 2.1649554e-15 2.2274584e-15 9.9999988e-01 1.0022439e-18
 6.5535843e-08 7.8524433e-16], sum to 1.0000
[2019-04-04 02:53:23,346] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0982
[2019-04-04 02:53:23,428] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.6, 100.0, 12.0, 0.0, 26.0, 25.36657949449557, 0.4285556188689405, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1411800.0000, 
sim time next is 1412400.0000, 
raw observation next is [-0.6, 100.0, 15.0, 0.0, 26.0, 25.30439068996875, 0.4696292003528839, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.44598337950138506, 1.0, 0.05, 0.0, 0.6666666666666666, 0.6086992241640624, 0.6565430667842946, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3652797], dtype=float32), 0.8669302]. 
=============================================
[2019-04-04 02:53:23,975] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.2221865e-23 1.3528418e-16 7.6514513e-18 1.0000000e+00 5.9402031e-20
 5.3860683e-10 1.3755561e-17], sum to 1.0000
[2019-04-04 02:53:23,977] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9933
[2019-04-04 02:53:24,047] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 100.0, 0.0, 0.0, 26.0, 25.2078193474441, 0.4588449171749525, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1495200.0000, 
sim time next is 1495800.0000, 
raw observation next is [1.1, 100.0, 0.0, 0.0, 26.0, 25.497279049842, 0.4669686168496733, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.49307479224376743, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6247732541534999, 0.6556562056165577, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.798206], dtype=float32), 0.3402855]. 
=============================================
[2019-04-04 02:53:25,224] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.1295813e-20 7.2397322e-14 2.0369333e-14 1.0000000e+00 1.0667744e-16
 3.4397505e-08 2.7114893e-14], sum to 1.0000
[2019-04-04 02:53:25,225] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5035
[2019-04-04 02:53:25,258] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.033333333333333, 94.0, 90.0, 706.6666666666666, 26.0, 26.33985262727072, 0.4874966064895017, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1510800.0000, 
sim time next is 1511400.0000, 
raw observation next is [4.216666666666667, 93.5, 92.0, 705.3333333333334, 26.0, 26.33031555952702, 0.6456878793603403, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.5794090489381348, 0.935, 0.30666666666666664, 0.7793738489871087, 0.6666666666666666, 0.6941929632939182, 0.7152292931201134, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.373961], dtype=float32), -2.1133897]. 
=============================================
[2019-04-04 02:53:35,646] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.7843231e-24 2.2558375e-17 3.7536815e-18 1.0000000e+00 9.0343277e-21
 2.3575358e-14 4.3366428e-18], sum to 1.0000
[2019-04-04 02:53:35,648] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2985
[2019-04-04 02:53:35,679] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.2, 95.33333333333334, 0.0, 0.0, 26.0, 25.4347912217205, 0.4658768872794581, 0.0, 1.0, 18763.95526423106], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1482000.0000, 
sim time next is 1482600.0000, 
raw observation next is [2.2, 95.66666666666666, 0.0, 0.0, 26.0, 25.42483260124683, 0.4545878671937204, 0.0, 1.0, 23320.40560176664], 
processed observation next is [1.0, 0.13043478260869565, 0.5235457063711911, 0.9566666666666666, 0.0, 0.0, 0.6666666666666666, 0.6187360501039025, 0.6515292890645735, 0.0, 1.0, 0.11104955048460305], 
reward next is 0.8890, 
noisyNet noise sample is [array([-0.02110281], dtype=float32), 0.6403568]. 
=============================================
[2019-04-04 02:53:59,036] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.8383789e-20 1.6229126e-15 7.2756098e-15 1.0000000e+00 3.5634868e-17
 1.9458662e-10 5.5096914e-15], sum to 1.0000
[2019-04-04 02:53:59,036] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1785
[2019-04-04 02:53:59,048] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.633333333333333, 77.0, 0.0, 0.0, 26.0, 24.04211598363629, 0.02218157786347177, 0.0, 1.0, 45144.11986635899], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1906800.0000, 
sim time next is 1907400.0000, 
raw observation next is [-7.716666666666667, 77.5, 0.0, 0.0, 26.0, 24.05570181906715, 0.01611875879608607, 0.0, 1.0, 45071.9636164663], 
processed observation next is [1.0, 0.043478260869565216, 0.24884579870729456, 0.775, 0.0, 0.0, 0.6666666666666666, 0.5046418182555957, 0.5053729195986953, 0.0, 1.0, 0.21462839817364907], 
reward next is 0.7854, 
noisyNet noise sample is [array([-0.5618247], dtype=float32), -1.1900766]. 
=============================================
[2019-04-04 02:53:59,530] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.5382454e-20 8.4529965e-16 1.0610322e-15 1.0000000e+00 2.1178692e-18
 2.1435692e-11 7.1833061e-15], sum to 1.0000
[2019-04-04 02:53:59,563] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1687
[2019-04-04 02:53:59,576] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.100000000000001, 86.33333333333333, 0.0, 0.0, 26.0, 24.31444852610964, 0.1558594376274976, 0.0, 1.0, 41916.45440820566], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1991400.0000, 
sim time next is 1992000.0000, 
raw observation next is [-6.0, 85.66666666666667, 0.0, 0.0, 26.0, 24.37355014459804, 0.1543164462316783, 0.0, 1.0, 41817.45730578968], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.8566666666666667, 0.0, 0.0, 0.6666666666666666, 0.5311291787165032, 0.5514388154105595, 0.0, 1.0, 0.19913074907518893], 
reward next is 0.8009, 
noisyNet noise sample is [array([-0.33485258], dtype=float32), 1.285643]. 
=============================================
[2019-04-04 02:53:59,587] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[82.90119]
 [82.85248]
 [82.88877]
 [82.89255]
 [82.96534]], R is [[82.87839508]
 [82.85001373]
 [82.8214798 ]
 [82.79273987]
 [82.76378632]].
[2019-04-04 02:54:01,463] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.9331566e-20 3.5255872e-13 1.0637238e-14 1.0000000e+00 7.8111178e-18
 4.2628705e-09 6.9266954e-15], sum to 1.0000
[2019-04-04 02:54:01,464] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6184
[2019-04-04 02:54:01,478] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.65, 98.0, 0.0, 0.0, 26.0, 25.49121151068973, 0.4654643124187166, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1492200.0000, 
sim time next is 1492800.0000, 
raw observation next is [1.466666666666667, 98.66666666666666, 0.0, 0.0, 26.0, 25.51645790243685, 0.46793714045677, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.5032317636195753, 0.9866666666666666, 0.0, 0.0, 0.6666666666666666, 0.6263714918697376, 0.6559790468189234, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.48409694], dtype=float32), -0.34416977]. 
=============================================
[2019-04-04 02:54:01,758] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.2988922e-25 8.1717851e-18 3.0587028e-18 1.0000000e+00 3.9261768e-19
 4.9667637e-14 4.5003537e-19], sum to 1.0000
[2019-04-04 02:54:01,758] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2807
[2019-04-04 02:54:01,775] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [13.9, 77.83333333333333, 0.0, 0.0, 26.0, 25.81587159087283, 0.5881625402342685, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1044600.0000, 
sim time next is 1045200.0000, 
raw observation next is [14.0, 77.66666666666667, 0.0, 0.0, 26.0, 25.83568028217674, 0.5848357069114534, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.8504155124653741, 0.7766666666666667, 0.0, 0.0, 0.6666666666666666, 0.6529733568480616, 0.6949452356371512, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.59563714], dtype=float32), 1.7148333]. 
=============================================
[2019-04-04 02:54:05,404] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.1441815e-19 2.0797915e-14 7.0569813e-15 9.9999976e-01 2.1814419e-17
 1.8845282e-07 1.4364952e-13], sum to 1.0000
[2019-04-04 02:54:05,405] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0311
[2019-04-04 02:54:05,463] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.716666666666667, 77.5, 0.0, 0.0, 26.0, 24.05541542574221, 0.01605324662404807, 0.0, 1.0, 45072.13300444961], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1907400.0000, 
sim time next is 1908000.0000, 
raw observation next is [-7.8, 78.0, 0.0, 0.0, 26.0, 24.0264268758637, 0.009424275265271448, 0.0, 1.0, 44998.02548127401], 
processed observation next is [1.0, 0.08695652173913043, 0.24653739612188366, 0.78, 0.0, 0.0, 0.6666666666666666, 0.5022022396553082, 0.5031414250884237, 0.0, 1.0, 0.2142763118155905], 
reward next is 0.7857, 
noisyNet noise sample is [array([-0.51088196], dtype=float32), -0.8554185]. 
=============================================
[2019-04-04 02:54:05,472] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[79.54188 ]
 [79.66146 ]
 [79.811554]
 [79.91416 ]
 [79.92842 ]], R is [[79.37221527]
 [79.36386108]
 [79.3552475 ]
 [79.34649658]
 [79.33778381]].
[2019-04-04 02:54:09,206] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.9606210e-27 4.2369269e-26 4.9739649e-23 1.0000000e+00 2.5723443e-28
 3.3210484e-20 1.5575106e-22], sum to 1.0000
[2019-04-04 02:54:09,214] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4572
[2019-04-04 02:54:09,230] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.68656230913465, 0.218235372340723, 0.0, 1.0, 39305.71535042136], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2342400.0000, 
sim time next is 2343000.0000, 
raw observation next is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.64510599658057, 0.210787301096035, 0.0, 1.0, 39424.30293271602], 
processed observation next is [0.0, 0.08695652173913043, 0.3988919667590028, 0.62, 0.0, 0.0, 0.6666666666666666, 0.5537588330483807, 0.5702624336986784, 0.0, 1.0, 0.18773477587007628], 
reward next is 0.8123, 
noisyNet noise sample is [array([-0.59301865], dtype=float32), -0.37028]. 
=============================================
[2019-04-04 02:54:09,238] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[81.36857]
 [81.30256]
 [81.14986]
 [81.03867]
 [80.86439]], R is [[81.51617432]
 [81.51383972]
 [81.51213074]
 [81.51094055]
 [81.51013184]].
[2019-04-04 02:54:18,789] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.8281287e-21 7.9237550e-17 2.3562184e-18 1.0000000e+00 2.0964406e-17
 3.3547103e-11 2.2164473e-16], sum to 1.0000
[2019-04-04 02:54:18,789] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4047
[2019-04-04 02:54:18,833] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.5, 44.33333333333334, 207.3333333333333, 143.5, 26.0, 26.47811059535823, 0.5399625336860265, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2643600.0000, 
sim time next is 2644200.0000, 
raw observation next is [0.5, 45.0, 216.0, 130.0, 26.0, 26.4213600279021, 0.4347133413525248, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.4764542936288089, 0.45, 0.72, 0.143646408839779, 0.6666666666666666, 0.701780002325175, 0.6449044471175083, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.01125128], dtype=float32), -1.1738791]. 
=============================================
[2019-04-04 02:54:25,492] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.8982644e-24 2.2385900e-17 1.5629314e-16 1.0000000e+00 2.0931292e-20
 1.6113786e-09 1.8298119e-16], sum to 1.0000
[2019-04-04 02:54:25,492] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4716
[2019-04-04 02:54:25,508] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.333333333333333, 97.66666666666667, 0.0, 0.0, 26.0, 24.77493703466548, 0.2199079629283535, 0.0, 1.0, 55431.75989944638], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2874000.0000, 
sim time next is 2874600.0000, 
raw observation next is [1.5, 96.5, 0.0, 0.0, 26.0, 24.73168228020796, 0.2261876887584386, 0.0, 1.0, 55379.38884209505], 
processed observation next is [1.0, 0.2608695652173913, 0.5041551246537397, 0.965, 0.0, 0.0, 0.6666666666666666, 0.5609735233506633, 0.5753958962528128, 0.0, 1.0, 0.2637113754385479], 
reward next is 0.7363, 
noisyNet noise sample is [array([-0.05949992], dtype=float32), 0.019348942]. 
=============================================
[2019-04-04 02:54:26,945] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.4830424e-23 2.5489421e-19 1.2382532e-18 1.0000000e+00 1.8274575e-20
 2.5486094e-13 4.8813060e-19], sum to 1.0000
[2019-04-04 02:54:26,948] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2133
[2019-04-04 02:54:26,986] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.733333333333333, 28.66666666666667, 178.8333333333333, 405.3333333333334, 26.0, 25.29429824226028, 0.3137859737820446, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2553600.0000, 
sim time next is 2554200.0000, 
raw observation next is [3.0, 28.0, 171.0, 482.0, 26.0, 25.41135616122699, 0.3418395023656134, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5457063711911359, 0.28, 0.57, 0.532596685082873, 0.6666666666666666, 0.6176130134355825, 0.6139465007885377, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2298123], dtype=float32), -2.4190388]. 
=============================================
[2019-04-04 02:54:29,868] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.7461609e-21 4.6109689e-16 3.6760188e-15 1.0000000e+00 2.3485715e-19
 5.4908639e-10 2.1009261e-15], sum to 1.0000
[2019-04-04 02:54:29,869] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3235
[2019-04-04 02:54:29,884] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.0, 68.0, 0.0, 0.0, 26.0, 24.7613511152143, 0.2401536372024166, 0.0, 1.0, 41810.02925418613], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2595600.0000, 
sim time next is 2596200.0000, 
raw observation next is [-5.0, 69.0, 0.0, 0.0, 26.0, 24.77656847714437, 0.2452720095896395, 0.0, 1.0, 41773.48411053811], 
processed observation next is [1.0, 0.043478260869565216, 0.32409972299168976, 0.69, 0.0, 0.0, 0.6666666666666666, 0.5647140397620308, 0.5817573365298798, 0.0, 1.0, 0.19892135290732435], 
reward next is 0.8011, 
noisyNet noise sample is [array([0.65397626], dtype=float32), 0.534313]. 
=============================================
[2019-04-04 02:54:32,963] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.5003124e-22 4.9396681e-14 8.7264484e-16 1.0000000e+00 3.3427209e-19
 1.3759231e-10 5.1021644e-16], sum to 1.0000
[2019-04-04 02:54:32,963] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2009
[2019-04-04 02:54:33,019] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 62.0, 0.0, 0.0, 26.0, 25.22959337072342, 0.356547259243625, 0.0, 1.0, 45819.4685473182], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2847600.0000, 
sim time next is 2848200.0000, 
raw observation next is [1.833333333333333, 63.66666666666667, 0.0, 0.0, 26.0, 25.23039111333417, 0.3548893371747357, 0.0, 1.0, 43937.55720996382], 
processed observation next is [1.0, 1.0, 0.5133887349953832, 0.6366666666666667, 0.0, 0.0, 0.6666666666666666, 0.6025325927778477, 0.6182964457249119, 0.0, 1.0, 0.20922646290458963], 
reward next is 0.7908, 
noisyNet noise sample is [array([-0.04150201], dtype=float32), 0.09611128]. 
=============================================
[2019-04-04 02:54:35,690] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.82277562e-20 1.08911544e-13 3.18613074e-15 9.99997616e-01
 3.40411027e-17 2.36457390e-06 6.44421040e-14], sum to 1.0000
[2019-04-04 02:54:35,691] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2759
[2019-04-04 02:54:35,756] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 75.5, 0.0, 0.0, 26.0, 25.1286031786817, 0.3093905851550491, 0.0, 1.0, 51576.67269339222], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2856600.0000, 
sim time next is 2857200.0000, 
raw observation next is [1.0, 76.66666666666667, 0.0, 0.0, 26.0, 25.08888653138186, 0.3073783117112457, 0.0, 1.0, 54058.57611203325], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 0.7666666666666667, 0.0, 0.0, 0.6666666666666666, 0.5907405442818217, 0.6024594372370818, 0.0, 1.0, 0.25742179100968215], 
reward next is 0.7426, 
noisyNet noise sample is [array([0.87932056], dtype=float32), 2.0014765]. 
=============================================
[2019-04-04 02:54:37,410] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.3071487e-24 8.5691073e-21 4.4631448e-22 1.0000000e+00 7.7893092e-25
 1.1596949e-19 1.6802879e-18], sum to 1.0000
[2019-04-04 02:54:37,410] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5917
[2019-04-04 02:54:37,439] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.96197189914403, 0.2769486754152713, 0.0, 1.0, 38224.09631744404], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3021000.0000, 
sim time next is 3021600.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.93079207444839, 0.27522046162136, 0.0, 1.0, 38150.93489300747], 
processed observation next is [0.0, 1.0, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5775660062040325, 0.5917401538737866, 0.0, 1.0, 0.1816711185381308], 
reward next is 0.8183, 
noisyNet noise sample is [array([0.12581491], dtype=float32), 0.36069053]. 
=============================================
[2019-04-04 02:54:46,503] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.5045292e-33 2.1142975e-29 6.7707897e-28 1.0000000e+00 1.1566218e-31
 3.6520237e-28 8.0713082e-27], sum to 1.0000
[2019-04-04 02:54:46,503] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7736
[2019-04-04 02:54:46,570] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.833333333333333, 27.0, 0.0, 0.0, 26.0, 24.85019481742238, 0.2154226213128038, 0.0, 1.0, 69451.71815946595], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2482800.0000, 
sim time next is 2483400.0000, 
raw observation next is [1.466666666666667, 27.5, 0.0, 0.0, 26.0, 24.86629144183858, 0.2201419757471057, 0.0, 1.0, 46346.72886936788], 
processed observation next is [0.0, 0.7391304347826086, 0.5032317636195753, 0.275, 0.0, 0.0, 0.6666666666666666, 0.5721909534865484, 0.5733806585823685, 0.0, 1.0, 0.2206987089017518], 
reward next is 0.7793, 
noisyNet noise sample is [array([-1.1492463], dtype=float32), -0.539301]. 
=============================================
[2019-04-04 02:54:47,711] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.8434071e-27 2.9124211e-23 4.5272942e-22 1.0000000e+00 3.5639463e-26
 6.5522987e-19 1.4551146e-21], sum to 1.0000
[2019-04-04 02:54:47,711] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7548
[2019-04-04 02:54:47,757] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.01271922552511, 0.3186110432206296, 0.0, 1.0, 31303.62691809875], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3006000.0000, 
sim time next is 3006600.0000, 
raw observation next is [-2.166666666666667, 60.83333333333333, 0.0, 0.0, 26.0, 25.01283722678718, 0.3152489380493137, 0.0, 1.0, 36496.99847353961], 
processed observation next is [0.0, 0.8260869565217391, 0.4025854108956602, 0.6083333333333333, 0.0, 0.0, 0.6666666666666666, 0.584403102232265, 0.6050829793497713, 0.0, 1.0, 0.17379523082637907], 
reward next is 0.8262, 
noisyNet noise sample is [array([-1.5337621], dtype=float32), -0.21269721]. 
=============================================
[2019-04-04 02:54:51,130] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.6342460e-17 1.9312271e-12 5.7103074e-13 2.9722994e-02 2.0405323e-13
 9.7027695e-01 7.6619492e-12], sum to 1.0000
[2019-04-04 02:54:51,132] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4396
[2019-04-04 02:54:51,164] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 92.0, 93.0, 511.5, 26.0, 26.00474520930705, 0.6065171355283948, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3229200.0000, 
sim time next is 3229800.0000, 
raw observation next is [-3.0, 92.0, 95.66666666666667, 558.6666666666666, 26.0, 26.06041061040421, 0.6153473686605356, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.3795013850415513, 0.92, 0.3188888888888889, 0.6173112338858194, 0.6666666666666666, 0.671700884200351, 0.705115789553512, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.9098265], dtype=float32), 1.3654685]. 
=============================================
[2019-04-04 02:54:55,621] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.8817531e-19 3.8716771e-14 7.0727759e-15 1.0000000e+00 4.5775604e-18
 9.4310122e-11 3.2239372e-14], sum to 1.0000
[2019-04-04 02:54:55,621] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5427
[2019-04-04 02:54:55,648] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.2, 64.66666666666667, 0.0, 0.0, 26.0, 25.24205194951186, 0.4437616764978887, 0.0, 1.0, 52616.41466216017], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2667000.0000, 
sim time next is 2667600.0000, 
raw observation next is [-1.2, 65.0, 0.0, 0.0, 26.0, 25.36179083998189, 0.458798593364042, 0.0, 1.0, 46975.64148713777], 
processed observation next is [1.0, 0.9130434782608695, 0.42936288088642666, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6134825699984908, 0.6529328644546807, 0.0, 1.0, 0.22369353089113225], 
reward next is 0.7763, 
noisyNet noise sample is [array([-0.40865108], dtype=float32), -1.3955653]. 
=============================================
[2019-04-04 02:54:59,023] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.3521539e-21 7.6183109e-17 3.1578587e-16 1.0000000e+00 7.5799266e-18
 1.8938757e-09 9.9346728e-17], sum to 1.0000
[2019-04-04 02:54:59,024] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6964
[2019-04-04 02:54:59,089] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.8, 55.33333333333333, 65.16666666666666, 20.5, 26.0, 25.52367381196018, 0.2558577432452391, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2536800.0000, 
sim time next is 2537400.0000, 
raw observation next is [-2.8, 55.66666666666667, 79.33333333333333, 23.0, 26.0, 25.60649849694947, 0.2598739055484688, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.38504155124653744, 0.5566666666666668, 0.2644444444444444, 0.02541436464088398, 0.6666666666666666, 0.6338748747457892, 0.5866246351828229, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3682698], dtype=float32), 0.65990347]. 
=============================================
[2019-04-04 02:55:05,912] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.4281905e-23 2.6772271e-18 6.6909430e-19 1.0000000e+00 2.1071812e-20
 1.1667411e-12 5.5009256e-18], sum to 1.0000
[2019-04-04 02:55:05,913] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8311
[2019-04-04 02:55:05,940] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 50.0, 149.5, 635.5, 26.0, 26.05913435227354, 0.4733956091463847, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2804400.0000, 
sim time next is 2805000.0000, 
raw observation next is [-0.5, 49.00000000000001, 141.3333333333333, 678.0, 26.0, 26.0882872331301, 0.4734031711807393, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.44875346260387816, 0.49000000000000005, 0.471111111111111, 0.7491712707182321, 0.6666666666666666, 0.674023936094175, 0.6578010570602465, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9056012], dtype=float32), -1.680421]. 
=============================================
[2019-04-04 02:55:05,952] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[89.15066]
 [89.34718]
 [89.65497]
 [89.97817]
 [89.93237]], R is [[89.22727966]
 [89.33500671]
 [89.44165802]
 [89.54724121]
 [89.65177155]].
[2019-04-04 02:55:08,239] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.7849669e-21 3.4604851e-16 1.7029856e-15 1.0000000e+00 5.8350565e-18
 5.1457658e-11 1.3291026e-15], sum to 1.0000
[2019-04-04 02:55:08,240] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1537
[2019-04-04 02:55:08,310] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 77.83333333333334, 0.0, 0.0, 26.0, 25.10469382796147, 0.3105341175900072, 0.0, 1.0, 55518.90196338411], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2857800.0000, 
sim time next is 2858400.0000, 
raw observation next is [1.0, 79.0, 0.0, 0.0, 26.0, 25.1289162946618, 0.3028954090739129, 0.0, 1.0, 56061.75947364331], 
processed observation next is [1.0, 0.08695652173913043, 0.4903047091412743, 0.79, 0.0, 0.0, 0.6666666666666666, 0.5940763578884832, 0.600965136357971, 0.0, 1.0, 0.2669607593983015], 
reward next is 0.7330, 
noisyNet noise sample is [array([-0.4351928], dtype=float32), 0.5238942]. 
=============================================
[2019-04-04 02:55:16,098] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1565709e-20 3.6613768e-13 1.3411987e-15 1.0000000e+00 8.4831309e-17
 1.2659658e-09 1.2731519e-14], sum to 1.0000
[2019-04-04 02:55:16,098] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9467
[2019-04-04 02:55:16,153] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 92.0, 57.00000000000001, 274.0, 26.0, 25.56268467507601, 0.5169366687863101, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3226200.0000, 
sim time next is 3226800.0000, 
raw observation next is [-3.0, 92.0, 71.00000000000001, 322.0000000000001, 26.0, 25.54002076985434, 0.5277450619351058, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.3795013850415513, 0.92, 0.23666666666666672, 0.3558011049723758, 0.6666666666666666, 0.6283350641545283, 0.6759150206450353, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7681552], dtype=float32), -0.054289967]. 
=============================================
[2019-04-04 02:55:16,243] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.4335846e-22 5.4643932e-16 5.0876076e-16 1.0000000e+00 4.0797579e-20
 2.7599176e-09 1.1730049e-15], sum to 1.0000
[2019-04-04 02:55:16,243] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7327
[2019-04-04 02:55:16,266] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 25.21337841430329, 0.355640973472734, 0.0, 1.0, 46852.19417580065], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2759400.0000, 
sim time next is 2760000.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 25.11511302560866, 0.3404558095451227, 0.0, 1.0, 46245.77147808426], 
processed observation next is [1.0, 0.9565217391304348, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5929260854673885, 0.6134852698483743, 0.0, 1.0, 0.22021795941944886], 
reward next is 0.7798, 
noisyNet noise sample is [array([2.3239057], dtype=float32), -1.7568877]. 
=============================================
[2019-04-04 02:55:16,269] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[82.75831 ]
 [82.703026]
 [82.76998 ]
 [82.797134]
 [82.84307 ]], R is [[82.81581879]
 [82.76455688]
 [82.70935059]
 [82.65250397]
 [82.59259796]].
[2019-04-04 02:55:24,635] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.7124994e-27 1.7582516e-22 1.9043978e-21 1.0000000e+00 3.0900384e-26
 1.4576384e-19 1.9596560e-20], sum to 1.0000
[2019-04-04 02:55:24,638] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0534
[2019-04-04 02:55:24,696] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 71.0, 119.0, 90.5, 26.0, 25.2236385879445, 0.3571306387360334, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2970000.0000, 
sim time next is 2970600.0000, 
raw observation next is [-4.0, 71.0, 130.6666666666667, 104.3333333333333, 26.0, 25.29211531990352, 0.3538415795148058, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.3518005540166205, 0.71, 0.4355555555555557, 0.11528545119705337, 0.6666666666666666, 0.6076762766586267, 0.617947193171602, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8013003], dtype=float32), -0.33121508]. 
=============================================
[2019-04-04 02:55:25,473] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.9468152e-29 5.5288441e-25 2.5340138e-25 1.0000000e+00 1.2689124e-28
 2.0372337e-24 6.3386122e-24], sum to 1.0000
[2019-04-04 02:55:25,477] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2770
[2019-04-04 02:55:25,530] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 60.00000000000001, 0.0, 0.0, 26.0, 25.01585083953071, 0.3201553267733197, 0.0, 1.0, 55986.621912015], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3003600.0000, 
sim time next is 3004200.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.00593615242135, 0.3209331465011671, 0.0, 1.0, 48856.31405889126], 
processed observation next is [0.0, 0.782608695652174, 0.40720221606648205, 0.6, 0.0, 0.0, 0.6666666666666666, 0.5838280127017791, 0.606977715500389, 0.0, 1.0, 0.23264911456614884], 
reward next is 0.7674, 
noisyNet noise sample is [array([0.5656108], dtype=float32), 0.94869304]. 
=============================================
[2019-04-04 02:55:30,388] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.1838594e-33 7.0625073e-29 1.3352217e-29 1.0000000e+00 9.6910656e-33
 9.2066980e-25 8.1217136e-27], sum to 1.0000
[2019-04-04 02:55:30,388] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7864
[2019-04-04 02:55:30,404] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.8333333333333334, 45.33333333333334, 66.0, 548.3333333333334, 26.0, 25.19275291954856, 0.3531070423879348, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3082200.0000, 
sim time next is 3082800.0000, 
raw observation next is [0.6666666666666667, 50.66666666666667, 61.5, 517.1666666666666, 26.0, 25.16466024387455, 0.3430893263409026, 0.0, 1.0, 18689.6969084658], 
processed observation next is [0.0, 0.6956521739130435, 0.4810710987996307, 0.5066666666666667, 0.205, 0.5714548802946593, 0.6666666666666666, 0.5970550203228792, 0.6143631087803009, 0.0, 1.0, 0.08899855670698001], 
reward next is 0.9110, 
noisyNet noise sample is [array([-1.273377], dtype=float32), 0.6590058]. 
=============================================
[2019-04-04 02:55:38,242] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.0403220e-26 1.5404945e-20 4.7173509e-21 1.0000000e+00 1.6750549e-23
 9.9212605e-18 4.2441871e-20], sum to 1.0000
[2019-04-04 02:55:38,242] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5482
[2019-04-04 02:55:38,248] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.0, 54.0, 116.0, 805.5, 26.0, 25.94001250645113, 0.5579035194380165, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3330000.0000, 
sim time next is 3330600.0000, 
raw observation next is [-4.833333333333334, 53.33333333333333, 115.3333333333333, 803.6666666666666, 26.0, 25.95104447915572, 0.5578603606589753, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.32871652816251157, 0.5333333333333333, 0.3844444444444443, 0.8880294659300184, 0.6666666666666666, 0.6625870399296433, 0.6859534535529918, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0171434], dtype=float32), -1.0302607]. 
=============================================
[2019-04-04 02:55:43,365] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.5923648e-22 2.7958605e-17 1.3383616e-16 1.0000000e+00 9.8551694e-19
 9.5606123e-16 4.7563573e-16], sum to 1.0000
[2019-04-04 02:55:43,365] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1694
[2019-04-04 02:55:43,438] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.333333333333333, 50.33333333333333, 102.1666666666667, 705.8333333333334, 26.0, 26.12634814454171, 0.5482214834416038, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3922800.0000, 
sim time next is 3923400.0000, 
raw observation next is [-7.166666666666667, 49.66666666666667, 104.3333333333333, 719.6666666666666, 26.0, 26.2090262547071, 0.5582370080442628, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.26408125577100644, 0.4966666666666667, 0.3477777777777777, 0.7952117863720073, 0.6666666666666666, 0.6840855212255917, 0.6860790026814209, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5392978], dtype=float32), -0.39734253]. 
=============================================
[2019-04-04 02:55:44,906] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.1601322e-26 6.5368847e-22 6.5885314e-22 1.0000000e+00 2.9744567e-22
 1.4428405e-18 5.1257761e-21], sum to 1.0000
[2019-04-04 02:55:44,906] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6947
[2019-04-04 02:55:44,938] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.833333333333333, 59.5, 90.33333333333334, 727.6666666666666, 26.0, 26.42936866450767, 0.6505960505945, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3424200.0000, 
sim time next is 3424800.0000, 
raw observation next is [2.666666666666667, 61.0, 87.16666666666666, 715.8333333333334, 26.0, 26.51819091983966, 0.661395020105613, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5364727608494922, 0.61, 0.2905555555555555, 0.7909760589318601, 0.6666666666666666, 0.7098492433199718, 0.720465006701871, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.12614825], dtype=float32), -0.5721075]. 
=============================================
[2019-04-04 02:55:50,448] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.4316805e-24 8.7836409e-20 1.0721130e-19 1.0000000e+00 9.5558016e-23
 3.2953851e-13 2.6933081e-19], sum to 1.0000
[2019-04-04 02:55:50,448] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5121
[2019-04-04 02:55:50,472] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.19873746367361, 0.552704415594855, 0.0, 1.0, 99875.50022084423], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3530400.0000, 
sim time next is 3531000.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.36500874454775, 0.5901072010854086, 0.0, 1.0, 62148.65418008626], 
processed observation next is [1.0, 0.8695652173913043, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6137507287123126, 0.6967024003618029, 0.0, 1.0, 0.29594597228612507], 
reward next is 0.7041, 
noisyNet noise sample is [array([0.38478416], dtype=float32), 0.7694657]. 
=============================================
[2019-04-04 02:55:50,479] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[84.18441 ]
 [83.847916]
 [83.09714 ]
 [82.38807 ]
 [82.08366 ]], R is [[84.11494446]
 [83.79819489]
 [83.11786652]
 [82.34085846]
 [82.2080307 ]].
[2019-04-04 02:55:57,403] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.0926837e-26 1.3850739e-20 2.7454532e-20 1.0000000e+00 2.4039496e-23
 1.1990008e-14 4.2537663e-20], sum to 1.0000
[2019-04-04 02:55:57,403] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0544
[2019-04-04 02:55:57,454] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.333333333333333, 61.66666666666667, 16.16666666666666, 159.5, 26.0, 25.41388068294515, 0.3731621937688612, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3397200.0000, 
sim time next is 3397800.0000, 
raw observation next is [-2.166666666666667, 60.83333333333333, 30.33333333333333, 212.0, 26.0, 25.57676508304479, 0.3973324791445768, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.4025854108956602, 0.6083333333333333, 0.1011111111111111, 0.23425414364640884, 0.6666666666666666, 0.6313970902537326, 0.632444159714859, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5041267], dtype=float32), 1.6270474]. 
=============================================
[2019-04-04 02:56:04,061] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.0112684e-31 3.3579925e-27 2.1871924e-24 1.0000000e+00 3.6402653e-31
 1.8607258e-25 3.4863234e-23], sum to 1.0000
[2019-04-04 02:56:04,062] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1374
[2019-04-04 02:56:04,174] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.833333333333333, 54.16666666666667, 116.6666666666667, 820.6666666666667, 26.0, 25.17564284376854, 0.4457887155060735, 0.0, 1.0, 18712.06596275383], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3586200.0000, 
sim time next is 3586800.0000, 
raw observation next is [-2.666666666666667, 53.33333333333334, 117.3333333333333, 821.8333333333334, 26.0, 25.17564198231288, 0.447908498664477, 0.0, 1.0, 18711.4892679825], 
processed observation next is [0.0, 0.5217391304347826, 0.38873499538319484, 0.5333333333333334, 0.391111111111111, 0.9081031307550645, 0.6666666666666666, 0.5979701651927399, 0.649302832888159, 0.0, 1.0, 0.0891023298475357], 
reward next is 0.9109, 
noisyNet noise sample is [array([1.6041746], dtype=float32), -0.036558777]. 
=============================================
[2019-04-04 02:56:27,194] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [9.4467913e-28 2.3915558e-23 6.0193470e-23 1.0000000e+00 2.5314893e-27
 9.1657478e-22 1.5010415e-21], sum to 1.0000
[2019-04-04 02:56:27,194] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5455
[2019-04-04 02:56:27,256] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.166666666666667, 65.0, 0.0, 0.0, 26.0, 25.36911293878934, 0.3731274052614971, 0.0, 1.0, 42218.08415722125], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3010800.0000, 
sim time next is 3011400.0000, 
raw observation next is [-3.25, 65.0, 0.0, 0.0, 26.0, 25.35505236424162, 0.3686580071194108, 0.0, 1.0, 41307.352422677], 
processed observation next is [0.0, 0.8695652173913043, 0.3725761772853186, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6129210303534682, 0.622886002373137, 0.0, 1.0, 0.19670167820322382], 
reward next is 0.8033, 
noisyNet noise sample is [array([0.24597564], dtype=float32), 0.551731]. 
=============================================
[2019-04-04 02:56:31,485] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.1993848e-22 8.0904225e-19 1.4071777e-17 1.0000000e+00 2.3912688e-19
 1.7671394e-16 1.8456122e-17], sum to 1.0000
[2019-04-04 02:56:31,486] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2169
[2019-04-04 02:56:31,533] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 25.02301865983964, 0.4072415879319553, 1.0, 1.0, 48601.61830365285], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4563000.0000, 
sim time next is 4563600.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 24.98435523756951, 0.4083190973408691, 1.0, 1.0, 56792.0657906144], 
processed observation next is [1.0, 0.8260869565217391, 0.518005540166205, 0.52, 0.0, 0.0, 0.6666666666666666, 0.5820296031307924, 0.6361063657802897, 1.0, 1.0, 0.27043840852673523], 
reward next is 0.7296, 
noisyNet noise sample is [array([-0.5871997], dtype=float32), 0.4774735]. 
=============================================
[2019-04-04 02:56:32,139] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0378706e-21 3.3916361e-17 1.0813533e-14 1.0000000e+00 1.1448519e-18
 1.2883401e-12 1.2753599e-16], sum to 1.0000
[2019-04-04 02:56:32,139] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3439
[2019-04-04 02:56:32,190] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.15, 67.5, 0.0, 0.0, 26.0, 25.68395774329121, 0.5366186244964438, 0.0, 1.0, 56231.46012685144], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4422600.0000, 
sim time next is 4423200.0000, 
raw observation next is [4.033333333333333, 67.66666666666666, 0.0, 0.0, 26.0, 25.63057109039772, 0.5336810534874534, 0.0, 1.0, 70577.66326132415], 
processed observation next is [1.0, 0.17391304347826086, 0.5743305632502309, 0.6766666666666665, 0.0, 0.0, 0.6666666666666666, 0.6358809241998099, 0.6778936844958178, 0.0, 1.0, 0.33608411076821026], 
reward next is 0.6639, 
noisyNet noise sample is [array([-0.6614023], dtype=float32), -1.3565402]. 
=============================================
[2019-04-04 02:56:33,015] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.6141507e-19 2.1271937e-13 2.5516297e-13 9.9999988e-01 1.5125180e-16
 1.4640236e-07 1.5918550e-13], sum to 1.0000
[2019-04-04 02:56:33,015] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9044
[2019-04-04 02:56:33,054] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 82.66666666666667, 0.0, 0.0, 26.0, 25.09541648081549, 0.4686428930787401, 0.0, 1.0, 64647.94884862096], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2925600.0000, 
sim time next is 2926200.0000, 
raw observation next is [-1.0, 83.83333333333334, 0.0, 0.0, 26.0, 25.27370341180789, 0.4795635219917112, 0.0, 1.0, 50967.44600158265], 
processed observation next is [1.0, 0.8695652173913043, 0.4349030470914128, 0.8383333333333334, 0.0, 0.0, 0.6666666666666666, 0.6061419509839908, 0.6598545073305704, 0.0, 1.0, 0.24270212381706022], 
reward next is 0.7573, 
noisyNet noise sample is [array([0.885356], dtype=float32), 0.56594026]. 
=============================================
[2019-04-04 02:56:47,519] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.2299501e-23 5.4579951e-17 2.9813387e-16 1.0000000e+00 1.1265770e-20
 1.7191438e-11 6.5621105e-17], sum to 1.0000
[2019-04-04 02:56:47,519] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5368
[2019-04-04 02:56:47,669] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.5, 61.0, 6.0, 163.0, 26.0, 25.15349080208281, 0.3760308947122656, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3915000.0000, 
sim time next is 3915600.0000, 
raw observation next is [-7.666666666666666, 60.0, 20.16666666666666, 213.5, 26.0, 25.4495680017515, 0.4334689222626136, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.25023084025854114, 0.6, 0.0672222222222222, 0.23591160220994475, 0.6666666666666666, 0.6207973334792918, 0.6444896407542046, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3980069], dtype=float32), 0.24444479]. 
=============================================
[2019-04-04 02:56:51,591] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.6718533e-25 1.5200301e-18 2.4961152e-19 1.0000000e+00 1.9306416e-22
 1.3750112e-16 1.2803844e-18], sum to 1.0000
[2019-04-04 02:56:51,591] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6491
[2019-04-04 02:56:51,655] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.166666666666667, 72.0, 0.0, 0.0, 26.0, 24.87891969087406, 0.2837646603671368, 0.0, 1.0, 43895.2022870355], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3820200.0000, 
sim time next is 3820800.0000, 
raw observation next is [-4.333333333333334, 73.0, 0.0, 0.0, 26.0, 24.87511254118837, 0.2788298240208502, 0.0, 1.0, 43832.59129206888], 
processed observation next is [1.0, 0.21739130434782608, 0.3425669436749769, 0.73, 0.0, 0.0, 0.6666666666666666, 0.5729260450990307, 0.5929432746736167, 0.0, 1.0, 0.208726625200328], 
reward next is 0.7913, 
noisyNet noise sample is [array([1.1271145], dtype=float32), 0.95210886]. 
=============================================
[2019-04-04 02:56:58,858] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.2978356e-28 3.9774379e-23 3.5955800e-22 1.0000000e+00 9.2002588e-28
 1.6774051e-19 3.4340559e-22], sum to 1.0000
[2019-04-04 02:56:58,858] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6741
[2019-04-04 02:56:58,897] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.2, 93.0, 0.0, 0.0, 26.0, 23.94451709058494, 0.1098644016122228, 0.0, 1.0, 41838.28627752957], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4777200.0000, 
sim time next is 4777800.0000, 
raw observation next is [-6.166666666666667, 92.83333333333333, 0.0, 0.0, 26.0, 23.91174836676707, 0.1027464530255962, 0.0, 1.0, 41873.49583099969], 
processed observation next is [0.0, 0.30434782608695654, 0.2917820867959372, 0.9283333333333332, 0.0, 0.0, 0.6666666666666666, 0.49264569723058926, 0.5342488176751987, 0.0, 1.0, 0.1993975991952366], 
reward next is 0.8006, 
noisyNet noise sample is [array([-0.1603246], dtype=float32), 0.9696056]. 
=============================================
[2019-04-04 02:56:59,603] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.23025670e-22 1.10384566e-16 4.61420437e-17 1.00000000e+00
 3.57913296e-21 1.21620936e-09 2.22283878e-17], sum to 1.0000
[2019-04-04 02:56:59,603] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1105
[2019-04-04 02:56:59,616] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 55.83333333333334, 0.0, 0.0, 26.0, 25.6358032435081, 0.4723376959558251, 0.0, 1.0, 23011.2701639566], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3881400.0000, 
sim time next is 3882000.0000, 
raw observation next is [-1.0, 56.66666666666667, 0.0, 0.0, 26.0, 25.55273531205545, 0.452802165051499, 0.0, 1.0, 73097.73390735895], 
processed observation next is [1.0, 0.9565217391304348, 0.4349030470914128, 0.5666666666666668, 0.0, 0.0, 0.6666666666666666, 0.6293946093379542, 0.6509340550171664, 0.0, 1.0, 0.34808444717789977], 
reward next is 0.6519, 
noisyNet noise sample is [array([-1.3526739], dtype=float32), 0.63808745]. 
=============================================
[2019-04-04 02:56:59,654] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[81.408325]
 [81.27551 ]
 [81.241516]
 [81.22839 ]
 [81.14874 ]], R is [[81.5138855 ]
 [81.58916473]
 [81.77327728]
 [81.95554352]
 [82.13598633]].
[2019-04-04 02:56:59,767] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.0077383e-23 3.5480662e-16 6.9812513e-18 1.0000000e+00 1.3739853e-21
 1.9130976e-11 1.6337874e-17], sum to 1.0000
[2019-04-04 02:56:59,771] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7953
[2019-04-04 02:56:59,881] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 56.66666666666667, 0.0, 0.0, 26.0, 25.55273531205545, 0.452802165051499, 0.0, 1.0, 73097.73390735895], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3882000.0000, 
sim time next is 3882600.0000, 
raw observation next is [-1.0, 57.5, 0.0, 0.0, 26.0, 25.36992751533969, 0.4468633569191034, 0.0, 1.0, 149191.69982808], 
processed observation next is [1.0, 0.9565217391304348, 0.4349030470914128, 0.575, 0.0, 0.0, 0.6666666666666666, 0.6141606262783075, 0.6489544523063678, 0.0, 1.0, 0.7104366658479999], 
reward next is 0.2896, 
noisyNet noise sample is [array([-0.28682008], dtype=float32), -0.68389255]. 
=============================================
[2019-04-04 02:57:16,869] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.9951804e-29 3.9245026e-25 2.4281520e-23 1.0000000e+00 5.5778371e-27
 5.9350524e-23 5.5439504e-23], sum to 1.0000
[2019-04-04 02:57:16,869] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5192
[2019-04-04 02:57:16,926] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 49.0, 18.33333333333333, 8.833333333333332, 26.0, 25.32767760986342, 0.3176526049635957, 0.0, 1.0, 39126.87387941254], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4261200.0000, 
sim time next is 4261800.0000, 
raw observation next is [3.0, 49.0, 36.66666666666666, 17.66666666666666, 26.0, 25.32378069729394, 0.3213507069090958, 0.0, 1.0, 39077.93794209539], 
processed observation next is [0.0, 0.30434782608695654, 0.5457063711911359, 0.49, 0.12222222222222219, 0.01952117863720073, 0.6666666666666666, 0.6103150581078284, 0.607116902303032, 0.0, 1.0, 0.1860854187718828], 
reward next is 0.8139, 
noisyNet noise sample is [array([-1.169904], dtype=float32), -0.28177837]. 
=============================================
[2019-04-04 02:57:17,815] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.5725568e-23 2.8757538e-18 1.8007209e-17 1.0000000e+00 7.6961433e-20
 4.6234370e-17 4.9180735e-18], sum to 1.0000
[2019-04-04 02:57:17,815] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8572
[2019-04-04 02:57:17,904] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 25.02301846602548, 0.4072415074385731, 1.0, 1.0, 48601.54508000093], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4563000.0000, 
sim time next is 4563600.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 24.98435514315961, 0.4083190113301726, 1.0, 1.0, 56791.98030552644], 
processed observation next is [1.0, 0.8260869565217391, 0.518005540166205, 0.52, 0.0, 0.0, 0.6666666666666666, 0.5820295952633009, 0.6361063371100576, 1.0, 1.0, 0.27043800145488783], 
reward next is 0.7296, 
noisyNet noise sample is [array([1.1119802], dtype=float32), 0.5496578]. 
=============================================
[2019-04-04 02:57:19,989] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.7413878e-26 1.7203635e-19 1.5332949e-20 1.0000000e+00 6.5909846e-25
 8.3613939e-18 1.1744533e-19], sum to 1.0000
[2019-04-04 02:57:19,989] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6072
[2019-04-04 02:57:20,061] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.533333333333333, 75.33333333333334, 0.0, 0.0, 26.0, 25.52085177600966, 0.4213486038982484, 0.0, 1.0, 27926.64818760942], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4315200.0000, 
sim time next is 4315800.0000, 
raw observation next is [4.466666666666667, 75.16666666666666, 0.0, 0.0, 26.0, 25.55946789408356, 0.4195886445346815, 0.0, 1.0, 18743.18404797413], 
processed observation next is [0.0, 0.9565217391304348, 0.5863342566943676, 0.7516666666666666, 0.0, 0.0, 0.6666666666666666, 0.6299556578402967, 0.6398628815115605, 0.0, 1.0, 0.08925325737130538], 
reward next is 0.9107, 
noisyNet noise sample is [array([0.07789602], dtype=float32), 0.7582343]. 
=============================================
[2019-04-04 02:57:24,910] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2444555e-27 6.8869159e-21 3.0271252e-21 1.0000000e+00 5.0890153e-27
 4.6817523e-19 6.6528071e-22], sum to 1.0000
[2019-04-04 02:57:24,910] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9851
[2019-04-04 02:57:24,934] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 63.0, 0.0, 0.0, 26.0, 25.41466759906021, 0.4590589988975434, 0.0, 1.0, 29389.11038631316], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3700800.0000, 
sim time next is 3701400.0000, 
raw observation next is [2.833333333333333, 62.83333333333333, 0.0, 0.0, 26.0, 25.60286907506634, 0.4708168907860046, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.541089566020314, 0.6283333333333333, 0.0, 0.0, 0.6666666666666666, 0.6335724229221951, 0.6569389635953349, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.45930132], dtype=float32), -0.5056754]. 
=============================================
[2019-04-04 02:57:28,835] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.08377406e-20 4.41725903e-15 2.11157095e-16 1.00000000e+00
 3.05471493e-18 3.92298694e-12 1.92469963e-15], sum to 1.0000
[2019-04-04 02:57:28,836] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5107
[2019-04-04 02:57:28,904] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 50.0, 0.0, 0.0, 26.0, 25.51440186889772, 0.3123477763780044, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4933800.0000, 
sim time next is 4934400.0000, 
raw observation next is [-1.0, 50.0, 0.0, 0.0, 26.0, 25.41465261619032, 0.2967458433395336, 0.0, 1.0, 65475.59636810767], 
processed observation next is [1.0, 0.08695652173913043, 0.4349030470914128, 0.5, 0.0, 0.0, 0.6666666666666666, 0.6178877180158601, 0.5989152811131778, 0.0, 1.0, 0.31178855413384604], 
reward next is 0.6882, 
noisyNet noise sample is [array([1.3276061], dtype=float32), -0.65866214]. 
=============================================
[2019-04-04 02:57:33,185] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:57:33,185] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:57:33,192] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run29
[2019-04-04 02:57:35,558] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0006660e-26 1.0233325e-21 3.8734933e-22 1.0000000e+00 9.8775289e-26
 1.9510680e-15 7.4635667e-21], sum to 1.0000
[2019-04-04 02:57:35,562] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6162
[2019-04-04 02:57:35,573] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.433333333333334, 75.33333333333334, 0.0, 0.0, 26.0, 25.60243741560092, 0.4113903736852769, 0.0, 1.0, 18736.63910049286], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4317600.0000, 
sim time next is 4318200.0000, 
raw observation next is [4.45, 75.5, 0.0, 0.0, 26.0, 25.56264959637625, 0.4022868699700223, 0.0, 1.0, 34441.33884266952], 
processed observation next is [0.0, 1.0, 0.5858725761772854, 0.755, 0.0, 0.0, 0.6666666666666666, 0.6302207996980208, 0.6340956233233408, 0.0, 1.0, 0.1640063754412834], 
reward next is 0.8360, 
noisyNet noise sample is [array([-0.24093266], dtype=float32), -1.1726805]. 
=============================================
[2019-04-04 02:57:37,403] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.6573789e-33 6.4644765e-29 2.9346784e-25 1.0000000e+00 8.1216082e-30
 2.8557462e-27 3.5978914e-26], sum to 1.0000
[2019-04-04 02:57:37,403] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7480
[2019-04-04 02:57:37,433] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 26.0, 25.41505493849343, 0.3407607704023989, 0.0, 1.0, 39624.90611292307], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4242000.0000, 
sim time next is 4242600.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 26.0, 25.41165340330228, 0.3396343681697987, 0.0, 1.0, 40490.92241956653], 
processed observation next is [0.0, 0.08695652173913043, 0.5457063711911359, 0.45, 0.0, 0.0, 0.6666666666666666, 0.6176377836085235, 0.6132114560565995, 0.0, 1.0, 0.19281391628365013], 
reward next is 0.8072, 
noisyNet noise sample is [array([0.77232796], dtype=float32), 1.2852602]. 
=============================================
[2019-04-04 02:57:38,025] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.9001076e-22 1.3162414e-15 5.6142470e-15 1.0000000e+00 3.0045679e-19
 8.7850457e-13 1.1722668e-15], sum to 1.0000
[2019-04-04 02:57:38,025] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9033
[2019-04-04 02:57:38,091] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 71.0, 0.0, 0.0, 26.0, 25.25979086257642, 0.3957750444208741, 0.0, 1.0, 40828.88977315786], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4515000.0000, 
sim time next is 4515600.0000, 
raw observation next is [-1.0, 71.0, 0.0, 0.0, 26.0, 25.24035896959118, 0.3883089155377359, 0.0, 1.0, 40765.35280427132], 
processed observation next is [1.0, 0.2608695652173913, 0.4349030470914128, 0.71, 0.0, 0.0, 0.6666666666666666, 0.6033632474659317, 0.6294363051792453, 0.0, 1.0, 0.19412072763938726], 
reward next is 0.8059, 
noisyNet noise sample is [array([-0.5282634], dtype=float32), -0.18811265]. 
=============================================
[2019-04-04 02:57:43,940] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:57:43,940] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:57:43,957] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run29
[2019-04-04 02:57:47,437] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.3629953e-30 3.0179741e-26 1.1962307e-24 1.0000000e+00 2.4581730e-29
 4.4402024e-22 2.3789436e-24], sum to 1.0000
[2019-04-04 02:57:47,438] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8638
[2019-04-04 02:57:47,462] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.166666666666667, 51.66666666666667, 0.0, 0.0, 26.0, 25.40774846176443, 0.3842113222563097, 0.0, 1.0, 35306.68912851213], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3622200.0000, 
sim time next is 3622800.0000, 
raw observation next is [-2.333333333333333, 53.33333333333334, 0.0, 0.0, 26.0, 25.38914314293941, 0.3791828321416284, 0.0, 1.0, 46043.90986708747], 
processed observation next is [0.0, 0.9565217391304348, 0.3979686057248385, 0.5333333333333334, 0.0, 0.0, 0.6666666666666666, 0.6157619285782842, 0.6263942773805428, 0.0, 1.0, 0.21925671365279745], 
reward next is 0.7807, 
noisyNet noise sample is [array([0.29134578], dtype=float32), -1.0744749]. 
=============================================
[2019-04-04 02:57:47,509] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:57:47,509] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:57:47,526] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run29
[2019-04-04 02:57:57,438] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1723474e-15 8.2255017e-14 2.3589179e-13 1.0000000e+00 3.7503849e-13
 1.5396387e-10 1.5860046e-11], sum to 1.0000
[2019-04-04 02:57:57,438] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2370
[2019-04-04 02:57:57,487] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.25, 87.5, 0.0, 0.0, 19.0, 18.15725591059596, -1.167414118275946, 0.0, 1.0, 197528.8984568237], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 70200.0000, 
sim time next is 70800.0000, 
raw observation next is [3.066666666666667, 88.0, 0.0, 0.0, 19.0, 18.2295003639421, -1.110161708731561, 0.0, 1.0, 199144.4534676382], 
processed observation next is [0.0, 0.8260869565217391, 0.5475530932594646, 0.88, 0.0, 0.0, 0.08333333333333333, 0.01912503032850843, 0.12994609708947968, 0.0, 1.0, 0.9483069212744676], 
reward next is 0.0517, 
noisyNet noise sample is [array([0.8214795], dtype=float32), 1.5093408]. 
=============================================
[2019-04-04 02:58:03,012] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:58:03,019] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:58:03,022] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run29
[2019-04-04 02:58:15,898] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.4556116e-27 3.7979376e-21 3.2912073e-21 1.0000000e+00 6.6980694e-26
 1.9368662e-16 1.0431497e-21], sum to 1.0000
[2019-04-04 02:58:15,898] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5775
[2019-04-04 02:58:15,932] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.5, 76.0, 0.0, 0.0, 26.0, 25.4523640266377, 0.4106878611652727, 0.0, 1.0, 59718.21128338791], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4320000.0000, 
sim time next is 4320600.0000, 
raw observation next is [4.45, 75.83333333333334, 0.0, 0.0, 26.0, 25.49740761569257, 0.4161958151267966, 0.0, 1.0, 21908.44205403669], 
processed observation next is [1.0, 0.0, 0.5858725761772854, 0.7583333333333334, 0.0, 0.0, 0.6666666666666666, 0.6247839679743808, 0.6387319383755988, 0.0, 1.0, 0.10432591454303185], 
reward next is 0.8957, 
noisyNet noise sample is [array([0.54346466], dtype=float32), -0.85992974]. 
=============================================
[2019-04-04 02:58:17,309] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-04-04 02:58:17,326] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 02:58:17,327] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:58:17,339] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 02:58:17,339] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:58:17,341] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run39
[2019-04-04 02:58:17,382] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 02:58:17,384] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:58:17,388] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run39
[2019-04-04 02:58:17,413] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run39
[2019-04-04 03:00:04,325] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.85637754], dtype=float32), 0.056262642]
[2019-04-04 03:00:04,325] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-2.701961767333334, 63.48358402333334, 0.0, 0.0, 20.0, 19.80822762875103, -0.922691200837194, 0.0, 1.0, 0.0]
[2019-04-04 03:00:04,325] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 03:00:04,326] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [4.4502211e-16 1.3596407e-15 2.4267662e-13 1.0000000e+00 2.1233085e-15
 7.4683003e-12 1.8883649e-12], sampled 0.11473500160912042
[2019-04-04 03:00:34,952] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5764.1154 155034546.6189 -1788.3242
[2019-04-04 03:00:49,361] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5275.9049 181489267.1220 -2429.2178
[2019-04-04 03:00:58,098] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.85637754], dtype=float32), 0.056262642]
[2019-04-04 03:00:58,098] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [9.4678727335, 19.22636963, 0.0, 0.0, 20.0, 22.53503536021142, -0.2529150530834874, 0.0, 1.0, 0.0]
[2019-04-04 03:00:58,098] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 03:00:58,098] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.7616396e-17 6.6125182e-17 9.1721454e-15 1.0000000e+00 1.2241884e-15
 8.9765435e-14 4.5871842e-14], sampled 0.8306610767592729
[2019-04-04 03:00:58,215] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5268.2788 194208590.8254 -2349.3598
[2019-04-04 03:00:59,246] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 3800000, evaluation results [3800000.0, 5275.904862089485, 181489267.12202036, -2429.2177547133747, 5764.115390425701, 155034546.61886975, -1788.3241770510049, 5268.278799288253, 194208590.82538438, -2349.359764412906]
[2019-04-04 03:01:05,916] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.6554513e-25 3.5161376e-21 1.2839151e-19 1.0000000e+00 4.1951503e-23
 2.0483287e-18 3.0622971e-19], sum to 1.0000
[2019-04-04 03:01:05,933] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3629
[2019-04-04 03:01:05,941] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.666666666666666, 27.0, 0.0, 0.0, 26.0, 25.76539353434229, 0.5229746788711399, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4999200.0000, 
sim time next is 4999800.0000, 
raw observation next is [4.333333333333333, 28.0, 0.0, 0.0, 26.0, 25.69048092767626, 0.5221075206273086, 0.0, 1.0, 196217.9094192961], 
processed observation next is [1.0, 0.8695652173913043, 0.58264081255771, 0.28, 0.0, 0.0, 0.6666666666666666, 0.6408734106396882, 0.6740358402091028, 0.0, 1.0, 0.9343709972347434], 
reward next is 0.0656, 
noisyNet noise sample is [array([2.1993759], dtype=float32), -1.1972058]. 
=============================================
[2019-04-04 03:01:06,004] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:01:06,004] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:01:06,053] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run29
[2019-04-04 03:01:06,848] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:01:06,848] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:01:06,853] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run29
[2019-04-04 03:01:09,670] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:01:09,670] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:01:09,674] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run29
[2019-04-04 03:01:09,986] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.5141874e-28 2.5552733e-24 5.1269025e-24 1.0000000e+00 1.3024530e-27
 7.6498846e-23 4.4739842e-23], sum to 1.0000
[2019-04-04 03:01:09,986] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9025
[2019-04-04 03:01:10,014] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 38.66666666666667, 0.0, 0.0, 26.0, 25.53698485041682, 0.3993467956144763, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4911600.0000, 
sim time next is 4912200.0000, 
raw observation next is [1.0, 38.0, 0.0, 0.0, 26.0, 25.5436487648732, 0.3898638449528689, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.4903047091412743, 0.38, 0.0, 0.0, 0.6666666666666666, 0.6286373970727667, 0.6299546149842896, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.03992454], dtype=float32), 1.0141298]. 
=============================================
[2019-04-04 03:01:13,060] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.0166493e-18 2.5487624e-15 7.9330776e-15 1.0000000e+00 9.7485042e-17
 2.8551363e-12 2.5976459e-14], sum to 1.0000
[2019-04-04 03:01:13,060] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1281
[2019-04-04 03:01:13,089] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:01:13,089] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:01:13,101] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run29
[2019-04-04 03:01:13,128] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-9.583333333333334, 40.33333333333334, 0.0, 0.0, 24.0, 23.23111962084915, -0.1507213101688857, 1.0, 1.0, 60833.66664494321], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 414600.0000, 
sim time next is 415200.0000, 
raw observation next is [-9.666666666666668, 40.66666666666667, 0.0, 0.0, 24.0, 23.25290471842042, -0.1555574335471774, 1.0, 1.0, 53603.51667033975], 
processed observation next is [1.0, 0.8260869565217391, 0.19482917820867957, 0.40666666666666673, 0.0, 0.0, 0.5, 0.4377420598683684, 0.44814752215094084, 1.0, 1.0, 0.2552548412873321], 
reward next is 0.7447, 
noisyNet noise sample is [array([0.9516886], dtype=float32), 1.0751863]. 
=============================================
[2019-04-04 03:01:13,162] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:01:13,163] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:01:13,166] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run29
[2019-04-04 03:01:15,969] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:01:15,976] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:01:15,984] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run29
[2019-04-04 03:01:19,641] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.5098160e-19 1.2715555e-17 1.4490694e-16 1.0000000e+00 1.2336764e-16
 5.9606672e-16 1.2521339e-16], sum to 1.0000
[2019-04-04 03:01:19,658] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8525
[2019-04-04 03:01:19,693] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.9166666666666667, 34.5, 44.0, 0.0, 24.0, 23.67664705196534, -0.1718285756037693, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 489000.0000, 
sim time next is 489600.0000, 
raw observation next is [1.1, 34.0, 38.0, 0.0, 24.0, 23.71709250133999, -0.1673758284352964, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.49307479224376743, 0.34, 0.12666666666666668, 0.0, 0.5, 0.4764243751116659, 0.44420805718823453, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.87435085], dtype=float32), -1.5026479]. 
=============================================
[2019-04-04 03:01:20,562] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [8.2901792e-20 3.2346679e-17 2.4570279e-15 1.0000000e+00 2.5250776e-18
 1.3556944e-11 1.4759380e-15], sum to 1.0000
[2019-04-04 03:01:20,563] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0305
[2019-04-04 03:01:20,632] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-12.3, 63.0, 0.0, 0.0, 24.0, 23.54328794197205, -0.05826669486639446, 1.0, 1.0, 71653.38489279101], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 327600.0000, 
sim time next is 328200.0000, 
raw observation next is [-12.38333333333333, 64.16666666666667, 0.0, 0.0, 24.0, 23.6408605406308, -0.05053782254075421, 1.0, 1.0, 65429.69286518553], 
processed observation next is [1.0, 0.8260869565217391, 0.1195752539242845, 0.6416666666666667, 0.0, 0.0, 0.5, 0.4700717117192334, 0.4831540591530819, 1.0, 1.0, 0.311569966024693], 
reward next is 0.6884, 
noisyNet noise sample is [array([0.9383143], dtype=float32), -0.6448975]. 
=============================================
[2019-04-04 03:01:21,988] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:01:21,989] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:01:21,995] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run29
[2019-04-04 03:01:22,445] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:01:22,446] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:01:22,464] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run29
[2019-04-04 03:01:22,801] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:01:22,801] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:01:22,805] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run29
[2019-04-04 03:01:30,017] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.90279639e-20 3.42569241e-16 5.60195946e-14 1.00000000e+00
 1.44517684e-17 1.05329606e-10 1.93330273e-14], sum to 1.0000
[2019-04-04 03:01:30,041] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1256
[2019-04-04 03:01:30,068] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 24.0, 21.45363500403677, -0.5626255808891872, 0.0, 1.0, 45806.96611395788], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 191400.0000, 
sim time next is 192000.0000, 
raw observation next is [-8.900000000000002, 78.0, 0.0, 0.0, 24.0, 21.47466429546348, -0.5644724207056417, 0.0, 1.0, 45868.18953121254], 
processed observation next is [1.0, 0.21739130434782608, 0.2160664819944598, 0.78, 0.0, 0.0, 0.5, 0.28955535795529, 0.31184252643145277, 0.0, 1.0, 0.21841995014863114], 
reward next is 0.7816, 
noisyNet noise sample is [array([-1.4502914], dtype=float32), -0.46296936]. 
=============================================
[2019-04-04 03:01:30,073] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[84.344246]
 [84.315674]
 [84.276115]
 [84.226746]
 [84.170715]], R is [[84.30859375]
 [84.24737549]
 [84.18714905]
 [84.12787628]
 [84.06956482]].
[2019-04-04 03:01:47,092] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.4894754e-19 1.7447204e-16 3.2511863e-15 1.0000000e+00 1.1840045e-17
 8.3934450e-11 1.1407862e-15], sum to 1.0000
[2019-04-04 03:01:47,093] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1956
[2019-04-04 03:01:47,149] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 60.0, 66.16666666666667, 0.0, 24.0, 23.70411082008261, -0.1369834245626681, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 228000.0000, 
sim time next is 228600.0000, 
raw observation next is [-3.1, 60.5, 56.0, 0.0, 24.0, 23.73686396345196, -0.1381570566086723, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.37673130193905824, 0.605, 0.18666666666666668, 0.0, 0.5, 0.47807199695432995, 0.4539476477971092, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.15734707], dtype=float32), -0.18416867]. 
=============================================
[2019-04-04 03:01:50,852] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:01:50,853] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:01:50,864] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run29
[2019-04-04 03:01:53,358] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.7040843e-26 9.1289737e-21 2.3970867e-19 1.0000000e+00 1.6405336e-23
 3.4554706e-17 4.5470320e-19], sum to 1.0000
[2019-04-04 03:01:53,359] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7392
[2019-04-04 03:01:53,430] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.666666666666667, 48.66666666666667, 0.0, 0.0, 26.0, 24.87436627905667, 0.2415010874807087, 1.0, 1.0, 8336.977796167172], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4951200.0000, 
sim time next is 4951800.0000, 
raw observation next is [-2.5, 48.0, 0.0, 0.0, 26.0, 25.15228602957552, 0.2651845976489826, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.39335180055401664, 0.48, 0.0, 0.0, 0.6666666666666666, 0.59602383579796, 0.5883948658829942, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.22907232], dtype=float32), 1.1442301]. 
=============================================
[2019-04-04 03:02:03,118] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:02:03,118] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:02:03,122] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run29
[2019-04-04 03:02:04,927] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.9100946e-27 1.0225988e-21 1.5348933e-19 1.0000000e+00 4.3814740e-25
 5.3631400e-16 7.9276649e-22], sum to 1.0000
[2019-04-04 03:02:04,927] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4960
[2019-04-04 03:02:04,950] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.9, 97.33333333333333, 0.0, 0.0, 26.0, 25.31190309538698, 0.5921122105772354, 0.0, 1.0, 40687.06030235728], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1282800.0000, 
sim time next is 1283400.0000, 
raw observation next is [5.8, 98.0, 0.0, 0.0, 26.0, 25.40282212546247, 0.6046260735455818, 0.0, 1.0, 23650.24365148732], 
processed observation next is [0.0, 0.8695652173913043, 0.6232686980609419, 0.98, 0.0, 0.0, 0.6666666666666666, 0.6169018437885393, 0.7015420245151939, 0.0, 1.0, 0.11262020786422533], 
reward next is 0.8874, 
noisyNet noise sample is [array([-0.34680185], dtype=float32), -0.4148498]. 
=============================================
[2019-04-04 03:02:09,980] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.7097839e-21 5.7532081e-19 1.6628761e-15 1.0000000e+00 1.4012939e-20
 6.2774879e-14 3.0600712e-16], sum to 1.0000
[2019-04-04 03:02:09,990] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1684
[2019-04-04 03:02:10,025] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.9, 80.5, 0.0, 0.0, 23.0, 22.29380511660801, -0.3725598604907405, 0.0, 1.0, 44053.93052137464], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 613800.0000, 
sim time next is 614400.0000, 
raw observation next is [-3.899999999999999, 78.66666666666667, 0.0, 0.0, 23.0, 22.30402422874839, -0.3755934606146973, 0.0, 1.0, 44131.76604218144], 
processed observation next is [0.0, 0.08695652173913043, 0.35457063711911363, 0.7866666666666667, 0.0, 0.0, 0.4166666666666667, 0.35866868572903243, 0.3748021797951009, 0.0, 1.0, 0.2101512668675307], 
reward next is 0.7898, 
noisyNet noise sample is [array([-0.26349565], dtype=float32), -1.3580625]. 
=============================================
[2019-04-04 03:02:10,174] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:02:10,174] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:02:10,186] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run29
[2019-04-04 03:02:10,630] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.5877140e-23 3.3652098e-21 1.5021142e-19 1.0000000e+00 6.4156328e-23
 3.7520925e-18 6.3403213e-19], sum to 1.0000
[2019-04-04 03:02:10,630] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7680
[2019-04-04 03:02:10,676] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.933333333333333, 59.33333333333334, 170.3333333333333, 94.16666666666666, 24.0, 23.03921826409319, -0.1861187539488871, 0.0, 1.0, 23578.38217862555], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 652800.0000, 
sim time next is 653400.0000, 
raw observation next is [-1.75, 59.5, 182.0, 93.0, 24.0, 23.04213733047625, -0.1824642721058978, 0.0, 1.0, 23772.0313090928], 
processed observation next is [0.0, 0.5652173913043478, 0.4141274238227147, 0.595, 0.6066666666666667, 0.10276243093922652, 0.5, 0.4201781108730209, 0.4391785759647007, 0.0, 1.0, 0.11320014909091809], 
reward next is 0.8868, 
noisyNet noise sample is [array([0.06247605], dtype=float32), 0.07662093]. 
=============================================
[2019-04-04 03:02:14,996] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.7681459e-24 1.8297812e-19 2.3081793e-17 1.0000000e+00 7.2564858e-22
 8.7041775e-15 3.7873962e-18], sum to 1.0000
[2019-04-04 03:02:14,996] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1792
[2019-04-04 03:02:15,014] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 88.0, 0.0, 0.0, 26.0, 23.72196145511401, -0.03403429140365611, 0.0, 1.0, 41339.51194270408], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 540000.0000, 
sim time next is 540600.0000, 
raw observation next is [1.0, 88.66666666666667, 0.0, 0.0, 26.0, 23.69456931460129, -0.0391152811435711, 0.0, 1.0, 41364.16295556671], 
processed observation next is [0.0, 0.2608695652173913, 0.4903047091412743, 0.8866666666666667, 0.0, 0.0, 0.6666666666666666, 0.4745474428834407, 0.486961572952143, 0.0, 1.0, 0.19697220455031766], 
reward next is 0.8030, 
noisyNet noise sample is [array([0.86997503], dtype=float32), -0.9374875]. 
=============================================
[2019-04-04 03:02:25,230] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.7144493e-18 1.9979290e-13 1.2603636e-10 9.9994481e-01 7.0304670e-14
 5.5150929e-05 4.0465825e-12], sum to 1.0000
[2019-04-04 03:02:25,256] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8553
[2019-04-04 03:02:25,270] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.2, 81.66666666666667, 0.0, 0.0, 25.0, 22.981787652544, -0.1797857015105535, 0.0, 1.0, 44190.44634667553], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 99600.0000, 
sim time next is 100200.0000, 
raw observation next is [-3.3, 80.33333333333334, 0.0, 0.0, 25.0, 22.96258188366127, -0.1856213850011309, 0.0, 1.0, 44276.5400566582], 
processed observation next is [1.0, 0.13043478260869565, 0.37119113573407203, 0.8033333333333335, 0.0, 0.0, 0.5833333333333334, 0.4135484903051057, 0.43812620499962307, 0.0, 1.0, 0.21084066693646764], 
reward next is 0.7892, 
noisyNet noise sample is [array([-1.2762077], dtype=float32), -0.43834388]. 
=============================================
[2019-04-04 03:02:26,565] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.7472196e-23 2.0495803e-17 1.0546926e-15 1.0000000e+00 1.2351672e-19
 4.4538644e-12 1.1951319e-16], sum to 1.0000
[2019-04-04 03:02:26,567] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3212
[2019-04-04 03:02:26,635] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.7, 73.66666666666666, 10.66666666666666, 0.0, 26.0, 24.3846895847299, 0.15035503337356, 1.0, 1.0, 104175.5711940741], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 805800.0000, 
sim time next is 806400.0000, 
raw observation next is [-6.7, 75.0, 16.0, 0.0, 26.0, 24.86855672123097, 0.179052988105264, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.2770083102493075, 0.75, 0.05333333333333334, 0.0, 0.6666666666666666, 0.5723797267692475, 0.5596843293684214, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6215463], dtype=float32), 0.22110422]. 
=============================================
[2019-04-04 03:02:41,831] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.6506103e-20 2.6650154e-14 7.6628547e-15 1.0000000e+00 2.7002239e-17
 2.8436530e-08 8.4552904e-15], sum to 1.0000
[2019-04-04 03:02:41,831] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4456
[2019-04-04 03:02:41,856] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.166666666666667, 93.66666666666666, 0.0, 0.0, 26.0, 25.22273030048348, 0.4091454784827248, 0.0, 1.0, 38129.22551944583], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 951600.0000, 
sim time next is 952200.0000, 
raw observation next is [5.25, 92.5, 0.0, 0.0, 26.0, 25.27363856192785, 0.4093580080588792, 0.0, 1.0, 38048.65679839382], 
processed observation next is [1.0, 0.0, 0.60803324099723, 0.925, 0.0, 0.0, 0.6666666666666666, 0.6061365468273209, 0.6364526693529597, 0.0, 1.0, 0.18118407999235153], 
reward next is 0.8188, 
noisyNet noise sample is [array([-1.5035329], dtype=float32), 0.866408]. 
=============================================
[2019-04-04 03:02:43,090] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.5016830e-21 5.5248211e-16 4.3053949e-16 1.0000000e+00 2.1376392e-20
 1.0021411e-10 2.6633053e-16], sum to 1.0000
[2019-04-04 03:02:43,092] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5062
[2019-04-04 03:02:43,123] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 83.0, 0.0, 0.0, 26.0, 24.79744111422723, 0.2478638445756283, 0.0, 1.0, 41583.55188542858], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 856800.0000, 
sim time next is 857400.0000, 
raw observation next is [-3.3, 82.33333333333334, 0.0, 0.0, 26.0, 24.81944910249406, 0.2439342178985209, 0.0, 1.0, 41509.00274405611], 
processed observation next is [1.0, 0.9565217391304348, 0.37119113573407203, 0.8233333333333335, 0.0, 0.0, 0.6666666666666666, 0.5682874252078385, 0.5813114059661736, 0.0, 1.0, 0.19766191782883863], 
reward next is 0.8023, 
noisyNet noise sample is [array([-0.2161329], dtype=float32), 0.44265056]. 
=============================================
[2019-04-04 03:02:43,852] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.8234889e-20 1.1512953e-13 8.6616928e-13 9.9998701e-01 9.2846312e-17
 1.2936269e-05 1.1884753e-14], sum to 1.0000
[2019-04-04 03:02:43,854] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8658
[2019-04-04 03:02:43,878] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 26.0, 25.63437640624229, 0.5609333303813139, 0.0, 1.0, 72826.1409749801], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1035600.0000, 
sim time next is 1036200.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 26.0, 25.60761983071732, 0.5688305933775013, 0.0, 1.0, 58430.52900755512], 
processed observation next is [1.0, 1.0, 0.8614958448753465, 0.75, 0.0, 0.0, 0.6666666666666666, 0.6339683192264433, 0.6896101977925003, 0.0, 1.0, 0.278240614321691], 
reward next is 0.7218, 
noisyNet noise sample is [array([0.63741225], dtype=float32), -0.12427392]. 
=============================================
[2019-04-04 03:02:44,369] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.9999712e-33 1.1077527e-22 1.1647722e-25 1.0000000e+00 4.0182052e-29
 5.2608670e-18 3.7819484e-28], sum to 1.0000
[2019-04-04 03:02:44,370] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8884
[2019-04-04 03:02:44,375] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [18.1, 64.33333333333334, 0.0, 0.0, 26.0, 24.75802112080958, 0.4209053218418239, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1189200.0000, 
sim time next is 1189800.0000, 
raw observation next is [18.0, 65.0, 0.0, 0.0, 26.0, 24.77365392515712, 0.4177629008122175, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.782608695652174, 0.9612188365650972, 0.65, 0.0, 0.0, 0.6666666666666666, 0.56447116042976, 0.6392543002707392, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.75582206], dtype=float32), 2.0979352]. 
=============================================
[2019-04-04 03:02:44,472] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4050635e-33 2.4765990e-24 1.2353493e-29 1.0000000e+00 3.6589524e-29
 3.9390682e-09 4.7447846e-28], sum to 1.0000
[2019-04-04 03:02:44,475] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9171
[2019-04-04 03:02:44,479] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [15.0, 96.0, 0.0, 0.0, 26.0, 23.4669257165976, 0.1390458241312748, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1233000.0000, 
sim time next is 1233600.0000, 
raw observation next is [15.0, 96.0, 0.0, 0.0, 26.0, 23.44938604985875, 0.1345030996655983, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.2608695652173913, 0.8781163434903049, 0.96, 0.0, 0.0, 0.6666666666666666, 0.4541155041548957, 0.5448343665551995, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.25687712], dtype=float32), 0.7739275]. 
=============================================
[2019-04-04 03:02:47,900] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.4408781e-28 2.3302279e-23 7.9171134e-23 1.0000000e+00 5.8278505e-25
 4.1529681e-17 2.0806038e-23], sum to 1.0000
[2019-04-04 03:02:47,902] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4568
[2019-04-04 03:02:47,909] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [13.26666666666667, 100.0, 24.33333333333333, 0.0, 26.0, 24.60140497589487, 0.4195130726386987, 0.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1268400.0000, 
sim time next is 1269000.0000, 
raw observation next is [13.0, 100.0, 19.0, 0.0, 26.0, 24.59769507967463, 0.419227809047992, 0.0, 1.0, 22497.92558953888], 
processed observation next is [0.0, 0.6956521739130435, 0.8227146814404434, 1.0, 0.06333333333333334, 0.0, 0.6666666666666666, 0.5498079233062191, 0.6397426030159973, 0.0, 1.0, 0.1071329789978042], 
reward next is 0.8929, 
noisyNet noise sample is [array([0.65089124], dtype=float32), 0.13020165]. 
=============================================
[2019-04-04 03:02:47,930] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[93.33723 ]
 [93.578964]
 [93.74453 ]
 [93.75867 ]
 [93.79183 ]], R is [[92.99815369]
 [92.97922516]
 [92.96047974]
 [92.94192505]
 [92.92355347]].
[2019-04-04 03:02:58,465] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.08477143e-13 1.60190883e-08 3.84939995e-06 9.82661545e-03
 1.10036896e-10 9.90169466e-01 1.60194826e-08], sum to 1.0000
[2019-04-04 03:02:58,465] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6836
[2019-04-04 03:02:58,530] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.733333333333333, 77.66666666666667, 156.6666666666667, 288.1666666666667, 26.0, 25.7903000619702, 0.3405272423566604, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1938000.0000, 
sim time next is 1938600.0000, 
raw observation next is [-6.449999999999999, 77.0, 171.0, 236.0, 26.0, 25.84036607233224, 0.3358428916045828, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.28393351800554023, 0.77, 0.57, 0.26077348066298345, 0.6666666666666666, 0.65336383936102, 0.6119476305348609, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.01359804], dtype=float32), -0.344057]. 
=============================================
[2019-04-04 03:03:03,830] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.7021437e-20 5.9735542e-15 2.6729621e-14 1.0000000e+00 4.3445246e-18
 2.7160099e-09 2.9375607e-15], sum to 1.0000
[2019-04-04 03:03:03,834] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1769
[2019-04-04 03:03:03,855] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.966666666666667, 84.0, 0.0, 0.0, 26.0, 24.60967560527202, 0.1894728987348536, 0.0, 1.0, 40381.83905643939], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 535200.0000, 
sim time next is 535800.0000, 
raw observation next is [1.783333333333333, 84.5, 0.0, 0.0, 26.0, 24.5814374406829, 0.1844629848941279, 0.0, 1.0, 40465.45988026774], 
processed observation next is [0.0, 0.17391304347826086, 0.5120036934441367, 0.845, 0.0, 0.0, 0.6666666666666666, 0.5484531200569084, 0.561487661631376, 0.0, 1.0, 0.19269266609651306], 
reward next is 0.8073, 
noisyNet noise sample is [array([-0.65917397], dtype=float32), -0.61597157]. 
=============================================
[2019-04-04 03:03:11,378] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.0510729e-15 5.1388799e-08 1.3104924e-08 9.9768174e-01 9.1254313e-12
 2.3182563e-03 1.5175057e-09], sum to 1.0000
[2019-04-04 03:03:11,378] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3372
[2019-04-04 03:03:11,397] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 26.0, 25.29276110800377, 0.4808313296990818, 0.0, 1.0, 39518.18180665024], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1392600.0000, 
sim time next is 1393200.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 26.0, 25.34686002720957, 0.4877721785368156, 0.0, 1.0, 39477.44765183692], 
processed observation next is [1.0, 0.13043478260869565, 0.46260387811634357, 0.95, 0.0, 0.0, 0.6666666666666666, 0.6122383356007974, 0.6625907261789385, 0.0, 1.0, 0.1879878459611282], 
reward next is 0.8120, 
noisyNet noise sample is [array([-0.63065237], dtype=float32), 1.147884]. 
=============================================
[2019-04-04 03:03:14,374] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.0518592e-15 1.1934837e-07 1.7236301e-08 2.1767120e-01 3.8303489e-11
 7.8232867e-01 2.5873145e-10], sum to 1.0000
[2019-04-04 03:03:14,374] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9903
[2019-04-04 03:03:14,388] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.2, 92.33333333333334, 0.0, 0.0, 26.0, 25.36030932061622, 0.479919103192908, 0.0, 1.0, 44088.75173457859], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1476600.0000, 
sim time next is 1477200.0000, 
raw observation next is [2.2, 92.66666666666667, 0.0, 0.0, 26.0, 25.43465627181946, 0.4877381108746134, 0.0, 1.0, 18764.76563379408], 
processed observation next is [1.0, 0.08695652173913043, 0.5235457063711911, 0.9266666666666667, 0.0, 0.0, 0.6666666666666666, 0.6195546893182883, 0.6625793702915378, 0.0, 1.0, 0.08935602682759085], 
reward next is 0.9106, 
noisyNet noise sample is [array([-0.07656913], dtype=float32), -1.2393242]. 
=============================================
[2019-04-04 03:03:14,509] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.9765533e-13 7.6396365e-09 4.5684490e-07 4.7234058e-02 1.2994377e-07
 9.5276535e-01 1.1291938e-09], sum to 1.0000
[2019-04-04 03:03:14,525] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0283
[2019-04-04 03:03:14,547] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [13.71666666666667, 49.33333333333334, 100.3333333333333, 0.0, 26.0, 26.4794017765853, 0.7589638745406831, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1609800.0000, 
sim time next is 1610400.0000, 
raw observation next is [13.63333333333333, 49.66666666666667, 89.16666666666666, 0.0, 26.0, 26.79899943115907, 0.7920121608685701, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.840258541089566, 0.4966666666666667, 0.29722222222222217, 0.0, 0.6666666666666666, 0.7332499525965893, 0.7640040536228567, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.50907093], dtype=float32), -0.9568883]. 
=============================================
[2019-04-04 03:03:20,261] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.4145373e-18 1.9302131e-11 4.7250624e-11 4.4457724e-05 1.9400956e-13
 9.9995553e-01 8.1197419e-12], sum to 1.0000
[2019-04-04 03:03:20,262] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9958
[2019-04-04 03:03:20,269] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 26.0, 25.73132672884401, 0.5664968219973209, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1035000.0000, 
sim time next is 1035600.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 26.0, 25.6635216757087, 0.5663982133019438, 0.0, 1.0, 89739.25314483952], 
processed observation next is [1.0, 1.0, 0.8614958448753465, 0.75, 0.0, 0.0, 0.6666666666666666, 0.6386268063090584, 0.6887994044339812, 0.0, 1.0, 0.4273297768801882], 
reward next is 0.5727, 
noisyNet noise sample is [array([0.38899314], dtype=float32), 0.4951665]. 
=============================================
[2019-04-04 03:03:30,188] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.5925052e-24 2.6496453e-16 1.8282639e-15 1.4267550e-10 1.6872891e-18
 1.0000000e+00 2.4385744e-17], sum to 1.0000
[2019-04-04 03:03:30,194] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8798
[2019-04-04 03:03:30,212] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.200000000000001, 83.0, 0.0, 0.0, 26.0, 25.69729023963209, 0.4604586511074485, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 973200.0000, 
sim time next is 973800.0000, 
raw observation next is [9.4, 83.0, 0.0, 0.0, 26.0, 25.61747703473096, 0.4453063705934408, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.7229916897506927, 0.83, 0.0, 0.0, 0.6666666666666666, 0.6347897528942467, 0.6484354568644802, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4858839], dtype=float32), -0.1760932]. 
=============================================
[2019-04-04 03:03:33,118] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0729996e-16 1.3759660e-11 1.4620713e-10 9.9998975e-01 1.9056359e-13
 1.0279460e-05 2.2050832e-11], sum to 1.0000
[2019-04-04 03:03:33,119] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7442
[2019-04-04 03:03:33,134] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.1, 91.0, 0.0, 0.0, 26.0, 25.26968571264437, 0.4376310837330218, 0.0, 1.0, 42945.9751148573], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1737000.0000, 
sim time next is 1737600.0000, 
raw observation next is [0.06666666666666668, 91.0, 0.0, 0.0, 26.0, 25.27581958893827, 0.4334361361515692, 0.0, 1.0, 42958.01812971487], 
processed observation next is [0.0, 0.08695652173913043, 0.46445060018467227, 0.91, 0.0, 0.0, 0.6666666666666666, 0.6063182990781891, 0.644478712050523, 0.0, 1.0, 0.20456199109388032], 
reward next is 0.7954, 
noisyNet noise sample is [array([-0.23378755], dtype=float32), -1.2738961]. 
=============================================
[2019-04-04 03:03:35,256] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.5138757e-15 2.5555183e-08 1.9666417e-10 1.8344741e-08 1.0099547e-10
 1.0000000e+00 7.4735766e-11], sum to 1.0000
[2019-04-04 03:03:35,256] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6217
[2019-04-04 03:03:35,346] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.833333333333334, 81.33333333333334, 99.33333333333334, 496.8333333333333, 26.0, 25.65542318864983, 0.3052689306733842, 1.0, 1.0, 18738.19266249159], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1935600.0000, 
sim time next is 1936200.0000, 
raw observation next is [-7.566666666666666, 80.16666666666667, 113.6666666666667, 444.6666666666667, 26.0, 25.65660002841804, 0.3079859811525136, 1.0, 1.0, 20434.71137218988], 
processed observation next is [1.0, 0.391304347826087, 0.2530009233610342, 0.8016666666666667, 0.378888888888889, 0.4913443830570903, 0.6666666666666666, 0.63805000236817, 0.6026619937175045, 1.0, 1.0, 0.09730814939138037], 
reward next is 0.9027, 
noisyNet noise sample is [array([-1.9673603], dtype=float32), 1.359531]. 
=============================================
[2019-04-04 03:03:37,202] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.1687348e-22 7.0299425e-18 8.5086974e-18 1.0000000e+00 8.0795270e-21
 4.0867735e-17 1.9059869e-16], sum to 1.0000
[2019-04-04 03:03:37,202] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2164
[2019-04-04 03:03:37,239] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.35, 29.5, 0.0, 0.0, 26.0, 24.94139547101455, 0.2493861505611407, 0.0, 1.0, 106876.657649884], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2489400.0000, 
sim time next is 2490000.0000, 
raw observation next is [-0.4666666666666666, 29.33333333333334, 0.0, 0.0, 26.0, 25.0181164317201, 0.2650450381233177, 0.0, 1.0, 62842.33159333417], 
processed observation next is [0.0, 0.8260869565217391, 0.44967682363804257, 0.2933333333333334, 0.0, 0.0, 0.6666666666666666, 0.5848430359766749, 0.5883483460411059, 0.0, 1.0, 0.29924919806349604], 
reward next is 0.7008, 
noisyNet noise sample is [array([-1.2216188], dtype=float32), 1.015273]. 
=============================================
[2019-04-04 03:03:37,242] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[77.64981]
 [77.44179]
 [77.41574]
 [77.44934]
 [77.42427]], R is [[77.4814682 ]
 [77.19772339]
 [76.59914398]
 [76.66500092]
 [76.70256042]].
[2019-04-04 03:03:51,410] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.02372036e-13 5.38727618e-10 1.32892879e-08 1.34309055e-04
 6.30922092e-10 9.99865651e-01 9.21464904e-10], sum to 1.0000
[2019-04-04 03:03:51,425] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5511
[2019-04-04 03:03:51,465] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 62.0, 124.5, 0.0, 26.0, 25.74437321441011, 0.345198953842127, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1951200.0000, 
sim time next is 1951800.0000, 
raw observation next is [-3.3, 62.0, 120.3333333333333, 0.0, 26.0, 25.74159086570314, 0.3446942854039105, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.37119113573407203, 0.62, 0.401111111111111, 0.0, 0.6666666666666666, 0.6451325721419284, 0.6148980951346369, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5486001], dtype=float32), 1.311111]. 
=============================================
[2019-04-04 03:03:56,679] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.3248124e-15 4.1222087e-12 2.2644502e-10 2.1851420e-08 4.1291883e-12
 1.0000000e+00 3.9817945e-12], sum to 1.0000
[2019-04-04 03:03:56,680] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9928
[2019-04-04 03:03:56,741] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.8, 66.66666666666666, 145.0, 0.0, 26.0, 25.78978945496852, 0.4462565361754661, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2122800.0000, 
sim time next is 2123400.0000, 
raw observation next is [-5.7, 67.33333333333334, 141.0, 0.0, 26.0, 25.93524506187362, 0.3565488913827037, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.30470914127423826, 0.6733333333333335, 0.47, 0.0, 0.6666666666666666, 0.6612704218228016, 0.6188496304609012, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5758867], dtype=float32), 0.16542211]. 
=============================================
[2019-04-04 03:04:18,619] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.1322353e-20 4.9793577e-15 1.9635193e-14 1.0000000e+00 2.0563849e-18
 1.4543890e-08 3.1887648e-15], sum to 1.0000
[2019-04-04 03:04:18,623] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9178
[2019-04-04 03:04:18,741] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 61.5, 83.0, 359.0, 26.0, 24.05591560608956, 0.2163552327861788, 0.0, 1.0, 201798.4112089362], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3054600.0000, 
sim time next is 3055200.0000, 
raw observation next is [-6.0, 60.66666666666666, 85.66666666666667, 405.0, 26.0, 24.93317286702716, 0.2892126733306565, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.296398891966759, 0.6066666666666666, 0.28555555555555556, 0.44751381215469616, 0.6666666666666666, 0.5777644055855967, 0.5964042244435522, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8970153], dtype=float32), 1.581753]. 
=============================================
[2019-04-04 03:04:19,855] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.16525237e-16 2.36032270e-11 4.07883241e-11 6.48364797e-03
 1.12873419e-15 9.93516326e-01 1.01535004e-11], sum to 1.0000
[2019-04-04 03:04:19,855] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6445
[2019-04-04 03:04:19,916] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 82.0, 0.0, 0.0, 26.0, 25.00178633603126, 0.3222311942051726, 0.0, 1.0, 47204.67898310118], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1791000.0000, 
sim time next is 1791600.0000, 
raw observation next is [-3.899999999999999, 82.0, 0.0, 0.0, 26.0, 25.00029236924566, 0.3212893872910124, 0.0, 1.0, 46636.4540565619], 
processed observation next is [0.0, 0.7391304347826086, 0.35457063711911363, 0.82, 0.0, 0.0, 0.6666666666666666, 0.5833576974371383, 0.6070964624303374, 0.0, 1.0, 0.22207835265029477], 
reward next is 0.7779, 
noisyNet noise sample is [array([-1.0468705], dtype=float32), -1.0169629]. 
=============================================
[2019-04-04 03:04:21,397] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.2856648e-16 3.1441286e-10 1.6554273e-10 5.3327215e-01 1.6906552e-14
 4.6672788e-01 3.2165156e-11], sum to 1.0000
[2019-04-04 03:04:21,397] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6803
[2019-04-04 03:04:21,439] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.666666666666667, 63.33333333333334, 121.8333333333333, 781.8333333333334, 26.0, 25.09260238643319, 0.4094572685272949, 0.0, 1.0, 18722.71516424127], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2985600.0000, 
sim time next is 2986200.0000, 
raw observation next is [-2.5, 62.5, 110.0, 800.0, 26.0, 25.12333574342828, 0.4095234359292603, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.39335180055401664, 0.625, 0.36666666666666664, 0.8839779005524862, 0.6666666666666666, 0.5936113119523568, 0.6365078119764201, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([3.2481601], dtype=float32), 0.93213236]. 
=============================================
[2019-04-04 03:04:35,728] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.00705323e-19 1.93463071e-14 3.39864821e-14 3.03654020e-13
 5.03009986e-16 1.00000000e+00 8.14248166e-16], sum to 1.0000
[2019-04-04 03:04:35,729] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7518
[2019-04-04 03:04:35,752] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.3, 54.0, 234.5, 159.0, 26.0, 25.75085206437578, 0.3943845345266874, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2635200.0000, 
sim time next is 2635800.0000, 
raw observation next is [-2.016666666666667, 52.83333333333334, 238.0, 155.0, 26.0, 25.75377647030899, 0.3977683157459277, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.4067405355493998, 0.5283333333333334, 0.7933333333333333, 0.1712707182320442, 0.6666666666666666, 0.6461480391924157, 0.6325894385819759, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.44895795], dtype=float32), -0.44018412]. 
=============================================
[2019-04-04 03:04:35,802] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.4772589e-19 3.0063712e-13 2.5432159e-12 3.7297491e-12 4.2842952e-15
 1.0000000e+00 3.4906841e-14], sum to 1.0000
[2019-04-04 03:04:35,802] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4326
[2019-04-04 03:04:35,871] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.7, 75.0, 85.0, 45.5, 26.0, 25.89290430340298, 0.3562136872733008, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2624400.0000, 
sim time next is 2625000.0000, 
raw observation next is [-6.416666666666667, 73.33333333333333, 87.66666666666667, 60.66666666666667, 26.0, 25.83574978724129, 0.3530694577841842, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.2848568790397045, 0.7333333333333333, 0.2922222222222222, 0.06703499079189687, 0.6666666666666666, 0.6529791489367742, 0.6176898192613948, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.3618765], dtype=float32), 0.6816863]. 
=============================================
[2019-04-04 03:04:35,876] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[86.748276]
 [86.879906]
 [86.98595 ]
 [87.17015 ]
 [87.17061 ]], R is [[86.88115692]
 [87.01234436]
 [87.14221954]
 [87.27079773]
 [87.39809418]].
[2019-04-04 03:04:41,359] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.8808232e-16 5.4667659e-10 1.2367994e-10 3.2576003e-10 2.7662624e-13
 1.0000000e+00 8.3761734e-11], sum to 1.0000
[2019-04-04 03:04:41,359] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3930
[2019-04-04 03:04:41,463] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-15.0, 83.0, 0.0, 0.0, 26.0, 23.15408240817996, -0.008650667893776595, 1.0, 1.0, 203105.0743423545], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2705400.0000, 
sim time next is 2706000.0000, 
raw observation next is [-15.0, 83.0, 13.33333333333333, 54.99999999999999, 26.0, 23.95473501847939, 0.1049425189406661, 1.0, 1.0, 154107.0809003285], 
processed observation next is [1.0, 0.30434782608695654, 0.04709141274238226, 0.83, 0.04444444444444443, 0.060773480662983416, 0.6666666666666666, 0.4962279182066158, 0.5349808396468887, 1.0, 1.0, 0.7338432423825166], 
reward next is 0.2662, 
noisyNet noise sample is [array([-0.96527225], dtype=float32), -0.72868705]. 
=============================================
[2019-04-04 03:04:41,470] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[86.49784 ]
 [84.30349 ]
 [83.77013 ]
 [83.621704]
 [83.35758 ]], R is [[87.74356842]
 [86.89896393]
 [86.06626129]
 [85.99941254]
 [85.93350983]].
[2019-04-04 03:04:54,588] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.9033457e-20 3.2695686e-15 2.3823532e-13 2.5935606e-11 5.2438390e-17
 1.0000000e+00 2.8987553e-15], sum to 1.0000
[2019-04-04 03:04:54,588] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7826
[2019-04-04 03:04:54,605] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.5, 61.5, 0.0, 0.0, 26.0, 24.47663660495467, 0.1493314367720666, 0.0, 1.0, 40796.00831883346], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2781000.0000, 
sim time next is 2781600.0000, 
raw observation next is [-6.666666666666666, 62.33333333333333, 0.0, 0.0, 26.0, 24.48126057463639, 0.1409887690763818, 0.0, 1.0, 40847.32936789012], 
processed observation next is [1.0, 0.17391304347826086, 0.2779316712834719, 0.6233333333333333, 0.0, 0.0, 0.6666666666666666, 0.5401050478863659, 0.5469962563587939, 0.0, 1.0, 0.1945110922280482], 
reward next is 0.8055, 
noisyNet noise sample is [array([1.4621098], dtype=float32), -1.5516312]. 
=============================================
[2019-04-04 03:04:59,200] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2585428e-21 5.4450988e-18 2.4416101e-16 1.0000000e+00 1.2846885e-20
 4.3365051e-13 4.1557435e-16], sum to 1.0000
[2019-04-04 03:04:59,200] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5863
[2019-04-04 03:04:59,246] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.65, 60.5, 0.0, 0.0, 26.0, 23.2980636506052, -0.1346459962570317, 0.0, 1.0, 44251.6332628743], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2439000.0000, 
sim time next is 2439600.0000, 
raw observation next is [-8.733333333333334, 60.33333333333334, 0.0, 0.0, 26.0, 23.26352219754531, -0.1432525101515555, 0.0, 1.0, 44211.21258584638], 
processed observation next is [0.0, 0.21739130434782608, 0.22068328716528163, 0.6033333333333334, 0.0, 0.0, 0.6666666666666666, 0.4386268497954425, 0.4522491632828148, 0.0, 1.0, 0.2105295837421256], 
reward next is 0.7895, 
noisyNet noise sample is [array([-1.6444126], dtype=float32), 1.3509674]. 
=============================================
[2019-04-04 03:05:19,572] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.1197702e-24 2.1461799e-18 6.6069678e-18 1.6895442e-17 1.0007759e-20
 1.0000000e+00 4.7216971e-18], sum to 1.0000
[2019-04-04 03:05:19,573] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8936
[2019-04-04 03:05:19,651] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.833333333333333, 100.0, 0.0, 0.0, 26.0, 26.42656026544026, 0.771992629410258, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3179400.0000, 
sim time next is 3180000.0000, 
raw observation next is [3.666666666666667, 100.0, 0.0, 0.0, 26.0, 26.31150831207552, 0.7488577869451204, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.564173591874423, 1.0, 0.0, 0.0, 0.6666666666666666, 0.69262569267296, 0.7496192623150401, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.8855654], dtype=float32), 0.25574094]. 
=============================================
[2019-04-04 03:05:19,671] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[78.40192 ]
 [78.003746]
 [77.23849 ]
 [76.19613 ]
 [77.249916]], R is [[78.96554565]
 [79.17588806]
 [79.38413239]
 [79.59029388]
 [79.79439545]].
[2019-04-04 03:05:20,647] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.6176100e-20 8.8647034e-17 5.9266108e-15 9.9942720e-01 4.3521327e-19
 5.7288614e-04 4.5848513e-15], sum to 1.0000
[2019-04-04 03:05:20,647] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7194
[2019-04-04 03:05:20,683] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.0, 71.0, 0.0, 0.0, 26.0, 24.55835830929995, 0.1790914286299439, 0.0, 1.0, 38154.3526666366], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3029400.0000, 
sim time next is 3030000.0000, 
raw observation next is [-5.0, 71.0, 0.0, 0.0, 26.0, 24.51245616412816, 0.1692946960315893, 0.0, 1.0, 38262.24795522897], 
processed observation next is [0.0, 0.043478260869565216, 0.32409972299168976, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5427046803440133, 0.5564315653438631, 0.0, 1.0, 0.1822011807391856], 
reward next is 0.8178, 
noisyNet noise sample is [array([0.5248367], dtype=float32), 0.36992094]. 
=============================================
[2019-04-04 03:05:20,746] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[81.463646]
 [81.38613 ]
 [81.39446 ]
 [81.385155]
 [81.41247 ]], R is [[81.53449249]
 [81.53746033]
 [81.54067993]
 [81.54405212]
 [81.54788208]].
[2019-04-04 03:05:25,868] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.7637644e-23 4.2099258e-18 1.0207029e-17 1.0000000e+00 7.6912050e-22
 2.4535662e-12 1.3853727e-16], sum to 1.0000
[2019-04-04 03:05:25,868] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2177
[2019-04-04 03:05:25,895] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.8333333333333334, 41.5, 102.3333333333333, 785.3333333333334, 26.0, 25.11997753523782, 0.3624327287298327, 0.0, 1.0, 18707.01587987298], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3075000.0000, 
sim time next is 3075600.0000, 
raw observation next is [-0.6666666666666667, 41.0, 100.6666666666667, 780.1666666666667, 26.0, 25.12068626044752, 0.3625588434083071, 0.0, 1.0, 18705.36956875903], 
processed observation next is [0.0, 0.6086956521739131, 0.44413665743305636, 0.41, 0.33555555555555566, 0.8620626151012892, 0.6666666666666666, 0.59339052170396, 0.620852947802769, 0.0, 1.0, 0.08907318842266204], 
reward next is 0.9109, 
noisyNet noise sample is [array([0.46572563], dtype=float32), -1.154086]. 
=============================================
[2019-04-04 03:05:26,510] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.9251490e-17 2.1984245e-10 3.6776592e-11 1.2984228e-03 1.9083864e-13
 9.9870157e-01 4.5187487e-12], sum to 1.0000
[2019-04-04 03:05:26,511] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6134
[2019-04-04 03:05:26,520] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 49.0, 119.5, 822.5, 26.0, 26.38257915698065, 0.6102187996550852, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3931200.0000, 
sim time next is 3931800.0000, 
raw observation next is [-6.0, 48.33333333333334, 119.3333333333333, 826.6666666666667, 26.0, 26.34169313456568, 0.6117284456662058, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.296398891966759, 0.48333333333333345, 0.3977777777777777, 0.9134438305709025, 0.6666666666666666, 0.69514109454714, 0.7039094818887351, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4268764], dtype=float32), -0.9204177]. 
=============================================
[2019-04-04 03:05:30,781] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0207229e-16 1.7164572e-11 1.6025458e-12 7.0671207e-09 1.7584004e-13
 1.0000000e+00 3.3202258e-12], sum to 1.0000
[2019-04-04 03:05:30,782] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0409
[2019-04-04 03:05:30,792] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.333333333333333, 54.0, 117.3333333333333, 809.1666666666667, 26.0, 26.11295160414617, 0.5635285363396743, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3328800.0000, 
sim time next is 3329400.0000, 
raw observation next is [-5.166666666666667, 54.0, 116.6666666666667, 807.3333333333334, 26.0, 26.01433829730779, 0.5536264334954255, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.31948291782086796, 0.54, 0.388888888888889, 0.8920810313075507, 0.6666666666666666, 0.6678615247756493, 0.6845421444984753, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.36917976], dtype=float32), 1.2154291]. 
=============================================
[2019-04-04 03:05:36,209] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.7910246e-17 8.7956604e-12 5.6254255e-11 8.4268466e-02 2.1796370e-13
 9.1573149e-01 2.6176282e-11], sum to 1.0000
[2019-04-04 03:05:36,209] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0109
[2019-04-04 03:05:36,242] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.833333333333333, 20.33333333333334, 94.0, 739.3333333333334, 26.0, 26.70936475550857, 0.6853834446954611, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4029000.0000, 
sim time next is 4029600.0000, 
raw observation next is [-1.666666666666667, 20.66666666666667, 91.5, 725.6666666666667, 26.0, 26.78190683247479, 0.6945544868772188, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.4164358264081256, 0.20666666666666672, 0.305, 0.801841620626151, 0.6666666666666666, 0.7318255693728991, 0.7315181622924062, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.69922715], dtype=float32), 0.05946056]. 
=============================================
[2019-04-04 03:05:37,356] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.4009756e-20 4.0132225e-16 1.6415028e-13 1.0000000e+00 5.0007841e-18
 6.7111969e-09 2.1710686e-14], sum to 1.0000
[2019-04-04 03:05:37,359] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1525
[2019-04-04 03:05:37,377] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.833333333333333, 49.33333333333334, 0.0, 0.0, 26.0, 25.20433737007441, 0.3671116475942371, 0.0, 1.0, 39370.16052787632], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4157400.0000, 
sim time next is 4158000.0000, 
raw observation next is [-3.0, 50.0, 0.0, 0.0, 26.0, 25.17595096283303, 0.3623478521145461, 0.0, 1.0, 39412.51286622007], 
processed observation next is [0.0, 0.13043478260869565, 0.3795013850415513, 0.5, 0.0, 0.0, 0.6666666666666666, 0.5979959135694193, 0.6207826173715154, 0.0, 1.0, 0.18767863269628607], 
reward next is 0.8123, 
noisyNet noise sample is [array([-0.2415693], dtype=float32), 1.6519183]. 
=============================================
[2019-04-04 03:05:37,395] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[80.71611 ]
 [80.66159 ]
 [80.61698 ]
 [80.550766]
 [80.590805]], R is [[80.81787872]
 [80.82221985]
 [80.82675171]
 [80.83148956]
 [80.83644867]].
[2019-04-04 03:05:43,657] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.3070730e-17 7.7555253e-13 3.5003996e-13 3.9486084e-04 4.9142568e-14
 9.9960512e-01 3.1657409e-12], sum to 1.0000
[2019-04-04 03:05:43,657] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5980
[2019-04-04 03:05:43,737] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 79.00000000000001, 0.0, 0.0, 26.0, 25.03367212447237, 0.4427909838942885, 1.0, 1.0, 18704.54650603267], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3442200.0000, 
sim time next is 3442800.0000, 
raw observation next is [1.0, 79.0, 0.0, 0.0, 26.0, 25.07671868183039, 0.4569336439240259, 0.0, 1.0, 198590.6175434304], 
processed observation next is [1.0, 0.8695652173913043, 0.4903047091412743, 0.79, 0.0, 0.0, 0.6666666666666666, 0.589726556819199, 0.652311214641342, 0.0, 1.0, 0.9456696073496685], 
reward next is 0.0543, 
noisyNet noise sample is [array([-1.0075713], dtype=float32), -0.62564224]. 
=============================================
[2019-04-04 03:05:48,663] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-04 03:05:48,664] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 03:05:48,664] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 03:05:48,664] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:05:48,667] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run40
[2019-04-04 03:05:48,697] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 03:05:48,699] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:05:48,699] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:05:48,711] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run40
[2019-04-04 03:05:48,711] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run40
[2019-04-04 03:06:17,537] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.84050167], dtype=float32), -0.0013739711]
[2019-04-04 03:06:17,537] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-9.766666666666666, 57.66666666666667, 183.1666666666667, 337.0, 26.0, 25.60674942705966, 0.3501797545853862, 1.0, 1.0, 88854.1831732364]
[2019-04-04 03:06:17,537] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 03:06:17,538] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [2.8956990e-19 3.7622580e-16 9.4634407e-16 1.0000000e+00 1.0355982e-18
 3.6794321e-09 4.0239554e-15], sampled 0.8866083244273082
[2019-04-04 03:08:52,974] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.84050167], dtype=float32), -0.0013739711]
[2019-04-04 03:08:52,975] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [1.833333333333333, 48.5, 0.0, 0.0, 26.0, 25.61920344518761, 0.5403693612786581, 1.0, 1.0, 73963.41304692785]
[2019-04-04 03:08:52,975] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 03:08:52,976] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [5.8072267e-20 3.4012757e-17 3.0888872e-16 1.0000000e+00 4.8156689e-18
 3.0345185e-10 1.2469389e-15], sampled 0.942383092118197
[2019-04-04 03:09:01,712] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7357.3143 239123992.8184 1587.8454
[2019-04-04 03:09:20,528] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.84050167], dtype=float32), -0.0013739711]
[2019-04-04 03:09:20,529] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-4.687122759, 70.53962636, 39.879391675, 0.0, 26.0, 24.93436136401374, 0.3102434949533875, 1.0, 1.0, 101360.9171547783]
[2019-04-04 03:09:20,529] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 03:09:20,531] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.1943547e-19 1.2710761e-15 3.6876623e-14 9.9999952e-01 1.3405114e-17
 4.9883636e-07 2.3225876e-14], sampled 0.862943555341307
[2019-04-04 03:09:32,609] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.84050167], dtype=float32), -0.0013739711]
[2019-04-04 03:09:32,609] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-1.572767029333333, 64.13347565666668, 0.0, 0.0, 26.0, 24.66863947450039, 0.1869999497810864, 0.0, 1.0, 39108.83637016996]
[2019-04-04 03:09:32,609] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 03:09:32,610] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [5.1808528e-23 2.1527219e-19 1.9581794e-17 1.0000000e+00 1.6268436e-21
 4.1969427e-12 2.2727413e-17], sampled 0.2621016679351261
[2019-04-04 03:09:35,049] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7242.4125 263253376.8833 1555.8501
[2019-04-04 03:09:39,872] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7192.6291 273707897.4814 1119.6297
[2019-04-04 03:09:40,909] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 3900000, evaluation results [3900000.0, 7242.412491031753, 263253376.88332942, 1555.8500892173365, 7357.314319912365, 239123992.81840277, 1587.845370958397, 7192.629059612299, 273707897.48142016, 1119.6296787364013]
[2019-04-04 03:09:41,141] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.9348836e-18 2.7783552e-12 3.2224920e-13 3.1137588e-05 1.7343891e-14
 9.9996889e-01 9.1144201e-14], sum to 1.0000
[2019-04-04 03:09:41,141] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0621
[2019-04-04 03:09:41,173] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 49.0, 108.0, 790.5, 26.0, 26.47928137587965, 0.5327992282043911, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3420000.0000, 
sim time next is 3420600.0000, 
raw observation next is [3.0, 50.5, 106.3333333333333, 785.3333333333334, 26.0, 26.03711040806929, 0.5783848933662609, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.505, 0.35444444444444434, 0.8677716390423573, 0.6666666666666666, 0.6697592006724408, 0.6927949644554202, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1462355], dtype=float32), -0.9530064]. 
=============================================
[2019-04-04 03:09:52,710] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2640290e-19 1.6350057e-14 3.1513171e-13 9.1980672e-09 2.2647207e-15
 1.0000000e+00 2.5239898e-14], sum to 1.0000
[2019-04-04 03:09:52,713] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7440
[2019-04-04 03:09:52,740] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.8333333333333334, 67.83333333333334, 0.0, 0.0, 26.0, 25.33058434674924, 0.4013093743902682, 0.0, 1.0, 81214.32834796688], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3471000.0000, 
sim time next is 3471600.0000, 
raw observation next is [0.6666666666666667, 68.66666666666667, 0.0, 0.0, 26.0, 25.29074390496712, 0.3982820374359433, 0.0, 1.0, 59203.92789261929], 
processed observation next is [1.0, 0.17391304347826086, 0.4810710987996307, 0.6866666666666668, 0.0, 0.0, 0.6666666666666666, 0.6075619920805932, 0.6327606791453144, 0.0, 1.0, 0.28192346615533], 
reward next is 0.7181, 
noisyNet noise sample is [array([0.44598424], dtype=float32), -0.07304719]. 
=============================================
[2019-04-04 03:10:06,018] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.1546217e-19 1.1733005e-12 1.5989681e-12 6.6249333e-05 1.4529696e-16
 9.9993372e-01 2.8201765e-14], sum to 1.0000
[2019-04-04 03:10:06,018] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4194
[2019-04-04 03:10:06,040] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 26.0, 25.29401683201161, 0.4757486910619863, 0.0, 1.0, 50609.87768560493], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3279000.0000, 
sim time next is 3279600.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 26.0, 25.31550739448738, 0.4710526586977012, 0.0, 1.0, 45623.53773912128], 
processed observation next is [1.0, 1.0, 0.296398891966759, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6096256162072816, 0.6570175528992337, 0.0, 1.0, 0.21725494161486325], 
reward next is 0.7827, 
noisyNet noise sample is [array([-0.8193101], dtype=float32), -0.5007192]. 
=============================================
[2019-04-04 03:10:23,538] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.1667956e-16 7.3733124e-11 2.1130950e-10 4.1850290e-01 2.9168998e-12
 5.8149713e-01 2.7069666e-10], sum to 1.0000
[2019-04-04 03:10:23,539] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5956
[2019-04-04 03:10:23,546] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.166666666666667, 31.66666666666667, 109.3333333333333, 806.0, 26.0, 26.81507743028991, 0.712530339803211, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4111800.0000, 
sim time next is 4112400.0000, 
raw observation next is [3.333333333333333, 32.33333333333334, 107.6666666666667, 800.0, 26.0, 26.92466642098663, 0.7311094101318942, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5549399815327793, 0.3233333333333334, 0.358888888888889, 0.8839779005524862, 0.6666666666666666, 0.7437222017488857, 0.7437031367106314, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.43494794], dtype=float32), -0.037283167]. 
=============================================
[2019-04-04 03:10:32,326] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:10:32,327] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:10:32,365] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run30
[2019-04-04 03:10:37,349] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:10:37,349] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:10:37,359] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run30
[2019-04-04 03:10:38,121] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.4052875e-11 1.4342437e-05 7.9624334e-07 3.5912771e-02 3.8036362e-07
 9.6406955e-01 2.1611713e-06], sum to 1.0000
[2019-04-04 03:10:38,124] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2126
[2019-04-04 03:10:38,165] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 82.66666666666667, 73.33333333333334, 0.0, 26.0, 25.93254202517912, 0.5939526097578952, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4461600.0000, 
sim time next is 4462200.0000, 
raw observation next is [0.0, 81.5, 71.0, 0.0, 26.0, 26.11226511456195, 0.6046010457502553, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.46260387811634357, 0.815, 0.23666666666666666, 0.0, 0.6666666666666666, 0.6760220928801625, 0.7015336819167518, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.24790148], dtype=float32), 0.85897726]. 
=============================================
[2019-04-04 03:10:45,007] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:10:45,007] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:10:45,010] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run30
[2019-04-04 03:10:45,095] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.6163941e-20 3.4385858e-14 2.2855131e-13 5.1466840e-09 5.1769620e-15
 1.0000000e+00 1.3522137e-14], sum to 1.0000
[2019-04-04 03:10:45,095] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9653
[2019-04-04 03:10:45,097] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.4720538e-16 3.7324820e-11 1.1747051e-09 3.4521043e-01 5.6595753e-14
 6.5478951e-01 4.2751125e-10], sum to 1.0000
[2019-04-04 03:10:45,098] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3459
[2019-04-04 03:10:45,109] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.1, 31.66666666666666, 179.6666666666667, 524.1666666666667, 26.0, 27.63390311146646, 1.061111367867224, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4372800.0000, 
sim time next is 4373400.0000, 
raw observation next is [14.0, 31.83333333333333, 164.3333333333334, 419.3333333333334, 26.0, 28.11021820118559, 1.102555224788131, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.8504155124653741, 0.3183333333333333, 0.547777777777778, 0.46335174953959496, 0.6666666666666666, 0.8425181834321324, 0.8675184082627103, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.13318506], dtype=float32), -0.93196315]. 
=============================================
[2019-04-04 03:10:45,122] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.5, 71.0, 0.0, 0.0, 26.0, 25.17115531152762, 0.2714799060489981, 0.0, 1.0, 42381.40843331418], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3738600.0000, 
sim time next is 3739200.0000, 
raw observation next is [-3.666666666666667, 73.0, 0.0, 0.0, 26.0, 25.09361480701931, 0.2656069334297081, 0.0, 1.0, 42294.37200210661], 
processed observation next is [1.0, 0.2608695652173913, 0.3610341643582641, 0.73, 0.0, 0.0, 0.6666666666666666, 0.5911345672516092, 0.5885356444765694, 0.0, 1.0, 0.2014017714386029], 
reward next is 0.7986, 
noisyNet noise sample is [array([-0.6313218], dtype=float32), 0.89702994]. 
=============================================
[2019-04-04 03:10:46,516] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1399802e-16 5.2892310e-13 2.9083850e-11 1.2630705e-02 1.4467731e-13
 9.8736936e-01 1.9833047e-11], sum to 1.0000
[2019-04-04 03:10:46,518] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2045
[2019-04-04 03:10:46,530] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.833333333333333, 52.83333333333334, 0.0, 0.0, 26.0, 26.12979778076199, 0.6418544771163313, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4648200.0000, 
sim time next is 4648800.0000, 
raw observation next is [2.666666666666667, 52.66666666666667, 0.0, 0.0, 26.0, 26.05627241993977, 0.6236589344994877, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.5364727608494922, 0.5266666666666667, 0.0, 0.0, 0.6666666666666666, 0.6713560349949809, 0.7078863114998293, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.8369304], dtype=float32), 0.68535316]. 
=============================================
[2019-04-04 03:10:54,518] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:10:54,518] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:10:54,522] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run30
[2019-04-04 03:11:00,042] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.9103250e-16 2.3920648e-12 3.2272622e-11 9.9999738e-01 6.0243503e-12
 2.5728298e-06 2.7097247e-10], sum to 1.0000
[2019-04-04 03:11:00,043] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7318
[2019-04-04 03:11:00,075] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.4, 48.0, 33.33333333333334, 120.8333333333333, 26.0, 26.96375169486124, 0.6295270654953263, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4642800.0000, 
sim time next is 4643400.0000, 
raw observation next is [4.2, 48.5, 26.66666666666667, 96.66666666666667, 26.0, 26.73019745431639, 0.7494338801229751, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.5789473684210527, 0.485, 0.0888888888888889, 0.10681399631675875, 0.6666666666666666, 0.7275164545263658, 0.7498112933743251, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.27483076], dtype=float32), -0.38925883]. 
=============================================
[2019-04-04 03:11:11,392] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:11:11,392] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:11:11,396] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run30
[2019-04-04 03:11:12,407] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.7024208e-22 2.2011100e-18 2.6633244e-17 1.0000000e+00 8.1382997e-21
 2.9969649e-13 3.1698797e-16], sum to 1.0000
[2019-04-04 03:11:12,407] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0791
[2019-04-04 03:11:12,430] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 49.5, 0.0, 0.0, 26.0, 24.747630688253, 0.236480538346192, 0.0, 1.0, 39837.85545207234], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4170600.0000, 
sim time next is 4171200.0000, 
raw observation next is [-4.666666666666666, 49.33333333333333, 0.0, 0.0, 26.0, 24.71042144305057, 0.2276303424189823, 0.0, 1.0, 39940.70485348754], 
processed observation next is [0.0, 0.2608695652173913, 0.33333333333333337, 0.4933333333333333, 0.0, 0.0, 0.6666666666666666, 0.5592017869208809, 0.5758767808063274, 0.0, 1.0, 0.19019383263565498], 
reward next is 0.8098, 
noisyNet noise sample is [array([0.69792074], dtype=float32), -0.94250387]. 
=============================================
[2019-04-04 03:11:13,629] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:11:13,629] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:11:13,651] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run30
[2019-04-04 03:11:17,958] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:11:17,958] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:11:17,968] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run30
[2019-04-04 03:11:19,847] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.7207063e-14 1.7315416e-10 1.6184211e-08 9.8471642e-01 1.9151655e-10
 1.5283559e-02 7.8959494e-10], sum to 1.0000
[2019-04-04 03:11:19,848] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5260
[2019-04-04 03:11:19,894] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.1666666666666667, 70.16666666666667, 119.0, 22.0, 26.0, 26.17775125227169, 0.5282428520301398, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4525800.0000, 
sim time next is 4526400.0000, 
raw observation next is [0.3333333333333333, 68.33333333333334, 121.0, 11.0, 26.0, 26.20782564197388, 0.5247573111324654, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4718374884579871, 0.6833333333333335, 0.4033333333333333, 0.012154696132596685, 0.6666666666666666, 0.68398547016449, 0.6749191037108218, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.34266225], dtype=float32), 0.1130556]. 
=============================================
[2019-04-04 03:11:20,266] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:11:20,267] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:11:20,277] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run30
[2019-04-04 03:11:20,392] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:11:20,392] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:11:20,401] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run30
[2019-04-04 03:11:20,721] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:11:20,721] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:11:20,724] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run30
[2019-04-04 03:11:23,567] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.9753114e-17 2.6268943e-14 5.8071901e-11 6.0790229e-01 1.2700221e-13
 3.9209765e-01 4.4023327e-11], sum to 1.0000
[2019-04-04 03:11:23,569] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9546
[2019-04-04 03:11:23,702] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.699999999999999, 93.0, 78.5, 0.0, 24.0, 22.48143546822015, -0.2916984919496321, 0.0, 1.0, 51492.86313388494], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 42000.0000, 
sim time next is 42600.0000, 
raw observation next is [7.7, 93.0, 82.0, 0.0, 25.0, 22.49150747036271, -0.2493208688193134, 0.0, 1.0, 200349.2637493268], 
processed observation next is [0.0, 0.4782608695652174, 0.6759002770083103, 0.93, 0.2733333333333333, 0.0, 0.5833333333333334, 0.37429228919689245, 0.41689304372689556, 0.0, 1.0, 0.9540441130920324], 
reward next is 0.0460, 
noisyNet noise sample is [array([0.11892505], dtype=float32), -0.055545323]. 
=============================================
[2019-04-04 03:11:25,199] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:11:25,199] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:11:25,202] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run30
[2019-04-04 03:11:29,928] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:11:29,928] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:11:29,934] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run30
[2019-04-04 03:11:30,312] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:11:30,312] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:11:30,317] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run30
[2019-04-04 03:11:32,745] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.6895743e-11 2.3663096e-10 1.0946288e-07 3.6309853e-02 8.0654701e-08
 9.6368933e-01 5.6930639e-07], sum to 1.0000
[2019-04-04 03:11:32,745] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5106
[2019-04-04 03:11:32,796] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.45, 94.5, 0.0, 0.0, 24.0, 19.44597554989961, -0.8961735513837835, 0.0, 1.0, 68010.18599211487], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 12600.0000, 
sim time next is 13200.0000, 
raw observation next is [7.533333333333333, 94.0, 0.0, 0.0, 25.0, 19.8092294070669, -0.8617130444660983, 0.0, 1.0, 51589.77656477713], 
processed observation next is [0.0, 0.13043478260869565, 0.6712834718374886, 0.94, 0.0, 0.0, 0.5833333333333334, 0.1507691172555751, 0.21276231851130056, 0.0, 1.0, 0.2456656026894149], 
reward next is 0.7543, 
noisyNet noise sample is [array([-1.7085047], dtype=float32), -0.11719015]. 
=============================================
[2019-04-04 03:11:36,442] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.9701020e-16 1.7339793e-09 4.9154081e-09 6.1800218e-01 1.1382220e-12
 3.8199779e-01 5.3545240e-10], sum to 1.0000
[2019-04-04 03:11:36,442] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8797
[2019-04-04 03:11:36,518] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.466666666666667, 75.66666666666666, 0.0, 0.0, 26.0, 23.82492045155723, 0.01150152420779482, 0.0, 1.0, 43909.94212310956], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 103200.0000, 
sim time next is 103800.0000, 
raw observation next is [-4.733333333333333, 74.83333333333334, 0.0, 0.0, 26.0, 23.74044399801046, 0.0003936689963630706, 0.0, 1.0, 44022.29799663454], 
processed observation next is [1.0, 0.17391304347826086, 0.3314866112650046, 0.7483333333333334, 0.0, 0.0, 0.6666666666666666, 0.47837033316753824, 0.5001312229987877, 0.0, 1.0, 0.20962999046016445], 
reward next is 0.7904, 
noisyNet noise sample is [array([-2.9995065], dtype=float32), 0.20567676]. 
=============================================
[2019-04-04 03:11:39,130] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.8907623e-13 3.2804608e-09 2.6541265e-09 2.7046198e-01 7.2128570e-10
 7.2953808e-01 1.8917206e-09], sum to 1.0000
[2019-04-04 03:11:39,130] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4123
[2019-04-04 03:11:39,181] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.8, 71.83333333333334, 93.33333333333333, 12.0, 26.0, 25.35630963582375, 0.2558939572282383, 1.0, 1.0, 36531.23399913724], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 121800.0000, 
sim time next is 122400.0000, 
raw observation next is [-7.8, 74.0, 117.5, 18.0, 26.0, 25.31917816540653, 0.2561033509777847, 1.0, 1.0, 40337.85181090128], 
processed observation next is [1.0, 0.43478260869565216, 0.24653739612188366, 0.74, 0.39166666666666666, 0.019889502762430938, 0.6666666666666666, 0.6099315137838776, 0.5853677836592616, 1.0, 1.0, 0.19208500862333944], 
reward next is 0.8079, 
noisyNet noise sample is [array([-0.5903673], dtype=float32), -0.15247461]. 
=============================================
[2019-04-04 03:11:44,571] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.1887778e-19 2.2689081e-13 2.1129719e-13 9.2350042e-08 3.0863293e-17
 9.9999988e-01 2.3307628e-14], sum to 1.0000
[2019-04-04 03:11:44,571] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8699
[2019-04-04 03:11:44,599] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6, 91.0, 0.0, 0.0, 26.0, 24.33051414304401, 0.1375875785097319, 0.0, 1.0, 40798.82214318825], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 90000.0000, 
sim time next is 90600.0000, 
raw observation next is [-0.7833333333333333, 91.00000000000001, 0.0, 0.0, 26.0, 24.30545569595752, 0.1329324512187851, 0.0, 1.0, 41026.94450894653], 
processed observation next is [1.0, 0.043478260869565216, 0.44090489381348114, 0.9100000000000001, 0.0, 0.0, 0.6666666666666666, 0.5254546413297932, 0.5443108170729284, 0.0, 1.0, 0.1953664024235549], 
reward next is 0.8046, 
noisyNet noise sample is [array([-0.5572262], dtype=float32), 0.54623836]. 
=============================================
[2019-04-04 03:11:47,495] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.2997214e-17 8.7650834e-13 3.0449167e-13 6.7761377e-03 8.5208838e-14
 9.9322391e-01 1.1358123e-12], sum to 1.0000
[2019-04-04 03:11:47,495] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2632
[2019-04-04 03:11:47,514] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 25.11258837701162, 0.4000090421666863, 0.0, 1.0, 56525.77913063369], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4562400.0000, 
sim time next is 4563000.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 25.00272814483849, 0.397840875842244, 1.0, 1.0, 94339.92270221763], 
processed observation next is [1.0, 0.8260869565217391, 0.518005540166205, 0.52, 0.0, 0.0, 0.6666666666666666, 0.5835606787365407, 0.632613625280748, 1.0, 1.0, 0.4492377271534173], 
reward next is 0.5508, 
noisyNet noise sample is [array([0.31022865], dtype=float32), 0.82854694]. 
=============================================
[2019-04-04 03:11:47,539] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[88.802315]
 [89.88196 ]
 [89.83105 ]
 [89.860085]
 [89.81674 ]], R is [[88.93909454]
 [88.78053284]
 [88.71875763]
 [88.74262238]
 [88.85519409]].
[2019-04-04 03:11:48,610] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.7265957e-17 4.0516828e-13 8.5217895e-13 4.7777985e-07 3.2118381e-14
 9.9999952e-01 1.1313966e-11], sum to 1.0000
[2019-04-04 03:11:48,610] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2362
[2019-04-04 03:11:48,692] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.633333333333334, 73.66666666666667, 82.66666666666666, 0.0, 26.0, 25.65420956886587, 0.2896690158290309, 1.0, 1.0, 45494.67760683088], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 814800.0000, 
sim time next is 815400.0000, 
raw observation next is [-5.35, 73.0, 87.0, 0.0, 26.0, 25.62754094279604, 0.2884047665278017, 1.0, 1.0, 32932.99502153404], 
processed observation next is [1.0, 0.43478260869565216, 0.31440443213296404, 0.73, 0.29, 0.0, 0.6666666666666666, 0.6356284118996699, 0.5961349221759339, 1.0, 1.0, 0.15682378581682876], 
reward next is 0.8432, 
noisyNet noise sample is [array([0.50737447], dtype=float32), 0.9471969]. 
=============================================
[2019-04-04 03:12:00,600] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:12:00,600] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:12:00,606] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run30
[2019-04-04 03:12:07,045] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.5640457e-22 3.1868537e-16 1.7501329e-16 1.6684108e-07 6.0277259e-18
 9.9999988e-01 8.4955327e-17], sum to 1.0000
[2019-04-04 03:12:07,045] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4067
[2019-04-04 03:12:07,092] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.3, 76.0, 29.0, 0.0, 26.0, 25.62494445300053, 0.2864615629607245, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 721800.0000, 
sim time next is 722400.0000, 
raw observation next is [-2.3, 76.0, 41.0, 8.166666666666664, 26.0, 25.67476621901809, 0.2915726803333531, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.3988919667590028, 0.76, 0.13666666666666666, 0.009023941068139961, 0.6666666666666666, 0.6395638515848407, 0.5971908934444511, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.44976908], dtype=float32), -0.8053137]. 
=============================================
[2019-04-04 03:12:08,004] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.3184234e-18 2.3304018e-11 2.7545839e-13 3.1977084e-02 5.0456130e-14
 9.6802294e-01 3.3427337e-11], sum to 1.0000
[2019-04-04 03:12:08,005] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7677
[2019-04-04 03:12:08,037] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.333333333333334, 25.66666666666667, 94.83333333333334, 781.5, 26.0, 26.24953812827087, 0.7430075035408179, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4980000.0000, 
sim time next is 4980600.0000, 
raw observation next is [8.5, 25.5, 92.0, 774.0, 26.0, 26.79413114132213, 0.8118623465688682, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.698060941828255, 0.255, 0.30666666666666664, 0.8552486187845304, 0.6666666666666666, 0.7328442617768441, 0.7706207821896228, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3399879], dtype=float32), -1.3921455]. 
=============================================
[2019-04-04 03:12:15,664] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:12:15,665] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:12:15,673] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run30
[2019-04-04 03:12:15,712] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.7634163e-16 2.0776257e-10 8.1382145e-10 2.4461949e-01 1.7228245e-12
 7.5538051e-01 8.1339002e-10], sum to 1.0000
[2019-04-04 03:12:15,715] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5234
[2019-04-04 03:12:15,739] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.7, 81.5, 0.0, 0.0, 26.0, 25.47811302489054, 0.4507507145322864, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 963000.0000, 
sim time next is 963600.0000, 
raw observation next is [7.699999999999999, 82.0, 0.0, 0.0, 26.0, 25.46362703609255, 0.4467517289118129, 0.0, 1.0, 18759.29491225242], 
processed observation next is [1.0, 0.13043478260869565, 0.6759002770083103, 0.82, 0.0, 0.0, 0.6666666666666666, 0.6219689196743792, 0.6489172429706044, 0.0, 1.0, 0.08932997577263056], 
reward next is 0.9107, 
noisyNet noise sample is [array([-1.053936], dtype=float32), -0.3589962]. 
=============================================
[2019-04-04 03:12:16,148] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.3279485e-18 7.4471679e-13 8.5575609e-12 4.1651819e-02 6.0041258e-16
 9.5834821e-01 1.4519154e-12], sum to 1.0000
[2019-04-04 03:12:16,148] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1653
[2019-04-04 03:12:16,315] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 96.0, 0.0, 0.0, 26.0, 24.6978416778491, 0.1852384188855585, 0.0, 1.0, 120669.7114799904], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 501600.0000, 
sim time next is 502200.0000, 
raw observation next is [1.1, 96.0, 0.0, 0.0, 26.0, 24.67410757811509, 0.201042238398136, 1.0, 1.0, 91011.40149124987], 
processed observation next is [1.0, 0.8260869565217391, 0.49307479224376743, 0.96, 0.0, 0.0, 0.6666666666666666, 0.5561756315095909, 0.5670140794660453, 1.0, 1.0, 0.4333876261488089], 
reward next is 0.5666, 
noisyNet noise sample is [array([-0.29966345], dtype=float32), -1.130872]. 
=============================================
[2019-04-04 03:12:23,311] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.5526259e-22 5.0369855e-19 9.5388855e-19 1.0000000e+00 2.3509433e-22
 1.4847076e-15 9.6393375e-18], sum to 1.0000
[2019-04-04 03:12:23,312] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1295
[2019-04-04 03:12:23,335] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 67.5, 0.0, 0.0, 26.0, 23.77783858305145, 0.004768927132785136, 0.0, 1.0, 44319.10758337923], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 623400.0000, 
sim time next is 624000.0000, 
raw observation next is [-4.5, 67.0, 0.0, 0.0, 26.0, 23.77727664467591, 0.0002029891771586819, 0.0, 1.0, 44229.17045406556], 
processed observation next is [0.0, 0.21739130434782608, 0.3379501385041552, 0.67, 0.0, 0.0, 0.6666666666666666, 0.48143972038965926, 0.5000676630590529, 0.0, 1.0, 0.2106150974003122], 
reward next is 0.7894, 
noisyNet noise sample is [array([-0.73616916], dtype=float32), -1.0311942]. 
=============================================
[2019-04-04 03:12:23,362] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[81.363174]
 [81.33597 ]
 [81.33351 ]
 [81.33513 ]
 [81.3378  ]], R is [[81.36360931]
 [81.33892822]
 [81.31414032]
 [81.28938293]
 [81.26485443]].
[2019-04-04 03:12:25,054] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:12:25,054] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:12:25,060] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run30
[2019-04-04 03:12:26,988] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.1491909e-21 4.4156349e-18 5.4725736e-15 1.0000000e+00 1.3821716e-19
 2.2099500e-09 1.7015505e-15], sum to 1.0000
[2019-04-04 03:12:26,988] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0281
[2019-04-04 03:12:27,022] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 66.0, 0.0, 0.0, 26.0, 23.74107948950464, -0.01055963089886721, 0.0, 1.0, 44063.54311134569], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 625200.0000, 
sim time next is 625800.0000, 
raw observation next is [-4.5, 65.5, 0.0, 0.0, 26.0, 23.71671886372673, -0.01156362631348457, 0.0, 1.0, 43981.93982254898], 
processed observation next is [0.0, 0.21739130434782608, 0.3379501385041552, 0.655, 0.0, 0.0, 0.6666666666666666, 0.4763932386438941, 0.4961454578955051, 0.0, 1.0, 0.20943780867880465], 
reward next is 0.7906, 
noisyNet noise sample is [array([0.57065207], dtype=float32), 1.5157301]. 
=============================================
[2019-04-04 03:12:42,293] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1924131e-15 1.0373079e-10 9.4251267e-11 1.8753868e-04 3.9126372e-11
 9.9981254e-01 2.8166580e-10], sum to 1.0000
[2019-04-04 03:12:42,293] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1839
[2019-04-04 03:12:42,320] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.616666666666666, 93.66666666666666, 101.3333333333333, 0.0, 26.0, 25.16366112078432, 0.2544583232558674, 1.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 910200.0000, 
sim time next is 910800.0000, 
raw observation next is [3.8, 93.0, 100.0, 0.0, 26.0, 25.1604228344238, 0.2523167954571557, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5678670360110805, 0.93, 0.3333333333333333, 0.0, 0.6666666666666666, 0.5967019028686501, 0.5841055984857185, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8711711], dtype=float32), -0.69798523]. 
=============================================
[2019-04-04 03:12:48,580] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.9277754e-20 6.2102300e-15 1.8137871e-14 6.5383898e-07 8.6795574e-18
 9.9999940e-01 1.4419924e-14], sum to 1.0000
[2019-04-04 03:12:48,580] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9436
[2019-04-04 03:12:48,604] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.7, 67.0, 0.0, 0.0, 26.0, 23.46767383408311, -0.08359279268149143, 0.0, 1.0, 42028.50369278779], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 802800.0000, 
sim time next is 803400.0000, 
raw observation next is [-6.700000000000001, 68.33333333333334, 0.0, 0.0, 26.0, 23.44883142928254, -0.08719896587550972, 0.0, 1.0, 42001.8886392428], 
processed observation next is [1.0, 0.30434782608695654, 0.2770083102493075, 0.6833333333333335, 0.0, 0.0, 0.6666666666666666, 0.4540692857735449, 0.4709336780414968, 0.0, 1.0, 0.20000899352020382], 
reward next is 0.8000, 
noisyNet noise sample is [array([0.7224406], dtype=float32), 0.9609249]. 
=============================================
[2019-04-04 03:12:49,611] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.18355910e-17 1.37739598e-10 3.39942267e-12 6.12552995e-08
 1.00001764e-10 9.99999881e-01 8.22315566e-12], sum to 1.0000
[2019-04-04 03:12:49,611] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8179
[2019-04-04 03:12:49,622] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.2, 83.0, 18.5, 58.66666666666666, 26.0, 25.90789073923567, 0.6511161972662763, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1066800.0000, 
sim time next is 1067400.0000, 
raw observation next is [12.2, 83.0, 22.0, 69.0, 26.0, 26.06439206417383, 0.6815922313224636, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.8005540166204987, 0.83, 0.07333333333333333, 0.07624309392265194, 0.6666666666666666, 0.6720326720144859, 0.7271974104408212, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6647623], dtype=float32), 0.68957454]. 
=============================================
[2019-04-04 03:12:51,012] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.7407127e-22 3.4066924e-16 3.3933720e-16 2.2390858e-09 1.0517872e-16
 1.0000000e+00 8.1885779e-17], sum to 1.0000
[2019-04-04 03:12:51,012] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5950
[2019-04-04 03:12:51,037] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.616666666666667, 92.0, 57.33333333333333, 0.0, 26.0, 25.97179541111928, 0.5604558381188705, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1677000.0000, 
sim time next is 1677600.0000, 
raw observation next is [1.5, 92.0, 59.5, 0.0, 26.0, 26.00486809624384, 0.560855371579513, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.5041551246537397, 0.92, 0.19833333333333333, 0.0, 0.6666666666666666, 0.6670723413536533, 0.6869517905265043, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.510191], dtype=float32), 1.9374281]. 
=============================================
[2019-04-04 03:12:52,415] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.1402743e-16 4.9710899e-12 1.1395938e-12 1.6216683e-04 4.7477870e-14
 9.9983776e-01 2.7494874e-13], sum to 1.0000
[2019-04-04 03:12:52,415] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2289
[2019-04-04 03:12:52,443] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 84.0, 80.33333333333334, 0.0, 26.0, 25.46628726578586, 0.2912686130041232, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 902400.0000, 
sim time next is 903000.0000, 
raw observation next is [1.1, 84.0, 83.66666666666666, 0.0, 26.0, 25.45243471680477, 0.291161667927934, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.49307479224376743, 0.84, 0.27888888888888885, 0.0, 0.6666666666666666, 0.6210362264003976, 0.5970538893093114, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.24708036], dtype=float32), 0.2792392]. 
=============================================
[2019-04-04 03:12:52,448] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[86.387215]
 [86.47605 ]
 [86.64783 ]
 [86.78426 ]
 [86.93127 ]], R is [[86.49227142]
 [86.62734985]
 [86.76107788]
 [86.89347076]
 [87.02453613]].
[2019-04-04 03:12:53,970] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.4417906e-24 2.5512184e-19 1.0216807e-17 8.8412405e-13 8.8433175e-20
 1.0000000e+00 5.3092298e-19], sum to 1.0000
[2019-04-04 03:12:53,972] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7119
[2019-04-04 03:12:53,988] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.7, 81.0, 0.0, 0.0, 26.0, 25.37202845463905, 0.4501169332523552, 0.0, 1.0, 43739.72229796693], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 962400.0000, 
sim time next is 963000.0000, 
raw observation next is [7.7, 81.5, 0.0, 0.0, 26.0, 25.47720022771188, 0.4509416242358853, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.6759002770083103, 0.815, 0.0, 0.0, 0.6666666666666666, 0.62310001897599, 0.6503138747452951, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.0022557], dtype=float32), 0.0071150567]. 
=============================================
[2019-04-04 03:12:53,997] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[94.32391 ]
 [94.31978 ]
 [94.121284]
 [93.83603 ]
 [93.56767 ]], R is [[94.35905457]
 [94.20718384]
 [94.0315094 ]
 [93.74523163]
 [93.45137024]].
[2019-04-04 03:13:02,548] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2116400e-22 1.0226843e-14 5.8061970e-16 9.9999988e-01 1.4215670e-17
 1.0081816e-07 1.0056798e-17], sum to 1.0000
[2019-04-04 03:13:02,548] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2103
[2019-04-04 03:13:02,553] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [15.33333333333333, 94.0, 0.0, 0.0, 26.0, 23.75594991407624, 0.2007860612719047, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1225200.0000, 
sim time next is 1225800.0000, 
raw observation next is [15.25, 94.5, 0.0, 0.0, 26.0, 23.73981623687026, 0.1962811702032332, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.17391304347826086, 0.8850415512465375, 0.945, 0.0, 0.0, 0.6666666666666666, 0.47831801973918847, 0.5654270567344111, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.02087635], dtype=float32), 0.13471498]. 
=============================================
[2019-04-04 03:13:05,228] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.8602347e-19 8.5459390e-14 1.8184731e-11 6.2716217e-06 1.3125442e-13
 9.9999368e-01 7.1985061e-13], sum to 1.0000
[2019-04-04 03:13:05,231] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1410
[2019-04-04 03:13:05,252] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.8, 78.0, 0.0, 0.0, 26.0, 25.70141784517527, 0.5661639380376763, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1054800.0000, 
sim time next is 1055400.0000, 
raw observation next is [13.71666666666667, 78.33333333333333, 0.0, 0.0, 26.0, 25.63310574207884, 0.5670943027482355, 0.0, 1.0, 110855.7728434199], 
processed observation next is [1.0, 0.21739130434782608, 0.8425669436749772, 0.7833333333333333, 0.0, 0.0, 0.6666666666666666, 0.6360921451732366, 0.6890314342494118, 0.0, 1.0, 0.5278846325877138], 
reward next is 0.4721, 
noisyNet noise sample is [array([-1.6752559], dtype=float32), -0.61066204]. 
=============================================
[2019-04-04 03:13:18,959] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.6855147e-22 4.9508525e-15 3.5958105e-16 8.1451800e-17 1.4320656e-18
 1.0000000e+00 1.2099810e-16], sum to 1.0000
[2019-04-04 03:13:18,959] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5367
[2019-04-04 03:13:18,977] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 26.0, 25.30020957467377, 0.5049904585936489, 0.0, 1.0, 41645.08992797633], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1380000.0000, 
sim time next is 1380600.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 26.0, 25.28688054460023, 0.5020799704485079, 0.0, 1.0, 41389.12198073055], 
processed observation next is [1.0, 1.0, 0.46260387811634357, 0.95, 0.0, 0.0, 0.6666666666666666, 0.6072400453833525, 0.6673599901495026, 0.0, 1.0, 0.19709105705109786], 
reward next is 0.8029, 
noisyNet noise sample is [array([-0.0615754], dtype=float32), 0.6978237]. 
=============================================
[2019-04-04 03:13:19,007] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.7764672e-22 3.9683455e-15 2.9644750e-16 5.9362028e-17 1.4506276e-18
 1.0000000e+00 1.1547129e-16], sum to 1.0000
[2019-04-04 03:13:19,008] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8704
[2019-04-04 03:13:19,029] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 26.0, 25.2780120337794, 0.4988796719079431, 0.0, 1.0, 41179.75829522575], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1381200.0000, 
sim time next is 1381800.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 26.0, 25.26376197849193, 0.5003049520554367, 0.0, 1.0, 40979.42037757034], 
processed observation next is [1.0, 1.0, 0.46260387811634357, 0.95, 0.0, 0.0, 0.6666666666666666, 0.605313498207661, 0.6667683173518122, 0.0, 1.0, 0.19514009703604926], 
reward next is 0.8049, 
noisyNet noise sample is [array([-0.0615754], dtype=float32), 0.6978237]. 
=============================================
[2019-04-04 03:13:19,205] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.8146147e-28 2.2308671e-24 8.8397576e-24 3.9595915e-22 5.2925191e-22
 1.0000000e+00 9.5396672e-23], sum to 1.0000
[2019-04-04 03:13:19,206] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8599
[2019-04-04 03:13:19,251] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.75, 52.5, 50.0, 37.0, 26.0, 27.48695372520818, 0.8715231936086232, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1614600.0000, 
sim time next is 1615200.0000, 
raw observation next is [12.56666666666667, 53.0, 41.83333333333334, 30.83333333333334, 26.0, 27.51113225920545, 0.7541830800605456, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.8107109879963068, 0.53, 0.13944444444444448, 0.03406998158379374, 0.6666666666666666, 0.7925943549337875, 0.7513943600201819, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4995448], dtype=float32), -0.7645954]. 
=============================================
[2019-04-04 03:13:21,454] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.0879594e-22 1.6592580e-15 7.6083242e-16 4.7889093e-16 4.6574421e-17
 1.0000000e+00 2.4632676e-16], sum to 1.0000
[2019-04-04 03:13:21,458] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7030
[2019-04-04 03:13:21,472] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.2, 96.0, 0.0, 0.0, 26.0, 25.34047954442577, 0.4490820817305503, 0.0, 1.0, 36925.08288782405], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1486800.0000, 
sim time next is 1487400.0000, 
raw observation next is [2.2, 96.0, 0.0, 0.0, 26.0, 25.31823038000005, 0.4423444042237469, 0.0, 1.0, 36946.12849648966], 
processed observation next is [1.0, 0.21739130434782608, 0.5235457063711911, 0.96, 0.0, 0.0, 0.6666666666666666, 0.6098525316666707, 0.647448134741249, 0.0, 1.0, 0.17593394522137934], 
reward next is 0.8241, 
noisyNet noise sample is [array([-0.6578308], dtype=float32), -0.23134]. 
=============================================
[2019-04-04 03:13:38,009] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.9867638e-34 2.4033763e-27 9.5007116e-28 8.3795350e-32 1.3697205e-28
 1.0000000e+00 2.4044193e-29], sum to 1.0000
[2019-04-04 03:13:38,010] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8709
[2019-04-04 03:13:38,053] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.7, 100.0, 0.0, 0.0, 26.0, 24.94863175459242, 0.2998629067262316, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 934200.0000, 
sim time next is 934800.0000, 
raw observation next is [4.8, 100.0, 0.0, 0.0, 26.0, 24.80294477709321, 0.2786167657485417, 0.0, 1.0, 18711.0538748008], 
processed observation next is [1.0, 0.8260869565217391, 0.5955678670360112, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5669120647577675, 0.5928722552495139, 0.0, 1.0, 0.08910025654667048], 
reward next is 0.9109, 
noisyNet noise sample is [array([1.9970545], dtype=float32), -0.7540578]. 
=============================================
[2019-04-04 03:14:26,655] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.6955419e-19 1.0685880e-14 3.3220452e-15 1.2306218e-15 1.6166401e-16
 1.0000000e+00 3.1041384e-15], sum to 1.0000
[2019-04-04 03:14:26,658] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7143
[2019-04-04 03:14:26,713] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.199999999999999, 73.16666666666667, 264.6666666666666, 87.33333333333334, 26.0, 25.69906010651282, 0.4001133582794429, 1.0, 1.0, 136736.3023919726], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2113800.0000, 
sim time next is 2114400.0000, 
raw observation next is [-7.100000000000001, 71.33333333333334, 278.8333333333334, 94.16666666666667, 26.0, 25.7180187592336, 0.4235513409690031, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.26592797783933514, 0.7133333333333334, 0.9294444444444447, 0.10405156537753224, 0.6666666666666666, 0.6431682299361334, 0.641183780323001, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.269529], dtype=float32), -0.28676945]. 
=============================================
[2019-04-04 03:14:26,807] A3C_AGENT_WORKER-Thread-4 INFO:Evaluating...
[2019-04-04 03:14:26,847] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 03:14:26,847] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:14:26,849] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run41
[2019-04-04 03:14:26,915] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 03:14:26,916] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:14:26,918] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run41
[2019-04-04 03:14:26,947] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 03:14:26,948] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:14:26,991] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run41
[2019-04-04 03:15:09,555] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.89226884], dtype=float32), -0.04874859]
[2019-04-04 03:15:09,555] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-4.819280078, 74.54061695, 0.0, 0.0, 26.0, 24.86110762922311, 0.2366048197255143, 0.0, 1.0, 42645.69636551855]
[2019-04-04 03:15:09,555] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 03:15:09,556] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.7406227e-19 1.0951631e-13 3.6487842e-14 3.0483654e-10 1.1711214e-17
 1.0000000e+00 2.8391298e-14], sampled 0.7423877663884049
[2019-04-04 03:17:15,772] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.89226884], dtype=float32), -0.04874859]
[2019-04-04 03:17:15,772] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [1.734086534666667, 77.87484832666667, 0.0, 0.0, 26.0, 25.73038191959641, 0.5207180729102451, 0.0, 1.0, 0.0]
[2019-04-04 03:17:15,772] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 03:17:15,773] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [6.9238475e-21 1.4388640e-14 5.1728954e-15 6.5682168e-11 1.1621313e-18
 1.0000000e+00 2.2689148e-15], sampled 0.781691784904603
[2019-04-04 03:17:22,640] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.89226884], dtype=float32), -0.04874859]
[2019-04-04 03:17:22,640] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [6.0, 46.33333333333334, 98.33333333333334, 752.3333333333333, 26.0, 25.49575073338831, 0.4852825839963197, 0.0, 1.0, 0.0]
[2019-04-04 03:17:22,641] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 03:17:22,642] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [7.2906433e-19 8.1222870e-13 1.3604866e-12 1.3572589e-02 6.4654729e-16
 9.8642737e-01 8.1605339e-13], sampled 0.5802288073782429
[2019-04-04 03:17:33,872] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7354.8212 239647553.0956 1604.5686
[2019-04-04 03:18:11,554] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7242.3398 263268635.5465 1551.8363
[2019-04-04 03:18:14,524] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7183.2711 275673061.9298 1232.9925
[2019-04-04 03:18:15,561] A3C_AGENT_WORKER-Thread-4 INFO:Global step: 4000000, evaluation results [4000000.0, 7242.339830730796, 263268635.54653218, 1551.836260713355, 7354.821175735349, 239647553.09557736, 1604.5685622162691, 7183.271133667819, 275673061.9297609, 1232.992523582966]
[2019-04-04 03:18:20,374] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.9531631e-19 6.6642100e-15 4.0786479e-15 3.1936357e-14 3.6157412e-16
 1.0000000e+00 2.0563617e-14], sum to 1.0000
[2019-04-04 03:18:20,374] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0550
[2019-04-04 03:18:20,454] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.533333333333333, 68.66666666666667, 0.0, 0.0, 26.0, 25.86854026862719, 0.4811061476841416, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2226000.0000, 
sim time next is 2226600.0000, 
raw observation next is [-4.55, 69.0, 0.0, 0.0, 26.0, 25.85488013964687, 0.4684737379031061, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.3365650969529086, 0.69, 0.0, 0.0, 0.6666666666666666, 0.6545733449705725, 0.6561579126343687, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.40784046], dtype=float32), -1.2611296]. 
=============================================
[2019-04-04 03:18:28,314] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.6820412e-19 9.6741869e-14 1.2698948e-14 3.6596009e-18 1.1931150e-14
 1.0000000e+00 5.3008220e-15], sum to 1.0000
[2019-04-04 03:18:28,314] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8131
[2019-04-04 03:18:28,384] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 92.0, 75.0, 0.0, 26.0, 25.03971026877156, 0.4618661354680851, 1.0, 1.0, 18998.92683773187], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1434000.0000, 
sim time next is 1434600.0000, 
raw observation next is [1.1, 92.0, 72.0, 0.0, 26.0, 25.53251474762754, 0.4925127451969925, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.49307479224376743, 0.92, 0.24, 0.0, 0.6666666666666666, 0.6277095623022951, 0.6641709150656642, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.071156], dtype=float32), -0.45536858]. 
=============================================
[2019-04-04 03:18:29,837] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.4866153e-15 1.4729397e-10 3.2686527e-09 8.7576878e-01 1.7041686e-13
 1.2423128e-01 1.2293857e-09], sum to 1.0000
[2019-04-04 03:18:29,837] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6638
[2019-04-04 03:18:29,897] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.9666666666666667, 44.66666666666667, 18.16666666666666, 23.0, 26.0, 24.96097681169825, 0.2727063425611752, 0.0, 1.0, 40137.63672454694], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2395200.0000, 
sim time next is 2395800.0000, 
raw observation next is [-1.15, 44.5, 0.0, 0.0, 26.0, 24.95568621643458, 0.2655087152284191, 0.0, 1.0, 44773.74864866846], 
processed observation next is [0.0, 0.7391304347826086, 0.4307479224376732, 0.445, 0.0, 0.0, 0.6666666666666666, 0.5796405180362149, 0.5885029050761398, 0.0, 1.0, 0.21320832689842123], 
reward next is 0.7868, 
noisyNet noise sample is [array([-0.9591905], dtype=float32), -0.24070588]. 
=============================================
[2019-04-04 03:18:44,276] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.7487616e-18 2.6680979e-13 3.0000522e-13 6.2827379e-01 1.9702956e-16
 3.7172621e-01 4.6156312e-12], sum to 1.0000
[2019-04-04 03:18:44,290] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9798
[2019-04-04 03:18:44,320] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.8, 44.66666666666667, 0.0, 0.0, 26.0, 24.52512129932759, 0.1424112939243188, 0.0, 1.0, 43146.60191131911], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2420400.0000, 
sim time next is 2421000.0000, 
raw observation next is [-5.9, 45.5, 0.0, 0.0, 26.0, 24.55089726145264, 0.1345790083604234, 0.0, 1.0, 43152.84221139584], 
processed observation next is [0.0, 0.0, 0.2991689750692521, 0.455, 0.0, 0.0, 0.6666666666666666, 0.5459081051210534, 0.5448596694534745, 0.0, 1.0, 0.20548972481617064], 
reward next is 0.7945, 
noisyNet noise sample is [array([-0.5749712], dtype=float32), -3.0703413]. 
=============================================
[2019-04-04 03:18:44,325] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[77.92194]
 [77.93664]
 [77.92635]
 [77.8802 ]
 [77.91111]], R is [[77.92715454]
 [77.94242096]
 [77.95757294]
 [77.97261047]
 [77.98755646]].
[2019-04-04 03:18:50,560] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.0226902e-18 3.2450822e-12 1.7650924e-11 6.7811975e-08 1.8857389e-14
 9.9999988e-01 2.5460695e-12], sum to 1.0000
[2019-04-04 03:18:50,560] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8457
[2019-04-04 03:18:50,585] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.666666666666668, 71.33333333333334, 0.0, 0.0, 26.0, 24.46508908568021, 0.193121069592089, 0.0, 1.0, 44405.56008098314], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2683200.0000, 
sim time next is 2683800.0000, 
raw observation next is [-10.0, 72.5, 0.0, 0.0, 26.0, 24.37653922067199, 0.1750333828361573, 0.0, 1.0, 44440.39590270347], 
processed observation next is [1.0, 0.043478260869565216, 0.18559556786703602, 0.725, 0.0, 0.0, 0.6666666666666666, 0.5313782683893326, 0.5583444609453857, 0.0, 1.0, 0.21162093287001654], 
reward next is 0.7884, 
noisyNet noise sample is [array([-0.08584615], dtype=float32), 0.8979191]. 
=============================================
[2019-04-04 03:19:06,244] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.6752731e-22 2.0435513e-19 1.8961368e-18 1.6723740e-11 1.4598914e-20
 1.0000000e+00 4.1891003e-18], sum to 1.0000
[2019-04-04 03:19:06,244] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1431
[2019-04-04 03:19:06,274] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.8, 62.0, 74.0, 0.0, 26.0, 25.63432269570811, 0.3237700175909284, 1.0, 1.0, 36291.60015946452], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1956600.0000, 
sim time next is 1957200.0000, 
raw observation next is [-2.8, 62.0, 66.66666666666667, 0.0, 26.0, 25.58891357281812, 0.3250803023516946, 1.0, 1.0, 29363.60547812051], 
processed observation next is [1.0, 0.6521739130434783, 0.38504155124653744, 0.62, 0.22222222222222224, 0.0, 0.6666666666666666, 0.6324094644015101, 0.6083601007838982, 1.0, 1.0, 0.1398266927529548], 
reward next is 0.8602, 
noisyNet noise sample is [array([0.14146967], dtype=float32), 1.6796796]. 
=============================================
[2019-04-04 03:19:14,011] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.6917830e-20 2.9035161e-15 1.1906675e-14 9.0760499e-01 9.1660121e-19
 9.2394978e-02 1.6315926e-14], sum to 1.0000
[2019-04-04 03:19:14,011] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5989
[2019-04-04 03:19:14,100] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 55.0, 31.0, 286.5, 26.0, 25.09625828805007, 0.3712358272005154, 0.0, 1.0, 19984.51482655581], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2998800.0000, 
sim time next is 2999400.0000, 
raw observation next is [-1.166666666666667, 55.83333333333334, 22.66666666666666, 224.0, 26.0, 25.09299143603532, 0.3608248520333914, 0.0, 1.0, 27162.40146736191], 
processed observation next is [0.0, 0.7391304347826086, 0.43028624192059095, 0.5583333333333335, 0.07555555555555554, 0.24751381215469614, 0.6666666666666666, 0.59108261966961, 0.6202749506777971, 0.0, 1.0, 0.12934476889219956], 
reward next is 0.8707, 
noisyNet noise sample is [array([0.07009405], dtype=float32), 0.20432803]. 
=============================================
[2019-04-04 03:19:36,826] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.8472748e-19 3.4956916e-12 1.8537019e-13 2.4393956e-15 9.2857840e-16
 1.0000000e+00 1.3357872e-14], sum to 1.0000
[2019-04-04 03:19:36,826] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2382
[2019-04-04 03:19:36,855] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.666666666666667, 71.0, 80.66666666666667, 656.8333333333334, 26.0, 26.18364147898242, 0.7733647858122242, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3253200.0000, 
sim time next is 3253800.0000, 
raw observation next is [-2.833333333333333, 71.0, 76.33333333333333, 627.6666666666666, 26.0, 26.58955784493896, 0.8037894538933964, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.3841181902123731, 0.71, 0.2544444444444444, 0.6935543278084714, 0.6666666666666666, 0.7157964870782466, 0.7679298179644655, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7217959], dtype=float32), 0.17399392]. 
=============================================
[2019-04-04 03:19:44,182] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.4316860e-21 1.4210217e-14 8.5623982e-16 4.0260259e-16 2.5567227e-19
 1.0000000e+00 5.3878908e-16], sum to 1.0000
[2019-04-04 03:19:44,183] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6754
[2019-04-04 03:19:44,196] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.666666666666666, 69.0, 0.0, 0.0, 26.0, 25.32545868924918, 0.424306389813871, 0.0, 1.0, 66761.18361544664], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3364800.0000, 
sim time next is 3365400.0000, 
raw observation next is [-4.833333333333334, 70.0, 0.0, 0.0, 26.0, 25.24984986953498, 0.4146385623843806, 0.0, 1.0, 53192.95039574343], 
processed observation next is [1.0, 0.9565217391304348, 0.32871652816251157, 0.7, 0.0, 0.0, 0.6666666666666666, 0.6041541557945816, 0.6382128541281269, 0.0, 1.0, 0.2532997637892544], 
reward next is 0.7467, 
noisyNet noise sample is [array([0.25022832], dtype=float32), -0.19693205]. 
=============================================
[2019-04-04 03:19:49,142] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.5305484e-23 2.3963015e-18 4.6888475e-19 2.0569938e-21 3.1101280e-20
 1.0000000e+00 3.5607383e-19], sum to 1.0000
[2019-04-04 03:19:49,142] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3409
[2019-04-04 03:19:49,152] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.833333333333333, 54.0, 117.3333333333333, 806.6666666666667, 26.0, 26.35530745178316, 0.6078942917764564, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3327000.0000, 
sim time next is 3327600.0000, 
raw observation next is [-5.666666666666666, 54.00000000000001, 117.6666666666667, 808.8333333333334, 26.0, 26.36016375078927, 0.5896709907155055, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.3056325023084026, 0.54, 0.3922222222222223, 0.8937384898710866, 0.6666666666666666, 0.6966803125657725, 0.6965569969051685, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3013908], dtype=float32), 0.13641542]. 
=============================================
[2019-04-04 03:19:53,907] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.4803543e-21 5.1744349e-15 1.0450333e-14 4.2595709e-09 1.5826870e-18
 1.0000000e+00 3.6330255e-15], sum to 1.0000
[2019-04-04 03:19:53,917] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4426
[2019-04-04 03:19:53,949] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.82680070682515, 0.3149506261217958, 0.0, 1.0, 43291.54623489578], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2942400.0000, 
sim time next is 2943000.0000, 
raw observation next is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.78894656286762, 0.3110962369216355, 0.0, 1.0, 43279.27801129546], 
processed observation next is [0.0, 0.043478260869565216, 0.40720221606648205, 0.85, 0.0, 0.0, 0.6666666666666666, 0.5657455469056348, 0.6036987456405452, 0.0, 1.0, 0.20609180005378788], 
reward next is 0.7939, 
noisyNet noise sample is [array([-1.7262076], dtype=float32), 1.133059]. 
=============================================
[2019-04-04 03:19:53,983] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[86.709076]
 [86.7728  ]
 [86.634964]
 [86.420746]
 [86.18698 ]], R is [[86.61986542]
 [86.54751587]
 [86.47590637]
 [86.40496826]
 [86.33461761]].
[2019-04-04 03:20:00,198] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.8683570e-26 4.3493477e-19 2.4305838e-20 2.6743536e-22 3.2857880e-21
 1.0000000e+00 3.5981047e-22], sum to 1.0000
[2019-04-04 03:20:00,203] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6343
[2019-04-04 03:20:00,216] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.666666666666667, 50.0, 112.8333333333333, 801.8333333333334, 26.0, 25.51535143777057, 0.5944023734158358, 1.0, 1.0, 18680.41167023055], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3505200.0000, 
sim time next is 3505800.0000, 
raw observation next is [2.833333333333333, 49.5, 110.6666666666667, 797.6666666666666, 26.0, 25.98943178694439, 0.6443771938334444, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.541089566020314, 0.495, 0.368888888888889, 0.8813996316758748, 0.6666666666666666, 0.6657859822453659, 0.7147923979444815, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.27699706], dtype=float32), -0.6431725]. 
=============================================
[2019-04-04 03:20:00,228] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1688964e-21 1.4571354e-15 5.6388356e-18 2.0665997e-11 6.4992501e-19
 1.0000000e+00 5.8979015e-16], sum to 1.0000
[2019-04-04 03:20:00,232] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4075
[2019-04-04 03:20:00,249] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 39.5, 0.0, 0.0, 26.0, 25.41621686827144, 0.4332860885382769, 0.0, 1.0, 21746.39572929266], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4150200.0000, 
sim time next is 4150800.0000, 
raw observation next is [-1.0, 39.0, 0.0, 0.0, 26.0, 25.38943439790764, 0.4249045710959288, 0.0, 1.0, 43780.70673011654], 
processed observation next is [0.0, 0.043478260869565216, 0.4349030470914128, 0.39, 0.0, 0.0, 0.6666666666666666, 0.6157861998256365, 0.6416348570319762, 0.0, 1.0, 0.20847955585769778], 
reward next is 0.7915, 
noisyNet noise sample is [array([1.0215603], dtype=float32), 0.13665706]. 
=============================================
[2019-04-04 03:20:10,714] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.28110225e-26 1.40067931e-20 6.68667180e-22 2.20715310e-20
 2.87242301e-24 1.00000000e+00 2.20217224e-22], sum to 1.0000
[2019-04-04 03:20:10,715] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8138
[2019-04-04 03:20:10,732] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.39598915068576, 0.43969750132302, 0.0, 1.0, 56993.17652596639], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4147200.0000, 
sim time next is 4147800.0000, 
raw observation next is [-1.0, 41.5, 0.0, 0.0, 26.0, 25.36684512303547, 0.4380774830580846, 0.0, 1.0, 60210.53159430097], 
processed observation next is [0.0, 0.0, 0.4349030470914128, 0.415, 0.0, 0.0, 0.6666666666666666, 0.6139037602529559, 0.6460258276860281, 0.0, 1.0, 0.2867168171157189], 
reward next is 0.7133, 
noisyNet noise sample is [array([-1.2337601], dtype=float32), -0.5193843]. 
=============================================
[2019-04-04 03:20:20,780] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4026416e-20 1.5760432e-16 4.8475365e-18 1.9695165e-22 2.7847136e-19
 1.0000000e+00 3.5739010e-17], sum to 1.0000
[2019-04-04 03:20:20,780] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6943
[2019-04-04 03:20:20,877] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.166666666666666, 41.66666666666667, 0.0, 0.0, 26.0, 25.88701509865912, 0.5685213580568156, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3957000.0000, 
sim time next is 3957600.0000, 
raw observation next is [-6.333333333333333, 42.33333333333334, 0.0, 0.0, 26.0, 25.83387475393076, 0.5408519311830257, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.28716528162511545, 0.42333333333333345, 0.0, 0.0, 0.6666666666666666, 0.6528228961608967, 0.6802839770610086, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.11561038], dtype=float32), 0.69344044]. 
=============================================
[2019-04-04 03:20:32,088] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.0543919e-17 2.2581652e-12 1.0857267e-11 1.1965870e-06 2.9651741e-15
 9.9999881e-01 1.4815823e-12], sum to 1.0000
[2019-04-04 03:20:32,093] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9495
[2019-04-04 03:20:32,119] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.88945725665313, 0.335965372275899, 0.0, 1.0, 40741.02461152412], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4758600.0000, 
sim time next is 4759200.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.85734967612857, 0.3338373577424476, 0.0, 1.0, 40684.60067294857], 
processed observation next is [0.0, 0.08695652173913043, 0.3518005540166205, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5714458063440476, 0.6112791192474826, 0.0, 1.0, 0.19373619368070746], 
reward next is 0.8063, 
noisyNet noise sample is [array([1.9486221], dtype=float32), 1.8456794]. 
=============================================
[2019-04-04 03:20:39,330] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4837215e-17 1.3032498e-14 3.0141696e-13 1.1029362e-04 1.6776531e-14
 9.9988973e-01 4.8944589e-13], sum to 1.0000
[2019-04-04 03:20:39,332] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1320
[2019-04-04 03:20:39,354] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 26.0, 25.35008867057964, 0.3235590594768463, 0.0, 1.0, 53031.33273364497], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4250400.0000, 
sim time next is 4251000.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 26.0, 25.34980119049187, 0.3251799686330314, 0.0, 1.0, 44544.61692887824], 
processed observation next is [0.0, 0.17391304347826086, 0.5457063711911359, 0.45, 0.0, 0.0, 0.6666666666666666, 0.6124834325409892, 0.6083933228776771, 0.0, 1.0, 0.21211722347084877], 
reward next is 0.7879, 
noisyNet noise sample is [array([-0.12217621], dtype=float32), -0.50091964]. 
=============================================
[2019-04-04 03:20:39,372] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[87.120056]
 [87.0495  ]
 [86.96556 ]
 [86.95624 ]
 [87.0148  ]], R is [[87.05632782]
 [86.93323517]
 [86.74221039]
 [86.6566925 ]
 [86.66837311]].
[2019-04-04 03:20:39,897] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.4689911e-21 6.9108859e-15 5.1861337e-15 4.7077382e-19 2.9088058e-20
 1.0000000e+00 6.6417680e-17], sum to 1.0000
[2019-04-04 03:20:39,897] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2326
[2019-04-04 03:20:39,925] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.833333333333333, 63.33333333333334, 0.0, 0.0, 26.0, 25.3301124900904, 0.5263189094375813, 0.0, 1.0, 57000.26875772679], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3358200.0000, 
sim time next is 3358800.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.51000677491783, 0.5364854573441796, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6258338979098191, 0.6788284857813932, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.41560912], dtype=float32), -0.22077571]. 
=============================================
[2019-04-04 03:20:43,224] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:20:43,224] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:20:43,236] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run31
[2019-04-04 03:20:47,364] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.5717631e-31 7.4756860e-26 3.8144179e-28 1.5009716e-30 4.2211518e-25
 1.0000000e+00 1.0676320e-26], sum to 1.0000
[2019-04-04 03:20:47,364] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6173
[2019-04-04 03:20:47,398] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [11.83333333333333, 51.33333333333334, 0.0, 0.0, 26.0, 27.82356267037786, 1.002626155198797, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4389000.0000, 
sim time next is 4389600.0000, 
raw observation next is [11.66666666666667, 52.66666666666667, 0.0, 0.0, 26.0, 27.72183055827545, 0.9823432694502645, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.785780240073869, 0.5266666666666667, 0.0, 0.0, 0.6666666666666666, 0.8101525465229541, 0.8274477564834215, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.48843905], dtype=float32), 0.94969267]. 
=============================================
[2019-04-04 03:20:50,452] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.5982975e-21 1.8407189e-16 4.6353557e-17 3.5118908e-04 1.6964127e-20
 9.9964881e-01 2.6444428e-17], sum to 1.0000
[2019-04-04 03:20:50,452] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9169
[2019-04-04 03:20:50,477] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.26886168943089, 0.3479292020225269, 0.0, 1.0, 43378.98516349485], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4840800.0000, 
sim time next is 4841400.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.24817941178323, 0.3440311211970846, 0.0, 1.0, 40589.7046955871], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.6, 0.0, 0.0, 0.6666666666666666, 0.6040149509819358, 0.6146770403990282, 0.0, 1.0, 0.19328430807422428], 
reward next is 0.8067, 
noisyNet noise sample is [array([1.569771], dtype=float32), 0.04581285]. 
=============================================
[2019-04-04 03:20:51,944] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:20:51,944] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:20:51,949] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run31
[2019-04-04 03:20:54,071] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:20:54,071] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:20:54,087] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run31
[2019-04-04 03:20:55,418] A3C_AGENT_WORKER-Thread-2 INFO:Local step 255000, global step 4063133: loss 0.4047
[2019-04-04 03:20:55,418] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 255000, global step 4063133: learning rate 0.0005
[2019-04-04 03:20:55,763] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1516690e-23 1.1413933e-19 4.6079483e-22 5.8710588e-22 5.4286644e-21
 1.0000000e+00 9.6016694e-21], sum to 1.0000
[2019-04-04 03:20:55,772] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1653
[2019-04-04 03:20:55,859] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.333333333333333, 81.66666666666667, 130.5, 0.0, 26.0, 24.91480753240043, 0.3906726159799401, 1.0, 1.0, 196217.9094192962], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4713600.0000, 
sim time next is 4714200.0000, 
raw observation next is [1.5, 79.5, 135.0, 0.0, 26.0, 24.35242074007177, 0.3896341058821187, 1.0, 1.0, 196918.6660088483], 
processed observation next is [1.0, 0.5652173913043478, 0.5041551246537397, 0.795, 0.45, 0.0, 0.6666666666666666, 0.529368395005981, 0.6298780352940395, 1.0, 1.0, 0.9377079333754681], 
reward next is 0.0623, 
noisyNet noise sample is [array([0.74112976], dtype=float32), 0.24370144]. 
=============================================
[2019-04-04 03:20:57,485] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.9023755e-26 2.9527650e-20 7.3590227e-20 1.9231426e-16 1.9130749e-23
 1.0000000e+00 3.0076495e-21], sum to 1.0000
[2019-04-04 03:20:57,489] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4497
[2019-04-04 03:20:57,511] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.0, 71.5, 0.0, 0.0, 26.0, 23.19329977937695, -0.1014283999647316, 0.0, 1.0, 45626.15848887497], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 109800.0000, 
sim time next is 110400.0000, 
raw observation next is [-7.1, 70.33333333333333, 0.0, 0.0, 26.0, 23.18167341400824, -0.1123951306675403, 0.0, 1.0, 45736.15936916885], 
processed observation next is [1.0, 0.2608695652173913, 0.2659279778393352, 0.7033333333333333, 0.0, 0.0, 0.6666666666666666, 0.43180611783402006, 0.4625349564441532, 0.0, 1.0, 0.21779123509128023], 
reward next is 0.7822, 
noisyNet noise sample is [array([0.02924058], dtype=float32), 1.2450551]. 
=============================================
[2019-04-04 03:21:04,310] A3C_AGENT_WORKER-Thread-19 INFO:Local step 255000, global step 4066743: loss 0.2919
[2019-04-04 03:21:04,311] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 255000, global step 4066743: learning rate 0.0005
[2019-04-04 03:21:06,333] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:21:06,333] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:21:06,342] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run31
[2019-04-04 03:21:06,731] A3C_AGENT_WORKER-Thread-17 INFO:Local step 255000, global step 4067745: loss 0.1601
[2019-04-04 03:21:06,732] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 255000, global step 4067745: learning rate 0.0005
[2019-04-04 03:21:18,780] A3C_AGENT_WORKER-Thread-11 INFO:Local step 255000, global step 4072598: loss 0.1684
[2019-04-04 03:21:18,782] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 255000, global step 4072598: learning rate 0.0005
[2019-04-04 03:21:19,640] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:21:19,641] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:21:19,646] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run31
[2019-04-04 03:21:20,145] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:21:20,145] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:21:20,149] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run31
[2019-04-04 03:21:20,283] A3C_AGENT_WORKER-Thread-2 INFO:Local step 255500, global step 4073218: loss 3.0183
[2019-04-04 03:21:20,284] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 255500, global step 4073218: learning rate 0.0005
[2019-04-04 03:21:22,499] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.8307624e-13 7.8490686e-08 1.4179029e-08 6.7074290e-03 1.9399820e-11
 9.9329245e-01 5.5119100e-09], sum to 1.0000
[2019-04-04 03:21:22,500] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5641
[2019-04-04 03:21:22,522] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.333333333333334, 29.66666666666667, 0.0, 0.0, 26.0, 25.67690817989673, 0.5046433527028537, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4051200.0000, 
sim time next is 4051800.0000, 
raw observation next is [-4.5, 30.0, 0.0, 0.0, 26.0, 25.62572212186652, 0.4893278536619363, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.3379501385041552, 0.3, 0.0, 0.0, 0.6666666666666666, 0.6354768434888767, 0.6631092845539788, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3792298], dtype=float32), 0.032105573]. 
=============================================
[2019-04-04 03:21:23,825] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:21:23,825] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:21:23,829] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run31
[2019-04-04 03:21:24,865] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:21:24,865] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:21:24,868] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run31
[2019-04-04 03:21:25,562] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:21:25,562] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:21:25,566] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run31
[2019-04-04 03:21:26,780] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.2339596e-16 5.5213195e-10 1.0063195e-12 4.7466227e-09 6.1284507e-16
 1.0000000e+00 9.4321273e-11], sum to 1.0000
[2019-04-04 03:21:26,780] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3957
[2019-04-04 03:21:26,811] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.6, 47.66666666666667, 0.0, 0.0, 26.0, 24.58536864529482, 0.1658163711028161, 0.0, 1.0, 45801.77330899928], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 422400.0000, 
sim time next is 423000.0000, 
raw observation next is [-10.6, 48.0, 0.0, 0.0, 26.0, 24.51206043615085, 0.151005972207634, 0.0, 1.0, 45676.46188517946], 
processed observation next is [1.0, 0.9130434782608695, 0.1689750692520776, 0.48, 0.0, 0.0, 0.6666666666666666, 0.5426717030125708, 0.5503353240692114, 0.0, 1.0, 0.2175069613579974], 
reward next is 0.7825, 
noisyNet noise sample is [array([1.2371243], dtype=float32), 0.543655]. 
=============================================
[2019-04-04 03:21:26,831] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[83.20112 ]
 [83.014305]
 [82.63424 ]
 [82.17577 ]
 [81.84762 ]], R is [[83.35198212]
 [83.30036163]
 [83.24868774]
 [83.197258  ]
 [83.14724731]].
[2019-04-04 03:21:29,053] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:21:29,054] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:21:29,058] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run31
[2019-04-04 03:21:29,767] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:21:29,768] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:21:29,776] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run31
[2019-04-04 03:21:29,887] A3C_AGENT_WORKER-Thread-19 INFO:Local step 255500, global step 4076000: loss 0.5532
[2019-04-04 03:21:29,889] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 255500, global step 4076000: learning rate 0.0005
[2019-04-04 03:21:33,045] A3C_AGENT_WORKER-Thread-17 INFO:Local step 255500, global step 4076713: loss 0.5745
[2019-04-04 03:21:33,046] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 255500, global step 4076713: learning rate 0.0005
[2019-04-04 03:21:33,227] A3C_AGENT_WORKER-Thread-15 INFO:Local step 255000, global step 4076771: loss 0.3515
[2019-04-04 03:21:33,228] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 255000, global step 4076771: learning rate 0.0005
[2019-04-04 03:21:33,597] A3C_AGENT_WORKER-Thread-10 INFO:Local step 255000, global step 4076877: loss 0.2829
[2019-04-04 03:21:33,597] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 255000, global step 4076877: learning rate 0.0005
[2019-04-04 03:21:34,319] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.2730158e-18 6.8331117e-14 9.6035981e-16 2.2918345e-09 2.7139171e-17
 1.0000000e+00 5.8077763e-14], sum to 1.0000
[2019-04-04 03:21:34,321] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0311
[2019-04-04 03:21:34,364] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.9, 36.33333333333333, 14.0, 274.3333333333334, 26.0, 26.16000665930731, 0.4413678939276358, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 406200.0000, 
sim time next is 406800.0000, 
raw observation next is [-8.9, 36.0, 10.5, 210.0, 26.0, 25.83918409944301, 0.3830886856988513, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.21606648199445982, 0.36, 0.035, 0.23204419889502761, 0.6666666666666666, 0.6532653416202509, 0.6276962285662838, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.09881079], dtype=float32), 0.16420981]. 
=============================================
[2019-04-04 03:21:35,957] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:21:35,957] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:21:35,960] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run31
[2019-04-04 03:21:36,857] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:21:36,857] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:21:36,861] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run31
[2019-04-04 03:21:37,035] A3C_AGENT_WORKER-Thread-20 INFO:Local step 255000, global step 4077934: loss 0.2603
[2019-04-04 03:21:37,035] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 255000, global step 4077934: learning rate 0.0005
[2019-04-04 03:21:38,669] A3C_AGENT_WORKER-Thread-18 INFO:Local step 255000, global step 4078397: loss 0.3637
[2019-04-04 03:21:38,669] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 255000, global step 4078397: learning rate 0.0005
[2019-04-04 03:21:38,847] A3C_AGENT_WORKER-Thread-6 INFO:Local step 255000, global step 4078449: loss 0.2847
[2019-04-04 03:21:38,847] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 255000, global step 4078449: learning rate 0.0005
[2019-04-04 03:21:39,144] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.6485649e-16 2.1580280e-09 6.0713871e-11 2.0562708e-11 6.8407828e-13
 1.0000000e+00 2.7163386e-11], sum to 1.0000
[2019-04-04 03:21:39,144] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8397
[2019-04-04 03:21:39,160] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 26.0, 24.53321005562013, 0.193196094469808, 0.0, 1.0, 40333.42867860464], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 79200.0000, 
sim time next is 79800.0000, 
raw observation next is [0.4666666666666667, 95.83333333333333, 0.0, 0.0, 26.0, 24.51105151585396, 0.1893462465294121, 0.0, 1.0, 40267.93925457565], 
processed observation next is [0.0, 0.9565217391304348, 0.4755309325946445, 0.9583333333333333, 0.0, 0.0, 0.6666666666666666, 0.5425876263211634, 0.563115415509804, 0.0, 1.0, 0.19175209168845547], 
reward next is 0.8082, 
noisyNet noise sample is [array([1.6554924], dtype=float32), 0.5146686]. 
=============================================
[2019-04-04 03:21:42,324] A3C_AGENT_WORKER-Thread-3 INFO:Local step 255000, global step 4079387: loss 0.1761
[2019-04-04 03:21:42,324] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 255000, global step 4079387: learning rate 0.0005
[2019-04-04 03:21:43,408] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.0543314e-20 9.8701499e-16 9.4443189e-16 2.2071155e-03 1.3172579e-18
 9.9779284e-01 1.5658909e-15], sum to 1.0000
[2019-04-04 03:21:43,408] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8614
[2019-04-04 03:21:43,419] A3C_AGENT_WORKER-Thread-5 INFO:Local step 255000, global step 4079776: loss 0.1906
[2019-04-04 03:21:43,420] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 255000, global step 4079776: learning rate 0.0005
[2019-04-04 03:21:43,510] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 49.66666666666667, 73.33333333333334, 35.33333333333334, 26.0, 25.31948860735155, 0.3337390586060147, 0.0, 1.0, 38912.16521950135], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4263000.0000, 
sim time next is 4263600.0000, 
raw observation next is [3.0, 50.33333333333334, 91.66666666666667, 44.16666666666667, 26.0, 25.32636871236485, 0.3694017652201667, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.5457063711911359, 0.5033333333333334, 0.3055555555555556, 0.04880294659300185, 0.6666666666666666, 0.6105307260304041, 0.6231339217400556, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.33879486], dtype=float32), 0.73518026]. 
=============================================
[2019-04-04 03:21:45,730] A3C_AGENT_WORKER-Thread-11 INFO:Local step 255500, global step 4080570: loss 0.4527
[2019-04-04 03:21:45,733] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 255500, global step 4080572: learning rate 0.0005
[2019-04-04 03:21:46,349] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0436579e-15 4.4988079e-11 2.9646408e-10 9.3464105e-04 6.0162616e-14
 9.9906534e-01 1.3591042e-10], sum to 1.0000
[2019-04-04 03:21:46,349] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3212
[2019-04-04 03:21:46,380] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.133333333333334, 92.66666666666667, 0.0, 0.0, 26.0, 23.88078998381985, 0.09590110448302765, 0.0, 1.0, 41902.19353192016], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4778400.0000, 
sim time next is 4779000.0000, 
raw observation next is [-6.1, 92.5, 0.0, 0.0, 26.0, 23.85189702328, 0.08934170077709307, 0.0, 1.0, 41923.90016931214], 
processed observation next is [0.0, 0.30434782608695654, 0.29362880886426596, 0.925, 0.0, 0.0, 0.6666666666666666, 0.48765808527333326, 0.5297805669256977, 0.0, 1.0, 0.19963761985386733], 
reward next is 0.8004, 
noisyNet noise sample is [array([-0.8851751], dtype=float32), -0.11588241]. 
=============================================
[2019-04-04 03:21:46,407] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[77.066086]
 [77.057014]
 [77.06707 ]
 [77.08094 ]
 [77.10343 ]], R is [[77.11719513]
 [77.146492  ]
 [77.17562866]
 [77.20463562]
 [77.23355103]].
[2019-04-04 03:21:48,192] A3C_AGENT_WORKER-Thread-2 INFO:Local step 256000, global step 4081286: loss 0.0026
[2019-04-04 03:21:48,192] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 256000, global step 4081286: learning rate 0.0005
[2019-04-04 03:21:48,930] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0801188e-15 1.1719399e-10 1.4164560e-09 1.1005621e-06 1.2639207e-13
 9.9999893e-01 6.2656838e-11], sum to 1.0000
[2019-04-04 03:21:48,931] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3443
[2019-04-04 03:21:48,961] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.7927264971293, -0.2458068910455428, 0.0, 1.0, 44895.04615093157], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 195000.0000, 
sim time next is 195600.0000, 
raw observation next is [-8.900000000000002, 78.0, 0.0, 0.0, 26.0, 22.73615229884505, -0.251572207834399, 0.0, 1.0, 44900.21008824737], 
processed observation next is [1.0, 0.2608695652173913, 0.2160664819944598, 0.78, 0.0, 0.0, 0.6666666666666666, 0.3946793582370874, 0.4161425973885337, 0.0, 1.0, 0.21381052422974936], 
reward next is 0.7862, 
noisyNet noise sample is [array([-1.0280797], dtype=float32), -1.1731404]. 
=============================================
[2019-04-04 03:21:48,999] A3C_AGENT_WORKER-Thread-12 INFO:Local step 255000, global step 4081555: loss 0.2658
[2019-04-04 03:21:49,000] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 255000, global step 4081555: learning rate 0.0005
[2019-04-04 03:21:49,860] A3C_AGENT_WORKER-Thread-16 INFO:Local step 255000, global step 4081851: loss 0.3079
[2019-04-04 03:21:49,860] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 255000, global step 4081851: learning rate 0.0005
[2019-04-04 03:21:53,291] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.6047218e-29 1.8557282e-25 3.7152919e-24 4.3192000e-22 9.9717935e-27
 1.0000000e+00 4.1549319e-25], sum to 1.0000
[2019-04-04 03:21:53,292] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4047
[2019-04-04 03:21:53,360] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.8, 74.0, 117.5, 18.0, 26.0, 25.32486671426712, 0.2576641182242838, 1.0, 1.0, 40327.86656633968], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 122400.0000, 
sim time next is 123000.0000, 
raw observation next is [-7.8, 76.0, 141.6666666666667, 24.0, 26.0, 25.28492636715743, 0.2671938620618008, 1.0, 1.0, 41651.25515214886], 
processed observation next is [1.0, 0.43478260869565216, 0.24653739612188366, 0.76, 0.4722222222222224, 0.026519337016574586, 0.6666666666666666, 0.607077197263119, 0.589064620687267, 1.0, 1.0, 0.19833931024832788], 
reward next is 0.8017, 
noisyNet noise sample is [array([-0.14701514], dtype=float32), -0.55390143]. 
=============================================
[2019-04-04 03:21:53,363] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[83.74789]
 [83.34803]
 [82.77425]
 [82.00446]
 [81.76121]], R is [[83.9438858 ]
 [83.91240692]
 [83.89940643]
 [83.92902374]
 [84.00050354]].
[2019-04-04 03:21:57,164] A3C_AGENT_WORKER-Thread-19 INFO:Local step 256000, global step 4084275: loss 0.0155
[2019-04-04 03:21:57,165] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 256000, global step 4084275: learning rate 0.0005
[2019-04-04 03:21:59,141] A3C_AGENT_WORKER-Thread-15 INFO:Local step 255500, global step 4084977: loss 2.3122
[2019-04-04 03:21:59,142] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 255500, global step 4084977: learning rate 0.0005
[2019-04-04 03:21:59,853] A3C_AGENT_WORKER-Thread-17 INFO:Local step 256000, global step 4085250: loss 0.0026
[2019-04-04 03:21:59,854] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 256000, global step 4085250: learning rate 0.0005
[2019-04-04 03:21:59,889] A3C_AGENT_WORKER-Thread-10 INFO:Local step 255500, global step 4085258: loss 0.3791
[2019-04-04 03:21:59,889] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 255500, global step 4085258: learning rate 0.0005
[2019-04-04 03:22:03,851] A3C_AGENT_WORKER-Thread-20 INFO:Local step 255500, global step 4086457: loss 0.4525
[2019-04-04 03:22:03,851] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 255500, global step 4086457: learning rate 0.0005
[2019-04-04 03:22:05,124] A3C_AGENT_WORKER-Thread-6 INFO:Local step 255500, global step 4086892: loss 0.4840
[2019-04-04 03:22:05,126] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 255500, global step 4086892: learning rate 0.0005
[2019-04-04 03:22:05,292] A3C_AGENT_WORKER-Thread-18 INFO:Local step 255500, global step 4086950: loss 0.4829
[2019-04-04 03:22:05,293] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 255500, global step 4086950: learning rate 0.0005
[2019-04-04 03:22:06,833] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:22:06,833] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:22:06,847] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run31
[2019-04-04 03:22:07,587] A3C_AGENT_WORKER-Thread-3 INFO:Local step 255500, global step 4087815: loss 0.3485
[2019-04-04 03:22:07,589] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 255500, global step 4087815: learning rate 0.0005
[2019-04-04 03:22:08,969] A3C_AGENT_WORKER-Thread-5 INFO:Local step 255500, global step 4088195: loss 2.6587
[2019-04-04 03:22:08,971] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 255500, global step 4088195: learning rate 0.0005
[2019-04-04 03:22:10,078] A3C_AGENT_WORKER-Thread-2 INFO:Local step 256500, global step 4088482: loss 0.0398
[2019-04-04 03:22:10,086] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 256500, global step 4088482: learning rate 0.0005
[2019-04-04 03:22:12,209] A3C_AGENT_WORKER-Thread-11 INFO:Local step 256000, global step 4089136: loss -0.0015
[2019-04-04 03:22:12,210] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 256000, global step 4089136: learning rate 0.0005
[2019-04-04 03:22:13,452] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.7564169e-21 2.3514899e-15 5.8637775e-17 2.2267791e-15 2.7212968e-18
 1.0000000e+00 1.5572711e-17], sum to 1.0000
[2019-04-04 03:22:13,453] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4522
[2019-04-04 03:22:13,514] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 82.66666666666667, 0.0, 0.0, 26.0, 25.07224495667634, 0.2886195118559041, 1.0, 1.0, 79614.21946783573], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 843000.0000, 
sim time next is 843600.0000, 
raw observation next is [-3.9, 83.33333333333334, 0.0, 0.0, 26.0, 25.07382495799477, 0.2678892081458549, 1.0, 1.0, 57154.22956850981], 
processed observation next is [1.0, 0.782608695652174, 0.3545706371191136, 0.8333333333333335, 0.0, 0.0, 0.6666666666666666, 0.5894854131662308, 0.5892964027152849, 1.0, 1.0, 0.2721629979452848], 
reward next is 0.7278, 
noisyNet noise sample is [array([-0.39088887], dtype=float32), 2.8463001]. 
=============================================
[2019-04-04 03:22:14,440] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.4636310e-34 3.3681660e-27 2.7960049e-28 1.2415151e-28 2.8884267e-28
 1.0000000e+00 8.9777170e-27], sum to 1.0000
[2019-04-04 03:22:14,444] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1941
[2019-04-04 03:22:14,455] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.11666666666667, 80.5, 0.0, 0.0, 26.0, 25.62171286918148, 0.5975836123727682, 0.0, 1.0, 23166.74647674163], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1062600.0000, 
sim time next is 1063200.0000, 
raw observation next is [12.93333333333334, 81.0, 0.0, 0.0, 26.0, 25.68581988043958, 0.6226771527935054, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.8208679593721148, 0.81, 0.0, 0.0, 0.6666666666666666, 0.6404849900366317, 0.7075590509311684, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.24380806], dtype=float32), -1.1335658]. 
=============================================
[2019-04-04 03:22:15,209] A3C_AGENT_WORKER-Thread-12 INFO:Local step 255500, global step 4090184: loss 0.3719
[2019-04-04 03:22:15,210] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 255500, global step 4090184: learning rate 0.0005
[2019-04-04 03:22:16,285] A3C_AGENT_WORKER-Thread-16 INFO:Local step 255500, global step 4090487: loss 0.4111
[2019-04-04 03:22:16,286] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 255500, global step 4090487: learning rate 0.0005
[2019-04-04 03:22:20,328] A3C_AGENT_WORKER-Thread-19 INFO:Local step 256500, global step 4091646: loss 0.0699
[2019-04-04 03:22:20,342] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 256500, global step 4091646: learning rate 0.0005
[2019-04-04 03:22:20,758] A3C_AGENT_WORKER-Thread-14 INFO:Local step 255000, global step 4091801: loss 0.1835
[2019-04-04 03:22:20,759] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 255000, global step 4091802: learning rate 0.0005
[2019-04-04 03:22:23,597] A3C_AGENT_WORKER-Thread-17 INFO:Local step 256500, global step 4092848: loss 0.0623
[2019-04-04 03:22:23,597] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 256500, global step 4092848: learning rate 0.0005
[2019-04-04 03:22:24,427] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.00780636e-18 8.03029162e-15 1.03670504e-13 2.89932988e-03
 1.73913771e-16 9.97100651e-01 1.85680776e-13], sum to 1.0000
[2019-04-04 03:22:24,429] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7712
[2019-04-04 03:22:24,449] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.066666666666667, 83.33333333333334, 0.0, 0.0, 26.0, 24.72630662494768, 0.2144458321165093, 0.0, 1.0, 40016.87731858368], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 531600.0000, 
sim time next is 532200.0000, 
raw observation next is [2.883333333333334, 82.66666666666667, 0.0, 0.0, 26.0, 24.70575660095009, 0.2107959903342932, 0.0, 1.0, 40071.99960732618], 
processed observation next is [0.0, 0.13043478260869565, 0.5424746075715605, 0.8266666666666667, 0.0, 0.0, 0.6666666666666666, 0.558813050079174, 0.5702653301114311, 0.0, 1.0, 0.19081904574917227], 
reward next is 0.8092, 
noisyNet noise sample is [array([0.19844392], dtype=float32), -0.8891172]. 
=============================================
[2019-04-04 03:22:25,494] A3C_AGENT_WORKER-Thread-2 INFO:Local step 257000, global step 4093412: loss 1.8162
[2019-04-04 03:22:25,494] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 257000, global step 4093412: learning rate 0.0005
[2019-04-04 03:22:27,680] A3C_AGENT_WORKER-Thread-15 INFO:Local step 256000, global step 4094061: loss 0.0123
[2019-04-04 03:22:27,696] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 256000, global step 4094065: learning rate 0.0005
[2019-04-04 03:22:28,070] A3C_AGENT_WORKER-Thread-10 INFO:Local step 256000, global step 4094173: loss 0.0163
[2019-04-04 03:22:28,090] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 256000, global step 4094174: learning rate 0.0005
[2019-04-04 03:22:28,173] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.5306924e-20 1.7452880e-14 1.1049031e-16 3.3438440e-11 1.7518498e-16
 1.0000000e+00 1.4594489e-16], sum to 1.0000
[2019-04-04 03:22:28,173] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6812
[2019-04-04 03:22:28,202] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.333333333333334, 25.66666666666666, 65.66666666666667, 584.8333333333334, 26.0, 27.42785309389255, 0.8677233684193473, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4984800.0000, 
sim time next is 4985400.0000, 
raw observation next is [8.166666666666666, 25.83333333333334, 59.33333333333334, 528.6666666666667, 26.0, 26.64100101745116, 0.7764154374614991, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.6888273314866113, 0.2583333333333334, 0.1977777777777778, 0.5841620626151014, 0.6666666666666666, 0.72008341812093, 0.7588051458204997, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.16864538], dtype=float32), -2.0138488]. 
=============================================
[2019-04-04 03:22:29,621] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:22:29,621] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:22:29,643] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run31
[2019-04-04 03:22:32,234] A3C_AGENT_WORKER-Thread-20 INFO:Local step 256000, global step 4095518: loss 0.0383
[2019-04-04 03:22:32,235] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 256000, global step 4095518: learning rate 0.0005
[2019-04-04 03:22:33,975] A3C_AGENT_WORKER-Thread-6 INFO:Local step 256000, global step 4096023: loss 0.0489
[2019-04-04 03:22:33,975] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 256000, global step 4096023: learning rate 0.0005
[2019-04-04 03:22:34,681] A3C_AGENT_WORKER-Thread-18 INFO:Local step 256000, global step 4096192: loss -0.0140
[2019-04-04 03:22:34,683] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 256000, global step 4096192: learning rate 0.0005
[2019-04-04 03:22:37,494] A3C_AGENT_WORKER-Thread-3 INFO:Local step 256000, global step 4097008: loss 0.1109
[2019-04-04 03:22:37,494] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 256000, global step 4097008: learning rate 0.0005
[2019-04-04 03:22:37,575] A3C_AGENT_WORKER-Thread-11 INFO:Local step 256500, global step 4097032: loss 0.0710
[2019-04-04 03:22:37,576] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 256500, global step 4097032: learning rate 0.0005
[2019-04-04 03:22:37,591] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:22:37,591] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:22:37,596] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run31
[2019-04-04 03:22:38,496] A3C_AGENT_WORKER-Thread-19 INFO:Local step 257000, global step 4097257: loss 2.1955
[2019-04-04 03:22:38,497] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 257000, global step 4097257: learning rate 0.0005
[2019-04-04 03:22:38,584] A3C_AGENT_WORKER-Thread-5 INFO:Local step 256000, global step 4097286: loss 0.0942
[2019-04-04 03:22:38,587] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 256000, global step 4097286: learning rate 0.0005
[2019-04-04 03:22:41,384] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.4484182e-24 3.2279715e-19 2.1699547e-18 1.4787543e-17 1.9012837e-21
 1.0000000e+00 1.8080290e-19], sum to 1.0000
[2019-04-04 03:22:41,384] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5086
[2019-04-04 03:22:41,401] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 75.0, 0.0, 0.0, 26.0, 24.32759676586603, 0.05363832678287492, 0.0, 1.0, 41608.92341805974], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 704400.0000, 
sim time next is 705000.0000, 
raw observation next is [-2.9, 75.0, 0.0, 0.0, 26.0, 24.31739657569295, 0.04642540293395908, 0.0, 1.0, 41636.20680572354], 
processed observation next is [1.0, 0.13043478260869565, 0.38227146814404434, 0.75, 0.0, 0.0, 0.6666666666666666, 0.5264497146410791, 0.5154751343113196, 0.0, 1.0, 0.19826765145582637], 
reward next is 0.8017, 
noisyNet noise sample is [array([1.0234871], dtype=float32), 0.40220886]. 
=============================================
[2019-04-04 03:22:41,404] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[82.21035 ]
 [82.13647 ]
 [82.072754]
 [82.00773 ]
 [82.016075]], R is [[82.25138092]
 [82.23073578]
 [82.21047211]
 [82.19050598]
 [82.17095947]].
[2019-04-04 03:22:42,069] A3C_AGENT_WORKER-Thread-17 INFO:Local step 257000, global step 4098209: loss 2.0819
[2019-04-04 03:22:42,069] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 257000, global step 4098209: learning rate 0.0005
[2019-04-04 03:22:44,252] A3C_AGENT_WORKER-Thread-4 INFO:Local step 255000, global step 4098699: loss 0.5123
[2019-04-04 03:22:44,253] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 255000, global step 4098699: learning rate 0.0005
[2019-04-04 03:22:47,024] A3C_AGENT_WORKER-Thread-12 INFO:Local step 256000, global step 4099353: loss 0.0485
[2019-04-04 03:22:47,030] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 256000, global step 4099354: learning rate 0.0005
[2019-04-04 03:22:47,850] A3C_AGENT_WORKER-Thread-16 INFO:Local step 256000, global step 4099531: loss 0.0111
[2019-04-04 03:22:47,851] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 256000, global step 4099531: learning rate 0.0005
[2019-04-04 03:22:48,676] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.6366365e-18 1.4578796e-12 3.1273950e-13 2.9044006e-07 1.5727276e-16
 9.9999976e-01 9.1794993e-14], sum to 1.0000
[2019-04-04 03:22:48,687] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5341
[2019-04-04 03:22:48,731] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.616666666666667, 78.50000000000001, 0.0, 0.0, 26.0, 24.58458630909381, 0.180262522426453, 0.0, 1.0, 39304.32458392317], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 875400.0000, 
sim time next is 876000.0000, 
raw observation next is [-1.533333333333333, 78.0, 0.0, 0.0, 26.0, 24.57544428953617, 0.1843025378791105, 0.0, 1.0, 39268.1284137635], 
processed observation next is [1.0, 0.13043478260869565, 0.42012927054478305, 0.78, 0.0, 0.0, 0.6666666666666666, 0.5479536907946807, 0.5614341792930369, 0.0, 1.0, 0.1869910876845881], 
reward next is 0.8130, 
noisyNet noise sample is [array([0.2115355], dtype=float32), -0.28125086]. 
=============================================
[2019-04-04 03:22:48,762] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[84.126915]
 [84.0337  ]
 [83.989655]
 [83.94331 ]
 [83.942375]], R is [[84.13169861]
 [84.10321808]
 [84.07485962]
 [84.04663849]
 [84.01866913]].
[2019-04-04 03:22:49,249] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.9477157e-21 2.9430440e-16 1.1689745e-17 1.0228240e-12 4.0241907e-20
 1.0000000e+00 5.0509597e-17], sum to 1.0000
[2019-04-04 03:22:49,250] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5081
[2019-04-04 03:22:49,265] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.716666666666667, 71.0, 0.0, 0.0, 26.0, 24.144247193852, 0.08951129824596733, 0.0, 1.0, 41539.84825419285], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 784200.0000, 
sim time next is 784800.0000, 
raw observation next is [-7.8, 71.0, 0.0, 0.0, 26.0, 24.08445295947685, 0.08090223672467375, 0.0, 1.0, 41539.14145317143], 
processed observation next is [1.0, 0.08695652173913043, 0.24653739612188366, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5070377466230708, 0.5269674122415579, 0.0, 1.0, 0.19780543549129254], 
reward next is 0.8022, 
noisyNet noise sample is [array([-1.604761], dtype=float32), 0.64100224]. 
=============================================
[2019-04-04 03:22:49,727] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-04 03:22:49,729] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 03:22:49,730] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 03:22:49,730] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 03:22:49,741] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:22:49,741] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:22:49,730] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:22:49,744] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run42
[2019-04-04 03:22:49,781] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run42
[2019-04-04 03:22:49,807] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run42
[2019-04-04 03:24:22,574] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.9816006], dtype=float32), -0.06913564]
[2019-04-04 03:24:22,574] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [2.5, 89.0, 108.0, 177.0, 26.0, 25.06593883623362, 0.4101818944677857, 1.0, 1.0, 0.0]
[2019-04-04 03:24:22,574] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 03:24:22,575] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [5.7267110e-20 3.1520939e-16 2.3789430e-17 1.7363806e-09 3.2062894e-17
 1.0000000e+00 8.0810847e-17], sampled 0.4805122820870714
[2019-04-04 03:25:57,833] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7354.7881 239654508.5115 1605.1702
[2019-04-04 03:25:57,895] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.9816006], dtype=float32), -0.06913564]
[2019-04-04 03:25:57,895] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-4.5, 30.0, 0.0, 0.0, 26.0, 25.62570957221757, 0.4893463300017037, 0.0, 1.0, 0.0]
[2019-04-04 03:25:57,895] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 03:25:57,896] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.4875431e-16 1.6874280e-12 3.4673918e-13 9.8812640e-01 1.4729437e-16
 1.1873619e-02 3.4592750e-12], sampled 0.46077158907197624
[2019-04-04 03:26:26,683] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7242.2182 263294187.9204 1551.4115
[2019-04-04 03:26:34,554] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.9339 275743873.8091 1233.2747
[2019-04-04 03:26:35,593] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 4100000, evaluation results [4100000.0, 7242.218152759944, 263294187.92041174, 1551.4115223259334, 7354.788054707083, 239654508.51151305, 1605.1702234618515, 7182.933934242192, 275743873.80914134, 1233.2746682155919]
[2019-04-04 03:26:36,821] A3C_AGENT_WORKER-Thread-2 INFO:Local step 257500, global step 4100320: loss 0.0265
[2019-04-04 03:26:36,821] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 257500, global step 4100320: learning rate 0.0005
[2019-04-04 03:26:38,225] A3C_AGENT_WORKER-Thread-14 INFO:Local step 255500, global step 4100650: loss 0.6165
[2019-04-04 03:26:38,227] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 255500, global step 4100650: learning rate 0.0005
[2019-04-04 03:26:39,214] A3C_AGENT_WORKER-Thread-13 INFO:Local step 255000, global step 4100885: loss 0.5441
[2019-04-04 03:26:39,217] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 255000, global step 4100885: learning rate 0.0005
[2019-04-04 03:26:40,766] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.6102330e-21 1.3100043e-16 1.0719919e-16 2.0811461e-13 5.1878671e-18
 1.0000000e+00 5.6013817e-18], sum to 1.0000
[2019-04-04 03:26:40,766] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0380
[2019-04-04 03:26:40,774] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.583333333333334, 65.16666666666666, 0.0, 0.0, 26.0, 26.2467148168402, 0.6895728018949067, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1623000.0000, 
sim time next is 1623600.0000, 
raw observation next is [9.4, 66.0, 0.0, 0.0, 26.0, 26.18657574318588, 0.6745769050972279, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.7229916897506927, 0.66, 0.0, 0.0, 0.6666666666666666, 0.68221464526549, 0.7248589683657426, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.07425479], dtype=float32), -0.5713276]. 
=============================================
[2019-04-04 03:26:40,867] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4644789e-24 8.1684153e-19 9.8524959e-20 1.3482031e-19 1.3536799e-21
 1.0000000e+00 6.9624148e-20], sum to 1.0000
[2019-04-04 03:26:40,868] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7633
[2019-04-04 03:26:40,924] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.56956207342317, 0.4633843226233612, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1410000.0000, 
sim time next is 1410600.0000, 
raw observation next is [-0.6, 100.0, 5.999999999999998, 0.0, 26.0, 25.48260320215841, 0.4544034240259558, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.44598337950138506, 1.0, 0.019999999999999993, 0.0, 0.6666666666666666, 0.623550266846534, 0.6514678080086519, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2164211], dtype=float32), 0.74059856]. 
=============================================
[2019-04-04 03:26:42,380] A3C_AGENT_WORKER-Thread-15 INFO:Local step 256500, global step 4101917: loss 0.1192
[2019-04-04 03:26:42,380] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 256500, global step 4101917: learning rate 0.0005
[2019-04-04 03:26:42,914] A3C_AGENT_WORKER-Thread-10 INFO:Local step 256500, global step 4102095: loss 0.0641
[2019-04-04 03:26:42,915] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 256500, global step 4102095: learning rate 0.0005
[2019-04-04 03:26:44,677] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.8639028e-16 1.3939874e-12 1.8369913e-12 4.2305012e-10 1.1324181e-15
 1.0000000e+00 1.0784974e-12], sum to 1.0000
[2019-04-04 03:26:44,678] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2945
[2019-04-04 03:26:44,731] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.566666666666666, 73.0, 128.8333333333333, 0.0, 26.0, 25.21987687631871, 0.1995735056798094, 1.0, 1.0, 37722.99299891658], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 211200.0000, 
sim time next is 211800.0000, 
raw observation next is [-6.383333333333333, 72.5, 133.6666666666667, 0.0, 26.0, 25.23125644949598, 0.2063200547822444, 1.0, 1.0, 27438.76021144688], 
processed observation next is [1.0, 0.43478260869565216, 0.28578024007386893, 0.725, 0.4455555555555557, 0.0, 0.6666666666666666, 0.602604704124665, 0.5687733515940815, 1.0, 1.0, 0.1306607629116518], 
reward next is 0.8693, 
noisyNet noise sample is [array([1.9565147], dtype=float32), -0.48040769]. 
=============================================
[2019-04-04 03:26:45,089] A3C_AGENT_WORKER-Thread-11 INFO:Local step 257000, global step 4102923: loss 2.6211
[2019-04-04 03:26:45,090] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 257000, global step 4102923: learning rate 0.0005
[2019-04-04 03:26:46,309] A3C_AGENT_WORKER-Thread-20 INFO:Local step 256500, global step 4103368: loss 0.0916
[2019-04-04 03:26:46,310] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 256500, global step 4103368: learning rate 0.0005
[2019-04-04 03:26:48,024] A3C_AGENT_WORKER-Thread-18 INFO:Local step 256500, global step 4104041: loss 0.0365
[2019-04-04 03:26:48,032] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 256500, global step 4104041: learning rate 0.0005
[2019-04-04 03:26:48,043] A3C_AGENT_WORKER-Thread-6 INFO:Local step 256500, global step 4104051: loss 0.0315
[2019-04-04 03:26:48,044] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 256500, global step 4104051: learning rate 0.0005
[2019-04-04 03:26:48,304] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.1767327e-14 8.6695756e-08 4.1120201e-09 2.1620135e-01 7.9713545e-12
 7.8379858e-01 2.8794336e-08], sum to 1.0000
[2019-04-04 03:26:48,304] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6805
[2019-04-04 03:26:48,317] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.3, 91.33333333333334, 0.0, 0.0, 26.0, 25.3403722403784, 0.4534680164838125, 0.0, 1.0, 42946.00193198321], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1734000.0000, 
sim time next is 1734600.0000, 
raw observation next is [0.25, 91.16666666666667, 0.0, 0.0, 26.0, 25.32768187495433, 0.4483725848215327, 0.0, 1.0, 42904.42533666836], 
processed observation next is [0.0, 0.043478260869565216, 0.46952908587257625, 0.9116666666666667, 0.0, 0.0, 0.6666666666666666, 0.6106401562461942, 0.6494575282738443, 0.0, 1.0, 0.2043067873174684], 
reward next is 0.7957, 
noisyNet noise sample is [array([-0.7509926], dtype=float32), 1.087714]. 
=============================================
[2019-04-04 03:26:48,699] A3C_AGENT_WORKER-Thread-19 INFO:Local step 257500, global step 4104332: loss 0.1513
[2019-04-04 03:26:48,701] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 257500, global step 4104332: learning rate 0.0005
[2019-04-04 03:26:50,777] A3C_AGENT_WORKER-Thread-17 INFO:Local step 257500, global step 4105276: loss 0.0476
[2019-04-04 03:26:50,787] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 257500, global step 4105277: learning rate 0.0005
[2019-04-04 03:26:51,077] A3C_AGENT_WORKER-Thread-3 INFO:Local step 256500, global step 4105407: loss 0.0692
[2019-04-04 03:26:51,079] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 256500, global step 4105408: learning rate 0.0005
[2019-04-04 03:26:51,414] A3C_AGENT_WORKER-Thread-5 INFO:Local step 256500, global step 4105578: loss 0.1312
[2019-04-04 03:26:51,422] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 256500, global step 4105578: learning rate 0.0005
[2019-04-04 03:26:51,739] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.0996333e-28 6.2703695e-21 2.5620861e-24 1.4130958e-23 6.3683927e-26
 1.0000000e+00 1.0975818e-22], sum to 1.0000
[2019-04-04 03:26:51,739] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9116
[2019-04-04 03:26:51,753] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 26.0, 25.28371260961472, 0.497949771303384, 0.0, 1.0, 40774.83646691036], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1382400.0000, 
sim time next is 1383000.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 26.0, 25.27500370638742, 0.4930206434979632, 0.0, 1.0, 40591.40606145238], 
processed observation next is [1.0, 0.0, 0.46260387811634357, 0.95, 0.0, 0.0, 0.6666666666666666, 0.6062503088656183, 0.664340214499321, 0.0, 1.0, 0.1932924098164399], 
reward next is 0.8067, 
noisyNet noise sample is [array([-0.42182377], dtype=float32), -0.62259173]. 
=============================================
[2019-04-04 03:26:51,769] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[86.315094]
 [86.24289 ]
 [86.24925 ]
 [86.26198 ]
 [86.1086  ]], R is [[86.38149261]
 [86.32350922]
 [86.26513672]
 [86.20639801]
 [86.14724731]].
[2019-04-04 03:26:52,016] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.8097500e-31 1.4767752e-25 7.8041724e-27 7.0489865e-24 4.4413308e-25
 1.0000000e+00 3.0395580e-25], sum to 1.0000
[2019-04-04 03:26:52,017] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4623
[2019-04-04 03:26:52,063] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.7, 92.5, 27.0, 0.0, 26.0, 25.71164218633859, 0.5083301221995778, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 981000.0000, 
sim time next is 981600.0000, 
raw observation next is [9.8, 92.33333333333333, 32.5, 0.0, 26.0, 25.92298986380746, 0.5361350022922308, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.7340720221606649, 0.9233333333333333, 0.10833333333333334, 0.0, 0.6666666666666666, 0.6602491553172882, 0.6787116674307435, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8618911], dtype=float32), -1.3467059]. 
=============================================
[2019-04-04 03:26:55,264] A3C_AGENT_WORKER-Thread-15 INFO:Local step 257000, global step 4107513: loss 2.0433
[2019-04-04 03:26:55,265] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 257000, global step 4107513: learning rate 0.0005
[2019-04-04 03:26:56,598] A3C_AGENT_WORKER-Thread-10 INFO:Local step 257000, global step 4108145: loss 2.2078
[2019-04-04 03:26:56,599] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 257000, global step 4108145: learning rate 0.0005
[2019-04-04 03:26:57,868] A3C_AGENT_WORKER-Thread-12 INFO:Local step 256500, global step 4108806: loss 0.0767
[2019-04-04 03:26:57,870] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 256500, global step 4108806: learning rate 0.0005
[2019-04-04 03:26:58,139] A3C_AGENT_WORKER-Thread-16 INFO:Local step 256500, global step 4108943: loss 0.0174
[2019-04-04 03:26:58,141] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 256500, global step 4108943: learning rate 0.0005
[2019-04-04 03:26:58,241] A3C_AGENT_WORKER-Thread-4 INFO:Local step 255500, global step 4108982: loss 0.7757
[2019-04-04 03:26:58,243] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 255500, global step 4108982: learning rate 0.0005
[2019-04-04 03:26:59,228] A3C_AGENT_WORKER-Thread-2 INFO:Local step 258000, global step 4109442: loss 0.2413
[2019-04-04 03:26:59,228] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 258000, global step 4109442: learning rate 0.0005
[2019-04-04 03:26:59,630] A3C_AGENT_WORKER-Thread-20 INFO:Local step 257000, global step 4109625: loss 2.1269
[2019-04-04 03:26:59,631] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 257000, global step 4109625: learning rate 0.0005
[2019-04-04 03:27:00,805] A3C_AGENT_WORKER-Thread-6 INFO:Local step 257000, global step 4110148: loss 2.2217
[2019-04-04 03:27:00,809] A3C_AGENT_WORKER-Thread-18 INFO:Local step 257000, global step 4110150: loss 2.2271
[2019-04-04 03:27:00,810] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 257000, global step 4110150: learning rate 0.0005
[2019-04-04 03:27:00,809] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 257000, global step 4110150: learning rate 0.0005
[2019-04-04 03:27:02,847] A3C_AGENT_WORKER-Thread-11 INFO:Local step 257500, global step 4111065: loss 0.0863
[2019-04-04 03:27:02,847] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 257500, global step 4111065: learning rate 0.0005
[2019-04-04 03:27:03,719] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.6967806e-33 2.8743974e-23 1.5153345e-26 3.5581294e-20 5.8633194e-28
 1.0000000e+00 1.0738569e-26], sum to 1.0000
[2019-04-04 03:27:03,720] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8682
[2019-04-04 03:27:03,782] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.9, 100.0, 76.66666666666667, 0.0, 26.0, 23.29986804225729, 0.1222561996367251, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1246200.0000, 
sim time next is 1246800.0000, 
raw observation next is [14.8, 100.0, 77.33333333333333, 0.0, 26.0, 23.29487108648842, 0.2411129437949887, 0.0, 1.0, 161477.0710336221], 
processed observation next is [0.0, 0.43478260869565216, 0.8725761772853187, 1.0, 0.2577777777777778, 0.0, 0.6666666666666666, 0.4412392572073684, 0.5803709812649962, 0.0, 1.0, 0.7689384334934386], 
reward next is 0.2311, 
noisyNet noise sample is [array([-1.270638], dtype=float32), -0.39413095]. 
=============================================
[2019-04-04 03:27:04,438] A3C_AGENT_WORKER-Thread-3 INFO:Local step 257000, global step 4111724: loss 2.3240
[2019-04-04 03:27:04,442] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 257000, global step 4111724: learning rate 0.0005
[2019-04-04 03:27:04,512] A3C_AGENT_WORKER-Thread-14 INFO:Local step 256000, global step 4111756: loss 0.0434
[2019-04-04 03:27:04,517] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 256000, global step 4111757: learning rate 0.0005
[2019-04-04 03:27:04,918] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.5495688e-12 2.3853334e-08 4.7748404e-07 5.8124355e-05 3.8252249e-12
 9.9994099e-01 3.0812373e-07], sum to 1.0000
[2019-04-04 03:27:04,919] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2998
[2019-04-04 03:27:04,962] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-16.2, 78.0, 0.0, 0.0, 26.0, 21.66256222472993, -0.5007834621815449, 0.0, 1.0, 49358.45442721264], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 370800.0000, 
sim time next is 371400.0000, 
raw observation next is [-16.28333333333333, 78.5, 0.0, 0.0, 26.0, 21.6057492411158, -0.5197198509821231, 0.0, 1.0, 49465.5461033371], 
processed observation next is [1.0, 0.30434782608695654, 0.011542012927054512, 0.785, 0.0, 0.0, 0.6666666666666666, 0.30047910342631656, 0.3267600496726256, 0.0, 1.0, 0.23555021953970048], 
reward next is 0.7644, 
noisyNet noise sample is [array([-2.060082], dtype=float32), -0.38463297]. 
=============================================
[2019-04-04 03:27:05,083] A3C_AGENT_WORKER-Thread-13 INFO:Local step 255500, global step 4112031: loss 0.8858
[2019-04-04 03:27:05,084] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 255500, global step 4112031: learning rate 0.0005
[2019-04-04 03:27:05,177] A3C_AGENT_WORKER-Thread-5 INFO:Local step 257000, global step 4112078: loss 1.8537
[2019-04-04 03:27:05,178] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 257000, global step 4112079: learning rate 0.0005
[2019-04-04 03:27:08,818] A3C_AGENT_WORKER-Thread-19 INFO:Local step 258000, global step 4113563: loss 0.1896
[2019-04-04 03:27:08,820] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 258000, global step 4113563: learning rate 0.0005
[2019-04-04 03:27:10,771] A3C_AGENT_WORKER-Thread-17 INFO:Local step 258000, global step 4114280: loss 0.2195
[2019-04-04 03:27:10,771] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 258000, global step 4114280: learning rate 0.0005
[2019-04-04 03:27:11,492] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0260507e-18 1.5472020e-13 3.6821766e-13 7.9434462e-12 4.9971090e-16
 1.0000000e+00 3.7573711e-14], sum to 1.0000
[2019-04-04 03:27:11,492] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0015
[2019-04-04 03:27:11,499] A3C_AGENT_WORKER-Thread-12 INFO:Local step 257000, global step 4114542: loss 1.8479
[2019-04-04 03:27:11,503] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 257000, global step 4114542: learning rate 0.0005
[2019-04-04 03:27:11,521] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.3, 75.0, 0.0, 0.0, 26.0, 24.14769182773384, 0.04417141665768756, 0.0, 1.0, 45154.62651785163], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1904400.0000, 
sim time next is 1905000.0000, 
raw observation next is [-7.383333333333333, 75.5, 0.0, 0.0, 26.0, 24.13659262846164, 0.03295988090467154, 0.0, 1.0, 45177.08170595885], 
processed observation next is [1.0, 0.043478260869565216, 0.25807940904893817, 0.755, 0.0, 0.0, 0.6666666666666666, 0.5113827190384699, 0.5109866269682238, 0.0, 1.0, 0.21512896050456595], 
reward next is 0.7849, 
noisyNet noise sample is [array([-1.2676978], dtype=float32), 0.44659415]. 
=============================================
[2019-04-04 03:27:11,555] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[82.117744]
 [82.08694 ]
 [81.865486]
 [81.64435 ]
 [81.19912 ]], R is [[82.08835602]
 [82.05245209]
 [82.01699829]
 [81.98204803]
 [81.94752502]].
[2019-04-04 03:27:12,164] A3C_AGENT_WORKER-Thread-16 INFO:Local step 257000, global step 4114828: loss 1.7019
[2019-04-04 03:27:12,165] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 257000, global step 4114828: learning rate 0.0005
[2019-04-04 03:27:13,397] A3C_AGENT_WORKER-Thread-15 INFO:Local step 257500, global step 4115359: loss 0.1735
[2019-04-04 03:27:13,398] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 257500, global step 4115359: learning rate 0.0005
[2019-04-04 03:27:14,966] A3C_AGENT_WORKER-Thread-10 INFO:Local step 257500, global step 4115913: loss 0.0584
[2019-04-04 03:27:14,967] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 257500, global step 4115913: learning rate 0.0005
[2019-04-04 03:27:17,917] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.9808889e-30 1.5073625e-24 6.9973534e-26 4.1522088e-24 4.5923562e-26
 1.0000000e+00 1.4640927e-25], sum to 1.0000
[2019-04-04 03:27:17,918] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5677
[2019-04-04 03:27:17,936] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [11.06666666666667, 55.33333333333333, 72.83333333333333, 24.33333333333334, 26.0, 26.71238563915195, 0.7403472839906565, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1528800.0000, 
sim time next is 1529400.0000, 
raw observation next is [10.78333333333333, 56.66666666666667, 58.66666666666667, 20.66666666666667, 26.0, 26.00999180222912, 0.6790670457605276, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.7613111726685133, 0.5666666666666668, 0.19555555555555557, 0.022836095764272566, 0.6666666666666666, 0.6674993168524267, 0.7263556819201759, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0051119], dtype=float32), 0.34687513]. 
=============================================
[2019-04-04 03:27:18,701] A3C_AGENT_WORKER-Thread-20 INFO:Local step 257500, global step 4117308: loss 0.0326
[2019-04-04 03:27:18,703] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 257500, global step 4117308: learning rate 0.0005
[2019-04-04 03:27:19,522] A3C_AGENT_WORKER-Thread-18 INFO:Local step 257500, global step 4117717: loss 0.0184
[2019-04-04 03:27:19,523] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 257500, global step 4117717: learning rate 0.0005
[2019-04-04 03:27:19,972] A3C_AGENT_WORKER-Thread-6 INFO:Local step 257500, global step 4117912: loss 0.0183
[2019-04-04 03:27:19,973] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 257500, global step 4117912: learning rate 0.0005
[2019-04-04 03:27:21,169] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.5928090e-27 6.5446198e-22 1.5516167e-21 7.9445094e-19 6.4007883e-25
 1.0000000e+00 1.1972141e-21], sum to 1.0000
[2019-04-04 03:27:21,170] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3623
[2019-04-04 03:27:21,182] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 85.66666666666667, 0.0, 0.0, 26.0, 24.23838941623939, 0.08507697775637217, 0.0, 1.0, 41093.74500896667], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2004000.0000, 
sim time next is 2004600.0000, 
raw observation next is [-6.1, 86.33333333333333, 0.0, 0.0, 26.0, 24.22569383547589, 0.08144325240425083, 0.0, 1.0, 41130.94234648373], 
processed observation next is [1.0, 0.17391304347826086, 0.29362880886426596, 0.8633333333333333, 0.0, 0.0, 0.6666666666666666, 0.5188078196229909, 0.527147750801417, 0.0, 1.0, 0.1958616302213511], 
reward next is 0.8041, 
noisyNet noise sample is [array([-0.8879137], dtype=float32), 1.9780113]. 
=============================================
[2019-04-04 03:27:23,419] A3C_AGENT_WORKER-Thread-3 INFO:Local step 257500, global step 4119167: loss 0.0356
[2019-04-04 03:27:23,421] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 257500, global step 4119168: learning rate 0.0005
[2019-04-04 03:27:24,023] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.8394407e-27 1.2589852e-20 5.4460279e-20 1.1119155e-20 1.6181714e-22
 1.0000000e+00 2.6761292e-22], sum to 1.0000
[2019-04-04 03:27:24,024] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5830
[2019-04-04 03:27:24,065] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.166666666666666, 81.0, 0.0, 0.0, 26.0, 25.51259349861963, 0.4828255897429279, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1582800.0000, 
sim time next is 1583400.0000, 
raw observation next is [5.083333333333334, 81.5, 13.0, 15.0, 26.0, 25.47029302177277, 0.4726356141345236, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.6034164358264081, 0.815, 0.043333333333333335, 0.016574585635359115, 0.6666666666666666, 0.622524418481064, 0.6575452047115079, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.61746633], dtype=float32), -0.819628]. 
=============================================
[2019-04-04 03:27:24,125] A3C_AGENT_WORKER-Thread-2 INFO:Local step 258500, global step 4119418: loss 0.0097
[2019-04-04 03:27:24,130] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 258500, global step 4119418: learning rate 0.0005
[2019-04-04 03:27:24,392] A3C_AGENT_WORKER-Thread-11 INFO:Local step 258000, global step 4119513: loss 0.1071
[2019-04-04 03:27:24,393] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 258000, global step 4119513: learning rate 0.0005
[2019-04-04 03:27:24,444] A3C_AGENT_WORKER-Thread-4 INFO:Local step 256000, global step 4119532: loss 0.0019
[2019-04-04 03:27:24,445] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 256000, global step 4119532: learning rate 0.0005
[2019-04-04 03:27:24,661] A3C_AGENT_WORKER-Thread-5 INFO:Local step 257500, global step 4119611: loss 0.1177
[2019-04-04 03:27:24,662] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 257500, global step 4119611: learning rate 0.0005
[2019-04-04 03:27:25,922] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.7358232e-29 4.8925747e-23 4.1741565e-24 2.3043935e-27 5.2609686e-25
 1.0000000e+00 8.4432246e-25], sum to 1.0000
[2019-04-04 03:27:25,926] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4056
[2019-04-04 03:27:25,943] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.6, 82.0, 0.0, 0.0, 26.0, 25.57989296310545, 0.4588207171755943, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 957600.0000, 
sim time next is 958200.0000, 
raw observation next is [6.783333333333333, 81.66666666666667, 0.0, 0.0, 26.0, 25.58502477087351, 0.4497181645669935, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.6505078485687905, 0.8166666666666668, 0.0, 0.0, 0.6666666666666666, 0.6320853975727925, 0.6499060548556644, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.0546198], dtype=float32), -1.687213]. 
=============================================
[2019-04-04 03:27:26,624] A3C_AGENT_WORKER-Thread-14 INFO:Local step 256500, global step 4120429: loss 0.0920
[2019-04-04 03:27:26,627] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 256500, global step 4120431: learning rate 0.0005
[2019-04-04 03:27:30,398] A3C_AGENT_WORKER-Thread-12 INFO:Local step 257500, global step 4121766: loss 0.0333
[2019-04-04 03:27:30,401] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 257500, global step 4121771: learning rate 0.0005
[2019-04-04 03:27:31,422] A3C_AGENT_WORKER-Thread-16 INFO:Local step 257500, global step 4122166: loss 0.0606
[2019-04-04 03:27:31,423] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 257500, global step 4122166: learning rate 0.0005
[2019-04-04 03:27:31,492] A3C_AGENT_WORKER-Thread-13 INFO:Local step 256000, global step 4122192: loss 0.0159
[2019-04-04 03:27:31,492] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 256000, global step 4122192: learning rate 0.0005
[2019-04-04 03:27:32,313] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0296098e-20 4.5252557e-15 6.2994879e-17 5.9118478e-15 6.8143521e-19
 1.0000000e+00 1.2436326e-15], sum to 1.0000
[2019-04-04 03:27:32,313] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4014
[2019-04-04 03:27:32,334] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.5, 91.0, 0.0, 0.0, 26.0, 23.40948981330546, -0.09018438839159297, 0.0, 1.0, 43041.82067648143], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2271600.0000, 
sim time next is 2272200.0000, 
raw observation next is [-9.5, 91.00000000000001, 0.0, 0.0, 26.0, 23.32747848271345, -0.1014316268457142, 0.0, 1.0, 43019.49708680777], 
processed observation next is [1.0, 0.30434782608695654, 0.1994459833795014, 0.9100000000000001, 0.0, 0.0, 0.6666666666666666, 0.44395654022612074, 0.4661894577180952, 0.0, 1.0, 0.20485474803241796], 
reward next is 0.7951, 
noisyNet noise sample is [array([-0.8481195], dtype=float32), 0.05271191]. 
=============================================
[2019-04-04 03:27:33,525] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.7130656e-19 5.2101894e-14 1.2975930e-12 7.6587857e-13 1.0396650e-15
 1.0000000e+00 1.8590583e-14], sum to 1.0000
[2019-04-04 03:27:33,525] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3101
[2019-04-04 03:27:33,554] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.1, 75.0, 0.0, 0.0, 26.0, 24.3660335341046, 0.05870419451152312, 0.0, 1.0, 41570.94561183515], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 703800.0000, 
sim time next is 704400.0000, 
raw observation next is [-3.0, 75.0, 0.0, 0.0, 26.0, 24.32776691328259, 0.05368148684225191, 0.0, 1.0, 41608.80374984926], 
processed observation next is [1.0, 0.13043478260869565, 0.3795013850415513, 0.75, 0.0, 0.0, 0.6666666666666666, 0.5273139094402159, 0.5178938289474173, 0.0, 1.0, 0.1981371607135679], 
reward next is 0.8019, 
noisyNet noise sample is [array([0.1682053], dtype=float32), 1.0044706]. 
=============================================
[2019-04-04 03:27:34,184] A3C_AGENT_WORKER-Thread-19 INFO:Local step 258500, global step 4123158: loss 0.0019
[2019-04-04 03:27:34,193] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 258500, global step 4123158: learning rate 0.0005
[2019-04-04 03:27:35,462] A3C_AGENT_WORKER-Thread-15 INFO:Local step 258000, global step 4123595: loss 0.0228
[2019-04-04 03:27:35,464] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 258000, global step 4123595: learning rate 0.0005
[2019-04-04 03:27:36,269] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.1196830e-34 1.1260140e-28 7.0180030e-30 5.7613490e-29 6.0821676e-33
 1.0000000e+00 1.0911739e-29], sum to 1.0000
[2019-04-04 03:27:36,271] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8244
[2019-04-04 03:27:36,287] A3C_AGENT_WORKER-Thread-17 INFO:Local step 258500, global step 4123856: loss 0.0033
[2019-04-04 03:27:36,288] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 258500, global step 4123856: learning rate 0.0005
[2019-04-04 03:27:36,311] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.2333333333333334, 54.66666666666667, 123.1666666666667, 504.0, 26.0, 25.79085561168439, 0.3737080815946212, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 735600.0000, 
sim time next is 736200.0000, 
raw observation next is [-0.04999999999999999, 53.5, 131.0, 449.0, 26.0, 25.74845481990387, 0.3607843449832689, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.461218836565097, 0.535, 0.43666666666666665, 0.49613259668508286, 0.6666666666666666, 0.6457045683253225, 0.6202614483277563, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.12879987], dtype=float32), -0.053723104]. 
=============================================
[2019-04-04 03:27:36,435] A3C_AGENT_WORKER-Thread-10 INFO:Local step 258000, global step 4123911: loss 0.0452
[2019-04-04 03:27:36,436] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 258000, global step 4123911: learning rate 0.0005
[2019-04-04 03:27:40,226] A3C_AGENT_WORKER-Thread-20 INFO:Local step 258000, global step 4125167: loss 0.0253
[2019-04-04 03:27:40,228] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 258000, global step 4125167: learning rate 0.0005
[2019-04-04 03:27:41,356] A3C_AGENT_WORKER-Thread-14 INFO:Local step 257000, global step 4125530: loss 1.2188
[2019-04-04 03:27:41,362] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 257000, global step 4125532: learning rate 0.0005
[2019-04-04 03:27:42,161] A3C_AGENT_WORKER-Thread-18 INFO:Local step 258000, global step 4125816: loss 0.0242
[2019-04-04 03:27:42,161] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 258000, global step 4125816: learning rate 0.0005
[2019-04-04 03:27:42,273] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.2919035e-23 6.4328957e-20 4.3330997e-20 6.5598549e-18 8.1783490e-22
 1.0000000e+00 1.1467789e-19], sum to 1.0000
[2019-04-04 03:27:42,277] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9809
[2019-04-04 03:27:42,309] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.916666666666667, 74.33333333333333, 78.33333333333334, 0.0, 26.0, 25.70513796712641, 0.2913826803308275, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 814200.0000, 
sim time next is 814800.0000, 
raw observation next is [-5.633333333333334, 73.66666666666667, 82.66666666666666, 0.0, 26.0, 25.65454139179841, 0.2897459023934748, 1.0, 1.0, 45492.69958248451], 
processed observation next is [1.0, 0.43478260869565216, 0.30655586334256696, 0.7366666666666667, 0.2755555555555555, 0.0, 0.6666666666666666, 0.6378784493165343, 0.5965819674644915, 1.0, 1.0, 0.21663190277373576], 
reward next is 0.7834, 
noisyNet noise sample is [array([0.71128637], dtype=float32), 0.4312274]. 
=============================================
[2019-04-04 03:27:42,877] A3C_AGENT_WORKER-Thread-6 INFO:Local step 258000, global step 4126063: loss 0.0177
[2019-04-04 03:27:42,879] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 258000, global step 4126063: learning rate 0.0005
[2019-04-04 03:27:45,260] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.7463174e-24 3.5680552e-18 8.1982853e-20 2.5092750e-16 7.0716103e-24
 1.0000000e+00 1.5275336e-19], sum to 1.0000
[2019-04-04 03:27:45,260] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1752
[2019-04-04 03:27:45,284] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.3, 80.83333333333334, 0.0, 0.0, 26.0, 24.14095606550396, 0.05155656452339148, 0.0, 1.0, 45049.22043949272], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1901400.0000, 
sim time next is 1902000.0000, 
raw observation next is [-7.300000000000001, 79.66666666666667, 0.0, 0.0, 26.0, 24.10526165434614, 0.0526594683065249, 0.0, 1.0, 45079.63867116239], 
processed observation next is [1.0, 0.0, 0.26038781163434904, 0.7966666666666667, 0.0, 0.0, 0.6666666666666666, 0.5087718045288451, 0.517553156102175, 0.0, 1.0, 0.21466494605315425], 
reward next is 0.7853, 
noisyNet noise sample is [array([-1.1341839], dtype=float32), -0.27632102]. 
=============================================
[2019-04-04 03:27:45,292] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[76.47423 ]
 [75.62362 ]
 [74.49949 ]
 [74.48216 ]
 [74.452415]], R is [[77.09297943]
 [77.10752869]
 [77.12210846]
 [77.13669586]
 [77.15126038]].
[2019-04-04 03:27:45,770] A3C_AGENT_WORKER-Thread-3 INFO:Local step 258000, global step 4126903: loss 0.0192
[2019-04-04 03:27:45,772] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 258000, global step 4126903: learning rate 0.0005
[2019-04-04 03:27:46,548] A3C_AGENT_WORKER-Thread-5 INFO:Local step 258000, global step 4127155: loss 0.0714
[2019-04-04 03:27:46,549] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 258000, global step 4127155: learning rate 0.0005
[2019-04-04 03:27:46,879] A3C_AGENT_WORKER-Thread-4 INFO:Local step 256500, global step 4127260: loss 0.0547
[2019-04-04 03:27:46,880] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 256500, global step 4127260: learning rate 0.0005
[2019-04-04 03:27:49,601] A3C_AGENT_WORKER-Thread-2 INFO:Local step 259000, global step 4128178: loss 0.2753
[2019-04-04 03:27:49,625] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 259000, global step 4128179: learning rate 0.0005
[2019-04-04 03:27:50,069] A3C_AGENT_WORKER-Thread-11 INFO:Local step 258500, global step 4128351: loss 0.0019
[2019-04-04 03:27:50,070] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 258500, global step 4128351: learning rate 0.0005
[2019-04-04 03:27:53,294] A3C_AGENT_WORKER-Thread-12 INFO:Local step 258000, global step 4129352: loss 0.0301
[2019-04-04 03:27:53,295] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 258000, global step 4129352: learning rate 0.0005
[2019-04-04 03:27:53,781] A3C_AGENT_WORKER-Thread-16 INFO:Local step 258000, global step 4129496: loss 0.0147
[2019-04-04 03:27:53,781] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 258000, global step 4129496: learning rate 0.0005
[2019-04-04 03:27:54,394] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.6464954e-37 6.7649741e-30 1.0099786e-31 5.3440985e-34 9.0534796e-35
 1.0000000e+00 2.2503466e-31], sum to 1.0000
[2019-04-04 03:27:54,396] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9956
[2019-04-04 03:27:54,412] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 88.5, 0.0, 0.0, 26.0, 24.83280840501304, 0.2783342376345959, 0.0, 1.0, 42570.37306962177], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2071800.0000, 
sim time next is 2072400.0000, 
raw observation next is [-4.5, 89.33333333333334, 0.0, 0.0, 26.0, 24.79039288850343, 0.2696477359642017, 0.0, 1.0, 42628.10590976518], 
processed observation next is [1.0, 1.0, 0.3379501385041552, 0.8933333333333334, 0.0, 0.0, 0.6666666666666666, 0.5658660740419524, 0.5898825786547339, 0.0, 1.0, 0.20299098052269135], 
reward next is 0.7970, 
noisyNet noise sample is [array([-0.32863072], dtype=float32), -0.3170775]. 
=============================================
[2019-04-04 03:27:54,684] A3C_AGENT_WORKER-Thread-13 INFO:Local step 256500, global step 4129782: loss 0.0170
[2019-04-04 03:27:54,697] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 256500, global step 4129782: learning rate 0.0005
[2019-04-04 03:27:59,577] A3C_AGENT_WORKER-Thread-19 INFO:Local step 259000, global step 4131579: loss 0.2116
[2019-04-04 03:27:59,583] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 259000, global step 4131579: learning rate 0.0005
[2019-04-04 03:28:00,979] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.8042153e-25 1.1318939e-18 8.9647056e-21 5.7465243e-17 8.0668613e-23
 1.0000000e+00 1.7112133e-19], sum to 1.0000
[2019-04-04 03:28:00,983] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4152
[2019-04-04 03:28:01,027] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.700000000000001, 81.33333333333334, 0.0, 0.0, 26.0, 23.98471657263793, 0.04788898214737917, 0.0, 1.0, 43623.51523155575], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2096400.0000, 
sim time next is 2097000.0000, 
raw observation next is [-6.7, 80.5, 0.0, 0.0, 26.0, 23.91855378758319, 0.03998914358776553, 0.0, 1.0, 43631.33545055603], 
processed observation next is [1.0, 0.2608695652173913, 0.2770083102493075, 0.805, 0.0, 0.0, 0.6666666666666666, 0.4932128156319324, 0.5133297145292551, 0.0, 1.0, 0.2077682640502668], 
reward next is 0.7922, 
noisyNet noise sample is [array([1.0502468], dtype=float32), -0.37970573]. 
=============================================
[2019-04-04 03:28:01,039] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[80.79884 ]
 [80.80448 ]
 [80.84566 ]
 [80.9149  ]
 [80.984726]], R is [[80.78002167]
 [80.76448822]
 [80.74923706]
 [80.73429871]
 [80.71981049]].
[2019-04-04 03:28:01,090] A3C_AGENT_WORKER-Thread-14 INFO:Local step 257500, global step 4132093: loss 0.0504
[2019-04-04 03:28:01,091] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 257500, global step 4132093: learning rate 0.0005
[2019-04-04 03:28:01,666] A3C_AGENT_WORKER-Thread-4 INFO:Local step 257000, global step 4132276: loss 1.7644
[2019-04-04 03:28:01,667] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 257000, global step 4132276: learning rate 0.0005
[2019-04-04 03:28:02,163] A3C_AGENT_WORKER-Thread-15 INFO:Local step 258500, global step 4132455: loss 0.1815
[2019-04-04 03:28:02,167] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 258500, global step 4132458: learning rate 0.0005
[2019-04-04 03:28:02,415] A3C_AGENT_WORKER-Thread-17 INFO:Local step 259000, global step 4132538: loss 0.1327
[2019-04-04 03:28:02,415] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 259000, global step 4132538: learning rate 0.0005
[2019-04-04 03:28:02,713] A3C_AGENT_WORKER-Thread-10 INFO:Local step 258500, global step 4132659: loss 0.0798
[2019-04-04 03:28:02,716] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 258500, global step 4132659: learning rate 0.0005
[2019-04-04 03:28:06,078] A3C_AGENT_WORKER-Thread-20 INFO:Local step 258500, global step 4133964: loss 0.0080
[2019-04-04 03:28:06,080] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 258500, global step 4133964: learning rate 0.0005
[2019-04-04 03:28:07,731] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.2036082e-24 2.2639571e-18 8.7433291e-19 2.0200554e-15 1.5424166e-21
 1.0000000e+00 7.0422854e-19], sum to 1.0000
[2019-04-04 03:28:07,731] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9600
[2019-04-04 03:28:07,810] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.466666666666667, 80.0, 60.16666666666666, 30.83333333333334, 26.0, 25.28326289734873, 0.3149292193313995, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2103600.0000, 
sim time next is 2104200.0000, 
raw observation next is [-7.55, 80.5, 72.0, 37.0, 26.0, 25.40898578397912, 0.3263695123499973, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.25346260387811637, 0.805, 0.24, 0.04088397790055249, 0.6666666666666666, 0.6174154819982599, 0.608789837449999, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.12998907], dtype=float32), -0.30102137]. 
=============================================
[2019-04-04 03:28:07,953] A3C_AGENT_WORKER-Thread-18 INFO:Local step 258500, global step 4134493: loss 0.0121
[2019-04-04 03:28:07,955] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 258500, global step 4134493: learning rate 0.0005
[2019-04-04 03:28:09,002] A3C_AGENT_WORKER-Thread-13 INFO:Local step 257000, global step 4134851: loss 1.6487
[2019-04-04 03:28:09,025] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 257000, global step 4134851: learning rate 0.0005
[2019-04-04 03:28:09,110] A3C_AGENT_WORKER-Thread-6 INFO:Local step 258500, global step 4134883: loss 0.0043
[2019-04-04 03:28:09,111] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 258500, global step 4134883: learning rate 0.0005
[2019-04-04 03:28:11,877] A3C_AGENT_WORKER-Thread-3 INFO:Local step 258500, global step 4135816: loss 0.0378
[2019-04-04 03:28:11,880] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 258500, global step 4135816: learning rate 0.0005
[2019-04-04 03:28:11,920] A3C_AGENT_WORKER-Thread-2 INFO:Local step 259500, global step 4135829: loss 0.0015
[2019-04-04 03:28:11,920] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 259500, global step 4135829: learning rate 0.0005
[2019-04-04 03:28:12,060] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.7139219e-22 8.3512300e-17 1.9559918e-17 2.6558387e-12 2.3826526e-21
 1.0000000e+00 1.5335080e-17], sum to 1.0000
[2019-04-04 03:28:12,073] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7991
[2019-04-04 03:28:12,105] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.45, 85.0, 0.0, 0.0, 26.0, 24.21895824303805, 0.09925941804944581, 0.0, 1.0, 43340.42919493283], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2093400.0000, 
sim time next is 2094000.0000, 
raw observation next is [-6.533333333333333, 84.33333333333333, 0.0, 0.0, 26.0, 24.18206259031269, 0.08848812219328912, 0.0, 1.0, 43424.79528980947], 
processed observation next is [1.0, 0.21739130434782608, 0.2816251154201293, 0.8433333333333333, 0.0, 0.0, 0.6666666666666666, 0.5151718825260575, 0.5294960407310964, 0.0, 1.0, 0.2067847394752832], 
reward next is 0.7932, 
noisyNet noise sample is [array([-0.82278967], dtype=float32), 1.6255498]. 
=============================================
[2019-04-04 03:28:12,134] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[77.815216]
 [77.86746 ]
 [77.909225]
 [77.96518 ]
 [78.068085]], R is [[77.79421997]
 [77.80989838]
 [77.82434845]
 [77.83742523]
 [77.85202789]].
[2019-04-04 03:28:12,220] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.0278321e-20 6.9969385e-15 7.2244102e-17 4.7641289e-16 1.2489167e-19
 1.0000000e+00 4.1067812e-15], sum to 1.0000
[2019-04-04 03:28:12,220] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2994
[2019-04-04 03:28:12,263] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 25.13660558130185, 0.410129377413181, 0.0, 1.0, 73821.35445340477], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2147400.0000, 
sim time next is 2148000.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 25.18022640443588, 0.4149593917640839, 0.0, 1.0, 53245.37374200506], 
processed observation next is [1.0, 0.8695652173913043, 0.30747922437673136, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5983522003696568, 0.6383197972546947, 0.0, 1.0, 0.25354939877145266], 
reward next is 0.7465, 
noisyNet noise sample is [array([0.88409585], dtype=float32), -1.9247825]. 
=============================================
[2019-04-04 03:28:12,271] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[77.25568 ]
 [77.25035 ]
 [77.704956]
 [78.54913 ]
 [79.488846]], R is [[77.28626251]
 [77.16186523]
 [76.86721802]
 [76.7556839 ]
 [76.40026855]].
[2019-04-04 03:28:13,914] A3C_AGENT_WORKER-Thread-5 INFO:Local step 258500, global step 4136393: loss 0.0107
[2019-04-04 03:28:13,915] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 258500, global step 4136393: learning rate 0.0005
[2019-04-04 03:28:15,873] A3C_AGENT_WORKER-Thread-11 INFO:Local step 259000, global step 4136989: loss 0.0960
[2019-04-04 03:28:15,875] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 259000, global step 4136990: learning rate 0.0005
[2019-04-04 03:28:19,253] A3C_AGENT_WORKER-Thread-12 INFO:Local step 258500, global step 4138148: loss 0.0080
[2019-04-04 03:28:19,261] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 258500, global step 4138148: learning rate 0.0005
[2019-04-04 03:28:19,614] A3C_AGENT_WORKER-Thread-16 INFO:Local step 258500, global step 4138280: loss 0.0362
[2019-04-04 03:28:19,626] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 258500, global step 4138280: learning rate 0.0005
[2019-04-04 03:28:20,981] A3C_AGENT_WORKER-Thread-4 INFO:Local step 257500, global step 4138689: loss 0.0100
[2019-04-04 03:28:20,981] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 257500, global step 4138689: learning rate 0.0005
[2019-04-04 03:28:22,478] A3C_AGENT_WORKER-Thread-19 INFO:Local step 259500, global step 4139156: loss 0.0013
[2019-04-04 03:28:22,479] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 259500, global step 4139156: learning rate 0.0005
[2019-04-04 03:28:23,461] A3C_AGENT_WORKER-Thread-14 INFO:Local step 258000, global step 4139442: loss 0.1027
[2019-04-04 03:28:23,462] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 258000, global step 4139442: learning rate 0.0005
[2019-04-04 03:28:25,009] A3C_AGENT_WORKER-Thread-17 INFO:Local step 259500, global step 4140002: loss 0.0384
[2019-04-04 03:28:25,010] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 259500, global step 4140002: learning rate 0.0005
[2019-04-04 03:28:27,821] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.3509850e-17 7.3780809e-13 2.1501428e-14 1.7720187e-03 1.6699819e-17
 9.9822801e-01 2.1360008e-13], sum to 1.0000
[2019-04-04 03:28:27,824] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7443
[2019-04-04 03:28:27,948] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.166666666666667, 49.33333333333334, 231.5, 157.6666666666667, 26.0, 25.08758605938344, 0.3138573361508877, 1.0, 1.0, 132698.6732542416], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2637600.0000, 
sim time next is 2638200.0000, 
raw observation next is [-0.8833333333333332, 48.16666666666667, 218.0, 168.3333333333333, 26.0, 24.39376867487794, 0.2998075000684869, 1.0, 1.0, 197762.3754950179], 
processed observation next is [1.0, 0.5217391304347826, 0.43813481071098803, 0.4816666666666667, 0.7266666666666667, 0.18600368324125224, 0.6666666666666666, 0.5328140562398284, 0.5999358333561623, 1.0, 1.0, 0.9417255975953234], 
reward next is 0.0583, 
noisyNet noise sample is [array([1.4765829], dtype=float32), 1.5536691]. 
=============================================
[2019-04-04 03:28:28,863] A3C_AGENT_WORKER-Thread-15 INFO:Local step 259000, global step 4141301: loss 0.0825
[2019-04-04 03:28:28,864] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 259000, global step 4141301: learning rate 0.0005
[2019-04-04 03:28:29,173] A3C_AGENT_WORKER-Thread-10 INFO:Local step 259000, global step 4141408: loss 0.0918
[2019-04-04 03:28:29,175] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 259000, global step 4141410: learning rate 0.0005
[2019-04-04 03:28:29,371] A3C_AGENT_WORKER-Thread-13 INFO:Local step 257500, global step 4141482: loss 0.0072
[2019-04-04 03:28:29,384] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 257500, global step 4141483: learning rate 0.0005
[2019-04-04 03:28:32,270] A3C_AGENT_WORKER-Thread-20 INFO:Local step 259000, global step 4142506: loss 0.0920
[2019-04-04 03:28:32,271] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 259000, global step 4142506: learning rate 0.0005
[2019-04-04 03:28:34,158] A3C_AGENT_WORKER-Thread-18 INFO:Local step 259000, global step 4143103: loss 0.0298
[2019-04-04 03:28:34,158] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 259000, global step 4143103: learning rate 0.0005
[2019-04-04 03:28:34,986] A3C_AGENT_WORKER-Thread-6 INFO:Local step 259000, global step 4143469: loss 0.0875
[2019-04-04 03:28:34,986] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 259000, global step 4143469: learning rate 0.0005
[2019-04-04 03:28:35,671] A3C_AGENT_WORKER-Thread-2 INFO:Local step 260000, global step 4143733: loss 0.0406
[2019-04-04 03:28:35,672] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 260000, global step 4143733: learning rate 0.0005
[2019-04-04 03:28:38,467] A3C_AGENT_WORKER-Thread-3 INFO:Local step 259000, global step 4144606: loss 0.0927
[2019-04-04 03:28:38,468] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 259000, global step 4144606: learning rate 0.0005
[2019-04-04 03:28:39,123] A3C_AGENT_WORKER-Thread-11 INFO:Local step 259500, global step 4144842: loss 0.0010
[2019-04-04 03:28:39,124] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 259500, global step 4144842: learning rate 0.0005
[2019-04-04 03:28:39,807] A3C_AGENT_WORKER-Thread-5 INFO:Local step 259000, global step 4145048: loss 0.0932
[2019-04-04 03:28:39,809] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 259000, global step 4145048: learning rate 0.0005
[2019-04-04 03:28:43,656] A3C_AGENT_WORKER-Thread-4 INFO:Local step 258000, global step 4146462: loss 0.0513
[2019-04-04 03:28:43,657] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 258000, global step 4146462: learning rate 0.0005
[2019-04-04 03:28:45,100] A3C_AGENT_WORKER-Thread-16 INFO:Local step 259000, global step 4146940: loss 0.0710
[2019-04-04 03:28:45,101] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 259000, global step 4146940: learning rate 0.0005
[2019-04-04 03:28:45,187] A3C_AGENT_WORKER-Thread-12 INFO:Local step 259000, global step 4146968: loss 0.0963
[2019-04-04 03:28:45,188] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 259000, global step 4146968: learning rate 0.0005
[2019-04-04 03:28:45,280] A3C_AGENT_WORKER-Thread-19 INFO:Local step 260000, global step 4147004: loss 0.0342
[2019-04-04 03:28:45,281] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 260000, global step 4147004: learning rate 0.0005
[2019-04-04 03:28:45,477] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.1284279e-16 1.4864543e-10 1.6234240e-11 9.8799545e-01 1.9535984e-17
 1.2004545e-02 6.3087549e-11], sum to 1.0000
[2019-04-04 03:28:45,477] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1115
[2019-04-04 03:28:45,595] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-15.0, 83.0, 0.0, 0.0, 26.0, 22.69860559061254, -0.2064098036183962, 0.0, 1.0, 43298.92513781194], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2704200.0000, 
sim time next is 2704800.0000, 
raw observation next is [-15.0, 83.0, 0.0, 0.0, 26.0, 22.65843088721968, -0.1114259012193268, 0.0, 1.0, 202379.7743053204], 
processed observation next is [1.0, 0.30434782608695654, 0.04709141274238226, 0.83, 0.0, 0.0, 0.6666666666666666, 0.3882025739349733, 0.46285803292689104, 0.0, 1.0, 0.9637132109777162], 
reward next is 0.0363, 
noisyNet noise sample is [array([-1.0065736], dtype=float32), 0.10283588]. 
=============================================
[2019-04-04 03:28:48,477] A3C_AGENT_WORKER-Thread-17 INFO:Local step 260000, global step 4148121: loss 0.0107
[2019-04-04 03:28:48,479] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 260000, global step 4148122: learning rate 0.0005
[2019-04-04 03:28:49,229] A3C_AGENT_WORKER-Thread-14 INFO:Local step 258500, global step 4148405: loss 0.1480
[2019-04-04 03:28:49,230] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 258500, global step 4148405: learning rate 0.0005
[2019-04-04 03:28:49,811] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.9935692e-17 4.4457632e-14 5.8931321e-13 1.8966235e-01 1.8046486e-16
 8.1033766e-01 1.3650579e-12], sum to 1.0000
[2019-04-04 03:28:49,813] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0144
[2019-04-04 03:28:49,853] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 52.5, 136.0, 33.0, 26.0, 25.70124480786043, 0.2930395344526524, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2539800.0000, 
sim time next is 2540400.0000, 
raw observation next is [-1.733333333333333, 51.33333333333333, 135.5, 35.0, 26.0, 25.75344463809813, 0.3003357306421055, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.41458910433979695, 0.5133333333333333, 0.45166666666666666, 0.03867403314917127, 0.6666666666666666, 0.6461203865081776, 0.6001119102140352, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.43386072], dtype=float32), -0.5609871]. 
=============================================
[2019-04-04 03:28:51,291] A3C_AGENT_WORKER-Thread-10 INFO:Local step 259500, global step 4149119: loss 0.0095
[2019-04-04 03:28:51,291] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 259500, global step 4149119: learning rate 0.0005
[2019-04-04 03:28:51,700] A3C_AGENT_WORKER-Thread-15 INFO:Local step 259500, global step 4149268: loss 0.1085
[2019-04-04 03:28:51,701] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 259500, global step 4149268: learning rate 0.0005
[2019-04-04 03:28:52,524] A3C_AGENT_WORKER-Thread-13 INFO:Local step 258000, global step 4149533: loss -0.0107
[2019-04-04 03:28:52,524] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 258000, global step 4149533: learning rate 0.0005
[2019-04-04 03:28:54,465] A3C_AGENT_WORKER-Thread-20 INFO:Local step 259500, global step 4150233: loss 0.0101
[2019-04-04 03:28:54,466] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 259500, global step 4150235: learning rate 0.0005
[2019-04-04 03:28:55,131] A3C_AGENT_WORKER-Thread-2 INFO:Local step 260500, global step 4150476: loss 0.1897
[2019-04-04 03:28:55,133] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 260500, global step 4150476: learning rate 0.0005
[2019-04-04 03:28:57,192] A3C_AGENT_WORKER-Thread-18 INFO:Local step 259500, global step 4151169: loss 0.0069
[2019-04-04 03:28:57,193] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 259500, global step 4151169: learning rate 0.0005
[2019-04-04 03:28:58,106] A3C_AGENT_WORKER-Thread-6 INFO:Local step 259500, global step 4151486: loss 0.0098
[2019-04-04 03:28:58,108] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 259500, global step 4151486: learning rate 0.0005
[2019-04-04 03:29:00,914] A3C_AGENT_WORKER-Thread-3 INFO:Local step 259500, global step 4152491: loss 0.1266
[2019-04-04 03:29:00,916] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 259500, global step 4152492: learning rate 0.0005
[2019-04-04 03:29:02,525] A3C_AGENT_WORKER-Thread-5 INFO:Local step 259500, global step 4153059: loss 0.0043
[2019-04-04 03:29:02,525] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 259500, global step 4153059: learning rate 0.0005
[2019-04-04 03:29:02,536] A3C_AGENT_WORKER-Thread-11 INFO:Local step 260000, global step 4153065: loss 0.0199
[2019-04-04 03:29:02,539] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 260000, global step 4153065: learning rate 0.0005
[2019-04-04 03:29:05,338] A3C_AGENT_WORKER-Thread-19 INFO:Local step 260500, global step 4154070: loss 0.0042
[2019-04-04 03:29:05,345] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 260500, global step 4154070: learning rate 0.0005
[2019-04-04 03:29:07,100] A3C_AGENT_WORKER-Thread-16 INFO:Local step 259500, global step 4154641: loss 0.0995
[2019-04-04 03:29:07,101] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 259500, global step 4154641: learning rate 0.0005
[2019-04-04 03:29:07,911] A3C_AGENT_WORKER-Thread-17 INFO:Local step 260500, global step 4154939: loss 0.2899
[2019-04-04 03:29:07,929] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 260500, global step 4154939: learning rate 0.0005
[2019-04-04 03:29:08,180] A3C_AGENT_WORKER-Thread-12 INFO:Local step 259500, global step 4155042: loss 0.0288
[2019-04-04 03:29:08,181] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 259500, global step 4155042: learning rate 0.0005
[2019-04-04 03:29:09,059] A3C_AGENT_WORKER-Thread-4 INFO:Local step 258500, global step 4155333: loss 0.0001
[2019-04-04 03:29:09,061] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 258500, global step 4155333: learning rate 0.0005
[2019-04-04 03:29:13,774] A3C_AGENT_WORKER-Thread-14 INFO:Local step 259000, global step 4157038: loss 0.0444
[2019-04-04 03:29:13,778] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 259000, global step 4157042: learning rate 0.0005
[2019-04-04 03:29:13,810] A3C_AGENT_WORKER-Thread-10 INFO:Local step 260000, global step 4157052: loss 0.0445
[2019-04-04 03:29:13,811] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 260000, global step 4157052: learning rate 0.0005
[2019-04-04 03:29:14,000] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.4935090e-25 2.2394760e-19 9.0553305e-23 1.0000000e+00 1.5233575e-24
 5.8742422e-10 8.1235088e-20], sum to 1.0000
[2019-04-04 03:29:14,000] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6117
[2019-04-04 03:29:14,013] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.3, 26.33333333333334, 59.66666666666667, 613.1666666666667, 26.0, 24.97655762507049, 0.2780381733759598, 0.0, 1.0, 18697.67111049219], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2475600.0000, 
sim time next is 2476200.0000, 
raw observation next is [3.3, 26.16666666666667, 57.33333333333333, 546.3333333333334, 26.0, 24.97956874799821, 0.274620701745774, 0.0, 1.0, 18697.52599203882], 
processed observation next is [0.0, 0.6521739130434783, 0.554016620498615, 0.2616666666666667, 0.1911111111111111, 0.6036832412523021, 0.6666666666666666, 0.5816307289998509, 0.591540233915258, 0.0, 1.0, 0.08903583805732772], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.11305993], dtype=float32), -0.72936285]. 
=============================================
[2019-04-04 03:29:14,225] A3C_AGENT_WORKER-Thread-2 INFO:Local step 261000, global step 4157225: loss 12.5084
[2019-04-04 03:29:14,225] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 261000, global step 4157225: learning rate 0.0005
[2019-04-04 03:29:14,692] A3C_AGENT_WORKER-Thread-15 INFO:Local step 260000, global step 4157411: loss 0.0397
[2019-04-04 03:29:14,693] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 260000, global step 4157411: learning rate 0.0005
[2019-04-04 03:29:17,711] A3C_AGENT_WORKER-Thread-20 INFO:Local step 260000, global step 4158610: loss 0.0405
[2019-04-04 03:29:17,712] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 260000, global step 4158610: learning rate 0.0005
[2019-04-04 03:29:18,354] A3C_AGENT_WORKER-Thread-13 INFO:Local step 258500, global step 4158818: loss 0.0019
[2019-04-04 03:29:18,355] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 258500, global step 4158818: learning rate 0.0005
[2019-04-04 03:29:19,441] A3C_AGENT_WORKER-Thread-18 INFO:Local step 260000, global step 4159244: loss 0.0125
[2019-04-04 03:29:19,442] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 260000, global step 4159244: learning rate 0.0005
[2019-04-04 03:29:21,390] A3C_AGENT_WORKER-Thread-6 INFO:Local step 260000, global step 4160062: loss 0.0199
[2019-04-04 03:29:21,390] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 260000, global step 4160062: learning rate 0.0005
[2019-04-04 03:29:21,762] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.2086670e-22 1.5996910e-15 3.0180915e-17 1.0000000e+00 6.0350540e-22
 3.4166884e-08 5.3804560e-16], sum to 1.0000
[2019-04-04 03:29:21,765] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9116
[2019-04-04 03:29:21,802] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.166666666666667, 50.83333333333334, 116.6666666666667, 819.3333333333334, 26.0, 25.19585472198949, 0.4506328210904778, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3588600.0000, 
sim time next is 3589200.0000, 
raw observation next is [-2.0, 50.0, 116.0, 817.5, 26.0, 25.19951913551582, 0.4517901484295015, 0.0, 1.0, 18703.71936326857], 
processed observation next is [0.0, 0.5652173913043478, 0.40720221606648205, 0.5, 0.38666666666666666, 0.9033149171270718, 0.6666666666666666, 0.5999599279596518, 0.6505967161431672, 0.0, 1.0, 0.0890653303012789], 
reward next is 0.9109, 
noisyNet noise sample is [array([1.2206516], dtype=float32), 1.7590044]. 
=============================================
[2019-04-04 03:29:21,849] A3C_AGENT_WORKER-Thread-11 INFO:Local step 260500, global step 4160260: loss 0.0079
[2019-04-04 03:29:21,851] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 260500, global step 4160260: learning rate 0.0005
[2019-04-04 03:29:23,553] A3C_AGENT_WORKER-Thread-3 INFO:Local step 260000, global step 4160921: loss 0.0046
[2019-04-04 03:29:23,554] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 260000, global step 4160921: learning rate 0.0005
[2019-04-04 03:29:23,601] A3C_AGENT_WORKER-Thread-19 INFO:Local step 261000, global step 4160936: loss 2.4841
[2019-04-04 03:29:23,604] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 261000, global step 4160938: learning rate 0.0005
[2019-04-04 03:29:24,633] A3C_AGENT_WORKER-Thread-5 INFO:Local step 260000, global step 4161376: loss 0.0057
[2019-04-04 03:29:24,633] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 260000, global step 4161376: learning rate 0.0005
[2019-04-04 03:29:26,143] A3C_AGENT_WORKER-Thread-17 INFO:Local step 261000, global step 4162057: loss 12.5891
[2019-04-04 03:29:26,144] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 261000, global step 4162057: learning rate 0.0005
[2019-04-04 03:29:29,521] A3C_AGENT_WORKER-Thread-16 INFO:Local step 260000, global step 4163504: loss 0.0031
[2019-04-04 03:29:29,523] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 260000, global step 4163505: learning rate 0.0005
[2019-04-04 03:29:30,471] A3C_AGENT_WORKER-Thread-12 INFO:Local step 260000, global step 4163884: loss 0.0038
[2019-04-04 03:29:30,472] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 260000, global step 4163884: learning rate 0.0005
[2019-04-04 03:29:32,103] A3C_AGENT_WORKER-Thread-2 INFO:Local step 261500, global step 4164610: loss 0.0646
[2019-04-04 03:29:32,104] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 261500, global step 4164610: learning rate 0.0005
[2019-04-04 03:29:32,185] A3C_AGENT_WORKER-Thread-10 INFO:Local step 260500, global step 4164639: loss 0.2149
[2019-04-04 03:29:32,186] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 260500, global step 4164639: learning rate 0.0005
[2019-04-04 03:29:32,914] A3C_AGENT_WORKER-Thread-4 INFO:Local step 259000, global step 4164954: loss 0.0116
[2019-04-04 03:29:32,915] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 259000, global step 4164954: learning rate 0.0005
[2019-04-04 03:29:33,030] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.7017195e-21 3.2789130e-14 3.9561007e-16 5.0791318e-09 1.4577559e-18
 1.0000000e+00 6.6279739e-17], sum to 1.0000
[2019-04-04 03:29:33,030] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4619
[2019-04-04 03:29:33,063] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.8333333333333334, 100.0, 0.0, 0.0, 26.0, 25.27188941104745, 0.3353043658228618, 0.0, 1.0, 39535.69080600623], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3113400.0000, 
sim time next is 3114000.0000, 
raw observation next is [1.0, 100.0, 0.0, 0.0, 26.0, 25.40561549045871, 0.3402381556248165, 0.0, 1.0, 20747.81083849273], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6171346242048926, 0.6134127185416055, 0.0, 1.0, 0.09879909923091776], 
reward next is 0.9012, 
noisyNet noise sample is [array([-0.7138715], dtype=float32), -1.2881768]. 
=============================================
[2019-04-04 03:29:33,077] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[82.62909 ]
 [82.729645]
 [82.8302  ]
 [82.89421 ]
 [82.949394]], R is [[82.56749725]
 [82.55355835]
 [82.54075623]
 [82.52739716]
 [82.51364899]].
[2019-04-04 03:29:33,222] A3C_AGENT_WORKER-Thread-15 INFO:Local step 260500, global step 4165089: loss 1.4324
[2019-04-04 03:29:33,223] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 260500, global step 4165089: learning rate 0.0005
[2019-04-04 03:29:34,912] A3C_AGENT_WORKER-Thread-14 INFO:Local step 259500, global step 4165750: loss 0.0030
[2019-04-04 03:29:34,912] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 259500, global step 4165750: learning rate 0.0005
[2019-04-04 03:29:35,845] A3C_AGENT_WORKER-Thread-20 INFO:Local step 260500, global step 4166175: loss 0.0487
[2019-04-04 03:29:35,846] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 260500, global step 4166175: learning rate 0.0005
[2019-04-04 03:29:37,161] A3C_AGENT_WORKER-Thread-18 INFO:Local step 260500, global step 4166763: loss 0.0613
[2019-04-04 03:29:37,164] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 260500, global step 4166763: learning rate 0.0005
[2019-04-04 03:29:38,967] A3C_AGENT_WORKER-Thread-11 INFO:Local step 261000, global step 4167592: loss 12.1207
[2019-04-04 03:29:38,968] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 261000, global step 4167592: learning rate 0.0005
[2019-04-04 03:29:39,213] A3C_AGENT_WORKER-Thread-6 INFO:Local step 260500, global step 4167696: loss 0.2090
[2019-04-04 03:29:39,240] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 260500, global step 4167696: learning rate 0.0005
[2019-04-04 03:29:40,748] A3C_AGENT_WORKER-Thread-3 INFO:Local step 260500, global step 4168343: loss 0.2405
[2019-04-04 03:29:40,749] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 260500, global step 4168343: learning rate 0.0005
[2019-04-04 03:29:41,318] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.2144239e-19 2.8263689e-12 2.3720516e-12 8.3672768e-03 2.1961048e-19
 9.9163276e-01 3.7890440e-13], sum to 1.0000
[2019-04-04 03:29:41,319] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6051
[2019-04-04 03:29:41,336] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 26.0, 25.41351532608543, 0.4989970882326524, 0.0, 1.0, 67276.69822835154], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3276600.0000, 
sim time next is 3277200.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 26.0, 25.43988285107442, 0.4883235989441794, 0.0, 1.0, 36369.81337636411], 
processed observation next is [1.0, 0.9565217391304348, 0.296398891966759, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6199902375895349, 0.6627745329813931, 0.0, 1.0, 0.17318958750649577], 
reward next is 0.8268, 
noisyNet noise sample is [array([1.462525], dtype=float32), -0.61224014]. 
=============================================
[2019-04-04 03:29:41,361] A3C_AGENT_WORKER-Thread-19 INFO:Local step 261500, global step 4168603: loss 0.0383
[2019-04-04 03:29:41,362] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 261500, global step 4168603: learning rate 0.0005
[2019-04-04 03:29:41,432] A3C_AGENT_WORKER-Thread-13 INFO:Local step 259000, global step 4168646: loss 0.0147
[2019-04-04 03:29:41,433] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 259000, global step 4168646: learning rate 0.0005
[2019-04-04 03:29:41,959] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.3624292e-20 3.2486400e-16 4.0919820e-16 5.6152476e-04 6.9142673e-19
 9.9943846e-01 7.7616048e-16], sum to 1.0000
[2019-04-04 03:29:41,959] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1183
[2019-04-04 03:29:42,038] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.666666666666667, 63.33333333333334, 0.0, 0.0, 26.0, 24.46940863167361, 0.2700119423745341, 1.0, 1.0, 202408.555768082], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3396000.0000, 
sim time next is 3396600.0000, 
raw observation next is [-2.5, 62.5, 2.0, 107.0, 26.0, 24.92116039896337, 0.3300644925704961, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.39335180055401664, 0.625, 0.006666666666666667, 0.11823204419889503, 0.6666666666666666, 0.5767633665802808, 0.6100214975234987, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.37530366], dtype=float32), 0.33616766]. 
=============================================
[2019-04-04 03:29:42,537] A3C_AGENT_WORKER-Thread-5 INFO:Local step 260500, global step 4169121: loss 0.0401
[2019-04-04 03:29:42,538] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 260500, global step 4169121: learning rate 0.0005
[2019-04-04 03:29:43,496] A3C_AGENT_WORKER-Thread-17 INFO:Local step 261500, global step 4169529: loss 0.0782
[2019-04-04 03:29:43,503] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 261500, global step 4169529: learning rate 0.0005
[2019-04-04 03:29:44,507] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.1481302e-15 8.9167866e-12 4.7811854e-12 1.2142872e-01 8.7770748e-15
 8.7857127e-01 1.9711269e-11], sum to 1.0000
[2019-04-04 03:29:44,507] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9169
[2019-04-04 03:29:44,523] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 48.5, 155.0, 206.0, 26.0, 25.98581489873162, 0.4784205559913586, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2647800.0000, 
sim time next is 2648400.0000, 
raw observation next is [0.5, 49.0, 141.6666666666667, 192.3333333333333, 26.0, 26.04004682624782, 0.4765543415291677, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.4764542936288089, 0.49, 0.4722222222222224, 0.21252302025782682, 0.6666666666666666, 0.6700039021873184, 0.6588514471763892, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.381775], dtype=float32), -0.6443361]. 
=============================================
[2019-04-04 03:29:46,447] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.2700523e-13 4.4455747e-10 4.0761289e-10 1.7670434e-02 3.0012364e-13
 9.8232955e-01 9.5637376e-10], sum to 1.0000
[2019-04-04 03:29:46,448] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0543
[2019-04-04 03:29:46,463] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.833333333333333, 33.0, 210.1666666666667, 98.66666666666666, 26.0, 25.64864087664761, 0.3171152238354417, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2551200.0000, 
sim time next is 2551800.0000, 
raw observation next is [2.016666666666667, 31.5, 202.3333333333333, 175.3333333333333, 26.0, 25.64865295263818, 0.312836440115329, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.5184672206832872, 0.315, 0.6744444444444443, 0.1937384898710865, 0.6666666666666666, 0.6373877460531817, 0.6042788133717764, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.9623094], dtype=float32), 0.3042191]. 
=============================================
[2019-04-04 03:29:46,550] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.55972639e-16 9.26905636e-11 2.00026211e-11 4.62337630e-03
 3.34612361e-15 9.95376587e-01 1.12256614e-10], sum to 1.0000
[2019-04-04 03:29:46,551] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5647
[2019-04-04 03:29:46,561] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.666666666666667, 71.0, 0.0, 0.0, 26.0, 25.26301289814914, 0.4394749711148949, 1.0, 1.0, 53802.46708175739], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3435600.0000, 
sim time next is 3436200.0000, 
raw observation next is [1.5, 73.0, 0.0, 0.0, 26.0, 25.1866783551628, 0.4291803372599074, 1.0, 1.0, 59686.59348849064], 
processed observation next is [1.0, 0.782608695652174, 0.5041551246537397, 0.73, 0.0, 0.0, 0.6666666666666666, 0.5988898629302334, 0.6430601124199692, 1.0, 1.0, 0.2842218737547173], 
reward next is 0.7158, 
noisyNet noise sample is [array([0.07018866], dtype=float32), 1.0945071]. 
=============================================
[2019-04-04 03:29:47,365] A3C_AGENT_WORKER-Thread-16 INFO:Local step 260500, global step 4171164: loss 0.1599
[2019-04-04 03:29:47,366] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 260500, global step 4171165: learning rate 0.0005
[2019-04-04 03:29:47,995] A3C_AGENT_WORKER-Thread-12 INFO:Local step 260500, global step 4171461: loss 0.0085
[2019-04-04 03:29:47,997] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 260500, global step 4171461: learning rate 0.0005
[2019-04-04 03:29:50,345] A3C_AGENT_WORKER-Thread-10 INFO:Local step 261000, global step 4172490: loss 9.6606
[2019-04-04 03:29:50,346] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 261000, global step 4172490: learning rate 0.0005
[2019-04-04 03:29:50,494] A3C_AGENT_WORKER-Thread-15 INFO:Local step 261000, global step 4172572: loss 10.7020
[2019-04-04 03:29:50,496] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 261000, global step 4172573: learning rate 0.0005
[2019-04-04 03:29:50,668] A3C_AGENT_WORKER-Thread-2 INFO:Local step 262000, global step 4172654: loss 0.4945
[2019-04-04 03:29:50,669] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 262000, global step 4172654: learning rate 0.0005
[2019-04-04 03:29:51,622] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.3036975e-17 2.4238332e-13 6.2996803e-13 9.9868280e-01 1.5050616e-16
 1.3172363e-03 4.6804710e-13], sum to 1.0000
[2019-04-04 03:29:51,623] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3258
[2019-04-04 03:29:51,656] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.166666666666667, 35.5, 107.3333333333333, 709.0, 26.0, 26.33968392993473, 0.5405537148465022, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4096200.0000, 
sim time next is 4096800.0000, 
raw observation next is [-2.0, 35.0, 109.0, 724.0, 26.0, 26.42823986262841, 0.561765719469837, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.40720221606648205, 0.35, 0.36333333333333334, 0.8, 0.6666666666666666, 0.7023533218857008, 0.687255239823279, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.22149232], dtype=float32), 0.8225426]. 
=============================================
[2019-04-04 03:29:52,846] A3C_AGENT_WORKER-Thread-20 INFO:Local step 261000, global step 4173771: loss 11.1311
[2019-04-04 03:29:52,847] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 261000, global step 4173771: learning rate 0.0005
[2019-04-04 03:29:53,122] A3C_AGENT_WORKER-Thread-4 INFO:Local step 259500, global step 4173911: loss 0.0254
[2019-04-04 03:29:53,125] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 259500, global step 4173911: learning rate 0.0005
[2019-04-04 03:29:53,953] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.0519597e-22 1.8423122e-18 2.8304458e-18 1.0000000e+00 1.3230578e-22
 1.3775779e-13 4.9679687e-17], sum to 1.0000
[2019-04-04 03:29:53,953] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9159
[2019-04-04 03:29:53,971] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 68.33333333333333, 14.66666666666666, 118.1666666666667, 26.0, 23.60195820864882, -0.04178100314533262, 0.0, 1.0, 40808.65686807837], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3051600.0000, 
sim time next is 3052200.0000, 
raw observation next is [-6.0, 66.16666666666667, 28.33333333333333, 166.3333333333333, 26.0, 23.57551878632735, -0.0374305444208335, 0.0, 1.0, 40758.06680810255], 
processed observation next is [0.0, 0.30434782608695654, 0.296398891966759, 0.6616666666666667, 0.09444444444444443, 0.18379373848987104, 0.6666666666666666, 0.4646265655272792, 0.4875231518597221, 0.0, 1.0, 0.19408603241953595], 
reward next is 0.8059, 
noisyNet noise sample is [array([-0.1603642], dtype=float32), -0.14129812]. 
=============================================
[2019-04-04 03:29:54,132] A3C_AGENT_WORKER-Thread-18 INFO:Local step 261000, global step 4174401: loss 11.7273
[2019-04-04 03:29:54,136] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 261000, global step 4174404: learning rate 0.0005
[2019-04-04 03:29:55,829] A3C_AGENT_WORKER-Thread-6 INFO:Local step 261000, global step 4175245: loss 11.9204
[2019-04-04 03:29:55,829] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 261000, global step 4175245: learning rate 0.0005
[2019-04-04 03:29:55,925] A3C_AGENT_WORKER-Thread-14 INFO:Local step 260000, global step 4175288: loss 0.0542
[2019-04-04 03:29:55,928] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 260000, global step 4175290: learning rate 0.0005
[2019-04-04 03:29:56,001] A3C_AGENT_WORKER-Thread-11 INFO:Local step 261500, global step 4175324: loss -0.0645
[2019-04-04 03:29:56,002] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 261500, global step 4175325: learning rate 0.0005
[2019-04-04 03:29:56,830] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.9765399e-20 1.1599147e-16 8.9032904e-16 9.9997056e-01 5.6663961e-19
 2.9456356e-05 2.1699519e-15], sum to 1.0000
[2019-04-04 03:29:56,831] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5727
[2019-04-04 03:29:56,838] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.333333333333333, 29.33333333333334, 0.0, 0.0, 26.0, 25.64153539810536, 0.3687069300659672, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2828400.0000, 
sim time next is 2829000.0000, 
raw observation next is [5.166666666666667, 29.66666666666666, 0.0, 0.0, 26.0, 25.5136249515057, 0.3671772480980673, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.6057248384118191, 0.29666666666666663, 0.0, 0.0, 0.6666666666666666, 0.6261354126254751, 0.6223924160326891, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.60762113], dtype=float32), -1.1239111]. 
=============================================
[2019-04-04 03:29:56,853] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[85.37921]
 [85.60851]
 [85.70235]
 [85.85515]
 [85.98672]], R is [[85.65288544]
 [85.7963562 ]
 [85.93839264]
 [86.07901001]
 [86.21822357]].
[2019-04-04 03:29:57,394] A3C_AGENT_WORKER-Thread-3 INFO:Local step 261000, global step 4176055: loss 11.2546
[2019-04-04 03:29:57,394] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 261000, global step 4176055: learning rate 0.0005
[2019-04-04 03:29:59,270] A3C_AGENT_WORKER-Thread-19 INFO:Local step 262000, global step 4176933: loss 0.3299
[2019-04-04 03:29:59,270] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 262000, global step 4176933: learning rate 0.0005
[2019-04-04 03:29:59,284] A3C_AGENT_WORKER-Thread-5 INFO:Local step 261000, global step 4176944: loss 10.9395
[2019-04-04 03:29:59,301] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 261000, global step 4176944: learning rate 0.0005
[2019-04-04 03:29:59,504] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.0075625e-17 1.3667515e-11 1.3563312e-13 7.8441775e-01 6.6336910e-16
 2.1558224e-01 5.6298551e-12], sum to 1.0000
[2019-04-04 03:29:59,505] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4278
[2019-04-04 03:29:59,535] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 48.0, 99.66666666666666, 760.3333333333333, 26.0, 26.95835942434635, 0.6416185664127635, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3855000.0000, 
sim time next is 3855600.0000, 
raw observation next is [2.0, 48.0, 96.5, 749.5, 26.0, 26.33249755955417, 0.676685833549123, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.518005540166205, 0.48, 0.32166666666666666, 0.8281767955801105, 0.6666666666666666, 0.6943747966295142, 0.7255619445163743, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.276995], dtype=float32), -0.8936939]. 
=============================================
[2019-04-04 03:30:00,602] A3C_AGENT_WORKER-Thread-17 INFO:Local step 262000, global step 4177638: loss 0.9543
[2019-04-04 03:30:00,622] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 262000, global step 4177643: learning rate 0.0005
[2019-04-04 03:30:00,652] A3C_AGENT_WORKER-Thread-13 INFO:Local step 259500, global step 4177665: loss -0.0003
[2019-04-04 03:30:00,653] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 259500, global step 4177665: learning rate 0.0005
[2019-04-04 03:30:01,572] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.0886886e-19 1.2206253e-13 3.8166488e-12 1.3209565e-02 4.7413884e-19
 9.8679048e-01 1.8833757e-13], sum to 1.0000
[2019-04-04 03:30:01,577] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6698
[2019-04-04 03:30:01,595] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.81643862333008, 0.2855431314827311, 0.0, 1.0, 43962.07326985119], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3819600.0000, 
sim time next is 3820200.0000, 
raw observation next is [-4.166666666666667, 72.0, 0.0, 0.0, 26.0, 24.87877521177581, 0.2837381017789587, 0.0, 1.0, 43895.1370361952], 
processed observation next is [1.0, 0.21739130434782608, 0.3471837488457987, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5732312676479842, 0.5945793672596529, 0.0, 1.0, 0.20902446207712], 
reward next is 0.7910, 
noisyNet noise sample is [array([-3.457283], dtype=float32), 1.5708619]. 
=============================================
[2019-04-04 03:30:03,517] A3C_AGENT_WORKER-Thread-16 INFO:Local step 261000, global step 4179107: loss 11.1252
[2019-04-04 03:30:03,519] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 261000, global step 4179107: learning rate 0.0005
[2019-04-04 03:30:04,217] A3C_AGENT_WORKER-Thread-12 INFO:Local step 261000, global step 4179472: loss 11.8567
[2019-04-04 03:30:04,217] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 261000, global step 4179472: learning rate 0.0005
[2019-04-04 03:30:04,727] A3C_AGENT_WORKER-Thread-2 INFO:Local step 262500, global step 4179757: loss 0.0141
[2019-04-04 03:30:04,729] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 262500, global step 4179757: learning rate 0.0005
[2019-04-04 03:30:05,576] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.9561122e-17 1.1028235e-12 1.4873578e-12 2.0321512e-01 2.2673007e-16
 7.9678488e-01 1.5065610e-13], sum to 1.0000
[2019-04-04 03:30:05,577] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8810
[2019-04-04 03:30:05,614] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 71.0, 96.0, 563.5, 26.0, 26.07576106891174, 0.5106205641436842, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3834000.0000, 
sim time next is 3834600.0000, 
raw observation next is [-3.666666666666667, 69.16666666666667, 97.66666666666667, 602.3333333333334, 26.0, 26.13716868985281, 0.5202500002346192, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.3610341643582641, 0.6916666666666668, 0.3255555555555556, 0.6655616942909761, 0.6666666666666666, 0.6780973908210676, 0.6734166667448731, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.79306537], dtype=float32), -2.3435855]. 
=============================================
[2019-04-04 03:30:06,299] A3C_AGENT_WORKER-Thread-15 INFO:Local step 261500, global step 4180554: loss 0.0450
[2019-04-04 03:30:06,301] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 261500, global step 4180555: learning rate 0.0005
[2019-04-04 03:30:06,377] A3C_AGENT_WORKER-Thread-10 INFO:Local step 261500, global step 4180591: loss 0.0666
[2019-04-04 03:30:06,378] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 261500, global step 4180591: learning rate 0.0005
[2019-04-04 03:30:09,117] A3C_AGENT_WORKER-Thread-20 INFO:Local step 261500, global step 4181846: loss 0.1178
[2019-04-04 03:30:09,121] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 261500, global step 4181847: learning rate 0.0005
[2019-04-04 03:30:10,442] A3C_AGENT_WORKER-Thread-18 INFO:Local step 261500, global step 4182473: loss 0.0466
[2019-04-04 03:30:10,445] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 261500, global step 4182473: learning rate 0.0005
[2019-04-04 03:30:11,833] A3C_AGENT_WORKER-Thread-6 INFO:Local step 261500, global step 4183088: loss -1.3920
[2019-04-04 03:30:11,843] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 261500, global step 4183089: learning rate 0.0005
[2019-04-04 03:30:12,698] A3C_AGENT_WORKER-Thread-11 INFO:Local step 262000, global step 4183419: loss 0.2784
[2019-04-04 03:30:12,702] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 262000, global step 4183422: learning rate 0.0005
[2019-04-04 03:30:13,088] A3C_AGENT_WORKER-Thread-4 INFO:Local step 260000, global step 4183605: loss 0.0047
[2019-04-04 03:30:13,088] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 260000, global step 4183605: learning rate 0.0005
[2019-04-04 03:30:13,194] A3C_AGENT_WORKER-Thread-14 INFO:Local step 260500, global step 4183663: loss 0.0811
[2019-04-04 03:30:13,196] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 260500, global step 4183664: learning rate 0.0005
[2019-04-04 03:30:13,684] A3C_AGENT_WORKER-Thread-3 INFO:Local step 261500, global step 4183871: loss -0.5192
[2019-04-04 03:30:13,686] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 261500, global step 4183872: learning rate 0.0005
[2019-04-04 03:30:13,975] A3C_AGENT_WORKER-Thread-19 INFO:Local step 262500, global step 4183999: loss 0.0340
[2019-04-04 03:30:13,979] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 262500, global step 4184000: learning rate 0.0005
[2019-04-04 03:30:14,948] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.4768213e-16 3.2287370e-10 4.5129750e-10 8.7857428e-05 1.8676675e-16
 9.9991214e-01 5.9834172e-11], sum to 1.0000
[2019-04-04 03:30:14,948] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5020
[2019-04-04 03:30:14,967] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.833333333333333, 64.16666666666667, 0.0, 0.0, 26.0, 25.38835031981777, 0.4472568770045697, 0.0, 1.0, 55070.54852650986], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3887400.0000, 
sim time next is 3888000.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.35388918251171, 0.4436653751289545, 0.0, 1.0, 56577.07901629811], 
processed observation next is [1.0, 0.0, 0.40720221606648205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6128240985426426, 0.6478884583763181, 0.0, 1.0, 0.26941466198237196], 
reward next is 0.7306, 
noisyNet noise sample is [array([0.14532095], dtype=float32), -0.16236109]. 
=============================================
[2019-04-04 03:30:14,984] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[82.04557 ]
 [82.032715]
 [82.11506 ]
 [82.27901 ]
 [82.391045]], R is [[81.79922485]
 [81.71899414]
 [81.7657547 ]
 [81.85874939]
 [81.93508911]].
[2019-04-04 03:30:15,024] A3C_AGENT_WORKER-Thread-17 INFO:Local step 262500, global step 4184509: loss -0.0059
[2019-04-04 03:30:15,025] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 262500, global step 4184509: learning rate 0.0005
[2019-04-04 03:30:15,639] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0263107e-20 1.4853914e-15 4.6082230e-15 4.9774990e-06 6.6993849e-20
 9.9999499e-01 3.3851081e-15], sum to 1.0000
[2019-04-04 03:30:15,639] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0552
[2019-04-04 03:30:15,664] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.666666666666667, 75.0, 0.0, 0.0, 26.0, 25.33570162102545, 0.3682014712599754, 0.0, 1.0, 41767.34880422287], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3904800.0000, 
sim time next is 3905400.0000, 
raw observation next is [-3.833333333333333, 76.0, 0.0, 0.0, 26.0, 25.27426220257826, 0.3587809492278559, 0.0, 1.0, 41180.91126486395], 
processed observation next is [1.0, 0.17391304347826086, 0.3564173591874424, 0.76, 0.0, 0.0, 0.6666666666666666, 0.6061885168815216, 0.6195936497426187, 0.0, 1.0, 0.1960995774517331], 
reward next is 0.8039, 
noisyNet noise sample is [array([0.5252867], dtype=float32), 0.8828003]. 
=============================================
[2019-04-04 03:30:15,796] A3C_AGENT_WORKER-Thread-5 INFO:Local step 261500, global step 4184896: loss -0.4266
[2019-04-04 03:30:15,799] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 261500, global step 4184897: learning rate 0.0005
[2019-04-04 03:30:18,497] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [9.0756127e-19 1.2042306e-14 7.3568816e-14 9.9999976e-01 7.1667585e-19
 2.0531431e-07 4.4255609e-13], sum to 1.0000
[2019-04-04 03:30:18,499] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5433
[2019-04-04 03:30:18,529] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.4574493e-19 2.1207516e-14 3.8457096e-15 1.0000000e+00 1.4119942e-19
 9.2234918e-12 4.9049095e-15], sum to 1.0000
[2019-04-04 03:30:18,529] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8662
[2019-04-04 03:30:18,535] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 39.5, 0.0, 0.0, 26.0, 24.56335734151053, 0.1662696449166153, 0.0, 1.0, 40119.39263242789], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4084200.0000, 
sim time next is 4084800.0000, 
raw observation next is [-4.666666666666666, 40.0, 0.0, 0.0, 26.0, 24.56295479628503, 0.1631557776133446, 0.0, 1.0, 40073.95524024699], 
processed observation next is [1.0, 0.2608695652173913, 0.33333333333333337, 0.4, 0.0, 0.0, 0.6666666666666666, 0.5469128996904192, 0.5543852592044481, 0.0, 1.0, 0.19082835828689043], 
reward next is 0.8092, 
noisyNet noise sample is [array([0.26317504], dtype=float32), -0.31802234]. 
=============================================
[2019-04-04 03:30:18,575] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.5, 42.0, 112.0, 781.0, 26.0, 26.57732936091568, 0.5825980242020713, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4012200.0000, 
sim time next is 4012800.0000, 
raw observation next is [-8.333333333333334, 41.33333333333333, 113.1666666666667, 786.8333333333334, 26.0, 26.58831393036956, 0.5816149070518916, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.23176361957525393, 0.4133333333333333, 0.37722222222222235, 0.8694290976058933, 0.6666666666666666, 0.7156928275307965, 0.6938716356839638, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.19250484], dtype=float32), 0.49172437]. 
=============================================
[2019-04-04 03:30:19,805] A3C_AGENT_WORKER-Thread-12 INFO:Local step 261500, global step 4186935: loss 0.0488
[2019-04-04 03:30:19,807] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 261500, global step 4186936: learning rate 0.0005
[2019-04-04 03:30:19,898] A3C_AGENT_WORKER-Thread-16 INFO:Local step 261500, global step 4186991: loss 0.0423
[2019-04-04 03:30:19,902] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 261500, global step 4186993: learning rate 0.0005
[2019-04-04 03:30:20,477] A3C_AGENT_WORKER-Thread-2 INFO:Local step 263000, global step 4187290: loss 0.0725
[2019-04-04 03:30:20,478] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 263000, global step 4187290: learning rate 0.0005
[2019-04-04 03:30:20,491] A3C_AGENT_WORKER-Thread-13 INFO:Local step 260000, global step 4187296: loss 0.0146
[2019-04-04 03:30:20,493] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 260000, global step 4187296: learning rate 0.0005
[2019-04-04 03:30:22,126] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.1776291e-22 2.9038752e-18 5.4937773e-17 1.0000000e+00 2.3531865e-22
 9.8224716e-13 3.5059006e-16], sum to 1.0000
[2019-04-04 03:30:22,127] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7795
[2019-04-04 03:30:22,168] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.5, 42.5, 0.0, 0.0, 26.0, 25.33243255582441, 0.4153067751321447, 0.0, 1.0, 40990.57957012922], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4152600.0000, 
sim time next is 4153200.0000, 
raw observation next is [-1.666666666666667, 43.66666666666666, 0.0, 0.0, 26.0, 25.34918393993919, 0.4118445297171776, 0.0, 1.0, 39606.82673512182], 
processed observation next is [0.0, 0.043478260869565216, 0.4164358264081256, 0.4366666666666666, 0.0, 0.0, 0.6666666666666666, 0.6124319949949326, 0.6372815099057259, 0.0, 1.0, 0.1886039368339134], 
reward next is 0.8114, 
noisyNet noise sample is [array([-0.62390274], dtype=float32), -1.3411491]. 
=============================================
[2019-04-04 03:30:22,515] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.0767378e-21 6.3781952e-17 5.3366884e-17 1.0000000e+00 3.9955081e-21
 1.2693760e-15 1.9668056e-15], sum to 1.0000
[2019-04-04 03:30:22,522] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2469
[2019-04-04 03:30:22,526] A3C_AGENT_WORKER-Thread-15 INFO:Local step 262000, global step 4188283: loss 0.4442
[2019-04-04 03:30:22,529] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 262000, global step 4188284: learning rate 0.0005
[2019-04-04 03:30:22,541] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.55, 41.5, 0.0, 0.0, 26.0, 25.02952658576731, 0.3126515802865666, 0.0, 1.0, 25777.6810841606], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4213800.0000, 
sim time next is 4214400.0000, 
raw observation next is [1.5, 41.66666666666667, 0.0, 0.0, 26.0, 25.01182231688612, 0.3071089485232068, 0.0, 1.0, 33754.19105588187], 
processed observation next is [0.0, 0.782608695652174, 0.5041551246537397, 0.41666666666666674, 0.0, 0.0, 0.6666666666666666, 0.5843185264071767, 0.6023696495077356, 0.0, 1.0, 0.160734243123247], 
reward next is 0.8393, 
noisyNet noise sample is [array([0.3876101], dtype=float32), 0.2564028]. 
=============================================
[2019-04-04 03:30:23,226] A3C_AGENT_WORKER-Thread-10 INFO:Local step 262000, global step 4188685: loss 0.6345
[2019-04-04 03:30:23,227] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 262000, global step 4188685: learning rate 0.0005
[2019-04-04 03:30:24,439] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.3685779e-21 3.6497723e-16 1.7734327e-15 6.6217291e-01 1.0139325e-20
 3.3782703e-01 2.3017083e-15], sum to 1.0000
[2019-04-04 03:30:24,440] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5061
[2019-04-04 03:30:24,487] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.066666666666666, 92.33333333333334, 0.0, 0.0, 26.0, 24.0917453613723, 0.1410429368443672, 0.0, 1.0, 41648.13123070736], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4774800.0000, 
sim time next is 4775400.0000, 
raw observation next is [-6.1, 92.5, 0.0, 0.0, 26.0, 24.05197210653346, 0.1329027134172081, 0.0, 1.0, 41703.04400276733], 
processed observation next is [0.0, 0.2608695652173913, 0.29362880886426596, 0.925, 0.0, 0.0, 0.6666666666666666, 0.5043310088777883, 0.5443009044724026, 0.0, 1.0, 0.19858592382270157], 
reward next is 0.8014, 
noisyNet noise sample is [array([0.15394194], dtype=float32), -1.8177284]. 
=============================================
[2019-04-04 03:30:26,985] A3C_AGENT_WORKER-Thread-20 INFO:Local step 262000, global step 4189922: loss 0.3837
[2019-04-04 03:30:26,987] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 262000, global step 4189923: learning rate 0.0005
[2019-04-04 03:30:28,751] A3C_AGENT_WORKER-Thread-11 INFO:Local step 262500, global step 4190426: loss 0.0528
[2019-04-04 03:30:28,751] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 262500, global step 4190426: learning rate 0.0005
[2019-04-04 03:30:28,798] A3C_AGENT_WORKER-Thread-18 INFO:Local step 262000, global step 4190438: loss 0.3944
[2019-04-04 03:30:28,800] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 262000, global step 4190438: learning rate 0.0005
[2019-04-04 03:30:28,906] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [9.3804584e-17 3.0310368e-12 2.4467765e-14 7.4150109e-01 9.5914868e-17
 2.5849885e-01 1.9190231e-12], sum to 1.0000
[2019-04-04 03:30:28,907] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9155
[2019-04-04 03:30:28,984] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.666666666666667, 50.0, 107.3333333333333, 760.0, 26.0, 25.63192789103952, 0.6044616179419071, 1.0, 1.0, 70268.03697115651], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3334800.0000, 
sim time next is 3335400.0000, 
raw observation next is [-3.5, 50.0, 106.0, 752.0, 26.0, 26.09217994215751, 0.6510356858905255, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.36565096952908593, 0.5, 0.35333333333333333, 0.830939226519337, 0.6666666666666666, 0.6743483285131259, 0.7170118952968418, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.00490034], dtype=float32), -0.7016592]. 
=============================================
[2019-04-04 03:30:30,612] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.6821324e-22 2.8712104e-17 2.0557682e-18 9.9998999e-01 4.6563329e-23
 1.0061997e-05 3.8830065e-17], sum to 1.0000
[2019-04-04 03:30:30,619] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8152
[2019-04-04 03:30:30,644] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.833333333333333, 59.16666666666666, 0.0, 0.0, 26.0, 25.37748190485842, 0.3848421280753382, 0.0, 1.0, 50774.55046404809], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4837800.0000, 
sim time next is 4838400.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.43766788085626, 0.3821687417198688, 0.0, 1.0, 18762.87638723334], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.6, 0.0, 0.0, 0.6666666666666666, 0.6198056567380217, 0.6273895805732895, 0.0, 1.0, 0.08934703041539686], 
reward next is 0.9107, 
noisyNet noise sample is [array([1.9391099], dtype=float32), -0.3159932]. 
=============================================
[2019-04-04 03:30:30,709] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1766397e-22 9.5120466e-16 1.9315387e-16 7.2770504e-08 5.8482366e-22
 9.9999988e-01 3.5300559e-16], sum to 1.0000
[2019-04-04 03:30:30,714] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5857
[2019-04-04 03:30:30,829] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 92.0, 0.0, 0.0, 26.0, 25.08036604469796, 0.4436794178149009, 0.0, 1.0, 41118.29455555379], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3222600.0000, 
sim time next is 3223200.0000, 
raw observation next is [-3.0, 92.0, 0.0, 0.0, 26.0, 25.13789890439568, 0.5059697746933977, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.3795013850415513, 0.92, 0.0, 0.0, 0.6666666666666666, 0.59482490869964, 0.6686565915644659, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.35471028], dtype=float32), -0.60356957]. 
=============================================
[2019-04-04 03:30:31,429] A3C_AGENT_WORKER-Thread-6 INFO:Local step 262000, global step 4191280: loss 0.2379
[2019-04-04 03:30:31,433] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 262000, global step 4191283: learning rate 0.0005
[2019-04-04 03:30:32,075] A3C_AGENT_WORKER-Thread-14 INFO:Local step 261000, global step 4191498: loss 10.3650
[2019-04-04 03:30:32,078] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 261000, global step 4191498: learning rate 0.0005
[2019-04-04 03:30:32,388] A3C_AGENT_WORKER-Thread-4 INFO:Local step 260500, global step 4191618: loss -0.0365
[2019-04-04 03:30:32,403] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 260500, global step 4191618: learning rate 0.0005
[2019-04-04 03:30:33,566] A3C_AGENT_WORKER-Thread-19 INFO:Local step 263000, global step 4192007: loss 0.0772
[2019-04-04 03:30:33,566] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 263000, global step 4192007: learning rate 0.0005
[2019-04-04 03:30:34,058] A3C_AGENT_WORKER-Thread-3 INFO:Local step 262000, global step 4192183: loss 0.2552
[2019-04-04 03:30:34,059] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 262000, global step 4192183: learning rate 0.0005
[2019-04-04 03:30:35,222] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.87828555e-21 3.00794237e-17 1.03678117e-16 1.00000000e+00
 1.21673227e-19 3.10991684e-11 1.20502073e-15], sum to 1.0000
[2019-04-04 03:30:35,223] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7606
[2019-04-04 03:30:35,240] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 26.0, 25.40704305339818, 0.3416402798425013, 0.0, 1.0, 39123.65491426222], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4243800.0000, 
sim time next is 4244400.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 26.0, 25.38166256657831, 0.340901790844355, 0.0, 1.0, 51991.98476884127], 
processed observation next is [0.0, 0.13043478260869565, 0.5457063711911359, 0.45, 0.0, 0.0, 0.6666666666666666, 0.6151385472148592, 0.6136339302814516, 0.0, 1.0, 0.2475808798516251], 
reward next is 0.7524, 
noisyNet noise sample is [array([1.7143779], dtype=float32), -1.1180999]. 
=============================================
[2019-04-04 03:30:35,263] A3C_AGENT_WORKER-Thread-17 INFO:Local step 263000, global step 4192572: loss 0.0795
[2019-04-04 03:30:35,265] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 263000, global step 4192573: learning rate 0.0005
[2019-04-04 03:30:38,065] A3C_AGENT_WORKER-Thread-5 INFO:Local step 262000, global step 4193421: loss 0.2474
[2019-04-04 03:30:38,073] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 262000, global step 4193426: learning rate 0.0005
[2019-04-04 03:30:38,393] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:30:38,395] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:30:38,399] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run32
[2019-04-04 03:30:38,888] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.7924723e-18 2.0269989e-13 1.5978530e-12 3.8048835e-04 3.4817915e-17
 9.9961948e-01 4.1853448e-13], sum to 1.0000
[2019-04-04 03:30:38,889] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8526
[2019-04-04 03:30:38,916] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 73.0, 111.6666666666667, 777.8333333333334, 26.0, 26.37604539364535, 0.5580760724434696, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3753600.0000, 
sim time next is 3754200.0000, 
raw observation next is [-3.0, 72.0, 112.3333333333333, 786.6666666666667, 26.0, 26.39308901882404, 0.5658808332005248, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.3795013850415513, 0.72, 0.37444444444444436, 0.8692449355432782, 0.6666666666666666, 0.6994240849020033, 0.688626944400175, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2884285], dtype=float32), -2.0393004]. 
=============================================
[2019-04-04 03:30:45,074] A3C_AGENT_WORKER-Thread-16 INFO:Local step 262000, global step 4195241: loss 0.2504
[2019-04-04 03:30:45,082] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 262000, global step 4195241: learning rate 0.0005
[2019-04-04 03:30:45,230] A3C_AGENT_WORKER-Thread-13 INFO:Local step 260500, global step 4195277: loss 0.1362
[2019-04-04 03:30:45,240] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 260500, global step 4195277: learning rate 0.0005
[2019-04-04 03:30:45,332] A3C_AGENT_WORKER-Thread-12 INFO:Local step 262000, global step 4195300: loss 0.2376
[2019-04-04 03:30:45,335] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 262000, global step 4195300: learning rate 0.0005
[2019-04-04 03:30:45,777] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.19993340e-19 1.01378798e-13 1.07671635e-14 1.73606037e-04
 3.22481594e-18 9.99826372e-01 5.57044204e-14], sum to 1.0000
[2019-04-04 03:30:45,799] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0479
[2019-04-04 03:30:45,833] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6, 73.0, 0.0, 0.0, 26.0, 25.5441110928462, 0.437158321314937, 0.0, 1.0, 31327.27523709165], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4497600.0000, 
sim time next is 4498200.0000, 
raw observation next is [-0.6, 73.0, 0.0, 0.0, 26.0, 25.39568129561862, 0.4304394824591777, 0.0, 1.0, 114151.0037480902], 
processed observation next is [1.0, 0.043478260869565216, 0.44598337950138506, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6163067746348849, 0.6434798274863925, 0.0, 1.0, 0.543576208324239], 
reward next is 0.4564, 
noisyNet noise sample is [array([0.07628439], dtype=float32), -0.18832183]. 
=============================================
[2019-04-04 03:30:47,247] A3C_AGENT_WORKER-Thread-15 INFO:Local step 262500, global step 4195735: loss 0.0858
[2019-04-04 03:30:47,251] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 262500, global step 4195735: learning rate 0.0005
[2019-04-04 03:30:50,592] A3C_AGENT_WORKER-Thread-10 INFO:Local step 262500, global step 4196389: loss 0.0141
[2019-04-04 03:30:50,608] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 262500, global step 4196389: learning rate 0.0005
[2019-04-04 03:30:56,608] A3C_AGENT_WORKER-Thread-20 INFO:Local step 262500, global step 4197417: loss 0.0982
[2019-04-04 03:30:56,609] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 262500, global step 4197417: learning rate 0.0005
[2019-04-04 03:30:59,981] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:30:59,981] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:30:59,984] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run32
[2019-04-04 03:31:00,716] A3C_AGENT_WORKER-Thread-18 INFO:Local step 262500, global step 4198003: loss 0.3217
[2019-04-04 03:31:00,723] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 262500, global step 4198003: learning rate 0.0005
[2019-04-04 03:31:03,316] A3C_AGENT_WORKER-Thread-11 INFO:Local step 263000, global step 4198473: loss 0.1224
[2019-04-04 03:31:03,322] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 263000, global step 4198473: learning rate 0.0005
[2019-04-04 03:31:03,646] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:31:03,647] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:31:03,740] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run32
[2019-04-04 03:31:05,017] A3C_AGENT_WORKER-Thread-6 INFO:Local step 262500, global step 4198734: loss 0.0460
[2019-04-04 03:31:05,021] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 262500, global step 4198734: learning rate 0.0005
[2019-04-04 03:31:06,793] A3C_AGENT_WORKER-Thread-14 INFO:Local step 261500, global step 4199092: loss 0.1772
[2019-04-04 03:31:06,801] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 261500, global step 4199092: learning rate 0.0005
[2019-04-04 03:31:10,126] A3C_AGENT_WORKER-Thread-3 INFO:Local step 262500, global step 4199599: loss 0.0200
[2019-04-04 03:31:10,134] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 262500, global step 4199599: learning rate 0.0005
[2019-04-04 03:31:10,242] A3C_AGENT_WORKER-Thread-4 INFO:Local step 261000, global step 4199610: loss 10.6013
[2019-04-04 03:31:10,288] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 261000, global step 4199618: learning rate 0.0005
[2019-04-04 03:31:12,145] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-04 03:31:12,145] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 03:31:12,145] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:31:12,174] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run43
[2019-04-04 03:31:12,267] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 03:31:12,270] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:31:12,294] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 03:31:12,296] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run43
[2019-04-04 03:31:12,359] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:31:12,362] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run43
[2019-04-04 03:33:13,250] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.0861763], dtype=float32), -0.08084612]
[2019-04-04 03:33:13,250] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [2.2, 66.0, 0.0, 0.0, 26.0, 24.95531732992694, 0.3932615121219233, 1.0, 1.0, 56350.11875313694]
[2019-04-04 03:33:13,250] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 03:33:13,251] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [2.8364348e-18 3.2056836e-14 2.3773777e-14 9.9926430e-01 2.7866710e-18
 7.3569862e-04 4.1438229e-13], sampled 0.6866250990615351
[2019-04-04 03:33:19,452] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.0861763], dtype=float32), -0.08084612]
[2019-04-04 03:33:19,452] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [3.116666666666667, 27.0, 72.66666666666667, 749.3333333333334, 26.0, 24.95614772872987, 0.2803003744980663, 0.0, 1.0, 18701.98482163671]
[2019-04-04 03:33:19,453] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 03:33:19,453] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.8168387e-25 2.7352611e-20 2.6547693e-20 1.0000000e+00 3.2191202e-25
 1.9858549e-13 4.1297093e-19], sampled 0.1357350368321013
[2019-04-04 03:33:42,790] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.0861763], dtype=float32), -0.08084612]
[2019-04-04 03:33:42,790] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-8.733333333333334, 61.33333333333334, 0.0, 0.0, 26.0, 23.78560388385001, 0.06788044177220272, 0.0, 1.0, 48326.63769471661]
[2019-04-04 03:33:42,790] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 03:33:42,791] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [4.2469218e-21 6.0076665e-17 7.2435928e-16 1.0000000e+00 2.8438728e-21
 2.8046168e-08 3.4390760e-15], sampled 0.778988077215316
[2019-04-04 03:33:57,119] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.0861763], dtype=float32), -0.08084612]
[2019-04-04 03:33:57,119] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-5.5, 86.5, 0.0, 0.0, 26.0, 25.61773424914594, 0.5522419503357036, 0.0, 1.0, 18732.31207958264]
[2019-04-04 03:33:57,119] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 03:33:57,120] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.9725208e-17 2.8110998e-11 6.2809476e-12 9.3562806e-01 4.0229309e-18
 6.4371996e-02 4.2133255e-11], sampled 0.3354831382001555
[2019-04-04 03:34:27,713] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7354.7881 239654508.5115 1605.1702
[2019-04-04 03:34:58,191] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7242.3611 263264166.0555 1555.1947
[2019-04-04 03:35:02,471] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.9733 275735612.4904 1232.5533
[2019-04-04 03:35:03,514] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 4200000, evaluation results [4200000.0, 7242.3611140215935, 263264166.05546936, 1555.19473528998, 7354.788054707083, 239654508.51151305, 1605.1702234618515, 7182.973273855296, 275735612.49038523, 1232.5533206192808]
[2019-04-04 03:35:04,239] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.1751113e-21 6.2039864e-14 1.5435434e-14 1.3073980e-09 8.6291700e-21
 1.0000000e+00 2.1529983e-14], sum to 1.0000
[2019-04-04 03:35:04,242] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9412
[2019-04-04 03:35:04,262] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 57.0, 0.0, 0.0, 26.0, 25.4573731309354, 0.5260734703424308, 0.0, 1.0, 105669.01429363], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4661400.0000, 
sim time next is 4662000.0000, 
raw observation next is [2.0, 57.0, 0.0, 0.0, 26.0, 25.45693121126704, 0.5358586053522721, 0.0, 1.0, 63020.40543777424], 
processed observation next is [1.0, 1.0, 0.518005540166205, 0.57, 0.0, 0.0, 0.6666666666666666, 0.6214109342722534, 0.678619535117424, 0.0, 1.0, 0.30009716875130593], 
reward next is 0.6999, 
noisyNet noise sample is [array([-0.3159429], dtype=float32), 1.4608815]. 
=============================================
[2019-04-04 03:35:04,284] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[83.52942]
 [83.18098]
 [82.7925 ]
 [82.62117]
 [82.79536]], R is [[83.47586823]
 [83.13792419]
 [82.67533875]
 [82.4635849 ]
 [82.63894653]].
[2019-04-04 03:35:06,009] A3C_AGENT_WORKER-Thread-5 INFO:Local step 262500, global step 4200683: loss 0.3568
[2019-04-04 03:35:06,010] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 262500, global step 4200683: learning rate 0.0005
[2019-04-04 03:35:12,859] A3C_AGENT_WORKER-Thread-13 INFO:Local step 261000, global step 4202747: loss 10.2037
[2019-04-04 03:35:12,873] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 261000, global step 4202747: learning rate 0.0005
[2019-04-04 03:35:13,437] A3C_AGENT_WORKER-Thread-16 INFO:Local step 262500, global step 4202914: loss 0.2307
[2019-04-04 03:35:13,455] A3C_AGENT_WORKER-Thread-12 INFO:Local step 262500, global step 4202915: loss 0.2372
[2019-04-04 03:35:13,455] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 262500, global step 4202915: learning rate 0.0005
[2019-04-04 03:35:13,458] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 262500, global step 4202915: learning rate 0.0005
[2019-04-04 03:35:15,120] A3C_AGENT_WORKER-Thread-15 INFO:Local step 263000, global step 4203470: loss 0.1604
[2019-04-04 03:35:15,121] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 263000, global step 4203470: learning rate 0.0005
[2019-04-04 03:35:17,329] A3C_AGENT_WORKER-Thread-10 INFO:Local step 263000, global step 4204038: loss 0.1303
[2019-04-04 03:35:17,339] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 263000, global step 4204039: learning rate 0.0005
[2019-04-04 03:35:18,424] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:35:18,424] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:35:18,431] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run32
[2019-04-04 03:35:21,161] A3C_AGENT_WORKER-Thread-20 INFO:Local step 263000, global step 4205053: loss 0.1801
[2019-04-04 03:35:21,162] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 263000, global step 4205053: learning rate 0.0005
[2019-04-04 03:35:23,000] A3C_AGENT_WORKER-Thread-18 INFO:Local step 263000, global step 4205545: loss 0.2152
[2019-04-04 03:35:23,003] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 263000, global step 4205545: learning rate 0.0005
[2019-04-04 03:35:24,412] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.1985125e-20 4.5433606e-15 1.5316853e-13 1.0434013e-06 4.8558917e-20
 9.9999893e-01 5.7407644e-14], sum to 1.0000
[2019-04-04 03:35:24,412] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5658
[2019-04-04 03:35:24,453] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.67386665245129, -0.2772369189875373, 0.0, 1.0, 44931.22878495276], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 198000.0000, 
sim time next is 198600.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.6187966849224, -0.2910159018991766, 0.0, 1.0, 44973.07530649676], 
processed observation next is [1.0, 0.30434782608695654, 0.21606648199445982, 0.78, 0.0, 0.0, 0.6666666666666666, 0.38489972374353343, 0.40299469936694116, 0.0, 1.0, 0.21415750145950838], 
reward next is 0.7858, 
noisyNet noise sample is [array([0.5918544], dtype=float32), 0.31728014]. 
=============================================
[2019-04-04 03:35:24,621] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1208266e-15 3.7234906e-09 4.4928497e-10 6.3699747e-07 3.7548310e-15
 9.9999940e-01 1.8843225e-09], sum to 1.0000
[2019-04-04 03:35:24,623] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2610
[2019-04-04 03:35:24,726] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 77.0, 0.0, 0.0, 26.0, 25.30862253572192, 0.4026605128258658, 0.0, 1.0, 44162.28475549674], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3807000.0000, 
sim time next is 3807600.0000, 
raw observation next is [-4.0, 77.0, 0.0, 0.0, 26.0, 25.3331288834344, 0.4078558571292547, 0.0, 1.0, 44115.14999829658], 
processed observation next is [1.0, 0.043478260869565216, 0.3518005540166205, 0.77, 0.0, 0.0, 0.6666666666666666, 0.6110940736195335, 0.6359519523764182, 0.0, 1.0, 0.21007214284903136], 
reward next is 0.7899, 
noisyNet noise sample is [array([-1.419468], dtype=float32), 0.6251064]. 
=============================================
[2019-04-04 03:35:25,707] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0030475e-25 4.3644513e-21 1.0217020e-21 1.4348958e-10 3.4030935e-24
 1.0000000e+00 5.0783841e-21], sum to 1.0000
[2019-04-04 03:35:25,709] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0257
[2019-04-04 03:35:25,739] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 34.5, 108.0, 717.0, 26.0, 26.08235973433065, 0.4704266290090124, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4959000.0000, 
sim time next is 4959600.0000, 
raw observation next is [0.3333333333333333, 33.0, 109.5, 731.3333333333333, 26.0, 26.18441350073343, 0.4931932658679042, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4718374884579871, 0.33, 0.365, 0.8081031307550643, 0.6666666666666666, 0.6820344583944525, 0.6643977552893013, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.82914865], dtype=float32), 0.5981883]. 
=============================================
[2019-04-04 03:35:26,338] A3C_AGENT_WORKER-Thread-6 INFO:Local step 263000, global step 4206470: loss 0.3401
[2019-04-04 03:35:26,366] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 263000, global step 4206470: learning rate 0.0005
[2019-04-04 03:35:27,388] A3C_AGENT_WORKER-Thread-14 INFO:Local step 262000, global step 4206730: loss 0.3867
[2019-04-04 03:35:27,388] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 262000, global step 4206730: learning rate 0.0005
[2019-04-04 03:35:28,381] A3C_AGENT_WORKER-Thread-4 INFO:Local step 261500, global step 4207058: loss 0.0072
[2019-04-04 03:35:28,383] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 261500, global step 4207058: learning rate 0.0005
[2019-04-04 03:35:28,407] A3C_AGENT_WORKER-Thread-3 INFO:Local step 263000, global step 4207067: loss 0.2150
[2019-04-04 03:35:28,407] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 263000, global step 4207067: learning rate 0.0005
[2019-04-04 03:35:29,994] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.3965728e-23 2.7229568e-19 3.6917381e-18 1.0000000e+00 9.3430628e-23
 3.3605603e-13 1.7113000e-17], sum to 1.0000
[2019-04-04 03:35:29,997] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4000
[2019-04-04 03:35:30,040] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.5, 62.5, 0.0, 0.0, 26.0, 24.57579135230947, 0.1765810335435534, 0.0, 1.0, 39481.24352302573], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4861800.0000, 
sim time next is 4862400.0000, 
raw observation next is [-3.666666666666667, 63.33333333333333, 0.0, 0.0, 26.0, 24.56036559915417, 0.1709809527306212, 0.0, 1.0, 39479.1027429807], 
processed observation next is [0.0, 0.2608695652173913, 0.3610341643582641, 0.6333333333333333, 0.0, 0.0, 0.6666666666666666, 0.5466971332628475, 0.556993650910207, 0.0, 1.0, 0.18799572734752715], 
reward next is 0.8120, 
noisyNet noise sample is [array([-1.4404062], dtype=float32), 1.7439015]. 
=============================================
[2019-04-04 03:35:31,076] A3C_AGENT_WORKER-Thread-5 INFO:Local step 263000, global step 4208121: loss 1.0919
[2019-04-04 03:35:31,078] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 263000, global step 4208121: learning rate 0.0005
[2019-04-04 03:35:32,666] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2173174e-26 3.2960081e-20 4.2151757e-20 1.6162436e-07 1.4886586e-25
 9.9999988e-01 2.0054858e-19], sum to 1.0000
[2019-04-04 03:35:32,668] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6354
[2019-04-04 03:35:32,700] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.9, 19.0, 0.0, 0.0, 26.0, 26.99969443417, 0.828389546558816, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5088000.0000, 
sim time next is 5088600.0000, 
raw observation next is [8.85, 19.0, 0.0, 0.0, 26.0, 26.93979100076043, 0.816093761925854, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.7077562326869806, 0.19, 0.0, 0.0, 0.6666666666666666, 0.7449825833967024, 0.7720312539752846, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.171799], dtype=float32), 1.8944821]. 
=============================================
[2019-04-04 03:35:34,134] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:35:34,135] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:35:34,148] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run32
[2019-04-04 03:35:35,300] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:35:35,301] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:35:35,321] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run32
[2019-04-04 03:35:36,030] A3C_AGENT_WORKER-Thread-13 INFO:Local step 261500, global step 4210033: loss 0.0174
[2019-04-04 03:35:36,033] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 261500, global step 4210034: learning rate 0.0005
[2019-04-04 03:35:36,434] A3C_AGENT_WORKER-Thread-12 INFO:Local step 263000, global step 4210156: loss 0.0988
[2019-04-04 03:35:36,437] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 263000, global step 4210156: learning rate 0.0005
[2019-04-04 03:35:37,384] A3C_AGENT_WORKER-Thread-16 INFO:Local step 263000, global step 4210426: loss 0.0449
[2019-04-04 03:35:37,386] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 263000, global step 4210426: learning rate 0.0005
[2019-04-04 03:35:37,692] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:35:37,692] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:35:37,697] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run32
[2019-04-04 03:35:38,393] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:35:38,393] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:35:38,397] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run32
[2019-04-04 03:35:41,526] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:35:41,528] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:35:41,537] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run32
[2019-04-04 03:35:43,487] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:35:43,488] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:35:43,500] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run32
[2019-04-04 03:35:44,567] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.7969949e-21 7.8524355e-17 6.1630706e-17 6.1369559e-07 5.9866950e-21
 9.9999940e-01 1.8319900e-16], sum to 1.0000
[2019-04-04 03:35:44,573] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8614
[2019-04-04 03:35:44,593] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.333333333333333, 39.0, 60.66666666666667, 485.5000000000001, 26.0, 25.52695860170795, 0.4425005942207316, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4207200.0000, 
sim time next is 4207800.0000, 
raw observation next is [2.166666666666667, 39.5, 53.33333333333334, 421.0, 26.0, 25.5013640002587, 0.427126466838178, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.5226223453370269, 0.395, 0.1777777777777778, 0.46519337016574586, 0.6666666666666666, 0.625113666688225, 0.6423754889460593, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.11685692], dtype=float32), -1.0451355]. 
=============================================
[2019-04-04 03:35:46,183] A3C_AGENT_WORKER-Thread-14 INFO:Local step 262500, global step 4212727: loss 0.1656
[2019-04-04 03:35:46,207] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 262500, global step 4212727: learning rate 0.0005
[2019-04-04 03:35:47,168] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:35:47,168] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:35:47,323] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run32
[2019-04-04 03:35:48,851] A3C_AGENT_WORKER-Thread-4 INFO:Local step 262000, global step 4213392: loss 0.3888
[2019-04-04 03:35:48,863] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 262000, global step 4213392: learning rate 0.0005
[2019-04-04 03:35:52,555] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:35:52,555] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:35:52,559] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run32
[2019-04-04 03:35:54,193] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:35:54,193] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:35:54,196] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run32
[2019-04-04 03:35:57,372] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.4751666e-29 4.6886285e-21 6.0377418e-24 3.6929167e-28 5.6996414e-28
 1.0000000e+00 2.6328113e-22], sum to 1.0000
[2019-04-04 03:35:57,386] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8130
[2019-04-04 03:35:57,407] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.15, 94.0, 0.0, 0.0, 26.0, 24.87353131370679, 0.2376934204586822, 0.0, 1.0, 41458.55569850729], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 509400.0000, 
sim time next is 510000.0000, 
raw observation next is [2.333333333333333, 93.33333333333333, 0.0, 0.0, 26.0, 24.87074671918784, 0.2389788415636004, 0.0, 1.0, 41322.13135392027], 
processed observation next is [1.0, 0.9130434782608695, 0.5272391505078486, 0.9333333333333332, 0.0, 0.0, 0.6666666666666666, 0.5725622265989866, 0.5796596138545335, 0.0, 1.0, 0.19677205406628698], 
reward next is 0.8032, 
noisyNet noise sample is [array([2.9012492], dtype=float32), 0.74914813]. 
=============================================
[2019-04-04 03:35:57,413] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[80.31042]
 [79.92898]
 [79.62731]
 [79.33576]
 [79.19043]], R is [[80.53149414]
 [80.52876282]
 [80.52529907]
 [80.51920319]
 [80.50609589]].
[2019-04-04 03:35:58,973] A3C_AGENT_WORKER-Thread-13 INFO:Local step 262000, global step 4216153: loss 0.2621
[2019-04-04 03:35:58,975] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 262000, global step 4216153: learning rate 0.0005
[2019-04-04 03:36:01,296] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.0956293e-25 1.8602559e-19 5.3320405e-20 2.2500072e-20 2.7792834e-24
 1.0000000e+00 5.3890516e-20], sum to 1.0000
[2019-04-04 03:36:01,296] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5362
[2019-04-04 03:36:01,357] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.316666666666666, 64.5, 142.3333333333333, 0.0, 26.0, 25.18474622484081, 0.2313151106061029, 1.0, 1.0, 18721.76012531069], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 220200.0000, 
sim time next is 220800.0000, 
raw observation next is [-4.133333333333334, 64.0, 145.6666666666667, 0.0, 26.0, 25.1834760240462, 0.2414610125822879, 1.0, 1.0, 18719.85649116212], 
processed observation next is [1.0, 0.5652173913043478, 0.34810710987996313, 0.64, 0.48555555555555574, 0.0, 0.6666666666666666, 0.59862300200385, 0.5804870041940959, 1.0, 1.0, 0.08914217376743867], 
reward next is 0.9109, 
noisyNet noise sample is [array([1.1160096], dtype=float32), 0.15969163]. 
=============================================
[2019-04-04 03:36:07,669] A3C_AGENT_WORKER-Thread-14 INFO:Local step 263000, global step 4218709: loss 0.0710
[2019-04-04 03:36:07,673] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 263000, global step 4218709: learning rate 0.0005
[2019-04-04 03:36:09,578] A3C_AGENT_WORKER-Thread-4 INFO:Local step 262500, global step 4219287: loss 0.4616
[2019-04-04 03:36:09,579] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 262500, global step 4219287: learning rate 0.0005
[2019-04-04 03:36:18,026] A3C_AGENT_WORKER-Thread-13 INFO:Local step 262500, global step 4221845: loss 0.6494
[2019-04-04 03:36:18,027] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 262500, global step 4221845: learning rate 0.0005
[2019-04-04 03:36:21,838] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.9189314e-21 9.2403068e-16 3.9601645e-15 5.9775752e-12 1.1471902e-19
 1.0000000e+00 1.2474004e-15], sum to 1.0000
[2019-04-04 03:36:21,846] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3206
[2019-04-04 03:36:21,870] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.8, 72.0, 0.0, 0.0, 26.0, 23.99346579814654, 0.06227003666541945, 0.0, 1.0, 41500.30727797934], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 786000.0000, 
sim time next is 786600.0000, 
raw observation next is [-7.8, 72.5, 0.0, 0.0, 26.0, 23.95532674188026, 0.05457353146242473, 0.0, 1.0, 41472.00418320324], 
processed observation next is [1.0, 0.08695652173913043, 0.24653739612188366, 0.725, 0.0, 0.0, 0.6666666666666666, 0.49627722849002165, 0.5181911771541415, 0.0, 1.0, 0.1974857342057297], 
reward next is 0.8025, 
noisyNet noise sample is [array([1.3679938], dtype=float32), 0.49852425]. 
=============================================
[2019-04-04 03:36:23,684] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:36:23,685] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:36:23,688] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run32
[2019-04-04 03:36:24,618] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.07378484e-23 7.01539219e-19 2.63076853e-18 3.04593590e-14
 2.08073076e-23 1.00000000e+00 1.65061296e-18], sum to 1.0000
[2019-04-04 03:36:24,624] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7318
[2019-04-04 03:36:24,651] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.4, 69.5, 0.0, 0.0, 26.0, 23.462282493729, -0.0791440921151756, 0.0, 1.0, 46363.12168163477], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 273000.0000, 
sim time next is 273600.0000, 
raw observation next is [-9.5, 70.0, 0.0, 0.0, 26.0, 23.40555529663248, -0.09465050223036038, 0.0, 1.0, 46528.31995759065], 
processed observation next is [1.0, 0.17391304347826086, 0.1994459833795014, 0.7, 0.0, 0.0, 0.6666666666666666, 0.4504629413860399, 0.4684498325898799, 0.0, 1.0, 0.22156342836947926], 
reward next is 0.7784, 
noisyNet noise sample is [array([0.36266023], dtype=float32), 2.0138674]. 
=============================================
[2019-04-04 03:36:31,062] A3C_AGENT_WORKER-Thread-4 INFO:Local step 263000, global step 4225602: loss 0.0312
[2019-04-04 03:36:31,063] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 263000, global step 4225602: learning rate 0.0005
[2019-04-04 03:36:34,973] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.0176383e-19 1.7341273e-12 4.5655743e-13 4.5928865e-09 5.1556526e-17
 1.0000000e+00 2.0868671e-12], sum to 1.0000
[2019-04-04 03:36:34,975] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2528
[2019-04-04 03:36:35,005] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.533333333333333, 88.33333333333334, 0.0, 0.0, 26.0, 25.00461957388025, 0.2547726909327562, 0.0, 1.0, 39557.486533174], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 524400.0000, 
sim time next is 525000.0000, 
raw observation next is [4.416666666666667, 88.16666666666667, 0.0, 0.0, 26.0, 24.96717113007479, 0.2497244208316053, 0.0, 1.0, 39593.24638384028], 
processed observation next is [0.0, 0.043478260869565216, 0.584949215143121, 0.8816666666666667, 0.0, 0.0, 0.6666666666666666, 0.580597594172899, 0.583241473610535, 0.0, 1.0, 0.1885392684944775], 
reward next is 0.8115, 
noisyNet noise sample is [array([1.0472944], dtype=float32), -0.0027016469]. 
=============================================
[2019-04-04 03:36:35,031] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.9279904e-25 6.4479870e-18 8.9004543e-18 7.0346905e-18 2.9118087e-24
 1.0000000e+00 2.8125777e-19], sum to 1.0000
[2019-04-04 03:36:35,049] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4122
[2019-04-04 03:36:35,037] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[87.15768]
 [87.26844]
 [87.31912]
 [87.50738]
 [87.49829]], R is [[87.09062958]
 [87.03135681]
 [86.97286224]
 [86.9152298 ]
 [86.8572464 ]].
[2019-04-04 03:36:35,078] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 46.66666666666667, 87.5, 763.1666666666667, 26.0, 25.55660509072859, 0.4068949045886752, 1.0, 1.0, 18680.42957289572], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 740400.0000, 
sim time next is 741000.0000, 
raw observation next is [0.5, 45.83333333333333, 86.0, 753.3333333333333, 26.0, 25.708383335487, 0.4212737738710024, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.4764542936288089, 0.45833333333333326, 0.2866666666666667, 0.8324125230202577, 0.6666666666666666, 0.64236527795725, 0.6404245912903341, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8612227], dtype=float32), 0.81544095]. 
=============================================
[2019-04-04 03:36:35,093] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[90.083855]
 [90.47773 ]
 [90.51705 ]
 [90.553696]
 [90.6086  ]], R is [[89.81098175]
 [89.8239212 ]
 [89.83672333]
 [89.84940338]
 [89.86196136]].
[2019-04-04 03:36:38,268] A3C_AGENT_WORKER-Thread-13 INFO:Local step 263000, global step 4228139: loss 0.1957
[2019-04-04 03:36:38,271] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 263000, global step 4228139: learning rate 0.0005
[2019-04-04 03:36:44,071] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.8814406e-15 3.5218630e-11 3.8748222e-09 8.5565925e-01 4.9126657e-15
 1.4434074e-01 1.1034293e-09], sum to 1.0000
[2019-04-04 03:36:44,096] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9313
[2019-04-04 03:36:44,133] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 65.0, 0.0, 0.0, 26.0, 23.72261094933367, -0.01625323713454431, 0.0, 1.0, 43894.50990275358], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 626400.0000, 
sim time next is 627000.0000, 
raw observation next is [-4.5, 65.5, 0.0, 0.0, 26.0, 23.70262406239099, -0.02301313035382674, 0.0, 1.0, 43823.77124319643], 
processed observation next is [0.0, 0.2608695652173913, 0.3379501385041552, 0.655, 0.0, 0.0, 0.6666666666666666, 0.4752186718659157, 0.4923289565487244, 0.0, 1.0, 0.20868462496760207], 
reward next is 0.7913, 
noisyNet noise sample is [array([0.20027915], dtype=float32), 0.48696524]. 
=============================================
[2019-04-04 03:36:44,203] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[80.65289 ]
 [80.647934]
 [80.64    ]
 [80.62128 ]
 [80.6031  ]], R is [[80.64529419]
 [80.62982178]
 [80.61407471]
 [80.59809875]
 [80.58190155]].
[2019-04-04 03:36:46,360] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:36:46,360] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:36:46,369] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run32
[2019-04-04 03:36:46,641] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.3433098e-30 3.1249172e-23 2.1545589e-22 2.7500828e-15 7.4610672e-27
 1.0000000e+00 7.4222768e-23], sum to 1.0000
[2019-04-04 03:36:46,641] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5251
[2019-04-04 03:36:46,682] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.16666666666667, 82.5, 21.0, 0.3333333333333333, 26.0, 25.67094665629148, 0.6078197107522051, 0.0, 1.0, 34832.44070519948], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1152600.0000, 
sim time next is 1153200.0000, 
raw observation next is [13.63333333333333, 81.0, 26.0, 0.1666666666666666, 26.0, 25.65618937460761, 0.6074374597282214, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.840258541089566, 0.81, 0.08666666666666667, 0.00018416206261510123, 0.6666666666666666, 0.6380157812173008, 0.7024791532427405, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4038365], dtype=float32), -1.1754273]. 
=============================================
[2019-04-04 03:36:50,316] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.0288181e-15 1.1276181e-09 3.7746261e-11 8.1237544e-05 7.8608605e-15
 9.9991870e-01 5.1712131e-11], sum to 1.0000
[2019-04-04 03:36:50,316] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0369
[2019-04-04 03:36:50,342] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.8, 87.0, 0.0, 0.0, 26.0, 25.13343325214825, 0.3124109166332826, 0.0, 1.0, 44583.13865728381], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 590400.0000, 
sim time next is 591000.0000, 
raw observation next is [-2.8, 86.33333333333333, 0.0, 0.0, 26.0, 25.15338746382903, 0.309634828682035, 0.0, 1.0, 43780.44827974955], 
processed observation next is [0.0, 0.8695652173913043, 0.38504155124653744, 0.8633333333333333, 0.0, 0.0, 0.6666666666666666, 0.5961156219857525, 0.6032116095606783, 0.0, 1.0, 0.20847832514166453], 
reward next is 0.7915, 
noisyNet noise sample is [array([0.12567085], dtype=float32), 0.06731029]. 
=============================================
[2019-04-04 03:36:50,352] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[78.50037]
 [78.53715]
 [78.56454]
 [78.49164]
 [78.29707]], R is [[78.43039703]
 [78.43379211]
 [78.42438507]
 [78.37420654]
 [78.21140289]].
[2019-04-04 03:36:53,401] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:36:53,401] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:36:53,404] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run32
[2019-04-04 03:36:54,156] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.3717784e-31 6.4222133e-25 3.4412350e-26 1.2687285e-28 4.7940846e-30
 1.0000000e+00 1.5428893e-26], sum to 1.0000
[2019-04-04 03:36:54,156] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8890
[2019-04-04 03:36:54,255] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 26.0, 24.90863334199885, 0.4626854128304596, 1.0, 1.0, 81031.7475617611], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1368000.0000, 
sim time next is 1368600.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 26.0, 24.88645487462731, 0.4739965372570303, 1.0, 1.0, 69919.18560625943], 
processed observation next is [1.0, 0.8695652173913043, 0.4764542936288089, 0.96, 0.0, 0.0, 0.6666666666666666, 0.5738712395522759, 0.6579988457523435, 1.0, 1.0, 0.33294850288694966], 
reward next is 0.6671, 
noisyNet noise sample is [array([-0.60087043], dtype=float32), -0.18141308]. 
=============================================
[2019-04-04 03:36:59,922] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.2667905e-26 5.2877241e-19 8.8231000e-20 7.2736208e-20 4.4069197e-25
 1.0000000e+00 1.5607417e-20], sum to 1.0000
[2019-04-04 03:36:59,923] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1889
[2019-04-04 03:36:59,955] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.0, 69.0, 0.0, 0.0, 26.0, 24.46837148323553, 0.1839323943122538, 0.0, 1.0, 42179.83930509826], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 775800.0000, 
sim time next is 776400.0000, 
raw observation next is [-7.1, 69.66666666666666, 0.0, 0.0, 26.0, 24.4348349105388, 0.1757082270667359, 0.0, 1.0, 42063.70720829918], 
processed observation next is [1.0, 1.0, 0.2659279778393352, 0.6966666666666665, 0.0, 0.0, 0.6666666666666666, 0.5362362425449, 0.5585694090222453, 0.0, 1.0, 0.20030336765856752], 
reward next is 0.7997, 
noisyNet noise sample is [array([-1.1496398], dtype=float32), 1.7430809]. 
=============================================
[2019-04-04 03:37:15,094] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.3097401e-30 1.4104925e-26 1.4418065e-25 6.8765396e-23 1.6963618e-27
 1.0000000e+00 2.5877892e-26], sum to 1.0000
[2019-04-04 03:37:15,094] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2993
[2019-04-04 03:37:15,104] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.75, 81.5, 100.0, 234.0, 26.0, 26.6702871051075, 0.7840718385460482, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1071000.0000, 
sim time next is 1071600.0000, 
raw observation next is [12.93333333333333, 81.0, 102.3333333333333, 195.0, 26.0, 26.72178740019095, 0.8009565810792997, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.8208679593721145, 0.81, 0.341111111111111, 0.2154696132596685, 0.6666666666666666, 0.7268156166825793, 0.7669855270264332, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.17653252], dtype=float32), 2.3452432]. 
=============================================
[2019-04-04 03:37:26,268] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.9319069e-16 8.4858738e-11 9.9488939e-13 2.7663336e-07 1.8824224e-17
 9.9999976e-01 4.3647616e-12], sum to 1.0000
[2019-04-04 03:37:26,268] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5766
[2019-04-04 03:37:26,340] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-14.68333333333333, 70.0, 49.66666666666666, 735.0, 26.0, 25.60893918037797, 0.2880812803229469, 1.0, 1.0, 54597.58776828322], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 381000.0000, 
sim time next is 381600.0000, 
raw observation next is [-14.5, 66.0, 55.0, 733.5, 26.0, 25.67143772607848, 0.3234353900872995, 1.0, 1.0, 53162.44863808851], 
processed observation next is [1.0, 0.43478260869565216, 0.06094182825484763, 0.66, 0.18333333333333332, 0.8104972375690608, 0.6666666666666666, 0.6392864771732066, 0.6078117966957665, 1.0, 1.0, 0.253154517324231], 
reward next is 0.7468, 
noisyNet noise sample is [array([0.09752258], dtype=float32), 0.6856879]. 
=============================================
[2019-04-04 03:37:27,819] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.3441749e-17 4.7336416e-14 1.9045992e-14 9.9999535e-01 1.3970197e-17
 4.6255318e-06 5.4088174e-13], sum to 1.0000
[2019-04-04 03:37:27,821] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3618
[2019-04-04 03:37:27,878] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.6, 54.0, 82.0, 47.0, 26.0, 24.88158842535964, 0.229330720989817, 0.0, 1.0, 35012.8182723586], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 658800.0000, 
sim time next is 659400.0000, 
raw observation next is [-0.6, 54.0, 82.33333333333334, 44.0, 26.0, 24.90585140567342, 0.2280550208569189, 0.0, 1.0, 24157.91516952173], 
processed observation next is [0.0, 0.6521739130434783, 0.44598337950138506, 0.54, 0.2744444444444445, 0.04861878453038674, 0.6666666666666666, 0.5754876171394517, 0.5760183402856397, 0.0, 1.0, 0.11503769128343681], 
reward next is 0.8850, 
noisyNet noise sample is [array([2.9423208], dtype=float32), 0.09175151]. 
=============================================
[2019-04-04 03:37:39,461] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.3474148e-20 5.4171374e-15 6.2772061e-16 1.4620054e-10 6.7054530e-20
 1.0000000e+00 2.1123939e-15], sum to 1.0000
[2019-04-04 03:37:39,462] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7047
[2019-04-04 03:37:39,490] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.100000000000001, 78.66666666666667, 0.0, 0.0, 26.0, 24.33692322738409, 0.159166012278686, 0.0, 1.0, 42566.79159512909], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2164800.0000, 
sim time next is 2165400.0000, 
raw observation next is [-7.0, 78.5, 0.0, 0.0, 26.0, 24.31499203827165, 0.1562792596492425, 0.0, 1.0, 42585.33967784033], 
processed observation next is [1.0, 0.043478260869565216, 0.2686980609418283, 0.785, 0.0, 0.0, 0.6666666666666666, 0.5262493365226376, 0.5520930865497475, 0.0, 1.0, 0.2027873317992397], 
reward next is 0.7972, 
noisyNet noise sample is [array([0.05281356], dtype=float32), -0.5299099]. 
=============================================
[2019-04-04 03:37:41,664] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.4207508e-25 4.9140763e-19 7.5613207e-20 4.7198421e-18 1.5283216e-25
 1.0000000e+00 3.0584312e-20], sum to 1.0000
[2019-04-04 03:37:41,665] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4926
[2019-04-04 03:37:41,696] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.516666666666667, 92.0, 0.0, 0.0, 26.0, 25.31639021494135, 0.4876482908867462, 0.0, 1.0, 46682.18051412571], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1464600.0000, 
sim time next is 1465200.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 26.0, 25.33606255935414, 0.4939698613586531, 0.0, 1.0, 41265.81466818257], 
processed observation next is [1.0, 1.0, 0.5069252077562327, 0.92, 0.0, 0.0, 0.6666666666666666, 0.611338546612845, 0.6646566204528844, 0.0, 1.0, 0.19650387937229793], 
reward next is 0.8035, 
noisyNet noise sample is [array([-0.80762494], dtype=float32), -0.3883144]. 
=============================================
[2019-04-04 03:37:41,914] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.45255240e-22 1.30803625e-17 1.31079279e-18 1.22080124e-10
 4.10884671e-24 1.00000000e+00 1.16974123e-18], sum to 1.0000
[2019-04-04 03:37:41,914] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9529
[2019-04-04 03:37:41,928] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 86.0, 0.0, 0.0, 26.0, 24.98630752473551, 0.2942421852224391, 0.0, 1.0, 45947.97506330531], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1805400.0000, 
sim time next is 1806000.0000, 
raw observation next is [-5.0, 86.0, 0.0, 0.0, 26.0, 24.99760566150221, 0.2927188626481198, 0.0, 1.0, 45901.50743355524], 
processed observation next is [0.0, 0.9130434782608695, 0.32409972299168976, 0.86, 0.0, 0.0, 0.6666666666666666, 0.5831338051251841, 0.59757295421604, 0.0, 1.0, 0.21857860682645353], 
reward next is 0.7814, 
noisyNet noise sample is [array([0.3808158], dtype=float32), 0.6255873]. 
=============================================
[2019-04-04 03:37:41,969] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[77.12021]
 [77.12391]
 [77.16693]
 [77.1976 ]
 [77.24327]], R is [[77.12554169]
 [77.13548279]
 [77.14524078]
 [77.15481567]
 [77.16410828]].
[2019-04-04 03:37:48,583] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0990613e-21 6.1113555e-18 8.9190408e-19 6.6446067e-13 5.6718529e-22
 1.0000000e+00 3.5774594e-18], sum to 1.0000
[2019-04-04 03:37:48,592] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5084
[2019-04-04 03:37:48,613] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.533333333333334, 64.0, 174.6666666666667, 128.5, 26.0, 25.87215663405358, 0.4060175083797933, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2118000.0000, 
sim time next is 2118600.0000, 
raw observation next is [-6.45, 64.0, 151.0, 134.0, 26.0, 25.74971198356796, 0.3664346471598376, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.28393351800554023, 0.64, 0.5033333333333333, 0.14806629834254142, 0.6666666666666666, 0.6458093319639966, 0.6221448823866126, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7351824], dtype=float32), -0.04946892]. 
=============================================
[2019-04-04 03:37:49,738] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.2055857e-24 1.4858952e-18 1.0223312e-18 3.6646919e-15 4.9382000e-21
 1.0000000e+00 7.9579159e-19], sum to 1.0000
[2019-04-04 03:37:49,738] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4431
[2019-04-04 03:37:49,751] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.333333333333334, 95.33333333333334, 0.0, 0.0, 26.0, 25.600410607766, 0.5618928806741098, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1664400.0000, 
sim time next is 1665000.0000, 
raw observation next is [5.25, 94.5, 0.0, 0.0, 26.0, 25.65577880855717, 0.548366633033122, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.60803324099723, 0.945, 0.0, 0.0, 0.6666666666666666, 0.6379815673797641, 0.6827888776777074, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.18913642], dtype=float32), -0.19829507]. 
=============================================
[2019-04-04 03:37:49,759] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[88.49456 ]
 [88.654015]
 [88.76324 ]
 [88.81837 ]
 [88.84593 ]], R is [[88.457901  ]
 [88.57332611]
 [88.59832001]
 [88.48362732]
 [88.22174835]].
[2019-04-04 03:37:52,489] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.4104463e-18 7.1400581e-14 1.4508571e-13 7.0548464e-07 1.3967160e-17
 9.9999928e-01 7.6760918e-14], sum to 1.0000
[2019-04-04 03:37:52,490] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6452
[2019-04-04 03:37:52,504] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.4, 91.66666666666667, 0.0, 0.0, 26.0, 25.34262047435898, 0.4560995372977373, 0.0, 1.0, 43179.0854855334], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1732800.0000, 
sim time next is 1733400.0000, 
raw observation next is [0.35, 91.5, 0.0, 0.0, 26.0, 25.32602026591637, 0.4567288928435393, 0.0, 1.0, 42993.06021667966], 
processed observation next is [0.0, 0.043478260869565216, 0.47229916897506935, 0.915, 0.0, 0.0, 0.6666666666666666, 0.6105016888263641, 0.6522429642811798, 0.0, 1.0, 0.20472885817466505], 
reward next is 0.7953, 
noisyNet noise sample is [array([1.7809777], dtype=float32), 0.5573209]. 
=============================================
[2019-04-04 03:37:55,111] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3085456e-24 1.6243330e-20 1.1564391e-19 7.2019804e-18 2.8856242e-24
 1.0000000e+00 1.1929891e-19], sum to 1.0000
[2019-04-04 03:37:55,111] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7504
[2019-04-04 03:37:55,126] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.283333333333333, 73.16666666666667, 0.0, 0.0, 26.0, 25.44806284709752, 0.5366333427867241, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1541400.0000, 
sim time next is 1542000.0000, 
raw observation next is [7.366666666666667, 73.33333333333334, 0.0, 0.0, 26.0, 25.36781737656083, 0.5354627551771906, 0.0, 1.0, 196217.9094192961], 
processed observation next is [1.0, 0.8695652173913043, 0.6666666666666667, 0.7333333333333334, 0.0, 0.0, 0.6666666666666666, 0.6139847813800691, 0.6784875850590635, 0.0, 1.0, 0.9343709972347434], 
reward next is 0.0656, 
noisyNet noise sample is [array([0.00035919], dtype=float32), -0.22377376]. 
=============================================
[2019-04-04 03:37:55,195] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[83.43809 ]
 [84.11204 ]
 [85.34691 ]
 [84.474014]
 [86.19589 ]], R is [[82.01161957]
 [82.19150543]
 [82.36959076]
 [82.54589844]
 [82.72044373]].
[2019-04-04 03:38:19,555] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.1525800e-15 4.3423598e-10 2.0736469e-10 6.4821827e-01 7.7215498e-15
 3.5178176e-01 2.1167333e-10], sum to 1.0000
[2019-04-04 03:38:19,557] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0467
[2019-04-04 03:38:19,572] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.933333333333334, 77.66666666666667, 0.0, 0.0, 26.0, 24.49182267108909, 0.1241335392355416, 0.0, 1.0, 44897.39751482226], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1896000.0000, 
sim time next is 1896600.0000, 
raw observation next is [-7.116666666666666, 78.33333333333334, 0.0, 0.0, 26.0, 24.45066499568803, 0.1163037076557291, 0.0, 1.0, 44905.19789397142], 
processed observation next is [0.0, 0.9565217391304348, 0.265466297322253, 0.7833333333333334, 0.0, 0.0, 0.6666666666666666, 0.5375554163073358, 0.5387679025519098, 0.0, 1.0, 0.21383427568557817], 
reward next is 0.7862, 
noisyNet noise sample is [array([-0.7712744], dtype=float32), -0.6056697]. 
=============================================
[2019-04-04 03:38:22,220] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.9412276e-17 4.1422855e-12 4.0672809e-13 4.5643568e-02 1.5215234e-16
 9.5435643e-01 1.0213137e-12], sum to 1.0000
[2019-04-04 03:38:22,220] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0423
[2019-04-04 03:38:22,272] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.416666666666667, 85.5, 0.0, 0.0, 26.0, 25.03945544622491, 0.2446612592918147, 0.0, 1.0, 47309.72351870118], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1885800.0000, 
sim time next is 1886400.0000, 
raw observation next is [-5.6, 86.0, 0.0, 0.0, 26.0, 25.0279976794567, 0.2376164356582454, 0.0, 1.0, 45327.04179476407], 
processed observation next is [0.0, 0.8695652173913043, 0.30747922437673136, 0.86, 0.0, 0.0, 0.6666666666666666, 0.5856664732880583, 0.5792054785527484, 0.0, 1.0, 0.21584305616554317], 
reward next is 0.7842, 
noisyNet noise sample is [array([0.0695335], dtype=float32), -0.9704589]. 
=============================================
[2019-04-04 03:38:22,815] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.3224288e-15 2.9627877e-11 2.1959010e-12 9.9540663e-01 5.1566633e-15
 4.5934147e-03 3.1771526e-11], sum to 1.0000
[2019-04-04 03:38:22,819] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4051
[2019-04-04 03:38:22,873] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.3, 75.0, 250.5, 80.5, 26.0, 25.75074342541677, 0.3830376649359895, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2113200.0000, 
sim time next is 2113800.0000, 
raw observation next is [-7.199999999999999, 73.16666666666667, 264.6666666666666, 87.33333333333334, 26.0, 25.69905963896539, 0.400113160878409, 1.0, 1.0, 136736.2583252589], 
processed observation next is [1.0, 0.4782608695652174, 0.26315789473684215, 0.7316666666666667, 0.8822222222222219, 0.09650092081031308, 0.6666666666666666, 0.6415883032471159, 0.6333710536261363, 1.0, 1.0, 0.6511250396440901], 
reward next is 0.3489, 
noisyNet noise sample is [array([-0.15556177], dtype=float32), -0.38437495]. 
=============================================
[2019-04-04 03:38:34,446] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.1728389e-24 9.6351565e-16 2.0138443e-17 1.8057577e-08 1.2246203e-19
 1.0000000e+00 1.3208468e-18], sum to 1.0000
[2019-04-04 03:38:34,446] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9103
[2019-04-04 03:38:34,491] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [15.0, 96.0, 0.0, 0.0, 26.0, 23.48193541291039, 0.1450247767197637, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1234800.0000, 
sim time next is 1235400.0000, 
raw observation next is [15.0, 96.0, 0.0, 0.0, 26.0, 23.46156390042463, 0.1449086827076239, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.30434782608695654, 0.8781163434903049, 0.96, 0.0, 0.0, 0.6666666666666666, 0.45513032503538575, 0.5483028942358746, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4740925], dtype=float32), 0.52628714]. 
=============================================
[2019-04-04 03:38:36,076] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.9344260e-20 9.3582179e-17 7.0051054e-17 4.5528104e-05 6.8789099e-21
 9.9995446e-01 4.8602073e-16], sum to 1.0000
[2019-04-04 03:38:36,076] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3738
[2019-04-04 03:38:36,119] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.483333333333333, 66.0, 138.6666666666667, 0.0, 26.0, 25.3281975874191, 0.2938113894419408, 1.0, 1.0, 28353.55621026074], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2207400.0000, 
sim time next is 2208000.0000, 
raw observation next is [-3.566666666666667, 67.0, 141.3333333333333, 0.0, 26.0, 25.17812845231858, 0.3031984768185738, 1.0, 1.0, 61531.0666644603], 
processed observation next is [1.0, 0.5652173913043478, 0.3638042474607572, 0.67, 0.471111111111111, 0.0, 0.6666666666666666, 0.5981773710265484, 0.6010661589395246, 1.0, 1.0, 0.2930050793545728], 
reward next is 0.7070, 
noisyNet noise sample is [array([0.3210564], dtype=float32), 0.5532955]. 
=============================================
[2019-04-04 03:38:36,125] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[83.33028 ]
 [83.27582 ]
 [83.03244 ]
 [82.76134 ]
 [82.649506]], R is [[83.33506012]
 [83.36669159]
 [83.41840363]
 [83.46800995]
 [83.51693726]].
[2019-04-04 03:38:52,131] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.8923604e-21 2.4401934e-16 7.4664858e-16 1.5859601e-05 8.2776293e-20
 9.9998415e-01 2.3549702e-16], sum to 1.0000
[2019-04-04 03:38:52,132] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0022
[2019-04-04 03:38:52,165] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 93.0, 0.0, 0.0, 26.0, 24.92027359464265, 0.2597402723776701, 0.0, 1.0, 55605.93750033502], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2865600.0000, 
sim time next is 2866200.0000, 
raw observation next is [1.0, 93.0, 0.0, 0.0, 26.0, 24.92090268076719, 0.258021491834362, 0.0, 1.0, 55708.49071657787], 
processed observation next is [1.0, 0.17391304347826086, 0.4903047091412743, 0.93, 0.0, 0.0, 0.6666666666666666, 0.5767418900639324, 0.5860071639447874, 0.0, 1.0, 0.2652785272217994], 
reward next is 0.7347, 
noisyNet noise sample is [array([1.6064655], dtype=float32), 1.7007405]. 
=============================================
[2019-04-04 03:38:56,219] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.4250529e-23 9.5701581e-19 3.1034248e-17 1.0000000e+00 2.0478157e-23
 5.6872942e-13 7.6757745e-18], sum to 1.0000
[2019-04-04 03:38:56,219] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1278
[2019-04-04 03:38:56,263] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 50.0, 45.5, 387.1666666666667, 26.0, 25.9817667007508, 0.3524241379969372, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2738400.0000, 
sim time next is 2739000.0000, 
raw observation next is [-3.0, 50.0, 37.00000000000001, 320.3333333333334, 26.0, 25.9076550918393, 0.4586796390965608, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.3795013850415513, 0.5, 0.12333333333333335, 0.35395948434622476, 0.6666666666666666, 0.6589712576532749, 0.652893213032187, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2706696], dtype=float32), 1.7251736]. 
=============================================
[2019-04-04 03:38:56,279] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[81.61472]
 [82.44345]
 [82.61888]
 [82.76589]
 [83.16203]], R is [[81.26548767]
 [81.45283508]
 [81.63830566]
 [81.8219223 ]
 [82.00370026]].
[2019-04-04 03:38:58,289] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.3757834e-20 7.8630512e-17 4.2208370e-14 4.5760367e-05 8.8257297e-19
 9.9995422e-01 1.3217416e-14], sum to 1.0000
[2019-04-04 03:38:58,290] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8621
[2019-04-04 03:38:58,310] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.7, 42.0, 0.0, 0.0, 26.0, 24.9596983289583, 0.1851419133018492, 0.0, 1.0, 38660.35671585717], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2515200.0000, 
sim time next is 2515800.0000, 
raw observation next is [-1.7, 43.0, 0.0, 0.0, 26.0, 24.94797962160084, 0.1788605344893487, 0.0, 1.0, 38639.94036027542], 
processed observation next is [1.0, 0.08695652173913043, 0.4155124653739613, 0.43, 0.0, 0.0, 0.6666666666666666, 0.5789983018000701, 0.5596201781631162, 0.0, 1.0, 0.18399971600131151], 
reward next is 0.8160, 
noisyNet noise sample is [array([-1.2595481], dtype=float32), -1.7152079]. 
=============================================
[2019-04-04 03:39:03,771] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [8.0988973e-23 2.8402573e-17 1.5041054e-17 1.2743973e-08 9.5316692e-22
 1.0000000e+00 2.0544728e-17], sum to 1.0000
[2019-04-04 03:39:03,773] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1340
[2019-04-04 03:39:03,803] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 56.0, 0.0, 0.0, 26.0, 25.23345189716286, 0.3544678492158544, 0.0, 1.0, 66861.061975038], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2846400.0000, 
sim time next is 2847000.0000, 
raw observation next is [2.0, 59.0, 0.0, 0.0, 26.0, 25.21197373011475, 0.3547212597706822, 0.0, 1.0, 51853.23669410644], 
processed observation next is [1.0, 0.9565217391304348, 0.518005540166205, 0.59, 0.0, 0.0, 0.6666666666666666, 0.6009978108428958, 0.6182404199235607, 0.0, 1.0, 0.24692017473384018], 
reward next is 0.7531, 
noisyNet noise sample is [array([0.8866971], dtype=float32), 0.9288169]. 
=============================================
[2019-04-04 03:39:03,807] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[81.37729 ]
 [81.18995 ]
 [80.79179 ]
 [80.643555]
 [80.59138 ]], R is [[81.44709015]
 [81.3142395 ]
 [81.06249237]
 [80.99597168]
 [81.05236816]].
[2019-04-04 03:39:10,608] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.8105636e-16 9.3762793e-12 1.2989368e-12 1.6471995e-03 2.0387490e-16
 9.9835277e-01 3.5750753e-11], sum to 1.0000
[2019-04-04 03:39:10,609] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0832
[2019-04-04 03:39:10,625] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-15.0, 84.33333333333333, 0.0, 0.0, 26.0, 23.34260015907795, -0.05424100803236814, 0.0, 1.0, 44092.56754521622], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2695800.0000, 
sim time next is 2696400.0000, 
raw observation next is [-15.0, 83.0, 0.0, 0.0, 26.0, 23.28013876415815, -0.06043898347599327, 0.0, 1.0, 43963.98864058282], 
processed observation next is [1.0, 0.21739130434782608, 0.04709141274238226, 0.83, 0.0, 0.0, 0.6666666666666666, 0.4400115636798458, 0.4798536721746689, 0.0, 1.0, 0.2093523268599182], 
reward next is 0.7906, 
noisyNet noise sample is [array([-0.8807751], dtype=float32), 0.9091757]. 
=============================================
[2019-04-04 03:39:16,233] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.0813970e-21 4.8925566e-17 7.6403995e-19 6.7689216e-06 1.4318400e-22
 9.9999321e-01 1.0296049e-16], sum to 1.0000
[2019-04-04 03:39:16,233] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2724
[2019-04-04 03:39:16,288] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 59.0, 0.0, 0.0, 26.0, 25.19697653749953, 0.4041072454982921, 1.0, 1.0, 62292.46143638199], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2746800.0000, 
sim time next is 2747400.0000, 
raw observation next is [-5.0, 59.0, 0.0, 0.0, 26.0, 25.18841027461853, 0.3965465400492854, 1.0, 1.0, 58629.57088406651], 
processed observation next is [1.0, 0.8260869565217391, 0.32409972299168976, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5990341895515442, 0.6321821800164285, 1.0, 1.0, 0.2791884327812691], 
reward next is 0.7208, 
noisyNet noise sample is [array([0.6374638], dtype=float32), -0.8981963]. 
=============================================
[2019-04-04 03:39:18,187] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.3648906e-20 7.6220870e-14 3.3869980e-15 6.8277450e-10 2.1674457e-20
 1.0000000e+00 1.0483674e-14], sum to 1.0000
[2019-04-04 03:39:18,188] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7547
[2019-04-04 03:39:18,210] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 85.66666666666667, 0.0, 0.0, 26.0, 24.36582990763226, 0.1616911522927974, 0.0, 1.0, 42216.17870657162], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1989600.0000, 
sim time next is 1990200.0000, 
raw observation next is [-6.1, 86.33333333333333, 0.0, 0.0, 26.0, 24.3497854766854, 0.1584121806611973, 0.0, 1.0, 42109.4997501445], 
processed observation next is [1.0, 0.0, 0.29362880886426596, 0.8633333333333333, 0.0, 0.0, 0.6666666666666666, 0.5291487897237834, 0.5528040602203991, 0.0, 1.0, 0.2005214273816405], 
reward next is 0.7995, 
noisyNet noise sample is [array([-0.3832905], dtype=float32), 0.3262294]. 
=============================================
[2019-04-04 03:39:26,293] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.7492495e-17 1.0192503e-13 4.9667876e-13 4.7744471e-01 2.7078804e-16
 5.2255529e-01 6.4010047e-12], sum to 1.0000
[2019-04-04 03:39:26,293] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0356
[2019-04-04 03:39:26,331] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.50453685618691, 0.169445555399275, 0.0, 1.0, 40823.62435892415], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2777400.0000, 
sim time next is 2778000.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.49543177742892, 0.1668478495475048, 0.0, 1.0, 40775.48385153658], 
processed observation next is [1.0, 0.13043478260869565, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5412859814524099, 0.5556159498491683, 0.0, 1.0, 0.19416897072160275], 
reward next is 0.8058, 
noisyNet noise sample is [array([-0.38285205], dtype=float32), -1.6979378]. 
=============================================
[2019-04-04 03:39:26,360] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[81.30291 ]
 [81.3209  ]
 [81.33555 ]
 [81.320305]
 [81.32617 ]], R is [[81.28683472]
 [81.27957153]
 [81.2720871 ]
 [81.26430511]
 [81.25617981]].
[2019-04-04 03:39:31,467] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0758231e-22 3.4485739e-15 6.3631977e-18 3.8727665e-16 2.1244514e-22
 1.0000000e+00 7.4288992e-17], sum to 1.0000
[2019-04-04 03:39:31,468] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6500
[2019-04-04 03:39:31,511] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.333333333333333, 80.33333333333334, 0.0, 0.0, 26.0, 25.36605521263003, 0.4244775245720537, 0.0, 1.0, 46375.76146540394], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2931600.0000, 
sim time next is 2932200.0000, 
raw observation next is [-1.5, 81.5, 0.0, 0.0, 26.0, 25.27690996215867, 0.412851912087365, 0.0, 1.0, 44780.63916273043], 
processed observation next is [1.0, 0.9565217391304348, 0.4210526315789474, 0.815, 0.0, 0.0, 0.6666666666666666, 0.6064091635132224, 0.6376173040291216, 0.0, 1.0, 0.21324113887014493], 
reward next is 0.7868, 
noisyNet noise sample is [array([0.7977898], dtype=float32), 1.7879127]. 
=============================================
[2019-04-04 03:39:36,374] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.4164858e-18 1.3508336e-15 1.3539276e-17 9.9998260e-01 3.8254144e-19
 1.7395894e-05 1.5565209e-15], sum to 1.0000
[2019-04-04 03:39:36,376] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3680
[2019-04-04 03:39:36,419] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 65.0, 243.5, 351.8333333333333, 26.0, 24.96635202822455, 0.3649378348726735, 0.0, 1.0, 44526.94228351543], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2979600.0000, 
sim time next is 2980200.0000, 
raw observation next is [-3.0, 65.0, 231.0, 419.6666666666666, 26.0, 24.98628722823063, 0.3724435401104111, 0.0, 1.0, 31273.66387361489], 
processed observation next is [0.0, 0.4782608695652174, 0.3795013850415513, 0.65, 0.77, 0.4637200736648249, 0.6666666666666666, 0.5821906023525525, 0.6241478467034703, 0.0, 1.0, 0.14892220892197566], 
reward next is 0.8511, 
noisyNet noise sample is [array([1.1387383], dtype=float32), -0.02664838]. 
=============================================
[2019-04-04 03:39:39,827] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.9685751e-21 1.0904279e-16 9.6990914e-18 1.0000000e+00 5.0585855e-21
 2.5124555e-08 1.8320260e-16], sum to 1.0000
[2019-04-04 03:39:39,838] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6428
[2019-04-04 03:39:39,913] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.833333333333334, 76.0, 62.33333333333334, 347.6666666666667, 26.0, 25.35219353965322, 0.3803717887536537, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3831000.0000, 
sim time next is 3831600.0000, 
raw observation next is [-4.666666666666667, 75.0, 76.66666666666667, 397.3333333333333, 26.0, 25.35884784730901, 0.4160930857693041, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.3333333333333333, 0.75, 0.2555555555555556, 0.43904235727440144, 0.6666666666666666, 0.6132373206090843, 0.6386976952564347, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.49423146], dtype=float32), 0.6867856]. 
=============================================
[2019-04-04 03:39:43,158] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.5455875e-20 5.4388921e-14 1.3806384e-15 1.2781462e-04 4.0819804e-18
 9.9987221e-01 7.9676671e-15], sum to 1.0000
[2019-04-04 03:39:43,158] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1429
[2019-04-04 03:39:43,204] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 93.0, 17.5, 48.49999999999999, 26.0, 24.9819783759643, 0.2710986606686311, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2878800.0000, 
sim time next is 2879400.0000, 
raw observation next is [2.0, 93.0, 34.99999999999999, 69.99999999999999, 26.0, 25.00338357764691, 0.2652652316686382, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.518005540166205, 0.93, 0.11666666666666664, 0.07734806629834252, 0.6666666666666666, 0.5836152981372426, 0.588421743889546, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.1473439], dtype=float32), -1.0888383]. 
=============================================
[2019-04-04 03:39:45,246] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.4006027e-17 3.4133703e-14 4.1134668e-13 9.9536872e-01 4.3217841e-17
 4.6312786e-03 9.5850248e-13], sum to 1.0000
[2019-04-04 03:39:45,270] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5294
[2019-04-04 03:39:45,320] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.41672074099274, 0.4552188465439418, 0.0, 1.0, 18765.13620673728], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3896400.0000, 
sim time next is 3897000.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.69363997828148, 0.4419632423298444, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.40720221606648205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6411366648567901, 0.6473210807766149, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.06384473], dtype=float32), 0.39964968]. 
=============================================
[2019-04-04 03:39:45,325] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[80.5231 ]
 [80.47797]
 [80.35753]
 [80.28618]
 [80.23368]], R is [[80.69355011]
 [80.79725647]
 [80.78524017]
 [80.74404907]
 [80.6397171 ]].
[2019-04-04 03:39:45,949] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1431328e-25 4.4346165e-22 3.4821800e-21 1.0000000e+00 7.9938807e-26
 3.2482159e-12 5.6760145e-20], sum to 1.0000
[2019-04-04 03:39:45,949] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8050
[2019-04-04 03:39:46,045] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.8, 87.0, 0.0, 0.0, 26.0, 25.00200654770808, 0.2828349964713633, 0.0, 1.0, 30545.05940344291], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3090600.0000, 
sim time next is 3091200.0000, 
raw observation next is [-0.8666666666666667, 88.66666666666666, 0.0, 0.0, 26.0, 25.00440132770882, 0.2800314444504828, 0.0, 1.0, 31397.89247756771], 
processed observation next is [0.0, 0.782608695652174, 0.4385964912280702, 0.8866666666666666, 0.0, 0.0, 0.6666666666666666, 0.5837001106424017, 0.5933438148168276, 0.0, 1.0, 0.14951377370270338], 
reward next is 0.8505, 
noisyNet noise sample is [array([-0.81997484], dtype=float32), -0.111704595]. 
=============================================
[2019-04-04 03:39:51,759] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1672878e-21 8.4695182e-17 1.2712742e-17 2.7858207e-10 9.6694900e-20
 1.0000000e+00 1.8245634e-16], sum to 1.0000
[2019-04-04 03:39:51,759] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5537
[2019-04-04 03:39:51,832] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.166666666666666, 100.0, 55.66666666666668, 288.6666666666667, 26.0, 25.60881079626271, 0.3830874172768022, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3139800.0000, 
sim time next is 3140400.0000, 
raw observation next is [6.333333333333333, 100.0, 69.33333333333334, 340.3333333333334, 26.0, 25.67264996253904, 0.4163759194757056, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.6380424746075716, 1.0, 0.23111111111111116, 0.3760589318600369, 0.6666666666666666, 0.6393874968782534, 0.6387919731585686, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([3.071367], dtype=float32), 0.40473902]. 
=============================================
[2019-04-04 03:39:52,049] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.6807225e-25 5.2581757e-19 2.0436994e-19 2.0782695e-10 4.0753166e-24
 1.0000000e+00 8.8820619e-20], sum to 1.0000
[2019-04-04 03:39:52,049] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5177
[2019-04-04 03:39:52,084] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 86.0, 0.0, 0.0, 26.0, 25.5226185800715, 0.4797233141843245, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3454200.0000, 
sim time next is 3454800.0000, 
raw observation next is [1.0, 86.0, 0.0, 0.0, 26.0, 25.49432546046162, 0.4652038659074856, 0.0, 1.0, 18753.37833274642], 
processed observation next is [1.0, 1.0, 0.4903047091412743, 0.86, 0.0, 0.0, 0.6666666666666666, 0.624527121705135, 0.6550679553024952, 0.0, 1.0, 0.08930180158450676], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.42210707], dtype=float32), 1.0540742]. 
=============================================
[2019-04-04 03:40:01,509] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-04 03:40:01,516] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 03:40:01,517] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:40:01,519] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run44
[2019-04-04 03:40:01,570] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 03:40:01,572] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 03:40:01,573] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:40:01,574] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:40:01,578] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run44
[2019-04-04 03:40:01,625] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run44
[2019-04-04 03:40:13,472] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.0869806], dtype=float32), -0.080885895]
[2019-04-04 03:40:13,472] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-7.300000000000001, 68.0, 0.0, 0.0, 26.0, 22.99482678592881, -0.06392914224110968, 1.0, 1.0, 202333.8988729878]
[2019-04-04 03:40:13,472] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 03:40:13,472] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [8.9950179e-20 4.4687425e-16 6.8118093e-16 9.9996090e-01 9.2135991e-20
 3.9075836e-05 3.1451742e-15], sampled 0.36233789109727965
[2019-04-04 03:42:37,993] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.0869806], dtype=float32), -0.080885895]
[2019-04-04 03:42:37,993] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [7.3, 51.0, 111.5, 259.0, 26.0, 25.81983393291432, 0.4590422857312788, 0.0, 1.0, 0.0]
[2019-04-04 03:42:37,993] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 03:42:37,994] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [5.5777171e-22 5.1147419e-18 2.0813062e-17 9.9999499e-01 6.5434171e-21
 4.9971663e-06 6.4477610e-17], sampled 0.4194758535991625
[2019-04-04 03:42:56,862] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.0869806], dtype=float32), -0.080885895]
[2019-04-04 03:42:56,862] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [8.166666666666666, 27.33333333333333, 0.0, 0.0, 26.0, 25.47426129491999, 0.3781509090486966, 0.0, 1.0, 36739.09887816246]
[2019-04-04 03:42:56,863] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 03:42:56,863] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.00122116e-23 7.29309313e-21 6.50044020e-19 1.00000000e+00
 1.22343251e-22 9.27031008e-10 1.46379639e-18], sampled 0.21630233849749902
[2019-04-04 03:43:10,206] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7354.9783 239614550.0110 1605.2968
[2019-04-04 03:43:40,865] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7242.4907 263236944.6042 1551.5928
[2019-04-04 03:43:50,785] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7183.3820 275649781.8070 1231.3674
[2019-04-04 03:43:51,822] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 4300000, evaluation results [4300000.0, 7242.490739980087, 263236944.60418063, 1551.5928353761785, 7354.978333280864, 239614550.01102033, 1605.2967641962182, 7183.381991395213, 275649781.80700326, 1231.3674156324892]
[2019-04-04 03:43:56,462] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.3550610e-25 5.6052447e-20 1.3733833e-20 4.8626827e-12 3.1978848e-25
 1.0000000e+00 3.1805686e-20], sum to 1.0000
[2019-04-04 03:43:56,462] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6414
[2019-04-04 03:43:56,547] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.93142040067346, 0.3563838076514112, 0.0, 1.0, 41700.42275286913], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3372000.0000, 
sim time next is 3372600.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 25.03435914190698, 0.3552828705284315, 0.0, 1.0, 41414.0997494382], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5861965951589149, 0.6184276235094771, 0.0, 1.0, 0.19720999880684859], 
reward next is 0.8028, 
noisyNet noise sample is [array([-0.71973944], dtype=float32), -0.677578]. 
=============================================
[2019-04-04 03:43:57,351] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.8898690e-18 1.3651215e-14 1.6405394e-16 9.9999988e-01 8.0540508e-20
 7.3473984e-08 3.6281382e-14], sum to 1.0000
[2019-04-04 03:43:57,361] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8904
[2019-04-04 03:43:57,390] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.5, 46.5, 87.0, 717.0, 26.0, 26.39879438299907, 0.7016615650959669, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3857400.0000, 
sim time next is 3858000.0000, 
raw observation next is [2.666666666666667, 46.0, 83.16666666666666, 689.3333333333333, 26.0, 26.64437980007455, 0.7220907580633257, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5364727608494922, 0.46, 0.2772222222222222, 0.7616942909760589, 0.6666666666666666, 0.7203649833395458, 0.7406969193544418, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3124539], dtype=float32), -0.9518896]. 
=============================================
[2019-04-04 03:43:57,435] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[84.67743]
 [84.61377]
 [84.63346]
 [84.61925]
 [84.73652]], R is [[84.7201767 ]
 [84.87297821]
 [85.02424622]
 [85.1740036 ]
 [85.32226562]].
[2019-04-04 03:44:07,974] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.5006798e-29 3.7305570e-25 3.2238006e-26 1.0000000e+00 1.6356287e-30
 2.6098874e-18 2.6377142e-24], sum to 1.0000
[2019-04-04 03:44:07,974] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9190
[2019-04-04 03:44:08,025] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.833333333333334, 37.33333333333334, 50.33333333333334, 413.6666666666667, 26.0, 26.44020027073589, 0.5140232413391467, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3948600.0000, 
sim time next is 3949200.0000, 
raw observation next is [-5.0, 38.0, 42.5, 352.5, 26.0, 26.57321211805704, 0.6569060717295075, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.32409972299168976, 0.38, 0.14166666666666666, 0.38950276243093923, 0.6666666666666666, 0.71443434317142, 0.7189686905765025, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0572329], dtype=float32), -1.1333551]. 
=============================================
[2019-04-04 03:44:11,117] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.3735188e-19 2.1870262e-15 2.4389826e-17 9.5571434e-01 3.7716588e-21
 4.4285651e-02 3.5124868e-15], sum to 1.0000
[2019-04-04 03:44:11,119] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6650
[2019-04-04 03:44:11,145] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.833333333333333, 71.0, 76.33333333333333, 627.6666666666666, 26.0, 26.58975868421375, 0.8038277942204038, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3253800.0000, 
sim time next is 3254400.0000, 
raw observation next is [-3.0, 71.0, 72.0, 598.5, 26.0, 26.78295856092602, 0.8227990522829809, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.3795013850415513, 0.71, 0.24, 0.6613259668508288, 0.6666666666666666, 0.7319132134105016, 0.7742663507609936, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1070734], dtype=float32), 0.6757533]. 
=============================================
[2019-04-04 03:44:18,604] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.2409552e-14 7.6419987e-10 1.4438727e-10 8.4529281e-01 5.4211455e-15
 1.5470721e-01 2.7275893e-10], sum to 1.0000
[2019-04-04 03:44:18,605] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8754
[2019-04-04 03:44:18,622] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.26666666666667, 80.66666666666666, 0.0, 0.0, 26.0, 24.06692901385745, 0.1012320523102743, 0.0, 1.0, 44448.11255794431], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2688000.0000, 
sim time next is 2688600.0000, 
raw observation next is [-12.58333333333333, 81.83333333333334, 0.0, 0.0, 26.0, 23.96982488602537, 0.08788164138721637, 0.0, 1.0, 44478.03193280899], 
processed observation next is [1.0, 0.08695652173913043, 0.11403508771929832, 0.8183333333333335, 0.0, 0.0, 0.6666666666666666, 0.4974854071687809, 0.5292938804624054, 0.0, 1.0, 0.2118001520609952], 
reward next is 0.7882, 
noisyNet noise sample is [array([-0.45994782], dtype=float32), 0.03903797]. 
=============================================
[2019-04-04 03:44:18,741] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4512878e-23 4.9303709e-20 1.6006494e-22 9.9999976e-01 1.9087490e-26
 2.0475586e-07 1.1914880e-19], sum to 1.0000
[2019-04-04 03:44:18,741] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2818
[2019-04-04 03:44:18,785] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 30.0, 121.0, 816.0, 26.0, 26.76040998091555, 0.4069416652958994, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4102200.0000, 
sim time next is 4102800.0000, 
raw observation next is [0.3333333333333333, 29.33333333333333, 120.8333333333333, 820.1666666666666, 26.0, 26.72242103569085, 0.6317142641166744, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4718374884579871, 0.2933333333333333, 0.4027777777777777, 0.9062615101289134, 0.6666666666666666, 0.7268684196409042, 0.7105714213722248, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2727], dtype=float32), 0.32808745]. 
=============================================
[2019-04-04 03:44:24,427] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.8015643e-20 2.8827123e-15 5.5084336e-15 5.0962221e-02 1.2289076e-19
 9.4903785e-01 3.8631208e-15], sum to 1.0000
[2019-04-04 03:44:24,427] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7185
[2019-04-04 03:44:24,447] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 77.83333333333334, 0.0, 0.0, 26.0, 25.41696488357027, 0.4285120744953205, 0.0, 1.0, 78877.05738153453], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3463800.0000, 
sim time next is 3464400.0000, 
raw observation next is [1.0, 76.66666666666667, 0.0, 0.0, 26.0, 25.39240529992279, 0.4353743678748396, 0.0, 1.0, 71074.98603964876], 
processed observation next is [1.0, 0.08695652173913043, 0.4903047091412743, 0.7666666666666667, 0.0, 0.0, 0.6666666666666666, 0.6160337749935657, 0.6451247892916132, 0.0, 1.0, 0.3384523144745179], 
reward next is 0.6615, 
noisyNet noise sample is [array([0.09419976], dtype=float32), 2.222495]. 
=============================================
[2019-04-04 03:44:25,977] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.9473815e-22 1.2173784e-19 8.4673135e-20 9.9999297e-01 8.8823920e-23
 7.0855049e-06 1.1190795e-17], sum to 1.0000
[2019-04-04 03:44:25,978] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6102
[2019-04-04 03:44:26,012] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 47.00000000000001, 0.0, 0.0, 26.0, 25.46461961136234, 0.3767750804302402, 0.0, 1.0, 31761.18390092982], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4227000.0000, 
sim time next is 4227600.0000, 
raw observation next is [1.0, 47.0, 0.0, 0.0, 26.0, 25.4452345467151, 0.3728761435399231, 0.0, 1.0, 44289.20879130811], 
processed observation next is [0.0, 0.9565217391304348, 0.4903047091412743, 0.47, 0.0, 0.0, 0.6666666666666666, 0.6204362122262582, 0.624292047846641, 0.0, 1.0, 0.21090099424432435], 
reward next is 0.7891, 
noisyNet noise sample is [array([1.2351838], dtype=float32), 0.68368393]. 
=============================================
[2019-04-04 03:44:32,075] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.6818055e-20 7.0103771e-15 3.4656636e-16 1.0000000e+00 1.2376669e-19
 1.1913268e-08 5.4249339e-15], sum to 1.0000
[2019-04-04 03:44:32,076] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4256
[2019-04-04 03:44:32,092] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.47592056814221, 0.4378906224386436, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3892800.0000, 
sim time next is 3893400.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.4656064969247, 0.4225973316390916, 0.0, 1.0, 18757.97488872038], 
processed observation next is [1.0, 0.043478260869565216, 0.40720221606648205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.622133874743725, 0.6408657772130305, 0.0, 1.0, 0.08932368994628752], 
reward next is 0.9107, 
noisyNet noise sample is [array([0.12148173], dtype=float32), 0.2360258]. 
=============================================
[2019-04-04 03:44:34,213] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.2083310e-15 2.5133687e-11 8.6989062e-11 1.3694698e-01 4.1391698e-14
 8.6305302e-01 8.0925477e-10], sum to 1.0000
[2019-04-04 03:44:34,213] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3133
[2019-04-04 03:44:34,259] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.333333333333334, 73.0, 0.0, 0.0, 26.0, 24.87511293970223, 0.2788299560292406, 0.0, 1.0, 43832.59113505465], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3820800.0000, 
sim time next is 3821400.0000, 
raw observation next is [-4.5, 74.0, 0.0, 0.0, 26.0, 24.87011085951917, 0.2746749220494025, 0.0, 1.0, 43767.01673413835], 
processed observation next is [1.0, 0.21739130434782608, 0.3379501385041552, 0.74, 0.0, 0.0, 0.6666666666666666, 0.5725092382932641, 0.5915583073498009, 0.0, 1.0, 0.2084143654006588], 
reward next is 0.7916, 
noisyNet noise sample is [array([1.2582406], dtype=float32), -2.2134676]. 
=============================================
[2019-04-04 03:44:35,268] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.8331335e-20 3.1356749e-16 4.9031476e-16 9.1526227e-04 9.8165543e-20
 9.9908471e-01 6.3590084e-15], sum to 1.0000
[2019-04-04 03:44:35,268] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2549
[2019-04-04 03:44:35,352] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 77.0, 0.0, 0.0, 26.0, 24.65749357708808, 0.2806811991906866, 1.0, 1.0, 196447.4411329648], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3828000.0000, 
sim time next is 3828600.0000, 
raw observation next is [-5.0, 77.0, 5.0, 149.0, 26.0, 24.88219537372998, 0.3390872539664398, 1.0, 1.0, 18778.0579752685], 
processed observation next is [1.0, 0.30434782608695654, 0.32409972299168976, 0.77, 0.016666666666666666, 0.16464088397790055, 0.6666666666666666, 0.5735162811441651, 0.6130290846554799, 1.0, 1.0, 0.08941932369175476], 
reward next is 0.9106, 
noisyNet noise sample is [array([-0.4492503], dtype=float32), 0.16253263]. 
=============================================
[2019-04-04 03:44:39,832] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.3661838e-22 3.0680398e-18 6.4793439e-20 1.0000000e+00 3.7292252e-22
 1.2228440e-08 3.6671772e-17], sum to 1.0000
[2019-04-04 03:44:39,832] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4764
[2019-04-04 03:44:39,861] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.666666666666667, 38.0, 106.8333333333333, 794.0, 26.0, 26.86694851846022, 0.7578879040343202, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3939600.0000, 
sim time next is 3940200.0000, 
raw observation next is [-4.5, 38.0, 105.0, 788.0, 26.0, 26.89980452548673, 0.7609304442580914, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3379501385041552, 0.38, 0.35, 0.8707182320441988, 0.6666666666666666, 0.7416503771238941, 0.7536434814193638, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2011422], dtype=float32), 1.3481472]. 
=============================================
[2019-04-04 03:44:40,955] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2637884e-18 3.3311337e-13 5.1205063e-14 9.9999976e-01 2.8756216e-18
 2.6763124e-07 1.4634256e-13], sum to 1.0000
[2019-04-04 03:44:40,966] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2677
[2019-04-04 03:44:40,989] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 77.0, 0.0, 0.0, 26.0, 25.24115781848688, 0.3961627952966502, 0.0, 1.0, 44074.09706801368], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3805200.0000, 
sim time next is 3805800.0000, 
raw observation next is [-4.0, 77.0, 0.0, 0.0, 26.0, 25.23366531826213, 0.3988372126188941, 0.0, 1.0, 44155.26956009421], 
processed observation next is [1.0, 0.043478260869565216, 0.3518005540166205, 0.77, 0.0, 0.0, 0.6666666666666666, 0.6028054431885108, 0.6329457375396313, 0.0, 1.0, 0.210263188381401], 
reward next is 0.7897, 
noisyNet noise sample is [array([-2.1863515], dtype=float32), -0.5266445]. 
=============================================
[2019-04-04 03:44:41,816] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.7996850e-22 4.3067705e-18 7.0525047e-18 1.0000000e+00 9.2249274e-21
 5.4221987e-09 1.0021762e-16], sum to 1.0000
[2019-04-04 03:44:41,817] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2299
[2019-04-04 03:44:41,830] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.433333333333333, 68.66666666666666, 0.0, 0.0, 26.0, 25.464812100496, 0.4160103407527616, 0.0, 1.0, 20250.35194278053], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4593000.0000, 
sim time next is 4593600.0000, 
raw observation next is [-1.5, 69.0, 0.0, 0.0, 26.0, 25.41645606018048, 0.4050728918243192, 0.0, 1.0, 51584.72722551995], 
processed observation next is [1.0, 0.17391304347826086, 0.4210526315789474, 0.69, 0.0, 0.0, 0.6666666666666666, 0.6180380050150399, 0.635024297274773, 0.0, 1.0, 0.24564155821676167], 
reward next is 0.7544, 
noisyNet noise sample is [array([0.3949498], dtype=float32), -1.4911283]. 
=============================================
[2019-04-04 03:44:46,582] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.7270312e-17 7.8552019e-14 5.4081777e-13 9.9993050e-01 9.6883265e-17
 6.9538684e-05 1.9598455e-12], sum to 1.0000
[2019-04-04 03:44:46,582] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7319
[2019-04-04 03:44:46,609] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.6666666666666667, 82.0, 0.0, 0.0, 26.0, 25.47901394846458, 0.4499251532677342, 0.0, 1.0, 90956.55501216324], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4678800.0000, 
sim time next is 4679400.0000, 
raw observation next is [0.3333333333333333, 87.0, 0.0, 0.0, 26.0, 25.47511407995761, 0.4491895153915528, 0.0, 1.0, 60987.26419248541], 
processed observation next is [1.0, 0.13043478260869565, 0.4718374884579871, 0.87, 0.0, 0.0, 0.6666666666666666, 0.6229261733298008, 0.6497298384638509, 0.0, 1.0, 0.29041554377374007], 
reward next is 0.7096, 
noisyNet noise sample is [array([1.2444378], dtype=float32), 0.29782566]. 
=============================================
[2019-04-04 03:44:53,993] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.3091757e-19 2.6949825e-16 2.7298426e-15 9.9987614e-01 1.0381911e-18
 1.2386648e-04 3.6648127e-14], sum to 1.0000
[2019-04-04 03:44:53,994] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0881
[2019-04-04 03:44:54,034] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.07058973715694, 0.4683715925221009, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4477200.0000, 
sim time next is 4477800.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.06075389774816, 0.4642980750724715, 0.0, 1.0, 18706.20154208094], 
processed observation next is [1.0, 0.8260869565217391, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.58839615814568, 0.6547660250241572, 0.0, 1.0, 0.08907715020038542], 
reward next is 0.9109, 
noisyNet noise sample is [array([-0.16847719], dtype=float32), 0.48016658]. 
=============================================
[2019-04-04 03:44:55,748] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.1274186e-25 1.4274493e-21 1.5280333e-21 1.0000000e+00 1.3432657e-25
 1.9246125e-14 2.7879221e-20], sum to 1.0000
[2019-04-04 03:44:55,748] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8133
[2019-04-04 03:44:55,764] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.5, 42.5, 0.0, 0.0, 26.0, 25.33245314041257, 0.4153168373240757, 0.0, 1.0, 40992.76020621186], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4152600.0000, 
sim time next is 4153200.0000, 
raw observation next is [-1.666666666666667, 43.66666666666666, 0.0, 0.0, 26.0, 25.34920973466132, 0.4118552725003521, 0.0, 1.0, 39607.55124597775], 
processed observation next is [0.0, 0.043478260869565216, 0.4164358264081256, 0.4366666666666666, 0.0, 0.0, 0.6666666666666666, 0.6124341445551099, 0.6372850908334508, 0.0, 1.0, 0.18860738688560835], 
reward next is 0.8114, 
noisyNet noise sample is [array([0.22090279], dtype=float32), 0.23234399]. 
=============================================
[2019-04-04 03:44:57,800] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:44:57,800] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:44:57,803] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run33
[2019-04-04 03:45:05,766] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.33651805e-18 2.73088769e-16 4.17496821e-13 1.00000000e+00
 7.06410640e-19 1.10099145e-10 2.08552115e-13], sum to 1.0000
[2019-04-04 03:45:05,767] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9175
[2019-04-04 03:45:05,773] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.666666666666667, 24.0, 0.0, 0.0, 26.0, 25.85688959097191, 0.5545262539820306, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4997400.0000, 
sim time next is 4998000.0000, 
raw observation next is [5.333333333333333, 25.0, 0.0, 0.0, 26.0, 25.80816904312955, 0.541075863987306, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.6103416435826409, 0.25, 0.0, 0.0, 0.6666666666666666, 0.6506807535941291, 0.680358621329102, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.09520674], dtype=float32), -1.6956313]. 
=============================================
[2019-04-04 03:45:05,801] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[81.79274 ]
 [81.9022  ]
 [82.663536]
 [83.22853 ]
 [81.494316]], R is [[81.75450897]
 [81.93696594]
 [82.11759949]
 [82.29642487]
 [82.47346497]].
[2019-04-04 03:45:11,624] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.3776951e-21 2.6148714e-17 6.5807359e-17 1.0000000e+00 8.6631795e-20
 2.5262454e-09 1.1701044e-15], sum to 1.0000
[2019-04-04 03:45:11,627] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4579
[2019-04-04 03:45:11,640] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 55.0, 0.0, 0.0, 26.0, 25.39252012338274, 0.3901033999837405, 0.0, 1.0, 68065.99783663309], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5025600.0000, 
sim time next is 5026200.0000, 
raw observation next is [-1.0, 54.16666666666667, 0.0, 0.0, 26.0, 25.36041537163641, 0.3892675415538148, 0.0, 1.0, 63247.77150749498], 
processed observation next is [1.0, 0.17391304347826086, 0.4349030470914128, 0.5416666666666667, 0.0, 0.0, 0.6666666666666666, 0.6133679476363675, 0.629755847184605, 0.0, 1.0, 0.30117986432140464], 
reward next is 0.6988, 
noisyNet noise sample is [array([-0.3464715], dtype=float32), -2.102174]. 
=============================================
[2019-04-04 03:45:11,909] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:45:11,909] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:45:11,913] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run33
[2019-04-04 03:45:12,630] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.9831182e-19 3.1969444e-15 1.9771731e-14 9.9449748e-01 1.2106841e-18
 5.5025746e-03 1.4744280e-14], sum to 1.0000
[2019-04-04 03:45:12,637] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4718
[2019-04-04 03:45:12,654] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 71.0, 0.0, 0.0, 26.0, 25.31730167778647, 0.4013558474623007, 0.0, 1.0, 40930.56367207145], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4514400.0000, 
sim time next is 4515000.0000, 
raw observation next is [-1.0, 71.0, 0.0, 0.0, 26.0, 25.2851920443883, 0.3954706708607869, 0.0, 1.0, 40847.57164942432], 
processed observation next is [1.0, 0.2608695652173913, 0.4349030470914128, 0.71, 0.0, 0.0, 0.6666666666666666, 0.6070993370323583, 0.6318235569535956, 0.0, 1.0, 0.19451224594963962], 
reward next is 0.8055, 
noisyNet noise sample is [array([-0.64935905], dtype=float32), -0.1376294]. 
=============================================
[2019-04-04 03:45:12,662] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[82.56539]
 [82.58596]
 [82.59228]
 [82.61318]
 [82.65284]], R is [[82.53728485]
 [82.51700592]
 [82.49638367]
 [82.47547913]
 [82.45436096]].
[2019-04-04 03:45:14,504] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.5947570e-17 1.4757724e-13 7.7674577e-12 9.9807477e-01 1.9896467e-15
 1.9252186e-03 1.1919924e-11], sum to 1.0000
[2019-04-04 03:45:14,505] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6775
[2019-04-04 03:45:14,524] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.8, 69.66666666666667, 0.0, 0.0, 26.0, 25.63319336853975, 0.3890369165165044, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4335600.0000, 
sim time next is 4336200.0000, 
raw observation next is [3.75, 69.5, 0.0, 0.0, 26.0, 25.58669860428445, 0.3795136981361361, 0.0, 1.0, 18736.9948410621], 
processed observation next is [1.0, 0.17391304347826086, 0.5664819944598338, 0.695, 0.0, 0.0, 0.6666666666666666, 0.6322248836903709, 0.6265045660453787, 0.0, 1.0, 0.08922378495743857], 
reward next is 0.9108, 
noisyNet noise sample is [array([-0.10319677], dtype=float32), -0.112134025]. 
=============================================
[2019-04-04 03:45:16,574] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:45:16,575] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:45:16,584] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run33
[2019-04-04 03:45:23,353] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.6652431e-16 2.4393106e-16 6.2667538e-15 1.3574972e-12 4.1132980e-14
 1.0000000e+00 1.6392221e-13], sum to 1.0000
[2019-04-04 03:45:23,354] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9629
[2019-04-04 03:45:23,371] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.283333333333333, 95.5, 0.0, 0.0, 24.0, 19.42332105216042, -0.8958077809152848, 0.0, 1.0, 64072.20985621568], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 11400.0000, 
sim time next is 12000.0000, 
raw observation next is [7.366666666666667, 95.0, 0.0, 0.0, 25.0, 19.80135315618279, -0.8613579749894017, 0.0, 1.0, 50296.60120373273], 
processed observation next is [0.0, 0.13043478260869565, 0.6666666666666667, 0.95, 0.0, 0.0, 0.5833333333333334, 0.15011276301523235, 0.21288067500353278, 0.0, 1.0, 0.23950762477967968], 
reward next is 0.7605, 
noisyNet noise sample is [array([0.04242681], dtype=float32), 0.85798115]. 
=============================================
[2019-04-04 03:45:23,388] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[60.21773 ]
 [57.98057 ]
 [56.406216]
 [54.962616]
 [53.884422]], R is [[62.70838547]
 [62.77619553]
 [62.6581459 ]
 [62.16945648]
 [61.60380173]].
[2019-04-04 03:45:30,709] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:45:30,709] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:45:30,713] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run33
[2019-04-04 03:45:33,306] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.3440671e-21 6.0961979e-18 3.8653700e-17 9.9972922e-01 7.8971192e-22
 2.7071987e-04 2.2728464e-16], sum to 1.0000
[2019-04-04 03:45:33,314] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2153
[2019-04-04 03:45:33,326] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.14035031e-18 1.10325745e-14 6.42496302e-16 1.00000000e+00
 2.07632648e-19 7.74150113e-13 4.98016748e-14], sum to 1.0000
[2019-04-04 03:45:33,326] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2183
[2019-04-04 03:45:33,331] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.97896427702555, -0.1922691990458868, 0.0, 1.0, 44587.56758201246], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 190800.0000, 
sim time next is 191400.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.95903614063921, -0.1945095326281323, 0.0, 1.0, 44674.1062215674], 
processed observation next is [1.0, 0.21739130434782608, 0.21606648199445982, 0.78, 0.0, 0.0, 0.6666666666666666, 0.41325301171993417, 0.4351634891239559, 0.0, 1.0, 0.21273383915032096], 
reward next is 0.7873, 
noisyNet noise sample is [array([-0.02001353], dtype=float32), -0.4095834]. 
=============================================
[2019-04-04 03:45:33,359] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 66.0, 0.0, 0.0, 26.0, 25.17301423217327, 0.447258949526079, 1.0, 1.0, 59939.31319857676], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3786600.0000, 
sim time next is 3787200.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.13619499530803, 0.4487009568699614, 1.0, 1.0, 64097.27994873455], 
processed observation next is [1.0, 0.8695652173913043, 0.40720221606648205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5946829162756693, 0.6495669856233205, 1.0, 1.0, 0.30522514261302164], 
reward next is 0.6948, 
noisyNet noise sample is [array([0.80994505], dtype=float32), 0.40151367]. 
=============================================
[2019-04-04 03:45:33,606] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.9476594e-17 3.5066056e-14 1.9663908e-13 9.9500173e-01 8.7980510e-18
 4.9982369e-03 2.4118574e-12], sum to 1.0000
[2019-04-04 03:45:33,607] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7713
[2019-04-04 03:45:33,629] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-15.0, 69.0, 0.0, 0.0, 26.0, 23.29828260041688, -0.1377091052938758, 0.0, 1.0, 48373.26634609301], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 352800.0000, 
sim time next is 353400.0000, 
raw observation next is [-15.0, 69.0, 0.0, 0.0, 26.0, 23.23676357220463, -0.1613266197819617, 0.0, 1.0, 48593.34435400147], 
processed observation next is [1.0, 0.08695652173913043, 0.04709141274238226, 0.69, 0.0, 0.0, 0.6666666666666666, 0.43639696435038583, 0.44622446007267946, 0.0, 1.0, 0.23139687787619748], 
reward next is 0.7686, 
noisyNet noise sample is [array([-0.13008417], dtype=float32), -0.8614317]. 
=============================================
[2019-04-04 03:45:36,819] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.4075337e-18 8.1858572e-14 4.7025760e-16 1.0000000e+00 8.5580378e-20
 1.3728783e-08 1.6518288e-14], sum to 1.0000
[2019-04-04 03:45:36,819] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6029
[2019-04-04 03:45:36,866] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.666666666666667, 82.66666666666667, 0.0, 0.0, 26.0, 25.0988107972156, 0.5019868602261451, 0.0, 1.0, 73967.01608828352], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4740000.0000, 
sim time next is 4740600.0000, 
raw observation next is [-1.833333333333333, 83.83333333333334, 0.0, 0.0, 26.0, 25.27099393373664, 0.5237353768399136, 0.0, 1.0, 52557.61247800801], 
processed observation next is [1.0, 0.8695652173913043, 0.41181902123730385, 0.8383333333333334, 0.0, 0.0, 0.6666666666666666, 0.60591616114472, 0.6745784589466378, 0.0, 1.0, 0.2502743451333715], 
reward next is 0.7497, 
noisyNet noise sample is [array([-0.3276604], dtype=float32), -1.218186]. 
=============================================
[2019-04-04 03:45:43,929] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:45:43,929] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:45:43,937] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run33
[2019-04-04 03:45:45,310] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.12053886e-26 1.31574915e-23 7.26426114e-24 1.00000000e+00
 8.72865925e-28 1.79055099e-15 2.14763330e-22], sum to 1.0000
[2019-04-04 03:45:45,311] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2820
[2019-04-04 03:45:45,329] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.333333333333333, 37.66666666666667, 117.1666666666667, 815.0, 26.0, 27.32157264930553, 0.8079969240173623, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5049600.0000, 
sim time next is 5050200.0000, 
raw observation next is [4.666666666666667, 36.83333333333333, 118.3333333333333, 821.0, 26.0, 27.40098652059005, 0.5883802752663566, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.5918744228993538, 0.3683333333333333, 0.3944444444444443, 0.907182320441989, 0.6666666666666666, 0.7834155433825041, 0.6961267584221189, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1690508], dtype=float32), -1.2975792]. 
=============================================
[2019-04-04 03:45:46,285] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:45:46,285] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:45:46,290] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run33
[2019-04-04 03:45:47,895] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:45:47,895] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:45:47,899] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run33
[2019-04-04 03:45:48,822] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:45:48,822] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:45:48,840] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run33
[2019-04-04 03:45:50,882] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:45:50,883] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:45:50,890] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run33
[2019-04-04 03:45:51,717] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:45:51,717] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:45:51,721] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run33
[2019-04-04 03:45:55,484] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.9323954e-18 9.5286051e-13 5.9068184e-12 1.3345289e-05 6.8272775e-17
 9.9998665e-01 5.1493049e-13], sum to 1.0000
[2019-04-04 03:45:55,487] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9153
[2019-04-04 03:45:55,523] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 36.0, 0.0, 0.0, 26.0, 25.75403052497801, 0.4816689402928977, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5005200.0000, 
sim time next is 5005800.0000, 
raw observation next is [3.0, 35.5, 0.0, 0.0, 26.0, 25.60719804477544, 0.4702050286949981, 0.0, 1.0, 115783.829713115], 
processed observation next is [1.0, 0.9565217391304348, 0.5457063711911359, 0.355, 0.0, 0.0, 0.6666666666666666, 0.6339331703979534, 0.6567350095649994, 0.0, 1.0, 0.5513515700624524], 
reward next is 0.4486, 
noisyNet noise sample is [array([1.0503304], dtype=float32), -0.09307033]. 
=============================================
[2019-04-04 03:45:57,627] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:45:57,627] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:45:57,633] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run33
[2019-04-04 03:46:01,062] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:46:01,063] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:46:01,094] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run33
[2019-04-04 03:46:01,815] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.1531982e-23 1.0775328e-20 1.4199660e-20 1.0000000e+00 2.8486471e-23
 1.3214226e-16 2.7779444e-19], sum to 1.0000
[2019-04-04 03:46:01,816] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1708
[2019-04-04 03:46:01,884] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.1, 73.5, 184.0, 13.0, 26.0, 25.39787230804593, 0.3184020877408813, 1.0, 1.0, 46226.21128868239], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 127800.0000, 
sim time next is 128400.0000, 
raw observation next is [-8.2, 69.33333333333334, 175.0, 111.3333333333333, 26.0, 25.39495603963364, 0.3386280299390682, 1.0, 1.0, 46658.78379759013], 
processed observation next is [1.0, 0.4782608695652174, 0.23545706371191139, 0.6933333333333335, 0.5833333333333334, 0.12302025782688762, 0.6666666666666666, 0.6162463366361367, 0.6128760099796894, 1.0, 1.0, 0.22218468475042918], 
reward next is 0.7778, 
noisyNet noise sample is [array([-1.3426385], dtype=float32), 0.04626393]. 
=============================================
[2019-04-04 03:46:02,025] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:46:02,025] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:46:02,042] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run33
[2019-04-04 03:46:06,377] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.8611089e-21 1.2558446e-18 6.7425172e-20 1.0000000e+00 8.1225087e-22
 1.7980316e-08 7.7923085e-18], sum to 1.0000
[2019-04-04 03:46:06,378] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0676
[2019-04-04 03:46:06,449] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 78.0, 0.0, 0.0, 26.0, 25.25983692825508, 0.4709556601853022, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4735200.0000, 
sim time next is 4735800.0000, 
raw observation next is [-1.0, 78.0, 0.0, 0.0, 26.0, 25.2953899372314, 0.4573338049290565, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.4349030470914128, 0.78, 0.0, 0.0, 0.6666666666666666, 0.6079491614359499, 0.6524446016430189, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1201813], dtype=float32), 1.3860843]. 
=============================================
[2019-04-04 03:46:10,128] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.3381794e-15 9.0176965e-13 4.4024381e-11 3.6690544e-02 2.8342647e-14
 9.6330953e-01 4.4217598e-11], sum to 1.0000
[2019-04-04 03:46:10,128] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7779
[2019-04-04 03:46:10,214] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.7, 93.0, 35.16666666666666, 0.0, 26.0, 22.78167294881008, -0.244130020738056, 0.0, 1.0, 59366.2751673606], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 33600.0000, 
sim time next is 34200.0000, 
raw observation next is [7.7, 93.0, 38.0, 0.0, 26.0, 22.87829328128193, -0.2211273046334797, 0.0, 1.0, 59084.57918552704], 
processed observation next is [0.0, 0.391304347826087, 0.6759002770083103, 0.93, 0.12666666666666668, 0.0, 0.6666666666666666, 0.40652444010682753, 0.4262908984555068, 0.0, 1.0, 0.2813551389787002], 
reward next is 0.7186, 
noisyNet noise sample is [array([1.1557721], dtype=float32), 0.7244271]. 
=============================================
[2019-04-04 03:46:14,137] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.5679531e-21 1.4774266e-17 2.1095938e-17 1.3645661e-01 7.9349473e-21
 8.6354339e-01 6.1081319e-17], sum to 1.0000
[2019-04-04 03:46:14,137] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7478
[2019-04-04 03:46:14,224] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.9, 68.33333333333333, 0.0, 0.0, 26.0, 23.52414286895645, -0.06755246882573292, 0.0, 1.0, 42068.70598564966], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 801600.0000, 
sim time next is 802200.0000, 
raw observation next is [-6.800000000000001, 67.66666666666667, 0.0, 0.0, 26.0, 23.51567359934492, -0.07768225487519335, 0.0, 1.0, 42040.23768741019], 
processed observation next is [1.0, 0.2608695652173913, 0.2742382271468144, 0.6766666666666667, 0.0, 0.0, 0.6666666666666666, 0.45963946661207683, 0.4741059150416022, 0.0, 1.0, 0.20019160803528663], 
reward next is 0.7998, 
noisyNet noise sample is [array([0.00564047], dtype=float32), 0.6523595]. 
=============================================
[2019-04-04 03:46:18,875] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.4483643e-19 7.0068480e-15 7.7980301e-15 6.1619598e-09 1.0671641e-18
 1.0000000e+00 3.3058087e-14], sum to 1.0000
[2019-04-04 03:46:18,909] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6336
[2019-04-04 03:46:18,938] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.3, 69.0, 0.0, 0.0, 26.0, 23.45771657683475, -0.07386519620682759, 0.0, 1.0, 46230.70867874508], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 272400.0000, 
sim time next is 273000.0000, 
raw observation next is [-9.4, 69.5, 0.0, 0.0, 26.0, 23.44956034571815, -0.08438070484670845, 0.0, 1.0, 46369.91518711288], 
processed observation next is [1.0, 0.13043478260869565, 0.20221606648199447, 0.695, 0.0, 0.0, 0.6666666666666666, 0.45413002880984593, 0.47187309838443053, 0.0, 1.0, 0.22080911993863275], 
reward next is 0.7792, 
noisyNet noise sample is [array([-0.5143845], dtype=float32), 0.16283198]. 
=============================================
[2019-04-04 03:46:18,948] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[79.650734]
 [79.58005 ]
 [79.50969 ]
 [79.43678 ]
 [79.38104 ]], R is [[79.67486572]
 [79.65797424]
 [79.64185333]
 [79.62641907]
 [79.61170959]].
[2019-04-04 03:46:26,981] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.6186964e-16 3.2949928e-12 8.3742726e-12 6.7684525e-03 7.1959405e-15
 9.9323153e-01 1.7993988e-11], sum to 1.0000
[2019-04-04 03:46:26,985] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4562
[2019-04-04 03:46:27,003] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.3333333333333333, 50.0, 0.0, 0.0, 26.0, 25.5883177153795, 0.4273700496131744, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5020800.0000, 
sim time next is 5021400.0000, 
raw observation next is [-0.6666666666666667, 52.5, 0.0, 0.0, 26.0, 25.53114358263506, 0.4078405427118173, 0.0, 1.0, 20262.88043529635], 
processed observation next is [1.0, 0.08695652173913043, 0.44413665743305636, 0.525, 0.0, 0.0, 0.6666666666666666, 0.6275952985529217, 0.6359468475706057, 0.0, 1.0, 0.09648990683474452], 
reward next is 0.9035, 
noisyNet noise sample is [array([0.9463909], dtype=float32), -0.93888515]. 
=============================================
[2019-04-04 03:46:32,901] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:46:32,901] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:46:32,914] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run33
[2019-04-04 03:46:56,820] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:46:56,820] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:46:56,824] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run33
[2019-04-04 03:47:01,597] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:47:01,597] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:47:01,609] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run33
[2019-04-04 03:47:02,210] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.0606109e-17 8.2042160e-12 2.0302656e-14 1.1476233e-08 2.1417012e-15
 1.0000000e+00 1.2478282e-12], sum to 1.0000
[2019-04-04 03:47:02,221] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3960
[2019-04-04 03:47:02,235] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 92.16666666666667, 53.0, 0.0, 26.0, 25.70664306283301, 0.51310091275175, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1350600.0000, 
sim time next is 1351200.0000, 
raw observation next is [1.1, 92.33333333333334, 48.5, 0.0, 26.0, 25.70647179610421, 0.5107232482411477, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.49307479224376743, 0.9233333333333335, 0.16166666666666665, 0.0, 0.6666666666666666, 0.6422059830086843, 0.6702410827470492, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7105125], dtype=float32), -0.2281896]. 
=============================================
[2019-04-04 03:47:22,174] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.6980891e-19 3.5803945e-16 2.9347699e-14 1.0991351e-03 4.4126016e-17
 9.9890089e-01 6.0080359e-14], sum to 1.0000
[2019-04-04 03:47:22,175] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9846
[2019-04-04 03:47:22,194] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.8, 83.0, 0.0, 0.0, 26.0, 25.45412031112074, 0.444904922996437, 0.0, 1.0, 52317.42332909848], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 970800.0000, 
sim time next is 971400.0000, 
raw observation next is [8.8, 83.0, 0.0, 0.0, 26.0, 25.46278972816884, 0.4673564740853545, 0.0, 1.0, 38915.5321653835], 
processed observation next is [1.0, 0.21739130434782608, 0.7063711911357342, 0.83, 0.0, 0.0, 0.6666666666666666, 0.62189914401407, 0.6557854913617849, 0.0, 1.0, 0.18531205793039762], 
reward next is 0.8147, 
noisyNet noise sample is [array([0.38087863], dtype=float32), -0.118276335]. 
=============================================
[2019-04-04 03:47:25,041] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.3638059e-36 1.8724984e-30 6.2892175e-30 1.4632526e-21 1.2015172e-32
 1.0000000e+00 2.0704314e-29], sum to 1.0000
[2019-04-04 03:47:25,042] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6308
[2019-04-04 03:47:25,056] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 26.0, 25.59968897090263, 0.5796787741458368, 0.0, 1.0, 57096.8886723162], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1036200.0000, 
sim time next is 1036800.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 26.0, 25.60598770878951, 0.586025138214934, 0.0, 1.0, 34344.1564040495], 
processed observation next is [1.0, 0.0, 0.8614958448753465, 0.75, 0.0, 0.0, 0.6666666666666666, 0.6338323090657925, 0.6953417127383114, 0.0, 1.0, 0.16354360192404524], 
reward next is 0.8365, 
noisyNet noise sample is [array([0.0972185], dtype=float32), 0.49930182]. 
=============================================
[2019-04-04 03:47:27,621] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4022225e-30 4.9394549e-24 7.8183935e-25 3.3225098e-10 1.1910887e-26
 1.0000000e+00 1.5370860e-25], sum to 1.0000
[2019-04-04 03:47:27,626] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5244
[2019-04-04 03:47:27,678] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.5, 100.0, 83.66666666666667, 0.0, 26.0, 24.45579728330012, 0.4257430458360332, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1248600.0000, 
sim time next is 1249200.0000, 
raw observation next is [14.4, 100.0, 86.5, 0.0, 26.0, 24.70821638670726, 0.4493924796177445, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.8614958448753465, 1.0, 0.28833333333333333, 0.0, 0.6666666666666666, 0.559018032225605, 0.6497974932059148, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.63148713], dtype=float32), 1.5965306]. 
=============================================
[2019-04-04 03:47:28,661] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.2960387e-20 3.0010543e-17 8.9714731e-15 1.0000000e+00 4.0269894e-18
 1.9137019e-09 4.8972815e-15], sum to 1.0000
[2019-04-04 03:47:28,662] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2408
[2019-04-04 03:47:28,685] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [10.36666666666667, 78.33333333333334, 0.0, 0.0, 26.0, 25.63085176575805, 0.6302231325118076, 0.0, 1.0, 30854.63266496218], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1131600.0000, 
sim time next is 1132200.0000, 
raw observation next is [10.55, 78.0, 0.0, 0.0, 26.0, 25.63014228442714, 0.6302467576140868, 0.0, 1.0, 27568.14373571681], 
processed observation next is [0.0, 0.08695652173913043, 0.754847645429363, 0.78, 0.0, 0.0, 0.6666666666666666, 0.6358451903689284, 0.7100822525380289, 0.0, 1.0, 0.1312768749319848], 
reward next is 0.8687, 
noisyNet noise sample is [array([0.3187443], dtype=float32), 1.0519714]. 
=============================================
[2019-04-04 03:47:38,406] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.6583754e-15 1.0784686e-12 6.9061532e-12 5.8532512e-04 6.9391353e-16
 9.9941468e-01 2.4245282e-12], sum to 1.0000
[2019-04-04 03:47:38,407] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6784
[2019-04-04 03:47:38,448] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 26.0, 24.95386770837223, 0.4906591424323396, 1.0, 1.0, 25896.55886801855], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1364400.0000, 
sim time next is 1365000.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 26.0, 25.06380758072535, 0.505394982134583, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.4764542936288089, 0.96, 0.0, 0.0, 0.6666666666666666, 0.5886506317271124, 0.6684649940448609, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8110757], dtype=float32), 1.5754741]. 
=============================================
[2019-04-04 03:47:38,453] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[91.41651 ]
 [91.18938 ]
 [90.929405]
 [90.92455 ]
 [90.97754 ]], R is [[91.33744049]
 [91.30075073]
 [90.56209564]
 [90.10758209]
 [90.20650482]].
[2019-04-04 03:47:38,953] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.4299399e-21 1.1119808e-17 3.4834643e-16 2.6149779e-12 7.9634687e-20
 1.0000000e+00 7.8442371e-17], sum to 1.0000
[2019-04-04 03:47:38,956] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5020
[2019-04-04 03:47:38,993] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.28049036159027, 0.4602025799431431, 0.0, 1.0, 39513.64943432098], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1398000.0000, 
sim time next is 1398600.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.28973693248049, 0.459773092534047, 0.0, 1.0, 38996.77959939339], 
processed observation next is [1.0, 0.17391304347826086, 0.44598337950138506, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6074780777067076, 0.653257697511349, 0.0, 1.0, 0.18569895047330187], 
reward next is 0.8143, 
noisyNet noise sample is [array([0.27727118], dtype=float32), -1.1572546]. 
=============================================
[2019-04-04 03:47:43,208] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.5089854e-16 8.2985105e-14 7.3925837e-14 1.0000000e+00 2.3499913e-16
 3.2295230e-08 4.9456852e-13], sum to 1.0000
[2019-04-04 03:47:43,208] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4775
[2019-04-04 03:47:43,239] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 92.0, 75.0, 0.0, 26.0, 25.03971457024414, 0.4618662299291518, 1.0, 1.0, 18997.02291567632], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1434000.0000, 
sim time next is 1434600.0000, 
raw observation next is [1.1, 92.0, 72.0, 0.0, 26.0, 25.53251925728006, 0.4925125203291805, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.49307479224376743, 0.92, 0.24, 0.0, 0.6666666666666666, 0.6277099381066717, 0.6641708401097268, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.15089259], dtype=float32), 2.103782]. 
=============================================
[2019-04-04 03:47:43,520] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3269887e-32 1.3442512e-23 1.5459596e-25 2.1586098e-19 5.2359940e-28
 1.0000000e+00 2.6351255e-27], sum to 1.0000
[2019-04-04 03:47:43,521] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1504
[2019-04-04 03:47:43,526] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [15.0, 96.0, 0.0, 0.0, 26.0, 23.60919223731249, 0.1767331755678332, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1229400.0000, 
sim time next is 1230000.0000, 
raw observation next is [15.0, 96.0, 0.0, 0.0, 26.0, 23.6242748772564, 0.1736234934020317, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.21739130434782608, 0.8781163434903049, 0.96, 0.0, 0.0, 0.6666666666666666, 0.46868957310470005, 0.5578744978006772, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.27967152], dtype=float32), -0.16650665]. 
=============================================
[2019-04-04 03:47:43,539] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[91.350685]
 [91.353195]
 [91.376045]
 [91.44035 ]
 [91.5289  ]], R is [[91.43865204]
 [91.5242691 ]
 [91.60902405]
 [91.69293213]
 [91.77600098]].
[2019-04-04 03:47:48,032] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.6006842e-22 2.4920633e-19 6.8344199e-21 5.6859400e-02 9.1903596e-23
 9.4314057e-01 2.6245347e-18], sum to 1.0000
[2019-04-04 03:47:48,035] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0374
[2019-04-04 03:47:48,081] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 78.33333333333334, 153.3333333333333, 0.0, 26.0, 25.32274819047745, 0.3100801205413064, 1.0, 1.0, 35237.53073300966], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2033400.0000, 
sim time next is 2034000.0000, 
raw observation next is [-4.5, 79.0, 152.0, 0.0, 26.0, 25.36735259821008, 0.2169385576487105, 1.0, 1.0, 32062.40142469529], 
processed observation next is [1.0, 0.5652173913043478, 0.3379501385041552, 0.79, 0.5066666666666667, 0.0, 0.6666666666666666, 0.6139460498508399, 0.5723128525495702, 1.0, 1.0, 0.15267810202235851], 
reward next is 0.8473, 
noisyNet noise sample is [array([-2.095457], dtype=float32), -1.7396679]. 
=============================================
[2019-04-04 03:47:48,085] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[85.89664 ]
 [86.02966 ]
 [86.10655 ]
 [86.234955]
 [86.35124 ]], R is [[85.67977142]
 [85.65517426]
 [85.59222412]
 [85.48596191]
 [85.63110352]].
[2019-04-04 03:47:49,036] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.7892258e-18 5.5133362e-15 7.0258302e-16 9.9993265e-01 3.3359243e-18
 6.7297609e-05 7.8783813e-15], sum to 1.0000
[2019-04-04 03:47:49,039] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2040
[2019-04-04 03:47:49,088] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.899999999999999, 83.33333333333334, 0.0, 0.0, 26.0, 25.38469600742881, 0.2954634038143928, 1.0, 1.0, 18709.89583970519], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 841200.0000, 
sim time next is 841800.0000, 
raw observation next is [-3.9, 82.66666666666667, 0.0, 0.0, 26.0, 25.16627409742635, 0.2883926934942336, 1.0, 1.0, 54725.13007776145], 
processed observation next is [1.0, 0.7391304347826086, 0.3545706371191136, 0.8266666666666667, 0.0, 0.0, 0.6666666666666666, 0.5971895081188624, 0.5961308978314112, 1.0, 1.0, 0.2605958575131498], 
reward next is 0.7394, 
noisyNet noise sample is [array([-1.4715754], dtype=float32), -0.95131266]. 
=============================================
[2019-04-04 03:47:51,299] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.7289652e-24 2.0692860e-22 9.3372293e-21 5.4691391e-08 1.1495227e-23
 1.0000000e+00 3.2281053e-20], sum to 1.0000
[2019-04-04 03:47:51,299] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0681
[2019-04-04 03:47:51,343] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.3, 92.0, 66.0, 0.0, 26.0, 25.99060736059069, 0.5387423907502277, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1679400.0000, 
sim time next is 1680000.0000, 
raw observation next is [1.233333333333333, 92.0, 68.83333333333333, 0.0, 26.0, 25.90452393150894, 0.5359798538537296, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.49676823638042483, 0.92, 0.22944444444444442, 0.0, 0.6666666666666666, 0.658710327625745, 0.6786599512845766, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.32120064], dtype=float32), 0.90250456]. 
=============================================
[2019-04-04 03:47:51,390] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[90.67711 ]
 [90.935196]
 [91.16097 ]
 [91.43546 ]
 [91.69901 ]], R is [[90.72999573]
 [90.82269287]
 [90.91446686]
 [91.00532532]
 [91.09527588]].
[2019-04-04 03:48:00,919] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.4215492e-19 8.8861176e-15 3.5734938e-16 1.2430608e-08 2.4944210e-19
 1.0000000e+00 2.3960584e-15], sum to 1.0000
[2019-04-04 03:48:00,919] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8260
[2019-04-04 03:48:00,987] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 86.0, 0.0, 0.0, 26.0, 24.91251030903835, 0.3660791814740574, 0.0, 1.0, 152092.7076522096], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2059200.0000, 
sim time next is 2059800.0000, 
raw observation next is [-3.9, 86.0, 0.0, 0.0, 26.0, 24.98741099396297, 0.3865240525797913, 0.0, 1.0, 47402.63295559777], 
processed observation next is [1.0, 0.8695652173913043, 0.3545706371191136, 0.86, 0.0, 0.0, 0.6666666666666666, 0.5822842494969143, 0.6288413508599304, 0.0, 1.0, 0.22572682359808463], 
reward next is 0.7743, 
noisyNet noise sample is [array([-1.6242946], dtype=float32), 0.645756]. 
=============================================
[2019-04-04 03:48:01,156] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.1023834e-19 2.0006738e-15 8.5921997e-15 1.2795507e-06 6.6226256e-19
 9.9999869e-01 2.1376755e-14], sum to 1.0000
[2019-04-04 03:48:01,156] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1407
[2019-04-04 03:48:01,183] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.283333333333333, 73.16666666666667, 0.0, 0.0, 26.0, 25.44726234790279, 0.5364675105636664, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1541400.0000, 
sim time next is 1542000.0000, 
raw observation next is [7.366666666666667, 73.33333333333334, 0.0, 0.0, 26.0, 25.3670091981352, 0.5352956841125999, 0.0, 1.0, 196217.9094192961], 
processed observation next is [1.0, 0.8695652173913043, 0.6666666666666667, 0.7333333333333334, 0.0, 0.0, 0.6666666666666666, 0.6139174331779333, 0.6784318947042, 0.0, 1.0, 0.9343709972347434], 
reward next is 0.0656, 
noisyNet noise sample is [array([-0.03790873], dtype=float32), -0.39759848]. 
=============================================
[2019-04-04 03:48:01,198] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[84.207   ]
 [84.70624 ]
 [85.80488 ]
 [84.941414]
 [86.272736]], R is [[83.167099  ]
 [83.33542633]
 [83.5020752 ]
 [83.66705322]
 [83.8303833 ]].
[2019-04-04 03:48:26,589] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.1680628e-21 3.7179299e-16 1.3936150e-16 4.7802787e-15 4.0055345e-21
 1.0000000e+00 6.0085831e-17], sum to 1.0000
[2019-04-04 03:48:26,590] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6640
[2019-04-04 03:48:26,612] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 26.0, 25.53607681301812, 0.550544479462073, 0.0, 1.0, 18745.64789421979], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1719000.0000, 
sim time next is 1719600.0000, 
raw observation next is [0.5, 92.0, 0.0, 0.0, 26.0, 25.53544590306776, 0.544990667183933, 0.0, 1.0, 23606.61199765188], 
processed observation next is [1.0, 0.9130434782608695, 0.4764542936288089, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6279538252556467, 0.6816635557279777, 0.0, 1.0, 0.11241243808405657], 
reward next is 0.8876, 
noisyNet noise sample is [array([0.11572429], dtype=float32), 1.0341401]. 
=============================================
[2019-04-04 03:48:35,195] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.2627103e-24 1.3922320e-18 2.5853526e-18 6.3516392e-15 2.1240694e-23
 1.0000000e+00 3.1811769e-19], sum to 1.0000
[2019-04-04 03:48:35,195] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4600
[2019-04-04 03:48:35,244] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.600000000000001, 83.0, 0.0, 0.0, 26.0, 24.6437919072561, 0.2224099416924049, 0.0, 1.0, 42988.65739135416], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1984200.0000, 
sim time next is 1984800.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.59952680928343, 0.2137634939137457, 0.0, 1.0, 42957.58582212275], 
processed observation next is [1.0, 1.0, 0.30747922437673136, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5499605674402858, 0.5712544979712485, 0.0, 1.0, 0.20455993248629883], 
reward next is 0.7954, 
noisyNet noise sample is [array([1.7208773], dtype=float32), 1.0560143]. 
=============================================
[2019-04-04 03:48:38,652] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-04 03:48:38,669] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 03:48:38,669] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:48:38,671] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run45
[2019-04-04 03:48:38,702] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 03:48:38,703] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:48:38,707] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run45
[2019-04-04 03:48:38,742] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 03:48:38,743] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:48:38,770] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run45
[2019-04-04 03:48:53,560] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.2457823], dtype=float32), -0.060263935]
[2019-04-04 03:48:53,560] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-6.7, 61.0, 119.0, 85.66666666666667, 26.0, 25.63475850787987, 0.3942566122491837, 1.0, 1.0, 151420.2794362891]
[2019-04-04 03:48:53,560] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 03:48:53,561] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.5303207e-17 8.8794575e-16 9.3479297e-16 9.9999845e-01 2.6291791e-18
 1.5824963e-06 3.6505399e-14], sampled 0.6471391499665704
[2019-04-04 03:49:21,462] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.2457823], dtype=float32), -0.060263935]
[2019-04-04 03:49:21,463] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-2.466539002333334, 89.77378802000001, 0.0, 0.0, 26.0, 24.99429436905038, 0.2848703942373314, 0.0, 1.0, 33430.27258452796]
[2019-04-04 03:49:21,463] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 03:49:21,463] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [5.3777910e-17 1.0090223e-13 2.3412508e-13 4.2610001e-04 3.5189412e-16
 9.9957389e-01 5.3607195e-13], sampled 0.14235396355148144
[2019-04-04 03:50:32,863] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.2457823], dtype=float32), -0.060263935]
[2019-04-04 03:50:32,864] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-2.0, 57.5, 0.0, 0.0, 26.0, 25.33428399037673, 0.36570195858516, 0.0, 1.0, 39967.04511073107]
[2019-04-04 03:50:32,864] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 03:50:32,865] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.5242242e-18 2.4886949e-14 5.6979290e-14 3.6683592e-05 1.6592335e-17
 9.9996328e-01 8.2714211e-14], sampled 0.04138926937583476
[2019-04-04 03:51:45,267] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7354.4378 239728053.9031 1605.2028
[2019-04-04 03:52:18,724] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7242.1308 263312521.7875 1550.2376
[2019-04-04 03:52:20,374] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.2457823], dtype=float32), -0.060263935]
[2019-04-04 03:52:20,374] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [4.146968537999999, 20.91417312, 39.72140660666666, 943.0485310000001, 26.0, 25.99090955940741, 0.5072059043794515, 1.0, 1.0, 0.0]
[2019-04-04 03:52:20,374] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 03:52:20,375] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [7.4525772e-23 1.7375833e-20 9.1725425e-20 1.0000000e+00 1.6677555e-22
 1.3947786e-10 2.7346169e-18], sampled 0.6939155598226546
[2019-04-04 03:52:23,215] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.7525 275781969.2288 1232.5365
[2019-04-04 03:52:24,260] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 4400000, evaluation results [4400000.0, 7242.130848630938, 263312521.78750217, 1550.2375587224044, 7354.437838556871, 239728053.9030623, 1605.2027771046398, 7182.752527482079, 275781969.228767, 1232.5364548109517]
[2019-04-04 03:52:32,329] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.5529667e-23 4.3300142e-18 2.8771678e-19 2.6189571e-15 4.7329403e-22
 1.0000000e+00 9.7072714e-18], sum to 1.0000
[2019-04-04 03:52:32,329] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1266
[2019-04-04 03:52:32,348] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 90.16666666666667, 0.0, 0.0, 26.0, 24.75017293425389, 0.2614133266535565, 0.0, 1.0, 42669.78713620899], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2073000.0000, 
sim time next is 2073600.0000, 
raw observation next is [-4.5, 91.0, 0.0, 0.0, 26.0, 24.71239025328342, 0.2535371582643208, 0.0, 1.0, 42695.21366922004], 
processed observation next is [1.0, 0.0, 0.3379501385041552, 0.91, 0.0, 0.0, 0.6666666666666666, 0.559365854440285, 0.584512386088107, 0.0, 1.0, 0.20331054128200018], 
reward next is 0.7967, 
noisyNet noise sample is [array([-1.0164548], dtype=float32), -1.7355272]. 
=============================================
[2019-04-04 03:52:40,005] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.6563561e-20 3.6531548e-17 3.7460855e-16 1.5367343e-04 1.5928539e-19
 9.9984634e-01 5.7007790e-16], sum to 1.0000
[2019-04-04 03:52:40,005] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3276
[2019-04-04 03:52:40,054] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 92.0, 9.0, 0.0, 26.0, 25.89552033412973, 0.5070867892081885, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1443600.0000, 
sim time next is 1444200.0000, 
raw observation next is [1.1, 91.33333333333334, 5.999999999999998, 0.0, 26.0, 25.93186006148606, 0.5074960167668637, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.49307479224376743, 0.9133333333333334, 0.019999999999999993, 0.0, 0.6666666666666666, 0.6609883384571716, 0.6691653389222879, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1833915], dtype=float32), 1.2828391]. 
=============================================
[2019-04-04 03:52:53,305] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.9868995e-24 9.6408296e-21 1.2216760e-19 8.4491127e-08 1.5035450e-22
 9.9999988e-01 2.5054830e-19], sum to 1.0000
[2019-04-04 03:52:53,305] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7316
[2019-04-04 03:52:53,348] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 73.16666666666667, 0.0, 0.0, 26.0, 25.09412597291695, 0.321693050279125, 0.0, 1.0, 50942.48387652777], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2855400.0000, 
sim time next is 2856000.0000, 
raw observation next is [1.0, 74.33333333333334, 0.0, 0.0, 26.0, 25.11354218885459, 0.3227562508411222, 0.0, 1.0, 50129.2564344319], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 0.7433333333333334, 0.0, 0.0, 0.6666666666666666, 0.5927951824045493, 0.6075854169470407, 0.0, 1.0, 0.23871074492586622], 
reward next is 0.7613, 
noisyNet noise sample is [array([0.11190303], dtype=float32), -0.43186206]. 
=============================================
[2019-04-04 03:52:53,379] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[82.45153 ]
 [82.42866 ]
 [82.41355 ]
 [82.309   ]
 [82.247246]], R is [[82.4017868 ]
 [82.33518219]
 [82.26500702]
 [82.19087982]
 [82.11251068]].
[2019-04-04 03:52:56,132] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.2574681e-18 8.1396997e-16 3.3778295e-15 9.9151200e-01 2.8380636e-17
 8.4880125e-03 2.4335778e-14], sum to 1.0000
[2019-04-04 03:52:56,132] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4326
[2019-04-04 03:52:56,151] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 92.0, 122.3333333333333, 0.0, 26.0, 26.05666760164877, 0.5788954289003087, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1338600.0000, 
sim time next is 1339200.0000, 
raw observation next is [1.1, 92.0, 120.0, 0.0, 26.0, 26.04681732337875, 0.5753867487217641, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.49307479224376743, 0.92, 0.4, 0.0, 0.6666666666666666, 0.6705681102815625, 0.6917955829072547, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.36852086], dtype=float32), -0.3943746]. 
=============================================
[2019-04-04 03:53:01,584] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.9280474e-20 5.9293650e-18 2.6921412e-16 1.0000000e+00 1.1819139e-19
 1.3234060e-11 5.2649678e-16], sum to 1.0000
[2019-04-04 03:53:01,585] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9726
[2019-04-04 03:53:01,629] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.283333333333333, 54.33333333333333, 0.0, 0.0, 26.0, 25.11539060987244, 0.3254765813228611, 1.0, 1.0, 64453.83172620123], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2315400.0000, 
sim time next is 2316000.0000, 
raw observation next is [-1.366666666666667, 54.66666666666667, 0.0, 0.0, 26.0, 25.03336723296891, 0.3185207798936089, 0.0, 1.0, 94714.1813368877], 
processed observation next is [1.0, 0.8260869565217391, 0.42474607571560485, 0.5466666666666667, 0.0, 0.0, 0.6666666666666666, 0.5861139360807425, 0.6061735932978697, 0.0, 1.0, 0.4510199111280367], 
reward next is 0.5490, 
noisyNet noise sample is [array([1.124254], dtype=float32), -0.5749323]. 
=============================================
[2019-04-04 03:53:01,634] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[85.04248 ]
 [84.86789 ]
 [84.741005]
 [84.59542 ]
 [84.71706 ]], R is [[83.01586914]
 [82.87879181]
 [82.96098328]
 [83.13137054]
 [83.30005646]].
[2019-04-04 03:53:02,497] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.5707561e-16 3.5089981e-15 3.3437066e-12 5.7156817e-03 1.7217023e-15
 9.9428433e-01 4.0725032e-12], sum to 1.0000
[2019-04-04 03:53:02,497] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9696
[2019-04-04 03:53:02,517] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.3, 46.5, 41.0, 0.0, 26.0, 25.88780540543794, 0.39474990007465, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2305800.0000, 
sim time next is 2306400.0000, 
raw observation next is [-0.4, 47.33333333333333, 35.00000000000001, 0.0, 26.0, 25.81884490325527, 0.3866009708185445, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.45152354570637127, 0.4733333333333333, 0.1166666666666667, 0.0, 0.6666666666666666, 0.6515704086046057, 0.6288669902728482, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5713114], dtype=float32), -0.7510145]. 
=============================================
[2019-04-04 03:53:02,709] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.2679095e-16 1.0630350e-12 3.7094744e-11 4.6509990e-01 6.8801094e-15
 5.3490007e-01 6.8469206e-12], sum to 1.0000
[2019-04-04 03:53:02,709] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3658
[2019-04-04 03:53:02,784] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.7, 85.66666666666667, 21.83333333333334, 0.0, 26.0, 24.66067206267529, 0.3363119083235453, 0.0, 1.0, 100461.9855521861], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1758000.0000, 
sim time next is 1758600.0000, 
raw observation next is [-1.7, 85.0, 26.0, 0.0, 26.0, 24.88589482844854, 0.3680606242035537, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.4155124653739613, 0.85, 0.08666666666666667, 0.0, 0.6666666666666666, 0.5738245690373782, 0.6226868747345179, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0619206], dtype=float32), 0.28417817]. 
=============================================
[2019-04-04 03:53:04,638] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.4441339e-14 4.6753185e-11 1.7239811e-10 9.6035230e-01 3.9937309e-14
 3.9647739e-02 9.2969399e-10], sum to 1.0000
[2019-04-04 03:53:04,638] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9274
[2019-04-04 03:53:04,665] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 92.0, 0.0, 0.0, 26.0, 25.24384870798007, 0.4377551887288617, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1452600.0000, 
sim time next is 1453200.0000, 
raw observation next is [1.1, 92.0, 0.0, 0.0, 26.0, 25.09290197896279, 0.4152915922898678, 0.0, 1.0, 24472.0805112241], 
processed observation next is [1.0, 0.8260869565217391, 0.49307479224376743, 0.92, 0.0, 0.0, 0.6666666666666666, 0.5910751649135658, 0.6384305307632893, 0.0, 1.0, 0.11653371672011476], 
reward next is 0.8835, 
noisyNet noise sample is [array([-1.4731711], dtype=float32), -0.75785184]. 
=============================================
[2019-04-04 03:53:08,527] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.5180881e-19 3.5489287e-18 4.1919316e-17 1.0000000e+00 6.5437560e-19
 1.4162811e-10 6.4957406e-16], sum to 1.0000
[2019-04-04 03:53:08,527] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7492
[2019-04-04 03:53:08,562] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.08571785054419, 0.07081958363273011, 0.0, 1.0, 40041.48621159692], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3038400.0000, 
sim time next is 3039000.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.0541787586082, 0.06391463659749952, 0.0, 1.0, 40108.53122067876], 
processed observation next is [0.0, 0.17391304347826086, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5045148965506833, 0.5213048788658332, 0.0, 1.0, 0.190993005812756], 
reward next is 0.8090, 
noisyNet noise sample is [array([1.848489], dtype=float32), 1.9430187]. 
=============================================
[2019-04-04 03:53:08,570] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[79.20359]
 [79.17386]
 [79.12426]
 [79.05847]
 [78.99947]], R is [[79.24559784]
 [79.26246643]
 [79.2795639 ]
 [79.29697418]
 [79.3147583 ]].
[2019-04-04 03:53:17,336] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.3602337e-20 3.4556667e-18 2.1066631e-16 1.0000000e+00 2.2509440e-19
 3.1538563e-12 5.2195536e-16], sum to 1.0000
[2019-04-04 03:53:17,338] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3144
[2019-04-04 03:53:17,364] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.7, 46.5, 0.0, 0.0, 26.0, 25.0143294133391, 0.1893714480756988, 0.0, 1.0, 38456.84521207326], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2518200.0000, 
sim time next is 2518800.0000, 
raw observation next is [-1.7, 47.33333333333333, 0.0, 0.0, 26.0, 25.03362828310687, 0.182684928874159, 0.0, 1.0, 38426.04252638454], 
processed observation next is [1.0, 0.13043478260869565, 0.4155124653739613, 0.4733333333333333, 0.0, 0.0, 0.6666666666666666, 0.586135690258906, 0.5608949762913863, 0.0, 1.0, 0.18298115488754543], 
reward next is 0.8170, 
noisyNet noise sample is [array([-0.9075235], dtype=float32), 0.8214581]. 
=============================================
[2019-04-04 03:53:24,218] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.5929340e-18 7.6392995e-15 2.3536547e-13 9.9999976e-01 1.2607698e-17
 1.9064079e-07 3.1533185e-13], sum to 1.0000
[2019-04-04 03:53:24,224] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9605
[2019-04-04 03:53:24,260] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.666666666666667, 70.0, 0.0, 0.0, 26.0, 24.86472334698608, 0.2975877900898621, 0.0, 1.0, 44449.01875591761], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2676000.0000, 
sim time next is 2676600.0000, 
raw observation next is [-6.0, 70.5, 0.0, 0.0, 26.0, 24.80768660801415, 0.2936082897752707, 0.0, 1.0, 44428.80138444198], 
processed observation next is [1.0, 1.0, 0.296398891966759, 0.705, 0.0, 0.0, 0.6666666666666666, 0.5673072173345126, 0.5978694299250903, 0.0, 1.0, 0.21156572087829512], 
reward next is 0.7884, 
noisyNet noise sample is [array([0.4285203], dtype=float32), 1.8311752]. 
=============================================
[2019-04-04 03:53:25,843] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.2290913e-28 3.1741590e-26 5.0993197e-25 1.0000000e+00 4.4279089e-28
 2.0580094e-18 9.7065753e-24], sum to 1.0000
[2019-04-04 03:53:25,843] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9451
[2019-04-04 03:53:25,859] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 42.0, 0.0, 0.0, 26.0, 25.10599205978757, 0.2782895432309098, 0.0, 1.0, 43114.71095625594], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2408400.0000, 
sim time next is 2409000.0000, 
raw observation next is [-3.583333333333333, 42.33333333333334, 0.0, 0.0, 26.0, 25.15300847384592, 0.2736349960978976, 0.0, 1.0, 43056.96233135147], 
processed observation next is [0.0, 0.9130434782608695, 0.36334256694367506, 0.42333333333333345, 0.0, 0.0, 0.6666666666666666, 0.5960840394871599, 0.5912116653659659, 0.0, 1.0, 0.20503315395881652], 
reward next is 0.7950, 
noisyNet noise sample is [array([0.78652054], dtype=float32), -0.33447102]. 
=============================================
[2019-04-04 03:53:25,864] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[81.6541 ]
 [81.62565]
 [81.5915 ]
 [81.4723 ]
 [81.44773]], R is [[81.57027435]
 [81.549263  ]
 [81.52827454]
 [81.50748444]
 [81.48710632]].
[2019-04-04 03:53:26,023] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0824383e-15 8.7370606e-12 3.0296713e-11 9.9829274e-01 2.7444886e-14
 1.7072244e-03 6.7280459e-11], sum to 1.0000
[2019-04-04 03:53:26,025] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2405
[2019-04-04 03:53:26,045] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 100.0, 0.0, 0.0, 26.0, 25.31680520740063, 0.3255578944430254, 0.0, 1.0, 44010.18389253215], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3108600.0000, 
sim time next is 3109200.0000, 
raw observation next is [0.0, 100.0, 0.0, 0.0, 26.0, 25.31103345649466, 0.3247016449678353, 0.0, 1.0, 41217.2418309989], 
processed observation next is [0.0, 1.0, 0.46260387811634357, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6092527880412216, 0.608233881655945, 0.0, 1.0, 0.1962725801476138], 
reward next is 0.8037, 
noisyNet noise sample is [array([0.04290841], dtype=float32), 0.359377]. 
=============================================
[2019-04-04 03:53:27,961] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.4000884e-17 2.6805845e-14 4.7984942e-15 1.0000000e+00 1.8622600e-17
 1.0826500e-09 2.6171022e-13], sum to 1.0000
[2019-04-04 03:53:27,961] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2687
[2019-04-04 03:53:27,979] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.916666666666667, 85.5, 0.0, 0.0, 26.0, 25.00650472231155, 0.3077422212104241, 0.0, 1.0, 46047.34030994861], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1803000.0000, 
sim time next is 1803600.0000, 
raw observation next is [-5.0, 86.0, 0.0, 0.0, 26.0, 25.03804251163737, 0.310326638414648, 0.0, 1.0, 46024.84901350315], 
processed observation next is [0.0, 0.9130434782608695, 0.32409972299168976, 0.86, 0.0, 0.0, 0.6666666666666666, 0.5865035426364477, 0.6034422128048827, 0.0, 1.0, 0.21916594768334835], 
reward next is 0.7808, 
noisyNet noise sample is [array([0.5219178], dtype=float32), 1.0357187]. 
=============================================
[2019-04-04 03:53:50,021] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.1016910e-26 1.8876099e-22 6.5869164e-24 1.0000000e+00 7.2158976e-25
 9.5259451e-18 5.8996390e-22], sum to 1.0000
[2019-04-04 03:53:50,025] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0811
[2019-04-04 03:53:50,060] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 65.0, 193.5, 623.1666666666667, 26.0, 25.0490225581593, 0.3907033449366815, 0.0, 1.0, 20159.24820540107], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2982000.0000, 
sim time next is 2982600.0000, 
raw observation next is [-3.0, 65.0, 181.0, 691.0, 26.0, 25.0550715801318, 0.3960615112740422, 0.0, 1.0, 24242.4984992661], 
processed observation next is [0.0, 0.5217391304347826, 0.3795013850415513, 0.65, 0.6033333333333334, 0.7635359116022099, 0.6666666666666666, 0.5879226316776499, 0.6320205037580141, 0.0, 1.0, 0.11544046904412429], 
reward next is 0.8846, 
noisyNet noise sample is [array([-0.413234], dtype=float32), -0.8485956]. 
=============================================
[2019-04-04 03:53:50,940] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.5592680e-17 4.2472688e-14 4.6882955e-14 9.9999988e-01 6.2606058e-17
 7.5384406e-08 1.9583185e-12], sum to 1.0000
[2019-04-04 03:53:50,949] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0369
[2019-04-04 03:53:51,005] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.333333333333333, 56.66666666666667, 78.5, 634.8333333333334, 26.0, 25.12885458858642, 0.4141500402786258, 0.0, 1.0, 18708.55405376733], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2994000.0000, 
sim time next is 2994600.0000, 
raw observation next is [-1.166666666666667, 55.83333333333334, 74.0, 602.6666666666667, 26.0, 25.13887195262248, 0.4120345372428783, 0.0, 1.0, 18706.83410841414], 
processed observation next is [0.0, 0.6521739130434783, 0.43028624192059095, 0.5583333333333335, 0.24666666666666667, 0.6659300184162064, 0.6666666666666666, 0.5949059960518733, 0.6373448457476262, 0.0, 1.0, 0.08908016242101971], 
reward next is 0.9109, 
noisyNet noise sample is [array([-1.5745835], dtype=float32), -0.16238075]. 
=============================================
[2019-04-04 03:53:56,448] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.0280276e-23 2.2847402e-20 2.0198029e-19 9.9999905e-01 1.7524166e-23
 9.5099233e-07 1.7054323e-18], sum to 1.0000
[2019-04-04 03:53:56,451] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5092
[2019-04-04 03:53:56,467] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.11467903968505, 0.0756312941863193, 0.0, 1.0, 39961.71530739096], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3037800.0000, 
sim time next is 3038400.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.08085984751475, 0.06856956801480575, 0.0, 1.0, 40044.96744564841], 
processed observation next is [0.0, 0.17391304347826086, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.506738320626229, 0.5228565226716019, 0.0, 1.0, 0.19069032116975435], 
reward next is 0.8093, 
noisyNet noise sample is [array([-0.10646921], dtype=float32), -1.1686505]. 
=============================================
[2019-04-04 03:53:57,764] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1881163e-18 3.0123543e-15 6.1139753e-15 9.6801364e-01 1.8010159e-17
 3.1986348e-02 9.6530429e-14], sum to 1.0000
[2019-04-04 03:53:57,765] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8145
[2019-04-04 03:53:57,833] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-11.0, 82.66666666666666, 29.99999999999999, 194.6666666666666, 26.0, 25.31658686199037, 0.39943435327621, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3311400.0000, 
sim time next is 3312000.0000, 
raw observation next is [-11.0, 84.0, 44.0, 245.0, 26.0, 25.5124519649332, 0.4123911692517576, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.15789473684210528, 0.84, 0.14666666666666667, 0.27071823204419887, 0.6666666666666666, 0.6260376637444333, 0.6374637230839192, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6891921], dtype=float32), -1.5403996]. 
=============================================
[2019-04-04 03:53:57,836] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[84.61471 ]
 [84.10735 ]
 [82.55733 ]
 [79.661064]
 [77.06004 ]], R is [[84.80207825]
 [84.95405579]
 [84.70075226]
 [83.88642883]
 [83.08387756]].
[2019-04-04 03:54:01,889] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.9900204e-23 2.0465801e-18 6.7709489e-20 9.9341136e-01 5.0040388e-22
 6.5886732e-03 1.5184139e-18], sum to 1.0000
[2019-04-04 03:54:01,891] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8923
[2019-04-04 03:54:01,900] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.0, 45.66666666666667, 101.1666666666667, 764.1666666666667, 26.0, 25.49319682929508, 0.484792161622449, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3681600.0000, 
sim time next is 3682200.0000, 
raw observation next is [6.0, 46.33333333333334, 98.33333333333334, 752.3333333333333, 26.0, 25.48953403435509, 0.4845373199049058, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.6288088642659281, 0.46333333333333343, 0.32777777777777783, 0.8313075506445672, 0.6666666666666666, 0.6241278361962573, 0.6615124399683019, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2186342], dtype=float32), 0.68320566]. 
=============================================
[2019-04-04 03:54:10,525] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3013068e-19 1.9713436e-15 2.4897979e-15 9.8970467e-01 1.6385123e-18
 1.0295393e-02 3.2264552e-14], sum to 1.0000
[2019-04-04 03:54:10,525] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5300
[2019-04-04 03:54:10,597] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 61.5, 83.0, 359.0, 26.0, 24.05640997708496, 0.2164543046327171, 0.0, 1.0, 201797.8836830581], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3054600.0000, 
sim time next is 3055200.0000, 
raw observation next is [-6.0, 60.66666666666666, 85.66666666666667, 405.0, 26.0, 24.93360012981169, 0.2893098527180664, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.296398891966759, 0.6066666666666666, 0.28555555555555556, 0.44751381215469616, 0.6666666666666666, 0.5778000108176409, 0.5964366175726888, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.06879622], dtype=float32), 0.47727263]. 
=============================================
[2019-04-04 03:54:22,337] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.6460503e-24 7.6387201e-22 1.9391836e-20 1.0000000e+00 3.4811919e-24
 2.1279097e-19 9.7476390e-19], sum to 1.0000
[2019-04-04 03:54:22,338] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0535
[2019-04-04 03:54:22,361] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.6666666666666667, 65.33333333333334, 0.0, 0.0, 26.0, 25.18077808861741, 0.4489430087110517, 0.0, 1.0, 60038.62166512842], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3526800.0000, 
sim time next is 3527400.0000, 
raw observation next is [0.3333333333333333, 68.66666666666666, 0.0, 0.0, 26.0, 25.10038322420455, 0.4461174495915665, 0.0, 1.0, 91031.76577037812], 
processed observation next is [1.0, 0.8260869565217391, 0.4718374884579871, 0.6866666666666665, 0.0, 0.0, 0.6666666666666666, 0.5916986020170457, 0.6487058165305222, 0.0, 1.0, 0.43348459890656243], 
reward next is 0.5665, 
noisyNet noise sample is [array([-0.2568566], dtype=float32), 0.0056003835]. 
=============================================
[2019-04-04 03:54:25,388] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.1722001e-19 9.9335132e-16 4.2207427e-15 9.9647480e-01 8.4519919e-19
 3.5252119e-03 8.0167764e-15], sum to 1.0000
[2019-04-04 03:54:25,389] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1665
[2019-04-04 03:54:25,465] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.3333333333333333, 71.66666666666667, 0.0, 0.0, 26.0, 25.15239766469909, 0.3851674236127265, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3482400.0000, 
sim time next is 3483000.0000, 
raw observation next is [-0.5, 71.5, 3.0, 107.0, 26.0, 25.37711307946408, 0.4032371430980017, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.44875346260387816, 0.715, 0.01, 0.11823204419889503, 0.6666666666666666, 0.6147594232886734, 0.6344123810326673, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.12469253], dtype=float32), -0.79837567]. 
=============================================
[2019-04-04 03:54:25,491] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[82.64731 ]
 [82.826096]
 [82.86107 ]
 [82.84535 ]
 [82.839874]], R is [[85.37611389]
 [85.52235413]
 [85.40415192]
 [85.30375671]
 [85.23245239]].
[2019-04-04 03:54:31,904] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.49290625e-21 3.65978750e-17 4.77004381e-16 9.81325269e-01
 1.06421717e-19 1.86747629e-02 3.89456273e-16], sum to 1.0000
[2019-04-04 03:54:31,909] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9392
[2019-04-04 03:54:31,946] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.333333333333333, 61.66666666666667, 0.0, 0.0, 26.0, 25.00539512161007, 0.3987273336621442, 0.0, 1.0, 138730.3032649172], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3699600.0000, 
sim time next is 3700200.0000, 
raw observation next is [3.166666666666667, 62.33333333333333, 0.0, 0.0, 26.0, 25.15026802145437, 0.4327358897219413, 0.0, 1.0, 75787.22723429666], 
processed observation next is [0.0, 0.8260869565217391, 0.5503231763619576, 0.6233333333333333, 0.0, 0.0, 0.6666666666666666, 0.5958556684545308, 0.6442452965739804, 0.0, 1.0, 0.3608915582585555], 
reward next is 0.6391, 
noisyNet noise sample is [array([-0.7963818], dtype=float32), -0.6051691]. 
=============================================
[2019-04-04 03:54:48,161] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.2979963e-25 6.0512801e-23 3.8438832e-19 1.0000000e+00 4.8864332e-25
 9.3622912e-16 7.5985050e-20], sum to 1.0000
[2019-04-04 03:54:48,162] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6172
[2019-04-04 03:54:48,177] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.333333333333333, 41.33333333333334, 0.0, 0.0, 26.0, 25.36549277572998, 0.4120385252535532, 0.0, 1.0, 56731.93564989294], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4152000.0000, 
sim time next is 4152600.0000, 
raw observation next is [-1.5, 42.5, 0.0, 0.0, 26.0, 25.33984483385822, 0.4120820719142895, 0.0, 1.0, 46921.31800494932], 
processed observation next is [0.0, 0.043478260869565216, 0.4210526315789474, 0.425, 0.0, 0.0, 0.6666666666666666, 0.6116537361548516, 0.6373606906380965, 0.0, 1.0, 0.22343484764261579], 
reward next is 0.7766, 
noisyNet noise sample is [array([0.85301095], dtype=float32), 1.0404534]. 
=============================================
[2019-04-04 03:54:49,638] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.3892348e-20 2.3532836e-16 1.0216068e-13 1.0000000e+00 1.4130888e-18
 2.8246019e-09 1.3532742e-14], sum to 1.0000
[2019-04-04 03:54:49,640] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9220
[2019-04-04 03:54:49,659] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.5, 96.5, 114.0, 805.0, 26.0, 27.11452224805852, 0.7803535506509837, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3151800.0000, 
sim time next is 3152400.0000, 
raw observation next is [7.666666666666666, 95.33333333333334, 113.8333333333333, 808.0, 26.0, 27.15605515163891, 0.7880820397776791, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.674976915974146, 0.9533333333333335, 0.3794444444444443, 0.8928176795580111, 0.6666666666666666, 0.7630045959699091, 0.7626940132592264, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6914613], dtype=float32), 0.54770225]. 
=============================================
[2019-04-04 03:54:49,772] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.6021273e-30 9.9857965e-28 3.4737182e-27 1.0000000e+00 5.4022794e-30
 2.2776960e-18 3.4152905e-25], sum to 1.0000
[2019-04-04 03:54:49,774] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7593
[2019-04-04 03:54:49,815] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.666666666666666, 36.66666666666666, 58.16666666666666, 474.8333333333334, 26.0, 26.19867559685581, 0.6575827933898345, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3948000.0000, 
sim time next is 3948600.0000, 
raw observation next is [-4.833333333333334, 37.33333333333334, 50.33333333333334, 413.6666666666667, 26.0, 26.44143415424527, 0.514289464418126, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.32871652816251157, 0.3733333333333334, 0.1677777777777778, 0.45709023941068144, 0.6666666666666666, 0.703452846187106, 0.6714298214727087, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.56104213], dtype=float32), -0.92317814]. 
=============================================
[2019-04-04 03:54:53,160] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.7563505e-23 1.5867408e-21 2.0300088e-19 1.0000000e+00 4.1860226e-23
 8.3163554e-17 3.9377883e-18], sum to 1.0000
[2019-04-04 03:54:53,160] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0945
[2019-04-04 03:54:53,180] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.333333333333333, 27.66666666666667, 0.0, 0.0, 26.0, 25.70641742466836, 0.4975186943080421, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4040400.0000, 
sim time next is 4041000.0000, 
raw observation next is [-3.5, 28.5, 0.0, 0.0, 26.0, 25.62207550166007, 0.4876763026279716, 1.0, 1.0, 91837.84590454983], 
processed observation next is [1.0, 0.782608695652174, 0.36565096952908593, 0.285, 0.0, 0.0, 0.6666666666666666, 0.6351729584716725, 0.6625587675426572, 1.0, 1.0, 0.4373230757359516], 
reward next is 0.5627, 
noisyNet noise sample is [array([-1.0361532], dtype=float32), -1.5625306]. 
=============================================
[2019-04-04 03:54:53,221] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[81.513176]
 [80.87221 ]
 [79.90954 ]
 [81.258354]
 [81.23111 ]], R is [[81.870224  ]
 [82.0515213 ]
 [81.55460358]
 [81.73905945]
 [81.92166901]].
[2019-04-04 03:54:56,314] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.2606151e-20 9.4375828e-19 3.5176494e-17 5.2688033e-03 2.6013483e-20
 9.9473119e-01 9.7245537e-17], sum to 1.0000
[2019-04-04 03:54:56,314] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2782
[2019-04-04 03:54:56,339] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 100.0, 63.5, 0.0, 26.0, 25.43286834437208, 0.3042497164268618, 1.0, 1.0, 18680.83657379219], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2887200.0000, 
sim time next is 2887800.0000, 
raw observation next is [0.1666666666666667, 98.83333333333334, 68.33333333333334, 0.0, 26.0, 25.41587557235408, 0.3057605434850039, 1.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.43478260869565216, 0.4672206832871654, 0.9883333333333334, 0.2277777777777778, 0.0, 0.6666666666666666, 0.6179896310295065, 0.601920181161668, 1.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.35188958], dtype=float32), -1.1473976]. 
=============================================
[2019-04-04 03:55:03,282] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.2786426e-28 1.7319333e-26 8.9281460e-23 1.0000000e+00 4.2611893e-27
 2.0622983e-20 1.1341823e-22], sum to 1.0000
[2019-04-04 03:55:03,282] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2526
[2019-04-04 03:55:03,293] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 34.5, 15.33333333333334, 38.00000000000001, 26.0, 26.24206067888169, 0.6468985625249907, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4125000.0000, 
sim time next is 4125600.0000, 
raw observation next is [3.0, 34.0, 0.0, 0.0, 26.0, 26.46013723129881, 0.6508869864978709, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.5457063711911359, 0.34, 0.0, 0.0, 0.6666666666666666, 0.7050114359415675, 0.7169623288326236, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6330995], dtype=float32), -1.8389972]. 
=============================================
[2019-04-04 03:55:12,290] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.6339170e-28 1.6808654e-25 4.0593047e-24 1.0000000e+00 3.6223164e-29
 2.6028017e-23 3.3232230e-23], sum to 1.0000
[2019-04-04 03:55:12,297] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9185
[2019-04-04 03:55:12,316] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.333333333333333, 22.0, 101.5, 780.3333333333334, 26.0, 25.90268909109106, 0.6466600133062629, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4027200.0000, 
sim time next is 4027800.0000, 
raw observation next is [-2.166666666666667, 21.0, 99.0, 766.6666666666666, 26.0, 26.42605059431851, 0.6852504672927716, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.4025854108956602, 0.21, 0.33, 0.8471454880294659, 0.6666666666666666, 0.7021708828598759, 0.7284168224309239, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.23899889], dtype=float32), 0.85677993]. 
=============================================
[2019-04-04 03:55:13,146] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:55:13,146] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:55:13,160] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run34
[2019-04-04 03:55:13,585] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.93475234e-21 1.27109925e-17 4.31952260e-17 4.18944410e-06
 2.27616504e-19 9.99995828e-01 3.20085175e-16], sum to 1.0000
[2019-04-04 03:55:13,586] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3063
[2019-04-04 03:55:13,607] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.766666666666666, 47.0, 108.3333333333333, 694.1666666666667, 26.0, 26.92444351781131, 0.7045547105514505, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4354800.0000, 
sim time next is 4355400.0000, 
raw observation next is [9.383333333333333, 44.5, 109.6666666666667, 711.3333333333333, 26.0, 27.10415291140507, 0.7352373215018556, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.7225300092336104, 0.445, 0.3655555555555557, 0.7860036832412522, 0.6666666666666666, 0.7586794092837558, 0.7450791071672852, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.07815909], dtype=float32), -0.24690658]. 
=============================================
[2019-04-04 03:55:17,166] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.7341596e-19 3.5651875e-15 1.4760389e-14 9.9996877e-01 8.0113042e-19
 3.1240143e-05 3.6225902e-14], sum to 1.0000
[2019-04-04 03:55:17,170] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9418
[2019-04-04 03:55:17,224] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 100.0, 0.0, 0.0, 26.0, 25.2980722900242, 0.5072807477552072, 0.0, 1.0, 40844.39121262628], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3214800.0000, 
sim time next is 3215400.0000, 
raw observation next is [-2.166666666666667, 100.0, 0.0, 0.0, 26.0, 25.35652621716067, 0.504916201250503, 0.0, 1.0, 40667.46023398238], 
processed observation next is [1.0, 0.21739130434782608, 0.4025854108956602, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6130438514300559, 0.6683054004168344, 0.0, 1.0, 0.19365457254277324], 
reward next is 0.8063, 
noisyNet noise sample is [array([1.3334792], dtype=float32), 0.37089738]. 
=============================================
[2019-04-04 03:55:21,000] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.0069998e-14 9.9207323e-11 1.9397843e-08 3.9153406e-01 1.7881542e-13
 6.0846591e-01 5.1997504e-09], sum to 1.0000
[2019-04-04 03:55:21,000] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8107
[2019-04-04 03:55:21,016] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6, 73.0, 0.0, 0.0, 26.0, 25.27527305852589, 0.4403310224695371, 0.0, 1.0, 48149.417672984], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4500000.0000, 
sim time next is 4500600.0000, 
raw observation next is [-0.6666666666666666, 73.0, 0.0, 0.0, 26.0, 25.28618141873695, 0.4469914092395461, 0.0, 1.0, 44359.03622788391], 
processed observation next is [1.0, 0.08695652173913043, 0.44413665743305636, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6071817848947457, 0.648997136413182, 0.0, 1.0, 0.21123350584706624], 
reward next is 0.7888, 
noisyNet noise sample is [array([0.5761243], dtype=float32), -0.26719102]. 
=============================================
[2019-04-04 03:55:23,174] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.6254117e-24 1.3693559e-19 3.0471367e-19 1.4618518e-14 2.6225568e-22
 1.0000000e+00 6.6981446e-19], sum to 1.0000
[2019-04-04 03:55:23,174] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5772
[2019-04-04 03:55:23,232] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.616666666666667, 86.0, 81.66666666666666, 0.0, 26.0, 24.41921369463691, 0.1526621107405409, 0.0, 1.0, 30920.45673008532], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 51000.0000, 
sim time next is 51600.0000, 
raw observation next is [7.533333333333333, 86.0, 80.33333333333334, 0.0, 26.0, 24.44942984439163, 0.1552548605871608, 0.0, 1.0, 20309.86587047094], 
processed observation next is [0.0, 0.6086956521739131, 0.6712834718374886, 0.86, 0.26777777777777784, 0.0, 0.6666666666666666, 0.5374524870326359, 0.5517516201957203, 0.0, 1.0, 0.09671364700224258], 
reward next is 0.9033, 
noisyNet noise sample is [array([-0.76403636], dtype=float32), 0.32955053]. 
=============================================
[2019-04-04 03:55:23,799] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:55:23,799] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:55:23,810] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run34
[2019-04-04 03:55:30,079] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:55:30,080] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:55:30,099] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run34
[2019-04-04 03:55:32,962] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.9775609e-27 1.0974735e-21 2.1416958e-20 8.1767371e-12 2.3086192e-25
 1.0000000e+00 3.4559117e-21], sum to 1.0000
[2019-04-04 03:55:32,963] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8105
[2019-04-04 03:55:32,975] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 53.66666666666667, 0.0, 0.0, 26.0, 25.63640796383719, 0.5282028284059166, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4664400.0000, 
sim time next is 4665000.0000, 
raw observation next is [2.0, 52.83333333333334, 0.0, 0.0, 26.0, 25.60462691788849, 0.5156093517616406, 0.0, 1.0, 18734.97040001351], 
processed observation next is [1.0, 1.0, 0.518005540166205, 0.5283333333333334, 0.0, 0.0, 0.6666666666666666, 0.6337189098240407, 0.6718697839205469, 0.0, 1.0, 0.0892141447619691], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.11853895], dtype=float32), -0.05907354]. 
=============================================
[2019-04-04 03:55:32,989] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[87.15585 ]
 [87.194954]
 [87.341606]
 [87.41993 ]
 [87.436264]], R is [[87.19507599]
 [87.32312775]
 [87.44989777]
 [87.57540131]
 [87.59088135]].
[2019-04-04 03:55:34,486] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.3486177e-28 4.8283099e-24 1.6569806e-23 5.5983700e-18 1.0758444e-28
 1.0000000e+00 2.0573369e-23], sum to 1.0000
[2019-04-04 03:55:34,490] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1944
[2019-04-04 03:55:34,526] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.666666666666667, 77.33333333333334, 145.1666666666667, 0.9999999999999998, 26.0, 25.0970641651048, 0.5008163934973998, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4714800.0000, 
sim time next is 4715400.0000, 
raw observation next is [1.833333333333333, 75.16666666666667, 155.3333333333333, 2.0, 26.0, 25.73455142366552, 0.5482322818627906, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5133887349953832, 0.7516666666666667, 0.5177777777777777, 0.0022099447513812156, 0.6666666666666666, 0.6445459519721265, 0.6827440939542635, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.06326225], dtype=float32), -0.6814496]. 
=============================================
[2019-04-04 03:55:41,684] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:55:41,684] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:55:41,691] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run34
[2019-04-04 03:55:53,944] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.1566092e-14 1.0791673e-09 4.7604420e-11 1.7638545e-02 4.0955957e-14
 9.8236138e-01 3.6959930e-10], sum to 1.0000
[2019-04-04 03:55:53,965] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0917
[2019-04-04 03:55:54,012] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 47.0, 104.5, 615.5, 26.0, 26.46478839686373, 0.5930694756453117, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5043600.0000, 
sim time next is 5044200.0000, 
raw observation next is [1.333333333333333, 46.00000000000001, 107.0, 643.0, 26.0, 26.48408065198769, 0.6055155041730076, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4995383194829178, 0.4600000000000001, 0.3566666666666667, 0.7104972375690608, 0.6666666666666666, 0.7070067209989741, 0.7018385013910026, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.49973324], dtype=float32), 1.2341611]. 
=============================================
[2019-04-04 03:55:54,722] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:55:54,723] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:55:54,761] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run34
[2019-04-04 03:55:55,897] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:55:55,897] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:55:55,903] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run34
[2019-04-04 03:55:56,296] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:55:56,296] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:55:56,310] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.2326148e-14 2.2919968e-09 4.7782529e-09 6.7061298e-02 3.7191777e-13
 9.3293869e-01 7.0069244e-09], sum to 1.0000
[2019-04-04 03:55:56,310] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3181
[2019-04-04 03:55:56,315] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run34
[2019-04-04 03:55:56,342] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 55.0, 0.0, 0.0, 26.0, 25.69566190909836, 0.5363165521830188, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3880800.0000, 
sim time next is 3881400.0000, 
raw observation next is [-1.0, 55.83333333333334, 0.0, 0.0, 26.0, 25.63736829972218, 0.4721636202558946, 0.0, 1.0, 22106.12521836629], 
processed observation next is [1.0, 0.9565217391304348, 0.4349030470914128, 0.5583333333333335, 0.0, 0.0, 0.6666666666666666, 0.6364473583101816, 0.6573878734186315, 0.0, 1.0, 0.10526726294460138], 
reward next is 0.8947, 
noisyNet noise sample is [array([0.07330383], dtype=float32), -0.40332586]. 
=============================================
[2019-04-04 03:55:56,753] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:55:56,753] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:55:56,761] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run34
[2019-04-04 03:55:57,225] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:55:57,225] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:55:57,229] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run34
[2019-04-04 03:55:58,949] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:55:58,949] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:55:58,953] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run34
[2019-04-04 03:56:06,309] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:56:06,310] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:56:06,321] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run34
[2019-04-04 03:56:06,556] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.0341060e-12 9.2876132e-12 1.8024645e-09 2.7861400e-03 1.6263167e-10
 9.9721390e-01 1.6888947e-09], sum to 1.0000
[2019-04-04 03:56:06,556] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1472
[2019-04-04 03:56:06,568] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.2, 96.0, 0.0, 0.0, 19.0, 18.96135769382584, -1.084352093026261, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 7200.0000, 
sim time next is 7800.0000, 
raw observation next is [7.2, 96.0, 0.0, 0.0, 20.0, 18.92865752030748, -1.075068581861753, 0.0, 1.0, 197182.6416966845], 
processed observation next is [0.0, 0.08695652173913043, 0.662049861495845, 0.96, 0.0, 0.0, 0.16666666666666666, 0.07738812669228985, 0.14164380604608237, 0.0, 1.0, 0.9389649604604023], 
reward next is 0.0610, 
noisyNet noise sample is [array([-0.42465693], dtype=float32), -0.038249634]. 
=============================================
[2019-04-04 03:56:12,725] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:56:12,725] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:56:12,728] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run34
[2019-04-04 03:56:15,869] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:56:15,869] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:56:15,875] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run34
[2019-04-04 03:56:25,298] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.6630498e-22 1.3638657e-17 3.5392051e-17 1.4104914e-06 4.3618507e-20
 9.9999857e-01 3.5059223e-16], sum to 1.0000
[2019-04-04 03:56:25,299] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6619
[2019-04-04 03:56:25,336] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.7, 69.33333333333334, 0.0, 0.0, 26.0, 25.55574690356253, 0.3654532415344007, 0.0, 1.0, 38858.27640672262], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4336800.0000, 
sim time next is 4337400.0000, 
raw observation next is [3.65, 69.16666666666666, 0.0, 0.0, 26.0, 25.45521859021292, 0.366381633678873, 0.0, 1.0, 88835.50289138383], 
processed observation next is [1.0, 0.17391304347826086, 0.5637119113573408, 0.6916666666666665, 0.0, 0.0, 0.6666666666666666, 0.6212682158510766, 0.622127211226291, 0.0, 1.0, 0.4230262042446849], 
reward next is 0.5770, 
noisyNet noise sample is [array([-0.0820958], dtype=float32), -0.8062468]. 
=============================================
[2019-04-04 03:56:32,625] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.1309143e-23 1.8186203e-18 1.6558916e-18 1.8335583e-11 4.9917944e-23
 1.0000000e+00 4.2034291e-17], sum to 1.0000
[2019-04-04 03:56:32,627] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1385
[2019-04-04 03:56:32,682] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.05243110402066, 0.5106650596485267, 0.0, 1.0, 164955.3601226566], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4480200.0000, 
sim time next is 4480800.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.16449660805179, 0.546209321638274, 0.0, 1.0, 88723.64482541164], 
processed observation next is [1.0, 0.8695652173913043, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5970413840043157, 0.6820697738794247, 0.0, 1.0, 0.4224935467876745], 
reward next is 0.5775, 
noisyNet noise sample is [array([0.24429491], dtype=float32), -0.653373]. 
=============================================
[2019-04-04 03:56:38,714] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.2193568e-23 1.3534805e-18 1.5897432e-17 7.8435356e-09 2.1927680e-21
 1.0000000e+00 6.9128530e-18], sum to 1.0000
[2019-04-04 03:56:38,714] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1366
[2019-04-04 03:56:38,746] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 75.0, 0.0, 0.0, 26.0, 24.27369202377275, 0.05424014001545933, 0.0, 1.0, 41347.32028566712], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 701400.0000, 
sim time next is 702000.0000, 
raw observation next is [-3.4, 75.0, 0.0, 0.0, 26.0, 24.27203494754824, 0.05343674951070787, 0.0, 1.0, 41433.44693333862], 
processed observation next is [1.0, 0.13043478260869565, 0.368421052631579, 0.75, 0.0, 0.0, 0.6666666666666666, 0.5226695789623532, 0.5178122498369025, 0.0, 1.0, 0.19730212825399343], 
reward next is 0.8027, 
noisyNet noise sample is [array([-0.77714676], dtype=float32), -1.883799]. 
=============================================
[2019-04-04 03:56:38,749] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[81.780304]
 [81.82129 ]
 [81.80094 ]
 [81.84218 ]
 [81.84268 ]], R is [[81.78024292]
 [81.76554871]
 [81.75138092]
 [81.73758698]
 [81.72412872]].
[2019-04-04 03:56:41,111] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.5720286e-19 5.1070486e-16 1.1314707e-13 1.9308378e-01 7.4472161e-19
 8.0691618e-01 9.8651739e-14], sum to 1.0000
[2019-04-04 03:56:41,111] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7733
[2019-04-04 03:56:41,221] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 26.0, 23.31952662574798, -0.1249594447579412, 0.0, 1.0, 44103.58534811069], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 186000.0000, 
sim time next is 186600.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 26.0, 23.24875132144285, -0.1360940305303433, 0.0, 1.0, 44150.66557347004], 
processed observation next is [1.0, 0.13043478260869565, 0.21606648199445982, 0.78, 0.0, 0.0, 0.6666666666666666, 0.4373959434535708, 0.45463532315655225, 0.0, 1.0, 0.2102412646355716], 
reward next is 0.7898, 
noisyNet noise sample is [array([-0.45534077], dtype=float32), -0.40114194]. 
=============================================
[2019-04-04 03:56:50,776] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.0192158e-22 5.4212141e-21 1.7381478e-19 1.0000000e+00 9.2458095e-23
 4.2019712e-09 4.1755709e-18], sum to 1.0000
[2019-04-04 03:56:50,778] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9892
[2019-04-04 03:56:50,866] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.9, 26.0, 123.1666666666667, 0.0, 26.0, 25.10988215925386, 0.1482764851794432, 1.0, 1.0, 54938.73197572079], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 474000.0000, 
sim time next is 474600.0000, 
raw observation next is [-1.8, 25.5, 124.3333333333333, 0.0, 26.0, 25.11317412464625, 0.1584567857679859, 1.0, 1.0, 35519.64878230995], 
processed observation next is [1.0, 0.4782608695652174, 0.41274238227146814, 0.255, 0.41444444444444434, 0.0, 0.6666666666666666, 0.5927645103871875, 0.5528189285893287, 1.0, 1.0, 0.16914118467766645], 
reward next is 0.8309, 
noisyNet noise sample is [array([0.5118067], dtype=float32), -0.1566932]. 
=============================================
[2019-04-04 03:56:51,162] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.3876687e-18 3.2351056e-13 2.0435981e-11 8.7640509e-03 3.7500686e-17
 9.9123597e-01 7.8134643e-12], sum to 1.0000
[2019-04-04 03:56:51,163] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2442
[2019-04-04 03:56:51,235] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.2, 30.0, 0.0, 0.0, 26.0, 25.85848601621463, 0.5442774389388948, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5096400.0000, 
sim time next is 5097000.0000, 
raw observation next is [8.15, 32.5, 0.0, 0.0, 26.0, 25.82278278795414, 0.5294389757848633, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.6883656509695293, 0.325, 0.0, 0.0, 0.6666666666666666, 0.651898565662845, 0.6764796585949545, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.56196415], dtype=float32), -0.33968613]. 
=============================================
[2019-04-04 03:56:51,283] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[92.59296]
 [92.58396]
 [92.58763]
 [92.58183]
 [92.57539]], R is [[92.66677094]
 [92.74010468]
 [92.81270599]
 [92.88458252]
 [92.95573425]].
[2019-04-04 03:56:52,415] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:56:52,415] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:56:52,419] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run34
[2019-04-04 03:56:53,940] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.3118534e-15 5.8131824e-12 2.5991530e-11 9.9988425e-01 1.7077640e-16
 1.1575454e-04 2.6223693e-10], sum to 1.0000
[2019-04-04 03:56:53,946] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6447
[2019-04-04 03:56:54,021] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-12.8, 73.5, 0.0, 0.0, 26.0, 25.19871277894651, 0.281110357903254, 0.0, 1.0, 39011.25016499369], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 333000.0000, 
sim time next is 333600.0000, 
raw observation next is [-12.8, 74.66666666666666, 0.0, 0.0, 26.0, 25.06497302466072, 0.2572359003930041, 0.0, 1.0, 44182.47932071325], 
processed observation next is [1.0, 0.8695652173913043, 0.1080332409972299, 0.7466666666666666, 0.0, 0.0, 0.6666666666666666, 0.58874775205506, 0.5857453001310013, 0.0, 1.0, 0.2103927586700631], 
reward next is 0.7896, 
noisyNet noise sample is [array([0.71095407], dtype=float32), -0.17327352]. 
=============================================
[2019-04-04 03:57:04,497] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-04 03:57:04,506] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 03:57:04,506] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:57:04,508] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run46
[2019-04-04 03:57:04,560] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 03:57:04,564] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:57:04,566] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 03:57:04,566] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:57:04,573] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run46
[2019-04-04 03:57:04,615] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run46
[2019-04-04 03:57:52,898] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.2694857], dtype=float32), -0.02353262]
[2019-04-04 03:57:52,898] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-2.0, 72.0, 101.0, 49.0, 26.0, 25.91607833025149, 0.3369343354029371, 1.0, 1.0, 0.0]
[2019-04-04 03:57:52,898] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 03:57:52,899] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.0305425e-15 1.6241666e-13 7.3868253e-14 1.4572851e-02 5.3176466e-15
 9.8542714e-01 3.4662288e-12], sampled 0.05857521003682575
[2019-04-04 04:00:13,465] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7354.7815 239655889.3506 1605.2107
[2019-04-04 04:00:37,112] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.2694857], dtype=float32), -0.02353262]
[2019-04-04 04:00:37,112] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-3.333333333333333, 61.66666666666667, 0.0, 0.0, 26.0, 24.6024924590533, 0.1814320903727871, 0.0, 1.0, 39491.53798039068]
[2019-04-04 04:00:37,113] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 04:00:37,113] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [3.4621185e-16 5.7634778e-13 8.8537690e-13 6.6627759e-01 1.2062427e-15
 3.3372247e-01 1.6059506e-11], sampled 0.094331882225
[2019-04-04 04:00:43,629] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7242.1308 263312521.7875 1550.2376
[2019-04-04 04:00:46,255] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7183.2291 275681883.0191 1232.5420
[2019-04-04 04:00:47,299] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 4500000, evaluation results [4500000.0, 7242.130848630938, 263312521.78750217, 1550.2375587224044, 7354.781479282865, 239655889.35059875, 1605.2107140940045, 7183.22912848042, 275681883.01911306, 1232.5419916878136]
[2019-04-04 04:01:01,983] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.25288455e-14 1.86632437e-12 2.81896620e-12 9.99999404e-01
 1.59870252e-14 5.72224167e-07 5.00807507e-10], sum to 1.0000
[2019-04-04 04:01:01,983] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1832
[2019-04-04 04:01:02,022] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.9, 54.5, 0.0, 0.0, 26.0, 25.37254978880059, 0.3353122178392665, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 757800.0000, 
sim time next is 758400.0000, 
raw observation next is [-3.899999999999999, 54.0, 0.0, 0.0, 26.0, 25.16789261681189, 0.3119486173850655, 1.0, 1.0, 18721.62580311131], 
processed observation next is [1.0, 0.782608695652174, 0.35457063711911363, 0.54, 0.0, 0.0, 0.6666666666666666, 0.5973243847343243, 0.6039828724616885, 1.0, 1.0, 0.0891505990624348], 
reward next is 0.9108, 
noisyNet noise sample is [array([-0.20308545], dtype=float32), 0.67039746]. 
=============================================
[2019-04-04 04:01:02,087] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.8387813e-16 6.8186151e-12 1.5431379e-13 1.3881000e-08 3.0438915e-14
 1.0000000e+00 7.8836259e-13], sum to 1.0000
[2019-04-04 04:01:02,091] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4489
[2019-04-04 04:01:02,118] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.65, 96.0, 0.0, 0.0, 26.0, 24.72623647735776, 0.4937831032280318, 0.0, 1.0, 169180.3654237622], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1279800.0000, 
sim time next is 1280400.0000, 
raw observation next is [6.466666666666667, 96.0, 0.0, 0.0, 26.0, 24.79949374273597, 0.5227026263582432, 0.0, 1.0, 91843.8634161899], 
processed observation next is [0.0, 0.8260869565217391, 0.6417359187442291, 0.96, 0.0, 0.0, 0.6666666666666666, 0.5666244785613307, 0.6742342087860811, 0.0, 1.0, 0.4373517305532852], 
reward next is 0.5626, 
noisyNet noise sample is [array([0.867381], dtype=float32), 0.9739351]. 
=============================================
[2019-04-04 04:01:05,700] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:01:05,700] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:01:05,703] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run34
[2019-04-04 04:01:09,008] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.8487313e-25 1.2417801e-21 2.0770067e-20 4.7290479e-12 1.8672123e-23
 1.0000000e+00 2.8976757e-20], sum to 1.0000
[2019-04-04 04:01:09,008] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2500
[2019-04-04 04:01:09,085] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 26.0, 25.23692456229158, 0.4682442263401674, 0.0, 1.0, 39921.60361155862], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1389600.0000, 
sim time next is 1390200.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 26.0, 25.21164290687557, 0.4686583110622509, 0.0, 1.0, 39781.01075030019], 
processed observation next is [1.0, 0.08695652173913043, 0.46260387811634357, 0.95, 0.0, 0.0, 0.6666666666666666, 0.6009702422396309, 0.6562194370207503, 0.0, 1.0, 0.189433384525239], 
reward next is 0.8106, 
noisyNet noise sample is [array([-0.41240785], dtype=float32), -1.9584463]. 
=============================================
[2019-04-04 04:01:09,360] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:01:09,361] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:01:09,364] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run34
[2019-04-04 04:01:11,627] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.5292885e-21 1.8409876e-16 9.6279150e-18 3.1518416e-06 1.9227807e-20
 9.9999690e-01 5.9157745e-16], sum to 1.0000
[2019-04-04 04:01:11,627] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2385
[2019-04-04 04:01:11,678] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 83.0, 0.0, 0.0, 26.0, 25.02776882934196, 0.3157199693246854, 0.0, 1.0, 90250.81518409352], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 850800.0000, 
sim time next is 851400.0000, 
raw observation next is [-3.4, 83.0, 0.0, 0.0, 26.0, 25.08155680773534, 0.3104113210151477, 0.0, 1.0, 66329.46084953193], 
processed observation next is [1.0, 0.8695652173913043, 0.368421052631579, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5901297339779449, 0.6034704403383825, 0.0, 1.0, 0.31585457547396156], 
reward next is 0.6841, 
noisyNet noise sample is [array([0.9903901], dtype=float32), -1.1997731]. 
=============================================
[2019-04-04 04:01:19,901] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.4213295e-23 5.5011718e-20 7.1624142e-19 4.5467019e-08 5.5028271e-21
 1.0000000e+00 3.6866982e-19], sum to 1.0000
[2019-04-04 04:01:19,901] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6252
[2019-04-04 04:01:19,917] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.4, 93.0, 13.5, 0.0, 26.0, 25.41435192709881, 0.4374573146600694, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 979200.0000, 
sim time next is 979800.0000, 
raw observation next is [9.5, 92.83333333333333, 18.0, 0.0, 26.0, 25.40605356945419, 0.4253350006872134, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.7257617728531857, 0.9283333333333332, 0.06, 0.0, 0.6666666666666666, 0.6171711307878492, 0.6417783335624044, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.0019355], dtype=float32), 0.2785966]. 
=============================================
[2019-04-04 04:01:27,088] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.9896477e-20 3.0499579e-17 5.5374673e-16 3.5059769e-02 2.5024134e-17
 9.6494031e-01 1.1363401e-15], sum to 1.0000
[2019-04-04 04:01:27,088] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4024
[2019-04-04 04:01:27,105] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.93333333333333, 81.0, 102.3333333333333, 195.0, 26.0, 26.72185644753479, 0.800969431040656, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1071600.0000, 
sim time next is 1072200.0000, 
raw observation next is [13.11666666666667, 80.5, 104.6666666666667, 156.0, 26.0, 26.83173845933026, 0.8184458791742282, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.8259464450600187, 0.805, 0.348888888888889, 0.1723756906077348, 0.6666666666666666, 0.7359782049441884, 0.7728152930580761, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.1753211], dtype=float32), 1.0528387]. 
=============================================
[2019-04-04 04:01:27,440] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.8918225e-20 1.4143844e-17 3.1493644e-14 1.9468191e-01 7.3602059e-20
 8.0531812e-01 1.1567471e-14], sum to 1.0000
[2019-04-04 04:01:27,440] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0464
[2019-04-04 04:01:27,460] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.9, 76.66666666666667, 0.0, 0.0, 26.0, 23.65622723365128, -0.06205812411111344, 0.0, 1.0, 44319.50377913158], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 182400.0000, 
sim time next is 183000.0000, 
raw observation next is [-8.9, 77.33333333333333, 0.0, 0.0, 26.0, 23.56082137220667, -0.06839302477063922, 0.0, 1.0, 44130.29778126779], 
processed observation next is [1.0, 0.08695652173913043, 0.21606648199445982, 0.7733333333333333, 0.0, 0.0, 0.6666666666666666, 0.4634017810172226, 0.4772023250764536, 0.0, 1.0, 0.21014427514889425], 
reward next is 0.7899, 
noisyNet noise sample is [array([-1.189538], dtype=float32), -0.5100423]. 
=============================================
[2019-04-04 04:01:27,470] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.7287966e-24 2.3167138e-21 1.9169132e-20 1.0000000e+00 5.5295848e-24
 2.8176592e-10 2.5836358e-19], sum to 1.0000
[2019-04-04 04:01:27,473] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3258
[2019-04-04 04:01:27,507] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[85.90526 ]
 [85.88751 ]
 [85.84796 ]
 [85.794525]
 [85.74913 ]], R is [[85.88925934]
 [85.81932068]
 [85.7515564 ]
 [85.68417358]
 [85.61729431]].
[2019-04-04 04:01:27,514] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 71.0, 108.1666666666667, 0.0, 26.0, 25.64469105643266, 0.3119446812667144, 1.0, 1.0, 27000.85657231777], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 819600.0000, 
sim time next is 820200.0000, 
raw observation next is [-4.5, 71.0, 106.3333333333333, 0.0, 26.0, 25.62500482860999, 0.3151447370334051, 1.0, 1.0, 27354.79024878706], 
processed observation next is [1.0, 0.4782608695652174, 0.3379501385041552, 0.71, 0.35444444444444434, 0.0, 0.6666666666666666, 0.6354170690508324, 0.6050482456778017, 1.0, 1.0, 0.13026090594660505], 
reward next is 0.8697, 
noisyNet noise sample is [array([0.3140231], dtype=float32), -0.38812643]. 
=============================================
[2019-04-04 04:01:29,311] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3911606e-24 2.5533272e-22 1.7598235e-20 1.0000000e+00 1.6238960e-24
 8.6793897e-12 2.8269159e-19], sum to 1.0000
[2019-04-04 04:01:29,316] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4183
[2019-04-04 04:01:29,329] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 84.0, 83.66666666666666, 0.0, 26.0, 25.45269756965915, 0.2912256323301263, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 903000.0000, 
sim time next is 903600.0000, 
raw observation next is [1.1, 84.0, 87.0, 0.0, 26.0, 25.4441271160561, 0.2831372288102869, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.49307479224376743, 0.84, 0.29, 0.0, 0.6666666666666666, 0.6203439263380085, 0.5943790762700957, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2272224], dtype=float32), -0.47983322]. 
=============================================
[2019-04-04 04:01:30,809] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 2.2696904e-33 1.2935661e-33 5.0148290e-24 3.3968601e-36
 1.0000000e+00 4.0742036e-36], sum to 1.0000
[2019-04-04 04:01:30,809] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5725
[2019-04-04 04:01:30,815] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [15.0, 96.0, 0.0, 0.0, 26.0, 23.66949337011581, 0.1823721646987753, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1227600.0000, 
sim time next is 1228200.0000, 
raw observation next is [15.0, 96.0, 0.0, 0.0, 26.0, 23.64518408423349, 0.1791697743513758, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.21739130434782608, 0.8781163434903049, 0.96, 0.0, 0.0, 0.6666666666666666, 0.47043200701945764, 0.5597232581171253, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0631341], dtype=float32), 0.8433108]. 
=============================================
[2019-04-04 04:01:35,225] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.5713615e-30 4.9558962e-23 1.4352284e-23 3.2677372e-14 1.1392572e-27
 1.0000000e+00 7.0592091e-25], sum to 1.0000
[2019-04-04 04:01:35,229] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9941
[2019-04-04 04:01:35,249] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.4, 58.5, 0.0, 0.0, 26.0, 26.01879443341743, 0.6965480352454757, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1107000.0000, 
sim time next is 1107600.0000, 
raw observation next is [14.2, 59.0, 0.0, 0.0, 26.0, 25.91767517473524, 0.681035223082724, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.8559556786703602, 0.59, 0.0, 0.0, 0.6666666666666666, 0.65980626456127, 0.7270117410275746, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.59038246], dtype=float32), 0.022539645]. 
=============================================
[2019-04-04 04:01:47,728] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.6370707e-20 1.4707251e-16 1.0494493e-16 9.9999869e-01 3.1122364e-20
 1.3618785e-06 3.8527693e-14], sum to 1.0000
[2019-04-04 04:01:47,729] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3702
[2019-04-04 04:01:47,794] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-13.95, 63.0, 71.0, 729.0, 26.0, 25.94469609906967, 0.3661516621995304, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 383400.0000, 
sim time next is 384000.0000, 
raw observation next is [-13.76666666666667, 62.0, 68.83333333333334, 734.8333333333333, 26.0, 25.9705422747607, 0.3664627524360188, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.08125577100646345, 0.62, 0.22944444444444448, 0.8119705340699815, 0.6666666666666666, 0.6642118562300583, 0.6221542508120063, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6005731], dtype=float32), -0.21447866]. 
=============================================
[2019-04-04 04:01:47,797] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[77.70158 ]
 [77.731476]
 [77.803894]
 [77.75239 ]
 [77.79046 ]], R is [[77.87174225]
 [78.09302521]
 [78.31209564]
 [78.52897644]
 [78.49013519]].
[2019-04-04 04:01:50,903] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.1282782e-21 5.3510570e-18 6.2637869e-16 1.0000000e+00 1.5221298e-19
 2.1275573e-11 4.6218167e-16], sum to 1.0000
[2019-04-04 04:01:50,908] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3234
[2019-04-04 04:01:50,923] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.0, 79.0, 0.0, 0.0, 26.0, 25.43412524332312, 0.518607657095692, 0.0, 1.0, 127125.0499625394], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1562400.0000, 
sim time next is 1563000.0000, 
raw observation next is [4.9, 80.16666666666667, 0.0, 0.0, 26.0, 25.42476650467636, 0.5361891782875888, 0.0, 1.0, 82057.0804012929], 
processed observation next is [1.0, 0.08695652173913043, 0.5983379501385043, 0.8016666666666667, 0.0, 0.0, 0.6666666666666666, 0.6187305420563632, 0.678729726095863, 0.0, 1.0, 0.3907480019109186], 
reward next is 0.6093, 
noisyNet noise sample is [array([0.07337297], dtype=float32), -0.7668771]. 
=============================================
[2019-04-04 04:01:50,930] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[83.8577 ]
 [83.71122]
 [83.68385]
 [83.66446]
 [83.60707]], R is [[83.90697479]
 [83.46255493]
 [83.28439331]
 [83.40697479]
 [83.57290649]].
[2019-04-04 04:01:51,226] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.5790530e-22 5.1930161e-21 6.3469939e-17 9.6661711e-01 2.6924899e-20
 3.3382829e-02 5.7716703e-16], sum to 1.0000
[2019-04-04 04:01:51,227] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7727
[2019-04-04 04:01:51,250] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [12.56666666666667, 53.0, 41.83333333333334, 30.83333333333334, 26.0, 27.51112713781512, 0.7541815802673142, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1615200.0000, 
sim time next is 1615800.0000, 
raw observation next is [12.38333333333333, 53.5, 33.66666666666667, 24.66666666666667, 26.0, 26.99957319430484, 0.7643963333276441, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.8056325023084026, 0.535, 0.11222222222222224, 0.027255985267034995, 0.6666666666666666, 0.7499644328587367, 0.7547987777758814, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.20826247], dtype=float32), 1.0633104]. 
=============================================
[2019-04-04 04:01:53,493] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.9156980e-25 5.2186194e-23 4.9419337e-20 1.0000000e+00 9.4045260e-24
 1.0143194e-14 2.1926938e-20], sum to 1.0000
[2019-04-04 04:01:53,494] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7050
[2019-04-04 04:01:53,504] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [13.43333333333334, 50.33333333333334, 158.0, 82.66666666666667, 26.0, 25.89536983549695, 0.6937816867734408, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1601400.0000, 
sim time next is 1602000.0000, 
raw observation next is [13.8, 49.0, 162.5, 62.0, 26.0, 26.3236001712182, 0.7420916125949465, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.844875346260388, 0.49, 0.5416666666666666, 0.06850828729281767, 0.6666666666666666, 0.6936333476015166, 0.7473638708649822, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.66921973], dtype=float32), 0.60449314]. 
=============================================
[2019-04-04 04:01:53,514] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[92.29004 ]
 [92.14677 ]
 [91.923035]
 [92.17827 ]
 [92.274864]], R is [[92.43964386]
 [92.51525116]
 [92.59010315]
 [92.66419983]
 [92.73755646]].
[2019-04-04 04:01:56,130] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.8383110e-26 1.6439102e-23 2.6503447e-22 1.0000000e+00 3.3256022e-26
 5.8962655e-14 1.1814265e-20], sum to 1.0000
[2019-04-04 04:01:56,130] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0834
[2019-04-04 04:01:56,182] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.833333333333334, 71.0, 130.6666666666667, 13.5, 26.0, 24.98176652003959, 0.2534986715501389, 0.0, 1.0, 44192.49808733584], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1858800.0000, 
sim time next is 1859400.0000, 
raw observation next is [-4.75, 71.0, 120.0, 0.0, 26.0, 24.96569020499926, 0.2507649266245373, 0.0, 1.0, 52876.17558364248], 
processed observation next is [0.0, 0.5217391304347826, 0.3310249307479225, 0.71, 0.4, 0.0, 0.6666666666666666, 0.5804741837499382, 0.5835883088748458, 0.0, 1.0, 0.2517913123030594], 
reward next is 0.7482, 
noisyNet noise sample is [array([-1.7077892], dtype=float32), 0.17768477]. 
=============================================
[2019-04-04 04:02:08,708] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.7056621e-25 8.0547416e-24 5.9812625e-22 1.0000000e+00 7.5021441e-25
 2.7037683e-18 4.9713333e-21], sum to 1.0000
[2019-04-04 04:02:08,708] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7420
[2019-04-04 04:02:08,757] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.6, 54.00000000000001, 82.66666666666667, 41.0, 26.0, 24.90508223545783, 0.2241339564180256, 0.0, 1.0, 31400.25200264906], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 660000.0000, 
sim time next is 660600.0000, 
raw observation next is [-0.6, 54.0, 83.0, 38.0, 26.0, 24.89357216963911, 0.2220712927261464, 0.0, 1.0, 41568.43293512873], 
processed observation next is [0.0, 0.6521739130434783, 0.44598337950138506, 0.54, 0.27666666666666667, 0.041988950276243095, 0.6666666666666666, 0.5744643474699259, 0.5740237642420488, 0.0, 1.0, 0.19794491873870823], 
reward next is 0.8021, 
noisyNet noise sample is [array([0.83499706], dtype=float32), 0.33811262]. 
=============================================
[2019-04-04 04:02:10,908] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.2200436e-21 2.0233682e-19 1.7027058e-16 9.6900797e-01 9.0015507e-20
 3.0992024e-02 1.1741059e-16], sum to 1.0000
[2019-04-04 04:02:10,908] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9269
[2019-04-04 04:02:10,918] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [12.56666666666667, 82.0, 0.0, 0.0, 26.0, 25.92156049596439, 0.6179974111508743, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1064400.0000, 
sim time next is 1065000.0000, 
raw observation next is [12.38333333333333, 82.5, 7.999999999999999, 27.66666666666666, 26.0, 25.8956190526785, 0.624078637117782, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.8056325023084026, 0.825, 0.026666666666666665, 0.030570902394106807, 0.6666666666666666, 0.6579682543898752, 0.708026212372594, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1093795], dtype=float32), 0.20361666]. 
=============================================
[2019-04-04 04:02:10,934] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[93.96401 ]
 [93.174995]
 [92.06303 ]
 [90.32214 ]
 [90.39915 ]], R is [[94.80032349]
 [94.85231781]
 [94.90379333]
 [94.95475769]
 [94.89141846]].
[2019-04-04 04:02:12,284] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.2990534e-24 5.7460081e-21 2.3547255e-19 1.0000000e+00 6.3400909e-23
 2.1767563e-15 1.9159135e-18], sum to 1.0000
[2019-04-04 04:02:12,284] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3942
[2019-04-04 04:02:12,294] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.466666666666667, 75.66666666666667, 0.0, 0.0, 26.0, 24.28483997809852, 0.04060935808492071, 0.0, 1.0, 41557.58671798435], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 708000.0000, 
sim time next is 708600.0000, 
raw observation next is [-2.383333333333333, 75.83333333333333, 0.0, 0.0, 26.0, 24.26422735993866, 0.04035838257969124, 0.0, 1.0, 41566.65230362817], 
processed observation next is [1.0, 0.17391304347826086, 0.3965835641735919, 0.7583333333333333, 0.0, 0.0, 0.6666666666666666, 0.5220189466615549, 0.5134527941932304, 0.0, 1.0, 0.19793643954108653], 
reward next is 0.8021, 
noisyNet noise sample is [array([2.4016142], dtype=float32), 1.0837206]. 
=============================================
[2019-04-04 04:02:26,291] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2837800e-23 9.9416549e-23 2.4138211e-21 9.9999988e-01 3.4370007e-24
 1.5352394e-07 9.3132255e-19], sum to 1.0000
[2019-04-04 04:02:26,292] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0471
[2019-04-04 04:02:26,318] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.066666666666666, 84.33333333333334, 0.0, 0.0, 26.0, 24.13479523157609, 0.1005048273877704, 0.0, 1.0, 46498.72344599913], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1822800.0000, 
sim time next is 1823400.0000, 
raw observation next is [-6.1, 85.0, 0.0, 0.0, 26.0, 24.10115016841938, 0.09202958490185609, 0.0, 1.0, 46559.27288568018], 
processed observation next is [0.0, 0.08695652173913043, 0.29362880886426596, 0.85, 0.0, 0.0, 0.6666666666666666, 0.508429180701615, 0.5306765283006186, 0.0, 1.0, 0.22171082326514369], 
reward next is 0.7783, 
noisyNet noise sample is [array([0.21307163], dtype=float32), -0.2010197]. 
=============================================
[2019-04-04 04:02:34,827] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.9100038e-28 4.1301041e-26 6.9266302e-26 1.0000000e+00 1.6624338e-28
 4.7807177e-22 9.7919689e-23], sum to 1.0000
[2019-04-04 04:02:34,827] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3376
[2019-04-04 04:02:34,890] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.0, 74.0, 0.0, 0.0, 26.0, 25.51222035684133, 0.4275937173839646, 1.0, 1.0, 18717.13337830121], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2142000.0000, 
sim time next is 2142600.0000, 
raw observation next is [-5.100000000000001, 75.5, 0.0, 0.0, 26.0, 25.52683513505777, 0.4221521208471445, 1.0, 1.0, 25738.80035679253], 
processed observation next is [1.0, 0.8260869565217391, 0.32132963988919666, 0.755, 0.0, 0.0, 0.6666666666666666, 0.6272362612548141, 0.6407173736157148, 1.0, 1.0, 0.12256571598472633], 
reward next is 0.8774, 
noisyNet noise sample is [array([-0.09624651], dtype=float32), -1.3533808]. 
=============================================
[2019-04-04 04:02:36,458] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.7304927e-27 2.6541638e-26 1.6865431e-25 1.0000000e+00 2.2742462e-28
 1.3678754e-12 2.6485948e-23], sum to 1.0000
[2019-04-04 04:02:36,459] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8590
[2019-04-04 04:02:36,506] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.4, 68.0, 129.0, 0.0, 26.0, 25.56351801456719, 0.4641411416856467, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2125200.0000, 
sim time next is 2125800.0000, 
raw observation next is [-5.3, 68.0, 125.0, 0.0, 26.0, 26.01928737660238, 0.4890201833127186, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.31578947368421056, 0.68, 0.4166666666666667, 0.0, 0.6666666666666666, 0.6682739480501985, 0.6630067277709062, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3817312], dtype=float32), 1.9957516]. 
=============================================
[2019-04-04 04:02:37,852] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.2030253e-26 1.3745239e-23 2.9901579e-21 1.0000000e+00 3.3364410e-26
 5.5002333e-18 2.6690144e-21], sum to 1.0000
[2019-04-04 04:02:37,854] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7141
[2019-04-04 04:02:37,915] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.916666666666667, 70.5, 0.0, 0.0, 26.0, 24.78234486826734, 0.414086498569608, 1.0, 1.0, 199951.7315870462], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2137800.0000, 
sim time next is 2138400.0000, 
raw observation next is [-5.0, 71.0, 0.0, 0.0, 26.0, 25.26310862938449, 0.476357433717728, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.32409972299168976, 0.71, 0.0, 0.0, 0.6666666666666666, 0.6052590524487075, 0.6587858112392427, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.15239075], dtype=float32), -1.612914]. 
=============================================
[2019-04-04 04:02:47,397] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.2022665e-21 3.7140120e-19 1.2175919e-18 1.0000000e+00 2.3737631e-20
 2.1643243e-11 1.3089204e-16], sum to 1.0000
[2019-04-04 04:02:47,397] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9540
[2019-04-04 04:02:47,474] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.8, 84.5, 69.0, 0.0, 26.0, 25.5078843808553, 0.2841921628078755, 1.0, 1.0, 18724.46386450716], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2021400.0000, 
sim time next is 2022000.0000, 
raw observation next is [-5.733333333333333, 84.0, 74.5, 0.0, 26.0, 25.53171527402204, 0.2910962518513228, 1.0, 1.0, 18721.93195735783], 
processed observation next is [1.0, 0.391304347826087, 0.30378578024007385, 0.84, 0.24833333333333332, 0.0, 0.6666666666666666, 0.6276429395018367, 0.5970320839504409, 1.0, 1.0, 0.08915205693979919], 
reward next is 0.9108, 
noisyNet noise sample is [array([-0.25864467], dtype=float32), 2.4187205]. 
=============================================
[2019-04-04 04:02:47,477] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[84.59473 ]
 [84.71778 ]
 [85.069405]
 [85.424545]
 [85.741516]], R is [[84.79723358]
 [84.86009979]
 [85.0114975 ]
 [85.16138458]
 [85.30976868]].
[2019-04-04 04:02:58,906] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.6323641e-28 3.2244277e-26 1.9536657e-24 1.0000000e+00 4.1556864e-28
 5.1401195e-17 4.5531499e-23], sum to 1.0000
[2019-04-04 04:02:58,920] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5299
[2019-04-04 04:02:58,924] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.7381043e-23 4.6189298e-22 2.7070164e-20 1.0000000e+00 2.3818596e-23
 1.7208488e-12 6.0484108e-18], sum to 1.0000
[2019-04-04 04:02:58,924] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5634
[2019-04-04 04:02:58,981] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 47.0, 83.66666666666667, 246.6666666666667, 26.0, 25.01884922435229, 0.30782246135695, 0.0, 1.0, 18714.59722764405], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2389800.0000, 
sim time next is 2390400.0000, 
raw observation next is [0.0, 47.0, 82.5, 199.5, 26.0, 25.009158312161, 0.2995995887663938, 0.0, 1.0, 19179.16182466474], 
processed observation next is [0.0, 0.6956521739130435, 0.46260387811634357, 0.47, 0.275, 0.22044198895027625, 0.6666666666666666, 0.5840965260134166, 0.5998665295887979, 0.0, 1.0, 0.09132934202221306], 
reward next is 0.9087, 
noisyNet noise sample is [array([-0.7252286], dtype=float32), -2.7926219]. 
=============================================
[2019-04-04 04:02:58,998] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.7833333333333333, 44.83333333333334, 30.33333333333333, 30.0, 26.0, 24.96204934001971, 0.2774859934738417, 0.0, 1.0, 35859.97709403827], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2394600.0000, 
sim time next is 2395200.0000, 
raw observation next is [-0.9666666666666667, 44.66666666666667, 18.16666666666666, 23.0, 26.0, 24.96082144817331, 0.272877989703289, 0.0, 1.0, 40100.46989482931], 
processed observation next is [0.0, 0.7391304347826086, 0.43582640812557716, 0.4466666666666667, 0.060555555555555536, 0.02541436464088398, 0.6666666666666666, 0.5800684540144424, 0.5909593299010963, 0.0, 1.0, 0.19095461854680623], 
reward next is 0.8090, 
noisyNet noise sample is [array([1.4241731], dtype=float32), -0.0151536185]. 
=============================================
[2019-04-04 04:03:03,931] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.3803260e-26 1.8922807e-23 8.5619107e-23 1.0000000e+00 2.3306257e-25
 3.8202028e-11 3.4256202e-20], sum to 1.0000
[2019-04-04 04:03:03,932] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7011
[2019-04-04 04:03:03,954] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.133333333333334, 43.33333333333334, 0.0, 0.0, 26.0, 25.03643263794786, 0.24610985201631, 0.0, 1.0, 43025.36965702363], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2410800.0000, 
sim time next is 2411400.0000, 
raw observation next is [-4.316666666666666, 43.66666666666667, 0.0, 0.0, 26.0, 24.98900607306554, 0.2372204374036286, 0.0, 1.0, 43019.07652258168], 
processed observation next is [0.0, 0.9130434782608695, 0.34302862419205915, 0.4366666666666667, 0.0, 0.0, 0.6666666666666666, 0.5824171727554616, 0.5790734791345429, 0.0, 1.0, 0.20485274534562706], 
reward next is 0.7951, 
noisyNet noise sample is [array([-0.2438951], dtype=float32), 0.9136076]. 
=============================================
[2019-04-04 04:03:06,643] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.0456083e-23 2.0407126e-22 4.4020056e-21 1.0000000e+00 3.4466661e-23
 1.0760352e-17 5.4093006e-19], sum to 1.0000
[2019-04-04 04:03:06,644] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5058
[2019-04-04 04:03:06,701] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.733333333333333, 51.33333333333333, 135.5, 35.0, 26.0, 25.75119352255755, 0.2998798716680259, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2540400.0000, 
sim time next is 2541000.0000, 
raw observation next is [-1.466666666666667, 50.16666666666667, 135.0, 37.0, 26.0, 25.79382747498477, 0.3041726190998515, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.42197599261311175, 0.5016666666666667, 0.45, 0.04088397790055249, 0.6666666666666666, 0.6494856229153975, 0.6013908730332839, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.22536366], dtype=float32), -0.9636305]. 
=============================================
[2019-04-04 04:03:06,704] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[86.96584]
 [87.22163]
 [87.02469]
 [86.97763]
 [86.92888]], R is [[86.94587708]
 [87.07641602]
 [87.20565033]
 [87.33359528]
 [87.46025848]].
[2019-04-04 04:03:17,967] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.4217790e-23 9.4258077e-20 3.2640305e-18 1.0000000e+00 1.4617574e-21
 1.1969868e-08 1.6462120e-17], sum to 1.0000
[2019-04-04 04:03:17,968] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2796
[2019-04-04 04:03:18,007] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 88.0, 101.1666666666667, 0.0, 26.0, 25.36537229586765, 0.5594333792853609, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1689600.0000, 
sim time next is 1690200.0000, 
raw observation next is [1.1, 88.0, 100.0, 0.0, 26.0, 25.79266918302398, 0.5837773865838048, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.49307479224376743, 0.88, 0.3333333333333333, 0.0, 0.6666666666666666, 0.6493890985853316, 0.6945924621946017, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.59762675], dtype=float32), 0.97812045]. 
=============================================
[2019-04-04 04:03:29,477] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.3961311e-16 2.6853540e-14 4.5303209e-13 1.0000000e+00 4.0547091e-16
 7.1650708e-10 4.9420763e-12], sum to 1.0000
[2019-04-04 04:03:29,477] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4304
[2019-04-04 04:03:29,513] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.7, 78.0, 0.0, 0.0, 26.0, 24.50036949432829, 0.1581027285487162, 0.0, 1.0, 42501.33061133909], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2169000.0000, 
sim time next is 2169600.0000, 
raw observation next is [-6.700000000000001, 78.0, 0.0, 0.0, 26.0, 24.44755267657634, 0.1608049595448317, 0.0, 1.0, 42498.49022455345], 
processed observation next is [1.0, 0.08695652173913043, 0.2770083102493075, 0.78, 0.0, 0.0, 0.6666666666666666, 0.5372960563813617, 0.5536016531816106, 0.0, 1.0, 0.20237376297406404], 
reward next is 0.7976, 
noisyNet noise sample is [array([0.22732508], dtype=float32), -0.34584418]. 
=============================================
[2019-04-04 04:03:31,116] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.7714206e-18 7.9559331e-15 1.5246109e-13 9.1664457e-01 9.1480670e-17
 8.3355442e-02 2.4011359e-13], sum to 1.0000
[2019-04-04 04:03:31,117] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1786
[2019-04-04 04:03:31,136] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.733333333333333, 99.33333333333334, 79.66666666666666, 649.0, 26.0, 27.14119836453491, 0.936485492200633, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3166800.0000, 
sim time next is 3167400.0000, 
raw observation next is [6.666666666666666, 99.16666666666666, 75.33333333333334, 619.0, 26.0, 27.37107486180789, 0.9628887811958315, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.6472760849492153, 0.9916666666666666, 0.2511111111111111, 0.6839779005524862, 0.6666666666666666, 0.7809229051506575, 0.8209629270652772, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.72065115], dtype=float32), -0.9365212]. 
=============================================
[2019-04-04 04:03:42,996] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.2188581e-19 1.0186709e-15 2.7843044e-14 9.9965036e-01 7.4700530e-18
 3.4958703e-04 5.0742356e-14], sum to 1.0000
[2019-04-04 04:03:42,999] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7098
[2019-04-04 04:03:43,018] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.86367943971129, 0.3370373333450251, 0.0, 1.0, 43350.7791125916], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2939400.0000, 
sim time next is 2940000.0000, 
raw observation next is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.86028185419406, 0.3312414242292867, 0.0, 1.0, 43336.77037045686], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.85, 0.0, 0.0, 0.6666666666666666, 0.5716901545161717, 0.6104138080764289, 0.0, 1.0, 0.20636557319265172], 
reward next is 0.7936, 
noisyNet noise sample is [array([-0.79524237], dtype=float32), 1.7153485]. 
=============================================
[2019-04-04 04:03:43,057] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[79.945274]
 [79.88141 ]
 [79.9353  ]
 [80.03975 ]
 [80.50638 ]], R is [[80.15180969]
 [80.14386749]
 [80.13597107]
 [80.1281662 ]
 [80.12047577]].
[2019-04-04 04:03:58,156] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.0471806e-21 3.4610810e-15 5.1748247e-17 2.7333032e-14 4.5399109e-18
 1.0000000e+00 8.9011126e-15], sum to 1.0000
[2019-04-04 04:03:58,168] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1747
[2019-04-04 04:03:58,195] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.0, 100.0, 110.1666666666667, 798.8333333333334, 26.0, 27.46674017719147, 0.9002296862125717, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3159600.0000, 
sim time next is 3160200.0000, 
raw observation next is [7.0, 100.0, 108.3333333333333, 791.6666666666666, 26.0, 27.53842407200927, 0.9159235761737395, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.6565096952908588, 1.0, 0.361111111111111, 0.8747697974217311, 0.6666666666666666, 0.7948686726674392, 0.8053078587245799, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.56206596], dtype=float32), -1.1183239]. 
=============================================
[2019-04-04 04:04:15,071] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.9893916e-30 2.4714719e-26 4.2566263e-26 9.2014314e-25 1.2269581e-27
 1.0000000e+00 1.9878085e-26], sum to 1.0000
[2019-04-04 04:04:15,072] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5501
[2019-04-04 04:04:15,098] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.0, 100.0, 105.0, 713.0, 26.0, 26.71041453723959, 0.6561241636466023, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3147000.0000, 
sim time next is 3147600.0000, 
raw observation next is [7.0, 100.0, 106.5, 729.5, 26.0, 26.81291631996064, 0.6784463828248005, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.6565096952908588, 1.0, 0.355, 0.8060773480662984, 0.6666666666666666, 0.7344096933300532, 0.7261487942749335, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3496608], dtype=float32), 1.8156629]. 
=============================================
[2019-04-04 04:04:16,029] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.2925585e-20 5.8246678e-19 5.2067215e-18 1.0000000e+00 7.5811911e-20
 1.5780700e-13 7.3730802e-18], sum to 1.0000
[2019-04-04 04:04:16,029] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1109
[2019-04-04 04:04:16,072] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.316666666666666, 43.66666666666667, 0.0, 0.0, 26.0, 24.98835441583925, 0.2378244494403143, 0.0, 1.0, 43019.00558868746], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2411400.0000, 
sim time next is 2412000.0000, 
raw observation next is [-4.5, 44.0, 0.0, 0.0, 26.0, 24.94299974835931, 0.237103566258737, 0.0, 1.0, 43007.76508347125], 
processed observation next is [0.0, 0.9565217391304348, 0.3379501385041552, 0.44, 0.0, 0.0, 0.6666666666666666, 0.5785833123632758, 0.5790345220862457, 0.0, 1.0, 0.20479888134986307], 
reward next is 0.7952, 
noisyNet noise sample is [array([-0.6053437], dtype=float32), 0.31647393]. 
=============================================
[2019-04-04 04:04:16,078] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[81.28422 ]
 [81.37695 ]
 [81.49422 ]
 [81.636566]
 [81.76701 ]], R is [[81.22834778]
 [81.21121216]
 [81.1942215 ]
 [81.17737579]
 [81.16069031]].
[2019-04-04 04:04:23,390] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.1194531e-17 2.0681067e-13 8.1697398e-14 2.0946020e-01 2.6083458e-16
 7.9053974e-01 3.9947903e-13], sum to 1.0000
[2019-04-04 04:04:23,391] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2982
[2019-04-04 04:04:23,433] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.666666666666667, 61.33333333333334, 109.1666666666667, 744.3333333333334, 26.0, 25.63801950321502, 0.4900504264225902, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3579600.0000, 
sim time next is 3580200.0000, 
raw observation next is [-4.5, 59.5, 111.0, 758.0, 26.0, 25.5712043637542, 0.4816136495428507, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.3379501385041552, 0.595, 0.37, 0.8375690607734807, 0.6666666666666666, 0.6309336969795168, 0.6605378831809502, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0722499], dtype=float32), -2.1121838]. 
=============================================
[2019-04-04 04:04:27,682] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.0467611e-21 3.1809582e-19 5.9634838e-17 9.9999988e-01 2.1923757e-20
 6.7618238e-08 5.4350359e-17], sum to 1.0000
[2019-04-04 04:04:27,685] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5832
[2019-04-04 04:04:27,712] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [11.83333333333333, 24.66666666666666, 112.6666666666667, 780.6666666666667, 26.0, 25.65735680637996, 0.495284672584979, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3667800.0000, 
sim time next is 3668400.0000, 
raw observation next is [12.0, 24.0, 113.5, 789.5, 26.0, 25.68766485929445, 0.4991904408213663, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.7950138504155125, 0.24, 0.37833333333333335, 0.8723756906077348, 0.6666666666666666, 0.6406387382745375, 0.6663968136071221, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8007521], dtype=float32), 0.41988015]. 
=============================================
[2019-04-04 04:04:32,696] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.0075803e-22 8.0903563e-20 2.7766638e-18 1.0000000e+00 5.6345020e-22
 8.4947564e-17 6.2924237e-18], sum to 1.0000
[2019-04-04 04:04:32,697] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9219
[2019-04-04 04:04:32,706] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.55556245752625, 0.4279204134278483, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3616200.0000, 
sim time next is 3616800.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.56032705236972, 0.4175973927875624, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.4349030470914128, 0.42, 0.0, 0.0, 0.6666666666666666, 0.6300272543641432, 0.6391991309291875, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.01160765], dtype=float32), 0.6329868]. 
=============================================
[2019-04-04 04:04:33,355] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.7017568e-23 2.4507702e-20 6.8091595e-19 9.9916553e-01 4.1459005e-22
 8.3450804e-04 2.4450950e-18], sum to 1.0000
[2019-04-04 04:04:33,355] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6747
[2019-04-04 04:04:33,368] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 48.0, 0.0, 0.0, 26.0, 25.41397223338036, 0.3548470959604867, 0.0, 1.0, 41805.93415749203], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4234200.0000, 
sim time next is 4234800.0000, 
raw observation next is [2.0, 48.0, 0.0, 0.0, 26.0, 25.39832151090484, 0.3530715090874635, 0.0, 1.0, 48528.99469657204], 
processed observation next is [0.0, 0.0, 0.518005540166205, 0.48, 0.0, 0.0, 0.6666666666666666, 0.6165267925754033, 0.6176905030291545, 0.0, 1.0, 0.23109045093605732], 
reward next is 0.7689, 
noisyNet noise sample is [array([0.05518738], dtype=float32), 0.5615306]. 
=============================================
[2019-04-04 04:04:34,218] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.7058699e-18 7.9748851e-16 2.7254357e-15 1.0000000e+00 9.5344527e-18
 5.1952482e-09 1.7667706e-14], sum to 1.0000
[2019-04-04 04:04:34,220] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7765
[2019-04-04 04:04:34,235] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.833333333333334, 63.16666666666666, 0.0, 0.0, 26.0, 24.80115503564255, 0.2531949021472623, 0.0, 1.0, 42487.50701783867], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3912600.0000, 
sim time next is 3913200.0000, 
raw observation next is [-7.0, 64.0, 0.0, 0.0, 26.0, 24.72087517098385, 0.239579032405146, 0.0, 1.0, 42633.40688422684], 
processed observation next is [1.0, 0.30434782608695654, 0.2686980609418283, 0.64, 0.0, 0.0, 0.6666666666666666, 0.5600729309153207, 0.5798596774683821, 0.0, 1.0, 0.20301622325822305], 
reward next is 0.7970, 
noisyNet noise sample is [array([0.36390433], dtype=float32), -0.96890527]. 
=============================================
[2019-04-04 04:04:34,550] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.7958899e-14 6.7071557e-11 1.9818525e-11 2.5443125e-05 2.1134607e-15
 9.9997461e-01 2.5757305e-09], sum to 1.0000
[2019-04-04 04:04:34,550] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4365
[2019-04-04 04:04:34,623] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-15.0, 83.0, 40.0, 165.0, 26.0, 24.70283941213613, 0.2384627024384917, 1.0, 1.0, 87746.67560959405], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2707200.0000, 
sim time next is 2707800.0000, 
raw observation next is [-14.83333333333333, 84.33333333333334, 53.33333333333334, 220.0, 26.0, 25.03093796485835, 0.286137727729982, 1.0, 1.0, 58860.99104406253], 
processed observation next is [1.0, 0.34782608695652173, 0.05170821791320413, 0.8433333333333334, 0.1777777777777778, 0.2430939226519337, 0.6666666666666666, 0.5859114970715291, 0.5953792425766607, 1.0, 1.0, 0.2802904335431549], 
reward next is 0.7197, 
noisyNet noise sample is [array([0.7182483], dtype=float32), -1.8152541]. 
=============================================
[2019-04-04 04:04:36,619] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1054888e-16 2.6720384e-14 2.2592948e-13 9.9999201e-01 1.1897825e-15
 7.9329902e-06 6.8164506e-13], sum to 1.0000
[2019-04-04 04:04:36,619] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2333
[2019-04-04 04:04:36,644] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.46513589504378, 0.4225184695420769, 0.0, 1.0, 18758.00274813247], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3893400.0000, 
sim time next is 3894000.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.39283689980958, 0.4108806698624453, 0.0, 1.0, 60797.90479160737], 
processed observation next is [1.0, 0.043478260869565216, 0.40720221606648205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6160697416507984, 0.6369602232874817, 0.0, 1.0, 0.2895138323409875], 
reward next is 0.7105, 
noisyNet noise sample is [array([-0.02424293], dtype=float32), -0.56762415]. 
=============================================
[2019-04-04 04:04:36,657] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[85.39356 ]
 [85.42882 ]
 [85.592354]
 [85.641464]
 [85.68504 ]], R is [[85.33924103]
 [85.39652252]
 [85.54255676]
 [85.56918335]
 [85.52418518]].
[2019-04-04 04:04:40,363] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4225926e-16 8.9869518e-15 1.0499310e-12 1.6354790e-03 1.4493303e-14
 9.9836451e-01 9.3386939e-12], sum to 1.0000
[2019-04-04 04:04:40,363] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3409
[2019-04-04 04:04:40,378] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.0, 35.5, 60.33333333333333, 0.0, 26.0, 28.23767643950608, 1.065291315133377, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4378200.0000, 
sim time next is 4378800.0000, 
raw observation next is [13.0, 36.0, 49.66666666666666, 0.0, 26.0, 27.90927580270177, 1.052213345663305, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.8227146814404434, 0.36, 0.1655555555555555, 0.0, 0.6666666666666666, 0.8257729835584809, 0.8507377818877684, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.71992636], dtype=float32), -0.47991255]. 
=============================================
[2019-04-04 04:04:42,109] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.1322816e-28 1.9320174e-24 9.2986437e-25 2.1884258e-18 1.8425517e-25
 1.0000000e+00 2.0654521e-23], sum to 1.0000
[2019-04-04 04:04:42,116] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8102
[2019-04-04 04:04:42,130] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 100.0, 0.0, 0.0, 26.0, 25.43513764938113, 0.541841893280664, 0.0, 1.0, 102453.4330804997], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3204000.0000, 
sim time next is 3204600.0000, 
raw observation next is [-0.1666666666666667, 100.0, 0.0, 0.0, 26.0, 25.3609345932101, 0.5514701091847816, 0.0, 1.0, 98835.94016233257], 
processed observation next is [1.0, 0.08695652173913043, 0.4579870729455217, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6134112161008417, 0.6838233697282604, 0.0, 1.0, 0.47064733410634557], 
reward next is 0.5294, 
noisyNet noise sample is [array([-0.8155947], dtype=float32), 0.20446435]. 
=============================================
[2019-04-04 04:04:45,000] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.2083950e-14 1.7518502e-10 7.9246065e-10 3.0170353e-02 7.8154372e-13
 9.6982962e-01 3.4570504e-09], sum to 1.0000
[2019-04-04 04:04:45,001] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3798
[2019-04-04 04:04:45,022] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 25.08039182751996, 0.4125180055440403, 0.0, 1.0, 18688.68287072429], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4562400.0000, 
sim time next is 4563000.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 25.02301899543354, 0.4072417667866051, 1.0, 1.0, 48601.74882574539], 
processed observation next is [1.0, 0.8260869565217391, 0.518005540166205, 0.52, 0.0, 0.0, 0.6666666666666666, 0.5852515829527949, 0.635747255595535, 1.0, 1.0, 0.23143689917021612], 
reward next is 0.7686, 
noisyNet noise sample is [array([0.16641462], dtype=float32), 0.3121488]. 
=============================================
[2019-04-04 04:04:45,033] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[82.26238 ]
 [83.95217 ]
 [83.83275 ]
 [83.755356]
 [83.44162 ]], R is [[82.91824341]
 [83.00006866]
 [83.04428101]
 [82.99198151]
 [82.80405426]].
[2019-04-04 04:04:46,027] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.1155105e-18 1.2593399e-13 6.9535979e-13 9.9999964e-01 1.4452873e-16
 3.1379295e-07 1.0494665e-12], sum to 1.0000
[2019-04-04 04:04:46,027] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4527
[2019-04-04 04:04:46,102] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 61.0, 0.0, 0.0, 26.0, 25.26376921675267, 0.4580545098826347, 0.0, 1.0, 71373.14970988093], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4575000.0000, 
sim time next is 4575600.0000, 
raw observation next is [1.0, 61.0, 0.0, 0.0, 26.0, 25.28216951561626, 0.4659590349163249, 0.0, 1.0, 49204.26930612395], 
processed observation next is [1.0, 1.0, 0.4903047091412743, 0.61, 0.0, 0.0, 0.6666666666666666, 0.6068474596346883, 0.6553196783054417, 0.0, 1.0, 0.23430604431487595], 
reward next is 0.7657, 
noisyNet noise sample is [array([0.39704967], dtype=float32), 0.4997588]. 
=============================================
[2019-04-04 04:04:50,993] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.5233033e-16 1.9897438e-14 3.1975395e-12 9.9999988e-01 1.9397950e-15
 1.3964616e-07 7.6134923e-12], sum to 1.0000
[2019-04-04 04:04:50,993] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8349
[2019-04-04 04:04:51,050] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 25.19603861490254, 0.4327420813069212, 1.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4558800.0000, 
sim time next is 4559400.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 25.24159616600569, 0.4236613791998925, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.518005540166205, 0.52, 0.0, 0.0, 0.6666666666666666, 0.6034663471671408, 0.6412204597332974, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1963071], dtype=float32), 0.0087335175]. 
=============================================
[2019-04-04 04:05:09,947] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.7175203e-19 2.1908666e-15 2.1293387e-14 4.6997360e-04 5.6532613e-18
 9.9953008e-01 2.2694506e-14], sum to 1.0000
[2019-04-04 04:05:09,947] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6976
[2019-04-04 04:05:09,992] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 92.0, 0.0, 0.0, 26.0, 25.3202284113577, 0.3660871577818174, 0.0, 1.0, 50412.17634072433], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3095400.0000, 
sim time next is 3096000.0000, 
raw observation next is [-1.0, 92.0, 0.0, 0.0, 26.0, 25.4743841707991, 0.3719170577884582, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.4349030470914128, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6228653475665918, 0.6239723525961528, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9749495], dtype=float32), 2.3920221]. 
=============================================
[2019-04-04 04:05:09,997] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[85.46303 ]
 [85.386734]
 [85.21802 ]
 [84.78769 ]
 [84.333954]], R is [[85.60727692]
 [85.51114655]
 [85.32701111]
 [84.88906097]
 [84.08914948]].
[2019-04-04 04:05:12,095] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.5520087e-27 1.0862641e-23 4.8906505e-24 1.2068212e-12 1.5134066e-24
 1.0000000e+00 1.1844849e-22], sum to 1.0000
[2019-04-04 04:05:12,097] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3052
[2019-04-04 04:05:12,109] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.0, 100.0, 105.0, 713.0, 26.0, 26.71222186831811, 0.6561552967039804, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3147000.0000, 
sim time next is 3147600.0000, 
raw observation next is [7.0, 100.0, 106.5, 729.5, 26.0, 26.81461478521663, 0.6784584709172924, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.6565096952908588, 1.0, 0.355, 0.8060773480662984, 0.6666666666666666, 0.7345512321013858, 0.7261528236390974, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2484909], dtype=float32), -0.5232397]. 
=============================================
[2019-04-04 04:05:17,302] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-04 04:05:17,310] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 04:05:17,310] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:05:17,314] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 04:05:17,315] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:05:17,321] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run47
[2019-04-04 04:05:17,350] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 04:05:17,351] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:05:17,357] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run47
[2019-04-04 04:05:17,359] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run47
[2019-04-04 04:05:36,416] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.2312278], dtype=float32), 0.020656578]
[2019-04-04 04:05:36,416] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-7.813716667333334, 72.02191273000001, 103.523217975, 0.0, 26.0, 25.11260492615748, 0.1958691862245667, 1.0, 1.0, 71423.02572917556]
[2019-04-04 04:05:36,416] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 04:05:36,417] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [4.5846713e-19 6.9298869e-17 2.6049966e-16 9.9992001e-01 7.3022326e-19
 8.0038226e-05 4.5098499e-15], sampled 0.4167646744907654
[2019-04-04 04:06:42,058] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.2312278], dtype=float32), 0.020656578]
[2019-04-04 04:06:42,058] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [12.0, 50.66666666666666, 78.66666666666667, 403.0000000000001, 26.0, 26.82861946327948, 0.7679176158239622, 1.0, 1.0, 0.0]
[2019-04-04 04:06:42,058] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 04:06:42,059] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [7.9297127e-19 5.0043252e-16 1.3601305e-14 8.1198168e-01 7.6071394e-17
 1.8801834e-01 4.8941801e-14], sampled 0.7882261726267802
[2019-04-04 04:08:34,609] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7354.7375 239665131.9947 1605.1706
[2019-04-04 04:09:06,191] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7242.1308 263312521.7875 1550.2376
[2019-04-04 04:09:08,667] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.7525 275781969.2288 1232.5365
[2019-04-04 04:09:09,706] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 4600000, evaluation results [4600000.0, 7242.130848630938, 263312521.78750217, 1550.2375587224044, 7354.737466691969, 239665131.99468583, 1605.170574513078, 7182.752527482079, 275781969.228767, 1232.5364548109517]
[2019-04-04 04:09:11,362] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:09:11,362] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:09:11,368] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run35
[2019-04-04 04:09:27,701] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:09:27,701] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:09:27,705] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run35
[2019-04-04 04:09:32,515] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.8989172e-18 2.2711657e-15 7.1180846e-15 1.2887365e-03 3.9181005e-17
 9.9871123e-01 2.8484382e-13], sum to 1.0000
[2019-04-04 04:09:32,516] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4971
[2019-04-04 04:09:32,640] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.8, 76.0, 141.6666666666667, 24.0, 26.0, 25.29122046689993, 0.2688609344964296, 1.0, 1.0, 41643.35907443127], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 123000.0000, 
sim time next is 123600.0000, 
raw observation next is [-7.8, 78.0, 165.8333333333333, 30.0, 26.0, 25.30896023954381, 0.2832753643895076, 1.0, 1.0, 42003.35290434462], 
processed observation next is [1.0, 0.43478260869565216, 0.24653739612188366, 0.78, 0.5527777777777776, 0.03314917127071823, 0.6666666666666666, 0.6090800199619842, 0.5944251214631692, 1.0, 1.0, 0.20001596621116485], 
reward next is 0.8000, 
noisyNet noise sample is [array([1.594846], dtype=float32), 0.39058912]. 
=============================================
[2019-04-04 04:09:33,349] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.3662186e-24 1.5695004e-20 1.2648925e-19 4.3269402e-08 1.0484316e-21
 1.0000000e+00 3.6431769e-19], sum to 1.0000
[2019-04-04 04:09:33,349] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0956
[2019-04-04 04:09:33,392] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.26666666666667, 46.0, 0.0, 0.0, 26.0, 27.91473045214299, 1.037850344416526, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4386000.0000, 
sim time next is 4386600.0000, 
raw observation next is [12.2, 47.0, 0.0, 0.0, 26.0, 27.94723470136467, 1.026341492511343, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.8005540166204987, 0.47, 0.0, 0.0, 0.6666666666666666, 0.8289362251137226, 0.8421138308371144, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.7285786], dtype=float32), -0.038351938]. 
=============================================
[2019-04-04 04:09:36,041] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:09:36,041] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:09:36,044] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run35
[2019-04-04 04:09:42,922] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.2274824e-15 1.9769375e-12 9.7408415e-11 7.2289011e-05 6.6250282e-14
 9.9992776e-01 4.9694446e-11], sum to 1.0000
[2019-04-04 04:09:42,923] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9229
[2019-04-04 04:09:42,954] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6666666666666667, 52.5, 0.0, 0.0, 26.0, 25.5311546355176, 0.4078457480753602, 0.0, 1.0, 20255.05222560953], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5021400.0000, 
sim time next is 5022000.0000, 
raw observation next is [-1.0, 55.0, 0.0, 0.0, 26.0, 25.43953950939471, 0.4177873554806887, 0.0, 1.0, 78028.59787987248], 
processed observation next is [1.0, 0.13043478260869565, 0.4349030470914128, 0.55, 0.0, 0.0, 0.6666666666666666, 0.6199616257828925, 0.6392624518268962, 0.0, 1.0, 0.37156475180891657], 
reward next is 0.6284, 
noisyNet noise sample is [array([-0.19760543], dtype=float32), 0.69373494]. 
=============================================
[2019-04-04 04:09:42,984] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[80.84928]
 [81.00288]
 [81.21542]
 [81.43171]
 [81.59856]], R is [[80.73554993]
 [80.83174133]
 [81.02342224]
 [81.21318817]
 [81.31173706]].
[2019-04-04 04:09:50,567] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:09:50,567] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:09:50,571] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run35
[2019-04-04 04:09:56,674] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.23540751e-21 1.16291436e-17 6.29356597e-17 9.96053696e-01
 1.00546964e-19 3.94629873e-03 6.48144826e-15], sum to 1.0000
[2019-04-04 04:09:56,677] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8934
[2019-04-04 04:09:56,710] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.333333333333334, 42.33333333333334, 113.6666666666667, 819.8333333333334, 26.0, 25.29057271963008, 0.4541498796596165, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3676800.0000, 
sim time next is 3677400.0000, 
raw observation next is [5.5, 42.5, 113.0, 818.0, 26.0, 25.29170366290096, 0.455071124921623, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.6149584487534627, 0.425, 0.37666666666666665, 0.9038674033149171, 0.6666666666666666, 0.6076419719084134, 0.6516903749738744, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.39695346], dtype=float32), -0.54114133]. 
=============================================
[2019-04-04 04:09:57,466] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.3462427e-28 4.3610616e-26 1.2762287e-27 1.0000000e+00 1.5358177e-28
 1.8613175e-11 2.7417322e-24], sum to 1.0000
[2019-04-04 04:09:57,467] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1380
[2019-04-04 04:09:57,475] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.8, 44.33333333333334, 266.0, 385.6666666666667, 26.0, 25.07915826649026, 0.3702323870797403, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4887600.0000, 
sim time next is 4888200.0000, 
raw observation next is [1.9, 44.16666666666667, 260.0, 383.3333333333333, 26.0, 25.10243433020454, 0.3702805876056276, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.515235457063712, 0.4416666666666667, 0.8666666666666667, 0.42357274401473294, 0.6666666666666666, 0.5918695275170451, 0.6234268625352092, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8558217], dtype=float32), 0.46445692]. 
=============================================
[2019-04-04 04:10:01,744] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.8028949e-20 5.0920123e-17 4.8590335e-18 1.0000000e+00 8.9335377e-21
 4.1991505e-13 6.1544085e-16], sum to 1.0000
[2019-04-04 04:10:01,746] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0947
[2019-04-04 04:10:01,763] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 60.0, 96.5, 743.5, 26.0, 26.72292848087591, 0.6798595468646974, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3769200.0000, 
sim time next is 3769800.0000, 
raw observation next is [0.0, 60.0, 93.33333333333334, 732.6666666666667, 26.0, 26.76050312330819, 0.6902062637657022, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.46260387811634357, 0.6, 0.3111111111111111, 0.8095764272559853, 0.6666666666666666, 0.7300419269423492, 0.7300687545885673, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.57508695], dtype=float32), 0.9372366]. 
=============================================
[2019-04-04 04:10:01,934] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.0756171e-20 2.1173624e-16 1.5850026e-16 5.3759686e-10 1.3357736e-19
 1.0000000e+00 2.7064657e-15], sum to 1.0000
[2019-04-04 04:10:01,934] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3341
[2019-04-04 04:10:01,945] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 57.0, 0.0, 0.0, 26.0, 25.89575846752337, 0.5633694951756212, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4659000.0000, 
sim time next is 4659600.0000, 
raw observation next is [2.0, 57.00000000000001, 0.0, 0.0, 26.0, 25.8304389125502, 0.5353282965882412, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.518005540166205, 0.5700000000000001, 0.0, 0.0, 0.6666666666666666, 0.65253657604585, 0.6784427655294137, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.14984427], dtype=float32), 0.20495477]. 
=============================================
[2019-04-04 04:10:03,311] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.85060094e-28 1.64709818e-24 1.25747004e-25 1.00000000e+00
 1.03355656e-28 1.07327125e-11 8.36526532e-23], sum to 1.0000
[2019-04-04 04:10:03,313] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1716
[2019-04-04 04:10:03,382] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-11.7, 51.0, 56.5, 896.0, 26.0, 25.62282372800173, 0.4049962008598366, 1.0, 1.0, 68882.85049864683], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 392400.0000, 
sim time next is 393000.0000, 
raw observation next is [-11.5, 50.16666666666666, 56.0, 893.0, 26.0, 25.75811351467526, 0.424886342478346, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.1440443213296399, 0.5016666666666666, 0.18666666666666668, 0.9867403314917127, 0.6666666666666666, 0.6465094595562716, 0.6416287808261153, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.1787286], dtype=float32), 0.88026553]. 
=============================================
[2019-04-04 04:10:03,410] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[86.75927]
 [86.80983]
 [86.83176]
 [87.04837]
 [86.56431]], R is [[86.86249542]
 [86.66586304]
 [86.79920197]
 [86.62063599]
 [86.25883484]].
[2019-04-04 04:10:04,122] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:10:04,122] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:10:04,127] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run35
[2019-04-04 04:10:08,537] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:10:08,537] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:10:08,540] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run35
[2019-04-04 04:10:08,653] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:10:08,653] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:10:08,656] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run35
[2019-04-04 04:10:09,021] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:10:09,021] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:10:09,025] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run35
[2019-04-04 04:10:09,098] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:10:09,098] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:10:09,102] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run35
[2019-04-04 04:10:12,102] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:10:12,102] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:10:12,109] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run35
[2019-04-04 04:10:20,582] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:10:20,582] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:10:20,586] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run35
[2019-04-04 04:10:28,695] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:10:28,696] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:10:28,707] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run35
[2019-04-04 04:10:29,389] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1828829e-15 2.0613246e-13 1.1329298e-14 9.9999726e-01 9.6043044e-16
 2.7663546e-06 1.5957458e-12], sum to 1.0000
[2019-04-04 04:10:29,389] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0276
[2019-04-04 04:10:29,446] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.3, 61.0, 0.0, 0.0, 26.0, 25.03133836300061, 0.3311881097807456, 1.0, 1.0, 93485.97753579976], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 151200.0000, 
sim time next is 151800.0000, 
raw observation next is [-7.383333333333333, 61.5, 0.0, 0.0, 26.0, 25.13492121400959, 0.3435520506313242, 1.0, 1.0, 18752.96759809057], 
processed observation next is [1.0, 0.782608695652174, 0.25807940904893817, 0.615, 0.0, 0.0, 0.6666666666666666, 0.5945767678341326, 0.6145173502104414, 1.0, 1.0, 0.0892998457051932], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.76585203], dtype=float32), 1.6950016]. 
=============================================
[2019-04-04 04:10:33,991] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:10:33,992] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:10:33,995] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run35
[2019-04-04 04:10:38,499] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1633448e-21 2.6658322e-19 2.6158079e-18 1.0000000e+00 5.4252482e-21
 2.5742131e-10 1.2075970e-16], sum to 1.0000
[2019-04-04 04:10:38,499] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1702
[2019-04-04 04:10:38,551] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.3, 71.0, 0.0, 0.0, 26.0, 23.80466689486778, -0.01047366802701741, 0.0, 1.0, 41710.50485218169], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 795600.0000, 
sim time next is 796200.0000, 
raw observation next is [-7.3, 71.0, 0.0, 0.0, 26.0, 23.75917609461741, -0.02278341215293418, 0.0, 1.0, 41820.23702090861], 
processed observation next is [1.0, 0.21739130434782608, 0.26038781163434904, 0.71, 0.0, 0.0, 0.6666666666666666, 0.4799313412181174, 0.49240552928235526, 0.0, 1.0, 0.1991439858138505], 
reward next is 0.8009, 
noisyNet noise sample is [array([-1.4903448], dtype=float32), 0.501955]. 
=============================================
[2019-04-04 04:10:49,889] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0080585e-21 9.4897333e-20 1.6105429e-19 2.2188811e-02 3.7935626e-21
 9.7781122e-01 4.7037166e-18], sum to 1.0000
[2019-04-04 04:10:49,890] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8192
[2019-04-04 04:10:49,933] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.1, 70.33333333333333, 0.0, 0.0, 26.0, 23.18167341400824, -0.1123951306675403, 0.0, 1.0, 45736.15936916885], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 110400.0000, 
sim time next is 111000.0000, 
raw observation next is [-7.199999999999999, 69.16666666666667, 0.0, 0.0, 26.0, 23.11751297466009, -0.1161613230279944, 0.0, 1.0, 45816.87689155009], 
processed observation next is [1.0, 0.2608695652173913, 0.26315789473684215, 0.6916666666666668, 0.0, 0.0, 0.6666666666666666, 0.4264594145550076, 0.4612795589906686, 0.0, 1.0, 0.21817560424547663], 
reward next is 0.7818, 
noisyNet noise sample is [array([0.13572922], dtype=float32), -0.6613897]. 
=============================================
[2019-04-04 04:10:49,946] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[77.78644 ]
 [77.71344 ]
 [77.635254]
 [77.58211 ]
 [77.56577 ]], R is [[77.84505463]
 [77.84881592]
 [77.85305786]
 [77.85796356]
 [77.86357117]].
[2019-04-04 04:11:06,276] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:11:06,276] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:11:06,285] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run35
[2019-04-04 04:11:14,583] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.3174046e-28 1.8941700e-26 9.6751847e-25 1.0000000e+00 1.8633814e-26
 2.1621579e-15 2.1046326e-22], sum to 1.0000
[2019-04-04 04:11:14,583] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4866
[2019-04-04 04:11:14,625] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.899999999999999, 78.66666666666667, 0.0, 0.0, 26.0, 24.22206624825679, 0.09813623034162233, 0.0, 1.0, 42538.02983481785], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 614400.0000, 
sim time next is 615000.0000, 
raw observation next is [-3.9, 76.83333333333333, 0.0, 0.0, 26.0, 24.19292764226914, 0.09086543548641464, 0.0, 1.0, 42677.5247146553], 
processed observation next is [0.0, 0.08695652173913043, 0.3545706371191136, 0.7683333333333333, 0.0, 0.0, 0.6666666666666666, 0.5160773035224283, 0.5302884784954716, 0.0, 1.0, 0.20322630816502524], 
reward next is 0.7968, 
noisyNet noise sample is [array([0.3983286], dtype=float32), -0.73400885]. 
=============================================
[2019-04-04 04:11:14,639] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[79.70566 ]
 [79.66762 ]
 [79.6474  ]
 [79.634026]
 [79.61457 ]], R is [[79.74006653]
 [79.74010468]
 [79.74066925]
 [79.74167633]
 [79.74305725]].
[2019-04-04 04:11:14,928] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.0753969e-22 4.2816048e-20 4.6154519e-18 1.0000000e+00 2.8531089e-21
 1.2304430e-08 3.9958971e-17], sum to 1.0000
[2019-04-04 04:11:14,931] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5585
[2019-04-04 04:11:14,986] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.8, 87.0, 0.0, 0.0, 26.0, 24.93727417942198, 0.2797617685769051, 0.0, 1.0, 55396.3828698084], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 586800.0000, 
sim time next is 587400.0000, 
raw observation next is [-2.8, 87.0, 0.0, 0.0, 26.0, 24.94111351032065, 0.2838389350019521, 0.0, 1.0, 46032.71066919185], 
processed observation next is [0.0, 0.8260869565217391, 0.38504155124653744, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5784261258600543, 0.5946129783339841, 0.0, 1.0, 0.21920338413900883], 
reward next is 0.7808, 
noisyNet noise sample is [array([-1.1013316], dtype=float32), -2.5163908]. 
=============================================
[2019-04-04 04:11:17,739] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.1250913e-25 1.1214436e-18 6.4898978e-19 3.2733448e-03 4.6706101e-20
 9.9672663e-01 5.5526310e-18], sum to 1.0000
[2019-04-04 04:11:17,739] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7050
[2019-04-04 04:11:17,825] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [18.3, 65.0, 133.1666666666667, 0.0, 26.0, 25.04693626185877, 0.5004141684156086, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1174800.0000, 
sim time next is 1175400.0000, 
raw observation next is [18.3, 65.0, 128.0, 0.0, 26.0, 25.07110757847316, 0.5004046975042687, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.9695290858725764, 0.65, 0.4266666666666667, 0.0, 0.6666666666666666, 0.5892589648727634, 0.6668015658347562, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5477655], dtype=float32), -1.3469646]. 
=============================================
[2019-04-04 04:11:22,906] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.2285744e-28 2.1756637e-27 5.7935566e-26 1.0000000e+00 9.0640811e-29
 1.0625763e-21 3.9131900e-23], sum to 1.0000
[2019-04-04 04:11:22,906] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4744
[2019-04-04 04:11:22,936] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.6, 63.00000000000001, 93.16666666666666, 660.5000000000001, 26.0, 25.82418437412064, 0.375107547061966, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 732000.0000, 
sim time next is 732600.0000, 
raw observation next is [-0.6, 61.5, 84.0, 779.0, 26.0, 25.83394930382906, 0.3892758419456093, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.44598337950138506, 0.615, 0.28, 0.8607734806629834, 0.6666666666666666, 0.6528291086524215, 0.6297586139818697, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3234466], dtype=float32), 1.9798101]. 
=============================================
[2019-04-04 04:11:29,068] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1933329e-30 1.8618651e-29 1.0709477e-27 1.0000000e+00 1.6176587e-30
 2.5913513e-14 6.4753525e-24], sum to 1.0000
[2019-04-04 04:11:29,071] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9360
[2019-04-04 04:11:29,080] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [11.16666666666667, 19.83333333333334, 113.3333333333333, 832.6666666666667, 26.0, 27.8907985888797, 0.9953089240820141, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5062200.0000, 
sim time next is 5062800.0000, 
raw observation next is [11.33333333333333, 19.66666666666667, 112.1666666666667, 825.8333333333333, 26.0, 28.10849292779853, 1.027299647728704, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.7765466297322253, 0.1966666666666667, 0.373888888888889, 0.9125230202578268, 0.6666666666666666, 0.8423744106498775, 0.8424332159095679, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2109041], dtype=float32), 0.80194247]. 
=============================================
[2019-04-04 04:11:31,173] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:11:31,173] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:11:31,177] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run35
[2019-04-04 04:11:31,770] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:11:31,770] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:11:31,792] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run35
[2019-04-04 04:11:38,907] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0225021e-27 1.5199377e-25 1.8561895e-24 1.0000000e+00 9.3404783e-27
 9.9709715e-24 7.5594054e-23], sum to 1.0000
[2019-04-04 04:11:38,908] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5383
[2019-04-04 04:11:38,921] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.2, 89.66666666666667, 0.0, 0.0, 26.0, 25.19694431215384, 0.415283153247628, 0.0, 1.0, 43077.12870316147], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1740000.0000, 
sim time next is 1740600.0000, 
raw observation next is [-0.3, 89.0, 0.0, 0.0, 26.0, 25.17371976558139, 0.4099169513663005, 0.0, 1.0, 43112.64627714289], 
processed observation next is [0.0, 0.13043478260869565, 0.4542936288088643, 0.89, 0.0, 0.0, 0.6666666666666666, 0.5978099804651157, 0.6366389837887668, 0.0, 1.0, 0.20529831560544234], 
reward next is 0.7947, 
noisyNet noise sample is [array([-1.4979327], dtype=float32), 0.781069]. 
=============================================
[2019-04-04 04:11:41,446] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0616878e-23 7.3216339e-22 1.5597291e-21 1.0000000e+00 2.1169671e-23
 1.7008894e-14 5.1938218e-20], sum to 1.0000
[2019-04-04 04:11:41,447] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4952
[2019-04-04 04:11:41,519] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.3, 83.0, 122.5, 0.0, 26.0, 24.94552757806611, 0.3451940653877553, 0.0, 1.0, 39785.43588687392], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1771200.0000, 
sim time next is 1771800.0000, 
raw observation next is [-2.383333333333333, 83.0, 123.6666666666667, 0.0, 26.0, 24.94193500848719, 0.3455950199803988, 0.0, 1.0, 45565.32558715263], 
processed observation next is [0.0, 0.5217391304347826, 0.3965835641735919, 0.83, 0.4122222222222223, 0.0, 0.6666666666666666, 0.5784945840405991, 0.6151983399934663, 0.0, 1.0, 0.21697774089120297], 
reward next is 0.7830, 
noisyNet noise sample is [array([-1.2378559], dtype=float32), -1.9742616]. 
=============================================
[2019-04-04 04:11:50,951] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.3206936e-21 7.6050918e-19 1.7143841e-17 1.0000000e+00 4.6050534e-20
 4.5742080e-11 4.5960950e-16], sum to 1.0000
[2019-04-04 04:11:50,952] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1383
[2019-04-04 04:11:51,004] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.9, 100.0, 0.0, 0.0, 26.0, 24.69231491991573, 0.2738582199165874, 1.0, 1.0, 73836.9874167687], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 935400.0000, 
sim time next is 936000.0000, 
raw observation next is [5.0, 100.0, 0.0, 0.0, 26.0, 24.63014457429284, 0.2874047857396136, 1.0, 1.0, 87419.74331366224], 
processed observation next is [1.0, 0.8695652173913043, 0.6011080332409973, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5525120478577366, 0.5958015952465379, 1.0, 1.0, 0.4162844919698202], 
reward next is 0.5837, 
noisyNet noise sample is [array([-0.2528385], dtype=float32), 0.748519]. 
=============================================
[2019-04-04 04:11:51,007] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[86.49889]
 [85.78764]
 [86.6631 ]
 [87.92575]
 [87.88054]], R is [[87.15910339]
 [86.93590546]
 [86.97743988]
 [87.10766602]
 [87.23658752]].
[2019-04-04 04:11:53,269] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.10073500e-19 4.59428024e-18 1.02894346e-16 9.99720633e-01
 1.33319212e-18 2.79306754e-04 2.28374638e-13], sum to 1.0000
[2019-04-04 04:11:53,270] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3460
[2019-04-04 04:11:53,287] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.9000000000000001, 94.0, 22.33333333333333, 0.0, 26.0, 25.70202293710231, 0.5088509080313263, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1354800.0000, 
sim time next is 1355400.0000, 
raw observation next is [0.8, 94.5, 18.0, 0.0, 26.0, 25.69199437141465, 0.4849902603463925, 1.0, 1.0, 52305.41677080314], 
processed observation next is [1.0, 0.6956521739130435, 0.4847645429362882, 0.945, 0.06, 0.0, 0.6666666666666666, 0.640999530951221, 0.6616634201154642, 1.0, 1.0, 0.24907341319430065], 
reward next is 0.7509, 
noisyNet noise sample is [array([-0.12839927], dtype=float32), -1.1806827]. 
=============================================
[2019-04-04 04:12:01,001] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.2126361e-19 1.2122211e-17 4.1527823e-15 1.0000000e+00 4.0871300e-18
 4.5867291e-14 2.0221460e-14], sum to 1.0000
[2019-04-04 04:12:01,009] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0876
[2019-04-04 04:12:01,065] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.2, 87.0, 10.33333333333333, 0.0, 26.0, 25.16550552441132, 0.2574169210712581, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2015400.0000, 
sim time next is 2016000.0000, 
raw observation next is [-6.2, 87.0, 15.0, 0.0, 26.0, 25.26801679880227, 0.2622753982170238, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.2908587257617729, 0.87, 0.05, 0.0, 0.6666666666666666, 0.605668066566856, 0.587425132739008, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6488585], dtype=float32), 0.90640086]. 
=============================================
[2019-04-04 04:12:01,091] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[83.318726]
 [82.93581 ]
 [81.88893 ]
 [80.64894 ]
 [79.38982 ]], R is [[83.67882538]
 [83.84204102]
 [84.00362396]
 [83.20033264]
 [82.40434265]].
[2019-04-04 04:12:04,485] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.4430049e-19 3.3287617e-19 1.5941938e-18 1.0000000e+00 2.7707477e-19
 3.9923592e-11 1.8586992e-15], sum to 1.0000
[2019-04-04 04:12:04,485] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0233
[2019-04-04 04:12:04,524] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.4, 98.33333333333334, 41.33333333333334, 0.0, 26.0, 25.91862518366696, 0.5086978480777733, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1416000.0000, 
sim time next is 1416600.0000, 
raw observation next is [-0.3, 97.5, 46.0, 0.0, 26.0, 25.94703811017621, 0.5154998924476278, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4542936288088643, 0.975, 0.15333333333333332, 0.0, 0.6666666666666666, 0.6622531758480174, 0.6718332974825426, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1238588], dtype=float32), -1.2069398]. 
=============================================
[2019-04-04 04:12:10,809] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.3211765e-29 8.3276230e-25 1.4444614e-23 3.7070031e-09 1.5817992e-26
 1.0000000e+00 1.9925206e-21], sum to 1.0000
[2019-04-04 04:12:10,811] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4107
[2019-04-04 04:12:10,829] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.36666666666667, 65.33333333333334, 0.0, 0.0, 26.0, 25.66159051661642, 0.6444177862642967, 0.0, 1.0, 113236.5128936075], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1118400.0000, 
sim time next is 1119000.0000, 
raw observation next is [12.28333333333333, 65.66666666666666, 0.0, 0.0, 26.0, 25.63454635055957, 0.6562154404067543, 0.0, 1.0, 82803.88107216012], 
processed observation next is [1.0, 0.9565217391304348, 0.8028624192059095, 0.6566666666666666, 0.0, 0.0, 0.6666666666666666, 0.6362121958799642, 0.7187384801355847, 0.0, 1.0, 0.39430419558171487], 
reward next is 0.6057, 
noisyNet noise sample is [array([-2.9698071], dtype=float32), 0.28911877]. 
=============================================
[2019-04-04 04:12:10,857] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[91.27141]
 [91.44227]
 [91.39003]
 [91.48184]
 [91.46087]], R is [[91.16790009]
 [90.71700287]
 [90.80983734]
 [90.90174103]
 [90.99272156]].
[2019-04-04 04:12:12,601] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.06488673e-16 1.05769045e-13 1.97868862e-13 9.98849750e-01
 7.62573810e-16 1.15024194e-03 4.38278667e-12], sum to 1.0000
[2019-04-04 04:12:12,602] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5027
[2019-04-04 04:12:12,619] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 100.0, 0.0, 0.0, 26.0, 25.42828573798649, 0.4160813643833809, 0.0, 1.0, 65982.09513859551], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1494000.0000, 
sim time next is 1494600.0000, 
raw observation next is [1.1, 100.0, 0.0, 0.0, 26.0, 25.2539786666289, 0.4131153045721848, 0.0, 1.0, 82332.79811615628], 
processed observation next is [1.0, 0.30434782608695654, 0.49307479224376743, 1.0, 0.0, 0.0, 0.6666666666666666, 0.604498222219075, 0.6377051015240616, 0.0, 1.0, 0.392060943410268], 
reward next is 0.6079, 
noisyNet noise sample is [array([0.16657794], dtype=float32), -0.25622895]. 
=============================================
[2019-04-04 04:12:14,422] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.1398114e-27 8.9963670e-27 9.9843361e-23 1.0000000e+00 7.3924714e-29
 2.1876453e-22 7.5278054e-24], sum to 1.0000
[2019-04-04 04:12:14,423] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6230
[2019-04-04 04:12:14,508] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-9.3, 38.66666666666666, 0.0, 0.0, 26.0, 25.00987497394798, 0.3225430378165807, 1.0, 1.0, 199913.5213049959], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 409200.0000, 
sim time next is 409800.0000, 
raw observation next is [-9.4, 39.33333333333334, 0.0, 0.0, 26.0, 25.21562066014231, 0.3398182409142165, 1.0, 1.0, 42852.49697663034], 
processed observation next is [1.0, 0.7391304347826086, 0.20221606648199447, 0.3933333333333334, 0.0, 0.0, 0.6666666666666666, 0.6013017216785258, 0.6132727469714055, 1.0, 1.0, 0.20405950941252543], 
reward next is 0.7959, 
noisyNet noise sample is [array([0.03241361], dtype=float32), -2.3306816]. 
=============================================
[2019-04-04 04:12:17,237] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.3095057e-22 1.4489560e-20 9.5818518e-18 1.0000000e+00 1.4538433e-22
 1.5514341e-20 2.1477452e-17], sum to 1.0000
[2019-04-04 04:12:17,237] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1611
[2019-04-04 04:12:17,349] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.9, 82.0, 0.0, 0.0, 26.0, 24.81971750199829, 0.2448709512723637, 1.0, 1.0, 198466.2584074775], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2050200.0000, 
sim time next is 2050800.0000, 
raw observation next is [-3.899999999999999, 82.0, 0.0, 0.0, 26.0, 23.92622800332705, 0.292120354556899, 1.0, 1.0, 199967.3930368165], 
processed observation next is [1.0, 0.7391304347826086, 0.35457063711911363, 0.82, 0.0, 0.0, 0.6666666666666666, 0.49385233361058756, 0.5973734515189664, 1.0, 1.0, 0.9522256811276977], 
reward next is 0.0478, 
noisyNet noise sample is [array([0.3500519], dtype=float32), -1.0300342]. 
=============================================
[2019-04-04 04:12:38,222] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.7024171e-19 5.2826719e-16 6.1914462e-17 1.0000000e+00 6.2983041e-19
 1.4260400e-13 1.1195289e-14], sum to 1.0000
[2019-04-04 04:12:38,222] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5019
[2019-04-04 04:12:38,253] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.2, 78.0, 0.0, 0.0, 26.0, 23.94055680746676, 0.003419001493767657, 0.0, 1.0, 44742.48342205876], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1910400.0000, 
sim time next is 1911000.0000, 
raw observation next is [-8.3, 78.0, 0.0, 0.0, 26.0, 24.03161160661871, 0.01281977494178224, 0.0, 1.0, 44658.33904945417], 
processed observation next is [1.0, 0.08695652173913043, 0.23268698060941828, 0.78, 0.0, 0.0, 0.6666666666666666, 0.5026343005515592, 0.5042732583139274, 0.0, 1.0, 0.21265875737835319], 
reward next is 0.7873, 
noisyNet noise sample is [array([-1.4657816], dtype=float32), -0.41524023]. 
=============================================
[2019-04-04 04:12:38,285] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[81.6965  ]
 [81.66377 ]
 [81.71968 ]
 [81.730965]
 [81.76158 ]], R is [[81.6311264 ]
 [81.60176086]
 [81.57233429]
 [81.54295349]
 [81.51358795]].
[2019-04-04 04:12:40,203] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.6647118e-26 1.5339948e-23 4.2964053e-23 1.0000000e+00 8.3609771e-26
 8.4920725e-12 8.4271537e-21], sum to 1.0000
[2019-04-04 04:12:40,203] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9560
[2019-04-04 04:12:40,221] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.55, 84.0, 0.0, 0.0, 26.0, 24.28488003593823, 0.1386217686684995, 0.0, 1.0, 43990.70830480546], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2255400.0000, 
sim time next is 2256000.0000, 
raw observation next is [-7.633333333333333, 84.66666666666666, 0.0, 0.0, 26.0, 24.34793294038454, 0.1185625562386158, 0.0, 1.0, 44097.48337353561], 
processed observation next is [1.0, 0.08695652173913043, 0.2511542012927055, 0.8466666666666666, 0.0, 0.0, 0.6666666666666666, 0.5289944116987115, 0.5395208520795386, 0.0, 1.0, 0.20998801606445527], 
reward next is 0.7900, 
noisyNet noise sample is [array([-0.0125303], dtype=float32), 1.4179155]. 
=============================================
[2019-04-04 04:12:40,264] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[77.22027]
 [77.21095]
 [77.19736]
 [77.21283]
 [77.24153]], R is [[77.23865509]
 [77.25679016]
 [77.2733078 ]
 [77.29109955]
 [77.30872345]].
[2019-04-04 04:12:53,247] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.6059048e-28 1.9903115e-27 1.6711920e-25 1.0000000e+00 5.0223562e-30
 3.1977351e-27 3.6453660e-23], sum to 1.0000
[2019-04-04 04:12:53,249] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3721
[2019-04-04 04:12:53,302] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 75.66666666666667, 153.0, 0.0, 26.0, 25.71792858034622, 0.3553184651024215, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2031000.0000, 
sim time next is 2031600.0000, 
raw observation next is [-4.5, 76.33333333333334, 154.5, 0.0, 26.0, 25.73706007394138, 0.318542073733068, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.3379501385041552, 0.7633333333333334, 0.515, 0.0, 0.6666666666666666, 0.6447550061617816, 0.606180691244356, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4328624], dtype=float32), 0.10599279]. 
=============================================
[2019-04-04 04:13:01,398] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.0983515e-28 2.4756627e-25 3.3711296e-23 1.0000000e+00 1.7722931e-28
 2.4931368e-19 5.6345883e-22], sum to 1.0000
[2019-04-04 04:13:01,399] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1895
[2019-04-04 04:13:01,433] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.2, 75.66666666666667, 0.0, 0.0, 26.0, 23.97493910670607, 0.04286377659153969, 0.0, 1.0, 41960.29413060752], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2178600.0000, 
sim time next is 2179200.0000, 
raw observation next is [-6.2, 76.33333333333334, 0.0, 0.0, 26.0, 23.91386606234167, 0.03290636374531355, 0.0, 1.0, 41953.82972488568], 
processed observation next is [1.0, 0.21739130434782608, 0.2908587257617729, 0.7633333333333334, 0.0, 0.0, 0.6666666666666666, 0.4928221718618057, 0.5109687879151045, 0.0, 1.0, 0.19978014154707469], 
reward next is 0.8002, 
noisyNet noise sample is [array([-0.7108996], dtype=float32), 0.1760608]. 
=============================================
[2019-04-04 04:13:15,294] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.2910473e-25 6.7888593e-23 8.0382361e-22 1.0000000e+00 5.8606227e-25
 2.8013345e-19 1.9138589e-20], sum to 1.0000
[2019-04-04 04:13:15,296] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1996
[2019-04-04 04:13:15,313] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.7, 92.0, 0.0, 0.0, 26.0, 25.52827866484474, 0.5648277326559474, 0.0, 1.0, 18746.34811216838], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1306800.0000, 
sim time next is 1307400.0000, 
raw observation next is [2.616666666666667, 92.0, 0.0, 0.0, 26.0, 25.52274503699961, 0.5556915872116887, 0.0, 1.0, 26053.52538348486], 
processed observation next is [1.0, 0.13043478260869565, 0.5350877192982457, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6268954197499674, 0.6852305290705628, 0.0, 1.0, 0.12406440658802313], 
reward next is 0.8759, 
noisyNet noise sample is [array([-0.02689853], dtype=float32), -0.39551643]. 
=============================================
[2019-04-04 04:13:19,203] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.0683721e-36 1.3234385e-34 2.9458952e-32 1.0000000e+00 2.1875702e-37
 0.0000000e+00 1.2720614e-30], sum to 1.0000
[2019-04-04 04:13:19,204] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5707
[2019-04-04 04:13:19,253] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 60.00000000000001, 0.0, 0.0, 26.0, 25.01580948909377, 0.3201736838232466, 0.0, 1.0, 55963.08743082819], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3003600.0000, 
sim time next is 3004200.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.00593759248439, 0.3209424341302344, 0.0, 1.0, 48822.29043340436], 
processed observation next is [0.0, 0.782608695652174, 0.40720221606648205, 0.6, 0.0, 0.0, 0.6666666666666666, 0.5838281327070325, 0.6069808113767449, 0.0, 1.0, 0.23248709730192554], 
reward next is 0.7675, 
noisyNet noise sample is [array([-0.8843962], dtype=float32), -1.3746269]. 
=============================================
[2019-04-04 04:13:32,686] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0382249e-25 2.4817627e-24 8.4645823e-23 1.0000000e+00 4.9011789e-26
 2.3527352e-24 6.3491730e-21], sum to 1.0000
[2019-04-04 04:13:32,687] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8690
[2019-04-04 04:13:32,700] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.0, 74.0, 0.0, 0.0, 26.0, 24.8189606497487, 0.2216302971334193, 0.0, 1.0, 41723.1156399555], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2601600.0000, 
sim time next is 2602200.0000, 
raw observation next is [-5.0, 74.0, 0.0, 0.0, 26.0, 24.82885851604091, 0.2117580638856661, 0.0, 1.0, 41765.55288863975], 
processed observation next is [1.0, 0.08695652173913043, 0.32409972299168976, 0.74, 0.0, 0.0, 0.6666666666666666, 0.5690715430034091, 0.5705860212952221, 0.0, 1.0, 0.19888358518399882], 
reward next is 0.8011, 
noisyNet noise sample is [array([-0.79912204], dtype=float32), -2.1451557]. 
=============================================
[2019-04-04 04:13:33,164] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0787880e-26 3.6798162e-25 5.1793286e-24 1.0000000e+00 6.1403009e-28
 1.5632987e-19 4.3403248e-22], sum to 1.0000
[2019-04-04 04:13:33,166] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5064
[2019-04-04 04:13:33,181] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.21189376079054, 0.09885793755686256, 0.0, 1.0, 39621.40636938938], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3036000.0000, 
sim time next is 3036600.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.18305404721996, 0.09066792999252983, 0.0, 1.0, 39744.73240942054], 
processed observation next is [0.0, 0.13043478260869565, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5152545039349968, 0.5302226433308433, 0.0, 1.0, 0.1892606305210502], 
reward next is 0.8107, 
noisyNet noise sample is [array([-0.18204689], dtype=float32), 2.3144088]. 
=============================================
[2019-04-04 04:13:35,775] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.8786857e-32 1.3703240e-32 1.5313766e-29 1.0000000e+00 3.5701674e-32
 2.4497879e-27 3.3274658e-27], sum to 1.0000
[2019-04-04 04:13:35,775] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6357
[2019-04-04 04:13:35,785] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [11.91666666666667, 55.16666666666666, 17.33333333333333, 12.33333333333333, 26.0, 26.5195132309399, 0.7305299859620682, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1617000.0000, 
sim time next is 1617600.0000, 
raw observation next is [11.63333333333333, 56.33333333333334, 0.0, 0.0, 26.0, 26.76686833767011, 0.7374350105383063, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.7848568790397045, 0.5633333333333335, 0.0, 0.0, 0.6666666666666666, 0.7305723614725093, 0.7458116701794354, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2178934], dtype=float32), -0.27150702]. 
=============================================
[2019-04-04 04:14:02,386] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.6964548e-25 2.4684229e-23 5.6605238e-23 1.0000000e+00 1.7051872e-25
 3.9383030e-16 2.1906443e-21], sum to 1.0000
[2019-04-04 04:14:02,387] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6915
[2019-04-04 04:14:02,430] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.63848009932282, 0.2049932586682824, 0.0, 1.0, 41386.44953675346], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2773800.0000, 
sim time next is 2774400.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.6317420937191, 0.2041610216637905, 0.0, 1.0, 41272.62099447054], 
processed observation next is [1.0, 0.08695652173913043, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5526451744765918, 0.5680536738879302, 0.0, 1.0, 0.1965362904498597], 
reward next is 0.8035, 
noisyNet noise sample is [array([0.22117116], dtype=float32), 1.1412269]. 
=============================================
[2019-04-04 04:14:08,065] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.1594789e-30 7.3119856e-27 4.8338126e-27 1.0000000e+00 2.2193079e-30
 7.0511423e-23 2.8855896e-25], sum to 1.0000
[2019-04-04 04:14:08,065] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1452
[2019-04-04 04:14:08,091] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 92.0, 0.0, 0.0, 26.0, 25.32059490009098, 0.3659937607143449, 0.0, 1.0, 50355.23082393387], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3095400.0000, 
sim time next is 3096000.0000, 
raw observation next is [-1.0, 92.0, 0.0, 0.0, 26.0, 25.47442897702697, 0.3717961795356888, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.4349030470914128, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6228690814189143, 0.6239320598452296, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.61495584], dtype=float32), -1.129629]. 
=============================================
[2019-04-04 04:14:08,162] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[82.1181  ]
 [82.00284 ]
 [81.76158 ]
 [81.280495]
 [80.44944 ]], R is [[82.35134125]
 [82.28804779]
 [82.13696289]
 [81.73282623]
 [80.96443939]].
[2019-04-04 04:14:11,320] A3C_AGENT_WORKER-Thread-5 INFO:Evaluating...
[2019-04-04 04:14:11,329] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 04:14:11,329] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:14:11,341] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 04:14:11,341] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:14:11,344] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 04:14:11,344] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:14:11,346] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run48
[2019-04-04 04:14:11,346] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run48
[2019-04-04 04:14:11,421] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run48
[2019-04-04 04:14:32,465] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.0870217], dtype=float32), 0.079577215]
[2019-04-04 04:14:32,465] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-9.600000000000001, 77.0, 0.0, 0.0, 22.0, 20.96626882555439, -0.6708338393848946, 0.0, 1.0, 49325.32483289253]
[2019-04-04 04:14:32,465] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 04:14:32,466] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [5.1620316e-16 3.9517568e-16 5.1762514e-15 1.0000000e+00 4.6585817e-16
 5.7706177e-11 6.4402108e-13], sampled 0.5869741932006265
[2019-04-04 04:17:14,643] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7300.0755 229367756.7944 1110.1017
[2019-04-04 04:17:22,444] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.0870217], dtype=float32), 0.079577215]
[2019-04-04 04:17:22,444] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-0.3666666666666667, 72.33333333333334, 0.0, 0.0, 25.0, 24.43058402463316, 0.2705249448964555, 0.0, 1.0, 18763.73018175871]
[2019-04-04 04:17:22,444] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 04:17:22,445] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.5166777e-24 2.6776100e-22 2.0674713e-22 1.0000000e+00 1.6859328e-24
 7.4538224e-17 2.7528361e-20], sampled 0.8433901637970226
[2019-04-04 04:17:38,107] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7301.3041 250878369.1053 925.3007
[2019-04-04 04:17:50,630] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7205.2471 270772811.9702 965.2097
[2019-04-04 04:17:51,680] A3C_AGENT_WORKER-Thread-5 INFO:Global step: 4700000, evaluation results [4700000.0, 7301.304099028549, 250878369.10528278, 925.300728215905, 7300.075549406626, 229367756.7944331, 1110.1016687319975, 7205.247121619456, 270772811.9702033, 965.2097191011585]
[2019-04-04 04:17:56,313] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.3724890e-33 4.7485819e-30 3.0805693e-30 1.0000000e+00 3.5344051e-33
 2.9859830e-23 6.6001800e-29], sum to 1.0000
[2019-04-04 04:17:56,313] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9004
[2019-04-04 04:17:56,410] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 54.0, 102.5, 697.0, 26.0, 25.24765341234181, 0.3196033671845805, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3060000.0000, 
sim time next is 3060600.0000, 
raw observation next is [-4.0, 54.0, 103.6666666666667, 717.6666666666667, 26.0, 25.20527045605798, 0.3199662075554939, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.3518005540166205, 0.54, 0.34555555555555567, 0.7930018416206263, 0.6666666666666666, 0.6004392046714985, 0.6066554025184979, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.45486608], dtype=float32), 0.86629695]. 
=============================================
[2019-04-04 04:18:00,424] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.1723903e-30 1.2111280e-28 2.1203248e-27 1.0000000e+00 3.1729782e-30
 6.2037713e-22 1.5882750e-26], sum to 1.0000
[2019-04-04 04:18:00,424] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2332
[2019-04-04 04:18:00,481] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.0, 69.5, 293.0, 101.0, 26.0, 25.79704007912949, 0.4347275640011508, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2115000.0000, 
sim time next is 2115600.0000, 
raw observation next is [-6.9, 67.66666666666667, 269.3333333333334, 106.5, 26.0, 25.88219648645149, 0.4410251628579567, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.27146814404432135, 0.6766666666666667, 0.8977777777777781, 0.11767955801104972, 0.6666666666666666, 0.6568497072042909, 0.6470083876193189, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4187895], dtype=float32), -0.7771814]. 
=============================================
[2019-04-04 04:18:01,354] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.3742034e-31 1.2585123e-31 1.4448758e-29 1.0000000e+00 1.9118270e-31
 1.4879395e-30 7.5675265e-27], sum to 1.0000
[2019-04-04 04:18:01,355] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4998
[2019-04-04 04:18:01,370] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.666666666666666, 24.83333333333334, 87.0, 25.33333333333333, 26.0, 25.79065035049224, 0.4136121312008436, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2821800.0000, 
sim time next is 2822400.0000, 
raw observation next is [6.6, 25.0, 83.0, 38.0, 26.0, 25.91075696412014, 0.4273163879287371, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.6454293628808865, 0.25, 0.27666666666666667, 0.041988950276243095, 0.6666666666666666, 0.6592297470100116, 0.6424387959762458, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.9787443], dtype=float32), -0.9867828]. 
=============================================
[2019-04-04 04:18:04,690] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.8330812e-24 1.4023091e-21 1.6429268e-22 1.0000000e+00 7.1916907e-24
 2.7568633e-11 1.8030621e-20], sum to 1.0000
[2019-04-04 04:18:04,690] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8277
[2019-04-04 04:18:04,710] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.0, 100.0, 103.5, 696.5, 26.0, 26.64292055615664, 0.6346879711301437, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3146400.0000, 
sim time next is 3147000.0000, 
raw observation next is [7.0, 100.0, 105.0, 713.0, 26.0, 26.71041453723959, 0.6561241636466023, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.6565096952908588, 1.0, 0.35, 0.7878453038674034, 0.6666666666666666, 0.7258678781032991, 0.7187080545488674, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.00059894], dtype=float32), -0.57183623]. 
=============================================
[2019-04-04 04:18:04,739] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[89.161896]
 [89.456696]
 [89.84948 ]
 [90.25627 ]
 [90.32933 ]], R is [[89.02578735]
 [89.13552856]
 [89.24417114]
 [89.35173035]
 [89.45821381]].
[2019-04-04 04:18:27,755] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.4189766e-34 4.3424060e-32 2.6358843e-31 1.0000000e+00 6.8618237e-35
 1.0717041e-34 1.2581255e-28], sum to 1.0000
[2019-04-04 04:18:27,763] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6571
[2019-04-04 04:18:27,827] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 56.5, 96.66666666666666, 751.3333333333334, 26.0, 26.21036733731897, 0.6187727184922839, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3423000.0000, 
sim time next is 3423600.0000, 
raw observation next is [3.0, 58.0, 93.5, 739.5, 26.0, 26.32714825623455, 0.6361146255222735, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5457063711911359, 0.58, 0.31166666666666665, 0.8171270718232044, 0.6666666666666666, 0.6939290213528793, 0.7120382085074245, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.7508011], dtype=float32), -0.8575905]. 
=============================================
[2019-04-04 04:18:28,968] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.4851887e-33 2.9078578e-31 1.0004665e-28 1.0000000e+00 5.4176048e-33
 1.2437516e-37 1.0533674e-27], sum to 1.0000
[2019-04-04 04:18:28,971] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9572
[2019-04-04 04:18:28,979] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 26.0, 25.29221339908503, 0.4613796847415613, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3438000.0000, 
sim time next is 3438600.0000, 
raw observation next is [1.0, 79.00000000000001, 0.0, 0.0, 26.0, 25.25227966213976, 0.4385403006270076, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.4903047091412743, 0.7900000000000001, 0.0, 0.0, 0.6666666666666666, 0.6043566385116467, 0.6461801002090025, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2566363], dtype=float32), -0.9468484]. 
=============================================
[2019-04-04 04:18:30,543] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.2502055e-24 1.3934526e-20 2.1256614e-20 1.0000000e+00 2.2167934e-24
 5.2885146e-14 1.8900184e-20], sum to 1.0000
[2019-04-04 04:18:30,543] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0331
[2019-04-04 04:18:30,553] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 100.0, 0.0, 0.0, 26.0, 25.67211965995588, 0.6509442950755702, 0.0, 1.0, 196217.9094192961], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3183600.0000, 
sim time next is 3184200.0000, 
raw observation next is [3.0, 100.0, 0.0, 0.0, 26.0, 25.60030088918752, 0.666189171840771, 0.0, 1.0, 196443.7229149847], 
processed observation next is [1.0, 0.8695652173913043, 0.5457063711911359, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6333584074322932, 0.722063057280257, 0.0, 1.0, 0.9354462995951653], 
reward next is 0.0646, 
noisyNet noise sample is [array([0.0294861], dtype=float32), 1.2161452]. 
=============================================
[2019-04-04 04:18:32,272] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.1007837e-25 3.2262660e-23 3.5608124e-23 1.0000000e+00 1.6766251e-26
 1.9259776e-15 2.6422816e-20], sum to 1.0000
[2019-04-04 04:18:32,272] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2624
[2019-04-04 04:18:32,284] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-13.0, 69.0, 0.0, 0.0, 26.0, 23.8415009038691, 0.04186308163523713, 0.0, 1.0, 43816.64654211824], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3992400.0000, 
sim time next is 3993000.0000, 
raw observation next is [-13.0, 68.0, 0.0, 0.0, 26.0, 23.78712630440057, 0.0289224039792507, 0.0, 1.0, 43826.4658320157], 
processed observation next is [1.0, 0.21739130434782608, 0.10249307479224376, 0.68, 0.0, 0.0, 0.6666666666666666, 0.4822605253667141, 0.5096408013264169, 0.0, 1.0, 0.2086974563429319], 
reward next is 0.7913, 
noisyNet noise sample is [array([1.7650295], dtype=float32), -0.6699842]. 
=============================================
[2019-04-04 04:18:32,340] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[81.11698 ]
 [81.1927  ]
 [81.264465]
 [81.35065 ]
 [81.428635]], R is [[81.05938721]
 [81.04014587]
 [81.02120209]
 [81.00260925]
 [80.98435974]].
[2019-04-04 04:18:34,285] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.7904799e-33 1.3346761e-30 8.5798953e-30 1.0000000e+00 2.8530480e-33
 7.4138818e-28 2.8843996e-26], sum to 1.0000
[2019-04-04 04:18:34,289] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5465
[2019-04-04 04:18:34,306] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.5, 62.5, 119.0, 829.0, 26.0, 26.52180568878266, 0.5952636104372581, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3760200.0000, 
sim time next is 3760800.0000, 
raw observation next is [-1.333333333333333, 61.66666666666667, 118.3333333333333, 827.1666666666667, 26.0, 26.41345227282728, 0.5852499448906908, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.42566943674976926, 0.6166666666666667, 0.3944444444444443, 0.9139963167587478, 0.6666666666666666, 0.7011210227356065, 0.6950833149635636, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4221673], dtype=float32), 0.5238745]. 
=============================================
[2019-04-04 04:18:46,120] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.0023741e-30 7.5628476e-28 2.3821012e-26 1.0000000e+00 1.2968933e-30
 1.5191513e-30 4.6918750e-25], sum to 1.0000
[2019-04-04 04:18:46,120] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8846
[2019-04-04 04:18:46,141] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.333333333333333, 75.0, 0.0, 0.0, 26.0, 25.09432896459586, 0.4514648993585825, 1.0, 1.0, 92167.49079160066], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3436800.0000, 
sim time next is 3437400.0000, 
raw observation next is [1.166666666666667, 77.0, 0.0, 0.0, 26.0, 25.17686630067877, 0.4678757637108603, 1.0, 1.0, 18682.27689955042], 
processed observation next is [1.0, 0.782608695652174, 0.49492151431209613, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5980721917232309, 0.6559585879036202, 1.0, 1.0, 0.08896322333119247], 
reward next is 0.9110, 
noisyNet noise sample is [array([1.344376], dtype=float32), 0.8003519]. 
=============================================
[2019-04-04 04:18:46,249] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.0095339e-34 8.5045568e-30 4.8014995e-29 1.0000000e+00 2.4062206e-34
 8.7278683e-26 8.3772830e-28], sum to 1.0000
[2019-04-04 04:18:46,254] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3898
[2019-04-04 04:18:46,267] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 60.0, 117.0, 828.5, 26.0, 26.58907725098116, 0.661326351005472, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3844800.0000, 
sim time next is 3845400.0000, 
raw observation next is [-0.6666666666666667, 58.5, 117.0, 830.6666666666667, 26.0, 26.62583100317428, 0.6711703879953769, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.44413665743305636, 0.585, 0.39, 0.9178637200736649, 0.6666666666666666, 0.7188192502645233, 0.7237234626651255, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3592931], dtype=float32), -0.3901792]. 
=============================================
[2019-04-04 04:18:52,911] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.5889878e-30 2.1937664e-26 4.1907132e-26 1.0000000e+00 4.0454544e-31
 3.3107325e-24 5.2345469e-24], sum to 1.0000
[2019-04-04 04:18:52,921] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1273
[2019-04-04 04:18:52,942] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.5, 34.0, 0.0, 0.0, 26.0, 25.3310290958063, 0.4063836258698676, 0.0, 1.0, 66176.16856249586], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4055400.0000, 
sim time next is 4056000.0000, 
raw observation next is [-5.666666666666667, 35.0, 0.0, 0.0, 26.0, 25.25824860653934, 0.3972461370635745, 0.0, 1.0, 52174.24839855955], 
processed observation next is [1.0, 0.9565217391304348, 0.30563250230840255, 0.35, 0.0, 0.0, 0.6666666666666666, 0.604854050544945, 0.6324153790211915, 0.0, 1.0, 0.24844880189790264], 
reward next is 0.7516, 
noisyNet noise sample is [array([-0.3705128], dtype=float32), -0.072104745]. 
=============================================
[2019-04-04 04:18:52,962] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[77.90736]
 [77.84259]
 [77.80578]
 [77.61757]
 [77.40652]], R is [[77.94374084]
 [77.84918213]
 [77.80936432]
 [77.70504761]
 [77.55151367]].
[2019-04-04 04:19:01,672] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.4574336e-28 2.5582076e-25 7.2101192e-25 1.0000000e+00 4.6294093e-29
 8.9526927e-25 1.4164860e-22], sum to 1.0000
[2019-04-04 04:19:01,673] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9688
[2019-04-04 04:19:01,697] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.9666666666666668, 73.0, 0.0, 0.0, 26.0, 25.438227496009, 0.4296258774067578, 0.0, 1.0, 32953.87636688806], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4504800.0000, 
sim time next is 4505400.0000, 
raw observation next is [-0.95, 73.0, 0.0, 0.0, 26.0, 25.39302889639057, 0.4291236637295003, 0.0, 1.0, 62557.49147946476], 
processed observation next is [1.0, 0.13043478260869565, 0.43628808864265933, 0.73, 0.0, 0.0, 0.6666666666666666, 0.616085741365881, 0.6430412212431668, 0.0, 1.0, 0.29789281656887984], 
reward next is 0.7021, 
noisyNet noise sample is [array([-0.8003341], dtype=float32), -1.490355]. 
=============================================
[2019-04-04 04:19:05,857] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1627118e-26 2.3465161e-23 8.8402347e-23 1.0000000e+00 2.4474819e-26
 2.6104962e-17 3.0092677e-21], sum to 1.0000
[2019-04-04 04:19:05,858] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9525
[2019-04-04 04:19:05,872] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 74.33333333333334, 0.0, 0.0, 26.0, 25.16021174886328, 0.3223504071622135, 0.0, 1.0, 50103.27869027861], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2856000.0000, 
sim time next is 2856600.0000, 
raw observation next is [1.0, 75.5, 0.0, 0.0, 26.0, 25.17578883167089, 0.3090960634519385, 0.0, 1.0, 51550.41069875067], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 0.755, 0.0, 0.0, 0.6666666666666666, 0.5979824026392407, 0.6030320211506462, 0.0, 1.0, 0.24547814618452699], 
reward next is 0.7545, 
noisyNet noise sample is [array([-0.08303101], dtype=float32), 1.0780495]. 
=============================================
[2019-04-04 04:19:11,379] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.5573329e-24 1.1680902e-20 7.2931771e-21 1.0000000e+00 1.3675097e-24
 3.6469673e-22 7.8609140e-19], sum to 1.0000
[2019-04-04 04:19:11,380] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5238
[2019-04-04 04:19:11,406] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.40034273900324, 0.4445401053726588, 0.0, 1.0, 24546.5502109583], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3892200.0000, 
sim time next is 3892800.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.47581864833996, 0.4378682302059023, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.40720221606648205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6229848873616634, 0.6459560767353008, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.28797266], dtype=float32), 1.678273]. 
=============================================
[2019-04-04 04:19:11,931] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.2582940e-34 1.0927807e-31 2.4026668e-30 1.0000000e+00 1.9295393e-35
 4.1567393e-25 2.4583418e-29], sum to 1.0000
[2019-04-04 04:19:11,931] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9657
[2019-04-04 04:19:11,947] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.5, 44.5, 33.0, 187.0, 26.0, 25.03166336218271, 0.3204604879198266, 0.0, 1.0, 48698.79273908356], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4901400.0000, 
sim time next is 4902000.0000, 
raw observation next is [2.333333333333333, 44.33333333333334, 27.5, 155.8333333333333, 26.0, 25.00377830316694, 0.3208063999542417, 0.0, 1.0, 49610.40135492169], 
processed observation next is [0.0, 0.7391304347826086, 0.5272391505078486, 0.4433333333333334, 0.09166666666666666, 0.17219152854511965, 0.6666666666666666, 0.5836481919305783, 0.6069354666514138, 0.0, 1.0, 0.23624000645200804], 
reward next is 0.7638, 
noisyNet noise sample is [array([-0.93793327], dtype=float32), -1.2050834]. 
=============================================
[2019-04-04 04:19:11,965] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[85.422676]
 [85.44787 ]
 [85.47372 ]
 [85.551865]
 [85.615395]], R is [[85.37023163]
 [85.28463745]
 [85.23136139]
 [85.24869537]
 [85.30725861]].
[2019-04-04 04:19:14,710] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.3510429e-32 4.6316193e-28 4.3713838e-28 1.0000000e+00 5.3854839e-32
 6.6262095e-23 4.0370324e-26], sum to 1.0000
[2019-04-04 04:19:14,711] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0642
[2019-04-04 04:19:14,728] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 25.55789283854738, 0.5066850430519759, 0.0, 1.0, 43099.41836907346], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4665600.0000, 
sim time next is 4666200.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 25.50995587839001, 0.5029733530343765, 0.0, 1.0, 61939.8993360442], 
processed observation next is [1.0, 0.0, 0.518005540166205, 0.52, 0.0, 0.0, 0.6666666666666666, 0.6258296565325008, 0.6676577843447923, 0.0, 1.0, 0.29495190160021045], 
reward next is 0.7050, 
noisyNet noise sample is [array([-0.09921157], dtype=float32), 1.4065819]. 
=============================================
[2019-04-04 04:19:18,289] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.2033743e-35 8.0020066e-31 1.0454071e-31 1.0000000e+00 1.1422165e-34
 4.8809400e-24 7.7804735e-29], sum to 1.0000
[2019-04-04 04:19:18,289] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3785
[2019-04-04 04:19:18,336] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.7, 71.0, 0.0, 0.0, 26.0, 24.87245875446573, 0.3242665696843197, 0.0, 1.0, 198305.4774798817], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4303800.0000, 
sim time next is 4304400.0000, 
raw observation next is [5.6, 71.66666666666667, 0.0, 0.0, 26.0, 24.91935324544276, 0.373280149348045, 0.0, 1.0, 177760.6748775576], 
processed observation next is [0.0, 0.8260869565217391, 0.6177285318559557, 0.7166666666666667, 0.0, 0.0, 0.6666666666666666, 0.5766127704535634, 0.6244267164493483, 0.0, 1.0, 0.8464794041788458], 
reward next is 0.1535, 
noisyNet noise sample is [array([0.14816774], dtype=float32), 0.68699527]. 
=============================================
[2019-04-04 04:19:21,042] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2560251e-28 4.1794902e-26 1.5129663e-26 1.0000000e+00 1.2728533e-29
 1.6898609e-16 2.2127801e-24], sum to 1.0000
[2019-04-04 04:19:21,045] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2888
[2019-04-04 04:19:21,059] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.39912448340321, 0.2106062423483306, 0.0, 1.0, 41192.77499290308], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4769400.0000, 
sim time next is 4770000.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.35718367891599, 0.2001836916927052, 0.0, 1.0, 41246.17479728038], 
processed observation next is [0.0, 0.21739130434782608, 0.296398891966759, 0.92, 0.0, 0.0, 0.6666666666666666, 0.5297653065763326, 0.5667278972309017, 0.0, 1.0, 0.1964103561775256], 
reward next is 0.8036, 
noisyNet noise sample is [array([-0.7101222], dtype=float32), -0.92334056]. 
=============================================
[2019-04-04 04:19:21,077] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[87.661736]
 [87.62146 ]
 [87.57334 ]
 [87.53903 ]
 [87.51869 ]], R is [[87.65080261]
 [87.57813263]
 [87.50637054]
 [87.43546295]
 [87.36552429]].
[2019-04-04 04:19:21,958] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2772964e-33 7.6400550e-31 9.5429498e-29 1.0000000e+00 1.0196289e-32
 1.5115918e-28 6.3552809e-28], sum to 1.0000
[2019-04-04 04:19:21,958] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6613
[2019-04-04 04:19:21,982] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [14.3, 31.33333333333334, 181.6666666666667, 664.5, 26.0, 28.6248200218477, 1.040130445091421, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4371600.0000, 
sim time next is 4372200.0000, 
raw observation next is [14.2, 31.5, 195.0, 629.0, 26.0, 28.2359212563766, 1.116779096912447, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.8559556786703602, 0.315, 0.65, 0.6950276243093922, 0.6666666666666666, 0.8529934380313833, 0.8722596989708157, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.00665981], dtype=float32), 0.27895474]. 
=============================================
[2019-04-04 04:19:22,245] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.4387947e-27 1.6858492e-24 3.0645913e-24 1.0000000e+00 4.3182310e-28
 5.3765217e-29 8.9256302e-22], sum to 1.0000
[2019-04-04 04:19:22,245] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7358
[2019-04-04 04:19:22,261] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-12.66666666666667, 67.0, 0.0, 0.0, 26.0, 23.98377784872974, 0.0695434884813728, 0.0, 1.0, 43753.03947496606], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3991200.0000, 
sim time next is 3991800.0000, 
raw observation next is [-12.83333333333333, 68.0, 0.0, 0.0, 26.0, 23.92079771845337, 0.05631668373188495, 0.0, 1.0, 43786.64377096957], 
processed observation next is [1.0, 0.17391304347826086, 0.10710987996306563, 0.68, 0.0, 0.0, 0.6666666666666666, 0.4933998098711143, 0.5187722279106283, 0.0, 1.0, 0.20850782748080746], 
reward next is 0.7915, 
noisyNet noise sample is [array([-0.5459687], dtype=float32), -0.21161857]. 
=============================================
[2019-04-04 04:19:23,250] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:19:23,252] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:19:23,261] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run36
[2019-04-04 04:19:27,417] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.7146364e-24 2.2023854e-19 1.2617991e-18 1.0000000e+00 1.6384314e-24
 1.7789449e-09 6.6688281e-18], sum to 1.0000
[2019-04-04 04:19:27,429] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7423
[2019-04-04 04:19:27,441] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.333333333333333, 59.66666666666667, 0.0, 0.0, 26.0, 25.73865869590844, 0.5562714691436967, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4570800.0000, 
sim time next is 4571400.0000, 
raw observation next is [1.166666666666667, 60.33333333333334, 0.0, 0.0, 26.0, 25.72502153382231, 0.5533441140873044, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.49492151431209613, 0.6033333333333334, 0.0, 0.0, 0.6666666666666666, 0.6437517944851926, 0.6844480380291015, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.718111], dtype=float32), 0.51291734]. 
=============================================
[2019-04-04 04:19:34,566] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.0863795e-32 3.6633378e-29 1.0623780e-28 1.0000000e+00 1.9617606e-32
 1.6564105e-17 1.5549903e-26], sum to 1.0000
[2019-04-04 04:19:34,574] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2461
[2019-04-04 04:19:34,608] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 24.93603461781422, 0.4109865083535882, 1.0, 1.0, 94862.21272484685], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4563600.0000, 
sim time next is 4564200.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 24.9361520289145, 0.4264435248153673, 0.0, 1.0, 55587.63899123335], 
processed observation next is [1.0, 0.8260869565217391, 0.518005540166205, 0.52, 0.0, 0.0, 0.6666666666666666, 0.5780126690762083, 0.6421478416051224, 0.0, 1.0, 0.2647030428153969], 
reward next is 0.7353, 
noisyNet noise sample is [array([-0.5130326], dtype=float32), 1.0312076]. 
=============================================
[2019-04-04 04:19:36,165] A3C_AGENT_WORKER-Thread-2 INFO:Local step 297500, global step 4740225: loss 0.0018
[2019-04-04 04:19:36,171] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 297500, global step 4740225: learning rate 0.0005
[2019-04-04 04:19:36,969] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:19:36,969] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:19:36,979] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run36
[2019-04-04 04:19:37,213] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.3383072e-28 6.2895934e-25 1.6417473e-24 1.0000000e+00 9.8866312e-30
 1.2636221e-22 7.9592310e-24], sum to 1.0000
[2019-04-04 04:19:37,221] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5415
[2019-04-04 04:19:37,240] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 55.0, 0.0, 0.0, 26.0, 25.36718209122643, 0.3865608699978571, 0.0, 1.0, 41225.54681352338], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4834800.0000, 
sim time next is 4835400.0000, 
raw observation next is [-1.166666666666667, 55.83333333333334, 0.0, 0.0, 26.0, 25.37700971518291, 0.38848241662953, 0.0, 1.0, 37263.21232937082], 
processed observation next is [0.0, 1.0, 0.43028624192059095, 0.5583333333333335, 0.0, 0.0, 0.6666666666666666, 0.614750809598576, 0.62949413887651, 0.0, 1.0, 0.17744386823509914], 
reward next is 0.8226, 
noisyNet noise sample is [array([-0.219191], dtype=float32), -1.0591514]. 
=============================================
[2019-04-04 04:19:38,946] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3679751e-30 3.5117568e-27 3.4695069e-27 1.0000000e+00 5.0923814e-32
 5.1213509e-24 1.3474798e-24], sum to 1.0000
[2019-04-04 04:19:38,946] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1640
[2019-04-04 04:19:38,951] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [8.75, 19.0, 0.0, 0.0, 26.0, 26.81079131049697, 0.7877931803081643, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5089800.0000, 
sim time next is 5090400.0000, 
raw observation next is [8.7, 19.0, 0.0, 0.0, 26.0, 26.7570806001278, 0.775793293007411, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.703601108033241, 0.19, 0.0, 0.0, 0.6666666666666666, 0.7297567166773167, 0.7585977643358036, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0883155], dtype=float32), -1.2703118]. 
=============================================
[2019-04-04 04:19:40,457] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:19:40,457] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:19:40,464] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run36
[2019-04-04 04:19:41,022] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.6026546e-32 2.0407862e-26 6.8933799e-29 1.0000000e+00 2.2505257e-32
 1.2247378e-18 1.4460423e-25], sum to 1.0000
[2019-04-04 04:19:41,039] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0002
[2019-04-04 04:19:41,055] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 26.0, 25.43106101240274, 0.5524652036462067, 0.0, 1.0, 18762.25114757975], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3445200.0000, 
sim time next is 3445800.0000, 
raw observation next is [1.0, 80.16666666666667, 0.0, 0.0, 26.0, 25.55072365381081, 0.5595260388287023, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.4903047091412743, 0.8016666666666667, 0.0, 0.0, 0.6666666666666666, 0.6292269711509008, 0.6865086796095675, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.02443518], dtype=float32), 0.8998835]. 
=============================================
[2019-04-04 04:19:45,177] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.8798403e-28 1.4261789e-24 3.9427063e-24 1.0000000e+00 7.9075819e-29
 6.5299247e-24 3.6248985e-23], sum to 1.0000
[2019-04-04 04:19:45,183] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8105
[2019-04-04 04:19:45,203] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.1666666666666666, 77.0, 19.33333333333334, 18.66666666666667, 26.0, 25.55107346156153, 0.486252202071156, 1.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4729800.0000, 
sim time next is 4730400.0000, 
raw observation next is [0.0, 78.0, 0.0, 0.0, 26.0, 25.47308855275642, 0.4759168361119215, 1.0, 1.0, 19949.85640337876], 
processed observation next is [1.0, 0.782608695652174, 0.46260387811634357, 0.78, 0.0, 0.0, 0.6666666666666666, 0.6227573793963682, 0.6586389453706405, 1.0, 1.0, 0.09499931620656552], 
reward next is 0.9050, 
noisyNet noise sample is [array([1.0343736], dtype=float32), 0.09200777]. 
=============================================
[2019-04-04 04:19:49,521] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.3583657e-27 3.8561238e-23 2.1761154e-24 1.0000000e+00 2.9504239e-27
 1.3620421e-18 1.1296879e-21], sum to 1.0000
[2019-04-04 04:19:49,527] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4459
[2019-04-04 04:19:49,540] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.05, 90.5, 0.0, 0.0, 26.0, 24.55508632388501, 0.1941123682097116, 0.0, 1.0, 40530.81817941895], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 77400.0000, 
sim time next is 78000.0000, 
raw observation next is [0.8666666666666667, 92.33333333333334, 0.0, 0.0, 26.0, 24.5318850159105, 0.1899385175786509, 0.0, 1.0, 40474.03252478424], 
processed observation next is [0.0, 0.9130434782608695, 0.4866112650046169, 0.9233333333333335, 0.0, 0.0, 0.6666666666666666, 0.5443237513258751, 0.5633128391928837, 0.0, 1.0, 0.19273348821325828], 
reward next is 0.8073, 
noisyNet noise sample is [array([0.95771587], dtype=float32), -0.8686901]. 
=============================================
[2019-04-04 04:19:49,548] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[83.76175 ]
 [83.79676 ]
 [83.873505]
 [83.91849 ]
 [84.00749 ]], R is [[83.71080017]
 [83.68069458]
 [83.6506424 ]
 [83.62068176]
 [83.59090424]].
[2019-04-04 04:19:49,891] A3C_AGENT_WORKER-Thread-19 INFO:Local step 297500, global step 4745795: loss 0.0122
[2019-04-04 04:19:49,891] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 297500, global step 4745795: learning rate 0.0005
[2019-04-04 04:19:52,478] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.2250054e-26 1.6278795e-22 1.3667549e-22 1.0000000e+00 4.1012006e-26
 1.9364577e-18 7.7745749e-20], sum to 1.0000
[2019-04-04 04:19:52,479] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0982
[2019-04-04 04:19:52,494] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 60.33333333333334, 0.0, 0.0, 26.0, 25.56098933404161, 0.4720203276324293, 0.0, 1.0, 57599.59130500078], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4672200.0000, 
sim time next is 4672800.0000, 
raw observation next is [2.0, 62.0, 0.0, 0.0, 26.0, 25.48371536776134, 0.4689387569615874, 0.0, 1.0, 83765.70381558311], 
processed observation next is [1.0, 0.08695652173913043, 0.518005540166205, 0.62, 0.0, 0.0, 0.6666666666666666, 0.6236429473134452, 0.6563129189871958, 0.0, 1.0, 0.3988843038837291], 
reward next is 0.6011, 
noisyNet noise sample is [array([0.11511558], dtype=float32), -1.4219075]. 
=============================================
[2019-04-04 04:19:53,588] A3C_AGENT_WORKER-Thread-17 INFO:Local step 297500, global step 4747355: loss 0.0039
[2019-04-04 04:19:53,590] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 297500, global step 4747355: learning rate 0.0005
[2019-04-04 04:19:53,741] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:19:53,741] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:19:53,746] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run36
[2019-04-04 04:19:57,645] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.6382818e-30 1.2266819e-27 4.5191037e-27 1.0000000e+00 8.2883937e-31
 8.6273221e-22 3.3531958e-25], sum to 1.0000
[2019-04-04 04:19:57,646] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0025
[2019-04-04 04:19:57,657] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 41.0, 0.0, 0.0, 26.0, 25.17055243889239, 0.3197735500967003, 0.0, 1.0, 40863.05657591492], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4068000.0000, 
sim time next is 4068600.0000, 
raw observation next is [-5.833333333333333, 40.50000000000001, 0.0, 0.0, 26.0, 25.13999053954011, 0.3221565328183271, 0.0, 1.0, 40871.43743609013], 
processed observation next is [1.0, 0.08695652173913043, 0.30101569713758086, 0.4050000000000001, 0.0, 0.0, 0.6666666666666666, 0.5949992116283426, 0.6073855109394424, 0.0, 1.0, 0.19462589255281013], 
reward next is 0.8054, 
noisyNet noise sample is [array([1.6139867], dtype=float32), 0.49620947]. 
=============================================
[2019-04-04 04:19:57,662] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.0663769e-29 7.4308247e-25 6.9651031e-26 1.0000000e+00 8.9205900e-30
 2.1182111e-23 2.8624800e-24], sum to 1.0000
[2019-04-04 04:19:57,662] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7133
[2019-04-04 04:19:57,682] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 79.66666666666667, 0.0, 0.0, 26.0, 25.08604756906662, 0.3902110626982782, 0.0, 1.0, 41414.98327575551], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4753200.0000, 
sim time next is 4753800.0000, 
raw observation next is [-4.0, 77.5, 0.0, 0.0, 26.0, 25.05608436598374, 0.3807748837044011, 0.0, 1.0, 41354.82747549655], 
processed observation next is [0.0, 0.0, 0.3518005540166205, 0.775, 0.0, 0.0, 0.6666666666666666, 0.588007030498645, 0.6269249612348003, 0.0, 1.0, 0.19692774988331693], 
reward next is 0.8031, 
noisyNet noise sample is [array([-0.64411485], dtype=float32), 0.952367]. 
=============================================
[2019-04-04 04:19:59,037] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:19:59,038] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:19:59,056] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run36
[2019-04-04 04:20:02,691] A3C_AGENT_WORKER-Thread-2 INFO:Local step 298000, global step 4750820: loss 0.0271
[2019-04-04 04:20:02,692] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 298000, global step 4750820: learning rate 0.0005
[2019-04-04 04:20:05,265] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:20:05,265] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:20:05,270] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run36
[2019-04-04 04:20:07,030] A3C_AGENT_WORKER-Thread-11 INFO:Local step 297500, global step 4752353: loss 0.0128
[2019-04-04 04:20:07,031] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 297500, global step 4752353: learning rate 0.0005
[2019-04-04 04:20:07,164] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.1916775e-35 6.7115828e-31 5.1167050e-31 1.0000000e+00 1.8912228e-35
 7.7513384e-25 7.5196063e-29], sum to 1.0000
[2019-04-04 04:20:07,165] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6432
[2019-04-04 04:20:07,186] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.166666666666667, 60.0, 112.3333333333333, 790.6666666666667, 26.0, 26.57283969962146, 0.6332536038193716, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3840600.0000, 
sim time next is 3841200.0000, 
raw observation next is [-1.0, 60.0, 113.5, 798.5, 26.0, 26.57345047262416, 0.6404820996255123, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4349030470914128, 0.6, 0.37833333333333335, 0.8823204419889503, 0.6666666666666666, 0.7144542060520133, 0.7134940332085041, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5023057], dtype=float32), 1.538883]. 
=============================================
[2019-04-04 04:20:07,867] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:20:07,867] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:20:07,871] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run36
[2019-04-04 04:20:08,042] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:20:08,042] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:20:08,058] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run36
[2019-04-04 04:20:10,309] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:20:10,309] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:20:10,313] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run36
[2019-04-04 04:20:12,350] A3C_AGENT_WORKER-Thread-15 INFO:Local step 297500, global step 4753822: loss 0.1754
[2019-04-04 04:20:12,359] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 297500, global step 4753822: learning rate 0.0005
[2019-04-04 04:20:14,677] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:20:14,677] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:20:14,694] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run36
[2019-04-04 04:20:17,522] A3C_AGENT_WORKER-Thread-19 INFO:Local step 298000, global step 4755260: loss 0.2021
[2019-04-04 04:20:17,523] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 298000, global step 4755260: learning rate 0.0005
[2019-04-04 04:20:18,190] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.1587047e-30 1.6823840e-27 6.7473746e-29 1.0000000e+00 2.6461201e-31
 5.6422408e-31 1.0425111e-25], sum to 1.0000
[2019-04-04 04:20:18,190] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7000
[2019-04-04 04:20:18,273] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 36.0, 0.0, 0.0, 26.0, 25.45170683545915, 0.3666669693728438, 0.0, 1.0, 24382.37171579819], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4917600.0000, 
sim time next is 4918200.0000, 
raw observation next is [0.8333333333333334, 36.5, 0.0, 0.0, 26.0, 25.43941818004361, 0.3625149991420087, 0.0, 1.0, 37300.44767980564], 
processed observation next is [0.0, 0.9565217391304348, 0.4856879039704525, 0.365, 0.0, 0.0, 0.6666666666666666, 0.6199515150036342, 0.6208383330473363, 0.0, 1.0, 0.1776211794276459], 
reward next is 0.8224, 
noisyNet noise sample is [array([-1.1446048], dtype=float32), -0.018403787]. 
=============================================
[2019-04-04 04:20:18,346] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.0706534e-29 5.4997312e-25 1.4547342e-25 1.0000000e+00 4.4375454e-29
 2.4286362e-21 5.7530648e-23], sum to 1.0000
[2019-04-04 04:20:18,349] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6394
[2019-04-04 04:20:18,365] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-13.9, 69.33333333333334, 0.0, 0.0, 26.0, 23.8782261115814, 0.01589364192545331, 0.0, 1.0, 47098.04197041821], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 342600.0000, 
sim time next is 343200.0000, 
raw observation next is [-13.9, 68.66666666666667, 0.0, 0.0, 26.0, 23.80952096124847, 0.00107203071390898, 0.0, 1.0, 47127.94110309132], 
processed observation next is [1.0, 1.0, 0.07756232686980608, 0.6866666666666668, 0.0, 0.0, 0.6666666666666666, 0.48412674677070583, 0.500357343571303, 0.0, 1.0, 0.2244187671575777], 
reward next is 0.7756, 
noisyNet noise sample is [array([-1.1930009], dtype=float32), -0.95764655]. 
=============================================
[2019-04-04 04:20:18,953] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.3205376e-30 2.1712861e-27 2.6728271e-27 1.0000000e+00 1.0505287e-30
 1.1117215e-18 6.1789631e-25], sum to 1.0000
[2019-04-04 04:20:18,953] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7327
[2019-04-04 04:20:18,974] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-15.0, 69.0, 0.0, 0.0, 26.0, 23.09229902879805, -0.1898409669875537, 0.0, 1.0, 48821.50314691781], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 354000.0000, 
sim time next is 354600.0000, 
raw observation next is [-15.0, 69.0, 0.0, 0.0, 26.0, 23.01214325541527, -0.1994672769892262, 0.0, 1.0, 48980.91312995016], 
processed observation next is [1.0, 0.08695652173913043, 0.04709141274238226, 0.69, 0.0, 0.0, 0.6666666666666666, 0.41767860461793926, 0.4335109076702579, 0.0, 1.0, 0.23324244347595313], 
reward next is 0.7668, 
noisyNet noise sample is [array([-2.0273876], dtype=float32), -0.5559355]. 
=============================================
[2019-04-04 04:20:19,304] A3C_AGENT_WORKER-Thread-18 INFO:Local step 297500, global step 4755795: loss 0.0069
[2019-04-04 04:20:19,309] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 297500, global step 4755795: learning rate 0.0005
[2019-04-04 04:20:21,068] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1449934e-30 1.5353673e-27 3.5556068e-27 1.0000000e+00 1.2519237e-31
 1.7391086e-20 3.1219237e-25], sum to 1.0000
[2019-04-04 04:20:21,069] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9882
[2019-04-04 04:20:21,174] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.3, 68.0, 0.0, 0.0, 26.0, 23.02494613938585, -0.1501976648351857, 0.0, 1.0, 45941.47558072583], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 112200.0000, 
sim time next is 112800.0000, 
raw observation next is [-7.300000000000001, 68.0, 0.0, 0.0, 26.0, 22.9563178144438, -0.0741208684241234, 1.0, 1.0, 202333.0902057569], 
processed observation next is [1.0, 0.30434782608695654, 0.26038781163434904, 0.68, 0.0, 0.0, 0.6666666666666666, 0.4130264845369833, 0.47529304385862553, 1.0, 1.0, 0.9634909057416995], 
reward next is 0.0365, 
noisyNet noise sample is [array([0.49155247], dtype=float32), 0.10094887]. 
=============================================
[2019-04-04 04:20:21,283] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:20:21,284] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:20:21,288] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run36
[2019-04-04 04:20:21,756] A3C_AGENT_WORKER-Thread-17 INFO:Local step 298000, global step 4756549: loss 0.0573
[2019-04-04 04:20:21,757] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 298000, global step 4756549: learning rate 0.0005
[2019-04-04 04:20:21,851] A3C_AGENT_WORKER-Thread-20 INFO:Local step 297500, global step 4756575: loss 0.0134
[2019-04-04 04:20:21,853] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 297500, global step 4756575: learning rate 0.0005
[2019-04-04 04:20:22,133] A3C_AGENT_WORKER-Thread-10 INFO:Local step 297500, global step 4756662: loss 0.0030
[2019-04-04 04:20:22,134] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 297500, global step 4756662: learning rate 0.0005
[2019-04-04 04:20:22,682] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.7963051e-30 7.9712250e-26 8.8524282e-28 1.0000000e+00 1.7317118e-30
 6.7714649e-27 2.0657585e-24], sum to 1.0000
[2019-04-04 04:20:22,682] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6360
[2019-04-04 04:20:22,764] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-12.06666666666667, 51.0, 57.5, 902.0, 26.0, 24.62816441453691, 0.3597100811265928, 1.0, 1.0, 201846.2065854692], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 391200.0000, 
sim time next is 391800.0000, 
raw observation next is [-11.88333333333333, 51.0, 57.0, 899.0, 26.0, 25.36105121172084, 0.431975447317784, 1.0, 1.0, 18774.16758703966], 
processed observation next is [1.0, 0.5217391304347826, 0.13342566943674988, 0.51, 0.19, 0.9933701657458563, 0.6666666666666666, 0.6134209343100702, 0.6439918157725947, 1.0, 1.0, 0.0894007980335222], 
reward next is 0.9106, 
noisyNet noise sample is [array([-1.8919697], dtype=float32), -1.1120118]. 
=============================================
[2019-04-04 04:20:23,894] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.3327557e-26 3.4769052e-24 1.4218827e-23 1.0000000e+00 1.1767580e-27
 1.7746867e-22 4.7379077e-22], sum to 1.0000
[2019-04-04 04:20:23,895] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9171
[2019-04-04 04:20:23,912] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 34.0, 0.0, 0.0, 26.0, 24.83291599145639, 0.2318680516785473, 0.0, 1.0, 40305.52513096506], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4078800.0000, 
sim time next is 4079400.0000, 
raw observation next is [-4.0, 34.66666666666667, 0.0, 0.0, 26.0, 24.83146879958473, 0.2338931890171138, 0.0, 1.0, 40250.97794963673], 
processed observation next is [1.0, 0.21739130434782608, 0.3518005540166205, 0.34666666666666673, 0.0, 0.0, 0.6666666666666666, 0.5692890666320608, 0.577964396339038, 0.0, 1.0, 0.1916713235696987], 
reward next is 0.8083, 
noisyNet noise sample is [array([0.9936152], dtype=float32), 0.7536846]. 
=============================================
[2019-04-04 04:20:24,264] A3C_AGENT_WORKER-Thread-3 INFO:Local step 297500, global step 4757390: loss 0.0521
[2019-04-04 04:20:24,265] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 297500, global step 4757390: learning rate 0.0005
[2019-04-04 04:20:25,585] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:20:25,585] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:20:25,595] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run36
[2019-04-04 04:20:29,241] A3C_AGENT_WORKER-Thread-6 INFO:Local step 297500, global step 4758709: loss 0.0287
[2019-04-04 04:20:29,255] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 297500, global step 4758709: learning rate 0.0005
[2019-04-04 04:20:30,042] A3C_AGENT_WORKER-Thread-2 INFO:Local step 298500, global step 4758943: loss 0.0116
[2019-04-04 04:20:30,043] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 298500, global step 4758943: learning rate 0.0005
[2019-04-04 04:20:30,105] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:20:30,105] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:20:30,108] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run36
[2019-04-04 04:20:35,566] A3C_AGENT_WORKER-Thread-5 INFO:Local step 297500, global step 4760453: loss 0.0010
[2019-04-04 04:20:35,570] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 297500, global step 4760453: learning rate 0.0005
[2019-04-04 04:20:36,390] A3C_AGENT_WORKER-Thread-11 INFO:Local step 298000, global step 4760681: loss 0.0507
[2019-04-04 04:20:36,391] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 298000, global step 4760681: learning rate 0.0005
[2019-04-04 04:20:38,530] A3C_AGENT_WORKER-Thread-15 INFO:Local step 298000, global step 4761386: loss 0.4670
[2019-04-04 04:20:38,532] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 298000, global step 4761387: learning rate 0.0005
[2019-04-04 04:20:39,542] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.6115413e-25 5.1831459e-23 1.4933996e-21 1.0000000e+00 1.6990050e-26
 8.4849951e-20 2.1977182e-20], sum to 1.0000
[2019-04-04 04:20:39,545] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1753
[2019-04-04 04:20:39,596] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-11.1, 48.5, 55.0, 887.0, 23.0, 23.22523725599907, -0.1966622712319661, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 394200.0000, 
sim time next is 394800.0000, 
raw observation next is [-10.9, 47.66666666666667, 53.83333333333334, 877.8333333333334, 23.0, 23.32214302426454, -0.1841292600403747, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.16066481994459833, 0.47666666666666674, 0.1794444444444445, 0.9699815837937386, 0.4166666666666667, 0.44351191868871176, 0.4386235799865417, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.69710654], dtype=float32), 1.6844318]. 
=============================================
[2019-04-04 04:20:40,243] A3C_AGENT_WORKER-Thread-12 INFO:Local step 297500, global step 4761930: loss 0.0072
[2019-04-04 04:20:40,244] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 297500, global step 4761930: learning rate 0.0005
[2019-04-04 04:20:44,239] A3C_AGENT_WORKER-Thread-16 INFO:Local step 297500, global step 4763090: loss 0.0064
[2019-04-04 04:20:44,243] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 297500, global step 4763090: learning rate 0.0005
[2019-04-04 04:20:45,993] A3C_AGENT_WORKER-Thread-19 INFO:Local step 298500, global step 4763658: loss 0.0155
[2019-04-04 04:20:45,994] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 298500, global step 4763658: learning rate 0.0005
[2019-04-04 04:20:46,450] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.5185852e-26 3.5788972e-23 9.0339002e-24 1.0000000e+00 6.6533738e-27
 6.5268552e-26 8.5592417e-21], sum to 1.0000
[2019-04-04 04:20:46,452] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9858
[2019-04-04 04:20:46,479] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-16.2, 78.0, 0.0, 0.0, 26.0, 21.88306161022304, -0.4628429377000138, 0.0, 1.0, 48768.20475274852], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 367200.0000, 
sim time next is 367800.0000, 
raw observation next is [-16.2, 78.0, 0.0, 0.0, 26.0, 21.77947334447889, -0.4680428838150696, 0.0, 1.0, 48880.89179750951], 
processed observation next is [1.0, 0.2608695652173913, 0.013850415512465375, 0.78, 0.0, 0.0, 0.6666666666666666, 0.3149561120399076, 0.3439857053949768, 0.0, 1.0, 0.23276615141671195], 
reward next is 0.7672, 
noisyNet noise sample is [array([-0.67117506], dtype=float32), -0.6778037]. 
=============================================
[2019-04-04 04:20:48,615] A3C_AGENT_WORKER-Thread-18 INFO:Local step 298000, global step 4764501: loss 0.6358
[2019-04-04 04:20:48,616] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 298000, global step 4764501: learning rate 0.0005
[2019-04-04 04:20:49,635] A3C_AGENT_WORKER-Thread-20 INFO:Local step 298000, global step 4764782: loss 0.0201
[2019-04-04 04:20:49,635] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 298000, global step 4764782: learning rate 0.0005
[2019-04-04 04:20:50,110] A3C_AGENT_WORKER-Thread-17 INFO:Local step 298500, global step 4764928: loss 0.0593
[2019-04-04 04:20:50,112] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 298500, global step 4764928: learning rate 0.0005
[2019-04-04 04:20:50,284] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.9732076e-27 1.2975313e-23 3.4603127e-24 1.0000000e+00 8.2358396e-27
 2.8920372e-17 4.1258453e-22], sum to 1.0000
[2019-04-04 04:20:50,284] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3398
[2019-04-04 04:20:50,341] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 65.0, 174.0, 246.0, 26.0, 25.55589814804758, 0.3864083846088384, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4870800.0000, 
sim time next is 4871400.0000, 
raw observation next is [-2.833333333333333, 64.16666666666667, 185.0, 223.6666666666667, 26.0, 25.55535973893937, 0.3809581533574664, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.3841181902123731, 0.6416666666666667, 0.6166666666666667, 0.24714548802946598, 0.6666666666666666, 0.6296133115782808, 0.6269860511191555, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.09068342], dtype=float32), -0.5268245]. 
=============================================
[2019-04-04 04:20:50,407] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.0293654e-31 5.6922752e-28 9.1252506e-28 1.0000000e+00 5.4828695e-32
 4.6731303e-28 1.2468660e-25], sum to 1.0000
[2019-04-04 04:20:50,408] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7002
[2019-04-04 04:20:50,418] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.5, 46.66666666666667, 87.5, 763.1666666666667, 26.0, 25.55647975166634, 0.4066727490452769, 1.0, 1.0, 18680.42852107725], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 740400.0000, 
sim time next is 741000.0000, 
raw observation next is [0.5, 45.83333333333333, 86.0, 753.3333333333333, 26.0, 25.70825404042836, 0.4210477396544034, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.4764542936288089, 0.45833333333333326, 0.2866666666666667, 0.8324125230202577, 0.6666666666666666, 0.6423545033690301, 0.6403492465514679, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.65282726], dtype=float32), 1.2482904]. 
=============================================
[2019-04-04 04:20:50,453] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[83.13197]
 [83.68449]
 [83.87794]
 [84.10499]
 [84.36285]], R is [[82.82421112]
 [82.90702057]
 [82.98899078]
 [83.07015228]
 [83.15049744]].
[2019-04-04 04:20:50,644] A3C_AGENT_WORKER-Thread-10 INFO:Local step 298000, global step 4765095: loss 0.2395
[2019-04-04 04:20:50,645] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 298000, global step 4765095: learning rate 0.0005
[2019-04-04 04:20:54,336] A3C_AGENT_WORKER-Thread-2 INFO:Local step 299000, global step 4766283: loss 1.0664
[2019-04-04 04:20:54,336] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 299000, global step 4766283: learning rate 0.0005
[2019-04-04 04:20:54,507] A3C_AGENT_WORKER-Thread-3 INFO:Local step 298000, global step 4766351: loss 0.3757
[2019-04-04 04:20:54,512] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 298000, global step 4766351: learning rate 0.0005
[2019-04-04 04:20:56,132] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.1205632e-28 1.2159218e-24 3.3927352e-25 1.0000000e+00 1.1512840e-28
 5.0790870e-19 8.9541014e-23], sum to 1.0000
[2019-04-04 04:20:56,132] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5412
[2019-04-04 04:20:56,146] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [14.4, 81.0, 0.0, 0.0, 26.0, 25.9584367649477, 0.5419906942144977, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1016400.0000, 
sim time next is 1017000.0000, 
raw observation next is [14.4, 81.0, 0.0, 0.0, 26.0, 25.77605268248397, 0.5309003292352171, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.8614958448753465, 0.81, 0.0, 0.0, 0.6666666666666666, 0.6480043902069976, 0.6769667764117391, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0767483], dtype=float32), 0.031222785]. 
=============================================
[2019-04-04 04:20:56,186] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[94.87565]
 [94.85489]
 [94.91907]
 [94.97427]
 [95.06095]], R is [[94.86951447]
 [94.92082214]
 [94.97161102]
 [95.02189636]
 [95.07167816]].
[2019-04-04 04:20:57,743] A3C_AGENT_WORKER-Thread-6 INFO:Local step 298000, global step 4767371: loss 1.1553
[2019-04-04 04:20:57,744] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 298000, global step 4767371: learning rate 0.0005
[2019-04-04 04:21:03,917] A3C_AGENT_WORKER-Thread-11 INFO:Local step 298500, global step 4769515: loss 0.0197
[2019-04-04 04:21:03,919] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 298500, global step 4769515: learning rate 0.0005
[2019-04-04 04:21:04,604] A3C_AGENT_WORKER-Thread-5 INFO:Local step 298000, global step 4769742: loss 0.3683
[2019-04-04 04:21:04,604] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 298000, global step 4769742: learning rate 0.0005
[2019-04-04 04:21:05,411] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:21:05,411] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:21:05,431] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run36
[2019-04-04 04:21:06,000] A3C_AGENT_WORKER-Thread-15 INFO:Local step 298500, global step 4770192: loss 0.0113
[2019-04-04 04:21:06,002] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 298500, global step 4770192: learning rate 0.0005
[2019-04-04 04:21:09,483] A3C_AGENT_WORKER-Thread-2 INFO:Local step 299500, global step 4771266: loss 0.1109
[2019-04-04 04:21:09,485] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 299500, global step 4771266: learning rate 0.0005
[2019-04-04 04:21:09,608] A3C_AGENT_WORKER-Thread-12 INFO:Local step 298000, global step 4771306: loss 0.6594
[2019-04-04 04:21:09,608] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 298000, global step 4771306: learning rate 0.0005
[2019-04-04 04:21:09,666] A3C_AGENT_WORKER-Thread-19 INFO:Local step 299000, global step 4771328: loss 0.1146
[2019-04-04 04:21:09,670] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 299000, global step 4771329: learning rate 0.0005
[2019-04-04 04:21:11,448] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.9262978e-30 2.3664236e-26 4.9580256e-26 1.0000000e+00 3.1085805e-31
 3.1003680e-27 3.2946188e-24], sum to 1.0000
[2019-04-04 04:21:11,448] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1951
[2019-04-04 04:21:11,493] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 51.0, 0.0, 0.0, 26.0, 25.46582008222997, 0.4055957337677551, 0.0, 1.0, 51409.59857856154], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4827600.0000, 
sim time next is 4828200.0000, 
raw observation next is [-0.1666666666666667, 51.66666666666666, 0.0, 0.0, 26.0, 25.4301693022985, 0.4034953832443307, 0.0, 1.0, 62432.34801487844], 
processed observation next is [0.0, 0.9130434782608695, 0.4579870729455217, 0.5166666666666666, 0.0, 0.0, 0.6666666666666666, 0.6191807751915416, 0.6344984610814436, 0.0, 1.0, 0.29729689530894493], 
reward next is 0.7027, 
noisyNet noise sample is [array([0.25866273], dtype=float32), 1.3101723]. 
=============================================
[2019-04-04 04:21:12,201] A3C_AGENT_WORKER-Thread-16 INFO:Local step 298000, global step 4772157: loss 0.1330
[2019-04-04 04:21:12,201] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 298000, global step 4772157: learning rate 0.0005
[2019-04-04 04:21:13,778] A3C_AGENT_WORKER-Thread-17 INFO:Local step 299000, global step 4772688: loss 0.1133
[2019-04-04 04:21:13,779] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 299000, global step 4772688: learning rate 0.0005
[2019-04-04 04:21:14,758] A3C_AGENT_WORKER-Thread-20 INFO:Local step 298500, global step 4773016: loss 0.0409
[2019-04-04 04:21:14,759] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 298500, global step 4773017: learning rate 0.0005
[2019-04-04 04:21:15,306] A3C_AGENT_WORKER-Thread-18 INFO:Local step 298500, global step 4773223: loss 0.0108
[2019-04-04 04:21:15,308] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 298500, global step 4773223: learning rate 0.0005
[2019-04-04 04:21:18,097] A3C_AGENT_WORKER-Thread-10 INFO:Local step 298500, global step 4774234: loss 0.0018
[2019-04-04 04:21:18,098] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 298500, global step 4774234: learning rate 0.0005
[2019-04-04 04:21:18,752] A3C_AGENT_WORKER-Thread-14 INFO:Local step 297500, global step 4774547: loss 0.0105
[2019-04-04 04:21:18,755] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 297500, global step 4774547: learning rate 0.0005
[2019-04-04 04:21:21,520] A3C_AGENT_WORKER-Thread-3 INFO:Local step 298500, global step 4775567: loss 0.0007
[2019-04-04 04:21:21,521] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 298500, global step 4775567: learning rate 0.0005
[2019-04-04 04:21:22,517] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.00417811e-34 1.27893586e-31 3.68964784e-29 1.00000000e+00
 1.38399192e-34 1.10269142e-18 1.41566665e-27], sum to 1.0000
[2019-04-04 04:21:22,519] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7634
[2019-04-04 04:21:22,546] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.066666666666666, 51.0, 56.66666666666667, 2.833333333333333, 26.0, 25.66415858256271, 0.421801088952211, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 751200.0000, 
sim time next is 751800.0000, 
raw observation next is [-2.433333333333334, 52.5, 45.33333333333334, 2.666666666666667, 26.0, 25.86502999301536, 0.4340417299225217, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.3951985226223454, 0.525, 0.15111111111111114, 0.002946593001841621, 0.6666666666666666, 0.6554191660846133, 0.6446805766408406, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4520333], dtype=float32), -1.1392965]. 
=============================================
[2019-04-04 04:21:23,570] A3C_AGENT_WORKER-Thread-19 INFO:Local step 299500, global step 4776407: loss 0.0586
[2019-04-04 04:21:23,575] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 299500, global step 4776407: learning rate 0.0005
[2019-04-04 04:21:24,295] A3C_AGENT_WORKER-Thread-6 INFO:Local step 298500, global step 4776690: loss 0.0070
[2019-04-04 04:21:24,295] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 298500, global step 4776690: learning rate 0.0005
[2019-04-04 04:21:26,122] A3C_AGENT_WORKER-Thread-11 INFO:Local step 299000, global step 4777450: loss 0.1167
[2019-04-04 04:21:26,126] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 299000, global step 4777450: learning rate 0.0005
[2019-04-04 04:21:27,636] A3C_AGENT_WORKER-Thread-2 INFO:Local step 300000, global step 4777960: loss 3.0073
[2019-04-04 04:21:27,636] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 300000, global step 4777960: learning rate 0.0005
[2019-04-04 04:21:27,979] A3C_AGENT_WORKER-Thread-17 INFO:Local step 299500, global step 4778098: loss 0.0888
[2019-04-04 04:21:27,980] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 299500, global step 4778098: learning rate 0.0005
[2019-04-04 04:21:28,812] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:21:28,812] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:21:28,818] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run36
[2019-04-04 04:21:30,078] A3C_AGENT_WORKER-Thread-15 INFO:Local step 299000, global step 4778876: loss 0.0122
[2019-04-04 04:21:30,079] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 299000, global step 4778876: learning rate 0.0005
[2019-04-04 04:21:30,689] A3C_AGENT_WORKER-Thread-5 INFO:Local step 298500, global step 4779115: loss 0.0238
[2019-04-04 04:21:30,693] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 298500, global step 4779117: learning rate 0.0005
[2019-04-04 04:21:30,974] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:21:30,974] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:21:30,996] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run36
[2019-04-04 04:21:31,667] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3633441e-22 1.0497013e-19 6.7910748e-19 1.0000000e+00 3.1120918e-24
 9.7929572e-12 3.9030494e-17], sum to 1.0000
[2019-04-04 04:21:31,668] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6630
[2019-04-04 04:21:31,757] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.1, 93.0, 90.0, 0.0, 26.0, 25.85291277567012, 0.289881070646705, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 916200.0000, 
sim time next is 916800.0000, 
raw observation next is [4.2, 93.0, 84.0, 0.0, 26.0, 25.44841931346365, 0.3062927211812262, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5789473684210527, 0.93, 0.28, 0.0, 0.6666666666666666, 0.6207016094553041, 0.6020975737270754, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.07072741], dtype=float32), -0.9253109]. 
=============================================
[2019-04-04 04:21:35,752] A3C_AGENT_WORKER-Thread-12 INFO:Local step 298500, global step 4780777: loss 0.0221
[2019-04-04 04:21:35,754] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 298500, global step 4780778: learning rate 0.0005
[2019-04-04 04:21:36,737] A3C_AGENT_WORKER-Thread-20 INFO:Local step 299000, global step 4781187: loss 0.0103
[2019-04-04 04:21:36,738] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 299000, global step 4781187: learning rate 0.0005
[2019-04-04 04:21:37,269] A3C_AGENT_WORKER-Thread-16 INFO:Local step 298500, global step 4781411: loss 0.0067
[2019-04-04 04:21:37,269] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 298500, global step 4781411: learning rate 0.0005
[2019-04-04 04:21:37,683] A3C_AGENT_WORKER-Thread-18 INFO:Local step 299000, global step 4781590: loss 0.0090
[2019-04-04 04:21:37,684] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 299000, global step 4781590: learning rate 0.0005
[2019-04-04 04:21:37,715] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.5806197e-28 3.4141441e-25 8.8575803e-24 1.0000000e+00 6.6596929e-28
 1.8235893e-16 2.1046486e-22], sum to 1.0000
[2019-04-04 04:21:37,725] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1404
[2019-04-04 04:21:37,739] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.55, 71.0, 0.0, 0.0, 26.0, 24.20707681481521, 0.1122383570714184, 0.0, 1.0, 41562.46441943449], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 783000.0000, 
sim time next is 783600.0000, 
raw observation next is [-7.633333333333333, 71.0, 0.0, 0.0, 26.0, 24.19574572064132, 0.101647270157255, 0.0, 1.0, 41543.24822172222], 
processed observation next is [1.0, 0.043478260869565216, 0.2511542012927055, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5163121433867767, 0.5338824233857516, 0.0, 1.0, 0.19782499153201058], 
reward next is 0.8022, 
noisyNet noise sample is [array([-0.24044788], dtype=float32), -0.33471245]. 
=============================================
[2019-04-04 04:21:37,962] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.3040640e-28 2.7450707e-24 4.4494051e-24 1.0000000e+00 1.9778686e-27
 3.1882062e-22 1.8269807e-23], sum to 1.0000
[2019-04-04 04:21:37,962] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7473
[2019-04-04 04:21:38,045] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.7, 85.66666666666667, 21.83333333333334, 0.0, 26.0, 24.66076180207481, 0.3363772613399079, 0.0, 1.0, 100409.7405690688], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1758000.0000, 
sim time next is 1758600.0000, 
raw observation next is [-1.7, 85.0, 26.0, 0.0, 26.0, 24.88598736121136, 0.3681369878144118, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.4155124653739613, 0.85, 0.08666666666666667, 0.0, 0.6666666666666666, 0.5738322801009467, 0.6227123292714706, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4152573], dtype=float32), -0.045259498]. 
=============================================
[2019-04-04 04:21:39,694] A3C_AGENT_WORKER-Thread-11 INFO:Local step 299500, global step 4782486: loss 0.1492
[2019-04-04 04:21:39,694] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 299500, global step 4782486: learning rate 0.0005
[2019-04-04 04:21:39,877] A3C_AGENT_WORKER-Thread-10 INFO:Local step 299000, global step 4782566: loss 0.0098
[2019-04-04 04:21:39,881] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 299000, global step 4782566: learning rate 0.0005
[2019-04-04 04:21:41,233] A3C_AGENT_WORKER-Thread-19 INFO:Local step 300000, global step 4783179: loss 3.2527
[2019-04-04 04:21:41,234] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 300000, global step 4783179: learning rate 0.0005
[2019-04-04 04:21:41,637] A3C_AGENT_WORKER-Thread-4 INFO:Local step 297500, global step 4783336: loss 0.0024
[2019-04-04 04:21:41,638] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 297500, global step 4783336: learning rate 0.0005
[2019-04-04 04:21:43,060] A3C_AGENT_WORKER-Thread-3 INFO:Local step 299000, global step 4784057: loss 0.0095
[2019-04-04 04:21:43,065] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 299000, global step 4784060: learning rate 0.0005
[2019-04-04 04:21:43,381] A3C_AGENT_WORKER-Thread-15 INFO:Local step 299500, global step 4784214: loss 0.0812
[2019-04-04 04:21:43,382] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 299500, global step 4784215: learning rate 0.0005
[2019-04-04 04:21:43,534] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.8795859e-25 3.0989143e-19 4.1597162e-20 1.0000000e+00 2.5486742e-23
 3.1193628e-11 6.4097230e-20], sum to 1.0000
[2019-04-04 04:21:43,534] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9143
[2019-04-04 04:21:43,550] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.5, 100.0, 0.0, 0.0, 26.0, 25.42711556798636, 0.5912374147658318, 0.0, 1.0, 46768.0883453772], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1288200.0000, 
sim time next is 1288800.0000, 
raw observation next is [5.5, 100.0, 0.0, 0.0, 26.0, 25.43543044195892, 0.5934552550283335, 0.0, 1.0, 35680.67631430607], 
processed observation next is [0.0, 0.9565217391304348, 0.6149584487534627, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6196192034965767, 0.6978184183427779, 0.0, 1.0, 0.1699079824490765], 
reward next is 0.8301, 
noisyNet noise sample is [array([0.19826607], dtype=float32), -0.4211052]. 
=============================================
[2019-04-04 04:21:43,683] A3C_AGENT_WORKER-Thread-13 INFO:Local step 297500, global step 4784358: loss 0.0007
[2019-04-04 04:21:43,686] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 297500, global step 4784359: learning rate 0.0005
[2019-04-04 04:21:44,926] A3C_AGENT_WORKER-Thread-14 INFO:Local step 298000, global step 4784987: loss 0.1880
[2019-04-04 04:21:44,932] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 298000, global step 4784987: learning rate 0.0005
[2019-04-04 04:21:44,998] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.6580788e-20 4.7941095e-17 6.9549367e-15 1.1000923e-01 2.4329576e-18
 8.8999075e-01 1.1168187e-14], sum to 1.0000
[2019-04-04 04:21:44,998] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9282
[2019-04-04 04:21:45,012] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.583333333333333, 92.0, 10.66666666666666, 0.0, 26.0, 25.53719876594673, 0.5072792244895599, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1669800.0000, 
sim time next is 1670400.0000, 
raw observation next is [3.3, 92.0, 15.5, 0.0, 26.0, 25.49241657920415, 0.4995744727407754, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.554016620498615, 0.92, 0.051666666666666666, 0.0, 0.6666666666666666, 0.6243680482670125, 0.6665248242469252, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7039195], dtype=float32), 1.5522983]. 
=============================================
[2019-04-04 04:21:45,460] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.3039709e-25 6.3973075e-22 5.4605456e-21 1.0000000e+00 4.0580173e-25
 1.9381248e-10 1.7049118e-20], sum to 1.0000
[2019-04-04 04:21:45,461] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2717
[2019-04-04 04:21:45,485] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 24.45700019955831, 0.1421779649004583, 0.0, 1.0, 38682.75098149259], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 886800.0000, 
sim time next is 887400.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 24.41654042017956, 0.143810491629538, 0.0, 1.0, 38639.86698131965], 
processed observation next is [1.0, 0.2608695652173913, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5347117016816302, 0.5479368305431793, 0.0, 1.0, 0.1839993665777126], 
reward next is 0.8160, 
noisyNet noise sample is [array([-0.00859887], dtype=float32), 1.0499107]. 
=============================================
[2019-04-04 04:21:45,541] A3C_AGENT_WORKER-Thread-6 INFO:Local step 299000, global step 4785273: loss 0.0030
[2019-04-04 04:21:45,542] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 299000, global step 4785274: learning rate 0.0005
[2019-04-04 04:21:45,794] A3C_AGENT_WORKER-Thread-17 INFO:Local step 300000, global step 4785397: loss 2.8668
[2019-04-04 04:21:45,813] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 300000, global step 4785404: learning rate 0.0005
[2019-04-04 04:21:48,640] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.1714803e-21 2.0289415e-18 1.1269108e-15 1.0000000e+00 8.6300868e-22
 1.0305317e-08 4.7855678e-16], sum to 1.0000
[2019-04-04 04:21:48,643] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5156
[2019-04-04 04:21:48,693] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.3, 69.33333333333334, 0.0, 0.0, 26.0, 25.456044242892, 0.3715740510604283, 1.0, 1.0, 59009.52833023287], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 148200.0000, 
sim time next is 148800.0000, 
raw observation next is [-7.300000000000001, 67.66666666666667, 0.0, 0.0, 26.0, 25.32914626621604, 0.3185423779330888, 1.0, 1.0, 47248.25680214756], 
processed observation next is [1.0, 0.7391304347826086, 0.26038781163434904, 0.6766666666666667, 0.0, 0.0, 0.6666666666666666, 0.6107621888513366, 0.6061807926443629, 1.0, 1.0, 0.22499169905784552], 
reward next is 0.7750, 
noisyNet noise sample is [array([0.01516764], dtype=float32), -1.5811722]. 
=============================================
[2019-04-04 04:21:48,777] A3C_AGENT_WORKER-Thread-2 INFO:Local step 300500, global step 4786721: loss 0.1309
[2019-04-04 04:21:48,777] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 300500, global step 4786721: learning rate 0.0005
[2019-04-04 04:21:49,445] A3C_AGENT_WORKER-Thread-18 INFO:Local step 299500, global step 4787058: loss 0.0957
[2019-04-04 04:21:49,447] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 299500, global step 4787058: learning rate 0.0005
[2019-04-04 04:21:49,734] A3C_AGENT_WORKER-Thread-20 INFO:Local step 299500, global step 4787232: loss 0.0900
[2019-04-04 04:21:49,737] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 299500, global step 4787233: learning rate 0.0005
[2019-04-04 04:21:51,398] A3C_AGENT_WORKER-Thread-5 INFO:Local step 299000, global step 4788061: loss 0.0009
[2019-04-04 04:21:51,400] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 299000, global step 4788063: learning rate 0.0005
[2019-04-04 04:21:52,190] A3C_AGENT_WORKER-Thread-10 INFO:Local step 299500, global step 4788387: loss 0.1184
[2019-04-04 04:21:52,191] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 299500, global step 4788387: learning rate 0.0005
[2019-04-04 04:21:52,675] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.6158812e-23 1.3258633e-20 6.7955873e-18 1.0000000e+00 4.3890731e-23
 1.0334845e-15 1.6593576e-17], sum to 1.0000
[2019-04-04 04:21:52,675] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2251
[2019-04-04 04:21:52,735] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.816666666666666, 65.0, 228.6666666666667, 6.0, 26.0, 25.76092904120306, 0.3513761537672153, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1944600.0000, 
sim time next is 1945200.0000, 
raw observation next is [-4.633333333333334, 65.0, 227.8333333333333, 5.0, 26.0, 25.80596292498069, 0.329532936430356, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.3342566943674977, 0.65, 0.7594444444444443, 0.0055248618784530384, 0.6666666666666666, 0.6504969104150575, 0.609844312143452, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.1182718], dtype=float32), 1.6406343]. 
=============================================
[2019-04-04 04:21:53,761] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.4980231e-24 5.0697508e-21 2.5666398e-20 9.9999988e-01 6.7725073e-24
 1.6562450e-07 4.4679039e-19], sum to 1.0000
[2019-04-04 04:21:53,762] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1191
[2019-04-04 04:21:53,780] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 26.0, 25.4915448020177, 0.5352444653310309, 0.0, 1.0, 56841.75353851047], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1316400.0000, 
sim time next is 1317000.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 26.0, 25.50724714577319, 0.5343518639049109, 0.0, 1.0, 39822.86672318114], 
processed observation next is [1.0, 0.21739130434782608, 0.5069252077562327, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6256039288144324, 0.6781172879683036, 0.0, 1.0, 0.18963269868181495], 
reward next is 0.8104, 
noisyNet noise sample is [array([0.41173193], dtype=float32), 0.9686443]. 
=============================================
[2019-04-04 04:21:53,798] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[88.25913]
 [88.26561]
 [88.29222]
 [88.3497 ]
 [88.3551 ]], R is [[88.30794525]
 [88.15419006]
 [88.27265167]
 [88.3899231 ]
 [88.50602722]].
[2019-04-04 04:21:53,842] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.42107788e-26 4.35498362e-23 1.14968935e-23 1.00000000e+00
 1.25102049e-25 5.50650450e-20 1.14344472e-21], sum to 1.0000
[2019-04-04 04:21:53,843] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3327
[2019-04-04 04:21:53,849] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [13.8, 100.0, 72.66666666666667, 0.0, 26.0, 24.77374918389576, 0.4501843839960374, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1262400.0000, 
sim time next is 1263000.0000, 
raw observation next is [13.8, 100.0, 68.33333333333333, 0.0, 26.0, 24.763804169685, 0.4461490698988539, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.844875346260388, 1.0, 0.22777777777777777, 0.0, 0.6666666666666666, 0.5636503474737499, 0.6487163566329514, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5622914], dtype=float32), -0.46184212]. 
=============================================
[2019-04-04 04:21:53,857] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[92.00507]
 [92.07842]
 [92.10808]
 [92.15474]
 [92.22327]], R is [[92.02205658]
 [92.10183716]
 [92.18081665]
 [92.25901031]
 [92.33641815]].
[2019-04-04 04:21:55,103] A3C_AGENT_WORKER-Thread-3 INFO:Local step 299500, global step 4789634: loss 0.0618
[2019-04-04 04:21:55,109] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 299500, global step 4789634: learning rate 0.0005
[2019-04-04 04:21:56,100] A3C_AGENT_WORKER-Thread-12 INFO:Local step 299000, global step 4790134: loss 0.0336
[2019-04-04 04:21:56,102] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 299000, global step 4790135: learning rate 0.0005
[2019-04-04 04:21:56,796] A3C_AGENT_WORKER-Thread-11 INFO:Local step 300000, global step 4790464: loss 4.1357
[2019-04-04 04:21:56,799] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 300000, global step 4790464: learning rate 0.0005
[2019-04-04 04:21:57,004] A3C_AGENT_WORKER-Thread-16 INFO:Local step 299000, global step 4790538: loss 0.0102
[2019-04-04 04:21:57,004] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 299000, global step 4790538: learning rate 0.0005
[2019-04-04 04:21:57,295] A3C_AGENT_WORKER-Thread-6 INFO:Local step 299500, global step 4790665: loss -0.9885
[2019-04-04 04:21:57,297] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 299500, global step 4790665: learning rate 0.0005
[2019-04-04 04:22:00,220] A3C_AGENT_WORKER-Thread-15 INFO:Local step 300000, global step 4791975: loss 6.3222
[2019-04-04 04:22:00,221] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 300000, global step 4791975: learning rate 0.0005
[2019-04-04 04:22:00,676] A3C_AGENT_WORKER-Thread-19 INFO:Local step 300500, global step 4792203: loss 0.1694
[2019-04-04 04:22:00,677] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 300500, global step 4792203: learning rate 0.0005
[2019-04-04 04:22:03,433] A3C_AGENT_WORKER-Thread-5 INFO:Local step 299500, global step 4793452: loss 0.1306
[2019-04-04 04:22:03,435] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 299500, global step 4793453: learning rate 0.0005
[2019-04-04 04:22:04,001] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.2952953e-21 5.2797883e-17 1.1144698e-16 1.0000000e+00 2.7935217e-20
 3.7353720e-09 1.3323514e-15], sum to 1.0000
[2019-04-04 04:22:04,002] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0455
[2019-04-04 04:22:04,019] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.283333333333333, 86.33333333333333, 0.0, 0.0, 26.0, 24.11540178420417, 0.1041127136017542, 0.0, 1.0, 43822.8511415641], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2092200.0000, 
sim time next is 2092800.0000, 
raw observation next is [-6.366666666666667, 85.66666666666667, 0.0, 0.0, 26.0, 24.17867859340522, 0.1069448545630888, 0.0, 1.0, 43565.19726025594], 
processed observation next is [1.0, 0.21739130434782608, 0.28624192059095105, 0.8566666666666667, 0.0, 0.0, 0.6666666666666666, 0.5148898827837683, 0.535648284854363, 0.0, 1.0, 0.20745332028693306], 
reward next is 0.7925, 
noisyNet noise sample is [array([1.1094209], dtype=float32), -0.20852518]. 
=============================================
[2019-04-04 04:22:05,751] A3C_AGENT_WORKER-Thread-18 INFO:Local step 300000, global step 4794661: loss 6.0898
[2019-04-04 04:22:05,752] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 300000, global step 4794662: learning rate 0.0005
[2019-04-04 04:22:05,835] A3C_AGENT_WORKER-Thread-17 INFO:Local step 300500, global step 4794712: loss 0.2356
[2019-04-04 04:22:05,836] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 300500, global step 4794712: learning rate 0.0005
[2019-04-04 04:22:06,302] A3C_AGENT_WORKER-Thread-20 INFO:Local step 300000, global step 4794966: loss 6.8911
[2019-04-04 04:22:06,302] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 300000, global step 4794966: learning rate 0.0005
[2019-04-04 04:22:06,387] A3C_AGENT_WORKER-Thread-4 INFO:Local step 298000, global step 4795007: loss 0.0397
[2019-04-04 04:22:06,389] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 298000, global step 4795007: learning rate 0.0005
[2019-04-04 04:22:08,474] A3C_AGENT_WORKER-Thread-13 INFO:Local step 298000, global step 4795457: loss 0.0160
[2019-04-04 04:22:08,475] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 298000, global step 4795457: learning rate 0.0005
[2019-04-04 04:22:09,366] A3C_AGENT_WORKER-Thread-12 INFO:Local step 299500, global step 4795641: loss -0.7574
[2019-04-04 04:22:09,370] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 299500, global step 4795641: learning rate 0.0005
[2019-04-04 04:22:10,780] A3C_AGENT_WORKER-Thread-16 INFO:Local step 299500, global step 4796006: loss 0.1345
[2019-04-04 04:22:10,782] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 299500, global step 4796007: learning rate 0.0005
[2019-04-04 04:22:10,833] A3C_AGENT_WORKER-Thread-14 INFO:Local step 298500, global step 4796022: loss 0.0636
[2019-04-04 04:22:10,858] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 298500, global step 4796022: learning rate 0.0005
[2019-04-04 04:22:11,021] A3C_AGENT_WORKER-Thread-10 INFO:Local step 300000, global step 4796064: loss 6.3340
[2019-04-04 04:22:11,025] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 300000, global step 4796064: learning rate 0.0005
[2019-04-04 04:22:13,957] A3C_AGENT_WORKER-Thread-2 INFO:Local step 301000, global step 4796695: loss 0.0581
[2019-04-04 04:22:14,037] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 301000, global step 4796695: learning rate 0.0005
[2019-04-04 04:22:17,652] A3C_AGENT_WORKER-Thread-3 INFO:Local step 300000, global step 4797408: loss 5.5866
[2019-04-04 04:22:17,652] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 300000, global step 4797408: learning rate 0.0005
[2019-04-04 04:22:22,509] A3C_AGENT_WORKER-Thread-6 INFO:Local step 300000, global step 4798372: loss 5.3317
[2019-04-04 04:22:22,513] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 300000, global step 4798372: learning rate 0.0005
[2019-04-04 04:22:24,648] A3C_AGENT_WORKER-Thread-11 INFO:Local step 300500, global step 4798762: loss 0.1085
[2019-04-04 04:22:24,650] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 300500, global step 4798762: learning rate 0.0005
[2019-04-04 04:22:31,002] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-04 04:22:31,008] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 04:22:31,008] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:22:31,023] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 04:22:31,023] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:22:31,023] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 04:22:31,025] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:22:31,026] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run49
[2019-04-04 04:22:31,083] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run49
[2019-04-04 04:22:31,139] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run49
[2019-04-04 04:24:16,235] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.2091602], dtype=float32), 0.024420142]
[2019-04-04 04:24:16,235] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-3.9, 82.0, 0.0, 0.0, 26.0, 26.31084809957507, 0.5353487818919968, 1.0, 1.0, 0.0]
[2019-04-04 04:24:16,235] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 04:24:16,236] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.7235757e-18 2.2920642e-14 1.5738785e-14 9.9991012e-01 1.1574087e-17
 8.9884561e-05 6.8105610e-14], sampled 0.7808450475237196
[2019-04-04 04:24:31,870] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.2091602], dtype=float32), 0.024420142]
[2019-04-04 04:24:31,871] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-3.4, 69.0, 25.33333333333334, 0.0, 26.0, 24.04225084937674, 0.06965197497502008, 0.0, 1.0, 41994.14826325521]
[2019-04-04 04:24:31,871] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 04:24:31,871] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.9790442e-22 7.6990838e-19 7.9265280e-19 1.0000000e+00 3.8299578e-22
 2.1149408e-12 9.7121972e-18], sampled 0.6352301738551199
[2019-04-04 04:25:21,391] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.2091602], dtype=float32), 0.024420142]
[2019-04-04 04:25:21,391] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [3.0, 42.5, 0.0, 0.0, 26.0, 25.35879447815959, 0.3804217454233251, 0.0, 1.0, 38349.59034374834]
[2019-04-04 04:25:21,391] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 04:25:21,392] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.2538405e-23 2.3040511e-19 1.5911308e-19 1.0000000e+00 1.9184684e-23
 1.0979600e-12 1.0702257e-18], sampled 0.0763961593192467
[2019-04-04 04:25:31,796] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.2091602], dtype=float32), 0.024420142]
[2019-04-04 04:25:31,796] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [5.166666666666667, 84.66666666666667, 0.0, 0.0, 26.0, 25.97362320870213, 0.5823827165675451, 0.0, 1.0, 0.0]
[2019-04-04 04:25:31,796] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 04:25:31,797] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [2.1615943e-17 4.8522310e-12 1.4276298e-12 5.3800440e-01 1.1582685e-16
 4.6199563e-01 2.0624477e-12], sampled 0.2237744681584568
[2019-04-04 04:25:33,544] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7354.8271 239646316.7057 1604.9210
[2019-04-04 04:26:04,373] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.2091602], dtype=float32), 0.024420142]
[2019-04-04 04:26:04,373] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-1.305447028833333, 83.985245105, 0.0, 0.0, 26.0, 24.94600992280097, 0.3518339653798994, 0.0, 1.0, 41198.45304155241]
[2019-04-04 04:26:04,373] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 04:26:04,374] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [5.0392318e-20 1.1808393e-15 1.8210311e-16 1.0000000e+00 9.0104461e-20
 3.9932413e-08 1.8639320e-15], sampled 0.05139613508717311
[2019-04-04 04:26:09,066] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7242.2714 263283005.3523 1555.9808
[2019-04-04 04:26:15,336] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7183.3750 275651248.4708 1231.2354
[2019-04-04 04:26:16,390] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 4800000, evaluation results [4800000.0, 7242.271403084141, 263283005.35233024, 1555.9807882157436, 7354.827063306185, 239646316.70569882, 1604.9210332400548, 7183.375007281732, 275651248.4708324, 1231.2354248550387]
[2019-04-04 04:26:16,901] A3C_AGENT_WORKER-Thread-15 INFO:Local step 300500, global step 4800093: loss 0.0888
[2019-04-04 04:26:16,901] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 300500, global step 4800093: learning rate 0.0005
[2019-04-04 04:26:21,070] A3C_AGENT_WORKER-Thread-5 INFO:Local step 300000, global step 4800885: loss 11.9933
[2019-04-04 04:26:21,078] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 300000, global step 4800888: learning rate 0.0005
[2019-04-04 04:26:22,779] A3C_AGENT_WORKER-Thread-19 INFO:Local step 301000, global step 4801206: loss 0.0332
[2019-04-04 04:26:22,827] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 301000, global step 4801206: learning rate 0.0005
[2019-04-04 04:26:31,976] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.75345344e-25 3.02123013e-21 5.93460569e-21 1.00000000e+00
 2.38624064e-25 9.09964378e-14 1.05172485e-20], sum to 1.0000
[2019-04-04 04:26:31,976] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3991
[2019-04-04 04:26:32,055] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.116666666666667, 59.16666666666667, 158.6666666666667, 95.33333333333334, 26.0, 24.90809751144895, 0.2413153707931059, 0.0, 1.0, 30013.73222040866], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 652200.0000, 
sim time next is 652800.0000, 
raw observation next is [-1.933333333333333, 59.33333333333334, 170.3333333333333, 94.16666666666666, 26.0, 24.91437748278879, 0.2421423382175256, 0.0, 1.0, 28164.24216983104], 
processed observation next is [0.0, 0.5652173913043478, 0.40904893813481075, 0.5933333333333334, 0.5677777777777776, 0.10405156537753221, 0.6666666666666666, 0.5761981235657325, 0.5807141127391752, 0.0, 1.0, 0.13411543890395733], 
reward next is 0.8659, 
noisyNet noise sample is [array([0.7103144], dtype=float32), 0.70050675]. 
=============================================
[2019-04-04 04:26:32,606] A3C_AGENT_WORKER-Thread-18 INFO:Local step 300500, global step 4802821: loss 0.1246
[2019-04-04 04:26:32,607] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 300500, global step 4802821: learning rate 0.0005
[2019-04-04 04:26:32,708] A3C_AGENT_WORKER-Thread-20 INFO:Local step 300500, global step 4802839: loss 0.1328
[2019-04-04 04:26:32,711] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 300500, global step 4802839: learning rate 0.0005
[2019-04-04 04:26:32,831] A3C_AGENT_WORKER-Thread-17 INFO:Local step 301000, global step 4802862: loss 0.0534
[2019-04-04 04:26:32,841] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 301000, global step 4802862: learning rate 0.0005
[2019-04-04 04:26:33,677] A3C_AGENT_WORKER-Thread-12 INFO:Local step 300000, global step 4803049: loss 6.5364
[2019-04-04 04:26:33,705] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 300000, global step 4803050: learning rate 0.0005
[2019-04-04 04:26:34,613] A3C_AGENT_WORKER-Thread-16 INFO:Local step 300000, global step 4803274: loss 6.2343
[2019-04-04 04:26:34,626] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 300000, global step 4803274: learning rate 0.0005
[2019-04-04 04:26:36,543] A3C_AGENT_WORKER-Thread-4 INFO:Local step 298500, global step 4803638: loss 0.0108
[2019-04-04 04:26:36,543] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 298500, global step 4803638: learning rate 0.0005
[2019-04-04 04:26:38,596] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.8576831e-25 1.9129164e-21 1.8591827e-21 1.0000000e+00 6.3590738e-25
 9.1861967e-13 3.3430999e-21], sum to 1.0000
[2019-04-04 04:26:38,596] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2041
[2019-04-04 04:26:38,657] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.2, 57.0, 13.5, 8.5, 26.0, 24.87709616219654, 0.2134685474601209, 0.0, 1.0, 46847.35615121551], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 666000.0000, 
sim time next is 666600.0000, 
raw observation next is [-1.2, 57.0, 0.0, 0.0, 26.0, 24.88369197549047, 0.2163308305548647, 0.0, 1.0, 40503.71501088434], 
processed observation next is [0.0, 0.7391304347826086, 0.42936288088642666, 0.57, 0.0, 0.0, 0.6666666666666666, 0.5736409979575393, 0.5721102768516216, 0.0, 1.0, 0.19287483338516354], 
reward next is 0.8071, 
noisyNet noise sample is [array([0.64383847], dtype=float32), -1.2731799]. 
=============================================
[2019-04-04 04:26:39,122] A3C_AGENT_WORKER-Thread-10 INFO:Local step 300500, global step 4804069: loss 0.0903
[2019-04-04 04:26:39,237] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 300500, global step 4804069: learning rate 0.0005
[2019-04-04 04:26:39,871] A3C_AGENT_WORKER-Thread-14 INFO:Local step 299000, global step 4804205: loss 0.0195
[2019-04-04 04:26:39,906] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 299000, global step 4804205: learning rate 0.0005
[2019-04-04 04:26:40,845] A3C_AGENT_WORKER-Thread-13 INFO:Local step 298500, global step 4804414: loss 0.0758
[2019-04-04 04:26:40,848] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 298500, global step 4804414: learning rate 0.0005
[2019-04-04 04:26:41,762] A3C_AGENT_WORKER-Thread-2 INFO:Local step 301500, global step 4804594: loss 0.3491
[2019-04-04 04:26:41,763] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 301500, global step 4804594: learning rate 0.0005
[2019-04-04 04:26:44,200] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.38538159e-23 2.64157895e-18 2.14004011e-18 1.00000000e+00
 1.14072127e-22 1.87348292e-09 1.34517865e-17], sum to 1.0000
[2019-04-04 04:26:44,200] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5184
[2019-04-04 04:26:44,254] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.7, 55.33333333333334, 0.0, 0.0, 26.0, 25.03607538264978, 0.3672129403590754, 0.0, 1.0, 180810.2651101294], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2319600.0000, 
sim time next is 2320200.0000, 
raw observation next is [-1.7, 55.0, 0.0, 0.0, 26.0, 25.03978066987146, 0.3891236026105938, 0.0, 1.0, 104520.5042008052], 
processed observation next is [1.0, 0.8695652173913043, 0.4155124653739613, 0.55, 0.0, 0.0, 0.6666666666666666, 0.5866483891559549, 0.6297078675368646, 0.0, 1.0, 0.49771668667050095], 
reward next is 0.5023, 
noisyNet noise sample is [array([-0.09969716], dtype=float32), -0.7645647]. 
=============================================
[2019-04-04 04:26:45,788] A3C_AGENT_WORKER-Thread-3 INFO:Local step 300500, global step 4805332: loss 0.0773
[2019-04-04 04:26:45,789] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 300500, global step 4805332: learning rate 0.0005
[2019-04-04 04:26:50,963] A3C_AGENT_WORKER-Thread-6 INFO:Local step 300500, global step 4806257: loss 0.0723
[2019-04-04 04:26:50,964] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 300500, global step 4806257: learning rate 0.0005
[2019-04-04 04:26:55,020] A3C_AGENT_WORKER-Thread-11 INFO:Local step 301000, global step 4806954: loss 0.1007
[2019-04-04 04:26:55,053] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 301000, global step 4806954: learning rate 0.0005
[2019-04-04 04:26:57,114] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.0560592e-22 1.8150648e-18 3.4316645e-19 1.0000000e+00 4.4926454e-22
 1.6985496e-13 4.0413476e-18], sum to 1.0000
[2019-04-04 04:26:57,149] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8307
[2019-04-04 04:26:57,213] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.2, 87.0, 0.0, 0.0, 26.0, 24.15896974493207, 0.05866153346875932, 0.0, 1.0, 41111.73304465984], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2008200.0000, 
sim time next is 2008800.0000, 
raw observation next is [-6.2, 87.0, 0.0, 0.0, 26.0, 24.13785516096782, 0.05767684615867227, 0.0, 1.0, 41133.03321914889], 
processed observation next is [1.0, 0.2608695652173913, 0.2908587257617729, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5114879300806517, 0.5192256153862241, 0.0, 1.0, 0.19587158675785188], 
reward next is 0.8041, 
noisyNet noise sample is [array([1.0554395], dtype=float32), 1.9412093]. 
=============================================
[2019-04-04 04:27:01,059] A3C_AGENT_WORKER-Thread-15 INFO:Local step 301000, global step 4808167: loss 0.0250
[2019-04-04 04:27:01,071] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 301000, global step 4808167: learning rate 0.0005
[2019-04-04 04:27:01,919] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.8143996e-23 4.9611269e-19 5.5966855e-17 1.0000000e+00 2.1248242e-22
 1.3352665e-08 3.2449630e-17], sum to 1.0000
[2019-04-04 04:27:01,919] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6991
[2019-04-04 04:27:02,041] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.366666666666667, 59.33333333333334, 212.0, 188.3333333333333, 26.0, 25.68352327039444, 0.3836611190043762, 1.0, 1.0, 49827.21662136177], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2632800.0000, 
sim time next is 2633400.0000, 
raw observation next is [-3.1, 58.0, 224.0, 171.0, 26.0, 25.73347641543501, 0.3870077156684564, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.37673130193905824, 0.58, 0.7466666666666667, 0.18895027624309393, 0.6666666666666666, 0.6444563679529175, 0.6290025718894855, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0364186], dtype=float32), 0.18930823]. 
=============================================
[2019-04-04 04:27:03,832] A3C_AGENT_WORKER-Thread-5 INFO:Local step 300500, global step 4808889: loss 0.1286
[2019-04-04 04:27:03,832] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 300500, global step 4808889: learning rate 0.0005
[2019-04-04 04:27:04,869] A3C_AGENT_WORKER-Thread-19 INFO:Local step 301500, global step 4809233: loss 0.2671
[2019-04-04 04:27:04,870] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 301500, global step 4809233: learning rate 0.0005
[2019-04-04 04:27:08,642] A3C_AGENT_WORKER-Thread-14 INFO:Local step 299500, global step 4810390: loss -0.1343
[2019-04-04 04:27:08,643] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 299500, global step 4810390: learning rate 0.0005
[2019-04-04 04:27:09,661] A3C_AGENT_WORKER-Thread-18 INFO:Local step 301000, global step 4810732: loss 0.0930
[2019-04-04 04:27:09,663] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 301000, global step 4810732: learning rate 0.0005
[2019-04-04 04:27:10,687] A3C_AGENT_WORKER-Thread-12 INFO:Local step 300500, global step 4811044: loss 0.1314
[2019-04-04 04:27:10,688] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 300500, global step 4811044: learning rate 0.0005
[2019-04-04 04:27:11,089] A3C_AGENT_WORKER-Thread-20 INFO:Local step 301000, global step 4811159: loss 0.0953
[2019-04-04 04:27:11,090] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 301000, global step 4811159: learning rate 0.0005
[2019-04-04 04:27:11,554] A3C_AGENT_WORKER-Thread-17 INFO:Local step 301500, global step 4811283: loss 0.5676
[2019-04-04 04:27:11,555] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 301500, global step 4811283: learning rate 0.0005
[2019-04-04 04:27:11,573] A3C_AGENT_WORKER-Thread-16 INFO:Local step 300500, global step 4811291: loss 0.0616
[2019-04-04 04:27:11,583] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 300500, global step 4811291: learning rate 0.0005
[2019-04-04 04:27:11,735] A3C_AGENT_WORKER-Thread-4 INFO:Local step 299000, global step 4811333: loss 0.0922
[2019-04-04 04:27:11,735] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 299000, global step 4811333: learning rate 0.0005
[2019-04-04 04:27:14,042] A3C_AGENT_WORKER-Thread-13 INFO:Local step 299000, global step 4812233: loss 0.0168
[2019-04-04 04:27:14,043] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 299000, global step 4812233: learning rate 0.0005
[2019-04-04 04:27:14,747] A3C_AGENT_WORKER-Thread-10 INFO:Local step 301000, global step 4812435: loss 0.0470
[2019-04-04 04:27:14,748] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 301000, global step 4812435: learning rate 0.0005
[2019-04-04 04:27:14,923] A3C_AGENT_WORKER-Thread-2 INFO:Local step 302000, global step 4812484: loss 0.0465
[2019-04-04 04:27:14,924] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 302000, global step 4812484: learning rate 0.0005
[2019-04-04 04:27:18,398] A3C_AGENT_WORKER-Thread-3 INFO:Local step 301000, global step 4813601: loss 0.0398
[2019-04-04 04:27:18,401] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 301000, global step 4813601: learning rate 0.0005
[2019-04-04 04:27:21,160] A3C_AGENT_WORKER-Thread-6 INFO:Local step 301000, global step 4814616: loss 0.0183
[2019-04-04 04:27:21,179] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 301000, global step 4814616: learning rate 0.0005
[2019-04-04 04:27:23,775] A3C_AGENT_WORKER-Thread-11 INFO:Local step 301500, global step 4815526: loss 0.3891
[2019-04-04 04:27:23,776] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 301500, global step 4815526: learning rate 0.0005
[2019-04-04 04:27:27,221] A3C_AGENT_WORKER-Thread-19 INFO:Local step 302000, global step 4816745: loss 0.0113
[2019-04-04 04:27:27,236] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 302000, global step 4816745: learning rate 0.0005
[2019-04-04 04:27:27,362] A3C_AGENT_WORKER-Thread-15 INFO:Local step 301500, global step 4816790: loss 0.3317
[2019-04-04 04:27:27,363] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 301500, global step 4816790: learning rate 0.0005
[2019-04-04 04:27:27,720] A3C_AGENT_WORKER-Thread-4 INFO:Local step 299500, global step 4816907: loss -1.0954
[2019-04-04 04:27:27,721] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 299500, global step 4816907: learning rate 0.0005
[2019-04-04 04:27:29,112] A3C_AGENT_WORKER-Thread-14 INFO:Local step 300000, global step 4817440: loss 14.1310
[2019-04-04 04:27:29,114] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 300000, global step 4817441: learning rate 0.0005
[2019-04-04 04:27:29,275] A3C_AGENT_WORKER-Thread-13 INFO:Local step 299500, global step 4817502: loss -1.1300
[2019-04-04 04:27:29,277] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 299500, global step 4817502: learning rate 0.0005
[2019-04-04 04:27:29,319] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.3063007e-25 2.1732711e-23 1.8703438e-21 1.0000000e+00 5.7917477e-25
 4.0110194e-23 7.1221420e-20], sum to 1.0000
[2019-04-04 04:27:29,320] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5749
[2019-04-04 04:27:29,330] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.8, 29.0, 49.00000000000001, 109.6666666666667, 26.0, 25.66113687832492, 0.3881982325829041, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2566200.0000, 
sim time next is 2566800.0000, 
raw observation next is [2.7, 29.0, 38.5, 83.5, 26.0, 25.75911905133869, 0.3955770580124042, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.5373961218836566, 0.29, 0.12833333333333333, 0.09226519337016574, 0.6666666666666666, 0.6465932542782241, 0.6318590193374681, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.99398166], dtype=float32), 0.49313667]. 
=============================================
[2019-04-04 04:27:29,831] A3C_AGENT_WORKER-Thread-5 INFO:Local step 301000, global step 4817693: loss 0.0697
[2019-04-04 04:27:29,833] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 301000, global step 4817694: learning rate 0.0005
[2019-04-04 04:27:33,903] A3C_AGENT_WORKER-Thread-17 INFO:Local step 302000, global step 4819071: loss 0.0331
[2019-04-04 04:27:33,905] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 302000, global step 4819071: learning rate 0.0005
[2019-04-04 04:27:35,210] A3C_AGENT_WORKER-Thread-18 INFO:Local step 301500, global step 4819502: loss 0.3052
[2019-04-04 04:27:35,213] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 301500, global step 4819502: learning rate 0.0005
[2019-04-04 04:27:35,899] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.7745545e-26 1.4356179e-22 1.0194396e-22 1.0000000e+00 6.1760376e-26
 1.1561098e-24 2.3570239e-21], sum to 1.0000
[2019-04-04 04:27:35,905] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7148
[2019-04-04 04:27:35,932] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.5833333333333334, 29.16666666666667, 0.0, 0.0, 26.0, 25.1440660874365, 0.2773028360600542, 0.0, 1.0, 47326.15691775537], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2490600.0000, 
sim time next is 2491200.0000, 
raw observation next is [-0.7, 29.0, 0.0, 0.0, 26.0, 25.2405443697931, 0.2847602056391716, 0.0, 1.0, 42257.73120442923], 
processed observation next is [0.0, 0.8695652173913043, 0.443213296398892, 0.29, 0.0, 0.0, 0.6666666666666666, 0.6033786974827583, 0.5949200685463906, 0.0, 1.0, 0.20122729144966298], 
reward next is 0.7988, 
noisyNet noise sample is [array([-0.74700123], dtype=float32), -0.45432603]. 
=============================================
[2019-04-04 04:27:36,347] A3C_AGENT_WORKER-Thread-12 INFO:Local step 301000, global step 4819951: loss 0.0531
[2019-04-04 04:27:36,353] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 301000, global step 4819952: learning rate 0.0005
[2019-04-04 04:27:36,935] A3C_AGENT_WORKER-Thread-16 INFO:Local step 301000, global step 4820156: loss 0.0492
[2019-04-04 04:27:36,936] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 301000, global step 4820156: learning rate 0.0005
[2019-04-04 04:27:36,967] A3C_AGENT_WORKER-Thread-20 INFO:Local step 301500, global step 4820166: loss 0.3003
[2019-04-04 04:27:36,968] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 301500, global step 4820166: learning rate 0.0005
[2019-04-04 04:27:38,023] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.9012285e-21 8.0821907e-18 7.4982546e-17 1.0000000e+00 2.6650263e-21
 5.4247980e-08 1.2767861e-16], sum to 1.0000
[2019-04-04 04:27:38,023] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0438
[2019-04-04 04:27:38,036] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.3, 57.0, 0.0, 0.0, 26.0, 24.77970808626586, 0.1431015531913595, 0.0, 1.0, 38362.80549977994], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2524200.0000, 
sim time next is 2524800.0000, 
raw observation next is [-2.3, 57.00000000000001, 0.0, 0.0, 26.0, 24.78725401184877, 0.1451904463008746, 0.0, 1.0, 38361.34173276385], 
processed observation next is [1.0, 0.21739130434782608, 0.3988919667590028, 0.5700000000000001, 0.0, 0.0, 0.6666666666666666, 0.5656045009873975, 0.5483968154336248, 0.0, 1.0, 0.18267305587030405], 
reward next is 0.8173, 
noisyNet noise sample is [array([0.03007011], dtype=float32), -1.422063]. 
=============================================
[2019-04-04 04:27:38,295] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.9414830e-24 8.2251900e-21 5.0264454e-20 1.0000000e+00 4.7917431e-23
 2.0838276e-09 5.5192096e-19], sum to 1.0000
[2019-04-04 04:27:38,302] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7766
[2019-04-04 04:27:38,322] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 95.0, 59.0, 0.0, 26.0, 26.0157345672202, 0.5191185279158853, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1418400.0000, 
sim time next is 1419000.0000, 
raw observation next is [0.0, 95.0, 63.33333333333334, 0.0, 26.0, 26.02197713498421, 0.5150380867016681, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.46260387811634357, 0.95, 0.21111111111111114, 0.0, 0.6666666666666666, 0.6684980945820174, 0.6716793622338894, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.34125653], dtype=float32), 1.5447521]. 
=============================================
[2019-04-04 04:27:38,353] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[88.40594]
 [88.52018]
 [88.55805]
 [88.61527]
 [88.66818]], R is [[88.35201263]
 [88.4684906 ]
 [88.5838089 ]
 [88.69797516]
 [88.81099701]].
[2019-04-04 04:27:38,564] A3C_AGENT_WORKER-Thread-2 INFO:Local step 302500, global step 4820630: loss 0.3223
[2019-04-04 04:27:38,577] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 302500, global step 4820636: learning rate 0.0005
[2019-04-04 04:27:41,048] A3C_AGENT_WORKER-Thread-10 INFO:Local step 301500, global step 4821496: loss 0.3090
[2019-04-04 04:27:41,063] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 301500, global step 4821496: learning rate 0.0005
[2019-04-04 04:27:43,919] A3C_AGENT_WORKER-Thread-3 INFO:Local step 301500, global step 4822525: loss 0.4809
[2019-04-04 04:27:43,921] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 301500, global step 4822525: learning rate 0.0005
[2019-04-04 04:27:46,552] A3C_AGENT_WORKER-Thread-6 INFO:Local step 301500, global step 4823519: loss 0.3559
[2019-04-04 04:27:46,555] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 301500, global step 4823520: learning rate 0.0005
[2019-04-04 04:27:46,888] A3C_AGENT_WORKER-Thread-11 INFO:Local step 302000, global step 4823643: loss 0.0094
[2019-04-04 04:27:46,893] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 302000, global step 4823645: learning rate 0.0005
[2019-04-04 04:27:46,920] A3C_AGENT_WORKER-Thread-4 INFO:Local step 300000, global step 4823652: loss 5.3547
[2019-04-04 04:27:46,921] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 300000, global step 4823652: learning rate 0.0005
[2019-04-04 04:27:48,776] A3C_AGENT_WORKER-Thread-13 INFO:Local step 300000, global step 4824292: loss 5.3266
[2019-04-04 04:27:48,782] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 300000, global step 4824292: learning rate 0.0005
[2019-04-04 04:27:49,888] A3C_AGENT_WORKER-Thread-15 INFO:Local step 302000, global step 4824757: loss 0.0103
[2019-04-04 04:27:49,895] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 302000, global step 4824757: learning rate 0.0005
[2019-04-04 04:27:50,697] A3C_AGENT_WORKER-Thread-19 INFO:Local step 302500, global step 4825065: loss 0.5787
[2019-04-04 04:27:50,700] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 302500, global step 4825067: learning rate 0.0005
[2019-04-04 04:27:51,575] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.5356805e-21 3.0277913e-16 3.2944043e-17 1.0000000e+00 1.3904685e-20
 1.6396598e-13 5.3976005e-16], sum to 1.0000
[2019-04-04 04:27:51,578] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6830
[2019-04-04 04:27:51,591] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.666666666666667, 71.0, 0.0, 0.0, 26.0, 24.60524990512631, 0.2496917276557019, 0.0, 1.0, 44430.04478252533], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2679600.0000, 
sim time next is 2680200.0000, 
raw observation next is [-8.0, 70.5, 0.0, 0.0, 26.0, 24.61941687123505, 0.240286696548222, 0.0, 1.0, 44418.49830711167], 
processed observation next is [1.0, 0.0, 0.24099722991689754, 0.705, 0.0, 0.0, 0.6666666666666666, 0.5516180726029208, 0.580095565516074, 0.0, 1.0, 0.21151665860529367], 
reward next is 0.7885, 
noisyNet noise sample is [array([1.9822732], dtype=float32), 0.24251874]. 
=============================================
[2019-04-04 04:27:52,537] A3C_AGENT_WORKER-Thread-14 INFO:Local step 300500, global step 4825788: loss 0.0526
[2019-04-04 04:27:52,538] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 300500, global step 4825788: learning rate 0.0005
[2019-04-04 04:27:56,209] A3C_AGENT_WORKER-Thread-5 INFO:Local step 301500, global step 4827010: loss 0.3821
[2019-04-04 04:27:56,210] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 301500, global step 4827010: learning rate 0.0005
[2019-04-04 04:27:56,872] A3C_AGENT_WORKER-Thread-17 INFO:Local step 302500, global step 4827256: loss 0.5466
[2019-04-04 04:27:56,872] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 302500, global step 4827256: learning rate 0.0005
[2019-04-04 04:27:57,156] A3C_AGENT_WORKER-Thread-18 INFO:Local step 302000, global step 4827365: loss 0.0168
[2019-04-04 04:27:57,161] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 302000, global step 4827367: learning rate 0.0005
[2019-04-04 04:27:57,862] A3C_AGENT_WORKER-Thread-2 INFO:Local step 303000, global step 4827638: loss 0.0001
[2019-04-04 04:27:57,866] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 303000, global step 4827639: learning rate 0.0005
[2019-04-04 04:27:59,269] A3C_AGENT_WORKER-Thread-20 INFO:Local step 302000, global step 4828150: loss 0.0164
[2019-04-04 04:27:59,294] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 302000, global step 4828152: learning rate 0.0005
[2019-04-04 04:28:00,135] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.6452806e-21 3.0039861e-17 1.2126841e-16 9.9999988e-01 7.8030346e-21
 1.1941641e-07 1.5515555e-16], sum to 1.0000
[2019-04-04 04:28:00,145] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3600
[2019-04-04 04:28:00,169] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.7, 86.0, 109.0, 752.0, 26.0, 26.41783479415498, 0.6985336037038358, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3234600.0000, 
sim time next is 3235200.0000, 
raw observation next is [-2.6, 84.0, 109.6666666666667, 761.8333333333334, 26.0, 26.41051770000829, 0.7062086388720124, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.3905817174515236, 0.84, 0.3655555555555557, 0.841804788213628, 0.6666666666666666, 0.7008764750006907, 0.7354028796240041, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6598701], dtype=float32), -0.2962384]. 
=============================================
[2019-04-04 04:28:01,545] A3C_AGENT_WORKER-Thread-12 INFO:Local step 301500, global step 4828901: loss 0.2998
[2019-04-04 04:28:01,545] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 301500, global step 4828901: learning rate 0.0005
[2019-04-04 04:28:01,770] A3C_AGENT_WORKER-Thread-16 INFO:Local step 301500, global step 4828982: loss 0.3145
[2019-04-04 04:28:01,772] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 301500, global step 4828982: learning rate 0.0005
[2019-04-04 04:28:03,307] A3C_AGENT_WORKER-Thread-10 INFO:Local step 302000, global step 4829568: loss 0.0290
[2019-04-04 04:28:03,310] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 302000, global step 4829568: learning rate 0.0005
[2019-04-04 04:28:04,369] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.3836597e-26 2.1914394e-20 2.8222676e-22 1.0000000e+00 1.3005428e-25
 5.2967363e-16 2.0577560e-20], sum to 1.0000
[2019-04-04 04:28:04,369] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4645
[2019-04-04 04:28:04,390] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.0, 84.0, 0.0, 0.0, 26.0, 24.81705463054836, 0.251541241055101, 0.0, 1.0, 45746.00664242697], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1809000.0000, 
sim time next is 1809600.0000, 
raw observation next is [-5.0, 83.33333333333334, 0.0, 0.0, 26.0, 24.77857780549353, 0.2454426227614355, 0.0, 1.0, 45698.64417665655], 
processed observation next is [0.0, 0.9565217391304348, 0.32409972299168976, 0.8333333333333335, 0.0, 0.0, 0.6666666666666666, 0.5648814837911275, 0.5818142075871452, 0.0, 1.0, 0.21761259131741212], 
reward next is 0.7824, 
noisyNet noise sample is [array([-0.46253088], dtype=float32), 1.1532457]. 
=============================================
[2019-04-04 04:28:05,137] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0330945e-22 9.0715295e-19 1.3392131e-18 1.0000000e+00 5.4025486e-22
 2.0694182e-09 3.2739068e-18], sum to 1.0000
[2019-04-04 04:28:05,138] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3649
[2019-04-04 04:28:05,237] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-11.0, 78.66666666666667, 0.0, 0.0, 26.0, 23.79699324704524, 0.1495082168866276, 1.0, 1.0, 202374.3299763302], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3309600.0000, 
sim time next is 3310200.0000, 
raw observation next is [-11.0, 80.0, 2.0, 94.0, 26.0, 24.15888187777697, 0.2604414396874473, 1.0, 1.0, 203137.0031065363], 
processed observation next is [1.0, 0.30434782608695654, 0.15789473684210528, 0.8, 0.006666666666666667, 0.10386740331491713, 0.6666666666666666, 0.5132401564814142, 0.586813813229149, 1.0, 1.0, 0.9673190624120775], 
reward next is 0.0327, 
noisyNet noise sample is [array([0.48412186], dtype=float32), 1.9933183]. 
=============================================
[2019-04-04 04:28:05,910] A3C_AGENT_WORKER-Thread-3 INFO:Local step 302000, global step 4830513: loss 0.0267
[2019-04-04 04:28:05,920] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 302000, global step 4830513: learning rate 0.0005
[2019-04-04 04:28:07,435] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.8360985e-27 9.1678758e-23 4.9727343e-22 1.0000000e+00 1.3027170e-26
 9.8711704e-17 4.1103874e-22], sum to 1.0000
[2019-04-04 04:28:07,436] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5446
[2019-04-04 04:28:07,463] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.333333333333333, 50.0, 102.8333333333333, 739.0, 26.0, 26.36828910259953, 0.6730844933456658, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3336000.0000, 
sim time next is 3336600.0000, 
raw observation next is [-3.166666666666667, 50.0, 99.66666666666666, 726.0, 26.0, 26.51813617801989, 0.6924883511769342, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3748845798707295, 0.5, 0.3322222222222222, 0.8022099447513812, 0.6666666666666666, 0.7098446815016576, 0.7308294503923114, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7535156], dtype=float32), 0.78349423]. 
=============================================
[2019-04-04 04:28:08,620] A3C_AGENT_WORKER-Thread-6 INFO:Local step 302000, global step 4831486: loss 0.0069
[2019-04-04 04:28:08,625] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 302000, global step 4831489: learning rate 0.0005
[2019-04-04 04:28:08,900] A3C_AGENT_WORKER-Thread-4 INFO:Local step 300500, global step 4831598: loss 0.0646
[2019-04-04 04:28:08,902] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 300500, global step 4831598: learning rate 0.0005
[2019-04-04 04:28:10,238] A3C_AGENT_WORKER-Thread-11 INFO:Local step 302500, global step 4832111: loss 0.5149
[2019-04-04 04:28:10,242] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 302500, global step 4832112: learning rate 0.0005
[2019-04-04 04:28:10,543] A3C_AGENT_WORKER-Thread-19 INFO:Local step 303000, global step 4832218: loss 0.0258
[2019-04-04 04:28:10,543] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 303000, global step 4832218: learning rate 0.0005
[2019-04-04 04:28:11,011] A3C_AGENT_WORKER-Thread-13 INFO:Local step 300500, global step 4832391: loss 0.0742
[2019-04-04 04:28:11,013] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 300500, global step 4832391: learning rate 0.0005
[2019-04-04 04:28:12,408] A3C_AGENT_WORKER-Thread-15 INFO:Local step 302500, global step 4832856: loss 0.9360
[2019-04-04 04:28:12,420] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 302500, global step 4832857: learning rate 0.0005
[2019-04-04 04:28:13,080] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.4735140e-27 1.4781413e-22 6.8555633e-22 1.0000000e+00 2.8904908e-26
 2.2949314e-09 1.6838337e-21], sum to 1.0000
[2019-04-04 04:28:13,081] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6511
[2019-04-04 04:28:13,088] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 49.0, 113.0, 806.0, 26.0, 26.07264809992181, 0.598287556332719, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3418200.0000, 
sim time next is 3418800.0000, 
raw observation next is [3.0, 49.0, 111.3333333333333, 800.8333333333334, 26.0, 26.29847862589666, 0.6139510290073927, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5457063711911359, 0.49, 0.371111111111111, 0.8848987108655617, 0.6666666666666666, 0.6915398854913883, 0.7046503430024642, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.44221625], dtype=float32), -0.38426533]. 
=============================================
[2019-04-04 04:28:15,507] A3C_AGENT_WORKER-Thread-2 INFO:Local step 303500, global step 4834019: loss 0.1278
[2019-04-04 04:28:15,510] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 303500, global step 4834019: learning rate 0.0005
[2019-04-04 04:28:16,137] A3C_AGENT_WORKER-Thread-17 INFO:Local step 303000, global step 4834281: loss 0.0050
[2019-04-04 04:28:16,139] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 303000, global step 4834281: learning rate 0.0005
[2019-04-04 04:28:17,275] A3C_AGENT_WORKER-Thread-14 INFO:Local step 301000, global step 4834733: loss 0.0046
[2019-04-04 04:28:17,282] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 301000, global step 4834737: learning rate 0.0005
[2019-04-04 04:28:18,012] A3C_AGENT_WORKER-Thread-5 INFO:Local step 302000, global step 4834983: loss 0.0347
[2019-04-04 04:28:18,014] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 302000, global step 4834983: learning rate 0.0005
[2019-04-04 04:28:18,928] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.8416583e-26 1.0752651e-22 2.4007708e-22 1.0000000e+00 1.8299083e-25
 1.5852134e-17 1.0206435e-19], sum to 1.0000
[2019-04-04 04:28:18,930] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6529
[2019-04-04 04:28:18,979] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.75, 71.0, 117.0, 0.0, 26.0, 25.96420909976561, 0.3852732189704968, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2197800.0000, 
sim time next is 2198400.0000, 
raw observation next is [-4.666666666666667, 71.0, 121.3333333333333, 0.0, 26.0, 25.93190807576925, 0.3783520592352789, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.3333333333333333, 0.71, 0.40444444444444433, 0.0, 0.6666666666666666, 0.6609923396474375, 0.6261173530784263, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.08597094], dtype=float32), 0.49332952]. 
=============================================
[2019-04-04 04:28:19,564] A3C_AGENT_WORKER-Thread-18 INFO:Local step 302500, global step 4835567: loss 0.3878
[2019-04-04 04:28:19,579] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 302500, global step 4835567: learning rate 0.0005
[2019-04-04 04:28:20,285] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.28635241e-24 5.50971677e-21 1.06098095e-20 1.00000000e+00
 4.27218026e-25 3.92124999e-09 2.70025201e-19], sum to 1.0000
[2019-04-04 04:28:20,286] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1456
[2019-04-04 04:28:20,300] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.38948021822723, 0.1217059282862251, 0.0, 1.0, 41468.21593940304], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1998000.0000, 
sim time next is 1998600.0000, 
raw observation next is [-5.600000000000001, 83.0, 0.0, 0.0, 26.0, 24.32468416676981, 0.120429727879404, 0.0, 1.0, 41423.72804709623], 
processed observation next is [1.0, 0.13043478260869565, 0.3074792243767313, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5270570138974842, 0.540143242626468, 0.0, 1.0, 0.19725584784331537], 
reward next is 0.8027, 
noisyNet noise sample is [array([-0.42465886], dtype=float32), 0.5886877]. 
=============================================
[2019-04-04 04:28:22,174] A3C_AGENT_WORKER-Thread-20 INFO:Local step 302500, global step 4836519: loss 0.6442
[2019-04-04 04:28:22,176] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 302500, global step 4836519: learning rate 0.0005
[2019-04-04 04:28:23,531] A3C_AGENT_WORKER-Thread-16 INFO:Local step 302000, global step 4836961: loss 0.0321
[2019-04-04 04:28:23,547] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 302000, global step 4836962: learning rate 0.0005
[2019-04-04 04:28:23,563] A3C_AGENT_WORKER-Thread-12 INFO:Local step 302000, global step 4836971: loss 0.0229
[2019-04-04 04:28:23,565] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 302000, global step 4836972: learning rate 0.0005
[2019-04-04 04:28:25,655] A3C_AGENT_WORKER-Thread-10 INFO:Local step 302500, global step 4837794: loss 1.3442
[2019-04-04 04:28:25,656] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 302500, global step 4837794: learning rate 0.0005
[2019-04-04 04:28:28,761] A3C_AGENT_WORKER-Thread-3 INFO:Local step 302500, global step 4839032: loss 0.8595
[2019-04-04 04:28:28,762] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 302500, global step 4839032: learning rate 0.0005
[2019-04-04 04:28:29,045] A3C_AGENT_WORKER-Thread-11 INFO:Local step 303000, global step 4839140: loss 0.0039
[2019-04-04 04:28:29,046] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 303000, global step 4839140: learning rate 0.0005
[2019-04-04 04:28:29,094] A3C_AGENT_WORKER-Thread-19 INFO:Local step 303500, global step 4839161: loss 0.0449
[2019-04-04 04:28:29,095] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 303500, global step 4839161: learning rate 0.0005
[2019-04-04 04:28:30,659] A3C_AGENT_WORKER-Thread-6 INFO:Local step 302500, global step 4839782: loss 1.0014
[2019-04-04 04:28:30,660] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 302500, global step 4839782: learning rate 0.0005
[2019-04-04 04:28:30,895] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.1258434e-26 1.4130165e-21 7.4466985e-22 1.0000000e+00 2.1834065e-25
 1.2172534e-17 7.7525409e-20], sum to 1.0000
[2019-04-04 04:28:30,901] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6159
[2019-04-04 04:28:30,913] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.83059764596101, 0.2871534533389459, 0.0, 1.0, 41104.58329008285], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3379800.0000, 
sim time next is 3380400.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.83932589609843, 0.2828699264887111, 0.0, 1.0, 41086.05076873735], 
processed observation next is [1.0, 0.13043478260869565, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5699438246748691, 0.5942899754962371, 0.0, 1.0, 0.19564786080351118], 
reward next is 0.8044, 
noisyNet noise sample is [array([0.69032604], dtype=float32), 1.3119583]. 
=============================================
[2019-04-04 04:28:31,098] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.6815329e-26 1.8821955e-22 7.4445397e-22 1.0000000e+00 2.3510642e-26
 1.9537174e-17 5.2408887e-21], sum to 1.0000
[2019-04-04 04:28:31,098] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0477
[2019-04-04 04:28:31,134] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.6666666666666666, 83.66666666666667, 0.0, 0.0, 26.0, 24.98965335673619, 0.2849679125579796, 0.0, 1.0, 42175.40043450436], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3089400.0000, 
sim time next is 3090000.0000, 
raw observation next is [-0.7333333333333334, 85.33333333333334, 0.0, 0.0, 26.0, 24.99513073555178, 0.2847377319450664, 0.0, 1.0, 34589.24070649501], 
processed observation next is [0.0, 0.782608695652174, 0.44228993536472766, 0.8533333333333334, 0.0, 0.0, 0.6666666666666666, 0.5829275612959816, 0.5949125773150221, 0.0, 1.0, 0.16471067003092862], 
reward next is 0.8353, 
noisyNet noise sample is [array([-1.2307833], dtype=float32), 2.72231]. 
=============================================
[2019-04-04 04:28:31,143] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[82.565315]
 [82.45283 ]
 [82.06728 ]
 [81.53179 ]
 [82.08909 ]], R is [[82.5320282 ]
 [82.50587463]
 [82.44107819]
 [82.36240387]
 [82.30994415]].
[2019-04-04 04:28:31,143] A3C_AGENT_WORKER-Thread-15 INFO:Local step 303000, global step 4839986: loss 0.0002
[2019-04-04 04:28:31,145] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 303000, global step 4839986: learning rate 0.0005
[2019-04-04 04:28:33,226] A3C_AGENT_WORKER-Thread-4 INFO:Local step 301000, global step 4840831: loss 1.1447
[2019-04-04 04:28:33,231] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 301000, global step 4840831: learning rate 0.0005
[2019-04-04 04:28:33,947] A3C_AGENT_WORKER-Thread-2 INFO:Local step 304000, global step 4841093: loss 0.2087
[2019-04-04 04:28:33,948] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 304000, global step 4841094: learning rate 0.0005
[2019-04-04 04:28:34,437] A3C_AGENT_WORKER-Thread-17 INFO:Local step 303500, global step 4841301: loss 0.0722
[2019-04-04 04:28:34,439] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 303500, global step 4841301: learning rate 0.0005
[2019-04-04 04:28:35,506] A3C_AGENT_WORKER-Thread-13 INFO:Local step 301000, global step 4841783: loss 0.0314
[2019-04-04 04:28:35,508] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 301000, global step 4841784: learning rate 0.0005
[2019-04-04 04:28:38,508] A3C_AGENT_WORKER-Thread-18 INFO:Local step 303000, global step 4842954: loss 0.0005
[2019-04-04 04:28:38,510] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 303000, global step 4842956: learning rate 0.0005
[2019-04-04 04:28:39,202] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.6495190e-27 2.3416415e-24 1.5497763e-22 1.0000000e+00 3.2552758e-26
 5.0303365e-21 5.3035973e-21], sum to 1.0000
[2019-04-04 04:28:39,203] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4660
[2019-04-04 04:28:39,280] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.833333333333333, 48.5, 0.0, 0.0, 26.0, 25.61920344518761, 0.5403693612786581, 1.0, 1.0, 73963.41304692785], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3867000.0000, 
sim time next is 3867600.0000, 
raw observation next is [1.666666666666667, 49.0, 0.0, 0.0, 26.0, 24.97219278706523, 0.4801050011892012, 1.0, 1.0, 184951.9997250898], 
processed observation next is [1.0, 0.782608695652174, 0.5087719298245615, 0.49, 0.0, 0.0, 0.6666666666666666, 0.5810160655887691, 0.6600350003964004, 1.0, 1.0, 0.8807238082147133], 
reward next is 0.1193, 
noisyNet noise sample is [array([-0.06046101], dtype=float32), -1.3956]. 
=============================================
[2019-04-04 04:28:40,528] A3C_AGENT_WORKER-Thread-5 INFO:Local step 302500, global step 4843721: loss 0.5266
[2019-04-04 04:28:40,531] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 302500, global step 4843722: learning rate 0.0005
[2019-04-04 04:28:41,033] A3C_AGENT_WORKER-Thread-20 INFO:Local step 303000, global step 4843922: loss 0.0003
[2019-04-04 04:28:41,044] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 303000, global step 4843922: learning rate 0.0005
[2019-04-04 04:28:41,490] A3C_AGENT_WORKER-Thread-14 INFO:Local step 301500, global step 4844111: loss 0.2989
[2019-04-04 04:28:41,495] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 301500, global step 4844111: learning rate 0.0005
[2019-04-04 04:28:41,650] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.3577978e-22 3.1961521e-18 1.8223635e-18 1.0000000e+00 3.1177526e-21
 1.4181213e-11 2.1758160e-17], sum to 1.0000
[2019-04-04 04:28:41,651] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9614
[2019-04-04 04:28:41,676] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 72.0, 0.0, 0.0, 26.0, 25.19470524790278, 0.3507666152102993, 0.0, 1.0, 51478.28792630383], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3811800.0000, 
sim time next is 3812400.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 26.0, 25.12611080557545, 0.3484267752950259, 0.0, 1.0, 46257.06237076917], 
processed observation next is [1.0, 0.13043478260869565, 0.3518005540166205, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5938425671312876, 0.6161422584316752, 0.0, 1.0, 0.22027172557509128], 
reward next is 0.7797, 
noisyNet noise sample is [array([-0.02852441], dtype=float32), -1.067404]. 
=============================================
[2019-04-04 04:28:43,822] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.3941440e-21 5.8066411e-17 1.0993955e-17 4.1344938e-05 4.5967348e-21
 9.9995863e-01 2.5696531e-17], sum to 1.0000
[2019-04-04 04:28:43,822] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9110
[2019-04-04 04:28:43,840] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.5, 100.0, 0.0, 0.0, 26.0, 25.48321174233826, 0.3154149699627688, 0.0, 1.0, 22940.61520053466], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3130200.0000, 
sim time next is 3130800.0000, 
raw observation next is [3.666666666666667, 100.0, 0.0, 0.0, 26.0, 25.44903574870888, 0.3040782506168263, 0.0, 1.0, 54092.19805017607], 
processed observation next is [1.0, 0.21739130434782608, 0.564173591874423, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6207529790590733, 0.6013594168722755, 0.0, 1.0, 0.2575818954770289], 
reward next is 0.7424, 
noisyNet noise sample is [array([0.1395835], dtype=float32), -0.42310143]. 
=============================================
[2019-04-04 04:28:44,371] A3C_AGENT_WORKER-Thread-10 INFO:Local step 303000, global step 4845251: loss 0.0009
[2019-04-04 04:28:44,379] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 303000, global step 4845254: learning rate 0.0005
[2019-04-04 04:28:45,653] A3C_AGENT_WORKER-Thread-12 INFO:Local step 302500, global step 4845729: loss 0.5561
[2019-04-04 04:28:45,658] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 302500, global step 4845730: learning rate 0.0005
[2019-04-04 04:28:46,066] A3C_AGENT_WORKER-Thread-16 INFO:Local step 302500, global step 4845935: loss 0.5973
[2019-04-04 04:28:46,069] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 302500, global step 4845937: learning rate 0.0005
[2019-04-04 04:28:46,690] A3C_AGENT_WORKER-Thread-19 INFO:Local step 304000, global step 4846228: loss 0.1090
[2019-04-04 04:28:46,691] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 304000, global step 4846228: learning rate 0.0005
[2019-04-04 04:28:47,100] A3C_AGENT_WORKER-Thread-3 INFO:Local step 303000, global step 4846422: loss 0.0089
[2019-04-04 04:28:47,103] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 303000, global step 4846422: learning rate 0.0005
[2019-04-04 04:28:47,297] A3C_AGENT_WORKER-Thread-11 INFO:Local step 303500, global step 4846524: loss 0.1018
[2019-04-04 04:28:47,306] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 303500, global step 4846524: learning rate 0.0005
[2019-04-04 04:28:48,860] A3C_AGENT_WORKER-Thread-15 INFO:Local step 303500, global step 4847199: loss 0.1145
[2019-04-04 04:28:48,865] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 303500, global step 4847199: learning rate 0.0005
[2019-04-04 04:28:49,273] A3C_AGENT_WORKER-Thread-6 INFO:Local step 303000, global step 4847401: loss 0.0040
[2019-04-04 04:28:49,275] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 303000, global step 4847401: learning rate 0.0005
[2019-04-04 04:28:52,345] A3C_AGENT_WORKER-Thread-17 INFO:Local step 304000, global step 4848714: loss 0.0895
[2019-04-04 04:28:52,347] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 304000, global step 4848715: learning rate 0.0005
[2019-04-04 04:28:52,814] A3C_AGENT_WORKER-Thread-2 INFO:Local step 304500, global step 4848928: loss 0.5791
[2019-04-04 04:28:52,816] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 304500, global step 4848928: learning rate 0.0005
[2019-04-04 04:28:56,354] A3C_AGENT_WORKER-Thread-18 INFO:Local step 303500, global step 4850414: loss 0.0157
[2019-04-04 04:28:56,356] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 303500, global step 4850414: learning rate 0.0005
[2019-04-04 04:28:56,817] A3C_AGENT_WORKER-Thread-4 INFO:Local step 301500, global step 4850612: loss 0.3961
[2019-04-04 04:28:56,818] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 301500, global step 4850612: learning rate 0.0005
[2019-04-04 04:28:58,786] A3C_AGENT_WORKER-Thread-5 INFO:Local step 303000, global step 4851542: loss 0.0003
[2019-04-04 04:28:58,788] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 303000, global step 4851542: learning rate 0.0005
[2019-04-04 04:28:59,329] A3C_AGENT_WORKER-Thread-13 INFO:Local step 301500, global step 4851780: loss 0.3319
[2019-04-04 04:28:59,330] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 301500, global step 4851780: learning rate 0.0005
[2019-04-04 04:28:59,905] A3C_AGENT_WORKER-Thread-20 INFO:Local step 303500, global step 4851974: loss 0.0942
[2019-04-04 04:28:59,906] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 303500, global step 4851974: learning rate 0.0005
[2019-04-04 04:29:01,580] A3C_AGENT_WORKER-Thread-10 INFO:Local step 303500, global step 4852765: loss 0.0537
[2019-04-04 04:29:01,580] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 303500, global step 4852765: learning rate 0.0005
[2019-04-04 04:29:01,734] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.2812876e-25 4.9146125e-22 5.4443244e-20 1.0000000e+00 3.0743568e-24
 1.7143029e-20 1.3586276e-18], sum to 1.0000
[2019-04-04 04:29:01,735] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0752
[2019-04-04 04:29:01,783] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.333333333333333, 36.33333333333333, 81.33333333333333, 373.6666666666667, 26.0, 27.32871254545077, 0.7736628693299462, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4120800.0000, 
sim time next is 4121400.0000, 
raw observation next is [3.166666666666667, 36.66666666666667, 69.66666666666667, 310.3333333333334, 26.0, 26.83043273351565, 0.7383358882209053, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.5503231763619576, 0.3666666666666667, 0.23222222222222225, 0.3429097605893187, 0.6666666666666666, 0.7358693944596375, 0.7461119627403018, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.12932755], dtype=float32), 0.32250714]. 
=============================================
[2019-04-04 04:29:01,944] A3C_AGENT_WORKER-Thread-14 INFO:Local step 302000, global step 4852948: loss 0.0160
[2019-04-04 04:29:01,948] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 302000, global step 4852949: learning rate 0.0005
[2019-04-04 04:29:03,774] A3C_AGENT_WORKER-Thread-16 INFO:Local step 303000, global step 4853754: loss 0.0043
[2019-04-04 04:29:03,775] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 303000, global step 4853754: learning rate 0.0005
[2019-04-04 04:29:04,133] A3C_AGENT_WORKER-Thread-12 INFO:Local step 303000, global step 4853929: loss 0.0159
[2019-04-04 04:29:04,147] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 303000, global step 4853929: learning rate 0.0005
[2019-04-04 04:29:04,374] A3C_AGENT_WORKER-Thread-11 INFO:Local step 304000, global step 4854050: loss 0.1344
[2019-04-04 04:29:04,375] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 304000, global step 4854051: learning rate 0.0005
[2019-04-04 04:29:04,596] A3C_AGENT_WORKER-Thread-3 INFO:Local step 303500, global step 4854139: loss 0.1077
[2019-04-04 04:29:04,596] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 303500, global step 4854139: learning rate 0.0005
[2019-04-04 04:29:05,041] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0259069e-31 1.5151950e-27 4.2416931e-28 1.0000000e+00 2.9855267e-31
 2.3352825e-23 3.3496911e-26], sum to 1.0000
[2019-04-04 04:29:05,041] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7111
[2019-04-04 04:29:05,073] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.833333333333334, 54.83333333333334, 176.3333333333333, 676.6666666666666, 26.0, 25.30453637630523, 0.3995317865392995, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4272600.0000, 
sim time next is 4273200.0000, 
raw observation next is [5.0, 55.0, 162.5, 713.0, 26.0, 25.2707982019676, 0.398203667718511, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.6011080332409973, 0.55, 0.5416666666666666, 0.7878453038674034, 0.6666666666666666, 0.6058998501639667, 0.6327345559061703, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7038857], dtype=float32), -0.45572707]. 
=============================================
[2019-04-04 04:29:05,623] A3C_AGENT_WORKER-Thread-19 INFO:Local step 304500, global step 4854509: loss 0.3749
[2019-04-04 04:29:05,633] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 304500, global step 4854509: learning rate 0.0005
[2019-04-04 04:29:06,233] A3C_AGENT_WORKER-Thread-6 INFO:Local step 303500, global step 4854795: loss 0.0950
[2019-04-04 04:29:06,234] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 303500, global step 4854795: learning rate 0.0005
[2019-04-04 04:29:07,226] A3C_AGENT_WORKER-Thread-15 INFO:Local step 304000, global step 4855309: loss 0.5734
[2019-04-04 04:29:07,228] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 304000, global step 4855309: learning rate 0.0005
[2019-04-04 04:29:08,830] A3C_AGENT_WORKER-Thread-2 INFO:Local step 305000, global step 4855999: loss 0.0516
[2019-04-04 04:29:08,832] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 305000, global step 4856000: learning rate 0.0005
[2019-04-04 04:29:11,202] A3C_AGENT_WORKER-Thread-17 INFO:Local step 304500, global step 4857011: loss 0.4565
[2019-04-04 04:29:11,202] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 304500, global step 4857011: learning rate 0.0005
[2019-04-04 04:29:13,522] A3C_AGENT_WORKER-Thread-18 INFO:Local step 304000, global step 4858102: loss 0.2945
[2019-04-04 04:29:13,524] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 304000, global step 4858103: learning rate 0.0005
[2019-04-04 04:29:13,703] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.4676120e-27 2.2705419e-23 3.3974045e-21 1.0000000e+00 2.7346982e-27
 5.4818454e-13 8.2037207e-22], sum to 1.0000
[2019-04-04 04:29:13,704] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9676
[2019-04-04 04:29:13,719] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 37.33333333333334, 0.0, 0.0, 26.0, 24.77212763883801, 0.2012030500552583, 0.0, 1.0, 40166.99077589496], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4081800.0000, 
sim time next is 4082400.0000, 
raw observation next is [-4.0, 38.0, 0.0, 0.0, 26.0, 24.71012492362333, 0.1948916151942137, 0.0, 1.0, 40164.9526004848], 
processed observation next is [1.0, 0.2608695652173913, 0.3518005540166205, 0.38, 0.0, 0.0, 0.6666666666666666, 0.5591770769686107, 0.5649638717314046, 0.0, 1.0, 0.19126167904992764], 
reward next is 0.8087, 
noisyNet noise sample is [array([-0.31622916], dtype=float32), 0.27602118]. 
=============================================
[2019-04-04 04:29:16,680] A3C_AGENT_WORKER-Thread-5 INFO:Local step 303500, global step 4859376: loss 0.2292
[2019-04-04 04:29:16,691] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 303500, global step 4859377: learning rate 0.0005
[2019-04-04 04:29:17,566] A3C_AGENT_WORKER-Thread-20 INFO:Local step 304000, global step 4859800: loss 0.2074
[2019-04-04 04:29:17,567] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 304000, global step 4859800: learning rate 0.0005
[2019-04-04 04:29:18,180] A3C_AGENT_WORKER-Thread-4 INFO:Local step 302000, global step 4860037: loss 0.0612
[2019-04-04 04:29:18,181] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 302000, global step 4860037: learning rate 0.0005
[2019-04-04 04:29:18,794] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.4977802e-25 2.1272919e-20 6.7709981e-20 1.0000000e+00 3.0069896e-25
 3.6240027e-14 3.4010518e-19], sum to 1.0000
[2019-04-04 04:29:18,817] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9224
[2019-04-04 04:29:18,840] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.666666666666667, 56.66666666666667, 0.0, 0.0, 26.0, 25.36198197110133, 0.3755688658983595, 0.0, 1.0, 44046.78087117204], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3624000.0000, 
sim time next is 3624600.0000, 
raw observation next is [-2.833333333333333, 58.33333333333333, 0.0, 0.0, 26.0, 25.34774548917314, 0.3721198262273824, 0.0, 1.0, 40285.61444382021], 
processed observation next is [0.0, 0.9565217391304348, 0.3841181902123731, 0.5833333333333333, 0.0, 0.0, 0.6666666666666666, 0.6123121240977616, 0.6240399420757942, 0.0, 1.0, 0.19183625925628672], 
reward next is 0.8082, 
noisyNet noise sample is [array([1.7909889], dtype=float32), 0.59377885]. 
=============================================
[2019-04-04 04:29:19,662] A3C_AGENT_WORKER-Thread-10 INFO:Local step 304000, global step 4860706: loss 0.2524
[2019-04-04 04:29:19,664] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 304000, global step 4860708: learning rate 0.0005
[2019-04-04 04:29:20,002] A3C_AGENT_WORKER-Thread-13 INFO:Local step 302000, global step 4860866: loss 0.0483
[2019-04-04 04:29:20,002] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 302000, global step 4860866: learning rate 0.0005
[2019-04-04 04:29:20,975] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0495147e-31 3.6979654e-27 2.0835461e-27 1.0000000e+00 1.0404624e-30
 2.1294900e-22 4.5808285e-25], sum to 1.0000
[2019-04-04 04:29:20,975] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4772
[2019-04-04 04:29:21,000] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [11.33333333333333, 26.66666666666667, 109.3333333333333, 746.3333333333334, 26.0, 25.63764579328943, 0.4746926863902073, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3666000.0000, 
sim time next is 3666600.0000, 
raw observation next is [11.5, 26.0, 111.0, 763.0, 26.0, 25.62678885638731, 0.481164418824829, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.7811634349030472, 0.26, 0.37, 0.8430939226519337, 0.6666666666666666, 0.6355657380322759, 0.6603881396082764, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3245931], dtype=float32), 0.18002205]. 
=============================================
[2019-04-04 04:29:21,634] A3C_AGENT_WORKER-Thread-19 INFO:Local step 305000, global step 4861520: loss 0.0172
[2019-04-04 04:29:21,638] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 305000, global step 4861522: learning rate 0.0005
[2019-04-04 04:29:21,795] A3C_AGENT_WORKER-Thread-12 INFO:Local step 303500, global step 4861591: loss 0.0219
[2019-04-04 04:29:21,802] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 303500, global step 4861591: learning rate 0.0005
[2019-04-04 04:29:21,812] A3C_AGENT_WORKER-Thread-16 INFO:Local step 303500, global step 4861596: loss 0.0137
[2019-04-04 04:29:21,829] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 303500, global step 4861599: learning rate 0.0005
[2019-04-04 04:29:22,198] A3C_AGENT_WORKER-Thread-3 INFO:Local step 304000, global step 4861776: loss 0.5221
[2019-04-04 04:29:22,203] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 304000, global step 4861776: learning rate 0.0005
[2019-04-04 04:29:22,457] A3C_AGENT_WORKER-Thread-11 INFO:Local step 304500, global step 4861911: loss 0.1295
[2019-04-04 04:29:22,458] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 304500, global step 4861911: learning rate 0.0005
[2019-04-04 04:29:22,911] A3C_AGENT_WORKER-Thread-14 INFO:Local step 302500, global step 4862127: loss 0.6110
[2019-04-04 04:29:22,913] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 302500, global step 4862128: learning rate 0.0005
[2019-04-04 04:29:23,553] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.6706924e-17 2.0285330e-13 1.0483262e-13 9.3791837e-01 1.6996518e-17
 6.2081568e-02 2.1620915e-12], sum to 1.0000
[2019-04-04 04:29:23,555] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2456
[2019-04-04 04:29:23,571] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 93.0, 0.0, 0.0, 26.0, 24.92064811092921, 0.2598243727745499, 0.0, 1.0, 55605.71046714977], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2865600.0000, 
sim time next is 2866200.0000, 
raw observation next is [1.0, 93.0, 0.0, 0.0, 26.0, 24.92127538869022, 0.2581052618397607, 0.0, 1.0, 55708.26449897036], 
processed observation next is [1.0, 0.17391304347826086, 0.4903047091412743, 0.93, 0.0, 0.0, 0.6666666666666666, 0.5767729490575183, 0.5860350872799202, 0.0, 1.0, 0.26527744999509695], 
reward next is 0.7347, 
noisyNet noise sample is [array([1.6360337], dtype=float32), -1.2340465]. 
=============================================
[2019-04-04 04:29:23,844] A3C_AGENT_WORKER-Thread-6 INFO:Local step 304000, global step 4862548: loss 0.2393
[2019-04-04 04:29:23,845] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 304000, global step 4862548: learning rate 0.0005
[2019-04-04 04:29:25,594] A3C_AGENT_WORKER-Thread-15 INFO:Local step 304500, global step 4863282: loss 0.2011
[2019-04-04 04:29:25,597] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 304500, global step 4863283: learning rate 0.0005
[2019-04-04 04:29:26,699] A3C_AGENT_WORKER-Thread-17 INFO:Local step 305000, global step 4863779: loss 0.0585
[2019-04-04 04:29:26,700] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 305000, global step 4863780: learning rate 0.0005
[2019-04-04 04:29:27,076] A3C_AGENT_WORKER-Thread-2 INFO:Local step 305500, global step 4863948: loss 0.0556
[2019-04-04 04:29:27,077] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 305500, global step 4863948: learning rate 0.0005
[2019-04-04 04:29:28,016] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.2652580e-21 3.7580532e-17 8.6828447e-17 1.0000000e+00 3.7346817e-20
 5.5066405e-09 5.4749330e-15], sum to 1.0000
[2019-04-04 04:29:28,017] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3997
[2019-04-04 04:29:28,071] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 92.0, 80.33333333333333, 0.0, 26.0, 26.17445415967781, 0.5224616243184208, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4697400.0000, 
sim time next is 4698000.0000, 
raw observation next is [0.0, 92.0, 89.0, 0.0, 26.0, 26.14714612025944, 0.5244882015779608, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.46260387811634357, 0.92, 0.2966666666666667, 0.0, 0.6666666666666666, 0.6789288433549533, 0.6748294005259869, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.20601799], dtype=float32), 0.1114893]. 
=============================================
[2019-04-04 04:29:28,085] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[87.926414]
 [87.81086 ]
 [87.59293 ]
 [87.1025  ]
 [86.88447 ]], R is [[88.05607605]
 [88.17551422]
 [88.29376221]
 [88.41082764]
 [88.52671814]].
[2019-04-04 04:29:31,779] A3C_AGENT_WORKER-Thread-18 INFO:Local step 304500, global step 4866139: loss 0.0415
[2019-04-04 04:29:31,792] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 304500, global step 4866140: learning rate 0.0005
[2019-04-04 04:29:32,020] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.2600284e-30 2.5737223e-25 8.2064356e-24 1.0000000e+00 3.7938550e-28
 2.0468035e-20 4.0372847e-23], sum to 1.0000
[2019-04-04 04:29:32,020] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3700
[2019-04-04 04:29:32,048] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 32.0, 117.5, 792.5, 26.0, 26.68412589389242, 0.6304452525201162, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4100400.0000, 
sim time next is 4101000.0000, 
raw observation next is [-0.6666666666666667, 31.33333333333334, 118.6666666666667, 800.3333333333334, 26.0, 26.71194456206582, 0.6381145371076267, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.44413665743305636, 0.3133333333333334, 0.39555555555555566, 0.8843462246777164, 0.6666666666666666, 0.7259953801721517, 0.7127048457025422, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.09120532], dtype=float32), 0.10696409]. 
=============================================
[2019-04-04 04:29:32,066] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[88.51495 ]
 [88.7477  ]
 [89.06136 ]
 [89.141365]
 [89.24092 ]], R is [[88.51587677]
 [88.63072205]
 [88.74441528]
 [88.85697174]
 [88.96840668]].
[2019-04-04 04:29:34,253] A3C_AGENT_WORKER-Thread-5 INFO:Local step 304000, global step 4867262: loss 0.1368
[2019-04-04 04:29:34,253] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 304000, global step 4867262: learning rate 0.0005
[2019-04-04 04:29:34,395] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2684420e-22 2.3521950e-18 3.2283531e-19 1.0000000e+00 1.4178328e-21
 1.0395774e-08 7.0758724e-17], sum to 1.0000
[2019-04-04 04:29:34,397] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2910
[2019-04-04 04:29:34,428] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 49.0, 0.0, 0.0, 26.0, 25.32779537262279, 0.3181901477938898, 0.0, 1.0, 39166.51016337236], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4260000.0000, 
sim time next is 4260600.0000, 
raw observation next is [3.0, 49.0, 0.0, 0.0, 26.0, 25.32029116417075, 0.3182152961117649, 0.0, 1.0, 39151.93571751002], 
processed observation next is [0.0, 0.30434782608695654, 0.5457063711911359, 0.49, 0.0, 0.0, 0.6666666666666666, 0.6100242636808959, 0.6060717653705883, 0.0, 1.0, 0.18643778913100012], 
reward next is 0.8136, 
noisyNet noise sample is [array([0.796568], dtype=float32), -0.89905035]. 
=============================================
[2019-04-04 04:29:36,020] A3C_AGENT_WORKER-Thread-20 INFO:Local step 304500, global step 4868004: loss 0.0302
[2019-04-04 04:29:36,029] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 304500, global step 4868006: learning rate 0.0005
[2019-04-04 04:29:38,044] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.9699440e-26 9.8706505e-23 8.5862905e-20 1.0000000e+00 1.3610653e-26
 2.6999300e-21 2.1779220e-20], sum to 1.0000
[2019-04-04 04:29:38,044] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5753
[2019-04-04 04:29:38,056] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [10.16666666666667, 18.66666666666667, 0.0, 0.0, 26.0, 27.57228231576032, 0.9441810983504605, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5082600.0000, 
sim time next is 5083200.0000, 
raw observation next is [10.0, 19.0, 0.0, 0.0, 26.0, 27.48735722391541, 0.9290661006533355, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.739612188365651, 0.19, 0.0, 0.0, 0.6666666666666666, 0.7906131019929509, 0.8096887002177785, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.51236254], dtype=float32), -0.17053796]. 
=============================================
[2019-04-04 04:29:38,172] A3C_AGENT_WORKER-Thread-10 INFO:Local step 304500, global step 4868998: loss 0.0673
[2019-04-04 04:29:38,174] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 304500, global step 4868998: learning rate 0.0005
[2019-04-04 04:29:38,231] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.18957115e-35 1.88888865e-33 6.84168216e-32 1.00000000e+00
 3.31668984e-37 4.26932789e-33 1.39874139e-27], sum to 1.0000
[2019-04-04 04:29:38,232] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8138
[2019-04-04 04:29:38,250] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.5, 28.5, 0.0, 0.0, 26.0, 25.76479856994114, 0.4926436039249159, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4041000.0000, 
sim time next is 4041600.0000, 
raw observation next is [-3.666666666666667, 29.33333333333333, 0.0, 0.0, 26.0, 25.61472691908663, 0.4876697806747499, 1.0, 1.0, 165357.756418384], 
processed observation next is [1.0, 0.782608695652174, 0.3610341643582641, 0.2933333333333333, 0.0, 0.0, 0.6666666666666666, 0.6345605765905523, 0.6625565935582499, 1.0, 1.0, 0.7874178877065905], 
reward next is 0.2126, 
noisyNet noise sample is [array([0.6631099], dtype=float32), 0.37044764]. 
=============================================
[2019-04-04 04:29:38,297] A3C_AGENT_WORKER-Thread-11 INFO:Local step 305000, global step 4869054: loss 0.1407
[2019-04-04 04:29:38,297] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 305000, global step 4869054: learning rate 0.0005
[2019-04-04 04:29:39,151] A3C_AGENT_WORKER-Thread-12 INFO:Local step 304000, global step 4869443: loss 0.2114
[2019-04-04 04:29:39,152] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 304000, global step 4869443: learning rate 0.0005
[2019-04-04 04:29:39,241] A3C_AGENT_WORKER-Thread-4 INFO:Local step 302500, global step 4869485: loss 0.5659
[2019-04-04 04:29:39,245] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 302500, global step 4869486: learning rate 0.0005
[2019-04-04 04:29:39,357] A3C_AGENT_WORKER-Thread-19 INFO:Local step 305500, global step 4869533: loss 0.1026
[2019-04-04 04:29:39,358] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 305500, global step 4869534: learning rate 0.0005
[2019-04-04 04:29:39,380] A3C_AGENT_WORKER-Thread-16 INFO:Local step 304000, global step 4869543: loss 0.1951
[2019-04-04 04:29:39,381] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 304000, global step 4869543: learning rate 0.0005
[2019-04-04 04:29:39,708] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:29:39,709] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:29:39,725] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run37
[2019-04-04 04:29:40,035] A3C_AGENT_WORKER-Thread-3 INFO:Local step 304500, global step 4869823: loss 0.0458
[2019-04-04 04:29:40,037] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 304500, global step 4869824: learning rate 0.0005
[2019-04-04 04:29:40,860] A3C_AGENT_WORKER-Thread-14 INFO:Local step 303000, global step 4870162: loss 0.0139
[2019-04-04 04:29:40,861] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 303000, global step 4870162: learning rate 0.0005
[2019-04-04 04:29:41,459] A3C_AGENT_WORKER-Thread-13 INFO:Local step 302500, global step 4870401: loss 0.5556
[2019-04-04 04:29:41,460] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 302500, global step 4870401: learning rate 0.0005
[2019-04-04 04:29:41,590] A3C_AGENT_WORKER-Thread-15 INFO:Local step 305000, global step 4870444: loss 0.1346
[2019-04-04 04:29:41,591] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 305000, global step 4870444: learning rate 0.0005
[2019-04-04 04:29:42,345] A3C_AGENT_WORKER-Thread-6 INFO:Local step 304500, global step 4870782: loss 0.0318
[2019-04-04 04:29:42,348] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 304500, global step 4870782: learning rate 0.0005
[2019-04-04 04:29:45,028] A3C_AGENT_WORKER-Thread-17 INFO:Local step 305500, global step 4872008: loss 0.0708
[2019-04-04 04:29:45,035] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 305500, global step 4872010: learning rate 0.0005
[2019-04-04 04:29:47,131] A3C_AGENT_WORKER-Thread-18 INFO:Local step 305000, global step 4873015: loss 0.0537
[2019-04-04 04:29:47,132] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 305000, global step 4873015: learning rate 0.0005
[2019-04-04 04:29:49,240] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.5672719e-22 3.7576105e-16 5.1531970e-16 9.9999988e-01 6.5158750e-21
 7.1182463e-08 6.1083315e-16], sum to 1.0000
[2019-04-04 04:29:49,251] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3393
[2019-04-04 04:29:49,274] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [11.83333333333333, 19.16666666666667, 106.0, 794.3333333333334, 26.0, 28.51536880594707, 1.090756460165643, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5064600.0000, 
sim time next is 5065200.0000, 
raw observation next is [12.0, 19.0, 103.5, 782.0, 26.0, 28.55251997499358, 1.108488460978931, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.7950138504155125, 0.19, 0.345, 0.8640883977900552, 0.6666666666666666, 0.8793766645827983, 0.8694961536596436, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.34810513], dtype=float32), -0.9188202]. 
=============================================
[2019-04-04 04:29:51,726] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:29:51,727] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:29:51,744] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run37
[2019-04-04 04:29:51,841] A3C_AGENT_WORKER-Thread-20 INFO:Local step 305000, global step 4875158: loss 0.0138
[2019-04-04 04:29:51,843] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 305000, global step 4875158: learning rate 0.0005
[2019-04-04 04:29:52,469] A3C_AGENT_WORKER-Thread-5 INFO:Local step 304500, global step 4875410: loss 0.0902
[2019-04-04 04:29:52,470] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 304500, global step 4875410: learning rate 0.0005
[2019-04-04 04:29:52,502] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.1115886e-27 3.3921340e-21 5.3669098e-22 1.0000000e+00 1.4399645e-26
 2.1033387e-16 1.1800662e-20], sum to 1.0000
[2019-04-04 04:29:52,502] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6767
[2019-04-04 04:29:52,548] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.5, 54.5, 114.0, 816.0, 26.0, 25.18572063992304, 0.4333187147569268, 0.0, 1.0, 34169.79310811562], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3583800.0000, 
sim time next is 3584400.0000, 
raw observation next is [-3.333333333333333, 54.66666666666667, 114.6666666666667, 817.1666666666666, 26.0, 25.147858955827, 0.441886419014404, 0.0, 1.0, 45450.49368042166], 
processed observation next is [0.0, 0.4782608695652174, 0.37026777469990774, 0.5466666666666667, 0.38222222222222235, 0.9029465930018415, 0.6666666666666666, 0.5956549129855834, 0.6472954730048014, 0.0, 1.0, 0.2164309222877222], 
reward next is 0.7836, 
noisyNet noise sample is [array([0.09313639], dtype=float32), -1.0166166]. 
=============================================
[2019-04-04 04:29:54,123] A3C_AGENT_WORKER-Thread-10 INFO:Local step 305000, global step 4876143: loss 0.0966
[2019-04-04 04:29:54,129] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 305000, global step 4876143: learning rate 0.0005
[2019-04-04 04:29:55,801] A3C_AGENT_WORKER-Thread-3 INFO:Local step 305000, global step 4876815: loss 0.1046
[2019-04-04 04:29:55,803] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 305000, global step 4876815: learning rate 0.0005
[2019-04-04 04:29:56,103] A3C_AGENT_WORKER-Thread-11 INFO:Local step 305500, global step 4876922: loss -0.1407
[2019-04-04 04:29:56,115] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 305500, global step 4876922: learning rate 0.0005
[2019-04-04 04:29:56,986] A3C_AGENT_WORKER-Thread-4 INFO:Local step 303000, global step 4877331: loss 0.0033
[2019-04-04 04:29:56,995] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 303000, global step 4877333: learning rate 0.0005
[2019-04-04 04:29:57,009] A3C_AGENT_WORKER-Thread-12 INFO:Local step 304500, global step 4877343: loss 0.0477
[2019-04-04 04:29:57,010] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 304500, global step 4877343: learning rate 0.0005
[2019-04-04 04:29:57,337] A3C_AGENT_WORKER-Thread-16 INFO:Local step 304500, global step 4877490: loss 0.0362
[2019-04-04 04:29:57,338] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 304500, global step 4877490: learning rate 0.0005
[2019-04-04 04:29:57,519] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:29:57,519] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:29:57,524] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run37
[2019-04-04 04:29:57,752] A3C_AGENT_WORKER-Thread-14 INFO:Local step 303500, global step 4877648: loss 0.0855
[2019-04-04 04:29:57,756] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 303500, global step 4877648: learning rate 0.0005
[2019-04-04 04:29:58,401] A3C_AGENT_WORKER-Thread-6 INFO:Local step 305000, global step 4877926: loss 0.2628
[2019-04-04 04:29:58,405] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 305000, global step 4877928: learning rate 0.0005
[2019-04-04 04:29:58,935] A3C_AGENT_WORKER-Thread-13 INFO:Local step 303000, global step 4878161: loss 0.0315
[2019-04-04 04:29:58,936] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 303000, global step 4878161: learning rate 0.0005
[2019-04-04 04:29:59,485] A3C_AGENT_WORKER-Thread-15 INFO:Local step 305500, global step 4878406: loss 0.0212
[2019-04-04 04:29:59,487] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 305500, global step 4878407: learning rate 0.0005
[2019-04-04 04:30:05,497] A3C_AGENT_WORKER-Thread-18 INFO:Local step 305500, global step 4880895: loss 0.0092
[2019-04-04 04:30:05,499] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 305500, global step 4880895: learning rate 0.0005
[2019-04-04 04:30:07,643] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.3380354e-24 5.3845975e-18 7.1077297e-20 2.3183686e-12 8.8145369e-24
 1.0000000e+00 1.0801761e-18], sum to 1.0000
[2019-04-04 04:30:07,644] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5013
[2019-04-04 04:30:07,657] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.4, 72.5, 0.0, 0.0, 26.0, 25.3550926607775, 0.4812559239538068, 0.0, 1.0, 43665.50261112143], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4491000.0000, 
sim time next is 4491600.0000, 
raw observation next is [-0.4333333333333333, 72.66666666666667, 0.0, 0.0, 26.0, 25.35175481868094, 0.4781460428166978, 0.0, 1.0, 43549.13731979762], 
processed observation next is [1.0, 1.0, 0.45060018467220686, 0.7266666666666667, 0.0, 0.0, 0.6666666666666666, 0.6126462348900784, 0.6593820142722326, 0.0, 1.0, 0.20737684437998868], 
reward next is 0.7926, 
noisyNet noise sample is [array([1.4085959], dtype=float32), -1.3863897]. 
=============================================
[2019-04-04 04:30:08,421] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.1704246e-27 9.8290176e-23 1.1049238e-24 1.5267040e-07 9.0441262e-27
 9.9999988e-01 1.9045627e-23], sum to 1.0000
[2019-04-04 04:30:08,421] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8026
[2019-04-04 04:30:08,475] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.699999999999999, 82.66666666666667, 39.33333333333334, 0.0, 26.0, 24.4981738175148, 0.1744603617539832, 0.0, 1.0, 23245.98003180679], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 57000.0000, 
sim time next is 57600.0000, 
raw observation next is [6.6, 82.0, 34.0, 0.0, 26.0, 24.537633821456, 0.1729135786219354, 0.0, 1.0, 18738.94460268019], 
processed observation next is [0.0, 0.6956521739130435, 0.6454293628808865, 0.82, 0.11333333333333333, 0.0, 0.6666666666666666, 0.5448028184546668, 0.5576378595406452, 0.0, 1.0, 0.08923306953657233], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.88056666], dtype=float32), 0.23273797]. 
=============================================
[2019-04-04 04:30:08,594] A3C_AGENT_WORKER-Thread-5 INFO:Local step 305000, global step 4882249: loss 0.0027
[2019-04-04 04:30:08,607] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 305000, global step 4882252: learning rate 0.0005
[2019-04-04 04:30:08,901] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:30:08,901] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:30:08,914] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run37
[2019-04-04 04:30:09,679] A3C_AGENT_WORKER-Thread-20 INFO:Local step 305500, global step 4882689: loss 0.0103
[2019-04-04 04:30:09,680] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 305500, global step 4882689: learning rate 0.0005
[2019-04-04 04:30:12,165] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:30:12,165] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:30:12,170] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run37
[2019-04-04 04:30:12,250] A3C_AGENT_WORKER-Thread-10 INFO:Local step 305500, global step 4883760: loss 0.0204
[2019-04-04 04:30:12,251] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 305500, global step 4883760: learning rate 0.0005
[2019-04-04 04:30:12,453] A3C_AGENT_WORKER-Thread-12 INFO:Local step 305000, global step 4883838: loss 0.0583
[2019-04-04 04:30:12,454] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 305000, global step 4883838: learning rate 0.0005
[2019-04-04 04:30:13,416] A3C_AGENT_WORKER-Thread-16 INFO:Local step 305000, global step 4884188: loss 0.0620
[2019-04-04 04:30:13,419] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 305000, global step 4884188: learning rate 0.0005
[2019-04-04 04:30:13,982] A3C_AGENT_WORKER-Thread-3 INFO:Local step 305500, global step 4884379: loss 0.0087
[2019-04-04 04:30:13,984] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 305500, global step 4884379: learning rate 0.0005
[2019-04-04 04:30:14,201] A3C_AGENT_WORKER-Thread-4 INFO:Local step 303500, global step 4884467: loss 0.0674
[2019-04-04 04:30:14,207] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.95838610e-27 3.99204765e-22 1.96967516e-21 1.00000000e+00
 1.21254385e-26 8.25128626e-18 1.59397316e-20], sum to 1.0000
[2019-04-04 04:30:14,207] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3549
[2019-04-04 04:30:14,214] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 303500, global step 4884473: learning rate 0.0005
[2019-04-04 04:30:14,220] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 29.0, 119.5, 824.0, 26.0, 26.67926989772284, 0.6126595100795299, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4964400.0000, 
sim time next is 4965000.0000, 
raw observation next is [3.5, 28.33333333333334, 120.3333333333333, 831.0, 26.0, 26.6679886594922, 0.6278549586010821, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.5595567867036012, 0.2833333333333334, 0.401111111111111, 0.918232044198895, 0.6666666666666666, 0.7223323882910165, 0.7092849862003607, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.37367812], dtype=float32), -0.3223995]. 
=============================================
[2019-04-04 04:30:14,226] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[86.994865]
 [87.16749 ]
 [87.40469 ]
 [87.50165 ]
 [87.549255]], R is [[87.08088684]
 [87.21007538]
 [87.33797455]
 [87.46459198]
 [87.58995056]].
[2019-04-04 04:30:15,377] A3C_AGENT_WORKER-Thread-14 INFO:Local step 304000, global step 4884944: loss 0.3391
[2019-04-04 04:30:15,389] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 304000, global step 4884945: learning rate 0.0005
[2019-04-04 04:30:15,966] A3C_AGENT_WORKER-Thread-6 INFO:Local step 305500, global step 4885217: loss 0.0067
[2019-04-04 04:30:15,967] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 305500, global step 4885217: learning rate 0.0005
[2019-04-04 04:30:16,198] A3C_AGENT_WORKER-Thread-13 INFO:Local step 303500, global step 4885315: loss 0.0310
[2019-04-04 04:30:16,201] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 303500, global step 4885317: learning rate 0.0005
[2019-04-04 04:30:16,446] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.0931015e-24 7.3892868e-21 1.0220139e-21 1.0000000e+00 1.4708446e-24
 1.0630666e-14 6.2463112e-19], sum to 1.0000
[2019-04-04 04:30:16,447] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3560
[2019-04-04 04:30:16,461] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 45.0, 122.3333333333333, 352.0, 26.0, 25.15653364614954, 0.3715209880926828, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4896600.0000, 
sim time next is 4897200.0000, 
raw observation next is [3.0, 45.0, 112.1666666666667, 334.5, 26.0, 25.15587321460182, 0.3760306354317995, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.5457063711911359, 0.45, 0.373888888888889, 0.3696132596685083, 0.6666666666666666, 0.5963227678834849, 0.6253435451439332, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4121219], dtype=float32), 0.007797197]. 
=============================================
[2019-04-04 04:30:17,066] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.4810331e-18 3.7781559e-14 8.3889830e-15 7.2081575e-06 4.8987898e-18
 9.9999285e-01 1.0515205e-13], sum to 1.0000
[2019-04-04 04:30:17,066] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3303
[2019-04-04 04:30:17,086] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 55.33333333333334, 0.0, 0.0, 26.0, 25.70402767622361, 0.5119770351724435, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4670400.0000, 
sim time next is 4671000.0000, 
raw observation next is [2.0, 57.0, 0.0, 0.0, 26.0, 25.69144887451136, 0.495896765555772, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.518005540166205, 0.57, 0.0, 0.0, 0.6666666666666666, 0.6409540728759465, 0.665298921851924, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1739595], dtype=float32), -0.67438304]. 
=============================================
[2019-04-04 04:30:17,135] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[80.77065 ]
 [80.7266  ]
 [80.7155  ]
 [80.75321 ]
 [80.762375]], R is [[81.07932281]
 [81.2685318 ]
 [81.45584869]
 [81.64128876]
 [81.7355957 ]].
[2019-04-04 04:30:17,501] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:30:17,502] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:30:17,510] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run37
[2019-04-04 04:30:21,743] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:30:21,743] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:30:21,762] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run37
[2019-04-04 04:30:22,624] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.6522627e-27 1.8983694e-22 1.6005268e-23 2.5407609e-17 2.4043220e-26
 1.0000000e+00 4.6191943e-22], sum to 1.0000
[2019-04-04 04:30:22,624] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3893
[2019-04-04 04:30:22,671] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.283333333333333, 86.0, 69.33333333333334, 0.0, 26.0, 24.48123522851534, 0.1508318691698967, 0.0, 1.0, 23090.32212303174], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 53400.0000, 
sim time next is 54000.0000, 
raw observation next is [7.2, 86.0, 64.5, 0.0, 26.0, 24.4565334770847, 0.150560064227158, 0.0, 1.0, 45492.63870419485], 
processed observation next is [0.0, 0.6521739130434783, 0.662049861495845, 0.86, 0.215, 0.0, 0.6666666666666666, 0.5380444564237251, 0.5501866880757194, 0.0, 1.0, 0.21663161287711832], 
reward next is 0.7834, 
noisyNet noise sample is [array([-0.34147593], dtype=float32), 0.1479635]. 
=============================================
[2019-04-04 04:30:22,691] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[82.241234]
 [82.42647 ]
 [82.6102  ]
 [82.68896 ]
 [82.75836 ]], R is [[82.10169983]
 [82.17073059]
 [82.25977325]
 [82.29403687]
 [82.36296844]].
[2019-04-04 04:30:23,610] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.23705455e-18 9.12135222e-15 1.24096466e-13 9.74781275e-01
 2.60968101e-18 2.52187122e-02 1.04498915e-13], sum to 1.0000
[2019-04-04 04:30:23,610] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2709
[2019-04-04 04:30:23,621] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 48.66666666666667, 0.0, 0.0, 26.0, 25.52058675455217, 0.4003067365979677, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5030400.0000, 
sim time next is 5031000.0000, 
raw observation next is [-1.0, 48.0, 0.0, 0.0, 26.0, 25.52492048372715, 0.3854003070124299, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.4349030470914128, 0.48, 0.0, 0.0, 0.6666666666666666, 0.6270767069772626, 0.6284667690041433, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7701757], dtype=float32), 1.1961029]. 
=============================================
[2019-04-04 04:30:23,648] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[84.72057 ]
 [84.77548 ]
 [84.782524]
 [84.8232  ]
 [84.90319 ]], R is [[84.82227325]
 [84.97405243]
 [85.03496552]
 [84.96572113]
 [84.95508575]].
[2019-04-04 04:30:24,523] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:30:24,523] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:30:24,530] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run37
[2019-04-04 04:30:26,194] A3C_AGENT_WORKER-Thread-5 INFO:Local step 305500, global step 4889009: loss 0.0078
[2019-04-04 04:30:26,195] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 305500, global step 4889009: learning rate 0.0005
[2019-04-04 04:30:26,355] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:30:26,356] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:30:26,360] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run37
[2019-04-04 04:30:26,563] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.6180275e-16 2.7461410e-12 1.3996539e-13 9.9998128e-01 3.0927910e-16
 1.8722978e-05 1.1503614e-11], sum to 1.0000
[2019-04-04 04:30:26,563] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2038
[2019-04-04 04:30:26,581] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 45.0, 175.1666666666667, 414.0, 26.0, 25.09188294447804, 0.3758644386309666, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4893600.0000, 
sim time next is 4894200.0000, 
raw observation next is [3.0, 45.0, 163.0, 422.0, 26.0, 25.10344056740801, 0.3766863630447593, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.5457063711911359, 0.45, 0.5433333333333333, 0.4662983425414365, 0.6666666666666666, 0.5919533806173343, 0.6255621210149197, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5057804], dtype=float32), 1.9466519]. 
=============================================
[2019-04-04 04:30:27,781] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.9333598e-29 2.3002089e-23 7.0069497e-24 8.2485397e-20 2.9728499e-27
 1.0000000e+00 2.5612263e-24], sum to 1.0000
[2019-04-04 04:30:27,795] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3643
[2019-04-04 04:30:27,823] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.35388918251171, 0.4436653751289545, 0.0, 1.0, 56577.07901629811], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3888000.0000, 
sim time next is 3888600.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.3297897495291, 0.4405908116177775, 0.0, 1.0, 47102.0047708314], 
processed observation next is [1.0, 0.0, 0.40720221606648205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6108158124607582, 0.6468636038725925, 0.0, 1.0, 0.22429526081348286], 
reward next is 0.7757, 
noisyNet noise sample is [array([-0.37278184], dtype=float32), -1.6855003]. 
=============================================
[2019-04-04 04:30:28,411] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:30:28,411] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:30:28,419] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run37
[2019-04-04 04:30:30,458] A3C_AGENT_WORKER-Thread-12 INFO:Local step 305500, global step 4890188: loss 0.0379
[2019-04-04 04:30:30,459] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 305500, global step 4890188: learning rate 0.0005
[2019-04-04 04:30:32,188] A3C_AGENT_WORKER-Thread-4 INFO:Local step 304000, global step 4890704: loss 0.3511
[2019-04-04 04:30:32,188] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 304000, global step 4890704: learning rate 0.0005
[2019-04-04 04:30:32,193] A3C_AGENT_WORKER-Thread-16 INFO:Local step 305500, global step 4890705: loss 0.0183
[2019-04-04 04:30:32,221] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 305500, global step 4890710: learning rate 0.0005
[2019-04-04 04:30:34,065] A3C_AGENT_WORKER-Thread-14 INFO:Local step 304500, global step 4891310: loss 0.1560
[2019-04-04 04:30:34,080] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 304500, global step 4891313: learning rate 0.0005
[2019-04-04 04:30:34,161] A3C_AGENT_WORKER-Thread-13 INFO:Local step 304000, global step 4891347: loss 0.0498
[2019-04-04 04:30:34,163] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 304000, global step 4891347: learning rate 0.0005
[2019-04-04 04:30:34,556] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.6253538e-14 3.5475078e-10 5.5967980e-11 1.1627052e-07 3.9384643e-14
 9.9999988e-01 1.8842650e-10], sum to 1.0000
[2019-04-04 04:30:34,559] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5375
[2019-04-04 04:30:34,626] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.4, 61.0, 157.0, 308.0, 26.0, 25.56021909313858, 0.3877144403549268, 1.0, 1.0, 47599.55374747247], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 129600.0000, 
sim time next is 130200.0000, 
raw observation next is [-8.3, 61.0, 148.0, 406.3333333333334, 26.0, 25.69899435093016, 0.406518619428393, 1.0, 1.0, 42284.85913361362], 
processed observation next is [1.0, 0.5217391304347826, 0.23268698060941828, 0.61, 0.49333333333333335, 0.44898710865561703, 0.6666666666666666, 0.6415828625775134, 0.635506206476131, 1.0, 1.0, 0.20135647206482676], 
reward next is 0.7986, 
noisyNet noise sample is [array([-1.151088], dtype=float32), -0.41568306]. 
=============================================
[2019-04-04 04:30:39,410] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:30:39,410] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:30:39,414] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run37
[2019-04-04 04:30:41,067] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.2566946e-14 4.3410342e-08 1.7165896e-09 1.6397135e-01 1.0701105e-12
 8.3602858e-01 9.4661994e-09], sum to 1.0000
[2019-04-04 04:30:41,068] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8331
[2019-04-04 04:30:41,081] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 73.0, 0.0, 0.0, 26.0, 24.07328382146233, 0.07666624110539327, 0.0, 1.0, 44488.29924150879], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 261000.0000, 
sim time next is 261600.0000, 
raw observation next is [-5.966666666666667, 71.0, 0.0, 0.0, 26.0, 24.10110719567387, 0.07727813166230023, 0.0, 1.0, 44567.70935950203], 
processed observation next is [1.0, 0.0, 0.2973222530009234, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5084255996394891, 0.5257593772207667, 0.0, 1.0, 0.21222718742620011], 
reward next is 0.7878, 
noisyNet noise sample is [array([1.5048605], dtype=float32), 0.019625643]. 
=============================================
[2019-04-04 04:30:43,246] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:30:43,246] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:30:43,265] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run37
[2019-04-04 04:30:44,632] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:30:44,633] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:30:44,639] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run37
[2019-04-04 04:30:54,474] A3C_AGENT_WORKER-Thread-14 INFO:Local step 305000, global step 4896806: loss 0.0052
[2019-04-04 04:30:54,474] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 305000, global step 4896806: learning rate 0.0005
[2019-04-04 04:30:56,639] A3C_AGENT_WORKER-Thread-4 INFO:Local step 304500, global step 4897242: loss 0.3164
[2019-04-04 04:30:56,639] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 304500, global step 4897242: learning rate 0.0005
[2019-04-04 04:30:59,578] A3C_AGENT_WORKER-Thread-13 INFO:Local step 304500, global step 4897817: loss 0.5181
[2019-04-04 04:30:59,584] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 304500, global step 4897818: learning rate 0.0005
[2019-04-04 04:31:10,956] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-04 04:31:10,961] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 04:31:10,961] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:31:10,963] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run50
[2019-04-04 04:31:11,037] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 04:31:11,037] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:31:11,043] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 04:31:11,044] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:31:11,047] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run50
[2019-04-04 04:31:11,113] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run50
[2019-04-04 04:33:20,781] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.179125], dtype=float32), 0.008694951]
[2019-04-04 04:33:20,782] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [2.466666666666667, 29.33333333333333, 186.6666666666667, 328.6666666666667, 26.0, 25.43056814658269, 0.2901059140682472, 1.0, 1.0, 0.0]
[2019-04-04 04:33:20,782] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 04:33:20,783] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [4.6572892e-29 1.3803542e-26 2.0698897e-26 1.0000000e+00 2.7239405e-29
 5.2876556e-22 1.7005435e-24], sampled 0.9051964014808523
[2019-04-04 04:33:32,873] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.179125], dtype=float32), 0.008694951]
[2019-04-04 04:33:32,873] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [5.183333333333334, 75.33333333333334, 163.3333333333333, 378.6666666666667, 26.0, 26.35080549023083, 0.747956014937134, 1.0, 1.0, 0.0]
[2019-04-04 04:33:32,873] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 04:33:32,874] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.4056665e-22 1.9908588e-19 1.0554350e-19 1.0000000e+00 2.2853861e-21
 1.0845121e-09 3.4194222e-18], sampled 0.629751365107062
[2019-04-04 04:34:07,322] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.179125], dtype=float32), 0.008694951]
[2019-04-04 04:34:07,322] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [12.0, 24.0, 113.5, 789.5, 26.0, 25.69035128039408, 0.4995033440372665, 0.0, 1.0, 0.0]
[2019-04-04 04:34:07,323] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 04:34:07,324] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.3247073e-29 5.7876181e-25 9.4090073e-25 1.0000000e+00 3.5617679e-28
 1.6234497e-17 1.5755807e-23], sampled 0.25670712387091943
[2019-04-04 04:34:12,724] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.179125], dtype=float32), 0.008694951]
[2019-04-04 04:34:12,724] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [4.231811024166667, 43.00655208166667, 112.2373393333333, 799.8520324333333, 26.0, 25.58064750635883, 0.4857514946602057, 0.0, 1.0, 0.0]
[2019-04-04 04:34:12,724] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 04:34:12,725] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [3.8178846e-27 6.4516118e-23 1.3806150e-22 1.0000000e+00 3.7714928e-26
 9.3788722e-17 1.9973453e-21], sampled 0.38623853489152515
[2019-04-04 04:34:20,709] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7354.8271 239646316.7057 1604.9210
[2019-04-04 04:34:49,587] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7242.2714 263283005.3523 1555.9808
[2019-04-04 04:34:56,249] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.9733 275735612.4904 1232.5533
[2019-04-04 04:34:57,285] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 4900000, evaluation results [4900000.0, 7242.271403084141, 263283005.35233024, 1555.9807882157436, 7354.827063306185, 239646316.70569882, 1604.9210332400548, 7182.973273855296, 275735612.49038523, 1232.5533206192808]
[2019-04-04 04:34:58,501] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.4919377e-15 1.8092715e-10 7.7909061e-11 9.0158409e-01 6.1824250e-14
 9.8415934e-02 1.2173640e-10], sum to 1.0000
[2019-04-04 04:34:58,512] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2299
[2019-04-04 04:34:58,585] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.600000000000001, 100.0, 0.0, 0.0, 26.0, 25.05932246039791, 0.3198337253251845, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 933600.0000, 
sim time next is 934200.0000, 
raw observation next is [4.7, 100.0, 0.0, 0.0, 26.0, 24.94851602393433, 0.2998132276665018, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.592797783933518, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5790430019945276, 0.5999377425555006, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.16661401], dtype=float32), -0.14114787]. 
=============================================
[2019-04-04 04:35:13,503] A3C_AGENT_WORKER-Thread-4 INFO:Local step 305000, global step 4903944: loss 0.0216
[2019-04-04 04:35:13,514] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 305000, global step 4903945: learning rate 0.0005
[2019-04-04 04:35:13,548] A3C_AGENT_WORKER-Thread-14 INFO:Local step 305500, global step 4903954: loss 0.2114
[2019-04-04 04:35:13,548] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 305500, global step 4903954: learning rate 0.0005
[2019-04-04 04:35:15,927] A3C_AGENT_WORKER-Thread-13 INFO:Local step 305000, global step 4904573: loss 0.0701
[2019-04-04 04:35:15,928] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 305000, global step 4904573: learning rate 0.0005
[2019-04-04 04:35:20,578] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.3814885e-26 2.9421369e-22 3.8609410e-21 1.0000000e+00 3.7957571e-25
 3.7127324e-14 5.2884826e-22], sum to 1.0000
[2019-04-04 04:35:20,578] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3164
[2019-04-04 04:35:20,615] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.7, 68.0, 120.0, 58.5, 26.0, 25.9431000454366, 0.3373905526114789, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 727200.0000, 
sim time next is 727800.0000, 
raw observation next is [-1.516666666666667, 67.66666666666667, 126.3333333333333, 61.66666666666666, 26.0, 25.93736332306815, 0.3319012270834454, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4205909510618652, 0.6766666666666667, 0.421111111111111, 0.06813996316758747, 0.6666666666666666, 0.6614469435890126, 0.6106337423611484, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.269867], dtype=float32), 0.061945617]. 
=============================================
[2019-04-04 04:35:25,728] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2102823e-29 5.0509585e-25 1.2183860e-26 1.0000000e+00 1.7274835e-29
 1.3502110e-18 7.7529983e-24], sum to 1.0000
[2019-04-04 04:35:25,729] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0803
[2019-04-04 04:35:25,761] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 69.0, 0.0, 0.0, 26.0, 24.82326656050582, 0.1773844430170915, 0.0, 1.0, 41969.45067589866], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 684000.0000, 
sim time next is 684600.0000, 
raw observation next is [-3.483333333333333, 69.33333333333333, 0.0, 0.0, 26.0, 24.79246893255126, 0.1696096387181472, 0.0, 1.0, 41903.04399760332], 
processed observation next is [0.0, 0.9565217391304348, 0.3661126500461681, 0.6933333333333332, 0.0, 0.0, 0.6666666666666666, 0.5660390777126049, 0.5565365462393824, 0.0, 1.0, 0.199538304750492], 
reward next is 0.8005, 
noisyNet noise sample is [array([-0.29855126], dtype=float32), -3.2487814]. 
=============================================
[2019-04-04 04:35:29,658] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:35:29,659] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:35:29,662] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run37
[2019-04-04 04:35:33,425] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.1454069e-28 5.1544517e-25 1.4569161e-26 1.0000000e+00 7.0309260e-28
 5.7416960e-27 2.3212584e-23], sum to 1.0000
[2019-04-04 04:35:33,425] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0609
[2019-04-04 04:35:33,476] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.2, 60.0, 131.5, 74.5, 26.0, 24.91618101702727, 0.2352974463651119, 0.0, 1.0, 36894.04043530174], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 655200.0000, 
sim time next is 655800.0000, 
raw observation next is [-1.1, 59.0, 114.6666666666667, 68.33333333333333, 26.0, 24.91882384187479, 0.2313743854369941, 0.0, 1.0, 34446.33941700976], 
processed observation next is [0.0, 0.6086956521739131, 0.4321329639889197, 0.59, 0.38222222222222235, 0.07550644567219153, 0.6666666666666666, 0.5765686534895659, 0.5771247951456647, 0.0, 1.0, 0.1640301877000465], 
reward next is 0.8360, 
noisyNet noise sample is [array([-1.8997434], dtype=float32), -1.7870976]. 
=============================================
[2019-04-04 04:35:34,624] A3C_AGENT_WORKER-Thread-4 INFO:Local step 305500, global step 4910605: loss 0.1199
[2019-04-04 04:35:34,626] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 305500, global step 4910605: learning rate 0.0005
[2019-04-04 04:35:37,458] A3C_AGENT_WORKER-Thread-13 INFO:Local step 305500, global step 4911641: loss 0.1082
[2019-04-04 04:35:37,459] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 305500, global step 4911641: learning rate 0.0005
[2019-04-04 04:35:38,656] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.54831093e-22 1.29439108e-17 1.06016935e-17 1.00000000e+00
 3.89921712e-21 2.51196425e-10 6.44191784e-17], sum to 1.0000
[2019-04-04 04:35:38,656] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6716
[2019-04-04 04:35:38,678] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.666666666666667, 95.0, 85.5, 590.0, 26.0, 26.19670465279594, 0.6130211774665197, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1509600.0000, 
sim time next is 1510200.0000, 
raw observation next is [3.85, 94.5, 88.0, 708.0, 26.0, 26.27412276656028, 0.6351394697690458, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.569252077562327, 0.945, 0.29333333333333333, 0.7823204419889502, 0.6666666666666666, 0.6895102305466899, 0.7117131565896818, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.744063], dtype=float32), 0.51150966]. 
=============================================
[2019-04-04 04:35:46,879] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.84477171e-19 4.23090619e-14 1.41370426e-14 9.99434173e-01
 1.12189398e-17 5.65801805e-04 2.77313166e-13], sum to 1.0000
[2019-04-04 04:35:46,879] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5373
[2019-04-04 04:35:46,899] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.7, 81.0, 0.0, 0.0, 26.0, 25.37191218771324, 0.4500763274793298, 0.0, 1.0, 43761.77346665708], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 962400.0000, 
sim time next is 963000.0000, 
raw observation next is [7.7, 81.5, 0.0, 0.0, 26.0, 25.477079568403, 0.4509024472579714, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.6759002770083103, 0.815, 0.0, 0.0, 0.6666666666666666, 0.6230899640335833, 0.6503008157526572, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.04254332], dtype=float32), 1.3399659]. 
=============================================
[2019-04-04 04:35:46,949] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[94.357895]
 [94.36389 ]
 [94.20004 ]
 [93.902824]
 [93.66099 ]], R is [[94.38220978]
 [94.22999573]
 [94.05400085]
 [93.76786804]
 [93.47377014]].
[2019-04-04 04:35:48,261] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:35:48,261] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:35:48,264] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run37
[2019-04-04 04:35:51,244] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:35:51,244] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:35:51,250] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run37
[2019-04-04 04:35:53,753] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1218148e-31 8.9010137e-22 5.0835325e-24 3.2676060e-03 2.2462504e-28
 9.9673235e-01 9.6718217e-26], sum to 1.0000
[2019-04-04 04:35:53,754] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8976
[2019-04-04 04:35:53,766] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [16.96666666666667, 72.33333333333334, 0.0, 0.0, 26.0, 24.52918592753695, 0.3595295794842905, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1201200.0000, 
sim time next is 1201800.0000, 
raw observation next is [16.78333333333333, 73.66666666666666, 0.0, 0.0, 26.0, 24.50701670920365, 0.3515220730157205, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.9130434782608695, 0.927516158818098, 0.7366666666666666, 0.0, 0.0, 0.6666666666666666, 0.5422513924336375, 0.6171740243385735, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3666164], dtype=float32), -0.6176231]. 
=============================================
[2019-04-04 04:36:02,053] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.6963339e-22 2.8085263e-18 7.2771214e-19 1.0000000e+00 2.3811432e-22
 1.2716746e-13 1.0989834e-18], sum to 1.0000
[2019-04-04 04:36:02,055] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0204
[2019-04-04 04:36:02,106] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.0, 71.0, 152.0, 40.5, 26.0, 24.98430507855928, 0.2632277552021051, 0.0, 1.0, 30981.62989478351], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1857600.0000, 
sim time next is 1858200.0000, 
raw observation next is [-4.916666666666667, 71.0, 141.3333333333333, 26.99999999999999, 26.0, 24.99250473780226, 0.2586676821989637, 0.0, 1.0, 32985.18835362862], 
processed observation next is [0.0, 0.5217391304347826, 0.32640812557710064, 0.71, 0.471111111111111, 0.029834254143646398, 0.6666666666666666, 0.5827087281501884, 0.586222560732988, 0.0, 1.0, 0.1570723254934696], 
reward next is 0.8429, 
noisyNet noise sample is [array([-0.80058545], dtype=float32), 0.89215803]. 
=============================================
[2019-04-04 04:36:02,857] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.6062302e-21 2.5455286e-15 6.4082463e-17 9.9987102e-01 1.6397247e-19
 1.2896641e-04 2.0996365e-16], sum to 1.0000
[2019-04-04 04:36:02,858] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2141
[2019-04-04 04:36:02,886] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.500000000000001, 100.0, 0.0, 0.0, 26.0, 25.45074478704143, 0.5948527813719071, 0.0, 1.0, 28915.03897675821], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1289400.0000, 
sim time next is 1290000.0000, 
raw observation next is [5.5, 100.0, 0.0, 0.0, 26.0, 25.45867246877445, 0.6000870404216211, 0.0, 1.0, 27893.08926620986], 
processed observation next is [0.0, 0.9565217391304348, 0.6149584487534627, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6215560390645374, 0.7000290134738737, 0.0, 1.0, 0.13282423460099932], 
reward next is 0.8672, 
noisyNet noise sample is [array([-0.11839186], dtype=float32), 0.7839707]. 
=============================================
[2019-04-04 04:36:02,892] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[83.93584 ]
 [83.988075]
 [83.995384]
 [83.91972 ]
 [83.84263 ]], R is [[83.95014954]
 [83.97296143]
 [83.96879578]
 [83.9205246 ]
 [83.82736969]].
[2019-04-04 04:36:09,093] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.7918427e-27 2.4203017e-22 3.0954375e-21 9.9999988e-01 1.9049812e-26
 8.1712493e-08 7.9684174e-21], sum to 1.0000
[2019-04-04 04:36:09,094] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4266
[2019-04-04 04:36:09,108] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [9.6, 83.0, 0.0, 0.0, 26.0, 25.55045603284462, 0.4361060655049731, 0.0, 1.0, 41678.84269595677], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 974400.0000, 
sim time next is 975000.0000, 
raw observation next is [9.8, 83.0, 0.0, 0.0, 26.0, 25.45490723367799, 0.4422914071906293, 0.0, 1.0, 87453.7879370735], 
processed observation next is [1.0, 0.2608695652173913, 0.7340720221606649, 0.83, 0.0, 0.0, 0.6666666666666666, 0.6212422694731657, 0.6474304690635431, 0.0, 1.0, 0.41644660922415955], 
reward next is 0.5836, 
noisyNet noise sample is [array([-0.7391753], dtype=float32), -1.3815397]. 
=============================================
[2019-04-04 04:36:09,117] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[92.37877]
 [92.24571]
 [92.09916]
 [91.97325]
 [91.88304]], R is [[92.25114441]
 [92.1301651 ]
 [92.2088623 ]
 [92.28677368]
 [92.36390686]].
[2019-04-04 04:36:09,870] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.6436989e-21 1.6490961e-18 4.4811352e-18 1.0000000e+00 2.8673148e-21
 8.9168739e-13 1.3407786e-16], sum to 1.0000
[2019-04-04 04:36:09,870] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1125
[2019-04-04 04:36:09,918] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.8, 62.0, 74.0, 0.0, 26.0, 25.63432717263352, 0.3237713636135149, 1.0, 1.0, 36291.59812105492], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1956600.0000, 
sim time next is 1957200.0000, 
raw observation next is [-2.8, 62.0, 66.66666666666667, 0.0, 26.0, 25.58891808738055, 0.3250816559478824, 1.0, 1.0, 29363.60324739462], 
processed observation next is [1.0, 0.6521739130434783, 0.38504155124653744, 0.62, 0.22222222222222224, 0.0, 0.6666666666666666, 0.6324098406150457, 0.6083605519826275, 1.0, 1.0, 0.13982668213045057], 
reward next is 0.8602, 
noisyNet noise sample is [array([-0.85652226], dtype=float32), 1.5169687]. 
=============================================
[2019-04-04 04:36:20,217] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.0868739e-23 2.3414063e-19 3.2181003e-18 1.0000000e+00 7.2986002e-22
 2.6638314e-14 5.4966884e-18], sum to 1.0000
[2019-04-04 04:36:20,217] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4004
[2019-04-04 04:36:20,230] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.1, 69.0, 0.0, 0.0, 26.0, 23.83867528842252, 0.003124231700112686, 0.0, 1.0, 45645.69555164583], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 268200.0000, 
sim time next is 268800.0000, 
raw observation next is [-8.366666666666667, 68.33333333333333, 0.0, 0.0, 26.0, 23.8331800980485, -0.01201382694605183, 0.0, 1.0, 45679.6305700183], 
processed observation next is [1.0, 0.08695652173913043, 0.23084025854108958, 0.6833333333333332, 0.0, 0.0, 0.6666666666666666, 0.4860983415040418, 0.4959953910179827, 0.0, 1.0, 0.21752205033342048], 
reward next is 0.7825, 
noisyNet noise sample is [array([1.169764], dtype=float32), -2.2073371]. 
=============================================
[2019-04-04 04:36:36,311] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.7362962e-22 7.7802499e-18 1.1105358e-16 1.0000000e+00 1.1424245e-21
 3.1018885e-08 9.3193490e-17], sum to 1.0000
[2019-04-04 04:36:36,313] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7807
[2019-04-04 04:36:36,339] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.9, 91.0, 0.0, 0.0, 26.0, 23.58457950909558, -0.03019869594764436, 0.0, 1.0, 43230.28071276065], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2267400.0000, 
sim time next is 2268000.0000, 
raw observation next is [-8.9, 91.0, 0.0, 0.0, 26.0, 23.6003872955986, -0.03514234754625559, 0.0, 1.0, 43205.08023782392], 
processed observation next is [1.0, 0.2608695652173913, 0.21606648199445982, 0.91, 0.0, 0.0, 0.6666666666666666, 0.46669894129988343, 0.48828588415124813, 0.0, 1.0, 0.20573847732297104], 
reward next is 0.7943, 
noisyNet noise sample is [array([1.3117787], dtype=float32), -0.77024406]. 
=============================================
[2019-04-04 04:36:36,350] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[82.06079 ]
 [82.04128 ]
 [82.021194]
 [81.991196]
 [81.98816 ]], R is [[82.03964996]
 [82.01339722]
 [81.98735809]
 [81.96143341]
 [81.93553162]].
[2019-04-04 04:36:39,891] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.5267616e-23 1.4505494e-19 4.0988984e-20 1.0000000e+00 6.1842517e-23
 2.1007612e-08 3.1970042e-19], sum to 1.0000
[2019-04-04 04:36:39,891] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8071
[2019-04-04 04:36:39,912] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.1, 92.0, 0.0, 0.0, 26.0, 25.39150465552319, 0.4632543411436816, 0.0, 1.0, 66602.60434824514], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1475400.0000, 
sim time next is 1476000.0000, 
raw observation next is [2.2, 92.0, 0.0, 0.0, 26.0, 25.34642191421198, 0.4678757689656323, 0.0, 1.0, 59154.93270060945], 
processed observation next is [1.0, 0.08695652173913043, 0.5235457063711911, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6122018261843317, 0.6559585896552108, 0.0, 1.0, 0.28169015571718786], 
reward next is 0.7183, 
noisyNet noise sample is [array([-0.70128095], dtype=float32), -0.9363807]. 
=============================================
[2019-04-04 04:36:39,925] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[83.43021 ]
 [83.25424 ]
 [83.067825]
 [82.97145 ]
 [82.948   ]], R is [[83.44313049]
 [83.29154968]
 [83.13748169]
 [83.04102325]
 [83.12135315]].
[2019-04-04 04:36:40,229] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.2469416e-26 3.2052173e-22 4.8278908e-22 1.0000000e+00 2.5078472e-25
 8.6072723e-14 8.4895235e-21], sum to 1.0000
[2019-04-04 04:36:40,232] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2633
[2019-04-04 04:36:40,248] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.666666666666667, 78.83333333333333, 0.0, 0.0, 26.0, 24.32802585771433, 0.1448325467216845, 0.0, 1.0, 46007.78260459084], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1818600.0000, 
sim time next is 1819200.0000, 
raw observation next is [-5.733333333333333, 79.66666666666667, 0.0, 0.0, 26.0, 24.29290767064881, 0.13912715284938, 0.0, 1.0, 46089.31530249972], 
processed observation next is [0.0, 0.043478260869565216, 0.30378578024007385, 0.7966666666666667, 0.0, 0.0, 0.6666666666666666, 0.5244089725540674, 0.54637571761646, 0.0, 1.0, 0.21947293001190343], 
reward next is 0.7805, 
noisyNet noise sample is [array([-0.9098998], dtype=float32), -1.7431461]. 
=============================================
[2019-04-04 04:36:43,131] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.9316882e-28 5.6584026e-24 3.6281477e-23 9.9999952e-01 1.1651533e-26
 4.7628131e-07 3.3698421e-23], sum to 1.0000
[2019-04-04 04:36:43,131] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7577
[2019-04-04 04:36:43,143] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.733333333333333, 76.33333333333333, 95.0, 700.3333333333334, 26.0, 25.30982282545867, 0.5485020362515236, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1515000.0000, 
sim time next is 1515600.0000, 
raw observation next is [7.2, 73.0, 92.5, 700.5, 26.0, 25.61553008932157, 0.5716962573185143, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.662049861495845, 0.73, 0.30833333333333335, 0.7740331491712708, 0.6666666666666666, 0.6346275074434642, 0.6905654191061714, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.32537302], dtype=float32), 0.40437856]. 
=============================================
[2019-04-04 04:36:47,067] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1857134e-24 2.8034834e-19 2.3778700e-19 1.0000000e+00 1.4964548e-23
 3.1319028e-13 1.5279948e-18], sum to 1.0000
[2019-04-04 04:36:47,067] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0398
[2019-04-04 04:36:47,085] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.52133190767552, 0.197815703482781, 0.0, 1.0, 42839.33549077449], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1986000.0000, 
sim time next is 1986600.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.48669685408947, 0.1906123076512668, 0.0, 1.0, 42754.12086925424], 
processed observation next is [1.0, 1.0, 0.30747922437673136, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5405580711741225, 0.5635374358837556, 0.0, 1.0, 0.20359105175835354], 
reward next is 0.7964, 
noisyNet noise sample is [array([1.8348444], dtype=float32), 0.97481495]. 
=============================================
[2019-04-04 04:36:48,768] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.20668125e-30 1.55181572e-24 9.61987080e-25 1.00000000e+00
 3.64336518e-30 8.56182414e-09 4.82781257e-24], sum to 1.0000
[2019-04-04 04:36:48,769] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1707
[2019-04-04 04:36:48,788] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.3, 69.5, 0.0, 0.0, 26.0, 25.28408574171292, 0.4094842988076843, 0.0, 1.0, 45030.62892302048], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2237400.0000, 
sim time next is 2238000.0000, 
raw observation next is [-5.4, 70.0, 0.0, 0.0, 26.0, 25.26503193266418, 0.4048234284443151, 0.0, 1.0, 44742.02102945004], 
processed observation next is [1.0, 0.9130434782608695, 0.31301939058171746, 0.7, 0.0, 0.0, 0.6666666666666666, 0.6054193277220149, 0.6349411428147717, 0.0, 1.0, 0.21305724299738116], 
reward next is 0.7869, 
noisyNet noise sample is [array([1.2952728], dtype=float32), 0.25090873]. 
=============================================
[2019-04-04 04:36:48,796] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[80.39664 ]
 [80.317635]
 [80.190895]
 [80.03672 ]
 [80.04713 ]], R is [[80.54534912]
 [80.52546692]
 [80.50317383]
 [80.47930908]
 [80.45057678]].
[2019-04-04 04:37:06,155] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.2518859e-24 5.0315620e-17 1.1320026e-19 5.7504374e-01 2.2591921e-23
 4.2495632e-01 2.9737176e-18], sum to 1.0000
[2019-04-04 04:37:06,155] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9002
[2019-04-04 04:37:06,161] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.6, 58.00000000000001, 0.0, 0.0, 26.0, 26.1490597253116, 0.7115997790940387, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1106400.0000, 
sim time next is 1107000.0000, 
raw observation next is [14.4, 58.5, 0.0, 0.0, 26.0, 26.01402915664162, 0.6952270672120603, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.8614958448753465, 0.585, 0.0, 0.0, 0.6666666666666666, 0.6678357630534683, 0.7317423557373535, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.05175884], dtype=float32), -1.2505517]. 
=============================================
[2019-04-04 04:37:06,164] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[93.55541]
 [96.10513]
 [94.02864]
 [93.4529 ]
 [95.1862 ]], R is [[91.6033783 ]
 [91.68734741]
 [91.77047729]
 [91.85277557]
 [91.93424988]].
[2019-04-04 04:37:06,605] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.2924960e-32 7.0312476e-28 4.6137385e-28 1.0000000e+00 9.9902289e-33
 3.1393132e-24 2.7136664e-26], sum to 1.0000
[2019-04-04 04:37:06,605] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9340
[2019-04-04 04:37:06,630] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.100000000000001, 69.66666666666667, 0.0, 0.0, 26.0, 23.50150830104977, -0.06887952134223574, 0.0, 1.0, 42122.60968695884], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 800400.0000, 
sim time next is 801000.0000, 
raw observation next is [-7.0, 69.0, 0.0, 0.0, 26.0, 23.48696784369654, -0.06522667693783678, 0.0, 1.0, 42110.8513989338], 
processed observation next is [1.0, 0.2608695652173913, 0.2686980609418283, 0.69, 0.0, 0.0, 0.6666666666666666, 0.457247320308045, 0.4782577743540544, 0.0, 1.0, 0.20052786380444668], 
reward next is 0.7995, 
noisyNet noise sample is [array([-0.3264968], dtype=float32), -0.4601411]. 
=============================================
[2019-04-04 04:37:06,636] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[80.09548 ]
 [80.03761 ]
 [80.012215]
 [80.02184 ]
 [80.02253 ]], R is [[80.17080688]
 [80.16851807]
 [80.1663208 ]
 [80.16423035]
 [80.16226196]].
[2019-04-04 04:37:10,065] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.6785456e-27 1.1038439e-18 1.4956377e-23 9.9999940e-01 1.8709605e-24
 5.8176386e-07 4.1615164e-23], sum to 1.0000
[2019-04-04 04:37:10,065] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4687
[2019-04-04 04:37:10,107] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [18.3, 65.0, 133.1666666666667, 0.0, 26.0, 25.04693626185877, 0.5004141684156086, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1174800.0000, 
sim time next is 1175400.0000, 
raw observation next is [18.3, 65.0, 128.0, 0.0, 26.0, 25.07110757847316, 0.5004046975042687, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.9695290858725764, 0.65, 0.4266666666666667, 0.0, 0.6666666666666666, 0.5892589648727634, 0.6668015658347562, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.39297503], dtype=float32), -1.2123228]. 
=============================================
[2019-04-04 04:37:13,357] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.1679230e-30 1.6425742e-24 5.6837381e-24 1.0000000e+00 1.9902346e-28
 3.2929804e-27 3.4898745e-24], sum to 1.0000
[2019-04-04 04:37:13,358] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9067
[2019-04-04 04:37:13,428] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.016666666666667, 35.16666666666666, 82.66666666666667, 811.6666666666667, 26.0, 24.98855071647758, 0.2499330865011199, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2459400.0000, 
sim time next is 2460000.0000, 
raw observation next is [-1.733333333333333, 34.33333333333334, 84.33333333333334, 820.3333333333334, 26.0, 24.98140855115674, 0.2498453064282617, 0.0, 1.0, 18734.07630688091], 
processed observation next is [0.0, 0.4782608695652174, 0.41458910433979695, 0.34333333333333343, 0.28111111111111114, 0.9064456721915286, 0.6666666666666666, 0.5817840459297283, 0.5832817688094206, 0.0, 1.0, 0.08920988717562338], 
reward next is 0.9108, 
noisyNet noise sample is [array([-0.350159], dtype=float32), 0.5412647]. 
=============================================
[2019-04-04 04:37:13,435] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[85.26427]
 [85.26538]
 [85.06859]
 [84.84067]
 [84.54797]], R is [[85.34253693]
 [85.48911285]
 [85.54497528]
 [85.49780273]
 [85.44641113]].
[2019-04-04 04:37:18,693] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.2572880e-32 1.3748972e-29 1.1473042e-26 1.0000000e+00 1.0957313e-32
 2.8998009e-26 1.6471961e-27], sum to 1.0000
[2019-04-04 04:37:18,693] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7206
[2019-04-04 04:37:18,737] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.1, 69.0, 140.5, 0.0, 26.0, 25.60340100760519, 0.363358981232521, 1.0, 1.0, 27877.57728224514], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2202000.0000, 
sim time next is 2202600.0000, 
raw observation next is [-4.0, 68.5, 138.0, 0.0, 26.0, 25.66215768942456, 0.3579155616970739, 1.0, 1.0, 25368.25418519403], 
processed observation next is [1.0, 0.4782608695652174, 0.3518005540166205, 0.685, 0.46, 0.0, 0.6666666666666666, 0.63851314078538, 0.619305187232358, 1.0, 1.0, 0.12080121040568585], 
reward next is 0.8792, 
noisyNet noise sample is [array([0.5372613], dtype=float32), 0.36938637]. 
=============================================
[2019-04-04 04:37:22,005] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1158268e-23 1.0216208e-19 4.3831908e-19 9.9999964e-01 2.4870525e-22
 3.2505039e-07 4.3581649e-19], sum to 1.0000
[2019-04-04 04:37:22,007] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1543
[2019-04-04 04:37:22,048] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [10.0, 92.0, 43.5, 0.0, 26.0, 26.14649745397769, 0.5740421598055695, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 982800.0000, 
sim time next is 983400.0000, 
raw observation next is [10.08333333333333, 92.16666666666667, 49.00000000000001, 0.0, 26.0, 26.24204596963417, 0.5790760914904138, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.7419205909510619, 0.9216666666666667, 0.16333333333333336, 0.0, 0.6666666666666666, 0.6868371641361808, 0.693025363830138, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.7280028], dtype=float32), -0.19228277]. 
=============================================
[2019-04-04 04:37:22,176] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0578279e-29 1.8339605e-26 7.4903588e-26 1.0000000e+00 8.8964325e-29
 2.6106172e-12 1.8462173e-25], sum to 1.0000
[2019-04-04 04:37:22,181] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5319
[2019-04-04 04:37:22,206] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [13.3, 80.0, 107.0, 117.0, 26.0, 26.96016378453706, 0.8361690827402987, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1072800.0000, 
sim time next is 1073400.0000, 
raw observation next is [13.66666666666667, 78.33333333333334, 109.3333333333333, 77.99999999999999, 26.0, 27.05460817096792, 0.8500444522079388, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.8411819021237306, 0.7833333333333334, 0.36444444444444435, 0.08618784530386739, 0.6666666666666666, 0.7545506809139934, 0.7833481507359795, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.66234374], dtype=float32), -1.0599352]. 
=============================================
[2019-04-04 04:37:29,757] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1559774e-24 1.2789730e-19 7.8050333e-20 1.0000000e+00 4.3263666e-25
 2.9451244e-14 9.2885021e-18], sum to 1.0000
[2019-04-04 04:37:29,758] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1830
[2019-04-04 04:37:29,791] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.333333333333334, 69.5, 0.0, 0.0, 26.0, 24.92198664845146, 0.3093040037187407, 0.0, 1.0, 44495.67909864627], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2675400.0000, 
sim time next is 2676000.0000, 
raw observation next is [-5.666666666666667, 70.0, 0.0, 0.0, 26.0, 24.86467628500203, 0.2975792562431601, 0.0, 1.0, 44448.41056928474], 
processed observation next is [1.0, 1.0, 0.30563250230840255, 0.7, 0.0, 0.0, 0.6666666666666666, 0.5720563570835026, 0.5991930854143867, 0.0, 1.0, 0.21165909794897494], 
reward next is 0.7883, 
noisyNet noise sample is [array([-0.13051036], dtype=float32), 1.1474034]. 
=============================================
[2019-04-04 04:37:29,807] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[77.24895 ]
 [77.47178 ]
 [77.542496]
 [77.80493 ]
 [78.04795 ]], R is [[77.02488708]
 [77.04275513]
 [77.0597229 ]
 [77.07415771]
 [77.0810318 ]].
[2019-04-04 04:37:33,509] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.6927768e-24 2.2390732e-20 1.1519042e-19 1.0000000e+00 9.3268910e-24
 7.3154073e-11 6.9071029e-19], sum to 1.0000
[2019-04-04 04:37:33,510] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3946
[2019-04-04 04:37:33,523] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 26.0, 25.48195771248811, 0.536532412700183, 0.0, 1.0, 55757.97655248571], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1316400.0000, 
sim time next is 1317000.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 26.0, 25.50386453954361, 0.5362390198861507, 0.0, 1.0, 34838.11134713991], 
processed observation next is [1.0, 0.21739130434782608, 0.5069252077562327, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6253220449619675, 0.6787463399620502, 0.0, 1.0, 0.16589576831971384], 
reward next is 0.8341, 
noisyNet noise sample is [array([-0.03314594], dtype=float32), 0.09916284]. 
=============================================
[2019-04-04 04:37:33,538] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[80.811516]
 [80.67035 ]
 [80.696976]
 [80.80852 ]
 [80.92657 ]], R is [[80.93521881]
 [80.86035919]
 [80.96250153]
 [81.15287781]
 [81.34134674]].
[2019-04-04 04:37:37,022] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 3.3984786e-37 7.8157218e-37 1.0000000e+00 0.0000000e+00
 2.5295542e-38 3.2065275e-36], sum to 1.0000
[2019-04-04 04:37:37,023] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0817
[2019-04-04 04:37:37,069] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.0, 71.0, 0.0, 0.0, 26.0, 24.60036608867453, 0.1895617798591109, 0.0, 1.0, 38095.45220180824], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3028800.0000, 
sim time next is 3029400.0000, 
raw observation next is [-5.0, 71.0, 0.0, 0.0, 26.0, 24.55828167029283, 0.1790442000380033, 0.0, 1.0, 38154.3898539919], 
processed observation next is [0.0, 0.043478260869565216, 0.32409972299168976, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5465234725244024, 0.5596814000126678, 0.0, 1.0, 0.18168757073329478], 
reward next is 0.8183, 
noisyNet noise sample is [array([1.3644736], dtype=float32), 1.628711]. 
=============================================
[2019-04-04 04:37:44,692] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.2349297e-23 4.8726528e-20 5.3096956e-19 1.0000000e+00 9.1786188e-23
 3.0912094e-16 3.2162226e-18], sum to 1.0000
[2019-04-04 04:37:44,692] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2808
[2019-04-04 04:37:44,742] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.4, 87.0, 73.0, 27.5, 26.0, 25.62787474579277, 0.3082245152181101, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2278800.0000, 
sim time next is 2279400.0000, 
raw observation next is [-8.116666666666667, 85.5, 82.33333333333334, 31.33333333333334, 26.0, 25.64343370713286, 0.3027735712205123, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.23776546629732226, 0.855, 0.2744444444444445, 0.03462246777163905, 0.6666666666666666, 0.6369528089277384, 0.6009245237401708, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4584731], dtype=float32), -0.2214073]. 
=============================================
[2019-04-04 04:37:48,837] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4207228e-30 7.0503712e-28 1.5777736e-26 1.0000000e+00 1.6828619e-31
 4.0960099e-26 5.9405929e-26], sum to 1.0000
[2019-04-04 04:37:48,837] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6807
[2019-04-04 04:37:48,851] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.466666666666667, 28.0, 151.5, 287.6666666666667, 26.0, 25.84373289477788, 0.3812492674528136, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2558400.0000, 
sim time next is 2559000.0000, 
raw observation next is [3.383333333333333, 28.5, 144.0, 300.3333333333333, 26.0, 25.82074480181916, 0.3777048237319227, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5563250230840259, 0.285, 0.48, 0.3318600368324125, 0.6666666666666666, 0.65172873348493, 0.6259016079106409, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3547024], dtype=float32), 0.70483524]. 
=============================================
[2019-04-04 04:37:48,855] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[81.39453 ]
 [81.687706]
 [82.10016 ]
 [82.396774]
 [82.5885  ]], R is [[81.43305206]
 [81.61872101]
 [81.80253601]
 [81.98451233]
 [82.16466522]].
[2019-04-04 04:37:51,910] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.3754119e-30 4.5591218e-26 6.6087375e-24 1.0000000e+00 5.4252261e-29
 2.3567746e-19 2.2802722e-23], sum to 1.0000
[2019-04-04 04:37:51,910] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9356
[2019-04-04 04:37:51,925] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 71.0, 9.0, 104.0, 26.0, 26.12917921159994, 0.6051560733542806, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3259800.0000, 
sim time next is 3260400.0000, 
raw observation next is [-4.0, 69.0, 0.0, 0.0, 26.0, 25.96633375946683, 0.604329156990361, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.3518005540166205, 0.69, 0.0, 0.0, 0.6666666666666666, 0.6638611466222359, 0.7014430523301204, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.17147718], dtype=float32), -0.22109143]. 
=============================================
[2019-04-04 04:37:54,594] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.8840906e-36 4.0654598e-30 3.7205729e-29 1.0000000e+00 5.3948722e-35
 1.2345498e-27 7.2351218e-29], sum to 1.0000
[2019-04-04 04:37:54,594] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7385
[2019-04-04 04:37:54,645] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.166666666666667, 55.83333333333334, 22.66666666666666, 224.0, 26.0, 25.09217578231254, 0.3586675297862016, 0.0, 1.0, 27084.43169277052], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2999400.0000, 
sim time next is 3000000.0000, 
raw observation next is [-1.333333333333333, 56.66666666666667, 14.33333333333333, 161.5, 26.0, 25.07576884458069, 0.3485030719572065, 0.0, 1.0, 39443.48549322032], 
processed observation next is [0.0, 0.7391304347826086, 0.42566943674976926, 0.5666666666666668, 0.047777777777777766, 0.17845303867403314, 0.6666666666666666, 0.5896474037150575, 0.6161676906524022, 0.0, 1.0, 0.18782612139628727], 
reward next is 0.8122, 
noisyNet noise sample is [array([-0.74624187], dtype=float32), -0.23260592]. 
=============================================
[2019-04-04 04:37:54,649] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[83.5027  ]
 [83.77257 ]
 [84.11319 ]
 [84.546036]
 [84.96022 ]], R is [[83.30451965]
 [83.34249878]
 [83.41416931]
 [83.40000153]
 [83.34941864]].
[2019-04-04 04:37:59,455] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.6889285e-26 2.9471243e-22 1.0874385e-21 1.0000000e+00 3.3844882e-25
 8.8785023e-13 2.4747235e-21], sum to 1.0000
[2019-04-04 04:37:59,455] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5705
[2019-04-04 04:37:59,509] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.8, 54.66666666666667, 43.5, 15.0, 26.0, 25.04186662787224, 0.217024971958802, 1.0, 1.0, 12488.8991689361], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2535600.0000, 
sim time next is 2536200.0000, 
raw observation next is [-2.8, 55.0, 51.0, 18.0, 26.0, 25.28678970403205, 0.2434769037830031, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.38504155124653744, 0.55, 0.17, 0.019889502762430938, 0.6666666666666666, 0.6072324753360041, 0.5811589679276677, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.37357035], dtype=float32), -0.3976928]. 
=============================================
[2019-04-04 04:38:00,965] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.7983007e-28 1.2240801e-24 7.1248994e-24 1.0000000e+00 6.1216377e-28
 5.4004525e-20 1.1683831e-22], sum to 1.0000
[2019-04-04 04:38:00,967] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4275
[2019-04-04 04:38:00,994] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 60.0, 0.0, 0.0, 26.0, 24.76543975811139, 0.2239131456564551, 0.0, 1.0, 42771.803678423], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3391200.0000, 
sim time next is 3391800.0000, 
raw observation next is [-3.0, 60.83333333333333, 0.0, 0.0, 26.0, 24.72168875062867, 0.2176899736209193, 0.0, 1.0, 42843.4825523837], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.6083333333333333, 0.0, 0.0, 0.6666666666666666, 0.5601407292190558, 0.5725633245403065, 0.0, 1.0, 0.20401658358277952], 
reward next is 0.7960, 
noisyNet noise sample is [array([0.8273393], dtype=float32), 0.8218974]. 
=============================================
[2019-04-04 04:38:17,320] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.5338151e-30 5.9045801e-26 2.3633935e-25 1.0000000e+00 2.0210971e-29
 3.8728048e-19 8.5734715e-25], sum to 1.0000
[2019-04-04 04:38:17,323] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6319
[2019-04-04 04:38:17,352] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 47.66666666666666, 116.3333333333333, 815.1666666666667, 26.0, 25.3875989671799, 0.5158332977527557, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3415200.0000, 
sim time next is 3415800.0000, 
raw observation next is [3.0, 48.33333333333334, 115.6666666666667, 813.3333333333334, 26.0, 25.66530465763478, 0.5104094773554638, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.5457063711911359, 0.48333333333333345, 0.38555555555555565, 0.8987108655616943, 0.6666666666666666, 0.6387753881362318, 0.6701364924518213, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.03043852], dtype=float32), -0.5245769]. 
=============================================
[2019-04-04 04:38:17,545] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.7847463e-31 1.5910030e-27 2.0624943e-28 1.0000000e+00 8.3451833e-31
 1.4958094e-31 1.9054756e-26], sum to 1.0000
[2019-04-04 04:38:17,548] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9920
[2019-04-04 04:38:17,592] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [8.5, 28.0, 0.0, 0.0, 26.0, 25.44931851532347, 0.3583107453514517, 0.0, 1.0, 40133.45509738794], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3645000.0000, 
sim time next is 3645600.0000, 
raw observation next is [8.666666666666668, 27.66666666666667, 0.0, 0.0, 26.0, 25.46170798265811, 0.3599192215654756, 0.0, 1.0, 28114.7165756856], 
processed observation next is [0.0, 0.17391304347826086, 0.7026777469990768, 0.2766666666666667, 0.0, 0.0, 0.6666666666666666, 0.6218089985548424, 0.6199730738551585, 0.0, 1.0, 0.13387960274136002], 
reward next is 0.8661, 
noisyNet noise sample is [array([0.26641756], dtype=float32), 0.022401158]. 
=============================================
[2019-04-04 04:38:19,959] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.6681918e-29 5.8173226e-25 1.0286346e-23 1.0000000e+00 8.8647919e-29
 2.5628726e-13 4.3117875e-24], sum to 1.0000
[2019-04-04 04:38:19,960] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2227
[2019-04-04 04:38:19,993] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.6666666666666667, 50.66666666666667, 61.5, 517.1666666666666, 26.0, 25.1640856313095, 0.3426171305025575, 0.0, 1.0, 18689.32890809375], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3082800.0000, 
sim time next is 3083400.0000, 
raw observation next is [0.5, 56.0, 57.0, 486.0, 26.0, 25.1153663151875, 0.3347910075926158, 0.0, 1.0, 34780.45647507798], 
processed observation next is [0.0, 0.6956521739130435, 0.4764542936288089, 0.56, 0.19, 0.5370165745856353, 0.6666666666666666, 0.5929471929322917, 0.611597002530872, 0.0, 1.0, 0.16562122130989515], 
reward next is 0.8344, 
noisyNet noise sample is [array([-0.14715457], dtype=float32), 0.011073861]. 
=============================================
[2019-04-04 04:38:20,099] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.4057953e-17 2.0274958e-12 1.8834171e-13 9.9997854e-01 3.0590253e-16
 2.1489332e-05 1.3802502e-12], sum to 1.0000
[2019-04-04 04:38:20,101] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3606
[2019-04-04 04:38:20,108] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.0, 100.0, 112.0, 806.0, 26.0, 27.39789873893012, 0.8843078198300769, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3159000.0000, 
sim time next is 3159600.0000, 
raw observation next is [7.0, 100.0, 110.1666666666667, 798.8333333333334, 26.0, 27.46703778246347, 0.9003103346602215, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.6565096952908588, 1.0, 0.36722222222222234, 0.8826887661141806, 0.6666666666666666, 0.7889198152052893, 0.8001034448867405, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0734885], dtype=float32), 0.9575529]. 
=============================================
[2019-04-04 04:38:20,148] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.12759879e-21 1.54963050e-16 2.87537854e-16 8.94387603e-01
 4.78219155e-20 1.05612345e-01 1.22943709e-15], sum to 1.0000
[2019-04-04 04:38:20,148] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0687
[2019-04-04 04:38:20,165] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 85.5, 101.0, 769.0, 26.0, 26.89308492509963, 0.8343686538574479, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3249000.0000, 
sim time next is 3249600.0000, 
raw observation next is [-2.666666666666667, 80.66666666666666, 98.33333333333334, 755.1666666666667, 26.0, 26.95469741254551, 0.8335819434460955, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.38873499538319484, 0.8066666666666665, 0.32777777777777783, 0.834438305709024, 0.6666666666666666, 0.7462247843787925, 0.7778606478153652, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.03038012], dtype=float32), 0.7395761]. 
=============================================
[2019-04-04 04:38:20,214] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.1071652e-20 3.1495218e-16 2.7578030e-16 1.0000000e+00 3.6791940e-20
 1.1593341e-12 2.1819440e-15], sum to 1.0000
[2019-04-04 04:38:20,215] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5123
[2019-04-04 04:38:20,233] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 80.16666666666667, 0.0, 0.0, 26.0, 25.4512035399338, 0.4509462437984235, 0.0, 1.0, 41226.71576259779], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3459000.0000, 
sim time next is 3459600.0000, 
raw observation next is [1.0, 79.0, 0.0, 0.0, 26.0, 25.43271770896438, 0.4489337480859494, 0.0, 1.0, 50078.48638902561], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 0.79, 0.0, 0.0, 0.6666666666666666, 0.6193931424136982, 0.6496445826953164, 0.0, 1.0, 0.23846898280488385], 
reward next is 0.7615, 
noisyNet noise sample is [array([1.5974343], dtype=float32), 0.097776555]. 
=============================================
[2019-04-04 04:38:25,188] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.2733855e-28 1.5230604e-25 2.4116547e-24 1.0000000e+00 2.7358865e-28
 5.1366539e-23 1.6335519e-23], sum to 1.0000
[2019-04-04 04:38:25,198] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1414
[2019-04-04 04:38:25,216] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.3, 26.5, 71.0, 76.0, 26.0, 24.80871736659121, 0.3045272272924154, 1.0, 1.0, 177550.974568699], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2824200.0000, 
sim time next is 2824800.0000, 
raw observation next is [6.2, 27.0, 60.00000000000001, 71.0, 26.0, 25.09962689149373, 0.3467951497928825, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.6343490304709142, 0.27, 0.2, 0.07845303867403315, 0.6666666666666666, 0.5916355742911442, 0.6155983832642942, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.617464], dtype=float32), 1.1207289]. 
=============================================
[2019-04-04 04:38:26,410] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.2328899e-24 5.7160604e-20 8.5911113e-19 1.0000000e+00 1.4122653e-24
 5.1115480e-11 2.3905679e-18], sum to 1.0000
[2019-04-04 04:38:26,411] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9629
[2019-04-04 04:38:26,431] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 50.0, 35.5, 317.0, 26.0, 26.26766723724559, 0.6269913031239617, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3344400.0000, 
sim time next is 3345000.0000, 
raw observation next is [-2.166666666666667, 50.83333333333334, 27.33333333333333, 255.6666666666666, 26.0, 26.42724294930774, 0.478165086564383, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.4025854108956602, 0.5083333333333334, 0.0911111111111111, 0.2825046040515653, 0.6666666666666666, 0.702270245775645, 0.6593883621881277, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5172851], dtype=float32), -0.6478282]. 
=============================================
[2019-04-04 04:38:26,474] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[80.15078 ]
 [80.41001 ]
 [80.79185 ]
 [81.73645 ]
 [81.921036]], R is [[79.68776703]
 [79.89089203]
 [80.09198761]
 [80.29106903]
 [80.48815918]].
[2019-04-04 04:38:27,480] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.6957976e-21 1.2553855e-17 5.1864846e-17 1.0000000e+00 4.4238055e-21
 1.4799713e-12 1.9339573e-16], sum to 1.0000
[2019-04-04 04:38:27,480] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4415
[2019-04-04 04:38:27,508] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.199999999999999, 87.0, 0.0, 0.0, 26.0, 24.21320948968037, 0.06389717825940262, 0.0, 1.0, 41079.61944171216], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2007600.0000, 
sim time next is 2008200.0000, 
raw observation next is [-6.2, 87.0, 0.0, 0.0, 26.0, 24.15916597209018, 0.0587006642287986, 0.0, 1.0, 41111.63749123511], 
processed observation next is [1.0, 0.21739130434782608, 0.2908587257617729, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5132638310075149, 0.5195668880762662, 0.0, 1.0, 0.1957697023392148], 
reward next is 0.8042, 
noisyNet noise sample is [array([0.7220288], dtype=float32), -0.03091207]. 
=============================================
[2019-04-04 04:38:30,644] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.7023236e-22 4.1823621e-18 6.4369524e-18 1.0000000e+00 2.9491965e-20
 4.0377890e-10 1.5466375e-18], sum to 1.0000
[2019-04-04 04:38:30,645] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0549
[2019-04-04 04:38:30,680] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 92.0, 87.66666666666667, 417.1666666666667, 26.0, 25.78182456318964, 0.576113232615249, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3228000.0000, 
sim time next is 3228600.0000, 
raw observation next is [-3.0, 92.0, 90.33333333333333, 464.3333333333333, 26.0, 25.90801242943732, 0.5948448643232312, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.3795013850415513, 0.92, 0.3011111111111111, 0.5130755064456721, 0.6666666666666666, 0.6590010357864434, 0.6982816214410771, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.42292872], dtype=float32), 1.4331057]. 
=============================================
[2019-04-04 04:38:32,052] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.06344477e-24 3.91403590e-19 6.49259067e-18 9.99889851e-01
 1.51608994e-22 1.10090084e-04 4.17391863e-17], sum to 1.0000
[2019-04-04 04:38:32,052] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3260
[2019-04-04 04:38:32,061] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.666666666666667, 71.0, 0.0, 0.0, 26.0, 25.2628036915975, 0.4394453930112063, 1.0, 1.0, 53807.89483952217], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3435600.0000, 
sim time next is 3436200.0000, 
raw observation next is [1.5, 73.0, 0.0, 0.0, 26.0, 25.18647190378085, 0.4291619213256875, 1.0, 1.0, 59799.02226032365], 
processed observation next is [1.0, 0.782608695652174, 0.5041551246537397, 0.73, 0.0, 0.0, 0.6666666666666666, 0.598872658648404, 0.6430539737752291, 1.0, 1.0, 0.28475724885868403], 
reward next is 0.7152, 
noisyNet noise sample is [array([-0.7885692], dtype=float32), -1.2519796]. 
=============================================
[2019-04-04 04:38:33,078] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.2051408e-19 4.4199615e-15 1.9365760e-13 2.6265380e-01 3.9428738e-18
 7.3734623e-01 5.2733212e-14], sum to 1.0000
[2019-04-04 04:38:33,078] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5460
[2019-04-04 04:38:33,101] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.1, 99.83333333333334, 41.33333333333334, 371.3333333333334, 26.0, 27.61258194248691, 0.7489505350982926, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3171000.0000, 
sim time next is 3171600.0000, 
raw observation next is [6.0, 100.0, 33.0, 307.5, 26.0, 27.61712564586911, 0.9764124987719184, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.6288088642659281, 1.0, 0.11, 0.3397790055248619, 0.6666666666666666, 0.8014271371557591, 0.8254708329239727, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3663547], dtype=float32), -0.1497657]. 
=============================================
[2019-04-04 04:38:36,232] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.3463123e-26 2.2661901e-21 3.1404881e-22 6.7044436e-07 3.4380754e-23
 9.9999928e-01 4.0717633e-20], sum to 1.0000
[2019-04-04 04:38:36,234] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2242
[2019-04-04 04:38:36,259] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 92.0, 101.0, 653.0, 26.0, 26.17376453633283, 0.6515807924783482, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3231000.0000, 
sim time next is 3231600.0000, 
raw observation next is [-3.0, 92.0, 102.3333333333333, 669.5, 26.0, 26.27160460801669, 0.6628205043778569, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.3795013850415513, 0.92, 0.341111111111111, 0.7397790055248619, 0.6666666666666666, 0.6893003840013909, 0.7209401681259523, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.38626638], dtype=float32), -0.9845014]. 
=============================================
[2019-04-04 04:38:40,906] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.2015101e-20 3.0492387e-16 1.7328119e-14 4.0773895e-02 1.4399000e-19
 9.5922613e-01 4.8968049e-14], sum to 1.0000
[2019-04-04 04:38:40,907] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4165
[2019-04-04 04:38:40,912] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.833333333333333, 69.0, 0.0, 0.0, 26.0, 25.36167839209563, 0.4460686140573246, 1.0, 1.0, 97231.32818479788], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3435000.0000, 
sim time next is 3435600.0000, 
raw observation next is [1.666666666666667, 71.0, 0.0, 0.0, 26.0, 25.2628036915975, 0.4394453930112063, 1.0, 1.0, 53807.89483952217], 
processed observation next is [1.0, 0.782608695652174, 0.5087719298245615, 0.71, 0.0, 0.0, 0.6666666666666666, 0.6052336409664584, 0.6464817976704021, 1.0, 1.0, 0.2562280706643913], 
reward next is 0.7438, 
noisyNet noise sample is [array([0.90713686], dtype=float32), -1.0928385]. 
=============================================
[2019-04-04 04:38:42,256] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.5460130e-17 6.3454580e-13 3.2450995e-13 1.0000000e+00 5.9512159e-16
 3.9975621e-08 1.4134859e-13], sum to 1.0000
[2019-04-04 04:38:42,259] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9329
[2019-04-04 04:38:42,277] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.0, 100.0, 1.0, 82.0, 26.0, 25.43271849114083, 0.3280101057085827, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3137400.0000, 
sim time next is 3138000.0000, 
raw observation next is [6.0, 100.0, 14.66666666666666, 133.6666666666667, 26.0, 25.47589519270504, 0.3369792228592272, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.6288088642659281, 1.0, 0.04888888888888887, 0.1476979742173113, 0.6666666666666666, 0.6229912660587532, 0.6123264076197424, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.45103523], dtype=float32), -0.650701]. 
=============================================
[2019-04-04 04:38:42,294] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[88.24761 ]
 [86.72205 ]
 [85.463844]
 [85.61707 ]
 [85.540215]], R is [[89.18773651]
 [89.29586029]
 [89.4029007 ]
 [89.41954041]
 [89.33316803]].
[2019-04-04 04:38:42,538] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.7743671e-25 9.7807047e-21 1.1971434e-17 1.0000000e+00 3.4438404e-23
 4.2770078e-17 2.7691008e-18], sum to 1.0000
[2019-04-04 04:38:42,538] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6344
[2019-04-04 04:38:42,545] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 53.5, 103.0, 775.0, 26.0, 25.74927599005802, 0.5707339584962374, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3421800.0000, 
sim time next is 3422400.0000, 
raw observation next is [3.0, 55.0, 99.83333333333334, 763.1666666666667, 26.0, 26.01086230847734, 0.5940063150804233, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.55, 0.3327777777777778, 0.8432780847145489, 0.6666666666666666, 0.6675718590397782, 0.6980021050268078, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.1009965], dtype=float32), 0.61584044]. 
=============================================
[2019-04-04 04:38:47,212] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.7676877e-30 2.4537933e-25 2.9383848e-26 1.0000000e+00 8.3660651e-30
 7.0205326e-19 9.7776252e-25], sum to 1.0000
[2019-04-04 04:38:47,213] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2940
[2019-04-04 04:38:47,232] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.166666666666667, 51.66666666666667, 0.0, 0.0, 26.0, 25.40774782326535, 0.3842314325375407, 0.0, 1.0, 35307.85858694623], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3622200.0000, 
sim time next is 3622800.0000, 
raw observation next is [-2.333333333333333, 53.33333333333334, 0.0, 0.0, 26.0, 25.38913885856413, 0.3792028097795135, 0.0, 1.0, 46046.41236308795], 
processed observation next is [0.0, 0.9565217391304348, 0.3979686057248385, 0.5333333333333334, 0.0, 0.0, 0.6666666666666666, 0.6157615715470109, 0.6264009365931712, 0.0, 1.0, 0.2192686303004188], 
reward next is 0.7807, 
noisyNet noise sample is [array([2.206017], dtype=float32), -0.95089114]. 
=============================================
[2019-04-04 04:38:51,668] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.2301503e-26 4.1166052e-20 3.0135066e-21 2.0012463e-10 1.5646252e-24
 1.0000000e+00 1.6780963e-19], sum to 1.0000
[2019-04-04 04:38:51,668] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6524
[2019-04-04 04:38:51,678] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.6, 84.0, 109.6666666666667, 761.8333333333334, 26.0, 26.41051420219193, 0.7062104223219036, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3235200.0000, 
sim time next is 3235800.0000, 
raw observation next is [-2.5, 82.0, 110.3333333333333, 771.6666666666666, 26.0, 26.44415635324828, 0.7131329489139299, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.39335180055401664, 0.82, 0.36777777777777765, 0.8526703499079189, 0.6666666666666666, 0.7036796961040235, 0.73771098297131, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.16757998], dtype=float32), -1.011885]. 
=============================================
[2019-04-04 04:38:51,934] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3819054e-24 8.0224509e-18 8.8986941e-18 9.9999678e-01 1.4318723e-22
 3.2639527e-06 4.1147123e-18], sum to 1.0000
[2019-04-04 04:38:51,935] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9891
[2019-04-04 04:38:51,962] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.666666666666667, 90.0, 111.6666666666667, 813.8333333333334, 26.0, 25.48458804222625, 0.6860308155186626, 1.0, 1.0, 65058.2805907216], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3244800.0000, 
sim time next is 3245400.0000, 
raw observation next is [-3.0, 92.5, 111.0, 812.0, 26.0, 25.98081404671264, 0.7333179069236553, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.3795013850415513, 0.925, 0.37, 0.8972375690607735, 0.6666666666666666, 0.6650678372260534, 0.7444393023078851, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.31857026], dtype=float32), 0.1792985]. 
=============================================
[2019-04-04 04:39:03,127] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1034294e-15 6.1428111e-12 3.6654451e-12 1.0000000e+00 1.8189423e-15
 4.1677122e-13 6.4034229e-11], sum to 1.0000
[2019-04-04 04:39:03,127] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7151
[2019-04-04 04:39:03,146] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 67.0, 0.0, 0.0, 26.0, 25.71796368148799, 0.373019183241547, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3433800.0000, 
sim time next is 3434400.0000, 
raw observation next is [2.0, 67.0, 0.0, 0.0, 26.0, 25.73249013599708, 0.490440894917787, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.518005540166205, 0.67, 0.0, 0.0, 0.6666666666666666, 0.6443741779997566, 0.663480298305929, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2511098], dtype=float32), -1.3633066]. 
=============================================
[2019-04-04 04:39:05,930] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.1459115e-29 1.3415771e-23 2.0819773e-23 1.0000000e+00 2.7095173e-27
 1.7480897e-18 3.8949930e-22], sum to 1.0000
[2019-04-04 04:39:05,930] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1152
[2019-04-04 04:39:05,938] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 60.0, 83.16666666666666, 682.3333333333333, 26.0, 26.80692742567268, 0.6999021513105256, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3771600.0000, 
sim time next is 3772200.0000, 
raw observation next is [0.0, 60.0, 79.33333333333334, 653.6666666666667, 26.0, 26.8346059020117, 0.7063697128442324, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.46260387811634357, 0.6, 0.2644444444444445, 0.7222836095764273, 0.6666666666666666, 0.7362171585009749, 0.7354565709480775, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.32703468], dtype=float32), -0.75836366]. 
=============================================
[2019-04-04 04:39:07,492] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.9948746e-29 9.5446440e-25 3.7236207e-24 1.0000000e+00 2.6010695e-27
 5.8950391e-18 3.2087880e-23], sum to 1.0000
[2019-04-04 04:39:07,492] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0399
[2019-04-04 04:39:07,546] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.166666666666667, 35.16666666666667, 30.66666666666666, 156.3333333333333, 26.0, 25.32908230287341, 0.3213835306150483, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4089000.0000, 
sim time next is 4089600.0000, 
raw observation next is [-4.0, 34.0, 46.0, 234.5, 26.0, 25.36682229528754, 0.3312724551705017, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.3518005540166205, 0.34, 0.15333333333333332, 0.2591160220994475, 0.6666666666666666, 0.6139018579406285, 0.6104241517235006, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6587379], dtype=float32), 0.7461137]. 
=============================================
[2019-04-04 04:39:14,343] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.4890011e-28 7.4889950e-24 8.5329522e-24 1.0000000e+00 1.2976072e-28
 1.5448933e-24 1.3115998e-23], sum to 1.0000
[2019-04-04 04:39:14,343] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4444
[2019-04-04 04:39:14,355] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.350000000000001, 73.0, 0.0, 0.0, 26.0, 25.57734873593186, 0.4715738604298068, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4306200.0000, 
sim time next is 4306800.0000, 
raw observation next is [5.300000000000001, 73.0, 0.0, 0.0, 26.0, 25.73168000202419, 0.4774043123336064, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.6094182825484765, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6443066668353493, 0.6591347707778689, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.8833896], dtype=float32), -0.88896644]. 
=============================================
[2019-04-04 04:39:14,359] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.2028796e-28 5.1207651e-24 6.3773697e-24 1.0000000e+00 1.0884597e-28
 9.9946912e-25 8.8078821e-24], sum to 1.0000
[2019-04-04 04:39:14,361] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3630
[2019-04-04 04:39:14,378] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.300000000000001, 73.0, 0.0, 0.0, 26.0, 25.73168000202419, 0.4774043123336064, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4306800.0000, 
sim time next is 4307400.0000, 
raw observation next is [5.25, 73.0, 0.0, 0.0, 26.0, 25.78719491567085, 0.4734624155050929, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.60803324099723, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6489329096392374, 0.6578208051683643, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.8833896], dtype=float32), -0.88896644]. 
=============================================
[2019-04-04 04:39:19,395] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.7571492e-25 2.0023891e-20 1.4458907e-21 1.0000000e+00 9.0866324e-26
 1.1437706e-12 9.4859748e-20], sum to 1.0000
[2019-04-04 04:39:19,396] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5927
[2019-04-04 04:39:19,411] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.333333333333333, 71.0, 0.0, 0.0, 26.0, 24.80415654733044, 0.2850048203487827, 0.0, 1.0, 44404.75925205241], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2677200.0000, 
sim time next is 2677800.0000, 
raw observation next is [-6.666666666666667, 71.5, 0.0, 0.0, 26.0, 24.7614995388647, 0.2715776374889488, 0.0, 1.0, 44401.19728329053], 
processed observation next is [1.0, 1.0, 0.27793167128347185, 0.715, 0.0, 0.0, 0.6666666666666666, 0.5634582949053918, 0.5905258791629829, 0.0, 1.0, 0.21143427277757396], 
reward next is 0.7886, 
noisyNet noise sample is [array([1.4098938], dtype=float32), -0.93285775]. 
=============================================
[2019-04-04 04:39:27,850] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.6529138e-21 3.3821271e-16 1.7125605e-16 9.9968290e-01 3.6778570e-21
 3.1713973e-04 4.0962962e-15], sum to 1.0000
[2019-04-04 04:39:27,854] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4274
[2019-04-04 04:39:27,891] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.84452597161954, 0.2886270205607149, 0.0, 1.0, 44700.98650010546], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2763000.0000, 
sim time next is 2763600.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.80210721230598, 0.2803991082154029, 0.0, 1.0, 44414.6783164977], 
processed observation next is [1.0, 1.0, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5668422676921651, 0.5934663694051343, 0.0, 1.0, 0.21149846817379858], 
reward next is 0.7885, 
noisyNet noise sample is [array([-0.831066], dtype=float32), -0.037140675]. 
=============================================
[2019-04-04 04:39:29,023] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.9436225e-27 6.3591529e-23 5.7422776e-23 1.0000000e+00 6.4884691e-28
 1.6549136e-17 1.0880318e-21], sum to 1.0000
[2019-04-04 04:39:29,026] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2278
[2019-04-04 04:39:29,054] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.166666666666667, 29.33333333333334, 0.0, 0.0, 26.0, 25.70745148348247, 0.5186688565202903, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4050600.0000, 
sim time next is 4051200.0000, 
raw observation next is [-4.333333333333334, 29.66666666666667, 0.0, 0.0, 26.0, 25.67690836519003, 0.5046667082466695, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.3425669436749769, 0.2966666666666667, 0.0, 0.0, 0.6666666666666666, 0.6397423637658358, 0.6682222360822232, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.39849243], dtype=float32), -0.7271317]. 
=============================================
[2019-04-04 04:39:30,385] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.1104172e-26 4.4023538e-23 4.4349232e-21 1.0000000e+00 6.6975074e-26
 4.6267189e-13 1.6657479e-20], sum to 1.0000
[2019-04-04 04:39:30,385] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6454
[2019-04-04 04:39:30,426] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 41.0, 0.0, 0.0, 26.0, 25.09940526231242, 0.3176620610591351, 0.0, 1.0, 40797.78895733381], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4066800.0000, 
sim time next is 4067400.0000, 
raw observation next is [-6.0, 41.0, 0.0, 0.0, 26.0, 25.10864399307521, 0.3101347409355174, 0.0, 1.0, 40820.48761451301], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.41, 0.0, 0.0, 0.6666666666666666, 0.592386999422934, 0.6033782469785058, 0.0, 1.0, 0.19438327435482386], 
reward next is 0.8056, 
noisyNet noise sample is [array([-0.54426795], dtype=float32), 0.3339931]. 
=============================================
[2019-04-04 04:39:32,297] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-04 04:39:32,309] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 04:39:32,309] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 04:39:32,309] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:39:32,310] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 04:39:32,310] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:39:32,312] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:39:32,317] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run51
[2019-04-04 04:39:32,360] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run51
[2019-04-04 04:39:32,442] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run51
[2019-04-04 04:42:42,712] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7369.8582 236489786.8835 1442.3703
[2019-04-04 04:43:18,371] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7242.3598 263264444.8073 1558.2911
[2019-04-04 04:43:23,081] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7192.2006 273797870.8306 1107.7799
[2019-04-04 04:43:24,134] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 5000000, evaluation results [5000000.0, 7242.359786631755, 263264444.80732435, 1558.2910726216553, 7369.858157697407, 236489786.8835459, 1442.3702946695137, 7192.200615092327, 273797870.8306112, 1107.7798887086515]
