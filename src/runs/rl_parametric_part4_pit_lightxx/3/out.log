Using TensorFlow backend.
[2019-04-03 21:51:53,410] A3C_AGENT_MAIN INFO:Namespace(action_repeat_n=1, action_space='part4_v1', activation='linear', agent_num=5, check_args_only=False, clip_norm=5.0, debug_log_prob=0.0005, decay_steps=1000000, dropout_prob=0.0, end_e=0.0, env='Part4-Light-Pit-Train-v1', eval_act_func='part4_v1', eval_env_res_max_keep=50, eval_epi_num=1, eval_freq=100000, forecast_dim=0, gamma=0.99, h_decay_bounds=[], h_regu_frac=[0.0], init_e=0.0, isNoisyNet=True, isNoisyNetEval_rmNoise=True, is_greedy_policy=False, is_learning_rate_decay_staircase=False, is_warm_start=False, job_mode='Train', learning_rate=0.0001, learning_rate_decay_rate=1.0, learning_rate_decay_steps=100000, max_interactions=5000000, metric_func='part4_v1', model_dir='None', model_param=[12, 1], model_type='nn', num_threads=16, output='./Part4-Light-Pit-Train-v1-res1', p_loss_frac=1.0, raw_state_prcs_func='cslDx_1', reward_func='part4_v1', rmsprop_decay=0.99, rmsprop_epsil=1e-10, rmsprop_momet=0.0, rwd_e_para=1.0, rwd_p_para=1.0, save_freq=500000, save_max_to_keep=5, save_scope='all', sharedNet_type='Dense', state_dim=10, test_env=['Part4-Light-Pit-Test-v1', 'Part4-Light-Pit-Test-v2'], test_mode='Multiple', train_act_func='part4_v1', train_freq=5, v_loss_frac=0.5, violation_penalty_scl=10.0, weight_initer='glorot_uniform', window_len=20)
[2019-04-03 21:51:53,410] A3C_AGENT_MAIN INFO:Start compiling...
2019-04-03 21:51:53.442038: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
[2019-04-03 21:52:09,657] A3C_AGENT_MAIN INFO:Start the learning...
[2019-04-03 21:52:09,658] A3C_AGENT_MAIN INFO:Prepare the evaluation environments ['Part4-Light-Pit-Train-v1', 'Part4-Light-Pit-Test-v1', 'Part4-Light-Pit-Test-v2'] ...
[2019-04-03 21:52:09,681] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation worker starts!
[2019-04-03 21:52:09,705] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation worker starts!
[2019-04-03 21:52:09,729] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation worker starts!
[2019-04-03 21:52:09,729] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:09,730] A3C_AGENT_WORKER-Thread-2 INFO:Local worker starts!
[2019-04-03 21:52:09,793] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:09,794] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run1
[2019-04-03 21:52:10,731] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:10,732] A3C_AGENT_WORKER-Thread-3 INFO:Local worker starts!
[2019-04-03 21:52:10,807] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:10,808] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run1
[2019-04-03 21:52:11,733] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:11,734] A3C_AGENT_WORKER-Thread-4 INFO:Local worker starts!
[2019-04-03 21:52:11,852] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:11,854] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run1
[2019-04-03 21:52:12,735] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:12,735] A3C_AGENT_WORKER-Thread-5 INFO:Local worker starts!
[2019-04-03 21:52:12,845] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:12,846] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run1
[2019-04-03 21:52:13,736] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:13,737] A3C_AGENT_WORKER-Thread-6 INFO:Local worker starts!
[2019-04-03 21:52:13,950] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:13,952] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run1
[2019-04-03 21:52:14,738] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:14,739] A3C_AGENT_WORKER-Thread-7 INFO:Local worker starts!
[2019-04-03 21:52:14,929] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:14,931] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run1
[2019-04-03 21:52:15,740] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:15,741] A3C_AGENT_WORKER-Thread-8 INFO:Local worker starts!
[2019-04-03 21:52:15,883] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:15,884] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run1
[2019-04-03 21:52:16,742] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:16,743] A3C_AGENT_WORKER-Thread-9 INFO:Local worker starts!
[2019-04-03 21:52:16,949] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:16,951] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run1
[2019-04-03 21:52:17,743] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:17,744] A3C_AGENT_WORKER-Thread-10 INFO:Local worker starts!
[2019-04-03 21:52:18,235] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-03 21:52:18,235] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-03 21:52:18,235] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:18,237] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run1
[2019-04-03 21:52:18,329] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-03 21:52:18,329] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:18,335] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run1
[2019-04-03 21:52:18,449] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-03 21:52:18,450] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:18,451] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run1
[2019-04-03 21:52:18,527] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:18,549] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run1
[2019-04-03 21:52:18,749] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:18,750] A3C_AGENT_WORKER-Thread-14 INFO:Local worker starts!
[2019-04-03 21:52:19,206] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:19,208] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run1
[2019-04-03 21:52:19,761] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:19,762] A3C_AGENT_WORKER-Thread-15 INFO:Local worker starts!
[2019-04-03 21:52:20,024] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:20,026] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run1
[2019-04-03 21:52:20,768] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:20,769] A3C_AGENT_WORKER-Thread-16 INFO:Local worker starts!
[2019-04-03 21:52:21,235] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:21,237] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run1
[2019-04-03 21:52:21,769] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:21,770] A3C_AGENT_WORKER-Thread-17 INFO:Local worker starts!
[2019-04-03 21:52:22,137] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:22,139] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run1
[2019-04-03 21:52:22,771] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:22,771] A3C_AGENT_WORKER-Thread-18 INFO:Local worker starts!
[2019-04-03 21:52:23,016] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:23,017] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run1
[2019-04-03 21:52:23,772] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:23,773] A3C_AGENT_WORKER-Thread-19 INFO:Local worker starts!
[2019-04-03 21:52:24,428] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:24,430] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run1
[2019-04-03 21:52:24,774] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:24,775] A3C_AGENT_WORKER-Thread-20 INFO:Local worker starts!
[2019-04-03 21:52:25,075] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:25,077] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run1
[2019-04-03 21:53:00,376] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-03 21:53:00,376] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-3.9, 82.66666666666667, 35.66666666666666, 0.0, 26.0, 25.52567725182879, 0.283370581003885, 1.0, 1.0, 0.0]
[2019-04-03 21:53:00,377] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-03 21:53:00,378] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0.0830365  0.29387978 0.01537636 0.09628686 0.16762201 0.24234644
 0.10145207], sampled 0.5689962540963055
[2019-04-03 21:53:10,482] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-03 21:53:10,482] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [0.08333333333333337, 66.0, 173.6666666666667, 244.6666666666667, 26.0, 25.05780457283657, 0.2248514800447684, 0.0, 1.0, 0.0]
[2019-04-03 21:53:10,483] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-03 21:53:10,483] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [0.06155102 0.27239046 0.02112751 0.089841   0.1746202  0.3249916
 0.05547817], sampled 0.9725627836976074
[2019-04-03 21:53:44,038] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-03 21:53:44,039] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-15.66666666666667, 83.0, 0.0, 0.0, 19.0, 20.89400155200677, -0.6694059872386536, 0.0, 1.0, 0.0]
[2019-04-03 21:53:44,039] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-03 21:53:44,039] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0.04877106 0.31218323 0.03308284 0.0753342  0.16585654 0.29510838
 0.06966372], sampled 0.6346868945095491
[2019-04-03 21:54:03,411] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-03 21:54:03,411] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-6.270575772333334, 79.09768339666667, 0.0, 0.0, 23.5, 23.84781712121328, 0.0539626798029356, 0.0, 1.0, 0.0]
[2019-04-03 21:54:03,411] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-03 21:54:03,412] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [0.05300341 0.21001269 0.01992442 0.10791225 0.26954463 0.2783858
 0.06121683], sampled 0.7452829594091173
[2019-04-03 21:54:16,487] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7458.2465 217898598.6321 1188.0247
[2019-04-03 21:54:17,527] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-03 21:54:17,527] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-4.666666666666666, 49.33333333333333, 0.0, 0.0, 25.0, 23.8647869193974, 0.01859521617423642, 0.0, 1.0, 41114.60008810717]
[2019-04-03 21:54:17,527] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-03 21:54:17,527] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0.06700297 0.24568288 0.02577572 0.07254801 0.26302525 0.26554278
 0.06042238], sampled 0.8849880706558982
[2019-04-03 21:54:39,504] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-03 21:54:39,504] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [12.0, 19.0, 89.33333333333333, 691.6666666666667, 26.0, 27.54196952147445, 1.008006539523706, 1.0, 1.0, 0.0]
[2019-04-03 21:54:39,504] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-03 21:54:39,505] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0.11034062 0.20574337 0.01241562 0.08540776 0.29847065 0.15598153
 0.1316404 ], sampled 0.8990834532068653
[2019-04-03 21:54:39,937] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7310.9398 246771801.5642 981.7301
[2019-04-03 21:54:44,651] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7251.2884 254092743.4161 692.7758
[2019-04-03 21:54:45,688] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 0, evaluation results [0.0, 7310.939790448689, 246771801.56421003, 981.7301101639936, 7458.246469990233, 217898598.63206992, 1188.0246807496706, 7251.288425681861, 254092743.41612852, 692.775760885787]
[2019-04-03 21:55:19,145] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.00625428 0.76935977 0.0086679  0.05095941 0.06223894 0.0409744
 0.06154535], sum to 1.0000
[2019-04-03 21:55:19,145] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9272
[2019-04-03 21:55:19,452] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 64.0, 15.0, 0.0, 20.0, 20.69099181407145, -0.801427886498418, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 232800.0000, 
sim time next is 233400.0000, 
raw observation next is [-3.4, 64.5, 12.0, 0.0, 21.0, 20.92331086826726, -0.7925392908515988, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.368421052631579, 0.645, 0.04, 0.0, 0.25, 0.2436092390222718, 0.2358202363828004, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.7393152], dtype=float32), 0.76753074]. 
=============================================
[2019-04-03 21:55:23,852] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00100787 0.86115426 0.00636307 0.02206041 0.05286574 0.03124317
 0.02530549], sum to 1.0000
[2019-04-03 21:55:23,854] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9517
[2019-04-03 21:55:24,145] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.41666666666667, 67.5, 0.0, 0.0, 19.0, 18.40900456528238, -1.240994577072108, 0.0, 1.0, 99825.71037043059], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 276600.0000, 
sim time next is 277200.0000, 
raw observation next is [-10.6, 67.0, 0.0, 0.0, 20.0, 18.42360824089398, -1.229682422680743, 0.0, 1.0, 89514.52217197987], 
processed observation next is [1.0, 0.21739130434782608, 0.1689750692520776, 0.67, 0.0, 0.0, 0.16666666666666666, 0.03530068674116501, 0.090105859106419, 0.0, 1.0, 0.42625962939038037], 
reward next is 0.5737, 
noisyNet noise sample is [array([0.15462697], dtype=float32), -1.0105674]. 
=============================================
[2019-04-03 21:55:26,351] A3C_AGENT_WORKER-Thread-15 INFO:Local step 500, global step 7525: loss -0.0486
[2019-04-03 21:55:26,430] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 500, global step 7525: learning rate 0.0001
[2019-04-03 21:55:26,543] A3C_AGENT_WORKER-Thread-7 INFO:Local step 500, global step 7595: loss -0.0761
[2019-04-03 21:55:26,545] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 500, global step 7597: learning rate 0.0001
[2019-04-03 21:55:26,604] A3C_AGENT_WORKER-Thread-6 INFO:Local step 500, global step 7614: loss -0.0225
[2019-04-03 21:55:26,605] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 500, global step 7614: learning rate 0.0001
[2019-04-03 21:55:26,695] A3C_AGENT_WORKER-Thread-17 INFO:Local step 500, global step 7639: loss 0.0248
[2019-04-03 21:55:26,702] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 500, global step 7639: learning rate 0.0001
[2019-04-03 21:55:26,925] A3C_AGENT_WORKER-Thread-19 INFO:Local step 500, global step 7691: loss -0.0590
[2019-04-03 21:55:26,954] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 500, global step 7703: learning rate 0.0001
[2019-04-03 21:55:27,335] A3C_AGENT_WORKER-Thread-4 INFO:Local step 500, global step 7794: loss 1.4576
[2019-04-03 21:55:27,338] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 500, global step 7794: learning rate 0.0001
[2019-04-03 21:55:28,361] A3C_AGENT_WORKER-Thread-9 INFO:Local step 500, global step 8014: loss -0.1754
[2019-04-03 21:55:28,372] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 500, global step 8017: learning rate 0.0001
[2019-04-03 21:55:28,442] A3C_AGENT_WORKER-Thread-5 INFO:Local step 500, global step 8034: loss -1.3104
[2019-04-03 21:55:28,448] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 500, global step 8034: learning rate 0.0001
[2019-04-03 21:55:28,532] A3C_AGENT_WORKER-Thread-18 INFO:Local step 500, global step 8052: loss -1.6118
[2019-04-03 21:55:28,532] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 500, global step 8052: learning rate 0.0001
[2019-04-03 21:55:29,048] A3C_AGENT_WORKER-Thread-3 INFO:Local step 500, global step 8161: loss -0.1961
[2019-04-03 21:55:29,048] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 500, global step 8161: learning rate 0.0001
[2019-04-03 21:55:29,453] A3C_AGENT_WORKER-Thread-16 INFO:Local step 500, global step 8246: loss -0.2478
[2019-04-03 21:55:29,455] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 500, global step 8246: learning rate 0.0001
[2019-04-03 21:55:29,578] A3C_AGENT_WORKER-Thread-10 INFO:Local step 500, global step 8272: loss -0.5155
[2019-04-03 21:55:29,578] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 500, global step 8272: learning rate 0.0001
[2019-04-03 21:55:29,900] A3C_AGENT_WORKER-Thread-2 INFO:Local step 500, global step 8322: loss -0.8753
[2019-04-03 21:55:29,922] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 500, global step 8322: learning rate 0.0001
[2019-04-03 21:55:29,961] A3C_AGENT_WORKER-Thread-8 INFO:Local step 500, global step 8327: loss -0.8345
[2019-04-03 21:55:29,999] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 500, global step 8327: learning rate 0.0001
[2019-04-03 21:55:30,140] A3C_AGENT_WORKER-Thread-20 INFO:Local step 500, global step 8369: loss -0.3077
[2019-04-03 21:55:30,141] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 500, global step 8369: learning rate 0.0001
[2019-04-03 21:55:30,526] A3C_AGENT_WORKER-Thread-14 INFO:Local step 500, global step 8443: loss -0.7027
[2019-04-03 21:55:30,549] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 500, global step 8443: learning rate 0.0001
[2019-04-03 21:55:40,201] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00429267 0.41753274 0.00836241 0.07413334 0.25690442 0.02071238
 0.21806195], sum to 1.0000
[2019-04-03 21:55:40,201] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8486
[2019-04-03 21:55:40,513] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.25, 51.0, 58.0, 905.0, 19.0, 19.86002820188061, -1.032467737311114, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 390600.0000, 
sim time next is 391200.0000, 
raw observation next is [-12.06666666666667, 51.0, 57.5, 902.0, 21.0, 19.5615510965748, -1.014966291278485, 1.0, 1.0, 196266.4560289789], 
processed observation next is [1.0, 0.5217391304347826, 0.1283471837488457, 0.51, 0.19166666666666668, 0.9966850828729282, 0.25, 0.13012925804789996, 0.16167790290717168, 1.0, 1.0, 0.9346021715665661], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.9630872], dtype=float32), 0.6736285]. 
=============================================
[2019-04-03 21:56:00,549] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.0820458e-05 8.1733674e-01 6.7538564e-04 7.1360110e-03 1.6530950e-01
 2.9803985e-03 6.5211761e-03], sum to 1.0000
[2019-04-03 21:56:00,553] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6769
[2019-04-03 21:56:00,741] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 19.0, 18.91243351784637, -1.159328563371327, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 546000.0000, 
sim time next is 546600.0000, 
raw observation next is [0.5, 92.0, 12.0, 37.99999999999999, 19.0, 18.82024210118262, -1.16214591365019, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.30434782608695654, 0.4764542936288089, 0.92, 0.04, 0.04198895027624309, 0.08333333333333333, 0.06835350843188515, 0.11261802878327003, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.14412707], dtype=float32), -0.09109919]. 
=============================================
[2019-04-03 21:56:02,546] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.4306358e-05 6.6884786e-01 2.6319822e-04 7.5253816e-03 3.1115088e-01
 3.0285148e-03 9.1499854e-03], sum to 1.0000
[2019-04-03 21:56:02,546] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5185
[2019-04-03 21:56:02,748] A3C_AGENT_WORKER-Thread-7 INFO:Local step 1000, global step 15275: loss 3.4864
[2019-04-03 21:56:02,751] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 1000, global step 15275: learning rate 0.0001
[2019-04-03 21:56:02,767] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.6666666666666667, 82.33333333333334, 87.0, 136.0, 19.0, 18.81769035200371, -1.120249461971347, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 559200.0000, 
sim time next is 559800.0000, 
raw observation next is [-0.7, 82.0, 89.0, 135.0, 19.0, 18.81414046089411, -1.127379448795909, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.443213296398892, 0.82, 0.2966666666666667, 0.14917127071823205, 0.08333333333333333, 0.0678450384078424, 0.12420685040136366, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.22962001], dtype=float32), -0.78329915]. 
=============================================
[2019-04-03 21:56:02,964] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1000, global step 15333: loss 3.0871
[2019-04-03 21:56:02,964] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1000, global step 15333: learning rate 0.0001
[2019-04-03 21:56:02,998] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1000, global step 15343: loss 5.8124
[2019-04-03 21:56:03,012] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1000, global step 15343: learning rate 0.0001
[2019-04-03 21:56:03,137] A3C_AGENT_WORKER-Thread-6 INFO:Local step 1000, global step 15390: loss 4.2925
[2019-04-03 21:56:03,139] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 1000, global step 15390: learning rate 0.0001
[2019-04-03 21:56:03,253] A3C_AGENT_WORKER-Thread-4 INFO:Local step 1000, global step 15425: loss 11.4023
[2019-04-03 21:56:03,258] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 1000, global step 15428: learning rate 0.0001
[2019-04-03 21:56:03,934] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1000, global step 15692: loss 2.9100
[2019-04-03 21:56:03,938] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1000, global step 15694: learning rate 0.0001
[2019-04-03 21:56:05,109] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1000, global step 16087: loss 4.4878
[2019-04-03 21:56:05,110] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1000, global step 16087: learning rate 0.0001
[2019-04-03 21:56:05,183] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.5340611e-06 8.8122320e-01 1.3833499e-04 4.2248075e-03 1.1155231e-01
 1.2184288e-03 1.6333155e-03], sum to 1.0000
[2019-04-03 21:56:05,185] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9791
[2019-04-03 21:56:05,294] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.5, 75.0, 0.0, 0.0, 19.0, 18.72710477176958, -1.215629520374553, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 619200.0000, 
sim time next is 619800.0000, 
raw observation next is [-4.5, 73.83333333333333, 0.0, 0.0, 19.5, 18.63573941359064, -1.216592548510036, 0.0, 1.0, 199012.8802064955], 
processed observation next is [0.0, 0.17391304347826086, 0.3379501385041552, 0.7383333333333333, 0.0, 0.0, 0.125, 0.05297828446588652, 0.09446915049665468, 0.0, 1.0, 0.9476803819356928], 
reward next is 0.0523, 
noisyNet noise sample is [array([0.68455493], dtype=float32), 0.9882759]. 
=============================================
[2019-04-03 21:56:05,514] A3C_AGENT_WORKER-Thread-5 INFO:Local step 1000, global step 16217: loss 8.3516
[2019-04-03 21:56:05,515] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 1000, global step 16217: learning rate 0.0001
[2019-04-03 21:56:05,657] A3C_AGENT_WORKER-Thread-9 INFO:Local step 1000, global step 16264: loss 4.4430
[2019-04-03 21:56:05,659] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 1000, global step 16265: learning rate 0.0001
[2019-04-03 21:56:05,719] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1000, global step 16287: loss 2.5268
[2019-04-03 21:56:05,724] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1000, global step 16288: learning rate 0.0001
[2019-04-03 21:56:05,885] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1000, global step 16357: loss 2.4789
[2019-04-03 21:56:05,895] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1000, global step 16357: learning rate 0.0001
[2019-04-03 21:56:06,020] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1000, global step 16409: loss 2.3747
[2019-04-03 21:56:06,021] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1000, global step 16409: learning rate 0.0001
[2019-04-03 21:56:06,281] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1000, global step 16497: loss 1.9751
[2019-04-03 21:56:06,283] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1000, global step 16498: learning rate 0.0001
[2019-04-03 21:56:06,312] A3C_AGENT_WORKER-Thread-8 INFO:Local step 1000, global step 16504: loss 20.9357
[2019-04-03 21:56:06,313] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 1000, global step 16504: learning rate 0.0001
[2019-04-03 21:56:06,319] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.9767971e-06 9.5299727e-01 3.8242539e-05 3.9324337e-03 4.2188514e-02
 2.7359542e-04 5.6683854e-04], sum to 1.0000
[2019-04-03 21:56:06,319] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1000, global step 16507: loss 4.7874
[2019-04-03 21:56:06,320] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0282
[2019-04-03 21:56:06,322] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1000, global step 16508: learning rate 0.0001
[2019-04-03 21:56:06,340] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.1, 75.0, 0.0, 0.0, 19.0, 18.68811901718497, -1.196601688398188, 0.0, 1.0, 18722.84417175294], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 616800.0000, 
sim time next is 617400.0000, 
raw observation next is [-4.2, 75.0, 0.0, 0.0, 19.0, 18.69081574205084, -1.206519878990285, 0.0, 1.0, 18720.55949081133], 
processed observation next is [0.0, 0.13043478260869565, 0.34626038781163443, 0.75, 0.0, 0.0, 0.08333333333333333, 0.05756797850423675, 0.09782670700323837, 0.0, 1.0, 0.08914552138481587], 
reward next is 0.9109, 
noisyNet noise sample is [array([1.0907762], dtype=float32), 0.79831874]. 
=============================================
[2019-04-03 21:56:06,589] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1000, global step 16622: loss 4.3416
[2019-04-03 21:56:06,591] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1000, global step 16622: learning rate 0.0001
[2019-04-03 21:56:14,638] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.1522765e-08 9.5091838e-01 5.3268805e-06 7.6433754e-04 4.8002619e-02
 6.4842294e-05 2.4450466e-04], sum to 1.0000
[2019-04-03 21:56:14,639] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7785
[2019-04-03 21:56:14,768] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.3, 76.0, 0.0, 0.0, 19.0, 18.70065276683887, -1.273322321179217, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 715200.0000, 
sim time next is 715800.0000, 
raw observation next is [-2.3, 76.0, 0.0, 0.0, 19.0, 18.65473482112485, -1.283901917883737, 0.0, 1.0, 100209.0069285641], 
processed observation next is [1.0, 0.2608695652173913, 0.3988919667590028, 0.76, 0.0, 0.0, 0.08333333333333333, 0.054561235093737594, 0.07203269403875434, 0.0, 1.0, 0.47718574727887664], 
reward next is 0.5228, 
noisyNet noise sample is [array([0.79952174], dtype=float32), -1.9420561]. 
=============================================
[2019-04-03 21:56:19,656] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1162074e-06 9.4929010e-01 4.0318129e-05 1.7123655e-03 4.3056749e-02
 3.0471722e-04 5.5946293e-03], sum to 1.0000
[2019-04-03 21:56:19,659] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8783
[2019-04-03 21:56:19,724] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.7, 75.0, 16.0, 0.0, 19.0, 19.50006860745345, -1.043770135449354, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 806400.0000, 
sim time next is 807000.0000, 
raw observation next is [-6.616666666666667, 75.0, 21.33333333333334, 0.0, 19.0, 19.87791619073923, -1.016566558352588, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.2793167128347184, 0.75, 0.07111111111111112, 0.0, 0.08333333333333333, 0.15649301589493594, 0.16114448054913733, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.2212375], dtype=float32), 0.8643654]. 
=============================================
[2019-04-03 21:56:19,759] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[42.11505 ]
 [42.494232]
 [42.47441 ]
 [42.42054 ]
 [43.118805]], R is [[41.08109665]
 [40.67028427]
 [40.26358032]
 [39.86094666]
 [39.46233749]].
[2019-04-03 21:56:21,400] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.9044859e-05 7.2750324e-01 1.1740578e-03 2.0742562e-02 5.5911928e-02
 6.9116947e-04 1.9393796e-01], sum to 1.0000
[2019-04-03 21:56:21,400] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5180
[2019-04-03 21:56:21,438] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.5, 73.66666666666667, 100.8333333333333, 0.0, 19.0, 19.54599849712263, -1.086847982716428, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 822000.0000, 
sim time next is 822600.0000, 
raw observation next is [-4.5, 75.0, 99.0, 0.0, 19.0, 19.61392985411389, -1.083682800447298, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.3379501385041552, 0.75, 0.33, 0.0, 0.08333333333333333, 0.13449415450949095, 0.13877239985090065, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.8877136], dtype=float32), -0.45074636]. 
=============================================
[2019-04-03 21:56:23,522] A3C_AGENT_WORKER-Thread-7 INFO:Local step 1500, global step 22881: loss -1.9505
[2019-04-03 21:56:23,540] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 1500, global step 22884: learning rate 0.0001
[2019-04-03 21:56:24,577] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1500, global step 23129: loss -1.3323
[2019-04-03 21:56:24,579] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1500, global step 23129: learning rate 0.0001
[2019-04-03 21:56:24,616] A3C_AGENT_WORKER-Thread-6 INFO:Local step 1500, global step 23139: loss 1.5207
[2019-04-03 21:56:24,616] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 1500, global step 23139: learning rate 0.0001
[2019-04-03 21:56:24,857] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1500, global step 23207: loss -0.0103
[2019-04-03 21:56:24,857] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1500, global step 23207: learning rate 0.0001
[2019-04-03 21:56:25,296] A3C_AGENT_WORKER-Thread-4 INFO:Local step 1500, global step 23312: loss 0.0997
[2019-04-03 21:56:25,297] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 1500, global step 23312: learning rate 0.0001
[2019-04-03 21:56:25,423] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1500, global step 23340: loss 4.7013
[2019-04-03 21:56:25,425] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1500, global step 23340: learning rate 0.0001
[2019-04-03 21:56:26,807] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1500, global step 23773: loss -4.0702
[2019-04-03 21:56:26,807] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1500, global step 23773: learning rate 0.0001
[2019-04-03 21:56:28,271] A3C_AGENT_WORKER-Thread-5 INFO:Local step 1500, global step 24154: loss 22.4454
[2019-04-03 21:56:28,273] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 1500, global step 24155: learning rate 0.0001
[2019-04-03 21:56:28,585] A3C_AGENT_WORKER-Thread-8 INFO:Local step 1500, global step 24226: loss -0.8800
[2019-04-03 21:56:28,586] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 1500, global step 24226: learning rate 0.0001
[2019-04-03 21:56:28,617] A3C_AGENT_WORKER-Thread-9 INFO:Local step 1500, global step 24235: loss -2.4682
[2019-04-03 21:56:28,618] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 1500, global step 24235: learning rate 0.0001
[2019-04-03 21:56:28,965] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1500, global step 24310: loss -2.1551
[2019-04-03 21:56:28,967] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1500, global step 24310: learning rate 0.0001
[2019-04-03 21:56:29,554] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1500, global step 24448: loss 0.1240
[2019-04-03 21:56:29,560] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1500, global step 24448: learning rate 0.0001
[2019-04-03 21:56:29,676] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1500, global step 24482: loss 1.0822
[2019-04-03 21:56:29,677] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1500, global step 24482: learning rate 0.0001
[2019-04-03 21:56:29,764] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1500, global step 24504: loss 5.7427
[2019-04-03 21:56:29,767] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1500, global step 24505: learning rate 0.0001
[2019-04-03 21:56:30,642] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1500, global step 24691: loss 21.6265
[2019-04-03 21:56:30,643] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1500, global step 24691: learning rate 0.0001
[2019-04-03 21:56:31,417] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1500, global step 24865: loss 13.1549
[2019-04-03 21:56:31,418] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1500, global step 24865: learning rate 0.0001
[2019-04-03 21:56:33,093] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.8546149e-08 5.1268858e-01 1.6291683e-06 1.0242566e-03 3.9003545e-01
 5.6532783e-05 9.6193515e-02], sum to 1.0000
[2019-04-03 21:56:33,093] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4545
[2019-04-03 21:56:33,314] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.9, 93.0, 92.0, 0.0, 24.0, 24.54740116398906, 0.07261437563255889, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 915000.0000, 
sim time next is 915600.0000, 
raw observation next is [4.0, 93.0, 91.0, 0.0, 23.0, 24.38939755018687, 0.03174673438244368, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5734072022160666, 0.93, 0.30333333333333334, 0.0, 0.4166666666666667, 0.532449795848906, 0.5105822447941478, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6697641], dtype=float32), -0.32754818]. 
=============================================
[2019-04-03 21:56:39,399] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1286885e-08 1.6411494e-01 3.5227922e-07 5.8388095e-03 1.2547576e-01
 1.1846721e-06 7.0456898e-01], sum to 1.0000
[2019-04-03 21:56:39,400] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5291
[2019-04-03 21:56:39,445] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 81.0, 106.5, 0.0, 26.0, 25.40919363179468, 0.372923875437582, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1000800.0000, 
sim time next is 1001400.0000, 
raw observation next is [14.4, 81.0, 102.3333333333333, 0.0, 26.0, 25.44876659299613, 0.3839928688641303, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.8614958448753465, 0.81, 0.341111111111111, 0.0, 0.6666666666666666, 0.6207305494163441, 0.6279976229547101, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2590536], dtype=float32), 0.22969675]. 
=============================================
[2019-04-03 21:56:49,446] A3C_AGENT_WORKER-Thread-7 INFO:Local step 2000, global step 30579: loss 5.4722
[2019-04-03 21:56:49,450] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 2000, global step 30579: learning rate 0.0001
[2019-04-03 21:56:49,459] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2000, global step 30586: loss 3.5049
[2019-04-03 21:56:49,460] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2000, global step 30586: learning rate 0.0001
[2019-04-03 21:56:49,597] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.5627838e-09 1.9803414e-02 1.4455500e-06 2.9362170e-03 2.0634598e-01
 6.2135581e-07 7.7091235e-01], sum to 1.0000
[2019-04-03 21:56:49,598] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4023
[2019-04-03 21:56:49,603] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.55, 55.0, 0.0, 0.0, 26.0, 26.3914371059545, 0.7110775490843357, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1103400.0000, 
sim time next is 1104000.0000, 
raw observation next is [15.36666666666667, 55.66666666666667, 0.0, 0.0, 26.0, 26.25262590384787, 0.6976246523411365, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.8882733148661128, 0.5566666666666668, 0.0, 0.0, 0.6666666666666666, 0.687718825320656, 0.7325415507803789, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4764101], dtype=float32), 0.5033084]. 
=============================================
[2019-04-03 21:56:49,621] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[88.383545]
 [88.49549 ]
 [88.59937 ]
 [88.65359 ]
 [88.677536]], R is [[88.4343338 ]
 [88.54998779]
 [88.66448975]
 [88.77784729]
 [88.89006805]].
[2019-04-03 21:56:49,998] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2000, global step 30850: loss 8.0485
[2019-04-03 21:56:50,001] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2000, global step 30850: learning rate 0.0001
[2019-04-03 21:56:50,139] A3C_AGENT_WORKER-Thread-6 INFO:Local step 2000, global step 30928: loss 6.2364
[2019-04-03 21:56:50,161] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 2000, global step 30928: learning rate 0.0001
[2019-04-03 21:56:50,281] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2000, global step 31000: loss 5.0423
[2019-04-03 21:56:50,283] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2000, global step 31000: learning rate 0.0001
[2019-04-03 21:56:51,129] A3C_AGENT_WORKER-Thread-4 INFO:Local step 2000, global step 31446: loss 2.4203
[2019-04-03 21:56:51,130] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 2000, global step 31447: learning rate 0.0001
[2019-04-03 21:56:51,797] A3C_AGENT_WORKER-Thread-5 INFO:Local step 2000, global step 31801: loss 1.8290
[2019-04-03 21:56:51,799] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 2000, global step 31801: learning rate 0.0001
[2019-04-03 21:56:52,487] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2000, global step 32155: loss 2.8476
[2019-04-03 21:56:52,489] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2000, global step 32157: learning rate 0.0001
[2019-04-03 21:56:53,077] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2000, global step 32432: loss 3.9590
[2019-04-03 21:56:53,079] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2000, global step 32435: learning rate 0.0001
[2019-04-03 21:56:53,301] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2000, global step 32530: loss 2.5936
[2019-04-03 21:56:53,306] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2000, global step 32531: learning rate 0.0001
[2019-04-03 21:56:53,317] A3C_AGENT_WORKER-Thread-9 INFO:Local step 2000, global step 32533: loss 4.0728
[2019-04-03 21:56:53,320] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 2000, global step 32534: learning rate 0.0001
[2019-04-03 21:56:53,480] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2000, global step 32602: loss 2.7973
[2019-04-03 21:56:53,480] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2000, global step 32602: learning rate 0.0001
[2019-04-03 21:56:53,567] A3C_AGENT_WORKER-Thread-8 INFO:Local step 2000, global step 32631: loss 2.7727
[2019-04-03 21:56:53,572] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 2000, global step 32631: learning rate 0.0001
[2019-04-03 21:56:54,517] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2000, global step 33063: loss 3.1414
[2019-04-03 21:56:54,519] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2000, global step 33063: learning rate 0.0001
[2019-04-03 21:56:54,684] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2000, global step 33131: loss 1.9605
[2019-04-03 21:56:54,685] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2000, global step 33131: learning rate 0.0001
[2019-04-03 21:56:55,195] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2000, global step 33364: loss 2.8160
[2019-04-03 21:56:55,199] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2000, global step 33364: learning rate 0.0001
[2019-04-03 21:57:08,836] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4930572e-11 2.4373764e-03 5.7923016e-10 5.5186375e-04 8.7489575e-02
 2.2274618e-09 9.0952122e-01], sum to 1.0000
[2019-04-03 21:57:08,836] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6425
[2019-04-03 21:57:09,175] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 26.0, 25.22969543171212, 0.4850250624730421, 0.0, 1.0, 41135.88733779678], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1381200.0000, 
sim time next is 1381800.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 26.0, 25.21286157783024, 0.4860583127794655, 0.0, 1.0, 40943.82426115229], 
processed observation next is [1.0, 1.0, 0.46260387811634357, 0.95, 0.0, 0.0, 0.6666666666666666, 0.6010717981525199, 0.6620194375931552, 0.0, 1.0, 0.1949705917197728], 
reward next is 0.8050, 
noisyNet noise sample is [array([-0.08669123], dtype=float32), 0.99879086]. 
=============================================
[2019-04-03 21:57:12,071] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.10157941e-11 1.45775371e-03 5.14838705e-09 5.36871725e-04
 1.02434024e-01 1.11539418e-08 8.95571351e-01], sum to 1.0000
[2019-04-03 21:57:12,072] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9620
[2019-04-03 21:57:12,331] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 54.66666666666666, 0.0, 26.0, 26.03959329027296, 0.5081324453996278, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1437000.0000, 
sim time next is 1437600.0000, 
raw observation next is [1.1, 92.0, 50.33333333333333, 0.0, 26.0, 25.92381638853897, 0.5069379827750623, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.49307479224376743, 0.92, 0.16777777777777778, 0.0, 0.6666666666666666, 0.6603180323782475, 0.6689793275916874, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.60893387], dtype=float32), -0.43537787]. 
=============================================
[2019-04-03 21:57:16,544] A3C_AGENT_WORKER-Thread-7 INFO:Local step 2500, global step 38746: loss 1.8536
[2019-04-03 21:57:16,545] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 2500, global step 38746: learning rate 0.0001
[2019-04-03 21:57:18,406] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2500, global step 39132: loss 1.5985
[2019-04-03 21:57:18,406] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2500, global step 39132: learning rate 0.0001
[2019-04-03 21:57:18,787] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2500, global step 39199: loss 0.3547
[2019-04-03 21:57:18,788] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2500, global step 39199: learning rate 0.0001
[2019-04-03 21:57:19,074] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2500, global step 39246: loss 2.0863
[2019-04-03 21:57:19,075] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2500, global step 39246: learning rate 0.0001
[2019-04-03 21:57:20,417] A3C_AGENT_WORKER-Thread-6 INFO:Local step 2500, global step 39448: loss 1.5538
[2019-04-03 21:57:20,449] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 2500, global step 39448: learning rate 0.0001
[2019-04-03 21:57:20,518] A3C_AGENT_WORKER-Thread-5 INFO:Local step 2500, global step 39471: loss 1.4079
[2019-04-03 21:57:20,518] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 2500, global step 39471: learning rate 0.0001
[2019-04-03 21:57:21,840] A3C_AGENT_WORKER-Thread-4 INFO:Local step 2500, global step 39690: loss 0.9287
[2019-04-03 21:57:21,853] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 2500, global step 39690: learning rate 0.0001
[2019-04-03 21:57:22,864] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2500, global step 39940: loss 1.0449
[2019-04-03 21:57:22,865] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2500, global step 39941: learning rate 0.0001
[2019-04-03 21:57:24,669] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [5.9701416e-10 2.6003813e-02 2.7803262e-07 4.5351064e-04 6.3143969e-01
 4.7021950e-07 3.4210229e-01], sum to 1.0000
[2019-04-03 21:57:24,669] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4894
[2019-04-03 21:57:24,726] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.7, 82.5, 0.0, 0.0, 26.0, 25.52778030014558, 0.5569354950050492, 0.0, 1.0, 9374.982461393132], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1564200.0000, 
sim time next is 1564800.0000, 
raw observation next is [4.600000000000001, 83.66666666666666, 0.0, 0.0, 26.0, 25.68106735461973, 0.5399238924807606, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.5900277008310251, 0.8366666666666666, 0.0, 0.0, 0.6666666666666666, 0.6400889462183109, 0.6799746308269201, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.20507033], dtype=float32), 0.4131347]. 
=============================================
[2019-04-03 21:57:25,207] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2500, global step 40545: loss 1.6702
[2019-04-03 21:57:25,208] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2500, global step 40545: learning rate 0.0001
[2019-04-03 21:57:25,562] A3C_AGENT_WORKER-Thread-9 INFO:Local step 2500, global step 40610: loss 2.5202
[2019-04-03 21:57:25,564] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 2500, global step 40610: learning rate 0.0001
[2019-04-03 21:57:25,709] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2500, global step 40650: loss 3.4142
[2019-04-03 21:57:25,709] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2500, global step 40650: learning rate 0.0001
[2019-04-03 21:57:25,814] A3C_AGENT_WORKER-Thread-8 INFO:Local step 2500, global step 40686: loss 2.0377
[2019-04-03 21:57:25,815] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 2500, global step 40686: learning rate 0.0001
[2019-04-03 21:57:25,891] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2500, global step 40706: loss 2.2380
[2019-04-03 21:57:25,891] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2500, global step 40706: learning rate 0.0001
[2019-04-03 21:57:26,043] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2500, global step 40748: loss 1.2117
[2019-04-03 21:57:26,045] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2500, global step 40748: learning rate 0.0001
[2019-04-03 21:57:26,800] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2500, global step 40985: loss 2.2545
[2019-04-03 21:57:26,801] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2500, global step 40985: learning rate 0.0001
[2019-04-03 21:57:27,269] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2500, global step 41128: loss 2.5498
[2019-04-03 21:57:27,269] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2500, global step 41128: learning rate 0.0001
[2019-04-03 21:57:30,351] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.1650292e-11 8.8066532e-04 9.3986809e-09 1.7783456e-04 1.4331539e-01
 1.5283087e-08 8.5562605e-01], sum to 1.0000
[2019-04-03 21:57:30,352] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3570
[2019-04-03 21:57:30,366] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.7, 74.0, 0.0, 0.0, 26.0, 25.42508175719121, 0.6829984091502088, 0.0, 1.0, 61556.08662674941], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1544400.0000, 
sim time next is 1545000.0000, 
raw observation next is [7.516666666666667, 74.33333333333334, 0.0, 0.0, 26.0, 25.8451037973143, 0.699457890700962, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.6708217913204063, 0.7433333333333334, 0.0, 0.0, 0.6666666666666666, 0.6537586497761918, 0.7331526302336541, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.9040961], dtype=float32), 1.54019]. 
=============================================
[2019-04-03 21:57:30,369] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[112.38593]
 [112.75068]
 [111.74757]
 [111.18656]
 [110.75132]], R is [[112.50554657]
 [112.08737183]
 [111.13957214]
 [110.08332062]
 [109.04597473]].
[2019-04-03 21:57:46,131] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.2436531e-10 1.6456170e-03 1.5045639e-08 3.0927688e-03 6.2786557e-02
 7.0893797e-08 9.3247497e-01], sum to 1.0000
[2019-04-03 21:57:46,142] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2929
[2019-04-03 21:57:46,288] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 88.0, 101.1666666666667, 0.0, 26.0, 24.76443139319572, 0.4265619932987548, 1.0, 1.0, 196464.9794054528], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1689600.0000, 
sim time next is 1690200.0000, 
raw observation next is [1.1, 88.0, 100.0, 0.0, 26.0, 24.24950551790538, 0.4312265888611167, 1.0, 1.0, 197950.3739572187], 
processed observation next is [1.0, 0.5652173913043478, 0.49307479224376743, 0.88, 0.3333333333333333, 0.0, 0.6666666666666666, 0.520792126492115, 0.6437421962870389, 1.0, 1.0, 0.942620828367708], 
reward next is 0.0574, 
noisyNet noise sample is [array([0.78706545], dtype=float32), -0.4702003]. 
=============================================
[2019-04-03 21:57:53,141] A3C_AGENT_WORKER-Thread-7 INFO:Local step 3000, global step 47013: loss -0.6050
[2019-04-03 21:57:53,142] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 3000, global step 47013: learning rate 0.0001
[2019-04-03 21:57:54,061] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3000, global step 47171: loss -0.1181
[2019-04-03 21:57:54,062] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3000, global step 47171: learning rate 0.0001
[2019-04-03 21:57:54,738] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3000, global step 47308: loss -0.0675
[2019-04-03 21:57:54,741] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3000, global step 47310: learning rate 0.0001
[2019-04-03 21:57:54,952] A3C_AGENT_WORKER-Thread-5 INFO:Local step 3000, global step 47349: loss -2.0796
[2019-04-03 21:57:54,952] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 3000, global step 47349: learning rate 0.0001
[2019-04-03 21:57:55,054] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3000, global step 47377: loss -0.4382
[2019-04-03 21:57:55,056] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3000, global step 47377: learning rate 0.0001
[2019-04-03 21:57:55,884] A3C_AGENT_WORKER-Thread-6 INFO:Local step 3000, global step 47538: loss -0.5203
[2019-04-03 21:57:55,884] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 3000, global step 47538: learning rate 0.0001
[2019-04-03 21:57:55,901] A3C_AGENT_WORKER-Thread-4 INFO:Local step 3000, global step 47541: loss -0.0650
[2019-04-03 21:57:55,902] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 3000, global step 47541: learning rate 0.0001
[2019-04-03 21:57:57,562] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3000, global step 47849: loss -0.2955
[2019-04-03 21:57:57,570] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3000, global step 47849: learning rate 0.0001
[2019-04-03 21:57:58,000] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4287312e-08 8.1773233e-03 8.0122624e-07 9.6338853e-04 4.4283849e-01
 3.3580263e-06 5.4801661e-01], sum to 1.0000
[2019-04-03 21:57:58,001] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5947
[2019-04-03 21:57:58,094] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.933333333333333, 82.16666666666667, 0.0, 0.0, 26.0, 24.20075307242129, 0.1211343904426185, 0.0, 1.0, 46314.64427647562], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1821000.0000, 
sim time next is 1821600.0000, 
raw observation next is [-6.0, 83.0, 0.0, 0.0, 26.0, 24.1660017885738, 0.1134127446249533, 0.0, 1.0, 46388.49555660396], 
processed observation next is [0.0, 0.08695652173913043, 0.296398891966759, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5138334823811501, 0.5378042482083177, 0.0, 1.0, 0.22089759788859026], 
reward next is 0.7791, 
noisyNet noise sample is [array([-0.37086922], dtype=float32), 1.6427202]. 
=============================================
[2019-04-03 21:58:00,979] A3C_AGENT_WORKER-Thread-9 INFO:Local step 3000, global step 48467: loss -0.5544
[2019-04-03 21:58:01,015] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 3000, global step 48467: learning rate 0.0001
[2019-04-03 21:58:01,712] A3C_AGENT_WORKER-Thread-8 INFO:Local step 3000, global step 48574: loss -0.1156
[2019-04-03 21:58:01,733] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 3000, global step 48574: learning rate 0.0001
[2019-04-03 21:58:02,449] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3000, global step 48675: loss -0.5827
[2019-04-03 21:58:02,450] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3000, global step 48675: learning rate 0.0001
[2019-04-03 21:58:02,826] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3000, global step 48736: loss -0.2886
[2019-04-03 21:58:02,827] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3000, global step 48736: learning rate 0.0001
[2019-04-03 21:58:03,033] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3000, global step 48775: loss -0.1102
[2019-04-03 21:58:03,033] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3000, global step 48775: learning rate 0.0001
[2019-04-03 21:58:03,141] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3000, global step 48793: loss -0.2821
[2019-04-03 21:58:03,144] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3000, global step 48793: learning rate 0.0001
[2019-04-03 21:58:03,346] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3000, global step 48828: loss -0.5986
[2019-04-03 21:58:03,347] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3000, global step 48828: learning rate 0.0001
[2019-04-03 21:58:04,791] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3000, global step 49076: loss -0.1697
[2019-04-03 21:58:04,846] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3000, global step 49076: learning rate 0.0001
[2019-04-03 21:58:35,298] A3C_AGENT_WORKER-Thread-7 INFO:Local step 3500, global step 55001: loss 1.2483
[2019-04-03 21:58:35,300] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 3500, global step 55001: learning rate 0.0001
[2019-04-03 21:58:35,679] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3500, global step 55071: loss -4.1549
[2019-04-03 21:58:35,724] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3500, global step 55071: learning rate 0.0001
[2019-04-03 21:58:36,020] A3C_AGENT_WORKER-Thread-6 INFO:Local step 3500, global step 55133: loss 0.3980
[2019-04-03 21:58:36,020] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 3500, global step 55133: learning rate 0.0001
[2019-04-03 21:58:36,360] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3500, global step 55188: loss -3.4242
[2019-04-03 21:58:36,361] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3500, global step 55188: learning rate 0.0001
[2019-04-03 21:58:36,493] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3500, global step 55214: loss -1.3922
[2019-04-03 21:58:36,494] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3500, global step 55214: learning rate 0.0001
[2019-04-03 21:58:36,995] A3C_AGENT_WORKER-Thread-5 INFO:Local step 3500, global step 55291: loss 0.8982
[2019-04-03 21:58:36,995] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 3500, global step 55291: learning rate 0.0001
[2019-04-03 21:58:37,035] A3C_AGENT_WORKER-Thread-4 INFO:Local step 3500, global step 55298: loss 0.9758
[2019-04-03 21:58:37,035] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 3500, global step 55298: learning rate 0.0001
[2019-04-03 21:58:38,512] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3500, global step 55591: loss -2.7182
[2019-04-03 21:58:38,513] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3500, global step 55591: learning rate 0.0001
[2019-04-03 21:58:41,025] A3C_AGENT_WORKER-Thread-8 INFO:Local step 3500, global step 56070: loss 0.5241
[2019-04-03 21:58:41,025] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 3500, global step 56070: learning rate 0.0001
[2019-04-03 21:58:41,132] A3C_AGENT_WORKER-Thread-9 INFO:Local step 3500, global step 56096: loss 0.8087
[2019-04-03 21:58:41,133] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 3500, global step 56096: learning rate 0.0001
[2019-04-03 21:58:42,111] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3500, global step 56290: loss -1.3856
[2019-04-03 21:58:42,112] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3500, global step 56290: learning rate 0.0001
[2019-04-03 21:58:43,432] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3500, global step 56602: loss 0.5464
[2019-04-03 21:58:43,433] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3500, global step 56602: learning rate 0.0001
[2019-04-03 21:58:43,523] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3500, global step 56628: loss -1.5060
[2019-04-03 21:58:43,533] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3500, global step 56631: learning rate 0.0001
[2019-04-03 21:58:44,559] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3500, global step 56893: loss -1.1343
[2019-04-03 21:58:44,562] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3500, global step 56893: learning rate 0.0001
[2019-04-03 21:58:44,594] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3500, global step 56904: loss -3.2122
[2019-04-03 21:58:44,595] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3500, global step 56904: learning rate 0.0001
[2019-04-03 21:58:45,566] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3500, global step 57163: loss -1.3093
[2019-04-03 21:58:45,567] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3500, global step 57163: learning rate 0.0001
[2019-04-03 21:58:57,971] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.9212962e-09 5.0755101e-04 3.3160140e-07 6.2778872e-04 2.6825506e-02
 4.2711821e-07 9.7203833e-01], sum to 1.0000
[2019-04-03 21:58:57,971] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0511
[2019-04-03 21:58:58,031] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.4, 87.0, 0.0, 0.0, 26.0, 24.05837324889223, 0.06309067626810543, 0.0, 1.0, 43611.92552822136], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2260800.0000, 
sim time next is 2261400.0000, 
raw observation next is [-8.483333333333334, 87.66666666666667, 0.0, 0.0, 26.0, 24.00770776592125, 0.05520447320968088, 0.0, 1.0, 43599.89011702919], 
processed observation next is [1.0, 0.17391304347826086, 0.2276084949215143, 0.8766666666666667, 0.0, 0.0, 0.6666666666666666, 0.5006423138267708, 0.5184014910698936, 0.0, 1.0, 0.20761852436680564], 
reward next is 0.7924, 
noisyNet noise sample is [array([0.7769887], dtype=float32), 1.2111375]. 
=============================================
[2019-04-03 21:59:12,840] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4000, global step 62970: loss 0.3131
[2019-04-03 21:59:12,841] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4000, global step 62970: learning rate 0.0001
[2019-04-03 21:59:14,233] A3C_AGENT_WORKER-Thread-6 INFO:Local step 4000, global step 63239: loss -0.9426
[2019-04-03 21:59:14,235] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 4000, global step 63239: learning rate 0.0001
[2019-04-03 21:59:14,444] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4000, global step 63273: loss -2.8108
[2019-04-03 21:59:14,445] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4000, global step 63274: learning rate 0.0001
[2019-04-03 21:59:14,567] A3C_AGENT_WORKER-Thread-7 INFO:Local step 4000, global step 63293: loss -2.6259
[2019-04-03 21:59:14,571] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 4000, global step 63293: learning rate 0.0001
[2019-04-03 21:59:15,476] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4000, global step 63442: loss -1.9253
[2019-04-03 21:59:15,493] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4000, global step 63442: learning rate 0.0001
[2019-04-03 21:59:15,876] A3C_AGENT_WORKER-Thread-5 INFO:Local step 4000, global step 63521: loss -1.3385
[2019-04-03 21:59:15,880] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 4000, global step 63521: learning rate 0.0001
[2019-04-03 21:59:16,232] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4000, global step 63589: loss 0.2427
[2019-04-03 21:59:16,232] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4000, global step 63592: learning rate 0.0001
[2019-04-03 21:59:16,314] A3C_AGENT_WORKER-Thread-4 INFO:Local step 4000, global step 63619: loss 0.0877
[2019-04-03 21:59:16,314] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 4000, global step 63619: learning rate 0.0001
[2019-04-03 21:59:18,989] A3C_AGENT_WORKER-Thread-9 INFO:Local step 4000, global step 64244: loss 0.4229
[2019-04-03 21:59:18,993] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 4000, global step 64244: learning rate 0.0001
[2019-04-03 21:59:19,250] A3C_AGENT_WORKER-Thread-8 INFO:Local step 4000, global step 64302: loss 0.4144
[2019-04-03 21:59:19,265] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 4000, global step 64302: learning rate 0.0001
[2019-04-03 21:59:19,787] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1097884e-07 9.8127581e-04 6.1038776e-07 1.0959207e-03 9.6223161e-02
 1.5588922e-06 9.0169740e-01], sum to 1.0000
[2019-04-03 21:59:19,787] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3495
[2019-04-03 21:59:19,831] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.4, 61.0, 0.0, 0.0, 26.0, 23.42792306477737, -0.1019867423006762, 0.0, 1.0, 44419.96303543804], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2436000.0000, 
sim time next is 2436600.0000, 
raw observation next is [-8.4, 61.0, 0.0, 0.0, 26.0, 23.39197589197315, -0.1098567909943452, 0.0, 1.0, 44407.77557752245], 
processed observation next is [0.0, 0.17391304347826086, 0.2299168975069252, 0.61, 0.0, 0.0, 0.6666666666666666, 0.44933132433109585, 0.4633810696685516, 0.0, 1.0, 0.21146559798820214], 
reward next is 0.7885, 
noisyNet noise sample is [array([-1.3445967], dtype=float32), -1.5296756]. 
=============================================
[2019-04-03 21:59:19,931] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4000, global step 64461: loss 0.4260
[2019-04-03 21:59:19,945] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4000, global step 64461: learning rate 0.0001
[2019-04-03 21:59:20,016] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.4800961e-09 9.2865397e-05 8.5624734e-09 1.0415121e-04 9.0756407e-03
 3.0719654e-08 9.9072731e-01], sum to 1.0000
[2019-04-03 21:59:20,033] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8377
[2019-04-03 21:59:20,097] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 47.00000000000001, 110.6666666666667, 227.3333333333334, 26.0, 24.94902872094423, 0.3080088270480636, 0.0, 1.0, 28171.87178833127], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2387400.0000, 
sim time next is 2388000.0000, 
raw observation next is [0.0, 47.0, 98.33333333333333, 284.1666666666667, 26.0, 24.96495857966568, 0.3129836073187096, 0.0, 1.0, 20454.0355888984], 
processed observation next is [0.0, 0.6521739130434783, 0.46260387811634357, 0.47, 0.3277777777777778, 0.31399631675874773, 0.6666666666666666, 0.58041321497214, 0.6043278691062365, 0.0, 1.0, 0.09740016947094476], 
reward next is 0.9026, 
noisyNet noise sample is [array([-0.83316994], dtype=float32), -0.48699385]. 
=============================================
[2019-04-03 21:59:20,156] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[80.57127 ]
 [80.71586 ]
 [80.856445]
 [81.06554 ]
 [81.29151 ]], R is [[80.57287598]
 [80.63299561]
 [80.64391327]
 [80.63078308]
 [80.64102173]].
[2019-04-03 21:59:20,836] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4000, global step 64639: loss 0.4709
[2019-04-03 21:59:20,836] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4000, global step 64639: learning rate 0.0001
[2019-04-03 21:59:22,011] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4000, global step 64890: loss 0.5322
[2019-04-03 21:59:22,037] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4000, global step 64890: learning rate 0.0001
[2019-04-03 21:59:22,498] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4000, global step 65021: loss 0.5795
[2019-04-03 21:59:22,528] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4000, global step 65025: learning rate 0.0001
[2019-04-03 21:59:22,852] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4000, global step 65126: loss 0.5602
[2019-04-03 21:59:22,853] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4000, global step 65126: learning rate 0.0001
[2019-04-03 21:59:24,368] A3C_AGENT_WORKER-Thread-10 INFO:Local step 4000, global step 65473: loss 0.6715
[2019-04-03 21:59:24,368] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 4000, global step 65473: learning rate 0.0001
[2019-04-03 21:59:24,618] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.1900217e-09 7.9967402e-05 2.6403375e-08 2.9287091e-04 7.2049196e-03
 2.7910509e-08 9.9242216e-01], sum to 1.0000
[2019-04-03 21:59:24,644] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5329
[2019-04-03 21:59:24,849] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.8999999999999999, 34.0, 0.0, 0.0, 26.0, 25.20039013585588, 0.2508426502497689, 0.0, 1.0, 40298.6105217578], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2500200.0000, 
sim time next is 2500800.0000, 
raw observation next is [-0.8, 34.33333333333334, 0.0, 0.0, 26.0, 25.18185990891742, 0.2475446971031683, 0.0, 1.0, 40253.90198284896], 
processed observation next is [0.0, 0.9565217391304348, 0.4404432132963989, 0.34333333333333343, 0.0, 0.0, 0.6666666666666666, 0.5984883257431184, 0.5825148990343895, 0.0, 1.0, 0.191685247537376], 
reward next is 0.8083, 
noisyNet noise sample is [array([1.4581487], dtype=float32), -0.41651046]. 
=============================================
[2019-04-03 21:59:45,575] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4500, global step 70690: loss 0.6299
[2019-04-03 21:59:45,580] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4500, global step 70691: learning rate 0.0001
[2019-04-03 21:59:46,880] A3C_AGENT_WORKER-Thread-7 INFO:Local step 4500, global step 70986: loss 0.4593
[2019-04-03 21:59:46,881] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 4500, global step 70986: learning rate 0.0001
[2019-04-03 21:59:46,989] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4500, global step 71034: loss 0.4606
[2019-04-03 21:59:46,990] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4500, global step 71034: learning rate 0.0001
[2019-04-03 21:59:47,334] A3C_AGENT_WORKER-Thread-6 INFO:Local step 4500, global step 71120: loss 0.4854
[2019-04-03 21:59:47,388] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 4500, global step 71120: learning rate 0.0001
[2019-04-03 21:59:48,404] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4500, global step 71344: loss -1.4661
[2019-04-03 21:59:48,424] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4500, global step 71344: learning rate 0.0001
[2019-04-03 21:59:48,594] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4500, global step 71389: loss 0.2943
[2019-04-03 21:59:48,610] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4500, global step 71391: learning rate 0.0001
[2019-04-03 21:59:48,720] A3C_AGENT_WORKER-Thread-5 INFO:Local step 4500, global step 71432: loss 0.3574
[2019-04-03 21:59:48,721] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 4500, global step 71432: learning rate 0.0001
[2019-04-03 21:59:49,200] A3C_AGENT_WORKER-Thread-4 INFO:Local step 4500, global step 71570: loss 0.2900
[2019-04-03 21:59:49,205] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 4500, global step 71571: learning rate 0.0001
[2019-04-03 21:59:50,573] A3C_AGENT_WORKER-Thread-9 INFO:Local step 4500, global step 71908: loss 0.2915
[2019-04-03 21:59:50,573] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 4500, global step 71908: learning rate 0.0001
[2019-04-03 21:59:51,738] A3C_AGENT_WORKER-Thread-8 INFO:Local step 4500, global step 72174: loss 0.3123
[2019-04-03 21:59:51,739] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 4500, global step 72174: learning rate 0.0001
[2019-04-03 21:59:51,872] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4500, global step 72207: loss 0.2930
[2019-04-03 21:59:51,899] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4500, global step 72207: learning rate 0.0001
[2019-04-03 21:59:52,760] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4500, global step 72433: loss 0.2370
[2019-04-03 21:59:52,763] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4500, global step 72433: learning rate 0.0001
[2019-04-03 21:59:53,979] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4500, global step 72734: loss 0.2156
[2019-04-03 21:59:53,986] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4500, global step 72734: learning rate 0.0001
[2019-04-03 21:59:54,044] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4500, global step 72757: loss 0.2292
[2019-04-03 21:59:54,048] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4500, global step 72757: learning rate 0.0001
[2019-04-03 21:59:54,373] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4500, global step 72840: loss 0.2216
[2019-04-03 21:59:54,374] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4500, global step 72840: learning rate 0.0001
[2019-04-03 21:59:56,834] A3C_AGENT_WORKER-Thread-10 INFO:Local step 4500, global step 73424: loss 0.1629
[2019-04-03 21:59:56,850] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 4500, global step 73424: learning rate 0.0001
[2019-04-03 22:00:05,855] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [7.2469597e-10 2.2056247e-06 7.3020612e-09 1.1218390e-04 3.7935909e-03
 2.1548682e-10 9.9609202e-01], sum to 1.0000
[2019-04-03 22:00:05,855] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3843
[2019-04-03 22:00:06,095] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 44.0, 0.0, 0.0, 26.0, 24.79273919836839, 0.3348117528267845, 0.0, 1.0, 183205.3147665771], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2838600.0000, 
sim time next is 2839200.0000, 
raw observation next is [2.0, 44.0, 0.0, 0.0, 26.0, 24.84034566075241, 0.3610959420495708, 0.0, 1.0, 105649.935703486], 
processed observation next is [1.0, 0.8695652173913043, 0.518005540166205, 0.44, 0.0, 0.0, 0.6666666666666666, 0.5700288050627007, 0.6203653140165236, 0.0, 1.0, 0.5030949319213619], 
reward next is 0.4969, 
noisyNet noise sample is [array([0.20314005], dtype=float32), -1.1430234]. 
=============================================
[2019-04-03 22:00:13,459] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.8527436e-12 6.9299915e-07 1.3731704e-11 5.5178070e-06 7.5193508e-05
 1.8631336e-11 9.9991858e-01], sum to 1.0000
[2019-04-03 22:00:13,459] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0782
[2019-04-03 22:00:13,478] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 100.0, 164.0, 0.0, 26.0, 25.35667432947472, 0.3363077041993843, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2899800.0000, 
sim time next is 2900400.0000, 
raw observation next is [2.0, 100.0, 151.6666666666667, 0.0, 26.0, 25.39731256301352, 0.3385448379912159, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.518005540166205, 1.0, 0.5055555555555558, 0.0, 0.6666666666666666, 0.6164427135844601, 0.6128482793304053, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3792747], dtype=float32), 0.26680544]. 
=============================================
[2019-04-03 22:00:15,078] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.7398662e-10 5.8447067e-06 1.4012098e-09 4.2196945e-05 1.7858453e-03
 1.0774004e-09 9.9816614e-01], sum to 1.0000
[2019-04-03 22:00:15,078] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2339
[2019-04-03 22:00:15,229] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 85.00000000000001, 0.0, 0.0, 26.0, 24.85789737026417, 0.3229278375925482, 0.0, 1.0, 43289.10546502016], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2941800.0000, 
sim time next is 2942400.0000, 
raw observation next is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.82694007722237, 0.3150732121356645, 0.0, 1.0, 43291.54801895672], 
processed observation next is [0.0, 0.043478260869565216, 0.40720221606648205, 0.85, 0.0, 0.0, 0.6666666666666666, 0.5689116731018643, 0.6050244040452215, 0.0, 1.0, 0.20615022866169866], 
reward next is 0.7938, 
noisyNet noise sample is [array([0.2388832], dtype=float32), -1.8051586]. 
=============================================
[2019-04-03 22:00:16,213] A3C_AGENT_WORKER-Thread-7 INFO:Local step 5000, global step 78671: loss 0.1472
[2019-04-03 22:00:16,213] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 5000, global step 78671: learning rate 0.0001
[2019-04-03 22:00:16,744] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5000, global step 78790: loss 0.1049
[2019-04-03 22:00:16,745] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5000, global step 78790: learning rate 0.0001
[2019-04-03 22:00:16,890] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [9.4423829e-09 1.1362171e-03 2.6746326e-07 1.7686271e-03 2.2663511e-02
 2.7522464e-07 9.7443116e-01], sum to 1.0000
[2019-04-03 22:00:16,914] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.0715
[2019-04-03 22:00:17,092] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 84.0, 0.0, 0.0, 26.0, 24.52220861900584, 0.2395859630912371, 0.0, 1.0, 42679.32363770415], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2950800.0000, 
sim time next is 2951400.0000, 
raw observation next is [-3.0, 84.0, 0.0, 0.0, 26.0, 24.48357815075277, 0.231484457791979, 0.0, 1.0, 42737.27427850183], 
processed observation next is [0.0, 0.13043478260869565, 0.3795013850415513, 0.84, 0.0, 0.0, 0.6666666666666666, 0.5402981792293975, 0.5771614859306596, 0.0, 1.0, 0.20351082989762775], 
reward next is 0.7965, 
noisyNet noise sample is [array([0.08637522], dtype=float32), -0.17345613]. 
=============================================
[2019-04-03 22:00:17,572] A3C_AGENT_WORKER-Thread-6 INFO:Local step 5000, global step 78924: loss 0.0974
[2019-04-03 22:00:17,572] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 5000, global step 78924: learning rate 0.0001
[2019-04-03 22:00:19,231] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [4.6125823e-08 1.0406420e-03 1.0655333e-07 2.0112409e-03 7.6880753e-02
 3.8203929e-08 9.2006713e-01], sum to 1.0000
[2019-04-03 22:00:19,237] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3060
[2019-04-03 22:00:19,282] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5000, global step 79283: loss 0.0741
[2019-04-03 22:00:19,293] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5000, global step 79284: learning rate 0.0001
[2019-04-03 22:00:19,447] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.14748836443009, 0.3184201692423906, 0.0, 1.0, 38639.58576301083], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3018000.0000, 
sim time next is 3018600.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.12005199160612, 0.3099281683767692, 0.0, 1.0, 38536.26499683742], 
processed observation next is [0.0, 0.9565217391304348, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5933376659671765, 0.6033093894589231, 0.0, 1.0, 0.1835060237944639], 
reward next is 0.8165, 
noisyNet noise sample is [array([0.9002551], dtype=float32), -0.111127965]. 
=============================================
[2019-04-03 22:00:21,019] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5000, global step 79729: loss 0.0765
[2019-04-03 22:00:21,020] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5000, global step 79729: learning rate 0.0001
[2019-04-03 22:00:21,273] A3C_AGENT_WORKER-Thread-5 INFO:Local step 5000, global step 79763: loss 0.0786
[2019-04-03 22:00:21,274] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 5000, global step 79763: learning rate 0.0001
[2019-04-03 22:00:22,102] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5000, global step 79949: loss 0.0818
[2019-04-03 22:00:22,103] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5000, global step 79949: learning rate 0.0001
[2019-04-03 22:00:22,882] A3C_AGENT_WORKER-Thread-4 INFO:Local step 5000, global step 80071: loss 0.0828
[2019-04-03 22:00:22,884] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 5000, global step 80071: learning rate 0.0001
[2019-04-03 22:00:23,380] A3C_AGENT_WORKER-Thread-9 INFO:Local step 5000, global step 80163: loss 0.0843
[2019-04-03 22:00:23,382] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 5000, global step 80163: learning rate 0.0001
[2019-04-03 22:00:23,623] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5000, global step 80186: loss 0.0685
[2019-04-03 22:00:23,624] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5000, global step 80186: learning rate 0.0001
[2019-04-03 22:00:23,863] A3C_AGENT_WORKER-Thread-8 INFO:Local step 5000, global step 80219: loss 0.1013
[2019-04-03 22:00:23,864] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 5000, global step 80219: learning rate 0.0001
[2019-04-03 22:00:25,283] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.9258216e-09 1.4452504e-04 3.9127961e-08 3.8594371e-04 1.1415353e-02
 1.5761644e-08 9.8805410e-01], sum to 1.0000
[2019-04-03 22:00:25,284] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9400
[2019-04-03 22:00:25,345] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.166666666666667, 55.83333333333334, 74.0, 602.6666666666667, 26.0, 25.13807049132855, 0.4095937275217976, 0.0, 1.0, 18706.97687464843], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2994600.0000, 
sim time next is 2995200.0000, 
raw observation next is [-1.0, 55.0, 69.5, 570.5, 26.0, 25.1485810504084, 0.4077719122581202, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.4349030470914128, 0.55, 0.23166666666666666, 0.6303867403314917, 0.6666666666666666, 0.5957150875340332, 0.6359239707527068, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.40561968], dtype=float32), -0.32710668]. 
=============================================
[2019-04-03 22:00:25,915] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5000, global step 80710: loss -1.2914
[2019-04-03 22:00:25,917] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5000, global step 80710: learning rate 0.0001
[2019-04-03 22:00:25,942] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5000, global step 80710: loss 0.1531
[2019-04-03 22:00:26,014] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5000, global step 80710: learning rate 0.0001
[2019-04-03 22:00:26,223] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3982670e-08 8.7446102e-04 6.7486191e-08 1.9572228e-03 8.8313604e-03
 1.6967127e-07 9.8833662e-01], sum to 1.0000
[2019-04-03 22:00:26,224] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0116
[2019-04-03 22:00:26,237] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 72.33333333333334, 0.0, 0.0, 26.0, 23.73353029670082, -0.01534898816244457, 0.0, 1.0, 40256.92130183658], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3046800.0000, 
sim time next is 3047400.0000, 
raw observation next is [-6.0, 73.5, 0.0, 0.0, 26.0, 23.70715570182383, -0.01997213569679513, 0.0, 1.0, 40308.97534106412], 
processed observation next is [0.0, 0.2608695652173913, 0.296398891966759, 0.735, 0.0, 0.0, 0.6666666666666666, 0.47559630848531914, 0.4933426214344016, 0.0, 1.0, 0.19194750162411486], 
reward next is 0.8081, 
noisyNet noise sample is [array([-0.32873872], dtype=float32), 0.18028028]. 
=============================================
[2019-04-03 22:00:26,445] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5000, global step 80822: loss 0.1538
[2019-04-03 22:00:26,466] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5000, global step 80822: learning rate 0.0001
[2019-04-03 22:00:27,578] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5000, global step 81104: loss 0.1979
[2019-04-03 22:00:27,579] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5000, global step 81104: learning rate 0.0001
[2019-04-03 22:00:28,740] A3C_AGENT_WORKER-Thread-10 INFO:Local step 5000, global step 81371: loss 0.2775
[2019-04-03 22:00:28,743] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 5000, global step 81371: learning rate 0.0001
[2019-04-03 22:00:29,559] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [9.6030361e-10 2.9563362e-05 2.3261710e-09 2.6002823e-04 8.2427950e-04
 7.0247164e-09 9.9888617e-01], sum to 1.0000
[2019-04-03 22:00:29,559] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4955
[2019-04-03 22:00:29,616] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 54.0, 102.5, 697.0, 26.0, 25.24264193947732, 0.3175440634906181, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3060000.0000, 
sim time next is 3060600.0000, 
raw observation next is [-4.0, 54.0, 103.6666666666667, 717.6666666666667, 26.0, 25.20023536099408, 0.3178828279636817, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.3518005540166205, 0.54, 0.34555555555555567, 0.7930018416206263, 0.6666666666666666, 0.6000196134161732, 0.6059609426545606, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1767713], dtype=float32), -1.5228125]. 
=============================================
[2019-04-03 22:00:33,155] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.04773294e-10 3.14449753e-05 6.59276411e-10 1.18182506e-04
 2.34107720e-03 8.87383611e-10 9.97509360e-01], sum to 1.0000
[2019-04-03 22:00:33,155] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2928
[2019-04-03 22:00:33,198] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.5, 100.0, 0.0, 0.0, 26.0, 25.36157446763756, 0.3415331958092201, 0.0, 1.0, 48787.61044959391], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3105000.0000, 
sim time next is 3105600.0000, 
raw observation next is [-0.3333333333333334, 100.0, 0.0, 0.0, 26.0, 25.39681327088925, 0.346671930546594, 0.0, 1.0, 30949.25314093123], 
processed observation next is [0.0, 0.9565217391304348, 0.4533702677747, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6164011059074376, 0.615557310182198, 0.0, 1.0, 0.14737739590919632], 
reward next is 0.8526, 
noisyNet noise sample is [array([1.8301619], dtype=float32), -0.81209934]. 
=============================================
[2019-04-03 22:00:40,246] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.3688444e-13 1.8527103e-07 4.1436216e-12 2.4917383e-06 4.8107468e-05
 6.4400100e-12 9.9994922e-01], sum to 1.0000
[2019-04-03 22:00:40,246] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8060
[2019-04-03 22:00:40,300] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.833333333333334, 100.0, 88.33333333333334, 477.0, 26.0, 26.07389162331545, 0.5029232584740516, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3142200.0000, 
sim time next is 3142800.0000, 
raw observation next is [7.0, 100.0, 91.0, 519.5, 26.0, 26.21322687214066, 0.5176531774067985, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.6565096952908588, 1.0, 0.30333333333333334, 0.5740331491712707, 0.6666666666666666, 0.6844355726783883, 0.6725510591355995, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.25388846], dtype=float32), -0.57077295]. 
=============================================
[2019-04-03 22:00:42,961] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [4.2295282e-11 4.0455980e-05 1.6745036e-09 1.8937375e-05 1.2448516e-04
 3.8385792e-10 9.9981624e-01], sum to 1.0000
[2019-04-03 22:00:42,962] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.4428
[2019-04-03 22:00:42,980] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.499773263622, 0.5488533285574565, 0.0, 1.0, 36777.34090674205], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3208800.0000, 
sim time next is 3209400.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.54497960224998, 0.5478956903448484, 0.0, 1.0, 18745.08055169804], 
processed observation next is [1.0, 0.13043478260869565, 0.4349030470914128, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6287483001874984, 0.6826318967816162, 0.0, 1.0, 0.08926228834141924], 
reward next is 0.9107, 
noisyNet noise sample is [array([-1.3980169], dtype=float32), -1.460611]. 
=============================================
[2019-04-03 22:00:43,919] A3C_AGENT_WORKER-Thread-7 INFO:Local step 5500, global step 86091: loss 0.0171
[2019-04-03 22:00:43,919] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 5500, global step 86091: learning rate 0.0001
[2019-04-03 22:00:44,674] A3C_AGENT_WORKER-Thread-6 INFO:Local step 5500, global step 86360: loss 0.0037
[2019-04-03 22:00:44,676] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 5500, global step 86360: learning rate 0.0001
[2019-04-03 22:00:45,234] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5500, global step 86534: loss 0.0053
[2019-04-03 22:00:45,265] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5500, global step 86534: learning rate 0.0001
[2019-04-03 22:00:47,466] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5500, global step 87215: loss 0.0063
[2019-04-03 22:00:47,467] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5500, global step 87215: learning rate 0.0001
[2019-04-03 22:00:48,113] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5500, global step 87443: loss 0.0104
[2019-04-03 22:00:48,113] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5500, global step 87443: learning rate 0.0001
[2019-04-03 22:00:48,558] A3C_AGENT_WORKER-Thread-5 INFO:Local step 5500, global step 87587: loss 0.0013
[2019-04-03 22:00:48,558] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 5500, global step 87587: learning rate 0.0001
[2019-04-03 22:00:48,871] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5500, global step 87699: loss 0.0034
[2019-04-03 22:00:48,872] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5500, global step 87699: learning rate 0.0001
[2019-04-03 22:00:49,887] A3C_AGENT_WORKER-Thread-4 INFO:Local step 5500, global step 88009: loss 0.0088
[2019-04-03 22:00:49,888] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 5500, global step 88009: learning rate 0.0001
[2019-04-03 22:00:50,002] A3C_AGENT_WORKER-Thread-8 INFO:Local step 5500, global step 88047: loss 0.0046
[2019-04-03 22:00:50,003] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 5500, global step 88047: learning rate 0.0001
[2019-04-03 22:00:50,820] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5500, global step 88309: loss 0.0095
[2019-04-03 22:00:50,821] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5500, global step 88309: learning rate 0.0001
[2019-04-03 22:00:50,862] A3C_AGENT_WORKER-Thread-9 INFO:Local step 5500, global step 88323: loss 0.0047
[2019-04-03 22:00:50,867] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 5500, global step 88323: learning rate 0.0001
[2019-04-03 22:00:51,416] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5500, global step 88511: loss 0.0147
[2019-04-03 22:00:51,417] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5500, global step 88511: learning rate 0.0001
[2019-04-03 22:00:52,002] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5500, global step 88716: loss 0.0055
[2019-04-03 22:00:52,004] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5500, global step 88716: learning rate 0.0001
[2019-04-03 22:00:52,635] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5500, global step 88907: loss 0.0071
[2019-04-03 22:00:52,636] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5500, global step 88907: learning rate 0.0001
[2019-04-03 22:00:53,079] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5500, global step 89053: loss 0.0101
[2019-04-03 22:00:53,081] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5500, global step 89053: learning rate 0.0001
[2019-04-03 22:00:54,695] A3C_AGENT_WORKER-Thread-10 INFO:Local step 5500, global step 89551: loss 0.0396
[2019-04-03 22:00:54,710] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 5500, global step 89551: learning rate 0.0001
[2019-04-03 22:00:55,136] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.9412743e-10 2.3331080e-05 7.9797752e-10 2.4133142e-04 2.2769012e-04
 2.7293431e-10 9.9950767e-01], sum to 1.0000
[2019-04-03 22:00:55,137] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7490
[2019-04-03 22:00:55,191] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.5, 48.0, 87.0, 674.0, 26.0, 26.70627341513327, 0.6046629561827696, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3339000.0000, 
sim time next is 3339600.0000, 
raw observation next is [-2.333333333333333, 47.33333333333333, 82.5, 645.1666666666667, 26.0, 26.0538704899812, 0.6325342196124738, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.3979686057248385, 0.4733333333333333, 0.275, 0.7128913443830571, 0.6666666666666666, 0.6711558741651, 0.7108447398708245, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.29045907], dtype=float32), -0.04359956]. 
=============================================
[2019-04-03 22:00:55,277] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.8776766e-10 3.9303002e-05 2.9974240e-10 1.3115176e-04 2.8797926e-04
 5.3266358e-10 9.9954152e-01], sum to 1.0000
[2019-04-03 22:00:55,277] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2384
[2019-04-03 22:00:55,309] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.666666666666667, 68.0, 109.8333333333333, 719.1666666666667, 26.0, 26.31540119310965, 0.5701598932122728, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3320400.0000, 
sim time next is 3321000.0000, 
raw observation next is [-7.5, 67.0, 111.0, 740.0, 26.0, 26.32762022032994, 0.5766294495061444, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.2548476454293629, 0.67, 0.37, 0.8176795580110497, 0.6666666666666666, 0.6939683516941617, 0.6922098165020482, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.365777], dtype=float32), 0.35418746]. 
=============================================
[2019-04-03 22:00:55,318] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[86.02207]
 [86.11234]
 [86.29052]
 [86.29841]
 [86.19036]], R is [[85.98202515]
 [86.12220764]
 [86.26098633]
 [86.39837646]
 [86.53439331]].
[2019-04-03 22:00:56,669] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.3414891e-11 1.0960712e-05 2.3420171e-10 5.2555674e-06 4.5028815e-04
 1.5688471e-11 9.9953353e-01], sum to 1.0000
[2019-04-03 22:00:56,690] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2568
[2019-04-03 22:00:56,808] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 49.33333333333334, 43.66666666666667, 378.3333333333334, 26.0, 26.53499404322155, 0.6501699207528522, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3343800.0000, 
sim time next is 3344400.0000, 
raw observation next is [-2.0, 50.0, 35.5, 317.0, 26.0, 26.27075852308992, 0.6276357247293252, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.40720221606648205, 0.5, 0.11833333333333333, 0.35027624309392263, 0.6666666666666666, 0.68922987692416, 0.7092119082431084, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6972879], dtype=float32), 0.20095523]. 
=============================================
[2019-04-03 22:01:00,509] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.4677504e-11 2.9749036e-04 1.3443696e-09 7.2182076e-05 9.4274880e-04
 1.3989021e-09 9.9868757e-01], sum to 1.0000
[2019-04-03 22:01:00,517] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7074
[2019-04-03 22:01:00,579] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.110223024625157e-16, 56.00000000000001, 97.0, 618.6666666666667, 26.0, 26.26090584973278, 0.5426733471029316, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3403200.0000, 
sim time next is 3403800.0000, 
raw observation next is [0.5, 54.0, 99.0, 658.0, 26.0, 26.34304856520561, 0.5625553505450777, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4764542936288089, 0.54, 0.33, 0.7270718232044199, 0.6666666666666666, 0.6952540471004675, 0.6875184501816926, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.86667836], dtype=float32), 0.31237835]. 
=============================================
[2019-04-03 22:01:06,123] A3C_AGENT_WORKER-Thread-7 INFO:Local step 6000, global step 94287: loss 0.0233
[2019-04-03 22:01:06,127] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 6000, global step 94289: learning rate 0.0001
[2019-04-03 22:01:06,224] A3C_AGENT_WORKER-Thread-6 INFO:Local step 6000, global step 94342: loss 0.0169
[2019-04-03 22:01:06,226] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 6000, global step 94342: learning rate 0.0001
[2019-04-03 22:01:07,009] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6000, global step 94705: loss 0.0023
[2019-04-03 22:01:07,010] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6000, global step 94705: learning rate 0.0001
[2019-04-03 22:01:07,583] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6000, global step 94967: loss 0.0000
[2019-04-03 22:01:07,599] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6000, global step 94977: learning rate 0.0001
[2019-04-03 22:01:08,861] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6000, global step 95588: loss 0.0072
[2019-04-03 22:01:08,863] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6000, global step 95588: learning rate 0.0001
[2019-04-03 22:01:08,873] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.2438455e-08 3.5934611e-03 1.2022623e-07 5.2659732e-04 7.7376748e-03
 6.7269490e-08 9.8814207e-01], sum to 1.0000
[2019-04-03 22:01:08,876] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9207
[2019-04-03 22:01:08,887] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.666666666666668, 25.66666666666667, 0.0, 0.0, 26.0, 25.5511002593219, 0.3612247262164186, 0.0, 1.0, 18745.17978307895], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3651600.0000, 
sim time next is 3652200.0000, 
raw observation next is [9.5, 26.0, 0.0, 0.0, 26.0, 25.53540918883996, 0.3563181339503146, 0.0, 1.0, 20387.70307928355], 
processed observation next is [0.0, 0.2608695652173913, 0.7257617728531857, 0.26, 0.0, 0.0, 0.6666666666666666, 0.6279507657366633, 0.6187727113167715, 0.0, 1.0, 0.09708430037754072], 
reward next is 0.9029, 
noisyNet noise sample is [array([1.6403562], dtype=float32), -0.5730477]. 
=============================================
[2019-04-03 22:01:08,980] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6000, global step 95653: loss -0.0007
[2019-04-03 22:01:08,986] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6000, global step 95655: learning rate 0.0001
[2019-04-03 22:01:09,281] A3C_AGENT_WORKER-Thread-5 INFO:Local step 6000, global step 95784: loss 0.0018
[2019-04-03 22:01:09,284] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 6000, global step 95786: learning rate 0.0001
[2019-04-03 22:01:10,011] A3C_AGENT_WORKER-Thread-8 INFO:Local step 6000, global step 96098: loss 0.0073
[2019-04-03 22:01:10,012] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 6000, global step 96098: learning rate 0.0001
[2019-04-03 22:01:10,309] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.6979711e-09 2.5627185e-03 1.5141948e-08 4.2146738e-04 1.2833985e-03
 2.0954818e-08 9.9573237e-01], sum to 1.0000
[2019-04-03 22:01:10,309] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9977
[2019-04-03 22:01:10,340] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.50561489477938, 0.4318808285389823, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3615600.0000, 
sim time next is 3616200.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.54917882427235, 0.4252203654692376, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.4349030470914128, 0.42, 0.0, 0.0, 0.6666666666666666, 0.6290982353560292, 0.6417401218230792, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.23884666], dtype=float32), -0.9642894]. 
=============================================
[2019-04-03 22:01:10,870] A3C_AGENT_WORKER-Thread-9 INFO:Local step 6000, global step 96491: loss -0.7093
[2019-04-03 22:01:10,871] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 6000, global step 96491: learning rate 0.0001
[2019-04-03 22:01:11,012] A3C_AGENT_WORKER-Thread-4 INFO:Local step 6000, global step 96565: loss 0.0259
[2019-04-03 22:01:11,014] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 6000, global step 96566: learning rate 0.0001
[2019-04-03 22:01:11,104] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6000, global step 96611: loss -1.2331
[2019-04-03 22:01:11,106] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6000, global step 96611: learning rate 0.0001
[2019-04-03 22:01:11,503] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6000, global step 96812: loss 0.0057
[2019-04-03 22:01:11,505] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6000, global step 96815: learning rate 0.0001
[2019-04-03 22:01:12,183] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6000, global step 97157: loss 0.0097
[2019-04-03 22:01:12,183] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6000, global step 97157: learning rate 0.0001
[2019-04-03 22:01:12,279] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6000, global step 97218: loss 0.0111
[2019-04-03 22:01:12,280] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6000, global step 97218: learning rate 0.0001
[2019-04-03 22:01:12,496] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6000, global step 97330: loss 0.0248
[2019-04-03 22:01:12,508] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6000, global step 97331: learning rate 0.0001
[2019-04-03 22:01:13,911] A3C_AGENT_WORKER-Thread-10 INFO:Local step 6000, global step 98070: loss 0.1109
[2019-04-03 22:01:13,911] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 6000, global step 98070: learning rate 0.0001
[2019-04-03 22:01:16,293] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.8002975e-10 3.4041066e-05 7.9522083e-10 2.9427445e-04 5.3825235e-04
 5.1370652e-10 9.9913341e-01], sum to 1.0000
[2019-04-03 22:01:16,294] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5917
[2019-04-03 22:01:16,301] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.0, 44.33333333333334, 105.5, 783.0, 26.0, 25.38568942612213, 0.4744536898532328, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3680400.0000, 
sim time next is 3681000.0000, 
raw observation next is [6.0, 45.0, 104.0, 776.0, 26.0, 25.43216353503763, 0.4840871601265515, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.6288088642659281, 0.45, 0.3466666666666667, 0.8574585635359117, 0.6666666666666666, 0.6193469612531359, 0.6613623867088505, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.18401125], dtype=float32), 1.0430648]. 
=============================================
[2019-04-03 22:01:16,305] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[89.687386]
 [89.699776]
 [89.71997 ]
 [89.76353 ]
 [89.85416 ]], R is [[89.75753021]
 [89.85995483]
 [89.96135712]
 [90.06174469]
 [90.16112518]].
[2019-04-03 22:01:16,787] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.2869516e-11 4.0982431e-04 7.0677858e-10 1.7478567e-05 2.4451513e-04
 7.8387852e-10 9.9932814e-01], sum to 1.0000
[2019-04-03 22:01:16,789] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5166
[2019-04-03 22:01:16,810] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 67.0, 0.0, 0.0, 26.0, 25.38746207620942, 0.3899052865941433, 0.0, 1.0, 43644.20503397665], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3712800.0000, 
sim time next is 3713400.0000, 
raw observation next is [-3.0, 68.0, 0.0, 0.0, 26.0, 25.3769960242748, 0.3879512731957052, 0.0, 1.0, 47192.68964014237], 
processed observation next is [0.0, 1.0, 0.3795013850415513, 0.68, 0.0, 0.0, 0.6666666666666666, 0.6147496686895666, 0.6293170910652351, 0.0, 1.0, 0.2247270935244875], 
reward next is 0.7753, 
noisyNet noise sample is [array([-1.8257245], dtype=float32), 1.573065]. 
=============================================
[2019-04-03 22:01:17,528] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-04-03 22:01:17,530] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-03 22:01:17,531] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:01:17,531] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-03 22:01:17,532] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-03 22:01:17,533] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:01:17,535] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:01:17,540] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run2
[2019-04-03 22:01:17,540] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run2
[2019-04-03 22:01:17,567] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run2
[2019-04-03 22:01:58,200] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.20674337], dtype=float32), 0.17511941]
[2019-04-03 22:01:58,201] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-2.775557561562891e-17, 68.5, 0.0, 0.0, 26.0, 25.05114603633005, 0.301761482894924, 0.0, 1.0, 37969.0278484925]
[2019-04-03 22:01:58,201] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-03 22:01:58,201] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [4.2546558e-10 7.0016901e-04 2.8563387e-09 3.1397492e-04 1.1751204e-03
 4.6212869e-09 9.9781078e-01], sampled 0.49261759790201287
[2019-04-03 22:02:53,222] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.20674337], dtype=float32), 0.17511941]
[2019-04-03 22:02:53,222] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [0.879747248, 50.0070141, 86.48930103, 673.3572466, 26.0, 25.17941956260518, 0.3540679933804154, 0.0, 1.0, 0.0]
[2019-04-03 22:02:53,222] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-03 22:02:53,222] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.3915908e-09 2.4126505e-04 2.9069038e-09 3.2093859e-04 1.6932940e-03
 1.8290791e-09 9.9774444e-01], sampled 0.8826710195353874
[2019-04-03 22:03:11,108] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7956 239862923.3492 1605.0219
[2019-04-03 22:03:37,236] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.7902 263384057.0069 1551.8724
[2019-04-03 22:03:40,081] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.6342 275806815.2143 1233.1762
[2019-04-03 22:03:41,127] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 100000, evaluation results [100000.0, 7241.790204729313, 263384057.00685066, 1551.8724049905088, 7353.795603099225, 239862923.34916055, 1605.0219348007852, 7182.634213265249, 275806815.21429336, 1233.1762374343539]
[2019-04-03 22:03:47,559] A3C_AGENT_WORKER-Thread-6 INFO:Local step 6500, global step 102095: loss 0.1155
[2019-04-03 22:03:47,561] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 6500, global step 102095: learning rate 0.0001
[2019-04-03 22:03:47,911] A3C_AGENT_WORKER-Thread-7 INFO:Local step 6500, global step 102217: loss 0.0793
[2019-04-03 22:03:47,912] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 6500, global step 102218: learning rate 0.0001
[2019-04-03 22:03:48,544] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6500, global step 102401: loss 0.1037
[2019-04-03 22:03:48,546] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6500, global step 102401: learning rate 0.0001
[2019-04-03 22:03:48,701] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.4202542e-10 1.7517102e-04 2.2977225e-08 3.0719908e-04 9.9841668e-04
 2.2938430e-08 9.9851912e-01], sum to 1.0000
[2019-04-03 22:03:48,703] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0292
[2019-04-03 22:03:48,716] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.55308843031354, 0.3987438724202033, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3898200.0000, 
sim time next is 3898800.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.39956451122486, 0.3781834128939551, 0.0, 1.0, 89275.63470299296], 
processed observation next is [1.0, 0.13043478260869565, 0.40720221606648205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6166303759354049, 0.6260611376313183, 0.0, 1.0, 0.4251220700142522], 
reward next is 0.5749, 
noisyNet noise sample is [array([1.6277987], dtype=float32), -0.43645126]. 
=============================================
[2019-04-03 22:03:48,804] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6500, global step 102492: loss 0.0946
[2019-04-03 22:03:48,836] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6500, global step 102493: learning rate 0.0001
[2019-04-03 22:03:52,456] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6500, global step 103593: loss 0.1304
[2019-04-03 22:03:52,460] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6500, global step 103593: learning rate 0.0001
[2019-04-03 22:03:52,673] A3C_AGENT_WORKER-Thread-5 INFO:Local step 6500, global step 103662: loss 0.1374
[2019-04-03 22:03:52,674] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 6500, global step 103662: learning rate 0.0001
[2019-04-03 22:03:53,003] A3C_AGENT_WORKER-Thread-8 INFO:Local step 6500, global step 103780: loss 0.1153
[2019-04-03 22:03:53,003] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 6500, global step 103780: learning rate 0.0001
[2019-04-03 22:03:53,279] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6500, global step 103874: loss 0.1180
[2019-04-03 22:03:53,280] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6500, global step 103875: learning rate 0.0001
[2019-04-03 22:03:54,033] A3C_AGENT_WORKER-Thread-9 INFO:Local step 6500, global step 104135: loss 0.1316
[2019-04-03 22:03:54,033] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 6500, global step 104135: learning rate 0.0001
[2019-04-03 22:03:54,514] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6500, global step 104286: loss 0.1430
[2019-04-03 22:03:54,516] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6500, global step 104286: learning rate 0.0001
[2019-04-03 22:03:54,580] A3C_AGENT_WORKER-Thread-4 INFO:Local step 6500, global step 104313: loss 0.1125
[2019-04-03 22:03:54,582] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 6500, global step 104313: learning rate 0.0001
[2019-04-03 22:03:55,078] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6500, global step 104476: loss 1.0642
[2019-04-03 22:03:55,082] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6500, global step 104476: learning rate 0.0001
[2019-04-03 22:03:55,813] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6500, global step 104738: loss 0.1099
[2019-04-03 22:03:55,813] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6500, global step 104738: learning rate 0.0001
[2019-04-03 22:03:57,014] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6500, global step 105093: loss 0.0808
[2019-04-03 22:03:57,027] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6500, global step 105093: learning rate 0.0001
[2019-04-03 22:03:57,147] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6500, global step 105136: loss 0.7678
[2019-04-03 22:03:57,169] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6500, global step 105142: learning rate 0.0001
[2019-04-03 22:03:59,517] A3C_AGENT_WORKER-Thread-10 INFO:Local step 6500, global step 105804: loss 0.0589
[2019-04-03 22:03:59,518] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 6500, global step 105804: learning rate 0.0001
[2019-04-03 22:04:06,942] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.9366924e-10 2.5573456e-05 9.9094999e-10 2.5664651e-04 2.6888883e-04
 2.1187342e-10 9.9944884e-01], sum to 1.0000
[2019-04-03 22:04:06,942] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8869
[2019-04-03 22:04:06,981] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.666666666666666, 31.66666666666666, 117.3333333333333, 839.1666666666667, 26.0, 26.22484852527413, 0.5598719934177322, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4020000.0000, 
sim time next is 4020600.0000, 
raw observation next is [-4.333333333333333, 30.33333333333333, 116.6666666666667, 837.3333333333334, 26.0, 25.91252103218011, 0.5290256711334403, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.342566943674977, 0.3033333333333333, 0.388888888888889, 0.9252302025782689, 0.6666666666666666, 0.6593767526816757, 0.6763418903778134, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8399045], dtype=float32), 0.45005718]. 
=============================================
[2019-04-03 22:04:07,813] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.7116437e-09 5.5707437e-03 5.9275848e-08 1.1650717e-03 5.3067007e-03
 6.4026004e-08 9.8795742e-01], sum to 1.0000
[2019-04-03 22:04:07,823] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0345
[2019-04-03 22:04:07,853] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 38.0, 0.0, 0.0, 26.0, 25.04249686603852, 0.284858269173319, 0.0, 1.0, 40731.10329138637], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4071600.0000, 
sim time next is 4072200.0000, 
raw observation next is [-5.0, 38.5, 0.0, 0.0, 26.0, 25.04526327714862, 0.2812463308696852, 0.0, 1.0, 40678.52027234907], 
processed observation next is [1.0, 0.13043478260869565, 0.32409972299168976, 0.385, 0.0, 0.0, 0.6666666666666666, 0.5871052730957184, 0.5937487769565618, 0.0, 1.0, 0.19370723939213844], 
reward next is 0.8063, 
noisyNet noise sample is [array([-0.75175244], dtype=float32), 0.3079668]. 
=============================================
[2019-04-03 22:04:08,288] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.4066623e-10 1.4641625e-04 1.5358366e-09 2.4953912e-04 7.8900834e-04
 3.4719507e-09 9.9881506e-01], sum to 1.0000
[2019-04-03 22:04:08,288] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1360
[2019-04-03 22:04:08,320] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.166666666666667, 40.16666666666666, 0.0, 0.0, 26.0, 25.35960760673904, 0.4191466188464352, 0.0, 1.0, 51480.76333918254], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4151400.0000, 
sim time next is 4152000.0000, 
raw observation next is [-1.333333333333333, 41.33333333333334, 0.0, 0.0, 26.0, 25.33291582289689, 0.4149989718932308, 0.0, 1.0, 44756.25450635418], 
processed observation next is [0.0, 0.043478260869565216, 0.42566943674976926, 0.41333333333333344, 0.0, 0.0, 0.6666666666666666, 0.6110763185747409, 0.638332990631077, 0.0, 1.0, 0.21312502145882942], 
reward next is 0.7869, 
noisyNet noise sample is [array([1.744721], dtype=float32), 0.14843857]. 
=============================================
[2019-04-03 22:04:08,338] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[81.99666]
 [82.10729]
 [82.11836]
 [82.45531]
 [82.97778]], R is [[81.60313416]
 [81.54195404]
 [81.51805115]
 [81.59931946]
 [81.65745544]].
[2019-04-03 22:04:11,934] A3C_AGENT_WORKER-Thread-6 INFO:Local step 7000, global step 109846: loss 0.1259
[2019-04-03 22:04:11,936] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 7000, global step 109846: learning rate 0.0001
[2019-04-03 22:04:13,054] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7000, global step 110218: loss 0.1450
[2019-04-03 22:04:13,056] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7000, global step 110218: learning rate 0.0001
[2019-04-03 22:04:13,137] A3C_AGENT_WORKER-Thread-7 INFO:Local step 7000, global step 110239: loss 0.1641
[2019-04-03 22:04:13,138] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 7000, global step 110239: learning rate 0.0001
[2019-04-03 22:04:14,414] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7000, global step 110679: loss 0.1094
[2019-04-03 22:04:14,418] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7000, global step 110679: learning rate 0.0001
[2019-04-03 22:04:18,179] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7000, global step 111887: loss 0.1662
[2019-04-03 22:04:18,193] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7000, global step 111893: learning rate 0.0001
[2019-04-03 22:04:18,582] A3C_AGENT_WORKER-Thread-8 INFO:Local step 7000, global step 112000: loss 0.1990
[2019-04-03 22:04:18,591] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 7000, global step 112000: learning rate 0.0001
[2019-04-03 22:04:18,644] A3C_AGENT_WORKER-Thread-5 INFO:Local step 7000, global step 112022: loss 0.1748
[2019-04-03 22:04:18,645] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 7000, global step 112022: learning rate 0.0001
[2019-04-03 22:04:18,820] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7000, global step 112081: loss 0.2079
[2019-04-03 22:04:18,837] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7000, global step 112082: learning rate 0.0001
[2019-04-03 22:04:19,298] A3C_AGENT_WORKER-Thread-9 INFO:Local step 7000, global step 112264: loss 0.2649
[2019-04-03 22:04:19,299] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 7000, global step 112264: learning rate 0.0001
[2019-04-03 22:04:19,566] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7000, global step 112353: loss 0.2727
[2019-04-03 22:04:19,567] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7000, global step 112353: learning rate 0.0001
[2019-04-03 22:04:19,776] A3C_AGENT_WORKER-Thread-4 INFO:Local step 7000, global step 112433: loss 0.2669
[2019-04-03 22:04:19,777] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 7000, global step 112433: learning rate 0.0001
[2019-04-03 22:04:19,810] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7000, global step 112445: loss 0.2001
[2019-04-03 22:04:19,812] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7000, global step 112445: learning rate 0.0001
[2019-04-03 22:04:21,486] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7000, global step 113024: loss 0.2432
[2019-04-03 22:04:21,490] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7000, global step 113025: learning rate 0.0001
[2019-04-03 22:04:21,861] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7000, global step 113186: loss 0.2516
[2019-04-03 22:04:21,862] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7000, global step 113186: learning rate 0.0001
[2019-04-03 22:04:22,233] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.12926815e-08 3.43498634e-03 5.01928810e-08 2.48486921e-03
 1.42822694e-02 5.45717391e-08 9.79797721e-01], sum to 1.0000
[2019-04-03 22:04:22,235] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6357
[2019-04-03 22:04:22,428] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 26.0, 25.35876651397377, 0.3201582309266646, 0.0, 1.0, 69163.77139398515], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4249800.0000, 
sim time next is 4250400.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 26.0, 25.32355858071357, 0.3204419322102182, 0.0, 1.0, 53152.20337123], 
processed observation next is [0.0, 0.17391304347826086, 0.5457063711911359, 0.45, 0.0, 0.0, 0.6666666666666666, 0.6102965483927975, 0.606813977403406, 0.0, 1.0, 0.2531057303391905], 
reward next is 0.7469, 
noisyNet noise sample is [array([-0.9119312], dtype=float32), 0.8026278]. 
=============================================
[2019-04-03 22:04:22,708] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7000, global step 113507: loss 0.1858
[2019-04-03 22:04:22,710] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7000, global step 113508: learning rate 0.0001
[2019-04-03 22:04:25,487] A3C_AGENT_WORKER-Thread-10 INFO:Local step 7000, global step 114440: loss 0.3353
[2019-04-03 22:04:25,491] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 7000, global step 114441: learning rate 0.0001
[2019-04-03 22:04:26,006] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.2752954e-11 1.9732408e-06 9.0625445e-11 7.5936339e-05 1.2186060e-04
 3.7685608e-11 9.9980026e-01], sum to 1.0000
[2019-04-03 22:04:26,006] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0386
[2019-04-03 22:04:26,016] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.0, 36.0, 49.66666666666666, 0.0, 26.0, 27.90900468853622, 1.052158556302801, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4378800.0000, 
sim time next is 4379400.0000, 
raw observation next is [13.0, 36.5, 39.0, 0.0, 26.0, 28.25499075099443, 1.07830264428852, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.8227146814404434, 0.365, 0.13, 0.0, 0.6666666666666666, 0.8545825625828692, 0.8594342147628401, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2289296], dtype=float32), 0.110360585]. 
=============================================
[2019-04-03 22:04:28,658] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.4599861e-11 1.5284761e-05 1.3952485e-10 3.4056080e-05 6.3932472e-05
 1.7643381e-10 9.9988675e-01], sum to 1.0000
[2019-04-03 22:04:28,658] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8687
[2019-04-03 22:04:28,693] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.466666666666667, 75.66666666666667, 0.0, 0.0, 26.0, 25.51303020706951, 0.4002701503412173, 0.0, 1.0, 58520.7963269426], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4318800.0000, 
sim time next is 4319400.0000, 
raw observation next is [4.483333333333333, 75.83333333333333, 0.0, 0.0, 26.0, 25.47313929112457, 0.4013011820039227, 0.0, 1.0, 66222.36309274769], 
processed observation next is [0.0, 1.0, 0.5867959372114497, 0.7583333333333333, 0.0, 0.0, 0.6666666666666666, 0.6227616075937142, 0.6337670606679743, 0.0, 1.0, 0.3153445861559414], 
reward next is 0.6847, 
noisyNet noise sample is [array([-1.0706701], dtype=float32), -0.39474976]. 
=============================================
[2019-04-03 22:04:28,996] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.4667111e-12 2.3637608e-06 1.4671547e-10 1.8348317e-05 6.0498740e-05
 3.3329006e-11 9.9991870e-01], sum to 1.0000
[2019-04-03 22:04:28,999] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6259
[2019-04-03 22:04:29,015] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.65, 61.83333333333334, 0.0, 0.0, 26.0, 25.94296269554719, 0.6199476950015744, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4402200.0000, 
sim time next is 4402800.0000, 
raw observation next is [8.5, 62.0, 0.0, 0.0, 26.0, 25.85675504078742, 0.6024882910131673, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.698060941828255, 0.62, 0.0, 0.0, 0.6666666666666666, 0.654729586732285, 0.7008294303377225, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.33980876], dtype=float32), -1.8605609]. 
=============================================
[2019-04-03 22:04:30,579] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [8.7162170e-12 1.8593720e-06 7.3806707e-12 3.3928627e-05 2.9349203e-05
 4.8528048e-12 9.9993491e-01], sum to 1.0000
[2019-04-03 22:04:30,579] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.0301
[2019-04-03 22:04:30,608] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.66666666666667, 28.83333333333334, 117.0, 849.3333333333334, 26.0, 26.65916462575019, 0.8709173970385912, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4366200.0000, 
sim time next is 4366800.0000, 
raw observation next is [14.6, 29.0, 116.5, 847.5, 26.0, 27.12831822885573, 0.920304773554478, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.8670360110803325, 0.29, 0.3883333333333333, 0.93646408839779, 0.6666666666666666, 0.7606931857379774, 0.8067682578514926, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.347751], dtype=float32), -0.89278024]. 
=============================================
[2019-04-03 22:04:35,163] A3C_AGENT_WORKER-Thread-6 INFO:Local step 7500, global step 117880: loss 0.7430
[2019-04-03 22:04:35,163] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 7500, global step 117880: learning rate 0.0001
[2019-04-03 22:04:35,251] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7500, global step 117913: loss 0.7142
[2019-04-03 22:04:35,252] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7500, global step 117913: learning rate 0.0001
[2019-04-03 22:04:35,765] A3C_AGENT_WORKER-Thread-7 INFO:Local step 7500, global step 118099: loss 0.7954
[2019-04-03 22:04:35,765] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 7500, global step 118099: learning rate 0.0001
[2019-04-03 22:04:36,547] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.0449891e-11 1.9521673e-05 5.1944099e-10 4.0224080e-05 1.0303159e-04
 2.2718724e-10 9.9983728e-01], sum to 1.0000
[2019-04-03 22:04:36,549] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6512
[2019-04-03 22:04:36,566] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 73.0, 0.0, 0.0, 26.0, 25.38996922372194, 0.4341119730864333, 0.0, 1.0, 112798.12922161], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4498200.0000, 
sim time next is 4498800.0000, 
raw observation next is [-0.6, 73.0, 0.0, 0.0, 26.0, 25.25928496614999, 0.4387687134486429, 0.0, 1.0, 87011.05573950936], 
processed observation next is [1.0, 0.043478260869565216, 0.44598337950138506, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6049404138458326, 0.6462562378162143, 0.0, 1.0, 0.41433836066433033], 
reward next is 0.5857, 
noisyNet noise sample is [array([-1.7069921], dtype=float32), 0.00088367204]. 
=============================================
[2019-04-03 22:04:36,635] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7500, global step 118422: loss 0.7178
[2019-04-03 22:04:36,660] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7500, global step 118423: learning rate 0.0001
[2019-04-03 22:04:40,038] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.2964802e-10 1.4234200e-04 1.4169276e-09 1.0932533e-04 4.0608441e-04
 2.1504991e-10 9.9934214e-01], sum to 1.0000
[2019-04-03 22:04:40,041] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7050
[2019-04-03 22:04:40,057] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.883333333333333, 80.66666666666667, 80.00000000000001, 154.6666666666667, 26.0, 25.48956545495795, 0.493740319267407, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4435800.0000, 
sim time next is 4436400.0000, 
raw observation next is [1.766666666666667, 81.33333333333334, 100.0, 193.3333333333333, 26.0, 25.49180058434925, 0.5191045215273543, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.5115420129270545, 0.8133333333333335, 0.3333333333333333, 0.21362799263351745, 0.6666666666666666, 0.6243167153624375, 0.6730348405091181, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.79792976], dtype=float32), -1.6092347]. 
=============================================
[2019-04-03 22:04:41,049] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7500, global step 119789: loss 1.0064
[2019-04-03 22:04:41,050] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7500, global step 119789: learning rate 0.0001
[2019-04-03 22:04:41,642] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7500, global step 119944: loss 0.9961
[2019-04-03 22:04:41,642] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7500, global step 119944: learning rate 0.0001
[2019-04-03 22:04:41,704] A3C_AGENT_WORKER-Thread-8 INFO:Local step 7500, global step 119962: loss 8.5760
[2019-04-03 22:04:41,705] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 7500, global step 119962: learning rate 0.0001
[2019-04-03 22:04:42,061] A3C_AGENT_WORKER-Thread-5 INFO:Local step 7500, global step 120062: loss 0.9910
[2019-04-03 22:04:42,062] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 7500, global step 120062: learning rate 0.0001
[2019-04-03 22:04:43,048] A3C_AGENT_WORKER-Thread-9 INFO:Local step 7500, global step 120409: loss 1.0008
[2019-04-03 22:04:43,050] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 7500, global step 120410: learning rate 0.0001
[2019-04-03 22:04:43,373] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7500, global step 120530: loss 0.9442
[2019-04-03 22:04:43,374] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7500, global step 120530: learning rate 0.0001
[2019-04-03 22:04:43,418] A3C_AGENT_WORKER-Thread-4 INFO:Local step 7500, global step 120544: loss 0.9398
[2019-04-03 22:04:43,420] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 7500, global step 120545: learning rate 0.0001
[2019-04-03 22:04:43,667] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7500, global step 120634: loss 0.8934
[2019-04-03 22:04:43,677] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7500, global step 120634: learning rate 0.0001
[2019-04-03 22:04:44,783] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4672872e-09 3.9327452e-03 2.8045601e-08 3.6439912e-03 1.8874423e-03
 1.6733607e-08 9.9053580e-01], sum to 1.0000
[2019-04-03 22:04:44,783] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9855
[2019-04-03 22:04:44,834] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.8666666666666667, 71.0, 0.0, 0.0, 26.0, 25.29154848797237, 0.4083542448904403, 0.0, 1.0, 41284.71107346577], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4512000.0000, 
sim time next is 4512600.0000, 
raw observation next is [-0.9, 71.0, 0.0, 0.0, 26.0, 25.2772795845034, 0.4033232025024243, 0.0, 1.0, 41206.78219669916], 
processed observation next is [1.0, 0.21739130434782608, 0.43767313019390586, 0.71, 0.0, 0.0, 0.6666666666666666, 0.6064399653752833, 0.6344410675008081, 0.0, 1.0, 0.1962227723652341], 
reward next is 0.8038, 
noisyNet noise sample is [array([0.81364566], dtype=float32), -1.0166075]. 
=============================================
[2019-04-03 22:04:45,215] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7500, global step 121043: loss 0.8204
[2019-04-03 22:04:45,216] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7500, global step 121043: learning rate 0.0001
[2019-04-03 22:04:45,729] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7500, global step 121182: loss 0.8287
[2019-04-03 22:04:45,731] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7500, global step 121182: learning rate 0.0001
[2019-04-03 22:04:45,954] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7500, global step 121246: loss 0.8204
[2019-04-03 22:04:45,954] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7500, global step 121246: learning rate 0.0001
[2019-04-03 22:04:48,876] A3C_AGENT_WORKER-Thread-10 INFO:Local step 7500, global step 122153: loss 0.7976
[2019-04-03 22:04:48,912] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 7500, global step 122155: learning rate 0.0001
[2019-04-03 22:04:55,652] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.8002431e-10 2.9072718e-05 4.7799826e-09 9.6699636e-04 9.9533144e-04
 2.6706892e-10 9.9800867e-01], sum to 1.0000
[2019-04-03 22:04:55,665] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1422
[2019-04-03 22:04:55,699] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 25.49910751142904, 0.548817811653853, 0.0, 1.0, 196217.9094192961], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4652400.0000, 
sim time next is 4653000.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 25.44844059390521, 0.5692014818800463, 0.0, 1.0, 197390.8288262744], 
processed observation next is [1.0, 0.8695652173913043, 0.518005540166205, 0.52, 0.0, 0.0, 0.6666666666666666, 0.6207033828254342, 0.6897338272933488, 0.0, 1.0, 0.9399563277441638], 
reward next is 0.0600, 
noisyNet noise sample is [array([0.86564523], dtype=float32), 1.3851882]. 
=============================================
[2019-04-03 22:04:55,724] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[80.21022 ]
 [79.52601 ]
 [79.85331 ]
 [79.97648 ]
 [80.228836]], R is [[80.27658844]
 [79.5394516 ]
 [79.62099457]
 [79.62262726]
 [79.82640076]].
[2019-04-03 22:04:58,521] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4485207e-10 6.8831821e-03 1.9123521e-08 7.0328201e-04 1.6943052e-04
 2.5301217e-09 9.9224418e-01], sum to 1.0000
[2019-04-03 22:04:58,521] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9050
[2019-04-03 22:04:58,547] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.58308593968467, 0.418833612933996, 0.0, 1.0, 23949.38070098848], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4683600.0000, 
sim time next is 4684200.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.42786908590456, 0.4097087157165821, 0.0, 1.0, 108328.3381161755], 
processed observation next is [1.0, 0.21739130434782608, 0.4349030470914128, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6189890904920468, 0.6365695719055274, 0.0, 1.0, 0.5158492291246453], 
reward next is 0.4842, 
noisyNet noise sample is [array([0.45363247], dtype=float32), 0.7493146]. 
=============================================
[2019-04-03 22:05:00,966] A3C_AGENT_WORKER-Thread-6 INFO:Local step 8000, global step 126010: loss 0.0310
[2019-04-03 22:05:00,988] A3C_AGENT_WORKER-Thread-7 INFO:Local step 8000, global step 126018: loss 0.0362
[2019-04-03 22:05:00,989] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 8000, global step 126010: learning rate 0.0001
[2019-04-03 22:05:00,991] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 8000, global step 126018: learning rate 0.0001
[2019-04-03 22:05:01,692] A3C_AGENT_WORKER-Thread-17 INFO:Local step 8000, global step 126181: loss 0.0356
[2019-04-03 22:05:01,693] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 8000, global step 126181: learning rate 0.0001
[2019-04-03 22:05:04,781] A3C_AGENT_WORKER-Thread-19 INFO:Local step 8000, global step 126703: loss 0.0220
[2019-04-03 22:05:04,785] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 8000, global step 126703: learning rate 0.0001
[2019-04-03 22:05:13,782] A3C_AGENT_WORKER-Thread-8 INFO:Local step 8000, global step 127838: loss 0.0016
[2019-04-03 22:05:13,783] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 8000, global step 127838: learning rate 0.0001
[2019-04-03 22:05:14,147] A3C_AGENT_WORKER-Thread-20 INFO:Local step 8000, global step 127884: loss 0.0016
[2019-04-03 22:05:14,174] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 8000, global step 127884: learning rate 0.0001
[2019-04-03 22:05:14,975] A3C_AGENT_WORKER-Thread-15 INFO:Local step 8000, global step 128006: loss 0.0018
[2019-04-03 22:05:14,981] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 8000, global step 128006: learning rate 0.0001
[2019-04-03 22:05:15,241] A3C_AGENT_WORKER-Thread-5 INFO:Local step 8000, global step 128048: loss 0.0015
[2019-04-03 22:05:15,242] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 8000, global step 128048: learning rate 0.0001
[2019-04-03 22:05:17,001] A3C_AGENT_WORKER-Thread-9 INFO:Local step 8000, global step 128366: loss 0.0017
[2019-04-03 22:05:17,004] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 8000, global step 128366: learning rate 0.0001
[2019-04-03 22:05:17,832] A3C_AGENT_WORKER-Thread-4 INFO:Local step 8000, global step 128557: loss 0.0014
[2019-04-03 22:05:17,832] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 8000, global step 128557: learning rate 0.0001
[2019-04-03 22:05:18,378] A3C_AGENT_WORKER-Thread-2 INFO:Local step 8000, global step 128668: loss 0.0019
[2019-04-03 22:05:18,381] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 8000, global step 128668: learning rate 0.0001
[2019-04-03 22:05:18,866] A3C_AGENT_WORKER-Thread-18 INFO:Local step 8000, global step 128771: loss 0.0019
[2019-04-03 22:05:18,869] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 8000, global step 128771: learning rate 0.0001
[2019-04-03 22:05:20,573] A3C_AGENT_WORKER-Thread-16 INFO:Local step 8000, global step 129106: loss 0.0022
[2019-04-03 22:05:20,573] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 8000, global step 129106: learning rate 0.0001
[2019-04-03 22:05:21,441] A3C_AGENT_WORKER-Thread-14 INFO:Local step 8000, global step 129265: loss 0.0007
[2019-04-03 22:05:21,456] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 8000, global step 129265: learning rate 0.0001
[2019-04-03 22:05:22,009] A3C_AGENT_WORKER-Thread-3 INFO:Local step 8000, global step 129369: loss 0.0019
[2019-04-03 22:05:22,009] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 8000, global step 129369: learning rate 0.0001
[2019-04-03 22:05:24,728] A3C_AGENT_WORKER-Thread-10 INFO:Local step 8000, global step 129913: loss 0.0048
[2019-04-03 22:05:24,732] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 8000, global step 129913: learning rate 0.0001
[2019-04-03 22:05:40,100] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.5514653e-09 1.2119223e-03 1.5983098e-08 2.0022348e-03 1.1396634e-02
 5.6792331e-09 9.8538929e-01], sum to 1.0000
[2019-04-03 22:05:40,101] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2503
[2019-04-03 22:05:40,116] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 50.0, 0.0, 0.0, 26.0, 25.02468451019073, 0.2279469764963744, 0.0, 1.0, 38655.10529833408], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4948200.0000, 
sim time next is 4948800.0000, 
raw observation next is [-3.0, 50.0, 0.0, 0.0, 26.0, 25.03052050303135, 0.2197574854590133, 0.0, 1.0, 38671.57250173028], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.5, 0.0, 0.0, 0.6666666666666666, 0.5858767085859459, 0.5732524951530045, 0.0, 1.0, 0.18415034524633467], 
reward next is 0.8158, 
noisyNet noise sample is [array([-1.8411827], dtype=float32), 0.34612533]. 
=============================================
[2019-04-03 22:05:40,271] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0397172e-09 2.7967992e-05 2.3586066e-09 5.9789536e-04 2.3094367e-03
 1.0432288e-10 9.9706465e-01], sum to 1.0000
[2019-04-03 22:05:40,271] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6243
[2019-04-03 22:05:40,483] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.333333333333333, 24.66666666666667, 122.8333333333333, 861.6666666666667, 26.0, 27.12834273913565, 0.726567398272214, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4969200.0000, 
sim time next is 4969800.0000, 
raw observation next is [6.5, 24.5, 123.0, 865.0, 26.0, 27.08696216024669, 0.7268544882331619, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.6426592797783934, 0.245, 0.41, 0.9558011049723757, 0.6666666666666666, 0.7572468466872241, 0.7422848294110539, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9113987], dtype=float32), 0.2679757]. 
=============================================
[2019-04-03 22:05:41,132] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.6553606e-09 3.1270392e-04 2.1879469e-09 7.8092478e-02 1.1984573e-03
 6.0120531e-10 9.2039633e-01], sum to 1.0000
[2019-04-03 22:05:41,132] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1559
[2019-04-03 22:05:41,138] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.0, 19.0, 86.0, 665.0, 26.0, 28.56673825462448, 1.135077813530631, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5068800.0000, 
sim time next is 5069400.0000, 
raw observation next is [12.0, 18.66666666666667, 82.66666666666666, 638.3333333333334, 26.0, 28.78468074891978, 1.165242524789347, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.7950138504155125, 0.1866666666666667, 0.2755555555555555, 0.705340699815838, 0.6666666666666666, 0.898723395743315, 0.8884141749297824, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.471105], dtype=float32), -0.1498639]. 
=============================================
[2019-04-03 22:05:41,307] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.0859120e-10 7.4193895e-06 7.4614231e-10 4.2888485e-03 1.1677060e-03
 9.1089393e-11 9.9453604e-01], sum to 1.0000
[2019-04-03 22:05:41,308] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9308
[2019-04-03 22:05:41,320] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.666666666666667, 25.83333333333334, 46.66666666666666, 416.3333333333333, 26.0, 27.30312593321145, 0.8571267513156414, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4986600.0000, 
sim time next is 4987200.0000, 
raw observation next is [7.333333333333334, 25.66666666666667, 40.33333333333333, 360.1666666666666, 26.0, 27.5515456986638, 0.8367428712373237, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.6657433056325024, 0.2566666666666667, 0.13444444444444442, 0.3979742173112338, 0.6666666666666666, 0.7959621415553167, 0.7789142904124412, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2085891], dtype=float32), 1.0592998]. 
=============================================
[2019-04-03 22:05:42,641] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:42,641] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:42,643] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run2
[2019-04-03 22:05:46,021] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:46,028] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:46,069] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run2
[2019-04-03 22:05:46,126] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:46,127] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:46,129] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run2
[2019-04-03 22:05:47,792] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:47,793] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:47,796] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run2
[2019-04-03 22:05:53,849] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:53,850] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:53,851] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run2
[2019-04-03 22:05:54,729] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:54,729] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:54,731] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run2
[2019-04-03 22:05:55,053] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:55,053] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:55,055] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run2
[2019-04-03 22:05:56,205] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:56,205] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:56,207] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run2
[2019-04-03 22:05:58,553] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:58,553] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:58,555] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run2
[2019-04-03 22:05:59,357] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:59,357] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:59,362] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run2
[2019-04-03 22:05:59,485] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:59,485] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:59,487] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run2
[2019-04-03 22:06:00,225] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:06:00,225] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:06:00,227] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run2
[2019-04-03 22:06:00,433] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:06:00,433] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:06:00,435] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run2
[2019-04-03 22:06:00,969] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:06:00,969] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:06:00,971] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run2
[2019-04-03 22:06:01,419] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:06:01,419] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:06:01,430] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run2
[2019-04-03 22:06:02,965] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:06:02,965] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:06:02,967] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run2
[2019-04-03 22:06:05,463] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.5330771e-10 1.5012801e-04 1.7585376e-09 2.6068179e-04 5.6558521e-04
 2.1189894e-09 9.9902356e-01], sum to 1.0000
[2019-04-03 22:06:05,463] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0986
[2019-04-03 22:06:05,508] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.7, 93.0, 0.0, 0.0, 26.0, 21.19215370389179, -0.5799949085836608, 0.0, 1.0, 40400.39719985658], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 23400.0000, 
sim time next is 24000.0000, 
raw observation next is [7.699999999999999, 93.0, 0.0, 0.0, 26.0, 21.21316093290682, -0.574606449456398, 0.0, 1.0, 40382.21349976964], 
processed observation next is [0.0, 0.2608695652173913, 0.6759002770083103, 0.93, 0.0, 0.0, 0.6666666666666666, 0.2677634110755684, 0.3084645168478673, 0.0, 1.0, 0.1922962547608078], 
reward next is 0.8077, 
noisyNet noise sample is [array([-1.5213186], dtype=float32), -1.113887]. 
=============================================
[2019-04-03 22:06:05,512] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[90.36793 ]
 [90.31767 ]
 [90.246666]
 [90.17342 ]
 [90.089   ]], R is [[90.33408356]
 [90.23835754]
 [90.14347839]
 [90.04940796]
 [89.95612335]].
[2019-04-03 22:06:18,743] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.2451041e-10 1.2153180e-05 1.0944258e-08 5.1477872e-04 1.1985715e-03
 9.2019847e-10 9.9827445e-01], sum to 1.0000
[2019-04-03 22:06:18,744] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3839
[2019-04-03 22:06:18,766] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.4, 71.0, 0.0, 0.0, 26.0, 23.8743729121492, 0.04050269585148047, 0.0, 1.0, 44645.25830883632], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 172200.0000, 
sim time next is 172800.0000, 
raw observation next is [-8.4, 71.0, 0.0, 0.0, 26.0, 23.8308937355709, 0.03032683161558022, 0.0, 1.0, 44590.278034094], 
processed observation next is [1.0, 0.0, 0.2299168975069252, 0.71, 0.0, 0.0, 0.6666666666666666, 0.485907811297575, 0.5101089438718601, 0.0, 1.0, 0.21233465730520953], 
reward next is 0.7877, 
noisyNet noise sample is [array([-0.80784154], dtype=float32), 0.5587367]. 
=============================================
[2019-04-03 22:06:29,775] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.5361504e-10 2.7127002e-04 1.9783608e-08 1.7695590e-03 1.1195957e-03
 1.7674067e-09 9.9683952e-01], sum to 1.0000
[2019-04-03 22:06:29,775] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6840
[2019-04-03 22:06:29,833] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.733333333333333, 74.83333333333334, 0.0, 0.0, 26.0, 23.78729652788273, 0.01255251800237771, 0.0, 1.0, 43983.43983548123], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 103800.0000, 
sim time next is 104400.0000, 
raw observation next is [-5.0, 74.0, 0.0, 0.0, 26.0, 23.72958116474457, -0.001227074442508682, 0.0, 1.0, 44091.23331460002], 
processed observation next is [1.0, 0.21739130434782608, 0.32409972299168976, 0.74, 0.0, 0.0, 0.6666666666666666, 0.4774650970620475, 0.49959097518583045, 0.0, 1.0, 0.2099582538790477], 
reward next is 0.7900, 
noisyNet noise sample is [array([-1.2254412], dtype=float32), -2.0170748]. 
=============================================
[2019-04-03 22:06:43,583] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.2071216e-10 3.6748081e-06 1.1772985e-09 6.0590351e-04 9.9813893e-05
 8.3840358e-11 9.9929059e-01], sum to 1.0000
[2019-04-03 22:06:43,583] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4856
[2019-04-03 22:06:43,659] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.5, 42.0, 74.0, 525.6666666666667, 26.0, 25.72602777588968, 0.4947124064919063, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 313800.0000, 
sim time next is 314400.0000, 
raw observation next is [-9.5, 42.0, 72.0, 501.3333333333333, 26.0, 26.07768774578741, 0.5181739161530378, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.1994459833795014, 0.42, 0.24, 0.5539594843462247, 0.6666666666666666, 0.6731406454822843, 0.6727246387176793, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4906169], dtype=float32), 1.9070853]. 
=============================================
[2019-04-03 22:06:43,832] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.7750471e-09 1.9052160e-05 4.5355524e-09 1.1055197e-03 5.1053305e-04
 3.2395450e-10 9.9836487e-01], sum to 1.0000
[2019-04-03 22:06:43,832] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7656
[2019-04-03 22:06:43,921] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.666666666666667, 65.0, 132.3333333333333, 0.0, 26.0, 25.12181138877006, 0.2229636088120984, 1.0, 1.0, 70751.3904542931], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 218400.0000, 
sim time next is 219000.0000, 
raw observation next is [-4.583333333333333, 65.0, 135.6666666666667, 0.0, 26.0, 25.15001017059532, 0.2290969070260579, 1.0, 1.0, 37347.39367718322], 
processed observation next is [1.0, 0.5217391304347826, 0.3356417359187443, 0.65, 0.45222222222222236, 0.0, 0.6666666666666666, 0.5958341808829433, 0.5763656356753527, 1.0, 1.0, 0.1778447317961106], 
reward next is 0.8222, 
noisyNet noise sample is [array([-0.9226406], dtype=float32), -0.22709227]. 
=============================================
[2019-04-03 22:06:43,927] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[76.61644]
 [76.57017]
 [76.73722]
 [76.92519]
 [76.95478]], R is [[76.54111481]
 [76.43878937]
 [76.46391296]
 [76.57187653]
 [76.55177307]].
[2019-04-03 22:06:53,033] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.4598413e-10 3.8969167e-05 2.6722415e-09 7.3896757e-05 1.0578865e-04
 6.2174477e-10 9.9978131e-01], sum to 1.0000
[2019-04-03 22:06:53,033] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3094
[2019-04-03 22:06:53,253] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.8, 70.0, 15.0, 205.5, 26.0, 24.41514897886819, 0.1185951097904449, 1.0, 1.0, 93587.46375274497], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 288000.0000, 
sim time next is 288600.0000, 
raw observation next is [-12.71666666666667, 69.5, 20.0, 265.6666666666667, 26.0, 24.66466479957528, 0.1783145451922401, 1.0, 1.0, 86633.082336185], 
processed observation next is [1.0, 0.34782608695652173, 0.1103416435826407, 0.695, 0.06666666666666667, 0.29355432780847146, 0.6666666666666666, 0.55538873329794, 0.5594381817307467, 1.0, 1.0, 0.4125384873151667], 
reward next is 0.5875, 
noisyNet noise sample is [array([0.9848882], dtype=float32), -0.09135237]. 
=============================================
[2019-04-03 22:06:57,380] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.8768005e-09 1.8826621e-03 2.8250420e-07 3.8080614e-03 2.4974982e-03
 1.7452905e-08 9.9181139e-01], sum to 1.0000
[2019-04-03 22:06:57,380] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9191
[2019-04-03 22:06:57,397] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.0, 51.0, 0.0, 0.0, 26.0, 23.02708203501658, -0.210894639354243, 0.0, 1.0, 46193.30568350979], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 445200.0000, 
sim time next is 445800.0000, 
raw observation next is [-11.1, 51.5, 0.0, 0.0, 26.0, 22.97454222382104, -0.224726834044513, 0.0, 1.0, 46294.59259444171], 
processed observation next is [1.0, 0.13043478260869565, 0.1551246537396122, 0.515, 0.0, 0.0, 0.6666666666666666, 0.41454518531841994, 0.42509105531849567, 0.0, 1.0, 0.2204504409259129], 
reward next is 0.7795, 
noisyNet noise sample is [array([-0.38780233], dtype=float32), 0.94630355]. 
=============================================
[2019-04-03 22:07:03,838] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.5506220e-11 8.7364951e-06 2.5989627e-10 7.3818475e-05 1.9193118e-05
 4.6236327e-11 9.9989820e-01], sum to 1.0000
[2019-04-03 22:07:03,840] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9375
[2019-04-03 22:07:03,857] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-13.1, 79.5, 0.0, 0.0, 26.0, 24.43409989114882, 0.1530192062437922, 0.0, 1.0, 47550.24547310801], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 336600.0000, 
sim time next is 337200.0000, 
raw observation next is [-13.2, 80.33333333333333, 0.0, 0.0, 26.0, 24.34969245520572, 0.1392303310622896, 0.0, 1.0, 47560.43384245523], 
processed observation next is [1.0, 0.9130434782608695, 0.09695290858725764, 0.8033333333333332, 0.0, 0.0, 0.6666666666666666, 0.52914103793381, 0.5464101103540965, 0.0, 1.0, 0.22647825639264396], 
reward next is 0.7735, 
noisyNet noise sample is [array([-0.46240026], dtype=float32), -0.94263816]. 
=============================================
[2019-04-03 22:07:04,108] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.0728021e-10 4.4452272e-06 1.5715631e-09 3.0435031e-05 1.3158753e-04
 1.4343095e-10 9.9983346e-01], sum to 1.0000
[2019-04-03 22:07:04,108] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8320
[2019-04-03 22:07:04,218] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.9, 59.00000000000001, 0.0, 0.0, 26.0, 25.70411482377166, 0.3761615058152095, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 325200.0000, 
sim time next is 325800.0000, 
raw observation next is [-12.0, 60.0, 0.0, 0.0, 26.0, 25.54318090564127, 0.3521450703563758, 1.0, 1.0, 104861.268217479], 
processed observation next is [1.0, 0.782608695652174, 0.13019390581717452, 0.6, 0.0, 0.0, 0.6666666666666666, 0.6285984088034393, 0.617381690118792, 1.0, 1.0, 0.4993393724641857], 
reward next is 0.5007, 
noisyNet noise sample is [array([-0.56465966], dtype=float32), 1.003047]. 
=============================================
[2019-04-03 22:07:05,755] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.2215101e-10 1.4209466e-04 3.9327066e-09 1.7488179e-04 3.7966695e-04
 4.1330586e-10 9.9930334e-01], sum to 1.0000
[2019-04-03 22:07:05,755] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8153
[2019-04-03 22:07:06,036] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-13.9, 68.0, 0.0, 0.0, 26.0, 23.72635098491163, -0.009128637589630348, 0.0, 1.0, 47184.94092371457], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 343800.0000, 
sim time next is 344400.0000, 
raw observation next is [-13.9, 67.33333333333334, 0.0, 0.0, 26.0, 23.65919300768117, -0.01447291577866559, 0.0, 1.0, 47224.31904413381], 
processed observation next is [1.0, 1.0, 0.07756232686980608, 0.6733333333333335, 0.0, 0.0, 0.6666666666666666, 0.4715994173067643, 0.4951756947404448, 0.0, 1.0, 0.2248777097339705], 
reward next is 0.7751, 
noisyNet noise sample is [array([-0.69134027], dtype=float32), -0.7748571]. 
=============================================
[2019-04-03 22:07:12,772] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.3210317e-09 7.6671444e-02 3.1323790e-08 1.8446088e-03 1.3728170e-02
 9.6346104e-09 9.0775573e-01], sum to 1.0000
[2019-04-03 22:07:12,772] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8816
[2019-04-03 22:07:12,894] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.5, 87.66666666666666, 121.6666666666667, 115.6666666666667, 26.0, 24.83131676880189, 0.2522061832559656, 0.0, 1.0, 63679.6801270498], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 553800.0000, 
sim time next is 554400.0000, 
raw observation next is [-0.6, 87.0, 110.5, 122.0, 26.0, 24.80459707971662, 0.2552938985192064, 0.0, 1.0, 70276.23744532927], 
processed observation next is [0.0, 0.43478260869565216, 0.44598337950138506, 0.87, 0.36833333333333335, 0.13480662983425415, 0.6666666666666666, 0.5670497566430516, 0.5850979661730688, 0.0, 1.0, 0.3346487497396632], 
reward next is 0.6654, 
noisyNet noise sample is [array([0.8770781], dtype=float32), -1.2753023]. 
=============================================
[2019-04-03 22:07:15,421] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0048126e-09 2.9971442e-04 3.6816910e-08 1.8336367e-03 1.5894551e-03
 3.6311765e-09 9.9627715e-01], sum to 1.0000
[2019-04-03 22:07:15,421] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6736
[2019-04-03 22:07:15,477] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.266666666666667, 37.66666666666667, 19.16666666666667, 0.0, 26.0, 24.83988940285458, 0.1410473673525513, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 462000.0000, 
sim time next is 462600.0000, 
raw observation next is [-7.0, 36.5, 23.0, 0.0, 26.0, 25.15169825162273, 0.1718135281529992, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.2686980609418283, 0.365, 0.07666666666666666, 0.0, 0.6666666666666666, 0.5959748543018941, 0.5572711760509997, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.02923788], dtype=float32), 0.7328665]. 
=============================================
[2019-04-03 22:07:24,181] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.7170657e-12 5.3039275e-05 3.4456499e-10 1.0921927e-05 1.7837925e-04
 8.4206028e-12 9.9975759e-01], sum to 1.0000
[2019-04-03 22:07:24,181] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1936
[2019-04-03 22:07:24,241] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.55, 96.5, 0.0, 0.0, 26.0, 24.83403332451908, 0.2344556379286678, 0.0, 1.0, 40012.19174868377], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 516600.0000, 
sim time next is 517200.0000, 
raw observation next is [3.633333333333333, 96.66666666666666, 0.0, 0.0, 26.0, 24.84276599367649, 0.2405629281880052, 0.0, 1.0, 39923.88670582083], 
processed observation next is [1.0, 1.0, 0.5632502308402586, 0.9666666666666666, 0.0, 0.0, 0.6666666666666666, 0.5702304994730408, 0.5801876427293351, 0.0, 1.0, 0.1901137462181944], 
reward next is 0.8099, 
noisyNet noise sample is [array([-1.6833549], dtype=float32), 0.7412499]. 
=============================================
[2019-04-03 22:07:31,214] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.8139580e-10 3.6143744e-04 7.7194706e-10 2.9115263e-05 5.0529261e-04
 3.7325931e-10 9.9910408e-01], sum to 1.0000
[2019-04-03 22:07:31,234] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8068
[2019-04-03 22:07:31,290] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.9333333333333333, 80.66666666666667, 123.1666666666667, 352.5, 26.0, 24.96144397079855, 0.3156118443923779, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 562800.0000, 
sim time next is 563400.0000, 
raw observation next is [-1.0, 80.5, 130.0, 396.0, 26.0, 24.95658355604128, 0.3144380456576992, 0.0, 1.0, 18731.33440420001], 
processed observation next is [0.0, 0.5217391304347826, 0.4349030470914128, 0.805, 0.43333333333333335, 0.4375690607734807, 0.6666666666666666, 0.5797152963367734, 0.6048126818858998, 0.0, 1.0, 0.08919683049619052], 
reward next is 0.9108, 
noisyNet noise sample is [array([-0.0448757], dtype=float32), 0.25044677]. 
=============================================
[2019-04-03 22:07:48,958] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.6260524e-09 1.3792336e-02 2.1688930e-08 1.7857691e-03 1.3383118e-02
 8.0606402e-09 9.7103882e-01], sum to 1.0000
[2019-04-03 22:07:48,961] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6085
[2019-04-03 22:07:48,974] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.3, 72.0, 0.0, 0.0, 26.0, 24.66851019148494, 0.172662614480049, 0.0, 1.0, 38923.86877160888], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 883800.0000, 
sim time next is 884400.0000, 
raw observation next is [-0.2, 72.0, 0.0, 0.0, 26.0, 24.6131477106589, 0.1619039558291192, 0.0, 1.0, 38908.32262860522], 
processed observation next is [1.0, 0.21739130434782608, 0.4570637119113574, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5510956425549084, 0.553967985276373, 0.0, 1.0, 0.18527772680288201], 
reward next is 0.8147, 
noisyNet noise sample is [array([-0.28650647], dtype=float32), -0.12798937]. 
=============================================
[2019-04-03 22:07:51,263] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.2569001e-10 2.2838417e-04 3.4314336e-09 4.8827223e-04 5.7832412e-03
 6.1294164e-10 9.9350005e-01], sum to 1.0000
[2019-04-03 22:07:51,263] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6651
[2019-04-03 22:07:51,311] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.04999999999999999, 53.5, 131.0, 449.0, 26.0, 25.74934088126603, 0.3610844601765453, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 736200.0000, 
sim time next is 736800.0000, 
raw observation next is [0.1333333333333333, 52.33333333333333, 124.0, 503.0, 26.0, 25.6592661048353, 0.3576568126056918, 1.0, 1.0, 35331.13486205231], 
processed observation next is [1.0, 0.5217391304347826, 0.46629732225300097, 0.5233333333333333, 0.41333333333333333, 0.5558011049723757, 0.6666666666666666, 0.6382721754029417, 0.6192189375352306, 1.0, 1.0, 0.16824349934310626], 
reward next is 0.8318, 
noisyNet noise sample is [array([-0.01450555], dtype=float32), -1.4044968]. 
=============================================
[2019-04-03 22:07:53,045] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.6463684e-11 1.5703797e-04 3.1154657e-10 2.6572042e-04 1.1717338e-03
 1.6021596e-11 9.9840552e-01], sum to 1.0000
[2019-04-03 22:07:53,045] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0151
[2019-04-03 22:07:53,162] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.916666666666667, 74.33333333333333, 78.33333333333334, 0.0, 26.0, 25.70373395535353, 0.2910413259584538, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 814200.0000, 
sim time next is 814800.0000, 
raw observation next is [-5.633333333333334, 73.66666666666667, 82.66666666666666, 0.0, 26.0, 25.65312982208356, 0.2894040307400862, 1.0, 1.0, 45500.16678389372], 
processed observation next is [1.0, 0.43478260869565216, 0.30655586334256696, 0.7366666666666667, 0.2755555555555555, 0.0, 0.6666666666666666, 0.6377608185069633, 0.5964680102466954, 1.0, 1.0, 0.2166674608756844], 
reward next is 0.7833, 
noisyNet noise sample is [array([0.3523495], dtype=float32), -0.39171782]. 
=============================================
[2019-04-03 22:07:57,254] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.2694794e-10 7.2037987e-04 3.9673163e-09 5.0585024e-04 1.8735114e-03
 5.6175242e-09 9.9690032e-01], sum to 1.0000
[2019-04-03 22:07:57,255] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7092
[2019-04-03 22:07:57,308] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.7, 73.66666666666666, 10.66666666666666, 0.0, 26.0, 24.79801380084196, 0.2403767352183723, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 805800.0000, 
sim time next is 806400.0000, 
raw observation next is [-6.7, 75.0, 16.0, 0.0, 26.0, 25.22101978421441, 0.2608986173580957, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.2770083102493075, 0.75, 0.05333333333333334, 0.0, 0.6666666666666666, 0.6017516486845341, 0.586966205786032, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.36057198], dtype=float32), -0.24072042]. 
=============================================
[2019-04-03 22:07:59,981] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.6857190e-10 7.3983811e-04 1.0421787e-08 3.4907574e-04 3.5938729e-02
 1.4118271e-09 9.6297234e-01], sum to 1.0000
[2019-04-03 22:07:59,983] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2725
[2019-04-03 22:08:00,005] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 24.5601401555592, 0.1637301616529894, 0.0, 1.0, 38441.52376961637], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 889200.0000, 
sim time next is 889800.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 24.58770374426504, 0.1614066843115627, 0.0, 1.0, 38423.39655862674], 
processed observation next is [1.0, 0.30434782608695654, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5489753120220865, 0.5538022281038543, 0.0, 1.0, 0.1829685550410797], 
reward next is 0.8170, 
noisyNet noise sample is [array([-0.06094564], dtype=float32), -0.3049925]. 
=============================================
[2019-04-03 22:08:07,146] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.0276211e-13 1.0351043e-05 5.6752784e-12 1.2568531e-05 1.9316249e-05
 2.6869636e-13 9.9995780e-01], sum to 1.0000
[2019-04-03 22:08:07,147] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6107
[2019-04-03 22:08:07,164] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.11666666666667, 81.83333333333333, 110.6666666666667, 0.0, 26.0, 26.54769370372154, 0.6813221899892926, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1000200.0000, 
sim time next is 1000800.0000, 
raw observation next is [14.4, 81.0, 106.5, 0.0, 26.0, 26.61576579237198, 0.6912607833356632, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.8614958448753465, 0.81, 0.355, 0.0, 0.6666666666666666, 0.7179804826976651, 0.7304202611118877, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.44699907], dtype=float32), -0.768994]. 
=============================================
[2019-04-03 22:08:09,281] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [7.2893216e-13 6.2314110e-05 1.5811314e-11 4.3246993e-05 1.3352429e-04
 3.4528417e-12 9.9976093e-01], sum to 1.0000
[2019-04-03 22:08:09,285] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7610
[2019-04-03 22:08:09,291] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 81.0, 0.0, 0.0, 26.0, 25.77504638112531, 0.5307345863828744, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1017000.0000, 
sim time next is 1017600.0000, 
raw observation next is [14.4, 81.0, 0.0, 0.0, 26.0, 25.65847252482949, 0.5225062654167406, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.8614958448753465, 0.81, 0.0, 0.0, 0.6666666666666666, 0.6382060437357909, 0.6741687551389135, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.24942584], dtype=float32), 0.65495354]. 
=============================================
[2019-04-03 22:08:22,719] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [3.8070841e-08 3.5397816e-02 2.8095852e-07 4.0232902e-03 9.7975694e-02
 1.2348441e-07 8.6260271e-01], sum to 1.0000
[2019-04-03 22:08:22,726] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.1730
[2019-04-03 22:08:22,732] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.0, 96.0, 0.0, 0.0, 26.0, 23.48267869540337, 0.1451954390579796, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1234800.0000, 
sim time next is 1235400.0000, 
raw observation next is [15.0, 96.0, 0.0, 0.0, 26.0, 23.46230294286677, 0.1450785873792985, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.30434782608695654, 0.8781163434903049, 0.96, 0.0, 0.0, 0.6666666666666666, 0.45519191190556424, 0.5483595291264328, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.39010862], dtype=float32), 2.083147]. 
=============================================
[2019-04-03 22:08:34,436] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5646727e-11 2.5149199e-04 1.2228070e-09 3.4284731e-05 1.7106438e-04
 3.0760822e-10 9.9954319e-01], sum to 1.0000
[2019-04-03 22:08:34,438] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1339
[2019-04-03 22:08:34,477] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.2, 96.0, 0.0, 0.0, 26.0, 25.31727606373381, 0.4543638821029813, 0.0, 1.0, 47911.65532473931], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1483800.0000, 
sim time next is 1484400.0000, 
raw observation next is [2.2, 96.0, 0.0, 0.0, 26.0, 25.34890072115504, 0.454246484321513, 0.0, 1.0, 40834.03964725122], 
processed observation next is [1.0, 0.17391304347826086, 0.5235457063711911, 0.96, 0.0, 0.0, 0.6666666666666666, 0.6124083934295866, 0.6514154947738376, 0.0, 1.0, 0.19444780784405344], 
reward next is 0.8056, 
noisyNet noise sample is [array([2.4556408], dtype=float32), 0.3833095]. 
=============================================
[2019-04-03 22:08:37,776] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.6605523e-11 1.7344725e-04 7.2636996e-10 3.5670837e-05 1.5374344e-04
 2.6975347e-10 9.9963713e-01], sum to 1.0000
[2019-04-03 22:08:37,777] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0537
[2019-04-03 22:08:37,796] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.5, 99.16666666666667, 0.0, 0.0, 26.0, 25.29032960453704, 0.4539178843128271, 0.0, 1.0, 56888.67930540042], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1396200.0000, 
sim time next is 1396800.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.21901016447978, 0.4562260120897386, 0.0, 1.0, 45532.73539683093], 
processed observation next is [1.0, 0.17391304347826086, 0.44598337950138506, 1.0, 0.0, 0.0, 0.6666666666666666, 0.601584180373315, 0.6520753373632462, 0.0, 1.0, 0.21682254950871874], 
reward next is 0.7832, 
noisyNet noise sample is [array([0.42081288], dtype=float32), -0.89833236]. 
=============================================
[2019-04-03 22:08:45,985] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.8111040e-11 9.1188422e-06 1.5515687e-10 9.6100939e-06 1.4376554e-04
 2.1202879e-12 9.9983752e-01], sum to 1.0000
[2019-04-03 22:08:45,985] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5919
[2019-04-03 22:08:46,030] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.31666666666667, 61.83333333333334, 0.0, 0.0, 26.0, 26.56742471213217, 0.7409464464812379, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1620600.0000, 
sim time next is 1621200.0000, 
raw observation next is [10.13333333333333, 62.66666666666667, 0.0, 0.0, 26.0, 26.5120138662639, 0.7318231002005954, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.7433056325023084, 0.6266666666666667, 0.0, 0.0, 0.6666666666666666, 0.7093344888553249, 0.7439410334001985, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.11452939], dtype=float32), -1.4828331]. 
=============================================
[2019-04-03 22:08:56,197] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.6341477e-12 3.1918036e-05 4.3400017e-10 2.6256953e-06 1.5467659e-04
 7.6828830e-12 9.9981076e-01], sum to 1.0000
[2019-04-03 22:08:56,246] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1913
[2019-04-03 22:08:56,269] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.600000000000001, 83.66666666666666, 0.0, 0.0, 26.0, 25.68868479370917, 0.5449829582043788, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1564800.0000, 
sim time next is 1565400.0000, 
raw observation next is [4.5, 84.83333333333334, 0.0, 0.0, 26.0, 25.70167638545807, 0.5338723515199878, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.5872576177285319, 0.8483333333333334, 0.0, 0.0, 0.6666666666666666, 0.6418063654548393, 0.6779574505066627, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0331435], dtype=float32), -1.5146003]. 
=============================================
[2019-04-03 22:09:09,491] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.01414585e-10 1.39726239e-04 1.24751653e-09 1.41717916e-04
 1.03903224e-03 2.80664852e-10 9.98679459e-01], sum to 1.0000
[2019-04-03 22:09:09,498] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8763
[2019-04-03 22:09:09,510] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.083333333333334, 92.83333333333334, 0.0, 0.0, 26.0, 25.61221475803555, 0.5405273200838544, 0.0, 1.0, 18734.67538624313], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1666200.0000, 
sim time next is 1666800.0000, 
raw observation next is [5.0, 92.0, 0.0, 0.0, 26.0, 25.58662480637297, 0.53767898909442, 0.0, 1.0, 28064.02042309564], 
processed observation next is [1.0, 0.30434782608695654, 0.6011080332409973, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6322187338644142, 0.6792263296981401, 0.0, 1.0, 0.13363819249093162], 
reward next is 0.8664, 
noisyNet noise sample is [array([0.00930906], dtype=float32), 1.1818485]. 
=============================================
[2019-04-03 22:09:16,605] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.3346695e-11 1.4081757e-05 7.8410306e-11 5.4589291e-05 5.3767033e-04
 4.8652991e-12 9.9939370e-01], sum to 1.0000
[2019-04-03 22:09:16,606] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7213
[2019-04-03 22:09:16,664] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.366666666666667, 75.33333333333333, 0.0, 0.0, 26.0, 25.30068056117317, 0.6029087173239025, 0.0, 1.0, 197762.5127839971], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1629600.0000, 
sim time next is 1630200.0000, 
raw observation next is [7.283333333333333, 75.66666666666667, 0.0, 0.0, 26.0, 25.32524699418589, 0.6539297109981442, 0.0, 1.0, 193118.1176617036], 
processed observation next is [1.0, 0.8695652173913043, 0.6643582640812559, 0.7566666666666667, 0.0, 0.0, 0.6666666666666666, 0.6104372495154907, 0.7179765703327147, 0.0, 1.0, 0.9196100841033505], 
reward next is 0.0804, 
noisyNet noise sample is [array([-0.8488714], dtype=float32), -0.056451086]. 
=============================================
[2019-04-03 22:09:32,743] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.6212030e-10 7.3471334e-04 7.9149691e-09 1.5589759e-04 3.7086196e-04
 3.0184968e-09 9.9873859e-01], sum to 1.0000
[2019-04-03 22:09:32,743] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4162
[2019-04-03 22:09:32,809] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 83.0, 122.5, 0.0, 26.0, 24.94535073758443, 0.3452374583605304, 0.0, 1.0, 39823.89444697162], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1771200.0000, 
sim time next is 1771800.0000, 
raw observation next is [-2.383333333333333, 83.0, 123.6666666666667, 0.0, 26.0, 24.9419784605958, 0.3456335143114311, 0.0, 1.0, 45468.65082247229], 
processed observation next is [0.0, 0.5217391304347826, 0.3965835641735919, 0.83, 0.4122222222222223, 0.0, 0.6666666666666666, 0.5784982050496499, 0.6152111714371437, 0.0, 1.0, 0.21651738486891567], 
reward next is 0.7835, 
noisyNet noise sample is [array([0.9549694], dtype=float32), -2.0753472]. 
=============================================
[2019-04-03 22:09:46,207] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [7.0271122e-11 3.0695808e-06 3.3328384e-09 2.7032502e-05 4.0329123e-05
 1.0756774e-11 9.9992955e-01], sum to 1.0000
[2019-04-03 22:09:46,207] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.3982
[2019-04-03 22:09:46,237] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.1, 62.0, 112.0, 0.0, 26.0, 25.7690023944222, 0.3420464870424442, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1953000.0000, 
sim time next is 1953600.0000, 
raw observation next is [-3.0, 62.0, 105.6666666666667, 0.0, 26.0, 25.75708213815803, 0.3361530915581085, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3795013850415513, 0.62, 0.3522222222222223, 0.0, 0.6666666666666666, 0.6464235115131691, 0.6120510305193695, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.96884584], dtype=float32), -0.95613474]. 
=============================================
[2019-04-03 22:09:59,648] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.17802616e-11 1.29075925e-05 2.69568812e-10 1.54677819e-05
 1.03355615e-05 2.23549772e-11 9.99961257e-01], sum to 1.0000
[2019-04-03 22:09:59,648] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1508
[2019-04-03 22:09:59,716] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.2, 87.0, 0.0, 0.0, 26.0, 24.34865462303422, 0.150896312037989, 0.0, 1.0, 42007.69989682148], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1990800.0000, 
sim time next is 1991400.0000, 
raw observation next is [-6.100000000000001, 86.33333333333333, 0.0, 0.0, 26.0, 24.31441310417484, 0.1558491147971126, 0.0, 1.0, 41916.4632623335], 
processed observation next is [1.0, 0.043478260869565216, 0.2936288088642659, 0.8633333333333333, 0.0, 0.0, 0.6666666666666666, 0.52620109201457, 0.5519497049323708, 0.0, 1.0, 0.1996022060111119], 
reward next is 0.8004, 
noisyNet noise sample is [array([-0.4608193], dtype=float32), -2.437209]. 
=============================================
[2019-04-03 22:10:29,616] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.5584245e-11 2.8685911e-06 2.6481517e-10 9.3080453e-06 9.1104623e-04
 1.1283042e-11 9.9907684e-01], sum to 1.0000
[2019-04-03 22:10:29,621] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1404
[2019-04-03 22:10:29,641] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 51.0, 241.5, 71.5, 26.0, 25.22074536790879, 0.3205782056164819, 1.0, 1.0, 18696.98945420844], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2293200.0000, 
sim time next is 2293800.0000, 
raw observation next is [-1.516666666666667, 50.0, 234.6666666666667, 70.66666666666667, 26.0, 25.24706516168987, 0.3251476260027613, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.4205909510618652, 0.5, 0.7822222222222224, 0.07808471454880295, 0.6666666666666666, 0.6039220968074893, 0.6083825420009205, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9762222], dtype=float32), -1.3482808]. 
=============================================
[2019-04-03 22:10:29,713] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.5396964e-11 5.2414002e-08 1.4323055e-10 1.9856639e-06 4.4301716e-05
 4.2517287e-12 9.9995363e-01], sum to 1.0000
[2019-04-03 22:10:29,713] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5585
[2019-04-03 22:10:29,743] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 64.0, 0.0, 0.0, 26.0, 24.88373265973871, 0.2947731546875648, 0.0, 1.0, 38538.34105702936], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2332200.0000, 
sim time next is 2332800.0000, 
raw observation next is [-2.3, 65.0, 0.0, 0.0, 26.0, 24.90319579017838, 0.3006188416844375, 0.0, 1.0, 38512.7067778012], 
processed observation next is [0.0, 0.0, 0.3988919667590028, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5752663158481983, 0.6002062805614792, 0.0, 1.0, 0.18339384179905333], 
reward next is 0.8166, 
noisyNet noise sample is [array([-0.8217913], dtype=float32), -0.028985534]. 
=============================================
[2019-04-03 22:10:33,324] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-03 22:10:33,325] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-03 22:10:33,325] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:10:33,327] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-03 22:10:33,328] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:10:33,328] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-03 22:10:33,328] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:10:33,329] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run3
[2019-04-03 22:10:33,349] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run3
[2019-04-03 22:10:33,349] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run3
[2019-04-03 22:12:54,573] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.5219 239920394.3441 1605.1240
[2019-04-03 22:13:23,981] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.7639 263389580.4542 1552.0485
[2019-04-03 22:13:28,064] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-03 22:13:29,099] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 200000, evaluation results [200000.0, 7241.763902599205, 263389580.45416775, 1552.0485098418105, 7353.521931694581, 239920394.3441402, 1605.1240493772043, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-03 22:13:46,183] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2396412e-09 6.5683707e-06 1.1848023e-09 8.6252039e-06 3.7949707e-04
 2.3298921e-10 9.9960524e-01], sum to 1.0000
[2019-04-03 22:13:46,184] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1473
[2019-04-03 22:13:46,229] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.383333333333333, 27.0, 81.0, 800.0, 26.0, 24.96796717476861, 0.2800231656216398, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2470200.0000, 
sim time next is 2470800.0000, 
raw observation next is [2.566666666666667, 27.0, 79.5, 792.0, 26.0, 24.97471083677251, 0.2791678425697103, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.5337026777469991, 0.27, 0.265, 0.8751381215469614, 0.6666666666666666, 0.5812259030643757, 0.5930559475232368, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.15200438], dtype=float32), -1.0708517]. 
=============================================
[2019-04-03 22:13:50,428] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.9222525e-11 7.7595837e-07 1.0817486e-10 9.4629348e-07 2.0888213e-04
 2.3875669e-11 9.9978942e-01], sum to 1.0000
[2019-04-03 22:13:50,430] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8935
[2019-04-03 22:13:50,479] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.583333333333333, 63.0, 0.0, 0.0, 26.0, 24.76994152239203, 0.2476062593194627, 0.0, 1.0, 41906.75236377244], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2592600.0000, 
sim time next is 2593200.0000, 
raw observation next is [-4.666666666666667, 64.0, 0.0, 0.0, 26.0, 24.72750565800479, 0.2384745028455935, 0.0, 1.0, 41913.22258540933], 
processed observation next is [1.0, 0.0, 0.3333333333333333, 0.64, 0.0, 0.0, 0.6666666666666666, 0.5606254715003992, 0.5794915009485312, 0.0, 1.0, 0.19958677421623489], 
reward next is 0.8004, 
noisyNet noise sample is [array([-1.3352516], dtype=float32), -0.5823446]. 
=============================================
[2019-04-03 22:13:52,916] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0949031e-11 5.4816314e-07 5.9969453e-11 4.6948895e-07 1.0458755e-05
 6.1401158e-12 9.9998844e-01], sum to 1.0000
[2019-04-03 22:13:52,916] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2740
[2019-04-03 22:13:52,938] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.733333333333333, 28.66666666666667, 178.8333333333333, 405.3333333333334, 26.0, 25.30308624850745, 0.3148524659078742, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2553600.0000, 
sim time next is 2554200.0000, 
raw observation next is [3.0, 28.0, 171.0, 482.0, 26.0, 25.4183425133729, 0.3428141598073027, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5457063711911359, 0.28, 0.57, 0.532596685082873, 0.6666666666666666, 0.6181952094477415, 0.6142713866024342, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1448046], dtype=float32), 0.25904402]. 
=============================================
[2019-04-03 22:13:56,017] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.7770720e-12 2.2820118e-06 7.3946023e-11 5.3544289e-07 1.1815287e-05
 1.7664913e-11 9.9998534e-01], sum to 1.0000
[2019-04-03 22:13:56,019] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2143
[2019-04-03 22:13:56,053] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 71.0, 0.0, 0.0, 26.0, 24.88613282435814, 0.2471861578937908, 0.0, 1.0, 41686.05936843724], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2597400.0000, 
sim time next is 2598000.0000, 
raw observation next is [-5.0, 72.0, 0.0, 0.0, 26.0, 24.88405404741618, 0.2457707775980502, 0.0, 1.0, 41670.95272255494], 
processed observation next is [1.0, 0.043478260869565216, 0.32409972299168976, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5736711706180149, 0.5819235925326834, 0.0, 1.0, 0.19843310820264257], 
reward next is 0.8016, 
noisyNet noise sample is [array([1.9287618], dtype=float32), -0.83737725]. 
=============================================
[2019-04-03 22:13:56,063] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[84.52456 ]
 [84.42936 ]
 [84.34782 ]
 [84.229836]
 [84.20193 ]], R is [[84.53393555]
 [84.49009705]
 [84.44650269]
 [84.40311432]
 [84.35998535]].
[2019-04-03 22:13:59,118] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2799661e-11 1.3289131e-06 2.9652408e-10 1.8874230e-06 6.9661182e-05
 9.4894839e-12 9.9992716e-01], sum to 1.0000
[2019-04-03 22:13:59,118] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4735
[2019-04-03 22:13:59,267] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.300000000000001, 79.0, 0.0, 0.0, 26.0, 23.82084104630318, 0.06666278058467502, 1.0, 1.0, 202354.9928688726], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2618400.0000, 
sim time next is 2619000.0000, 
raw observation next is [-7.3, 79.0, 0.0, 0.0, 26.0, 24.08338539231539, 0.1731902778785061, 1.0, 1.0, 203354.029469123], 
processed observation next is [1.0, 0.30434782608695654, 0.26038781163434904, 0.79, 0.0, 0.0, 0.6666666666666666, 0.5069487826929491, 0.5577300926261687, 1.0, 1.0, 0.968352521281538], 
reward next is 0.0316, 
noisyNet noise sample is [array([0.22241317], dtype=float32), -1.1417222]. 
=============================================
[2019-04-03 22:13:59,302] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[85.13573 ]
 [83.23333 ]
 [83.38809 ]
 [83.539566]
 [83.71463 ]], R is [[86.17852783]
 [85.35314941]
 [85.2858963 ]
 [85.21962738]
 [85.1543808 ]].
[2019-04-03 22:14:00,610] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1850264e-10 4.8918915e-05 1.2226747e-08 9.5314999e-06 2.2096794e-04
 9.7146358e-10 9.9972051e-01], sum to 1.0000
[2019-04-03 22:14:00,628] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1831
[2019-04-03 22:14:00,698] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-15.16666666666667, 83.0, 0.0, 0.0, 26.0, 23.27262899895617, -0.06865591368462533, 0.0, 1.0, 43825.15433495284], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2697000.0000, 
sim time next is 2697600.0000, 
raw observation next is [-15.33333333333334, 83.0, 0.0, 0.0, 26.0, 23.23865185876365, -0.07443361442917086, 0.0, 1.0, 43693.0803482123], 
processed observation next is [1.0, 0.21739130434782608, 0.03785780240073851, 0.83, 0.0, 0.0, 0.6666666666666666, 0.43655432156363744, 0.4751887951902764, 0.0, 1.0, 0.20806228737243954], 
reward next is 0.7919, 
noisyNet noise sample is [array([1.3525944], dtype=float32), 0.2220403]. 
=============================================
[2019-04-03 22:14:10,000] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.27513902e-10 1.36574417e-05 1.09083875e-09 1.02720905e-05
 1.04529216e-04 3.35346056e-11 9.99871492e-01], sum to 1.0000
[2019-04-03 22:14:10,031] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.9519
[2019-04-03 22:14:10,058] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.3, 26.5, 71.0, 76.0, 26.0, 24.80755228773688, 0.3045079592914465, 1.0, 1.0, 177989.6508815089], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2824200.0000, 
sim time next is 2824800.0000, 
raw observation next is [6.2, 27.0, 60.00000000000001, 71.0, 26.0, 25.09709708912077, 0.3468957978111817, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.6343490304709142, 0.27, 0.2, 0.07845303867403315, 0.6666666666666666, 0.5914247574267307, 0.6156319326037273, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.087851], dtype=float32), 0.29288056]. 
=============================================
[2019-04-03 22:15:01,071] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.03577711e-11 1.77140623e-06 6.79968248e-10 1.74567026e-06
 1.04132669e-05 1.35335285e-11 9.99986053e-01], sum to 1.0000
[2019-04-03 22:15:01,071] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2303
[2019-04-03 22:15:01,107] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.833333333333333, 63.33333333333334, 0.0, 0.0, 26.0, 25.33060608371655, 0.5263182122372924, 0.0, 1.0, 56875.7600934307], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3358200.0000, 
sim time next is 3358800.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.51044493533991, 0.5364345489633927, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6258704112783257, 0.6788115163211309, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.22608255], dtype=float32), -1.6763122]. 
=============================================
[2019-04-03 22:15:02,424] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.5864193e-12 2.2757275e-05 4.8758503e-10 5.5774740e-06 3.2187429e-05
 5.5660140e-11 9.9993944e-01], sum to 1.0000
[2019-04-03 22:15:02,464] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7486
[2019-04-03 22:15:02,490] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 26.0, 25.29442375448688, 0.4756725187153049, 0.0, 1.0, 50771.5163309534], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3279000.0000, 
sim time next is 3279600.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 26.0, 25.31585480705353, 0.4709978013030334, 0.0, 1.0, 45681.87279140203], 
processed observation next is [1.0, 1.0, 0.296398891966759, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6096545672544608, 0.6569992671010111, 0.0, 1.0, 0.21753272757810488], 
reward next is 0.7825, 
noisyNet noise sample is [array([1.6091805], dtype=float32), -0.21281959]. 
=============================================
[2019-04-03 22:15:09,853] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.2811680e-11 6.0526518e-06 5.0785121e-10 3.7704115e-06 4.1067735e-05
 3.3544181e-11 9.9994910e-01], sum to 1.0000
[2019-04-03 22:15:09,853] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8555
[2019-04-03 22:15:09,867] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 71.0, 0.0, 0.0, 26.0, 25.21176364014702, 0.4075222949873701, 0.0, 1.0, 45718.45656211058], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3366000.0000, 
sim time next is 3366600.0000, 
raw observation next is [-5.166666666666667, 72.0, 0.0, 0.0, 26.0, 25.1749578230314, 0.3995833930690086, 0.0, 1.0, 42961.25213688086], 
processed observation next is [1.0, 1.0, 0.31948291782086796, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5979131519192832, 0.6331944643563362, 0.0, 1.0, 0.2045773911280041], 
reward next is 0.7954, 
noisyNet noise sample is [array([-0.38234884], dtype=float32), 0.41561812]. 
=============================================
[2019-04-03 22:15:11,752] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.7242241e-10 5.0736405e-04 1.3123542e-09 3.7848815e-05 9.4005291e-04
 3.3616812e-09 9.9851483e-01], sum to 1.0000
[2019-04-03 22:15:11,753] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7956
[2019-04-03 22:15:11,774] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.333333333333333, 44.66666666666667, 0.0, 0.0, 26.0, 25.44548861641057, 0.3928209453043814, 0.0, 1.0, 56843.31282307065], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3619200.0000, 
sim time next is 3619800.0000, 
raw observation next is [-1.5, 46.0, 0.0, 0.0, 26.0, 25.417736359625, 0.3915067210706158, 0.0, 1.0, 59025.17434843891], 
processed observation next is [0.0, 0.9130434782608695, 0.4210526315789474, 0.46, 0.0, 0.0, 0.6666666666666666, 0.6181446966354166, 0.630502240356872, 0.0, 1.0, 0.28107225880209], 
reward next is 0.7189, 
noisyNet noise sample is [array([0.07605746], dtype=float32), 1.016255]. 
=============================================
[2019-04-03 22:15:12,440] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0851767e-11 2.0690927e-06 3.2695114e-11 1.0510400e-06 2.4114262e-05
 1.4808177e-12 9.9997282e-01], sum to 1.0000
[2019-04-03 22:15:12,440] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6301
[2019-04-03 22:15:12,482] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.5, 48.5, 109.0, 764.0, 26.0, 26.54282976056674, 0.6210439564036273, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3407400.0000, 
sim time next is 3408000.0000, 
raw observation next is [2.666666666666667, 48.66666666666666, 110.0, 770.6666666666667, 26.0, 26.56218586876994, 0.6347510213395395, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.5364727608494922, 0.4866666666666666, 0.36666666666666664, 0.8515653775322285, 0.6666666666666666, 0.7135154890641617, 0.7115836737798465, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.10905172], dtype=float32), 0.1376062]. 
=============================================
[2019-04-03 22:15:12,488] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[87.17318 ]
 [87.0234  ]
 [86.88397 ]
 [86.76624 ]
 [86.721954]], R is [[87.52577972]
 [87.65052032]
 [87.77401733]
 [87.89627838]
 [88.01731873]].
[2019-04-03 22:15:37,177] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.5403667e-10 5.3225493e-04 4.0228748e-10 1.1214983e-06 9.0467103e-05
 1.2700012e-10 9.9937624e-01], sum to 1.0000
[2019-04-03 22:15:37,205] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8191
[2019-04-03 22:15:37,229] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.166666666666667, 51.66666666666667, 0.0, 0.0, 26.0, 25.4077235994349, 0.3842037290571373, 0.0, 1.0, 35312.26329905964], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3622200.0000, 
sim time next is 3622800.0000, 
raw observation next is [-2.333333333333333, 53.33333333333334, 0.0, 0.0, 26.0, 25.38910226501576, 0.3791751830462357, 0.0, 1.0, 46056.21112495223], 
processed observation next is [0.0, 0.9565217391304348, 0.3979686057248385, 0.5333333333333334, 0.0, 0.0, 0.6666666666666666, 0.6157585220846468, 0.6263917276820786, 0.0, 1.0, 0.2193152910712011], 
reward next is 0.7807, 
noisyNet noise sample is [array([-0.47114316], dtype=float32), -1.3907156]. 
=============================================
[2019-04-03 22:15:37,901] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.7964592e-12 6.1371366e-06 1.4971887e-11 1.2460647e-06 4.0511128e-05
 6.1642293e-12 9.9995208e-01], sum to 1.0000
[2019-04-03 22:15:37,903] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6500
[2019-04-03 22:15:37,929] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.333333333333333, 77.0, 101.8333333333333, 690.6666666666666, 26.0, 26.15236911432388, 0.491105311195613, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3750000.0000, 
sim time next is 3750600.0000, 
raw observation next is [-3.166666666666667, 77.0, 103.6666666666667, 706.3333333333333, 26.0, 26.16898794957394, 0.5049300179528609, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.3748845798707295, 0.77, 0.34555555555555567, 0.7804788213627992, 0.6666666666666666, 0.6807489957978282, 0.6683100059842869, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.87266845], dtype=float32), -0.24036764]. 
=============================================
[2019-04-03 22:15:51,660] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.0679524e-11 2.1263044e-05 3.9482512e-10 9.6381973e-07 2.1555254e-05
 2.6956076e-10 9.9995625e-01], sum to 1.0000
[2019-04-03 22:15:51,664] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6959
[2019-04-03 22:15:51,754] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.66666666666667, 61.33333333333333, 62.00000000000001, 296.0000000000001, 26.0, 25.45738976692274, 0.3561941504222581, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4003800.0000, 
sim time next is 4004400.0000, 
raw observation next is [-12.33333333333333, 59.66666666666667, 77.5, 370.0, 26.0, 25.60059835135763, 0.4035815004608259, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.12096029547553101, 0.5966666666666667, 0.25833333333333336, 0.4088397790055249, 0.6666666666666666, 0.6333831959464691, 0.6345271668202753, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.14814413], dtype=float32), 0.5774517]. 
=============================================
[2019-04-03 22:16:07,114] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.3800365e-12 2.1724205e-07 2.9469076e-11 1.6348697e-07 2.5443248e-06
 2.2866503e-12 9.9999714e-01], sum to 1.0000
[2019-04-03 22:16:07,114] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0343
[2019-04-03 22:16:07,149] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.333333333333333, 50.0, 0.0, 0.0, 26.0, 25.38014433594826, 0.5267333337052943, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3868800.0000, 
sim time next is 3869400.0000, 
raw observation next is [1.166666666666667, 50.5, 0.0, 0.0, 26.0, 25.48811187827305, 0.5275001090394112, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.49492151431209613, 0.505, 0.0, 0.0, 0.6666666666666666, 0.6240093231894207, 0.6758333696798037, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.42386267], dtype=float32), -0.44554612]. 
=============================================
[2019-04-03 22:16:08,377] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.9813933e-12 1.2207253e-05 1.3741504e-10 1.3328705e-07 1.2330835e-05
 1.0551114e-11 9.9997532e-01], sum to 1.0000
[2019-04-03 22:16:08,377] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3141
[2019-04-03 22:16:08,550] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.31752638118938, 0.4043392763480445, 0.0, 1.0, 62346.04294055855], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3894600.0000, 
sim time next is 3895200.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.26133268746101, 0.4021228887712961, 0.0, 1.0, 49000.37991185666], 
processed observation next is [1.0, 0.08695652173913043, 0.40720221606648205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6051110572884175, 0.6340409629237653, 0.0, 1.0, 0.23333514243741266], 
reward next is 0.7667, 
noisyNet noise sample is [array([0.2171235], dtype=float32), 1.717481]. 
=============================================
[2019-04-03 22:16:48,640] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.4017232e-14 2.9443638e-07 1.5240111e-12 2.5014707e-08 1.9536293e-07
 1.7144977e-13 9.9999940e-01], sum to 1.0000
[2019-04-03 22:16:48,640] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1250
[2019-04-03 22:16:48,664] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 78.0, 49.0, 0.0, 26.0, 26.02787326175942, 0.5770719498697144, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4465800.0000, 
sim time next is 4466400.0000, 
raw observation next is [0.0, 78.0, 45.0, 9.166666666666664, 26.0, 26.10261001061986, 0.5828266706889906, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.46260387811634357, 0.78, 0.15, 0.010128913443830568, 0.6666666666666666, 0.6752175008849882, 0.6942755568963301, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.76041], dtype=float32), 1.3841473]. 
=============================================
[2019-04-03 22:16:50,420] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.6896935e-10 1.3685464e-04 4.7570559e-10 1.3149487e-06 2.8815144e-04
 4.2388806e-10 9.9957365e-01], sum to 1.0000
[2019-04-03 22:16:50,438] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9563
[2019-04-03 22:16:50,453] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.833333333333333, 38.16666666666667, 129.3333333333333, 542.0, 26.0, 25.35895530066799, 0.4368489390159924, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4204200.0000, 
sim time next is 4204800.0000, 
raw observation next is [3.0, 37.0, 114.0, 544.0, 26.0, 25.35635377426713, 0.4402211406094272, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.5457063711911359, 0.37, 0.38, 0.6011049723756906, 0.6666666666666666, 0.6130294811889275, 0.6467403802031424, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.39932257], dtype=float32), 0.071051486]. 
=============================================
[2019-04-03 22:16:51,670] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.0289239e-12 1.7948379e-06 1.5271199e-11 9.9897250e-08 2.1292144e-06
 3.4927432e-13 9.9999595e-01], sum to 1.0000
[2019-04-03 22:16:51,695] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5672
[2019-04-03 22:16:51,719] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 90.83333333333334, 122.0, 2.0, 26.0, 25.96008306047805, 0.6075250294476696, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4457400.0000, 
sim time next is 4458000.0000, 
raw observation next is [0.0, 89.66666666666667, 103.5, 0.9999999999999998, 26.0, 26.153083770429, 0.6217663403487708, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.46260387811634357, 0.8966666666666667, 0.345, 0.0011049723756906074, 0.6666666666666666, 0.6794236475357499, 0.7072554467829235, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0196289], dtype=float32), -1.7575984]. 
=============================================
[2019-04-03 22:16:51,766] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[91.27533 ]
 [91.4674  ]
 [91.38682 ]
 [90.764336]
 [89.532974]], R is [[90.91486359]
 [91.00571442]
 [91.09565735]
 [90.86146545]
 [90.01824951]].
[2019-04-03 22:16:52,711] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.5608450e-12 5.9093603e-05 3.9398210e-11 2.6693399e-07 4.9615407e-04
 2.0357894e-11 9.9944454e-01], sum to 1.0000
[2019-04-03 22:16:52,711] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0075
[2019-04-03 22:16:52,768] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.1, 68.33333333333334, 76.66666666666667, 409.1666666666667, 26.0, 25.50069213772108, 0.4611797574404797, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4350000.0000, 
sim time next is 4350600.0000, 
raw observation next is [4.65, 65.5, 92.0, 491.0, 26.0, 25.805236788614, 0.5054972833869602, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.5914127423822716, 0.655, 0.30666666666666664, 0.5425414364640884, 0.6666666666666666, 0.6504363990511667, 0.66849909446232, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.35450676], dtype=float32), -0.22929521]. 
=============================================
[2019-04-03 22:16:56,452] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.7563798e-12 7.2189760e-05 5.1527518e-11 3.7807776e-08 1.8750017e-05
 5.5964790e-12 9.9990904e-01], sum to 1.0000
[2019-04-03 22:16:56,452] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5422
[2019-04-03 22:16:56,511] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 73.0, 0.0, 0.0, 26.0, 25.54385416942845, 0.4371174984356787, 0.0, 1.0, 31442.25444549326], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4497600.0000, 
sim time next is 4498200.0000, 
raw observation next is [-0.6, 73.0, 0.0, 0.0, 26.0, 25.39544580692588, 0.4304156998491899, 0.0, 1.0, 114201.3732516114], 
processed observation next is [1.0, 0.043478260869565216, 0.44598337950138506, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6162871505771568, 0.64347189994973, 0.0, 1.0, 0.5438160631029114], 
reward next is 0.4562, 
noisyNet noise sample is [array([-0.72423077], dtype=float32), 1.3456929]. 
=============================================
[2019-04-03 22:17:00,080] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.2807955e-11 1.3405538e-05 1.8081137e-10 5.5673303e-07 1.4441527e-05
 4.8058602e-11 9.9997163e-01], sum to 1.0000
[2019-04-03 22:17:00,080] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4070
[2019-04-03 22:17:00,118] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.033333333333333, 68.16666666666666, 0.0, 0.0, 26.0, 25.01834704004518, 0.2939069652202982, 0.0, 1.0, 52557.72783773782], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4301400.0000, 
sim time next is 4302000.0000, 
raw observation next is [6.0, 69.0, 0.0, 0.0, 26.0, 24.96770276796358, 0.2905106203998265, 0.0, 1.0, 56549.3236518721], 
processed observation next is [0.0, 0.8260869565217391, 0.6288088642659281, 0.69, 0.0, 0.0, 0.6666666666666666, 0.5806418973302984, 0.5968368734666089, 0.0, 1.0, 0.26928249358034334], 
reward next is 0.7307, 
noisyNet noise sample is [array([-0.35153508], dtype=float32), 0.47514778]. 
=============================================
[2019-04-03 22:17:00,169] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[80.21581 ]
 [79.945114]
 [79.879135]
 [80.07745 ]
 [80.403366]], R is [[80.54854584]
 [80.49279022]
 [80.52422333]
 [80.71897888]
 [80.91178894]].
[2019-04-03 22:17:11,304] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.0684090e-12 1.5715533e-04 6.1776459e-11 2.5993452e-07 1.6989616e-05
 4.4454666e-12 9.9982554e-01], sum to 1.0000
[2019-04-03 22:17:11,306] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.2620
[2019-04-03 22:17:11,335] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 92.0, 130.5, 0.9999999999999998, 26.0, 26.37390250007046, 0.5680410835473847, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4700400.0000, 
sim time next is 4701000.0000, 
raw observation next is [0.0, 92.0, 146.0, 2.0, 26.0, 26.40040469175683, 0.5757208662328255, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.46260387811634357, 0.92, 0.4866666666666667, 0.0022099447513812156, 0.6666666666666666, 0.7000337243130691, 0.6919069554109418, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.17878945], dtype=float32), 0.70618844]. 
=============================================
[2019-04-03 22:17:11,339] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[92.265015]
 [92.252754]
 [92.32761 ]
 [92.190636]
 [92.32921 ]], R is [[92.40023804]
 [92.47623444]
 [92.55147552]
 [92.6259613 ]
 [92.6996994 ]].
[2019-04-03 22:17:15,304] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.1460333e-11 5.9373388e-06 1.6622680e-10 1.4495552e-07 3.2311011e-05
 6.9496416e-11 9.9996161e-01], sum to 1.0000
[2019-04-03 22:17:15,305] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0307
[2019-04-03 22:17:15,331] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.5, 73.5, 0.0, 0.0, 26.0, 25.17680853910562, 0.3751510952341134, 0.0, 1.0, 36188.79767894872], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4600200.0000, 
sim time next is 4600800.0000, 
raw observation next is [-2.6, 74.0, 0.0, 0.0, 26.0, 25.20454126345438, 0.3780697949527347, 0.0, 1.0, 36162.77286942682], 
processed observation next is [1.0, 0.2608695652173913, 0.3905817174515236, 0.74, 0.0, 0.0, 0.6666666666666666, 0.6003784386211984, 0.6260232649842449, 0.0, 1.0, 0.1722036803306039], 
reward next is 0.8278, 
noisyNet noise sample is [array([-0.43983167], dtype=float32), -1.580375]. 
=============================================
[2019-04-03 22:17:24,686] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.8065970e-12 1.6033705e-06 4.4136539e-12 3.8051493e-08 3.8174639e-06
 6.5331518e-13 9.9999452e-01], sum to 1.0000
[2019-04-03 22:17:24,689] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1872
[2019-04-03 22:17:24,713] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.5, 78.0, 0.0, 0.0, 26.0, 25.29692011224179, 0.438359369232137, 1.0, 1.0, 25752.18211764261], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4732200.0000, 
sim time next is 4732800.0000, 
raw observation next is [-0.6666666666666666, 78.0, 0.0, 0.0, 26.0, 25.17633733616131, 0.4193752662702163, 1.0, 1.0, 58456.17078819711], 
processed observation next is [1.0, 0.782608695652174, 0.44413665743305636, 0.78, 0.0, 0.0, 0.6666666666666666, 0.5980281113467759, 0.6397917554234055, 1.0, 1.0, 0.27836271803903384], 
reward next is 0.7216, 
noisyNet noise sample is [array([-1.0402035], dtype=float32), 0.878695]. 
=============================================
[2019-04-03 22:17:24,718] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.9218728e-12 2.5486203e-05 2.6739560e-11 6.6410529e-08 2.8189127e-06
 2.1152882e-12 9.9997163e-01], sum to 1.0000
[2019-04-03 22:17:24,722] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5514
[2019-04-03 22:17:24,737] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 80.5, 0.0, 0.0, 26.0, 25.34345750756383, 0.4501961521768711, 0.0, 1.0, 54805.34797477401], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4746600.0000, 
sim time next is 4747200.0000, 
raw observation next is [-3.0, 79.33333333333333, 0.0, 0.0, 26.0, 25.27318331220595, 0.4428964932455903, 0.0, 1.0, 48813.28174194528], 
processed observation next is [1.0, 0.9565217391304348, 0.3795013850415513, 0.7933333333333333, 0.0, 0.0, 0.6666666666666666, 0.606098609350496, 0.6476321644151968, 0.0, 1.0, 0.23244419877116798], 
reward next is 0.7676, 
noisyNet noise sample is [array([0.95406145], dtype=float32), -0.9321044]. 
=============================================
[2019-04-03 22:17:29,714] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.8704469e-12 5.1833584e-05 3.3312311e-11 4.6279286e-08 4.0998912e-06
 8.8528490e-12 9.9994409e-01], sum to 1.0000
[2019-04-03 22:17:29,716] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2140
[2019-04-03 22:17:29,740] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.5, 53.0, 0.0, 0.0, 26.0, 25.40866426579813, 0.4061592362609501, 0.0, 1.0, 46260.42998313937], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4829400.0000, 
sim time next is 4830000.0000, 
raw observation next is [-0.6666666666666666, 53.66666666666667, 0.0, 0.0, 26.0, 25.40624447841639, 0.4036897292734634, 0.0, 1.0, 41882.137371301], 
processed observation next is [0.0, 0.9130434782608695, 0.44413665743305636, 0.5366666666666667, 0.0, 0.0, 0.6666666666666666, 0.6171870398680325, 0.6345632430911544, 0.0, 1.0, 0.19943874938714765], 
reward next is 0.8006, 
noisyNet noise sample is [array([-1.4872329], dtype=float32), 0.7755796]. 
=============================================
[2019-04-03 22:17:29,752] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[82.45324 ]
 [82.460884]
 [82.29566 ]
 [82.06877 ]
 [81.89451 ]], R is [[82.38026428]
 [82.33617401]
 [82.217659  ]
 [82.09796143]
 [82.03230286]].
[2019-04-03 22:17:31,850] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.65653333e-11 7.32696935e-05 3.87415156e-10 5.42687985e-07
 7.89816477e-05 1.27631915e-11 9.99847174e-01], sum to 1.0000
[2019-04-03 22:17:31,851] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6105
[2019-04-03 22:17:31,879] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.333333333333333, 58.66666666666666, 0.0, 0.0, 26.0, 25.24584279355368, 0.34714025647812, 0.0, 1.0, 38023.33394124656], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5035200.0000, 
sim time next is 5035800.0000, 
raw observation next is [-2.666666666666667, 61.83333333333334, 0.0, 0.0, 26.0, 25.26128160551507, 0.3521453270437063, 0.0, 1.0, 37406.24316596101], 
processed observation next is [1.0, 0.2608695652173913, 0.38873499538319484, 0.6183333333333334, 0.0, 0.0, 0.6666666666666666, 0.6051068004595891, 0.6173817756812354, 0.0, 1.0, 0.1781249674569572], 
reward next is 0.8219, 
noisyNet noise sample is [array([-1.9529947], dtype=float32), -0.18336487]. 
=============================================
[2019-04-03 22:17:34,694] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.1641504e-12 6.4275459e-06 1.4048866e-11 4.9773725e-08 2.0131994e-04
 1.6095027e-12 9.9979228e-01], sum to 1.0000
[2019-04-03 22:17:34,697] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1755
[2019-04-03 22:17:34,707] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.0, 17.0, 0.0, 0.0, 26.0, 28.02647262029113, 1.024502676804169, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5079600.0000, 
sim time next is 5080200.0000, 
raw observation next is [10.83333333333333, 17.33333333333334, 0.0, 0.0, 26.0, 27.95625401554602, 1.013285672637775, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.76269621421976, 0.1733333333333334, 0.0, 0.0, 0.6666666666666666, 0.8296878346288349, 0.8377618908792583, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0969547], dtype=float32), -0.73028105]. 
=============================================
[2019-04-03 22:17:36,373] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:17:36,373] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:17:36,374] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run3
[2019-04-03 22:17:36,484] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:17:36,484] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:17:36,534] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run3
[2019-04-03 22:17:37,081] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:17:37,081] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:17:37,083] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run3
[2019-04-03 22:17:39,332] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:17:39,334] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:17:39,365] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run3
[2019-04-03 22:17:44,645] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.5386532e-14 2.5258356e-07 3.9532158e-12 1.9780364e-08 2.3752144e-05
 9.7714664e-13 9.9997604e-01], sum to 1.0000
[2019-04-03 22:17:44,646] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7724
[2019-04-03 22:17:44,663] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.65, 19.16666666666667, 0.0, 0.0, 26.0, 26.7212303878141, 0.7264059261101649, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5091000.0000, 
sim time next is 5091600.0000, 
raw observation next is [8.6, 19.33333333333334, 0.0, 0.0, 26.0, 26.70433751311858, 0.6972942165878857, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.700831024930748, 0.19333333333333338, 0.0, 0.0, 0.6666666666666666, 0.7253614594265484, 0.7324314055292952, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0438787], dtype=float32), -1.1495651]. 
=============================================
[2019-04-03 22:17:45,501] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:17:45,501] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:17:45,504] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run3
[2019-04-03 22:17:45,872] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:17:45,872] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:17:45,882] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run3
[2019-04-03 22:17:45,960] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:17:45,960] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:17:45,963] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run3
[2019-04-03 22:17:45,974] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:17:45,976] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:17:45,988] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run3
[2019-04-03 22:17:47,261] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:17:47,281] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:17:47,283] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run3
[2019-04-03 22:17:47,541] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:17:47,541] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:17:47,543] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run3
[2019-04-03 22:17:48,401] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:17:48,401] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:17:48,403] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run3
[2019-04-03 22:17:52,461] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:17:52,461] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:17:52,463] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run3
[2019-04-03 22:17:53,189] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:17:53,189] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:17:53,191] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run3
[2019-04-03 22:17:54,302] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:17:54,302] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:17:54,304] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run3
[2019-04-03 22:17:54,365] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:17:54,365] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:17:54,367] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run3
[2019-04-03 22:17:56,775] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:17:56,775] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:17:56,799] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run3
[2019-04-03 22:18:03,310] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.2064590e-12 2.9305226e-04 9.5367048e-11 2.9806870e-07 1.1787244e-05
 3.1410825e-11 9.9969482e-01], sum to 1.0000
[2019-04-03 22:18:03,313] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6749
[2019-04-03 22:18:03,335] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.4333333333333333, 95.66666666666666, 0.0, 0.0, 26.0, 24.51594559739407, 0.1924455531638918, 0.0, 1.0, 40206.38593472949], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 80400.0000, 
sim time next is 81000.0000, 
raw observation next is [0.4, 95.5, 0.0, 0.0, 26.0, 24.49101794008482, 0.188008830655655, 0.0, 1.0, 40161.77912175525], 
processed observation next is [0.0, 0.9565217391304348, 0.4736842105263158, 0.955, 0.0, 0.0, 0.6666666666666666, 0.540918161673735, 0.5626696102185517, 0.0, 1.0, 0.19124656724645359], 
reward next is 0.8088, 
noisyNet noise sample is [array([-1.1497953], dtype=float32), 0.64588404]. 
=============================================
[2019-04-03 22:18:03,345] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[86.27911]
 [86.3653 ]
 [86.43992]
 [86.5545 ]
 [86.68189]], R is [[86.1290741 ]
 [86.07632446]
 [86.02384186]
 [85.97156525]
 [85.91944122]].
[2019-04-03 22:18:09,092] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.1761581e-12 7.5531614e-07 5.0907507e-11 5.6552335e-07 3.5826455e-05
 3.0537948e-12 9.9996281e-01], sum to 1.0000
[2019-04-03 22:18:09,093] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0789
[2019-04-03 22:18:09,156] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.700000000000001, 61.0, 145.0, 231.9999999999999, 26.0, 25.92424416315448, 0.4446574751965781, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 137400.0000, 
sim time next is 138000.0000, 
raw observation next is [-6.700000000000001, 61.0, 146.5, 169.0, 26.0, 25.90108012763567, 0.4337984057604154, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.2770083102493075, 0.61, 0.48833333333333334, 0.1867403314917127, 0.6666666666666666, 0.6584233439696391, 0.6445994685868052, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6686051], dtype=float32), -0.3522012]. 
=============================================
[2019-04-03 22:18:09,159] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[77.86421 ]
 [78.50028 ]
 [79.04088 ]
 [79.37279 ]
 [79.692856]], R is [[77.31095886]
 [77.53784943]
 [77.76247406]
 [77.98484802]
 [78.20500183]].
[2019-04-03 22:18:12,601] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [5.3098959e-11 5.0379953e-04 5.7393329e-10 8.0012285e-07 7.1134850e-06
 7.1356171e-11 9.9948823e-01], sum to 1.0000
[2019-04-03 22:18:12,602] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.1572
[2019-04-03 22:18:12,675] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 26.0, 23.50629332475311, -0.03007512574710892, 1.0, 1.0, 158125.3409287521], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 200400.0000, 
sim time next is 201000.0000, 
raw observation next is [-8.9, 78.0, 11.33333333333333, 110.6666666666666, 26.0, 23.9355086424383, 0.06263352026122797, 1.0, 1.0, 109118.5890055595], 
processed observation next is [1.0, 0.30434782608695654, 0.21606648199445982, 0.78, 0.03777777777777777, 0.12228360957642719, 0.6666666666666666, 0.4946257202031917, 0.520877840087076, 1.0, 1.0, 0.5196123285979023], 
reward next is 0.4804, 
noisyNet noise sample is [array([-0.14425816], dtype=float32), -0.44044006]. 
=============================================
[2019-04-03 22:18:12,686] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[73.56697 ]
 [71.10435 ]
 [70.42837 ]
 [67.826065]
 [67.93423 ]], R is [[75.26080322]
 [74.75521851]
 [74.03866577]
 [73.33473969]
 [73.38699341]].
[2019-04-03 22:18:17,919] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.67667381e-11 6.49645517e-05 2.09027434e-10 2.41953927e-07
 1.54181216e-05 1.15721556e-11 9.99919415e-01], sum to 1.0000
[2019-04-03 22:18:17,920] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4315
[2019-04-03 22:18:17,971] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.766666666666667, 63.0, 143.6666666666667, 0.0, 26.0, 25.30132369799749, 0.3540707847525419, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 222000.0000, 
sim time next is 222600.0000, 
raw observation next is [-3.583333333333333, 62.5, 138.3333333333333, 0.0, 26.0, 25.843585592808, 0.4023010156367655, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.36334256694367506, 0.625, 0.46111111111111097, 0.0, 0.6666666666666666, 0.6536321327340001, 0.6341003385455884, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2501827], dtype=float32), -0.43373743]. 
=============================================
[2019-04-03 22:18:19,769] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.2792458e-09 8.6814769e-05 6.8977011e-09 8.1917515e-06 1.8304346e-04
 1.7570067e-09 9.9972194e-01], sum to 1.0000
[2019-04-03 22:18:19,770] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2292
[2019-04-03 22:18:19,809] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-14.91666666666667, 69.0, 0.0, 0.0, 26.0, 23.27685238191511, -0.1310316547095124, 0.0, 1.0, 48201.51666115654], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 352200.0000, 
sim time next is 352800.0000, 
raw observation next is [-15.0, 69.0, 0.0, 0.0, 26.0, 23.26581323464423, -0.1446673006140628, 0.0, 1.0, 48391.01285250529], 
processed observation next is [1.0, 0.08695652173913043, 0.04709141274238226, 0.69, 0.0, 0.0, 0.6666666666666666, 0.4388177695536859, 0.45177756646197903, 0.0, 1.0, 0.2304333945357395], 
reward next is 0.7696, 
noisyNet noise sample is [array([0.16731846], dtype=float32), 0.13735943]. 
=============================================
[2019-04-03 22:18:22,039] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.7959365e-11 3.6030722e-06 2.8735891e-10 5.6060207e-07 1.1205686e-04
 2.5013923e-11 9.9988377e-01], sum to 1.0000
[2019-04-03 22:18:22,052] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2940
[2019-04-03 22:18:22,087] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.399999999999999, 79.5, 0.0, 0.0, 26.0, 24.19656561694077, 0.1000108826217753, 0.0, 1.0, 44356.52889196049], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 258600.0000, 
sim time next is 259200.0000, 
raw observation next is [-4.5, 79.0, 0.0, 0.0, 26.0, 24.15870167770939, 0.09203649788015399, 0.0, 1.0, 44362.86153365797], 
processed observation next is [1.0, 0.0, 0.3379501385041552, 0.79, 0.0, 0.0, 0.6666666666666666, 0.513225139809116, 0.530678832626718, 0.0, 1.0, 0.21125172158884747], 
reward next is 0.7887, 
noisyNet noise sample is [array([-1.0102884], dtype=float32), -1.1121159]. 
=============================================
[2019-04-03 22:18:25,930] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.03690612e-09 4.76845016e-04 1.09902105e-08 9.62821196e-06
 5.55648352e-04 5.99785666e-10 9.98957872e-01], sum to 1.0000
[2019-04-03 22:18:25,930] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9028
[2019-04-03 22:18:25,987] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.4, 69.5, 0.0, 0.0, 26.0, 23.46750764233409, -0.07780798748475569, 0.0, 1.0, 46359.29128555916], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 273000.0000, 
sim time next is 273600.0000, 
raw observation next is [-9.5, 70.0, 0.0, 0.0, 26.0, 23.41080842201293, -0.09330872821628405, 0.0, 1.0, 46524.46015133633], 
processed observation next is [1.0, 0.17391304347826086, 0.1994459833795014, 0.7, 0.0, 0.0, 0.6666666666666666, 0.4509007018344109, 0.46889709059457196, 0.0, 1.0, 0.22154504833969682], 
reward next is 0.7785, 
noisyNet noise sample is [array([-0.04988495], dtype=float32), -0.41331273]. 
=============================================
[2019-04-03 22:18:53,949] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.7741348e-13 1.8363207e-06 3.8694438e-12 6.8893353e-09 4.0981608e-06
 1.4436451e-13 9.9999416e-01], sum to 1.0000
[2019-04-03 22:18:53,949] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0995
[2019-04-03 22:18:53,965] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.633333333333333, 96.66666666666666, 0.0, 0.0, 26.0, 24.84276599367649, 0.2405629281880052, 0.0, 1.0, 39923.88670582083], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 517200.0000, 
sim time next is 517800.0000, 
raw observation next is [3.716666666666666, 96.83333333333334, 0.0, 0.0, 26.0, 24.88396069463973, 0.2412304247799812, 0.0, 1.0, 39846.87247087662], 
processed observation next is [1.0, 1.0, 0.5655586334256695, 0.9683333333333334, 0.0, 0.0, 0.6666666666666666, 0.5736633912199774, 0.580410141593327, 0.0, 1.0, 0.18974701176607914], 
reward next is 0.8103, 
noisyNet noise sample is [array([0.05948848], dtype=float32), -0.58393717]. 
=============================================
[2019-04-03 22:18:55,538] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.1743290e-10 1.4080357e-02 6.9651340e-10 4.2255797e-06 2.1901843e-03
 9.7891362e-11 9.8372513e-01], sum to 1.0000
[2019-04-03 22:18:55,540] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2031
[2019-04-03 22:18:55,602] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.2, 81.5, 127.0, 467.0, 26.0, 24.98980047118538, 0.3543195270409796, 0.0, 1.0, 34569.20983397232], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 570600.0000, 
sim time next is 571200.0000, 
raw observation next is [-1.2, 82.0, 122.5, 401.3333333333334, 26.0, 25.03085718300046, 0.3527494720919501, 0.0, 1.0, 18721.56247302251], 
processed observation next is [0.0, 0.6086956521739131, 0.42936288088642666, 0.82, 0.4083333333333333, 0.443462246777164, 0.6666666666666666, 0.5859047652500383, 0.6175831573639834, 0.0, 1.0, 0.08915029749058338], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.41878024], dtype=float32), -0.14530657]. 
=============================================
[2019-04-03 22:19:02,014] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.6828190e-11 4.8947707e-04 8.3510193e-10 1.3740349e-06 4.8715118e-04
 5.7988517e-11 9.9902189e-01], sum to 1.0000
[2019-04-03 22:19:02,017] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9981
[2019-04-03 22:19:02,078] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 69.0, 0.0, 0.0, 26.0, 24.95664519812016, 0.2058833686426854, 0.0, 1.0, 42392.8270665032], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 680400.0000, 
sim time next is 681000.0000, 
raw observation next is [-3.4, 69.0, 0.0, 0.0, 26.0, 24.93353263979714, 0.1985390977415142, 0.0, 1.0, 42325.09253259456], 
processed observation next is [0.0, 0.9130434782608695, 0.368421052631579, 0.69, 0.0, 0.0, 0.6666666666666666, 0.5777943866497616, 0.5661796992471714, 0.0, 1.0, 0.2015480596790217], 
reward next is 0.7985, 
noisyNet noise sample is [array([-0.75582176], dtype=float32), -0.86576194]. 
=============================================
[2019-04-03 22:19:02,087] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[77.2992 ]
 [77.35598]
 [77.44784]
 [77.4893 ]
 [77.5469 ]], R is [[77.24897003]
 [77.2746048 ]
 [77.29969025]
 [77.3243866 ]
 [77.34760284]].
[2019-04-03 22:19:10,635] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.7065741e-12 2.1759239e-05 2.1120307e-11 8.4472418e-08 7.1325899e-06
 8.0582416e-13 9.9997103e-01], sum to 1.0000
[2019-04-03 22:19:10,635] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2733
[2019-04-03 22:19:10,701] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.45, 75.0, 32.0, 0.0, 26.0, 25.59155502017575, 0.31225846181947, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 808200.0000, 
sim time next is 808800.0000, 
raw observation next is [-6.366666666666667, 75.0, 36.83333333333333, 0.0, 26.0, 25.75384630888435, 0.3308070796805104, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.28624192059095105, 0.75, 0.12277777777777776, 0.0, 0.6666666666666666, 0.6461538590736957, 0.6102690265601701, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.16119951], dtype=float32), -0.33461607]. 
=============================================
[2019-04-03 22:19:11,830] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.9298712e-12 2.7845925e-04 1.7770457e-10 5.7638118e-07 1.4387348e-04
 1.8099005e-11 9.9957711e-01], sum to 1.0000
[2019-04-03 22:19:11,841] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0961
[2019-04-03 22:19:11,906] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.55, 63.5, 0.0, 0.0, 26.0, 24.89814836343407, 0.2185501404944946, 0.0, 1.0, 78031.08757712957], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 675000.0000, 
sim time next is 675600.0000, 
raw observation next is [-2.633333333333333, 64.0, 0.0, 0.0, 26.0, 24.92628056113482, 0.2215763537063617, 0.0, 1.0, 55345.10238962933], 
processed observation next is [0.0, 0.8260869565217391, 0.38965835641735924, 0.64, 0.0, 0.0, 0.6666666666666666, 0.5771900467612351, 0.5738587845687873, 0.0, 1.0, 0.26354810661728256], 
reward next is 0.7365, 
noisyNet noise sample is [array([1.4165918], dtype=float32), -0.83619493]. 
=============================================
[2019-04-03 22:19:14,246] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.8755571e-13 1.1773956e-04 3.4725597e-11 2.5405794e-07 2.9091936e-05
 6.1383673e-13 9.9985290e-01], sum to 1.0000
[2019-04-03 22:19:14,249] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7703
[2019-04-03 22:19:14,285] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.0, 96.0, 0.0, 0.0, 26.0, 25.18769880640989, 0.4001242837165144, 0.0, 1.0, 39583.81284539436], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 943800.0000, 
sim time next is 944400.0000, 
raw observation next is [5.0, 96.0, 0.0, 0.0, 26.0, 25.19818455702357, 0.4018416767911137, 0.0, 1.0, 39412.90971112327], 
processed observation next is [1.0, 0.9565217391304348, 0.6011080332409973, 0.96, 0.0, 0.0, 0.6666666666666666, 0.5998487130852975, 0.6339472255970379, 0.0, 1.0, 0.18768052243392033], 
reward next is 0.8123, 
noisyNet noise sample is [array([-0.39728627], dtype=float32), -0.26329386]. 
=============================================
[2019-04-03 22:19:15,565] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1326263e-12 9.4521429e-06 3.3468402e-11 1.6418134e-07 2.1740021e-05
 8.0266842e-13 9.9996865e-01], sum to 1.0000
[2019-04-03 22:19:15,565] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6341
[2019-04-03 22:19:15,596] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.5, 60.5, 0.0, 0.0, 26.0, 24.99157940172093, 0.3063623675524723, 0.0, 1.0, 45527.65921248797], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 766200.0000, 
sim time next is 766800.0000, 
raw observation next is [-5.6, 61.0, 0.0, 0.0, 26.0, 24.96173175495727, 0.2976190865613015, 0.0, 1.0, 44597.48449791086], 
processed observation next is [1.0, 0.9130434782608695, 0.30747922437673136, 0.61, 0.0, 0.0, 0.6666666666666666, 0.5801443129131059, 0.5992063621871005, 0.0, 1.0, 0.21236897379957553], 
reward next is 0.7876, 
noisyNet noise sample is [array([-0.16850673], dtype=float32), 0.79128474]. 
=============================================
[2019-04-03 22:19:24,846] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.5323853e-12 1.9007482e-04 4.6997323e-10 1.2813847e-06 6.4701313e-04
 1.6417654e-11 9.9916160e-01], sum to 1.0000
[2019-04-03 22:19:24,851] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9660
[2019-04-03 22:19:24,869] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.7, 72.66666666666666, 0.0, 0.0, 26.0, 24.54759913151238, 0.1655153577887809, 0.0, 1.0, 39041.2890409379], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 881400.0000, 
sim time next is 882000.0000, 
raw observation next is [-0.6, 72.0, 0.0, 0.0, 26.0, 24.54184069899271, 0.1742938231775472, 0.0, 1.0, 39027.9729748359], 
processed observation next is [1.0, 0.21739130434782608, 0.44598337950138506, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5451533915827259, 0.5580979410591824, 0.0, 1.0, 0.18584749035636144], 
reward next is 0.8142, 
noisyNet noise sample is [array([0.15611671], dtype=float32), 1.3444319]. 
=============================================
[2019-04-03 22:19:24,877] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[82.22088 ]
 [82.28932 ]
 [82.340126]
 [82.39983 ]
 [82.43881 ]], R is [[82.14382935]
 [82.13648224]
 [82.12921906]
 [82.12207031]
 [82.11508942]].
[2019-04-03 22:19:25,621] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [4.2455151e-10 7.5765856e-04 9.6330055e-10 1.0978176e-06 4.1674520e-04
 1.1730802e-10 9.9882454e-01], sum to 1.0000
[2019-04-03 22:19:25,625] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7823
[2019-04-03 22:19:25,630] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [18.71666666666667, 63.33333333333333, 163.6666666666667, 0.0, 26.0, 25.13644670983552, 0.5076462185348048, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1165800.0000, 
sim time next is 1166400.0000, 
raw observation next is [18.8, 63.0, 165.5, 0.0, 26.0, 25.12379322682811, 0.5062648368558045, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.9833795013850417, 0.63, 0.5516666666666666, 0.0, 0.6666666666666666, 0.5936494355690091, 0.6687549456186015, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.33479813], dtype=float32), 0.3620009]. 
=============================================
[2019-04-03 22:19:26,692] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.5948466e-11 6.9106491e-06 5.8297617e-11 5.2535650e-08 4.3323421e-06
 2.0982628e-12 9.9998879e-01], sum to 1.0000
[2019-04-03 22:19:26,693] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1610
[2019-04-03 22:19:26,700] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [16.6, 75.0, 0.0, 0.0, 26.0, 24.33003090288422, 0.3182658079234335, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1206000.0000, 
sim time next is 1206600.0000, 
raw observation next is [16.51666666666667, 75.5, 0.0, 0.0, 26.0, 24.30236734999059, 0.3139630349902843, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 1.0, 0.9201292705447832, 0.755, 0.0, 0.0, 0.6666666666666666, 0.5251972791658824, 0.6046543449967614, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.071127], dtype=float32), -0.7655808]. 
=============================================
[2019-04-03 22:19:28,347] A3C_AGENT_WORKER-Thread-8 INFO:Evaluating...
[2019-04-03 22:19:28,348] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-03 22:19:28,349] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-03 22:19:28,349] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:19:28,349] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:19:28,350] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-03 22:19:28,351] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run4
[2019-04-03 22:19:28,352] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:19:28,369] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run4
[2019-04-03 22:19:28,389] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run4
[2019-04-03 22:20:24,632] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.2101992], dtype=float32), 0.23505734]
[2019-04-03 22:20:24,633] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-9.5, 91.0, 0.0, 0.0, 26.0, 23.11923711002736, -0.1933500885695978, 0.0, 1.0, 44508.93037771605]
[2019-04-03 22:20:24,633] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-03 22:20:24,634] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.1517708e-10 8.8548783e-04 2.4188593e-09 9.5385246e-07 4.0594189e-04
 5.3237953e-10 9.9870753e-01], sampled 0.16666430688330303
[2019-04-03 22:21:58,917] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.0115 240027589.7957 1605.3639
[2019-04-03 22:22:31,346] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.8660 263368142.7523 1551.5776
[2019-04-03 22:22:35,575] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.7611 275780177.1969 1233.1879
[2019-04-03 22:22:36,610] A3C_AGENT_WORKER-Thread-8 INFO:Global step: 300000, evaluation results [300000.0, 7241.86598689388, 263368142.75228876, 1551.5775533335125, 7353.011477163436, 240027589.79567444, 1605.3639186669802, 7182.76106096703, 275780177.1969239, 1233.1878816783853]
[2019-04-03 22:22:44,113] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [8.2392724e-13 7.0675324e-06 4.2491431e-12 9.7625321e-09 5.0693503e-05
 5.7810858e-13 9.9994230e-01], sum to 1.0000
[2019-04-03 22:22:44,113] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2408
[2019-04-03 22:22:44,124] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.9, 68.5, 0.0, 0.0, 26.0, 25.73871952960575, 0.6671212951123594, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1121400.0000, 
sim time next is 1122000.0000, 
raw observation next is [11.8, 69.33333333333333, 0.0, 0.0, 26.0, 25.752300421283, 0.6637756120423003, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.7894736842105264, 0.6933333333333332, 0.0, 0.0, 0.6666666666666666, 0.6460250351069167, 0.7212585373474334, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.150308], dtype=float32), -0.4166789]. 
=============================================
[2019-04-03 22:22:44,127] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[92.4982 ]
 [92.58904]
 [92.78373]
 [92.7301 ]
 [92.53061]], R is [[92.37402344]
 [92.45028687]
 [92.52578735]
 [92.5113678 ]
 [92.37711334]].
[2019-04-03 22:22:46,111] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.0637096e-13 1.0730042e-04 7.8801479e-11 3.4435459e-08 6.5900604e-05
 4.1185787e-12 9.9982673e-01], sum to 1.0000
[2019-04-03 22:22:46,132] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7810
[2019-04-03 22:22:46,155] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 26.0, 25.55346706099766, 0.5326105432012667, 0.0, 1.0, 18744.19154646739], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1315800.0000, 
sim time next is 1316400.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 26.0, 25.4819746980034, 0.5365402490896706, 0.0, 1.0, 55763.4973781662], 
processed observation next is [1.0, 0.21739130434782608, 0.5069252077562327, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6234978915002832, 0.6788467496965569, 0.0, 1.0, 0.26554046370555334], 
reward next is 0.7345, 
noisyNet noise sample is [array([-0.16993219], dtype=float32), 0.5855329]. 
=============================================
[2019-04-03 22:22:58,244] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.0246099e-12 1.5040878e-04 4.1566018e-11 1.0766071e-07 3.2609343e-05
 7.7555002e-12 9.9981683e-01], sum to 1.0000
[2019-04-03 22:22:58,244] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5662
[2019-04-03 22:22:58,257] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [16.91666666666667, 68.33333333333333, 98.66666666666666, 0.0, 26.0, 25.60797127042032, 0.5625153461188096, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1158600.0000, 
sim time next is 1159200.0000, 
raw observation next is [17.2, 67.0, 106.5, 0.0, 26.0, 25.53300036541425, 0.5501755069607853, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.9390581717451525, 0.67, 0.355, 0.0, 0.6666666666666666, 0.6277500304511875, 0.6833918356535951, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.51473975], dtype=float32), 0.7833333]. 
=============================================
[2019-04-03 22:23:01,581] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.0136958e-14 1.0969776e-06 4.4971416e-13 2.5296920e-10 7.3740154e-07
 1.7287856e-14 9.9999821e-01], sum to 1.0000
[2019-04-03 22:23:01,611] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5173
[2019-04-03 22:23:01,625] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.6, 68.0, 85.0, 701.0, 26.0, 26.1517758328057, 0.6601149014286668, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1517400.0000, 
sim time next is 1518000.0000, 
raw observation next is [9.066666666666666, 66.33333333333334, 83.33333333333334, 694.6666666666667, 26.0, 26.38185424324445, 0.6845105898288927, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.713758079409049, 0.6633333333333334, 0.2777777777777778, 0.7675874769797423, 0.6666666666666666, 0.6984878536037042, 0.7281701966096309, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2682572], dtype=float32), 1.5377331]. 
=============================================
[2019-04-03 22:23:01,639] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[90.32755 ]
 [90.89709 ]
 [91.47179 ]
 [92.05981 ]
 [92.703636]], R is [[89.84503937]
 [89.94658661]
 [90.04711914]
 [90.14665222]
 [90.24518585]].
[2019-04-03 22:23:03,494] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.7530775e-13 8.7648132e-06 2.0175760e-12 7.9801517e-09 1.8638410e-07
 5.7335977e-13 9.9999094e-01], sum to 1.0000
[2019-04-03 22:23:03,496] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9183
[2019-04-03 22:23:03,598] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 26.0, 24.95891743551235, 0.5260664603625148, 0.0, 1.0, 117113.6733525194], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1369800.0000, 
sim time next is 1370400.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 26.0, 25.06887289108332, 0.5492013249300111, 0.0, 1.0, 68007.79781464477], 
processed observation next is [1.0, 0.8695652173913043, 0.4764542936288089, 0.96, 0.0, 0.0, 0.6666666666666666, 0.58907274092361, 0.6830671083100036, 0.0, 1.0, 0.3238466562602132], 
reward next is 0.6762, 
noisyNet noise sample is [array([1.37283], dtype=float32), 0.97824293]. 
=============================================
[2019-04-03 22:23:19,243] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.3350775e-13 1.2684404e-07 1.3754635e-12 3.3827563e-09 1.0507035e-06
 5.6867529e-14 9.9999881e-01], sum to 1.0000
[2019-04-03 22:23:19,244] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9999
[2019-04-03 22:23:19,255] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.6, 68.0, 85.0, 701.0, 26.0, 26.15106923323026, 0.6599642642310207, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1517400.0000, 
sim time next is 1518000.0000, 
raw observation next is [9.066666666666666, 66.33333333333334, 83.33333333333334, 694.6666666666667, 26.0, 26.38115001220662, 0.684358547827825, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.713758079409049, 0.6633333333333334, 0.2777777777777778, 0.7675874769797423, 0.6666666666666666, 0.6984291676838851, 0.7281195159426083, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9777969], dtype=float32), -0.44918865]. 
=============================================
[2019-04-03 22:23:19,318] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[90.60901 ]
 [91.12313 ]
 [91.65147 ]
 [92.199905]
 [92.81274 ]], R is [[90.18527985]
 [90.28342438]
 [90.38059235]
 [90.47678375]
 [90.57201385]].
[2019-04-03 22:23:20,503] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3241356e-13 1.1181303e-04 9.4053463e-12 1.1312144e-08 5.5364585e-06
 5.2224570e-12 9.9988270e-01], sum to 1.0000
[2019-04-03 22:23:20,503] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5739
[2019-04-03 22:23:20,582] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 100.0, 32.0, 0.0, 26.0, 25.90555192366789, 0.5149093283123346, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1414800.0000, 
sim time next is 1415400.0000, 
raw observation next is [-0.5, 99.16666666666667, 36.66666666666667, 0.0, 26.0, 25.96004918929946, 0.5069802509039095, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.44875346260387816, 0.9916666666666667, 0.12222222222222223, 0.0, 0.6666666666666666, 0.6633374324416218, 0.6689934169679699, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.057442], dtype=float32), -1.6289486]. 
=============================================
[2019-04-03 22:23:23,583] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.8568222e-11 1.7591101e-03 6.0724896e-09 5.2629053e-07 4.5291163e-04
 3.9263118e-10 9.9778736e-01], sum to 1.0000
[2019-04-03 22:23:23,587] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9952
[2019-04-03 22:23:23,722] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 85.66666666666667, 115.3333333333333, 0.0, 26.0, 24.90857816775353, 0.3460756405329682, 0.0, 1.0, 42096.36577851309], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1768800.0000, 
sim time next is 1769400.0000, 
raw observation next is [-2.3, 85.0, 119.0, 0.0, 26.0, 24.92737251724211, 0.3481257258758854, 0.0, 1.0, 32689.12305131619], 
processed observation next is [0.0, 0.4782608695652174, 0.3988919667590028, 0.85, 0.39666666666666667, 0.0, 0.6666666666666666, 0.577281043103509, 0.6160419086252952, 0.0, 1.0, 0.1556624907205533], 
reward next is 0.8443, 
noisyNet noise sample is [array([0.8272556], dtype=float32), 1.5459219]. 
=============================================
[2019-04-03 22:23:29,425] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.5779956e-13 3.2206779e-06 7.8830215e-12 1.0199925e-08 3.3468837e-06
 3.6171166e-13 9.9999344e-01], sum to 1.0000
[2019-04-03 22:23:29,458] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0710
[2019-04-03 22:23:29,481] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.7, 74.0, 0.0, 0.0, 26.0, 25.45992233334081, 0.6886929334818411, 0.0, 1.0, 42384.33540258616], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1544400.0000, 
sim time next is 1545000.0000, 
raw observation next is [7.516666666666667, 74.33333333333334, 0.0, 0.0, 26.0, 25.87185070231836, 0.7030376087667171, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.6708217913204063, 0.7433333333333334, 0.0, 0.0, 0.6666666666666666, 0.6559875585265299, 0.7343458695889057, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3018752], dtype=float32), 0.36164913]. 
=============================================
[2019-04-03 22:23:29,537] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[87.06454]
 [87.13143]
 [86.47834]
 [85.88192]
 [85.43183]], R is [[87.16272736]
 [87.08927155]
 [86.39094543]
 [85.58220673]
 [84.7898941 ]].
[2019-04-03 22:23:36,900] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.1680210e-11 3.2378375e-04 6.8372974e-10 4.6048865e-07 5.7111291e-05
 2.6747884e-10 9.9961865e-01], sum to 1.0000
[2019-04-03 22:23:36,900] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4691
[2019-04-03 22:23:36,989] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.2, 86.33333333333333, 75.66666666666666, 0.0, 26.0, 24.88781226523171, 0.3279426747770748, 0.0, 1.0, 70145.48939786661], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1763400.0000, 
sim time next is 1764000.0000, 
raw observation next is [-2.3, 87.0, 81.0, 0.0, 26.0, 24.88793870203348, 0.3334178595820632, 0.0, 1.0, 56703.60914417369], 
processed observation next is [0.0, 0.43478260869565216, 0.3988919667590028, 0.87, 0.27, 0.0, 0.6666666666666666, 0.5739948918361234, 0.6111392865273544, 0.0, 1.0, 0.2700171864008271], 
reward next is 0.7300, 
noisyNet noise sample is [array([-0.557956], dtype=float32), -1.1798203]. 
=============================================
[2019-04-03 22:23:36,993] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[77.423325]
 [77.232216]
 [76.84319 ]
 [76.746216]
 [76.90408 ]], R is [[77.53709412]
 [77.42770386]
 [77.16374207]
 [77.15273285]
 [77.3812027 ]].
[2019-04-03 22:23:42,627] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.4499592e-11 6.4069145e-06 4.1445496e-11 1.6424490e-07 9.5452233e-05
 5.5719838e-12 9.9989796e-01], sum to 1.0000
[2019-04-03 22:23:42,631] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1507
[2019-04-03 22:23:42,701] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.1, 66.66666666666667, 230.3333333333333, 8.0, 26.0, 25.6370799527529, 0.3284211426517152, 1.0, 1.0, 60061.81734862381], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1943400.0000, 
sim time next is 1944000.0000, 
raw observation next is [-5.0, 65.0, 229.5, 7.0, 26.0, 25.68891559616228, 0.344928611799167, 1.0, 1.0, 38800.64245352406], 
processed observation next is [1.0, 0.5217391304347826, 0.32409972299168976, 0.65, 0.765, 0.0077348066298342545, 0.6666666666666666, 0.6407429663468566, 0.6149762039330556, 1.0, 1.0, 0.18476496406440027], 
reward next is 0.8152, 
noisyNet noise sample is [array([-1.4719024], dtype=float32), -0.26996654]. 
=============================================
[2019-04-03 22:23:42,712] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[74.99292 ]
 [75.27241 ]
 [75.32473 ]
 [75.7309  ]
 [76.097145]], R is [[74.79077911]
 [74.75685883]
 [74.57781219]
 [74.83203125]
 [75.08370972]].
[2019-04-03 22:23:44,264] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.2769030e-12 7.1681548e-06 2.9472608e-10 2.5956891e-07 1.2413622e-04
 9.2153689e-12 9.9986851e-01], sum to 1.0000
[2019-04-03 22:23:44,272] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3369
[2019-04-03 22:23:44,328] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 62.0, 59.33333333333334, 0.0, 26.0, 25.60659880389239, 0.319918130445177, 1.0, 1.0, 27034.59980943718], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1957800.0000, 
sim time next is 1958400.0000, 
raw observation next is [-2.8, 62.0, 52.0, 0.0, 26.0, 25.55525722264509, 0.3159812957444894, 1.0, 1.0, 26354.14750232171], 
processed observation next is [1.0, 0.6956521739130435, 0.38504155124653744, 0.62, 0.17333333333333334, 0.0, 0.6666666666666666, 0.6296047685537575, 0.6053270985814965, 1.0, 1.0, 0.12549594048724624], 
reward next is 0.8745, 
noisyNet noise sample is [array([-0.62848985], dtype=float32), -1.1817126]. 
=============================================
[2019-04-03 22:24:18,186] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.3613707e-12 3.2804972e-05 1.8157886e-10 3.2151096e-08 1.8160937e-06
 9.5225928e-12 9.9996543e-01], sum to 1.0000
[2019-04-03 22:24:18,186] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7838
[2019-04-03 22:24:18,203] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.2, 87.0, 0.0, 0.0, 26.0, 24.19877649579291, 0.09549846359814428, 0.0, 1.0, 43475.66519349427], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2091600.0000, 
sim time next is 2092200.0000, 
raw observation next is [-6.283333333333333, 86.33333333333333, 0.0, 0.0, 26.0, 24.11543004374047, 0.1041188083931322, 0.0, 1.0, 43822.83791849922], 
processed observation next is [1.0, 0.21739130434782608, 0.288550323176362, 0.8633333333333333, 0.0, 0.0, 0.6666666666666666, 0.5096191703117059, 0.5347062694643774, 0.0, 1.0, 0.208680180564282], 
reward next is 0.7913, 
noisyNet noise sample is [array([-1.837195], dtype=float32), -0.30654222]. 
=============================================
[2019-04-03 22:24:18,577] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.7526822e-11 5.6150868e-05 9.7530671e-11 3.7028360e-07 2.0322266e-05
 5.9801266e-12 9.9992323e-01], sum to 1.0000
[2019-04-03 22:24:18,577] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8935
[2019-04-03 22:24:18,730] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.0, 69.5, 293.0, 101.0, 26.0, 25.79712855094739, 0.4347514694816823, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2115000.0000, 
sim time next is 2115600.0000, 
raw observation next is [-6.9, 67.66666666666667, 269.3333333333334, 106.5, 26.0, 25.88229925623976, 0.4410469741934289, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.27146814404432135, 0.6766666666666667, 0.8977777777777781, 0.11767955801104972, 0.6666666666666666, 0.6568582713533134, 0.6470156580644763, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.71097213], dtype=float32), -0.27611724]. 
=============================================
[2019-04-03 22:24:40,685] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.3252472e-13 1.0085462e-06 1.8097796e-11 1.0008952e-08 4.0227229e-07
 1.1187214e-12 9.9999857e-01], sum to 1.0000
[2019-04-03 22:24:40,685] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5561
[2019-04-03 22:24:40,705] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.616666666666667, 77.5, 0.0, 0.0, 26.0, 24.5123774053179, 0.2104150795076331, 0.0, 1.0, 44181.98174070633], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2245800.0000, 
sim time next is 2246400.0000, 
raw observation next is [-6.7, 78.0, 0.0, 0.0, 26.0, 24.46277649377204, 0.2002235510640354, 0.0, 1.0, 44246.38144604994], 
processed observation next is [1.0, 0.0, 0.2770083102493075, 0.78, 0.0, 0.0, 0.6666666666666666, 0.5385647078143366, 0.5667411836880117, 0.0, 1.0, 0.21069705450499973], 
reward next is 0.7893, 
noisyNet noise sample is [array([-0.0327334], dtype=float32), -1.36089]. 
=============================================
[2019-04-03 22:24:51,636] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.0731047e-12 7.6616798e-06 4.8383437e-12 3.5044120e-08 5.2422201e-06
 3.3192503e-13 9.9998701e-01], sum to 1.0000
[2019-04-03 22:24:51,653] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3182
[2019-04-03 22:24:51,681] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.3, 29.0, 129.0, 325.6666666666667, 26.0, 25.76414092336771, 0.3880765941309394, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2560200.0000, 
sim time next is 2560800.0000, 
raw observation next is [3.3, 29.0, 121.5, 338.3333333333333, 26.0, 25.81803044047701, 0.3893948985966237, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.554016620498615, 0.29, 0.405, 0.3738489871086556, 0.6666666666666666, 0.6515025367064174, 0.6297982995322079, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8694967], dtype=float32), 1.464952]. 
=============================================
[2019-04-03 22:24:59,367] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1934785e-10 2.2144952e-05 8.5170226e-10 2.4474710e-07 3.3591201e-05
 9.3054633e-11 9.9994397e-01], sum to 1.0000
[2019-04-03 22:24:59,367] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7157
[2019-04-03 22:24:59,419] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.75, 42.5, 0.0, 0.0, 26.0, 24.89207615363606, 0.2125057653100292, 0.0, 1.0, 42991.61512595251], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2413800.0000, 
sim time next is 2414400.0000, 
raw observation next is [-4.833333333333333, 42.0, 0.0, 0.0, 26.0, 24.84119686952986, 0.2029527543216852, 0.0, 1.0, 43015.9288699974], 
processed observation next is [0.0, 0.9565217391304348, 0.3287165281625116, 0.42, 0.0, 0.0, 0.6666666666666666, 0.5700997391274883, 0.5676509181072283, 0.0, 1.0, 0.20483775652379713], 
reward next is 0.7952, 
noisyNet noise sample is [array([-0.17796996], dtype=float32), 1.2745713]. 
=============================================
[2019-04-03 22:25:15,874] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [9.0339303e-13 2.4415237e-06 2.0651833e-12 1.5971901e-08 9.2870932e-06
 8.4295594e-14 9.9998820e-01], sum to 1.0000
[2019-04-03 22:25:15,874] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6516
[2019-04-03 22:25:15,911] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 50.0, 115.0, 165.0, 26.0, 25.96417183818371, 0.4595132589688102, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2649600.0000, 
sim time next is 2650200.0000, 
raw observation next is [0.5, 50.0, 101.6666666666667, 151.3333333333333, 26.0, 25.98012332495362, 0.4607374263144486, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.4764542936288089, 0.5, 0.338888888888889, 0.16721915285451192, 0.6666666666666666, 0.6650102770794684, 0.6535791421048162, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.43032306], dtype=float32), -0.29205716]. 
=============================================
[2019-04-03 22:25:25,726] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [8.1036100e-13 2.8145889e-06 2.7834308e-11 1.3127769e-08 8.0184072e-06
 1.2066234e-13 9.9998915e-01], sum to 1.0000
[2019-04-03 22:25:25,727] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2034
[2019-04-03 22:25:25,772] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.0549925e-12 2.0693828e-06 1.6504284e-10 2.2759455e-08 6.9060320e-06
 3.6718705e-12 9.9999094e-01], sum to 1.0000
[2019-04-03 22:25:25,779] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 56.5, 0.0, 0.0, 26.0, 25.47485604794691, 0.4137520988388615, 1.0, 1.0, 52124.60234094736], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2745000.0000, 
sim time next is 2745600.0000, 
raw observation next is [-4.666666666666666, 57.33333333333333, 0.0, 0.0, 26.0, 25.25423916166563, 0.4006010399475587, 1.0, 1.0, 52984.82599257762], 
processed observation next is [1.0, 0.782608695652174, 0.33333333333333337, 0.5733333333333333, 0.0, 0.0, 0.6666666666666666, 0.6045199301388026, 0.6335336799825195, 1.0, 1.0, 0.25230869520275057], 
reward next is 0.7477, 
noisyNet noise sample is [array([0.9879802], dtype=float32), 0.72265476]. 
=============================================
[2019-04-03 22:25:25,781] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2855
[2019-04-03 22:25:25,795] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.0, 72.5, 0.0, 0.0, 26.0, 24.37663389689201, 0.1750374253472832, 0.0, 1.0, 44440.30726646776], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2683800.0000, 
sim time next is 2684400.0000, 
raw observation next is [-10.33333333333333, 73.66666666666667, 0.0, 0.0, 26.0, 24.28332620344689, 0.1594221239523938, 0.0, 1.0, 44468.22600786677], 
processed observation next is [1.0, 0.043478260869565216, 0.17636195752539252, 0.7366666666666667, 0.0, 0.0, 0.6666666666666666, 0.5236105169539075, 0.5531407079841313, 0.0, 1.0, 0.21175345718031793], 
reward next is 0.7882, 
noisyNet noise sample is [array([1.3567177], dtype=float32), 0.06835887]. 
=============================================
[2019-04-03 22:25:37,357] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4320471e-12 5.7095099e-06 7.2029632e-11 6.5667940e-08 3.3184562e-05
 2.1908742e-12 9.9996114e-01], sum to 1.0000
[2019-04-03 22:25:37,357] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0168
[2019-04-03 22:25:37,408] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 53.0, 0.0, 0.0, 26.0, 25.33244246956481, 0.3596539664893707, 0.0, 1.0, 87985.96137907643], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2845800.0000, 
sim time next is 2846400.0000, 
raw observation next is [2.0, 56.0, 0.0, 0.0, 26.0, 25.25811797043865, 0.3573963521718664, 0.0, 1.0, 64486.86042100425], 
processed observation next is [1.0, 0.9565217391304348, 0.518005540166205, 0.56, 0.0, 0.0, 0.6666666666666666, 0.6048431642032209, 0.6191321173906221, 0.0, 1.0, 0.30708028771906787], 
reward next is 0.6929, 
noisyNet noise sample is [array([-0.09957619], dtype=float32), 0.13186911]. 
=============================================
[2019-04-03 22:25:38,502] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.3868758e-11 8.1767776e-04 9.4039199e-10 2.0087886e-07 1.0021412e-04
 1.1479209e-10 9.9908197e-01], sum to 1.0000
[2019-04-03 22:25:38,502] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9956
[2019-04-03 22:25:38,541] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.0, 64.0, 0.0, 0.0, 26.0, 24.09920079711095, 0.0222279845671012, 0.0, 1.0, 56215.19025214394], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2788200.0000, 
sim time next is 2788800.0000, 
raw observation next is [-7.0, 64.0, 0.0, 0.0, 26.0, 23.93957326101369, 0.006023015746727899, 0.0, 1.0, 60583.09291889046], 
processed observation next is [1.0, 0.2608695652173913, 0.2686980609418283, 0.64, 0.0, 0.0, 0.6666666666666666, 0.49496443841780763, 0.5020076719155759, 0.0, 1.0, 0.28849091866138316], 
reward next is 0.7115, 
noisyNet noise sample is [array([-0.11711025], dtype=float32), 0.90443563]. 
=============================================
[2019-04-03 22:25:44,309] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.3933112e-12 2.0021474e-05 3.7950580e-11 5.3215238e-08 4.5450586e-05
 2.6164331e-12 9.9993455e-01], sum to 1.0000
[2019-04-03 22:25:44,309] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9985
[2019-04-03 22:25:44,328] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.88039048743001, 0.3311491436989361, 0.0, 1.0, 43297.71681390614], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2941200.0000, 
sim time next is 2941800.0000, 
raw observation next is [-2.0, 85.00000000000001, 0.0, 0.0, 26.0, 24.85783820094465, 0.3229187708264029, 0.0, 1.0, 43289.12569314575], 
processed observation next is [0.0, 0.043478260869565216, 0.40720221606648205, 0.8500000000000001, 0.0, 0.0, 0.6666666666666666, 0.5714865167453874, 0.6076395902754677, 0.0, 1.0, 0.2061386937768845], 
reward next is 0.7939, 
noisyNet noise sample is [array([-0.20503668], dtype=float32), 0.8170208]. 
=============================================
[2019-04-03 22:25:55,696] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.43980421e-12 1.20658275e-04 1.63056263e-10 1.10981304e-07
 3.85923689e-04 2.18738309e-11 9.99493241e-01], sum to 1.0000
[2019-04-03 22:25:55,729] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6704
[2019-04-03 22:25:55,746] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.6666666666666666, 100.0, 0.0, 0.0, 26.0, 25.30081562123873, 0.3191802020628556, 0.0, 1.0, 39327.89554491204], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3112800.0000, 
sim time next is 3113400.0000, 
raw observation next is [0.8333333333333334, 100.0, 0.0, 0.0, 26.0, 25.27188218654041, 0.3352986684554417, 0.0, 1.0, 39536.00093918037], 
processed observation next is [1.0, 0.0, 0.4856879039704525, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6059901822117008, 0.6117662228184806, 0.0, 1.0, 0.18826667113895415], 
reward next is 0.8117, 
noisyNet noise sample is [array([-0.42595017], dtype=float32), 0.20958604]. 
=============================================
[2019-04-03 22:26:03,922] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.6247323e-11 1.3764678e-04 3.4599773e-10 1.3982620e-07 3.8712482e-05
 2.8100607e-11 9.9982351e-01], sum to 1.0000
[2019-04-03 22:26:03,923] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9794
[2019-04-03 22:26:03,936] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 60.0, 0.0, 0.0, 26.0, 24.7659527965481, 0.2240457100982016, 0.0, 1.0, 42771.54643831423], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3391200.0000, 
sim time next is 3391800.0000, 
raw observation next is [-3.0, 60.83333333333333, 0.0, 0.0, 26.0, 24.72220044336211, 0.2178222198129289, 0.0, 1.0, 42843.21904267136], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.6083333333333333, 0.0, 0.0, 0.6666666666666666, 0.5601833702801757, 0.5726074066043096, 0.0, 1.0, 0.2040153287746255], 
reward next is 0.7960, 
noisyNet noise sample is [array([-1.5106167], dtype=float32), 0.35131177]. 
=============================================
[2019-04-03 22:26:05,706] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.8555454e-13 1.7864336e-04 4.9441021e-11 3.0670524e-08 2.6201624e-05
 2.5027584e-12 9.9979514e-01], sum to 1.0000
[2019-04-03 22:26:05,706] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6501
[2019-04-03 22:26:05,722] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.1, 100.0, 0.0, 0.0, 26.0, 25.3610934262762, 0.2978959430392837, 0.0, 1.0, 99063.11249745169], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3121800.0000, 
sim time next is 3122400.0000, 
raw observation next is [2.2, 100.0, 0.0, 0.0, 26.0, 25.29859944431307, 0.2982146946018633, 0.0, 1.0, 77832.2631185539], 
processed observation next is [1.0, 0.13043478260869565, 0.5235457063711911, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6082166203594225, 0.5994048982006212, 0.0, 1.0, 0.3706298243740662], 
reward next is 0.6294, 
noisyNet noise sample is [array([0.56525874], dtype=float32), 2.7349749]. 
=============================================
[2019-04-03 22:26:11,146] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.1257751e-13 6.5136521e-07 6.0079518e-13 1.3768868e-09 1.6706019e-06
 8.2636915e-15 9.9999774e-01], sum to 1.0000
[2019-04-03 22:26:11,149] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2758
[2019-04-03 22:26:11,178] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 49.0, 92.66666666666666, 718.3333333333333, 26.0, 26.81057575383308, 0.627011604644984, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3510600.0000, 
sim time next is 3511200.0000, 
raw observation next is [3.0, 49.0, 90.33333333333334, 702.6666666666666, 26.0, 26.24733309050979, 0.6691645804550288, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5457063711911359, 0.49, 0.30111111111111116, 0.776427255985267, 0.6666666666666666, 0.6872777575424825, 0.7230548601516763, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.49884868], dtype=float32), -1.3078936]. 
=============================================
[2019-04-03 22:26:12,383] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.6421656e-11 8.4313950e-05 2.6199547e-09 1.2313131e-06 6.3037820e-05
 7.2343471e-11 9.9985135e-01], sum to 1.0000
[2019-04-03 22:26:12,386] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3234
[2019-04-03 22:26:12,409] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.0, 77.0, 0.0, 0.0, 26.0, 24.94824125736911, 0.3586935390864376, 0.0, 1.0, 43830.6488363183], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3290400.0000, 
sim time next is 3291000.0000, 
raw observation next is [-8.0, 77.0, 0.0, 0.0, 26.0, 24.97960274913984, 0.349248875779577, 0.0, 1.0, 43816.29087330391], 
processed observation next is [1.0, 0.08695652173913043, 0.24099722991689754, 0.77, 0.0, 0.0, 0.6666666666666666, 0.58163356242832, 0.6164162919265257, 0.0, 1.0, 0.20864900415859003], 
reward next is 0.7914, 
noisyNet noise sample is [array([0.27612013], dtype=float32), 0.89068395]. 
=============================================
[2019-04-03 22:26:12,415] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[70.357346]
 [70.55107 ]
 [70.6934  ]
 [70.92379 ]
 [71.125824]], R is [[70.28064728]
 [70.36912537]
 [70.45665741]
 [70.54341888]
 [70.62949371]].
[2019-04-03 22:26:14,870] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.5435041e-11 1.5644811e-04 2.0736206e-10 1.4133565e-07 5.9138845e-05
 4.4268571e-11 9.9978429e-01], sum to 1.0000
[2019-04-03 22:26:14,870] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3356
[2019-04-03 22:26:14,944] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.5, 70.0, 88.0, 425.0, 26.0, 24.5830002618378, 0.4092911304282303, 0.0, 1.0, 202112.1114429946], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3573000.0000, 
sim time next is 3573600.0000, 
raw observation next is [-6.333333333333334, 70.0, 90.0, 466.8333333333333, 26.0, 25.33973367715206, 0.4859278165059229, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.28716528162511545, 0.7, 0.3, 0.5158379373848987, 0.6666666666666666, 0.6116444730960051, 0.6619759388353076, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8403882], dtype=float32), 0.058535963]. 
=============================================
[2019-04-03 22:26:19,387] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [9.9037228e-12 1.6004797e-05 7.6375212e-11 3.9373557e-08 1.2359690e-04
 1.5386597e-11 9.9986029e-01], sum to 1.0000
[2019-04-03 22:26:19,391] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7188
[2019-04-03 22:26:19,400] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 48.33333333333334, 0.0, 0.0, 26.0, 25.31555918234606, 0.3705884339922544, 0.0, 1.0, 38480.7057384359], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3626400.0000, 
sim time next is 3627000.0000, 
raw observation next is [3.0, 42.5, 0.0, 0.0, 26.0, 25.35789187000995, 0.3802126218013648, 0.0, 1.0, 38351.87457138798], 
processed observation next is [0.0, 1.0, 0.5457063711911359, 0.425, 0.0, 0.0, 0.6666666666666666, 0.6131576558341626, 0.626737540600455, 0.0, 1.0, 0.18262797414946658], 
reward next is 0.8174, 
noisyNet noise sample is [array([-0.9397595], dtype=float32), -0.053404126]. 
=============================================
[2019-04-03 22:26:19,428] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[76.64601]
 [76.56737]
 [76.48454]
 [76.44859]
 [76.46834]], R is [[76.82051849]
 [76.86907196]
 [76.91668701]
 [76.9619751 ]
 [77.00060272]].
[2019-04-03 22:26:20,160] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4977274e-10 1.5057922e-04 1.4873005e-09 7.9796411e-07 5.1802183e-03
 2.6484315e-10 9.9466830e-01], sum to 1.0000
[2019-04-03 22:26:20,163] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2076
[2019-04-03 22:26:20,186] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.48843056740702, 0.4314322263140539, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3615600.0000, 
sim time next is 3616200.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.53639219362175, 0.4252793661662677, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.4349030470914128, 0.42, 0.0, 0.0, 0.6666666666666666, 0.6280326828018126, 0.6417597887220893, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.51831794], dtype=float32), -0.6517187]. 
=============================================
[2019-04-03 22:26:28,585] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.08081936e-11 8.34940420e-06 1.12561196e-10 9.86664190e-08
 7.92435458e-05 1.38939841e-11 9.99912262e-01], sum to 1.0000
[2019-04-03 22:26:28,588] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0153
[2019-04-03 22:26:28,614] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 60.0, 0.0, 0.0, 26.0, 25.33762305770028, 0.366918273532004, 0.0, 1.0, 38958.54952243254], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3625200.0000, 
sim time next is 3625800.0000, 
raw observation next is [-1.0, 54.16666666666666, 0.0, 0.0, 26.0, 25.31683977871457, 0.3673328777148704, 0.0, 1.0, 38573.39344987871], 
processed observation next is [0.0, 1.0, 0.4349030470914128, 0.5416666666666665, 0.0, 0.0, 0.6666666666666666, 0.6097366482262142, 0.6224442925716235, 0.0, 1.0, 0.1836828259518034], 
reward next is 0.8163, 
noisyNet noise sample is [array([-0.29093704], dtype=float32), 0.32474]. 
=============================================
[2019-04-03 22:26:31,176] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.9800342e-11 1.7119208e-05 6.7398437e-10 1.5261813e-07 2.6977679e-04
 6.6085221e-11 9.9971288e-01], sum to 1.0000
[2019-04-03 22:26:31,176] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5943
[2019-04-03 22:26:31,194] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.833333333333333, 69.16666666666667, 0.0, 0.0, 26.0, 25.26374498708966, 0.4227878648936662, 0.0, 1.0, 41341.73470651767], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3549000.0000, 
sim time next is 3549600.0000, 
raw observation next is [-3.0, 71.0, 0.0, 0.0, 26.0, 25.24262389058011, 0.425349921330435, 0.0, 1.0, 41139.67658168299], 
processed observation next is [0.0, 0.08695652173913043, 0.3795013850415513, 0.71, 0.0, 0.0, 0.6666666666666666, 0.6035519908816758, 0.641783307110145, 0.0, 1.0, 0.19590322181753803], 
reward next is 0.8041, 
noisyNet noise sample is [array([2.0225263], dtype=float32), -0.1113684]. 
=============================================
[2019-04-03 22:26:31,280] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.2881602e-11 2.0278131e-03 3.5523165e-10 1.1396928e-07 1.7815779e-04
 1.3981083e-10 9.9779391e-01], sum to 1.0000
[2019-04-03 22:26:31,288] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7431
[2019-04-03 22:26:31,356] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.166666666666667, 65.83333333333334, 103.6666666666667, 703.3333333333334, 26.0, 25.81619452879779, 0.5144092009047577, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3577800.0000, 
sim time next is 3578400.0000, 
raw observation next is [-5.0, 65.0, 105.5, 717.0, 26.0, 25.76153559785025, 0.5073627831216948, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.32409972299168976, 0.65, 0.3516666666666667, 0.7922651933701658, 0.6666666666666666, 0.6467946331541876, 0.6691209277072315, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3231826], dtype=float32), 1.0085715]. 
=============================================
[2019-04-03 22:26:32,825] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.5648568e-11 2.0489706e-06 6.7477461e-11 3.7303693e-08 1.1126412e-05
 8.6504354e-12 9.9998677e-01], sum to 1.0000
[2019-04-03 22:26:32,828] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7354
[2019-04-03 22:26:32,848] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.166666666666667, 50.83333333333334, 116.6666666666667, 819.3333333333334, 26.0, 25.19519511209842, 0.4504845800067681, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3588600.0000, 
sim time next is 3589200.0000, 
raw observation next is [-2.0, 50.0, 116.0, 817.5, 26.0, 25.19882337245548, 0.4516525351215623, 0.0, 1.0, 18703.67321859777], 
processed observation next is [0.0, 0.5652173913043478, 0.40720221606648205, 0.5, 0.38666666666666666, 0.9033149171270718, 0.6666666666666666, 0.5999019477046232, 0.6505508450405207, 0.0, 1.0, 0.08906511056475129], 
reward next is 0.9109, 
noisyNet noise sample is [array([-0.13295126], dtype=float32), -0.696508]. 
=============================================
[2019-04-03 22:26:34,292] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.6680194e-12 2.1924255e-04 1.1725087e-11 2.9189115e-08 2.2730588e-05
 8.3473782e-13 9.9975795e-01], sum to 1.0000
[2019-04-03 22:26:34,294] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5426
[2019-04-03 22:26:34,310] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.333333333333334, 42.33333333333334, 113.6666666666667, 819.8333333333334, 26.0, 25.29060484830892, 0.4541529935182582, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3676800.0000, 
sim time next is 3677400.0000, 
raw observation next is [5.5, 42.5, 113.0, 818.0, 26.0, 25.29173649857264, 0.4550744729564339, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.6149584487534627, 0.425, 0.37666666666666665, 0.9038674033149171, 0.6666666666666666, 0.6076447082143867, 0.651691490985478, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4277883], dtype=float32), 0.8081384]. 
=============================================
[2019-04-03 22:26:38,011] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.2752330e-13 1.3327724e-07 7.5085632e-12 7.0808968e-09 1.7401297e-06
 1.6698607e-13 9.9999809e-01], sum to 1.0000
[2019-04-03 22:26:38,019] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8878
[2019-04-03 22:26:38,047] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.333333333333334, 35.33333333333334, 69.66666666666666, 567.3333333333334, 26.0, 26.91382889416571, 0.4397512478970909, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3946800.0000, 
sim time next is 3947400.0000, 
raw observation next is [-4.5, 36.0, 66.0, 536.0, 26.0, 26.78934786911109, 0.7022364895340824, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.3379501385041552, 0.36, 0.22, 0.5922651933701657, 0.6666666666666666, 0.7324456557592575, 0.7340788298446941, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.77805525], dtype=float32), 0.023958502]. 
=============================================
[2019-04-03 22:26:40,559] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.4038811e-12 8.6985938e-06 8.8413533e-11 2.4108516e-08 5.7045872e-07
 2.9033035e-12 9.9999070e-01], sum to 1.0000
[2019-04-03 22:26:40,564] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4225
[2019-04-03 22:26:40,588] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 77.0, 0.0, 0.0, 26.0, 25.45882093859746, 0.5101841039214978, 0.0, 1.0, 76940.27959612344], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3794400.0000, 
sim time next is 3795000.0000, 
raw observation next is [-3.0, 76.0, 0.0, 0.0, 26.0, 25.43699107017922, 0.4676720030737827, 0.0, 1.0, 68446.57649030015], 
processed observation next is [1.0, 0.9565217391304348, 0.3795013850415513, 0.76, 0.0, 0.0, 0.6666666666666666, 0.6197492558482685, 0.6558906676912609, 0.0, 1.0, 0.3259360785252388], 
reward next is 0.6741, 
noisyNet noise sample is [array([1.1943661], dtype=float32), -0.1564214]. 
=============================================
[2019-04-03 22:26:40,614] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[74.703384]
 [74.07066 ]
 [73.61978 ]
 [73.28395 ]
 [73.22763 ]], R is [[75.18173981]
 [75.06354523]
 [74.98971558]
 [75.06619263]
 [75.31552887]].
[2019-04-03 22:26:41,860] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.8941545e-13 2.3045939e-06 1.9892488e-12 1.2186435e-08 5.3504623e-06
 4.5102924e-13 9.9999237e-01], sum to 1.0000
[2019-04-03 22:26:41,871] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6477
[2019-04-03 22:26:41,904] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.833333333333333, 60.0, 107.0, 743.3333333333334, 26.0, 26.46894131479482, 0.6026011789720924, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3838200.0000, 
sim time next is 3838800.0000, 
raw observation next is [-1.666666666666667, 60.00000000000001, 108.5, 759.1666666666667, 26.0, 26.51474528954822, 0.6138479954821133, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4164358264081256, 0.6000000000000001, 0.3616666666666667, 0.8388581952117865, 0.6666666666666666, 0.7095621074623516, 0.7046159984940378, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.407064], dtype=float32), 1.176272]. 
=============================================
[2019-04-03 22:26:48,741] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.4012233e-12 1.4903216e-05 9.2177543e-11 1.5842005e-08 2.2288448e-05
 3.6322861e-12 9.9996281e-01], sum to 1.0000
[2019-04-03 22:26:48,742] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9277
[2019-04-03 22:26:48,767] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.666666666666667, 71.0, 0.0, 0.0, 26.0, 25.21310398357436, 0.3412854306715429, 0.0, 1.0, 41096.08143402708], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3907200.0000, 
sim time next is 3907800.0000, 
raw observation next is [-5.0, 68.0, 0.0, 0.0, 26.0, 25.15070205857801, 0.3248468359466415, 0.0, 1.0, 41238.96076216097], 
processed observation next is [1.0, 0.21739130434782608, 0.32409972299168976, 0.68, 0.0, 0.0, 0.6666666666666666, 0.595891838214834, 0.6082822786488805, 0.0, 1.0, 0.19637600362933794], 
reward next is 0.8036, 
noisyNet noise sample is [array([0.6928429], dtype=float32), -0.6778521]. 
=============================================
[2019-04-03 22:26:51,087] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.8499248e-11 1.3458289e-05 1.0123268e-09 1.6897228e-07 4.4171778e-05
 1.7733903e-10 9.9994218e-01], sum to 1.0000
[2019-04-03 22:26:51,088] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8280
[2019-04-03 22:26:51,103] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-13.0, 66.0, 0.0, 0.0, 26.0, 23.70026795885194, 0.00801126805090649, 0.0, 1.0, 43787.43261170352], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3994200.0000, 
sim time next is 3994800.0000, 
raw observation next is [-13.0, 65.0, 0.0, 0.0, 26.0, 23.62336415586986, -0.007242373483682761, 0.0, 1.0, 43757.37639546395], 
processed observation next is [1.0, 0.21739130434782608, 0.10249307479224376, 0.65, 0.0, 0.0, 0.6666666666666666, 0.4686136796558218, 0.49758587550543903, 0.0, 1.0, 0.2083684590260188], 
reward next is 0.7916, 
noisyNet noise sample is [array([-0.5189956], dtype=float32), 2.5671637]. 
=============================================
[2019-04-03 22:26:52,196] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.5166833e-10 6.9487491e-04 3.3193881e-10 3.2003376e-07 2.7480963e-04
 8.3375071e-11 9.9902999e-01], sum to 1.0000
[2019-04-03 22:26:52,196] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.6023
[2019-04-03 22:26:52,231] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.666666666666667, 35.0, 113.0, 755.0, 26.0, 25.25486275987633, 0.3957936463478612, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4184400.0000, 
sim time next is 4185000.0000, 
raw observation next is [-1.5, 35.0, 114.0, 774.0, 26.0, 25.24754863239799, 0.3935908478723674, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.4210526315789474, 0.35, 0.38, 0.8552486187845304, 0.6666666666666666, 0.6039623860331659, 0.6311969492907892, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.0666273], dtype=float32), -0.8598968]. 
=============================================
[2019-04-03 22:26:52,234] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[82.97965 ]
 [83.07584 ]
 [83.04169 ]
 [83.03438 ]
 [83.066895]], R is [[83.10829926]
 [83.27721405]
 [83.44444275]
 [83.61000061]
 [83.77390289]].
[2019-04-03 22:26:53,322] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.4003908e-11 3.8933833e-04 9.4484948e-11 1.1945549e-07 5.8752488e-05
 2.6963464e-11 9.9955183e-01], sum to 1.0000
[2019-04-03 22:26:53,323] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6733
[2019-04-03 22:26:53,343] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 30.0, 118.5, 834.5, 26.0, 25.10590439556877, 0.3950250270600444, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4190400.0000, 
sim time next is 4191000.0000, 
raw observation next is [1.166666666666667, 30.66666666666666, 118.3333333333333, 838.6666666666667, 26.0, 25.14180591219151, 0.3977330248319351, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.49492151431209613, 0.3066666666666666, 0.3944444444444443, 0.9267034990791898, 0.6666666666666666, 0.595150492682626, 0.6325776749439783, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6168677], dtype=float32), 0.6120779]. 
=============================================
[2019-04-03 22:26:53,351] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[83.450836]
 [83.45191 ]
 [83.273796]
 [83.330826]
 [83.3352  ]], R is [[83.60797882]
 [83.77189636]
 [83.78522491]
 [83.8511734 ]
 [84.01266479]].
[2019-04-03 22:27:05,150] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.7570962e-11 7.9343590e-05 3.1124789e-10 1.2981766e-07 9.3189963e-05
 3.5951575e-11 9.9982733e-01], sum to 1.0000
[2019-04-03 22:27:05,152] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2957
[2019-04-03 22:27:05,163] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 39.0, 0.0, 0.0, 26.0, 25.38941200165718, 0.4249194526689545, 0.0, 1.0, 43759.15916760438], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4150800.0000, 
sim time next is 4151400.0000, 
raw observation next is [-1.166666666666667, 40.16666666666666, 0.0, 0.0, 26.0, 25.35962916969284, 0.4191638104431289, 0.0, 1.0, 51457.82841894306], 
processed observation next is [0.0, 0.043478260869565216, 0.43028624192059095, 0.40166666666666656, 0.0, 0.0, 0.6666666666666666, 0.6133024308077367, 0.6397212701477096, 0.0, 1.0, 0.24503727818544316], 
reward next is 0.7550, 
noisyNet noise sample is [array([2.1392055], dtype=float32), 0.8083228]. 
=============================================
[2019-04-03 22:27:10,316] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.12996504e-13 1.05658974e-05 1.30417534e-11 2.31147030e-08
 2.65033777e-05 7.11326939e-13 9.99962926e-01], sum to 1.0000
[2019-04-03 22:27:10,316] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2472
[2019-04-03 22:27:10,330] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 47.0, 264.0, 113.0, 26.0, 26.24306380271662, 0.607249771004142, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4545000.0000, 
sim time next is 4545600.0000, 
raw observation next is [3.0, 46.33333333333334, 245.5, 96.16666666666667, 26.0, 26.33292570271053, 0.619845780214018, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.46333333333333343, 0.8183333333333334, 0.10626151012891345, 0.6666666666666666, 0.6944104752258774, 0.7066152600713393, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.11612133], dtype=float32), 0.2704333]. 
=============================================
[2019-04-03 22:27:11,262] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.9152623e-14 5.5520777e-06 1.3496044e-11 2.8769986e-09 3.1582018e-05
 1.4247574e-14 9.9996281e-01], sum to 1.0000
[2019-04-03 22:27:11,262] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4371
[2019-04-03 22:27:11,270] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.46666666666667, 58.66666666666666, 0.0, 0.0, 26.0, 26.8387884990618, 0.8398759333643487, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4394400.0000, 
sim time next is 4395000.0000, 
raw observation next is [10.33333333333333, 58.83333333333334, 0.0, 0.0, 26.0, 26.81330639738505, 0.8296542632863207, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.7488457987072946, 0.5883333333333334, 0.0, 0.0, 0.6666666666666666, 0.7344421997820874, 0.7765514210954403, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1913117], dtype=float32), 0.073568515]. 
=============================================
[2019-04-03 22:27:11,289] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[89.972046]
 [91.61651 ]
 [93.2076  ]
 [95.77726 ]
 [95.26294 ]], R is [[89.22570801]
 [89.33345032]
 [89.44011688]
 [89.54571533]
 [89.65026093]].
[2019-04-03 22:27:13,163] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.3547101e-13 5.1083498e-06 7.8500791e-12 2.7889535e-09 6.0410713e-07
 5.0705140e-14 9.9999428e-01], sum to 1.0000
[2019-04-03 22:27:13,164] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5814
[2019-04-03 22:27:13,175] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 83.83333333333334, 75.66666666666666, 0.0, 26.0, 26.03885453954243, 0.5770653484822278, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4461000.0000, 
sim time next is 4461600.0000, 
raw observation next is [0.0, 82.66666666666667, 73.33333333333334, 0.0, 26.0, 26.1593564227452, 0.5841969893587414, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.46260387811634357, 0.8266666666666667, 0.24444444444444446, 0.0, 0.6666666666666666, 0.6799463685621001, 0.6947323297862472, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.289011], dtype=float32), 0.79614526]. 
=============================================
[2019-04-03 22:27:14,359] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.9523809e-12 1.3551065e-04 5.9445344e-11 2.1116309e-08 2.1323769e-05
 4.1195580e-12 9.9984312e-01], sum to 1.0000
[2019-04-03 22:27:14,360] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9988
[2019-04-03 22:27:14,370] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.199999999999999, 73.0, 0.0, 0.0, 26.0, 25.80800206419864, 0.4683724836443848, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4308000.0000, 
sim time next is 4308600.0000, 
raw observation next is [5.15, 73.0, 0.0, 0.0, 26.0, 25.8127497409524, 0.4599754450412316, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.605263157894737, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6510624784126998, 0.6533251483470772, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.92666], dtype=float32), 0.1982443]. 
=============================================
[2019-04-03 22:27:15,064] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.7889063e-13 2.1084113e-06 7.5168856e-12 1.8199253e-09 6.1052610e-06
 2.1171383e-13 9.9999177e-01], sum to 1.0000
[2019-04-03 22:27:15,070] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9832
[2019-04-03 22:27:15,078] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.65, 61.83333333333334, 0.0, 0.0, 26.0, 25.88000330412989, 0.5991473585904927, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4402200.0000, 
sim time next is 4402800.0000, 
raw observation next is [8.5, 62.0, 0.0, 0.0, 26.0, 25.79410833696405, 0.5817917560375389, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.698060941828255, 0.62, 0.0, 0.0, 0.6666666666666666, 0.6495090280803376, 0.6939305853458463, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3543772], dtype=float32), -0.4188246]. 
=============================================
[2019-04-03 22:27:20,433] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [4.0048417e-13 1.5863836e-07 5.8357108e-12 4.7174806e-09 1.4032970e-06
 9.8921664e-14 9.9999845e-01], sum to 1.0000
[2019-04-03 22:27:20,433] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.1822
[2019-04-03 22:27:20,463] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 52.0, 23.33333333333334, 23.33333333333334, 26.0, 25.46195803628043, 0.4883200331822358, 1.0, 1.0, 32983.4222461405], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4556400.0000, 
sim time next is 4557000.0000, 
raw observation next is [2.0, 52.0, 18.66666666666667, 18.66666666666667, 26.0, 25.38644441323368, 0.4965337276876782, 1.0, 1.0, 20889.50218544856], 
processed observation next is [1.0, 0.7391304347826086, 0.518005540166205, 0.52, 0.06222222222222224, 0.02062615101289135, 0.6666666666666666, 0.6155370344361399, 0.6655112425625594, 1.0, 1.0, 0.09947381993070743], 
reward next is 0.9005, 
noisyNet noise sample is [array([-2.1111288], dtype=float32), 0.57272977]. 
=============================================
[2019-04-03 22:27:20,480] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[79.00079 ]
 [79.02325 ]
 [79.38757 ]
 [79.810036]
 [80.15155 ]], R is [[79.22866058]
 [79.27931213]
 [79.48651886]
 [79.69165802]
 [79.89474487]].
[2019-04-03 22:27:25,829] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.88747759e-12 1.92480893e-05 1.22190216e-10 9.49909307e-09
 2.15108976e-05 1.28375045e-11 9.99959230e-01], sum to 1.0000
[2019-04-03 22:27:25,832] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7991
[2019-04-03 22:27:25,845] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.4, 73.0, 0.0, 0.0, 26.0, 25.15226614970445, 0.3719084486936102, 0.0, 1.0, 36217.98374276456], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4599600.0000, 
sim time next is 4600200.0000, 
raw observation next is [-2.5, 73.5, 0.0, 0.0, 26.0, 25.17680913442195, 0.3751512845094696, 0.0, 1.0, 36188.79822820047], 
processed observation next is [1.0, 0.21739130434782608, 0.39335180055401664, 0.735, 0.0, 0.0, 0.6666666666666666, 0.598067427868496, 0.6250504281698231, 0.0, 1.0, 0.1723276106104784], 
reward next is 0.8277, 
noisyNet noise sample is [array([-0.73358524], dtype=float32), -0.08095623]. 
=============================================
[2019-04-03 22:27:27,918] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.78280798e-13 2.75628054e-06 9.16794527e-13 1.91130622e-09
 2.43413774e-06 1.02369454e-13 9.99994874e-01], sum to 1.0000
[2019-04-03 22:27:27,920] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4723
[2019-04-03 22:27:27,938] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.95, 49.83333333333334, 200.3333333333333, 442.3333333333334, 26.0, 27.29536613160755, 0.8653869369967474, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4632600.0000, 
sim time next is 4633200.0000, 
raw observation next is [5.0, 50.0, 199.0, 364.0, 26.0, 27.39840751110675, 0.8812882459185506, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.6011080332409973, 0.5, 0.6633333333333333, 0.4022099447513812, 0.6666666666666666, 0.7832006259255625, 0.7937627486395168, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.42898637], dtype=float32), -0.34394258]. 
=============================================
[2019-04-03 22:27:33,853] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.76590252e-12 9.49564583e-06 1.10861815e-11 1.82541164e-08
 6.18348931e-05 3.32177320e-13 9.99928594e-01], sum to 1.0000
[2019-04-03 22:27:33,855] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0306
[2019-04-03 22:27:33,874] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.3333333333333333, 90.0, 212.1666666666667, 6.0, 26.0, 26.37231166196202, 0.5836777877907067, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4706400.0000, 
sim time next is 4707000.0000, 
raw observation next is [0.5, 89.0, 213.0, 6.0, 26.0, 26.36268267023568, 0.5748349739224716, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4764542936288089, 0.89, 0.71, 0.0066298342541436465, 0.6666666666666666, 0.6968902225196402, 0.6916116579741572, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.29105633], dtype=float32), 1.6134764]. 
=============================================
[2019-04-03 22:27:33,877] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[91.830475]
 [92.10567 ]
 [92.34944 ]
 [92.54667 ]
 [92.63522 ]], R is [[91.43565369]
 [91.52130127]
 [91.60608673]
 [91.69002533]
 [91.77312469]].
[2019-04-03 22:27:34,667] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-03 22:27:34,667] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-03 22:27:34,667] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:27:34,667] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-03 22:27:34,669] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:27:34,669] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-03 22:27:34,670] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run5
[2019-04-03 22:27:34,686] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:27:34,695] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run5
[2019-04-03 22:27:34,713] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run5
[2019-04-03 22:29:29,566] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.18832287], dtype=float32), 0.25546092]
[2019-04-03 22:29:29,567] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-5.0, 59.0, 0.0, 0.0, 26.0, 25.18841171285705, 0.3965468159810714, 1.0, 1.0, 58629.62185360191]
[2019-04-03 22:29:29,567] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-03 22:29:29,568] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.53025118e-11 6.94320115e-05 1.06358228e-10 1.00632626e-07
 1.22791520e-04 6.64054619e-12 9.99807775e-01], sampled 0.007695412520608458
[2019-04-03 22:30:18,100] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.8442 239852718.2087 1606.0366
[2019-04-03 22:30:48,527] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.3665 263473042.3734 1557.0848
[2019-04-03 22:30:53,713] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.9756 275735130.1179 1232.8990
[2019-04-03 22:30:54,751] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 400000, evaluation results [400000.0, 7241.366464888587, 263473042.37339455, 1557.0848052692845, 7353.8441990060055, 239852718.20873997, 1606.0365541615429, 7182.975570867066, 275735130.11792564, 1232.8989907060786]
[2019-04-03 22:30:56,755] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.6188869e-11 1.1368934e-03 1.7478550e-09 2.9834410e-07 2.4048315e-04
 5.1572563e-10 9.9862230e-01], sum to 1.0000
[2019-04-03 22:30:56,755] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6228
[2019-04-03 22:30:56,805] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.47121387225328, 0.2378643794269926, 0.0, 1.0, 40833.68626698024], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4765800.0000, 
sim time next is 4766400.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.42487278074377, 0.2286244388440725, 0.0, 1.0, 40924.50934480098], 
processed observation next is [0.0, 0.17391304347826086, 0.296398891966759, 0.92, 0.0, 0.0, 0.6666666666666666, 0.5354060650619807, 0.5762081462813575, 0.0, 1.0, 0.1948786159276237], 
reward next is 0.8051, 
noisyNet noise sample is [array([0.8738107], dtype=float32), 0.09564073]. 
=============================================
[2019-04-03 22:31:00,526] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.3779038e-10 7.9153600e-04 7.6766987e-10 2.6247803e-06 2.4221172e-03
 8.4881255e-11 9.9678373e-01], sum to 1.0000
[2019-04-03 22:31:00,526] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3213
[2019-04-03 22:31:00,569] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 45.0, 142.6666666666667, 387.0, 26.0, 25.11733888571968, 0.3712847393282085, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4895400.0000, 
sim time next is 4896000.0000, 
raw observation next is [3.0, 45.0, 132.5, 369.5, 26.0, 25.12200256961361, 0.3724107737053989, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.5457063711911359, 0.45, 0.44166666666666665, 0.40828729281767956, 0.6666666666666666, 0.5935002141344675, 0.6241369245684663, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4626094], dtype=float32), 0.049345795]. 
=============================================
[2019-04-03 22:31:00,582] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[77.81625 ]
 [78.07494 ]
 [78.31697 ]
 [78.53516 ]
 [78.775116]], R is [[77.77020264]
 [77.99250031]
 [78.21257782]
 [78.43045044]
 [78.64614868]].
[2019-04-03 22:31:11,609] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:31:11,609] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:31:11,611] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run4
[2019-04-03 22:31:11,865] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:31:11,865] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:31:11,867] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run4
[2019-04-03 22:31:13,470] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:31:13,471] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:31:13,472] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run4
[2019-04-03 22:31:14,637] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:31:14,637] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:31:14,757] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run4
[2019-04-03 22:31:23,277] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5965358e-07 2.8630389e-02 1.3362442e-06 1.7481112e-04 2.5811533e-03
 3.3404194e-07 9.6861178e-01], sum to 1.0000
[2019-04-03 22:31:23,277] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8187
[2019-04-03 22:31:23,310] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.200000000000001, 96.0, 0.0, 0.0, 26.0, 20.3823294811375, -0.7711514690131991, 0.0, 1.0, 43268.82661251873], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 8400.0000, 
sim time next is 9000.0000, 
raw observation next is [7.2, 96.0, 0.0, 0.0, 26.0, 20.45050668558164, -0.760783523060803, 0.0, 1.0, 43011.68768994651], 
processed observation next is [0.0, 0.08695652173913043, 0.662049861495845, 0.96, 0.0, 0.0, 0.6666666666666666, 0.20420889046513668, 0.24640549231306566, 0.0, 1.0, 0.20481756042831673], 
reward next is 0.7952, 
noisyNet noise sample is [array([0.9660485], dtype=float32), 1.254142]. 
=============================================
[2019-04-03 22:31:23,334] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[59.53053 ]
 [56.493534]
 [53.613018]
 [50.38683 ]
 [46.814453]], R is [[62.78957748]
 [62.95563889]
 [63.11878204]
 [63.27894974]
 [63.43599319]].
[2019-04-03 22:31:26,239] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:31:26,239] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:31:26,241] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run4
[2019-04-03 22:31:27,142] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.33159672e-12 2.45364390e-06 1.92051028e-11 1.32814808e-08
 1.17595555e-05 2.96686937e-13 9.99985695e-01], sum to 1.0000
[2019-04-03 22:31:27,142] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7007
[2019-04-03 22:31:27,243] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.0, 23.0, 0.0, 0.0, 26.0, 26.04836037175352, 0.5869964218774111, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4996800.0000, 
sim time next is 4997400.0000, 
raw observation next is [5.666666666666667, 24.0, 0.0, 0.0, 26.0, 25.98634234957513, 0.5709965400630156, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.6195752539242845, 0.24, 0.0, 0.0, 0.6666666666666666, 0.6655285291312607, 0.6903321800210053, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.27628642], dtype=float32), 0.076439045]. 
=============================================
[2019-04-03 22:31:31,644] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:31:31,645] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:31:31,648] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run4
[2019-04-03 22:31:32,211] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:31:32,211] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:31:32,213] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run4
[2019-04-03 22:31:32,974] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:31:32,974] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:31:32,976] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run4
[2019-04-03 22:31:33,249] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.3244382e-13 1.3014444e-05 3.6326222e-12 2.4392028e-09 3.3033686e-07
 2.0177047e-13 9.9998665e-01], sum to 1.0000
[2019-04-03 22:31:33,249] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8551
[2019-04-03 22:31:33,288] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.666666666666667, 45.0, 109.5, 670.5, 26.0, 26.53000295065297, 0.6247592569162822, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5044800.0000, 
sim time next is 5045400.0000, 
raw observation next is [2.0, 44.0, 112.0, 698.0, 26.0, 26.61348711462023, 0.6505322363207164, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.518005540166205, 0.44, 0.37333333333333335, 0.7712707182320442, 0.6666666666666666, 0.7177905928850192, 0.7168440787735721, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0817776], dtype=float32), -0.37783843]. 
=============================================
[2019-04-03 22:31:34,849] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:31:34,849] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:31:34,852] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run4
[2019-04-03 22:31:36,673] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:31:36,673] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:31:36,675] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run4
[2019-04-03 22:31:39,749] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:31:39,749] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:31:39,806] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run4
[2019-04-03 22:31:39,987] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:31:39,988] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:31:39,989] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run4
[2019-04-03 22:31:40,237] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:31:40,237] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:31:40,239] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run4
[2019-04-03 22:31:43,433] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:31:43,433] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:31:43,435] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run4
[2019-04-03 22:31:44,265] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:31:44,265] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:31:44,267] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run4
[2019-04-03 22:31:48,454] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.1757782e-11 3.3560407e-04 1.8460800e-10 3.2722522e-07 1.5793419e-04
 1.5457790e-11 9.9950612e-01], sum to 1.0000
[2019-04-03 22:31:48,454] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4537
[2019-04-03 22:31:48,550] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.033333333333333, 77.0, 71.50000000000001, 49.33333333333332, 26.0, 25.35996607478111, 0.2270609033574146, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 206400.0000, 
sim time next is 207000.0000, 
raw observation next is [-7.85, 76.5, 79.0, 0.0, 26.0, 25.40632516551008, 0.2209623480160316, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.24515235457063714, 0.765, 0.2633333333333333, 0.0, 0.6666666666666666, 0.6171937637925066, 0.5736541160053439, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8980204], dtype=float32), -0.9102019]. 
=============================================
[2019-04-03 22:31:48,584] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[84.99301 ]
 [85.81856 ]
 [86.503006]
 [87.20105 ]
 [87.52939 ]], R is [[84.4076004 ]
 [84.56352234]
 [84.71788788]
 [84.87071228]
 [85.02200317]].
[2019-04-03 22:31:48,736] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:31:48,736] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:31:48,738] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run4
[2019-04-03 22:32:10,444] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.1482081e-10 5.8197307e-05 3.9551997e-09 5.8620475e-07 2.0071197e-05
 7.8437812e-10 9.9992120e-01], sum to 1.0000
[2019-04-03 22:32:10,449] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7762
[2019-04-03 22:32:10,484] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-14.2, 67.5, 0.0, 0.0, 26.0, 23.39061031175273, -0.08735181387662495, 0.0, 1.0, 47518.81597000534], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 347400.0000, 
sim time next is 348000.0000, 
raw observation next is [-14.3, 68.0, 0.0, 0.0, 26.0, 23.32008352248707, -0.09518261907766423, 0.0, 1.0, 47583.25219810135], 
processed observation next is [1.0, 0.0, 0.06648199445983377, 0.68, 0.0, 0.0, 0.6666666666666666, 0.44334029354058924, 0.4682724603074453, 0.0, 1.0, 0.22658691522905405], 
reward next is 0.7734, 
noisyNet noise sample is [array([0.0352121], dtype=float32), 0.2515182]. 
=============================================
[2019-04-03 22:32:10,510] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[62.06089 ]
 [62.18702 ]
 [62.371025]
 [62.544594]
 [62.726482]], R is [[62.05063248]
 [62.20384598]
 [62.35585785]
 [62.50669479]
 [62.65637207]].
[2019-04-03 22:32:19,346] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.8876641e-11 6.1461462e-05 1.5329767e-09 4.7769625e-08 4.8580023e-05
 6.6672272e-11 9.9988985e-01], sum to 1.0000
[2019-04-03 22:32:19,347] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0205
[2019-04-03 22:32:19,402] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 26.0, 23.08898998626671, -0.1589418719373603, 0.0, 1.0, 44279.72124114567], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 187800.0000, 
sim time next is 188400.0000, 
raw observation next is [-8.900000000000002, 78.0, 0.0, 0.0, 26.0, 23.05167048102818, -0.1725415007231773, 0.0, 1.0, 44322.96510428788], 
processed observation next is [1.0, 0.17391304347826086, 0.2160664819944598, 0.78, 0.0, 0.0, 0.6666666666666666, 0.4209725400856816, 0.4424861664256075, 0.0, 1.0, 0.21106173859184707], 
reward next is 0.7889, 
noisyNet noise sample is [array([-0.347222], dtype=float32), -0.62316686]. 
=============================================
[2019-04-03 22:32:24,794] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.4163839e-12 9.9009349e-06 6.6718742e-11 2.3646248e-08 1.3801690e-05
 2.9297400e-12 9.9997628e-01], sum to 1.0000
[2019-04-03 22:32:24,794] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9103
[2019-04-03 22:32:24,808] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.966666666666667, 71.0, 0.0, 0.0, 26.0, 24.12413343229557, 0.08545579177673328, 0.0, 1.0, 44565.29230686714], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 261600.0000, 
sim time next is 262200.0000, 
raw observation next is [-6.333333333333334, 69.0, 0.0, 0.0, 26.0, 24.14993192803506, 0.0812965499261787, 0.0, 1.0, 44683.84938549842], 
processed observation next is [1.0, 0.0, 0.28716528162511545, 0.69, 0.0, 0.0, 0.6666666666666666, 0.5124943273362549, 0.5270988499753929, 0.0, 1.0, 0.2127802351690401], 
reward next is 0.7872, 
noisyNet noise sample is [array([0.23995891], dtype=float32), 0.5673977]. 
=============================================
[2019-04-03 22:32:28,824] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.6735500e-11 1.8146816e-06 9.5584769e-11 1.0066591e-07 6.5671379e-06
 1.2894551e-12 9.9999154e-01], sum to 1.0000
[2019-04-03 22:32:28,824] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3023
[2019-04-03 22:32:28,904] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.96666666666667, 51.66666666666667, 0.0, 0.0, 26.0, 25.69274636783045, 0.3921062531843091, 1.0, 1.0, 155980.3398587133], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 321600.0000, 
sim time next is 322200.0000, 
raw observation next is [-11.15, 53.0, 0.0, 0.0, 26.0, 25.69567113570359, 0.4011654208686277, 1.0, 1.0, 106320.850756951], 
processed observation next is [1.0, 0.7391304347826086, 0.15373961218836565, 0.53, 0.0, 0.0, 0.6666666666666666, 0.6413059279752993, 0.6337218069562093, 1.0, 1.0, 0.5062897655092905], 
reward next is 0.4937, 
noisyNet noise sample is [array([0.41796267], dtype=float32), 0.96763265]. 
=============================================
[2019-04-03 22:32:29,068] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.5327128e-11 1.9149590e-04 1.5048561e-09 1.5614161e-07 4.5742829e-05
 4.4584690e-11 9.9976259e-01], sum to 1.0000
[2019-04-03 22:32:29,068] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.5597
[2019-04-03 22:32:29,099] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-14.5, 69.0, 0.0, 0.0, 26.0, 23.26414369170375, -0.1081439439529799, 0.0, 1.0, 47675.46836704694], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 349200.0000, 
sim time next is 349800.0000, 
raw observation next is [-14.58333333333333, 69.0, 0.0, 0.0, 26.0, 23.26869452671967, -0.1139947866018163, 0.0, 1.0, 47726.69404553359], 
processed observation next is [1.0, 0.043478260869565216, 0.05863342566943682, 0.69, 0.0, 0.0, 0.6666666666666666, 0.4390578772266392, 0.4620017377993946, 0.0, 1.0, 0.22726997164539806], 
reward next is 0.7727, 
noisyNet noise sample is [array([1.8016001], dtype=float32), 0.5817062]. 
=============================================
[2019-04-03 22:32:31,662] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.7487649e-11 1.4763454e-04 2.6483600e-09 8.7547761e-07 1.9134211e-04
 1.0047000e-09 9.9966013e-01], sum to 1.0000
[2019-04-03 22:32:31,662] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4896
[2019-04-03 22:32:31,696] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-15.8, 74.66666666666667, 0.0, 0.0, 26.0, 22.18240022868527, -0.3911389787929263, 0.0, 1.0, 48635.43810963229], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 364800.0000, 
sim time next is 365400.0000, 
raw observation next is [-15.9, 75.5, 0.0, 0.0, 26.0, 22.09260767470404, -0.3987692275161987, 0.0, 1.0, 48616.22180920339], 
processed observation next is [1.0, 0.21739130434782608, 0.02216066481994457, 0.755, 0.0, 0.0, 0.6666666666666666, 0.3410506395586701, 0.3670769241612671, 0.0, 1.0, 0.23150581813906376], 
reward next is 0.7685, 
noisyNet noise sample is [array([-0.5174216], dtype=float32), 0.4925035]. 
=============================================
[2019-04-03 22:32:37,822] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.9448263e-14 1.8758985e-06 1.2728176e-12 1.9971740e-09 5.1408682e-07
 1.3964734e-14 9.9999762e-01], sum to 1.0000
[2019-04-03 22:32:37,822] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5088
[2019-04-03 22:32:37,849] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.1, 94.66666666666667, 0.0, 0.0, 26.0, 24.84140936145365, 0.23286656900516, 0.0, 1.0, 40669.4721206519], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 513600.0000, 
sim time next is 514200.0000, 
raw observation next is [3.2, 95.33333333333333, 0.0, 0.0, 26.0, 24.83883540037029, 0.2328424515335057, 0.0, 1.0, 40535.159348651], 
processed observation next is [1.0, 0.9565217391304348, 0.551246537396122, 0.9533333333333333, 0.0, 0.0, 0.6666666666666666, 0.5699029500308574, 0.5776141505111686, 0.0, 1.0, 0.19302456832690953], 
reward next is 0.8070, 
noisyNet noise sample is [array([-0.7146924], dtype=float32), 0.9374203]. 
=============================================
[2019-04-03 22:32:57,302] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.3931113e-13 1.9296633e-04 3.7514294e-11 1.4366677e-08 2.5967076e-06
 1.4238160e-12 9.9980444e-01], sum to 1.0000
[2019-04-03 22:32:57,302] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1388
[2019-04-03 22:32:57,393] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 91.0, 89.0, 103.5, 26.0, 25.12127962601887, 0.2950435476437003, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 550800.0000, 
sim time next is 551400.0000, 
raw observation next is [-0.09999999999999999, 90.33333333333334, 107.3333333333333, 103.3333333333333, 26.0, 25.08837845845053, 0.2880457744596266, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.4598337950138504, 0.9033333333333334, 0.3577777777777777, 0.11418047882136276, 0.6666666666666666, 0.5906982048708777, 0.5960152581532089, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0056915], dtype=float32), 0.21049738]. 
=============================================
[2019-04-03 22:32:59,206] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.5614090e-12 1.3629187e-04 2.2700872e-10 2.3020075e-07 1.6378764e-04
 1.9639700e-11 9.9969971e-01], sum to 1.0000
[2019-04-03 22:32:59,207] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5646
[2019-04-03 22:32:59,286] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 87.0, 0.0, 0.0, 26.0, 24.93652700834915, 0.279929529823899, 0.0, 1.0, 55423.96114403652], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 586800.0000, 
sim time next is 587400.0000, 
raw observation next is [-2.8, 87.0, 0.0, 0.0, 26.0, 24.94051795388465, 0.2840133886212677, 0.0, 1.0, 45983.22836862898], 
processed observation next is [0.0, 0.8260869565217391, 0.38504155124653744, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5783764961570542, 0.5946711295404226, 0.0, 1.0, 0.2189677541363285], 
reward next is 0.7810, 
noisyNet noise sample is [array([-0.25240076], dtype=float32), 0.038788434]. 
=============================================
[2019-04-03 22:33:00,429] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.8850056e-12 9.0483394e-05 9.7134731e-12 3.9449819e-08 2.0071000e-04
 2.0298041e-12 9.9970871e-01], sum to 1.0000
[2019-04-03 22:33:00,430] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8243
[2019-04-03 22:33:00,503] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.2, 82.0, 122.5, 401.3333333333334, 26.0, 25.03089374951435, 0.352769816312589, 0.0, 1.0, 18721.56855304877], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 571200.0000, 
sim time next is 571800.0000, 
raw observation next is [-1.2, 82.5, 118.0, 335.6666666666667, 26.0, 25.03036711223281, 0.3433812288695368, 0.0, 1.0, 18720.32766375913], 
processed observation next is [0.0, 0.6086956521739131, 0.42936288088642666, 0.825, 0.3933333333333333, 0.370902394106814, 0.6666666666666666, 0.5858639260194008, 0.6144604096231789, 0.0, 1.0, 0.08914441744647204], 
reward next is 0.9109, 
noisyNet noise sample is [array([-0.22749159], dtype=float32), 1.4131557]. 
=============================================
[2019-04-03 22:33:02,825] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [8.04756601e-15 1.27055555e-05 2.24635875e-13 1.23314503e-09
 9.55224550e-06 2.15691671e-14 9.99977708e-01], sum to 1.0000
[2019-04-03 22:33:02,825] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1076
[2019-04-03 22:33:02,843] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 58.5, 99.66666666666666, 669.0, 26.0, 25.86307608612963, 0.3895737347697715, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 733800.0000, 
sim time next is 734400.0000, 
raw observation next is [-0.6, 57.0, 107.5, 614.0, 26.0, 25.84242318358226, 0.3889886225688682, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.44598337950138506, 0.57, 0.35833333333333334, 0.6784530386740332, 0.6666666666666666, 0.6535352652985216, 0.6296628741896227, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6643582], dtype=float32), -1.6879895]. 
=============================================
[2019-04-03 22:33:05,000] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.61159484e-13 1.35324983e-04 5.74943467e-13 1.02913784e-08
 1.04198036e-04 7.65526761e-14 9.99760449e-01], sum to 1.0000
[2019-04-03 22:33:05,000] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5075
[2019-04-03 22:33:05,085] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.2333333333333334, 54.66666666666667, 123.1666666666667, 504.0, 26.0, 25.79166270175036, 0.3739685885639761, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 735600.0000, 
sim time next is 736200.0000, 
raw observation next is [-0.04999999999999999, 53.5, 131.0, 449.0, 26.0, 25.74934045993063, 0.3610843215657347, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.461218836565097, 0.535, 0.43666666666666665, 0.49613259668508286, 0.6666666666666666, 0.6457783716608857, 0.6203614405219116, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.009751], dtype=float32), 0.19087431]. 
=============================================
[2019-04-03 22:33:54,837] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.8372662e-13 2.0126190e-05 1.4681591e-12 4.8044861e-09 3.3475453e-06
 1.4810343e-13 9.9997652e-01], sum to 1.0000
[2019-04-03 22:33:54,861] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5167
[2019-04-03 22:33:54,871] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.2, 62.33333333333334, 0.0, 0.0, 26.0, 25.80543688720109, 0.7033146365648218, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1113000.0000, 
sim time next is 1113600.0000, 
raw observation next is [13.1, 62.66666666666667, 0.0, 0.0, 26.0, 25.8411887398038, 0.7008416150309588, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.8254847645429363, 0.6266666666666667, 0.0, 0.0, 0.6666666666666666, 0.6534323949836501, 0.7336138716769862, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1900343], dtype=float32), -0.81237274]. 
=============================================
[2019-04-03 22:34:01,137] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.6679267e-15 1.9902918e-05 5.2468609e-13 1.4160494e-09 1.1583473e-06
 1.2510494e-13 9.9997890e-01], sum to 1.0000
[2019-04-03 22:34:01,138] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0534
[2019-04-03 22:34:01,162] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.2, 83.0, 18.5, 58.66666666666666, 26.0, 25.90886324721222, 0.6510686003811065, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1066800.0000, 
sim time next is 1067400.0000, 
raw observation next is [12.2, 83.0, 22.0, 69.0, 26.0, 26.06846783951909, 0.6821834736301109, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.8005540166204987, 0.83, 0.07333333333333333, 0.07624309392265194, 0.6666666666666666, 0.6723723199599242, 0.727394491210037, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8644181], dtype=float32), 0.78731096]. 
=============================================
[2019-04-03 22:34:06,635] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.7419158e-13 6.3461099e-05 8.5136542e-12 8.4699687e-09 1.8680595e-06
 1.4958517e-12 9.9993467e-01], sum to 1.0000
[2019-04-03 22:34:06,637] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1351
[2019-04-03 22:34:06,663] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.6, 79.0, 0.0, 0.0, 26.0, 25.69791661686843, 0.6185968810850114, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1142400.0000, 
sim time next is 1143000.0000, 
raw observation next is [11.6, 80.0, 0.0, 0.0, 26.0, 25.69408047647835, 0.6138907990043675, 0.0, 1.0, 18721.72874102454], 
processed observation next is [0.0, 0.21739130434782608, 0.7839335180055402, 0.8, 0.0, 0.0, 0.6666666666666666, 0.6411733730398627, 0.7046302663347892, 0.0, 1.0, 0.089151089242974], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.96694285], dtype=float32), 1.2039814]. 
=============================================
[2019-04-03 22:34:06,708] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[91.46791 ]
 [91.53715 ]
 [91.51635 ]
 [91.493324]
 [91.43047 ]], R is [[91.52581024]
 [91.61054993]
 [91.60527802]
 [91.58770752]
 [91.53862   ]].
[2019-04-03 22:34:11,239] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.4076479e-14 6.6473735e-06 4.1424598e-12 6.1522959e-10 2.1830972e-07
 2.5854793e-13 9.9999309e-01], sum to 1.0000
[2019-04-03 22:34:11,244] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0736
[2019-04-03 22:34:11,262] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.2, 92.66666666666667, 0.0, 0.0, 26.0, 25.43554916510456, 0.4889175833930757, 0.0, 1.0, 18764.61200768632], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1477200.0000, 
sim time next is 1477800.0000, 
raw observation next is [2.2, 93.0, 0.0, 0.0, 26.0, 25.50531379848363, 0.4808908841697459, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.5235457063711911, 0.93, 0.0, 0.0, 0.6666666666666666, 0.6254428165403025, 0.6602969613899153, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.09455795], dtype=float32), -2.1217892]. 
=============================================
[2019-04-03 22:34:14,508] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.4318096e-14 2.2278505e-06 1.1529482e-11 1.3267910e-09 2.8014293e-08
 2.4122425e-13 9.9999774e-01], sum to 1.0000
[2019-04-03 22:34:14,514] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0183
[2019-04-03 22:34:14,531] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 26.0, 25.48871233591588, 0.5215442137621845, 0.0, 1.0, 40754.9296333681], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1317600.0000, 
sim time next is 1318200.0000, 
raw observation next is [1.516666666666667, 92.0, 0.0, 0.0, 26.0, 25.40570659932159, 0.5130729119506064, 0.0, 1.0, 81657.49092201325], 
processed observation next is [1.0, 0.2608695652173913, 0.5046168051708219, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6171422166101325, 0.6710243039835354, 0.0, 1.0, 0.3888451948667298], 
reward next is 0.6112, 
noisyNet noise sample is [array([-0.6622292], dtype=float32), 0.48821345]. 
=============================================
[2019-04-03 22:34:21,978] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [6.3464418e-14 2.6995442e-06 1.6040704e-12 1.3896558e-09 1.7386236e-08
 1.6229531e-13 9.9999726e-01], sum to 1.0000
[2019-04-03 22:34:21,981] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.4740
[2019-04-03 22:34:22,010] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 100.0, 51.33333333333334, 0.0, 26.0, 26.03560052235375, 0.5281914525603849, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1503600.0000, 
sim time next is 1504200.0000, 
raw observation next is [2.1, 100.0, 55.66666666666666, 0.0, 26.0, 26.0953151264114, 0.5320747108604459, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.5207756232686982, 1.0, 0.18555555555555553, 0.0, 0.6666666666666666, 0.6746095938676167, 0.677358236953482, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.35676876], dtype=float32), 0.97872764]. 
=============================================
[2019-04-03 22:34:27,650] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.3893349e-14 5.1571697e-06 5.1836893e-13 1.7162675e-08 1.4951233e-08
 4.4420617e-14 9.9999487e-01], sum to 1.0000
[2019-04-03 22:34:27,651] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8167
[2019-04-03 22:34:27,666] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.800000000000001, 84.66666666666667, 0.0, 0.0, 26.0, 25.99131065986735, 0.6266943578465319, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1635600.0000, 
sim time next is 1636200.0000, 
raw observation next is [6.9, 84.0, 0.0, 0.0, 26.0, 25.82975373054571, 0.6095682961377173, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.6537396121883658, 0.84, 0.0, 0.0, 0.6666666666666666, 0.652479477545476, 0.7031894320459058, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2205935], dtype=float32), -0.19704923]. 
=============================================
[2019-04-03 22:34:27,710] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.5646623e-14 3.9678375e-07 6.3312955e-14 9.6892894e-10 6.8993430e-08
 2.0862462e-15 9.9999952e-01], sum to 1.0000
[2019-04-03 22:34:27,714] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8889
[2019-04-03 22:34:27,723] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.533333333333333, 64.66666666666666, 81.66666666666666, 688.3333333333333, 26.0, 26.49890315135366, 0.7083654259055728, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1518600.0000, 
sim time next is 1519200.0000, 
raw observation next is [10.0, 63.0, 80.0, 682.0, 26.0, 26.63110348157565, 0.7308035579246727, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.739612188365651, 0.63, 0.26666666666666666, 0.7535911602209945, 0.6666666666666666, 0.7192586234646375, 0.7436011859748909, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3845115], dtype=float32), 0.61790353]. 
=============================================
[2019-04-03 22:34:29,906] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.3879129e-11 2.2360464e-04 4.6284143e-11 8.4257067e-08 2.2481465e-06
 1.6895350e-11 9.9977404e-01], sum to 1.0000
[2019-04-03 22:34:29,906] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9364
[2019-04-03 22:34:29,940] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 87.0, 108.0, 0.0, 26.0, 24.89827725105942, 0.3356723632431354, 0.0, 1.0, 63798.63110521323], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1767600.0000, 
sim time next is 1768200.0000, 
raw observation next is [-2.3, 86.33333333333333, 111.6666666666667, 0.0, 26.0, 24.89496275663548, 0.3414901862928701, 0.0, 1.0, 55568.31462205387], 
processed observation next is [0.0, 0.4782608695652174, 0.3988919667590028, 0.8633333333333333, 0.37222222222222234, 0.0, 0.6666666666666666, 0.5745802297196233, 0.6138300620976234, 0.0, 1.0, 0.2646110220097803], 
reward next is 0.7354, 
noisyNet noise sample is [array([-0.41996676], dtype=float32), -1.5306001]. 
=============================================
[2019-04-03 22:34:36,994] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.6995962e-14 3.9563016e-05 2.0091962e-12 1.8061870e-10 9.0585644e-08
 2.2258862e-13 9.9996030e-01], sum to 1.0000
[2019-04-03 22:34:36,998] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5749
[2019-04-03 22:34:37,010] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.6, 97.0, 0.0, 0.0, 26.0, 25.60330116244999, 0.5358891882591091, 0.0, 1.0, 106574.279429865], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1652400.0000, 
sim time next is 1653000.0000, 
raw observation next is [6.6, 97.0, 0.0, 0.0, 26.0, 25.48933404803802, 0.5397251719053842, 0.0, 1.0, 129423.0974060342], 
processed observation next is [1.0, 0.13043478260869565, 0.6454293628808865, 0.97, 0.0, 0.0, 0.6666666666666666, 0.6241111706698351, 0.679908390635128, 0.0, 1.0, 0.6163004638382581], 
reward next is 0.3837, 
noisyNet noise sample is [array([-0.01805815], dtype=float32), 0.092254415]. 
=============================================
[2019-04-03 22:34:37,028] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[87.17446 ]
 [86.70987 ]
 [86.75697 ]
 [86.83287 ]
 [86.923874]], R is [[87.23414612]
 [86.85430908]
 [86.98576355]
 [87.11590576]
 [87.24475098]].
[2019-04-03 22:35:10,645] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.4016279e-12 2.7114950e-04 1.5504301e-10 6.3410702e-08 1.2942063e-05
 2.3138680e-11 9.9971575e-01], sum to 1.0000
[2019-04-03 22:35:10,645] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1829
[2019-04-03 22:35:10,668] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.199999999999999, 87.0, 0.0, 0.0, 26.0, 24.21321926624926, 0.0638995299894536, 0.0, 1.0, 41079.6130000487], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2007600.0000, 
sim time next is 2008200.0000, 
raw observation next is [-6.2, 87.0, 0.0, 0.0, 26.0, 24.15917574545531, 0.05870301665495665, 0.0, 1.0, 41111.63103439956], 
processed observation next is [1.0, 0.21739130434782608, 0.2908587257617729, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5132646454546093, 0.5195676722183189, 0.0, 1.0, 0.19576967159237885], 
reward next is 0.8042, 
noisyNet noise sample is [array([-1.7698662], dtype=float32), 0.6237149]. 
=============================================
[2019-04-03 22:35:16,208] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.4842498e-12 7.6032447e-05 1.8591849e-11 5.8727803e-08 3.0155229e-07
 1.3530302e-12 9.9992359e-01], sum to 1.0000
[2019-04-03 22:35:16,208] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7628
[2019-04-03 22:35:16,274] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.7333333333333335, 43.33333333333334, 135.1666666666667, 45.0, 26.0, 25.84582509551527, 0.4285893620705887, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2301600.0000, 
sim time next is 2302200.0000, 
raw observation next is [0.55, 43.5, 138.0, 42.0, 26.0, 25.19644174744904, 0.3747504426637489, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.4778393351800555, 0.435, 0.46, 0.04640883977900553, 0.6666666666666666, 0.5997034789540866, 0.6249168142212497, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3051414], dtype=float32), 1.0885115]. 
=============================================
[2019-04-03 22:35:22,590] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.3708370e-12 8.9843900e-07 3.7377983e-12 1.6622991e-08 5.5263240e-07
 1.9733577e-13 9.9999845e-01], sum to 1.0000
[2019-04-03 22:35:22,591] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1515
[2019-04-03 22:35:22,631] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.300000000000001, 70.0, 138.6666666666667, 0.0, 26.0, 25.53259224149004, 0.3382447845586626, 1.0, 1.0, 52090.45366816923], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2200800.0000, 
sim time next is 2201400.0000, 
raw observation next is [-4.2, 69.5, 143.0, 0.0, 26.0, 25.54572033178955, 0.3521147726430722, 1.0, 1.0, 34939.25662634293], 
processed observation next is [1.0, 0.4782608695652174, 0.34626038781163443, 0.695, 0.4766666666666667, 0.0, 0.6666666666666666, 0.6288100276491292, 0.6173715908810241, 1.0, 1.0, 0.16637741250639493], 
reward next is 0.8336, 
noisyNet noise sample is [array([-1.5093772], dtype=float32), 0.93762183]. 
=============================================
[2019-04-03 22:35:27,823] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.9839269e-13 2.0697164e-04 4.0531771e-12 5.7583835e-09 1.3193837e-06
 6.9110879e-13 9.9979168e-01], sum to 1.0000
[2019-04-03 22:35:27,824] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5029
[2019-04-03 22:35:27,881] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 75.0, 21.5, 131.0, 26.0, 25.49604865484374, 0.3471602717850472, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2188800.0000, 
sim time next is 2189400.0000, 
raw observation next is [-5.600000000000001, 75.0, 28.0, 174.6666666666667, 26.0, 25.63152023884818, 0.3350694473764682, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.3074792243767313, 0.75, 0.09333333333333334, 0.1930018416206262, 0.6666666666666666, 0.6359600199040149, 0.6116898157921561, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.656604], dtype=float32), 0.30748156]. 
=============================================
[2019-04-03 22:35:28,529] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.6172245e-13 2.9124880e-05 7.5794909e-12 3.8261008e-09 8.9849647e-07
 8.7812971e-13 9.9996996e-01], sum to 1.0000
[2019-04-03 22:35:28,532] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0661
[2019-04-03 22:35:28,553] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.900000000000002, 91.0, 0.0, 0.0, 26.0, 23.69355740815719, -0.00880450846326535, 0.0, 1.0, 43320.51480658642], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2265600.0000, 
sim time next is 2266200.0000, 
raw observation next is [-8.9, 91.0, 0.0, 0.0, 26.0, 23.68407757142769, -0.01663578602112774, 0.0, 1.0, 43271.28039951844], 
processed observation next is [1.0, 0.21739130434782608, 0.21606648199445982, 0.91, 0.0, 0.0, 0.6666666666666666, 0.47367313095230745, 0.4944547379929574, 0.0, 1.0, 0.20605371618818302], 
reward next is 0.7939, 
noisyNet noise sample is [array([-1.6582556], dtype=float32), 0.29048985]. 
=============================================
[2019-04-03 22:35:37,112] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.3657050e-10 4.0912730e-04 7.2655315e-10 3.6710304e-07 8.2446177e-06
 5.5153375e-11 9.9958223e-01], sum to 1.0000
[2019-04-03 22:35:37,112] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9772
[2019-04-03 22:35:37,230] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 69.0, 25.33333333333334, 0.0, 26.0, 24.04225058023461, 0.06965190161459937, 0.0, 1.0, 41994.14846309182], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2362200.0000, 
sim time next is 2362800.0000, 
raw observation next is [-3.4, 69.0, 31.16666666666667, 0.0, 26.0, 24.06963236790867, 0.1323117145497074, 0.0, 1.0, 202421.3760289045], 
processed observation next is [0.0, 0.34782608695652173, 0.368421052631579, 0.69, 0.1038888888888889, 0.0, 0.6666666666666666, 0.5058026973257226, 0.5441039048499025, 0.0, 1.0, 0.9639113144233546], 
reward next is 0.0361, 
noisyNet noise sample is [array([0.1935141], dtype=float32), -0.24670961]. 
=============================================
[2019-04-03 22:35:37,320] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.2017448e-10 2.0557013e-04 3.0396766e-10 1.3633843e-07 1.4657029e-05
 7.5566907e-11 9.9977964e-01], sum to 1.0000
[2019-04-03 22:35:37,333] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3894
[2019-04-03 22:35:37,375] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.09999999999999998, 52.33333333333334, 180.6666666666667, 0.0, 26.0, 24.93253304312198, 0.3010743752525865, 0.0, 1.0, 32085.05329501612], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2382600.0000, 
sim time next is 2383200.0000, 
raw observation next is [0.0, 52.0, 175.5, 0.0, 26.0, 24.94656897009354, 0.3057531343285658, 0.0, 1.0, 20024.93663413872], 
processed observation next is [0.0, 0.6086956521739131, 0.46260387811634357, 0.52, 0.585, 0.0, 0.6666666666666666, 0.5788807475077951, 0.6019177114428552, 0.0, 1.0, 0.09535684111494629], 
reward next is 0.9046, 
noisyNet noise sample is [array([-1.5268933], dtype=float32), -0.25562847]. 
=============================================
[2019-04-03 22:35:54,097] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [5.6481756e-14 6.3107514e-06 9.0609508e-13 2.1016284e-09 1.2916789e-07
 2.7320191e-14 9.9999356e-01], sum to 1.0000
[2019-04-03 22:35:54,097] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.6529
[2019-04-03 22:35:54,176] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 58.0, 0.0, 0.0, 26.0, 25.00996230116721, 0.351014551214573, 1.0, 1.0, 124694.9800446237], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2659200.0000, 
sim time next is 2659800.0000, 
raw observation next is [-1.1, 59.0, 0.0, 0.0, 26.0, 24.99388697926278, 0.3760955333883615, 1.0, 1.0, 92153.02422370088], 
processed observation next is [1.0, 0.782608695652174, 0.4321329639889197, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5828239149385649, 0.6253651777961206, 1.0, 1.0, 0.43882392487476607], 
reward next is 0.5612, 
noisyNet noise sample is [array([-1.5353365], dtype=float32), 0.8415237]. 
=============================================
[2019-04-03 22:35:58,415] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.4894540e-12 6.6461304e-05 4.4141146e-11 6.4198389e-08 3.0050580e-06
 5.6006462e-12 9.9993050e-01], sum to 1.0000
[2019-04-03 22:35:58,416] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1802
[2019-04-03 22:35:58,453] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.1, 82.16666666666667, 0.0, 0.0, 26.0, 24.35870408480511, 0.1147649730160362, 0.0, 1.0, 42884.56054959602], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2609400.0000, 
sim time next is 2610000.0000, 
raw observation next is [-6.2, 83.0, 0.0, 0.0, 26.0, 24.30502292257525, 0.09987116631863302, 0.0, 1.0, 42997.12528376363], 
processed observation next is [1.0, 0.21739130434782608, 0.2908587257617729, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5254185768812709, 0.5332903887728776, 0.0, 1.0, 0.20474821563696965], 
reward next is 0.7953, 
noisyNet noise sample is [array([-0.5644164], dtype=float32), -2.1161666]. 
=============================================
[2019-04-03 22:35:58,477] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[80.6012  ]
 [80.6699  ]
 [80.72192 ]
 [80.771286]
 [80.81901 ]], R is [[80.51057434]
 [80.50125122]
 [80.49246979]
 [80.48409271]
 [80.47618103]].
[2019-04-03 22:36:10,146] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.62359850e-14 1.30345434e-05 1.75805941e-12 4.37208536e-09
 2.17873847e-07 1.03915066e-13 9.99986768e-01], sum to 1.0000
[2019-04-03 22:36:10,146] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6485
[2019-04-03 22:36:10,214] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 37.0, 0.0, 0.0, 26.0, 24.55287046565662, 0.3454039319748728, 1.0, 1.0, 87895.31178526561], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2833200.0000, 
sim time next is 2833800.0000, 
raw observation next is [2.833333333333333, 38.16666666666667, 0.0, 0.0, 26.0, 24.96591717107259, 0.3924754381005051, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.541089566020314, 0.3816666666666667, 0.0, 0.0, 0.6666666666666666, 0.5804930975893825, 0.6308251460335017, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3202604], dtype=float32), 0.40243393]. 
=============================================
[2019-04-03 22:36:23,285] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.2762742e-14 3.2578112e-06 1.1937954e-12 2.3254993e-09 1.8864889e-06
 1.9267550e-14 9.9999487e-01], sum to 1.0000
[2019-04-03 22:36:23,287] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2541
[2019-04-03 22:36:23,329] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 81.5, 0.0, 0.0, 26.0, 25.00089616684421, 0.4417484461020406, 0.0, 1.0, 101817.6854686568], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2925000.0000, 
sim time next is 2925600.0000, 
raw observation next is [-1.0, 82.66666666666667, 0.0, 0.0, 26.0, 25.09565862037598, 0.4686543638795432, 0.0, 1.0, 64672.91056831117], 
processed observation next is [1.0, 0.8695652173913043, 0.4349030470914128, 0.8266666666666667, 0.0, 0.0, 0.6666666666666666, 0.5913048850313315, 0.6562181212931811, 0.0, 1.0, 0.30796624080148177], 
reward next is 0.6920, 
noisyNet noise sample is [array([-1.1905777], dtype=float32), 0.20706016]. 
=============================================
[2019-04-03 22:36:29,217] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.0934148e-12 2.2335890e-04 9.2682195e-11 5.6957653e-08 2.6299682e-05
 4.4127969e-11 9.9975020e-01], sum to 1.0000
[2019-04-03 22:36:29,217] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0460
[2019-04-03 22:36:29,266] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 54.0, 106.0, 759.0, 26.0, 25.13109013462183, 0.3079333524865343, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3061800.0000, 
sim time next is 3062400.0000, 
raw observation next is [-4.0, 54.0, 106.8333333333333, 766.6666666666666, 26.0, 25.07488841737554, 0.3043622964538373, 0.0, 1.0, 35975.9901179529], 
processed observation next is [0.0, 0.43478260869565216, 0.3518005540166205, 0.54, 0.356111111111111, 0.8471454880294659, 0.6666666666666666, 0.589574034781295, 0.6014540988179458, 0.0, 1.0, 0.17131423865691855], 
reward next is 0.8287, 
noisyNet noise sample is [array([-0.5589723], dtype=float32), -0.4050138]. 
=============================================
[2019-04-03 22:36:35,565] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.2420440e-14 6.6879966e-07 4.8054085e-13 1.8181583e-09 1.3875909e-05
 9.6388072e-15 9.9998546e-01], sum to 1.0000
[2019-04-03 22:36:35,575] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3023
[2019-04-03 22:36:35,611] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.5, 48.0, 87.0, 674.0, 26.0, 26.70659148872784, 0.6047252152531053, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3339000.0000, 
sim time next is 3339600.0000, 
raw observation next is [-2.333333333333333, 47.33333333333333, 82.5, 645.1666666666667, 26.0, 26.05415910521073, 0.6323712281546745, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.3979686057248385, 0.4733333333333333, 0.275, 0.7128913443830571, 0.6666666666666666, 0.6711799254342274, 0.7107904093848916, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2738333], dtype=float32), 0.24560347]. 
=============================================
[2019-04-03 22:36:40,237] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.4631335e-12 2.9688906e-07 2.1539821e-12 8.5974650e-09 5.4552147e-06
 1.2806189e-14 9.9999416e-01], sum to 1.0000
[2019-04-03 22:36:40,237] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4834
[2019-04-03 22:36:40,252] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.166666666666667, 50.0, 99.66666666666666, 726.0, 26.0, 26.51813685043662, 0.6924886092527304, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3336600.0000, 
sim time next is 3337200.0000, 
raw observation next is [-3.0, 50.0, 96.5, 713.0, 26.0, 26.61746764298194, 0.7016439909026381, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.3795013850415513, 0.5, 0.32166666666666666, 0.7878453038674034, 0.6666666666666666, 0.7181223035818283, 0.7338813303008793, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.659929], dtype=float32), -0.90635794]. 
=============================================
[2019-04-03 22:36:41,108] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.47768557e-12 2.17469551e-06 1.77066722e-10 1.02079376e-07
 1.30450380e-05 2.72702065e-12 9.99984741e-01], sum to 1.0000
[2019-04-03 22:36:41,109] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6128
[2019-04-03 22:36:41,145] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 68.0, 0.0, 0.0, 26.0, 25.91456400142292, 0.6149275142332985, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3267000.0000, 
sim time next is 3267600.0000, 
raw observation next is [-4.0, 69.0, 0.0, 0.0, 26.0, 25.87489343979703, 0.5971728161651401, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.3518005540166205, 0.69, 0.0, 0.0, 0.6666666666666666, 0.6562411199830859, 0.69905760538838, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.7929906], dtype=float32), -0.6236124]. 
=============================================
[2019-04-03 22:36:49,395] A3C_AGENT_WORKER-Thread-8 INFO:Evaluating...
[2019-04-03 22:36:49,395] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-03 22:36:49,396] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:36:49,396] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-03 22:36:49,397] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:36:49,397] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-03 22:36:49,398] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:36:49,400] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run6
[2019-04-03 22:36:49,417] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run6
[2019-04-03 22:36:49,429] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run6
[2019-04-03 22:37:07,617] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.21776989], dtype=float32), 0.29300985]
[2019-04-03 22:37:07,617] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-10.4, 77.0, 0.0, 0.0, 26.0, 23.18334881779897, -0.117928794915944, 0.0, 1.0, 46660.28432119713]
[2019-04-03 22:37:07,617] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-03 22:37:07,617] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [5.0069404e-10 7.8359470e-03 7.3712134e-09 1.3448755e-06 4.0097203e-04
 1.7299043e-09 9.9176168e-01], sampled 0.8383522961119007
[2019-04-03 22:39:24,128] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.21776989], dtype=float32), 0.29300985]
[2019-04-03 22:39:24,128] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-2.7, 58.66666666666667, 21.66666666666667, 190.8333333333333, 26.0, 25.55714859405311, 0.4738464658221581, 1.0, 1.0, 158674.0045309217]
[2019-04-03 22:39:24,128] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-03 22:39:24,129] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [3.2828206e-12 1.1571273e-04 1.2051839e-11 3.8966252e-08 1.4117442e-05
 7.8384489e-13 9.9987018e-01], sampled 0.4128831272641784
[2019-04-03 22:39:48,494] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.4196 239941874.3313 1605.3476
[2019-04-03 22:40:17,571] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.8260 263376537.7352 1553.3210
[2019-04-03 22:40:23,441] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.7642 275779513.5059 1232.6471
[2019-04-03 22:40:24,480] A3C_AGENT_WORKER-Thread-8 INFO:Global step: 500000, evaluation results [500000.0, 7241.826010784863, 263376537.73517773, 1553.321032361531, 7353.419646041243, 239941874.33133844, 1605.347596910568, 7182.764221400699, 275779513.50585467, 1232.6470856617902]
[2019-04-03 22:40:28,129] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.6561755e-13 5.8486527e-05 2.2690913e-11 3.1989725e-08 1.4401756e-05
 3.7612432e-13 9.9992704e-01], sum to 1.0000
[2019-04-03 22:40:28,129] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1583
[2019-04-03 22:40:28,168] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 86.0, 0.0, 0.0, 26.0, 25.32106990493048, 0.4706594536055131, 0.0, 1.0, 57479.2358235124], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3451800.0000, 
sim time next is 3452400.0000, 
raw observation next is [1.0, 86.0, 0.0, 0.0, 26.0, 25.35505619429851, 0.4753809142007255, 0.0, 1.0, 46989.38642364014], 
processed observation next is [1.0, 1.0, 0.4903047091412743, 0.86, 0.0, 0.0, 0.6666666666666666, 0.6129213495248758, 0.6584603047335752, 0.0, 1.0, 0.22375898296971497], 
reward next is 0.7762, 
noisyNet noise sample is [array([1.1836249], dtype=float32), 0.7341133]. 
=============================================
[2019-04-03 22:40:28,229] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.2027424e-12 2.9764476e-04 2.3883796e-11 6.5695424e-08 4.3583354e-05
 2.5018285e-13 9.9965870e-01], sum to 1.0000
[2019-04-03 22:40:28,230] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.1077
[2019-04-03 22:40:28,258] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 49.0, 37.5, 338.0, 26.0, 26.55110607554683, 0.6863384516391955, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3517200.0000, 
sim time next is 3517800.0000, 
raw observation next is [2.833333333333333, 49.5, 29.33333333333333, 275.6666666666666, 26.0, 26.36457891814576, 0.6631091317956136, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.541089566020314, 0.495, 0.09777777777777776, 0.30460405156537745, 0.6666666666666666, 0.6970482431788133, 0.7210363772652045, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.86845005], dtype=float32), -0.36541322]. 
=============================================
[2019-04-03 22:40:31,401] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.8897389e-14 1.7647668e-05 5.6999522e-13 1.8057210e-09 1.8139714e-07
 2.8706116e-14 9.9998212e-01], sum to 1.0000
[2019-04-03 22:40:31,401] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7205
[2019-04-03 22:40:31,417] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.5, 50.5, 115.0, 806.0, 26.0, 26.01858734534509, 0.6259134841332806, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3504600.0000, 
sim time next is 3505200.0000, 
raw observation next is [2.666666666666667, 50.0, 112.8333333333333, 801.8333333333334, 26.0, 25.5153431143055, 0.594440269936327, 1.0, 1.0, 18680.41167023055], 
processed observation next is [1.0, 0.5652173913043478, 0.5364727608494922, 0.5, 0.376111111111111, 0.8860036832412523, 0.6666666666666666, 0.6262785928587918, 0.6981467566454423, 1.0, 1.0, 0.08895434128681215], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.6134383], dtype=float32), -0.9393594]. 
=============================================
[2019-04-03 22:40:33,348] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.6702406e-13 4.3419431e-04 7.2775588e-12 7.4620559e-09 3.0893254e-06
 9.5373572e-14 9.9956268e-01], sum to 1.0000
[2019-04-03 22:40:33,348] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4567
[2019-04-03 22:40:33,362] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.833333333333333, 49.5, 110.6666666666667, 797.6666666666666, 26.0, 25.98956553244324, 0.644412193158524, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3505800.0000, 
sim time next is 3506400.0000, 
raw observation next is [3.0, 49.0, 108.5, 793.5, 26.0, 26.31153157025155, 0.666646790561705, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.49, 0.3616666666666667, 0.8767955801104972, 0.6666666666666666, 0.6926276308542958, 0.7222155968539017, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.8645469], dtype=float32), 0.79395854]. 
=============================================
[2019-04-03 22:40:42,469] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.1258312e-11 1.7577341e-03 5.5788735e-11 5.1839578e-08 1.7227100e-04
 3.7150338e-11 9.9806994e-01], sum to 1.0000
[2019-04-03 22:40:42,469] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8905
[2019-04-03 22:40:42,481] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.333333333333333, 44.66666666666667, 0.0, 0.0, 26.0, 25.44716478977913, 0.3926915560734632, 0.0, 1.0, 57170.96803896352], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3619200.0000, 
sim time next is 3619800.0000, 
raw observation next is [-1.5, 46.0, 0.0, 0.0, 26.0, 25.41822427316493, 0.3913601890567307, 0.0, 1.0, 59799.85087491576], 
processed observation next is [0.0, 0.9130434782608695, 0.4210526315789474, 0.46, 0.0, 0.0, 0.6666666666666666, 0.6181853560970776, 0.6304533963522435, 0.0, 1.0, 0.28476119464245603], 
reward next is 0.7152, 
noisyNet noise sample is [array([0.7273178], dtype=float32), -1.2484875]. 
=============================================
[2019-04-03 22:40:48,460] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.8732888e-12 8.9429937e-05 5.3603660e-11 1.4253621e-08 1.0819423e-04
 4.9860979e-12 9.9980241e-01], sum to 1.0000
[2019-04-03 22:40:48,461] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1569
[2019-04-03 22:40:48,478] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.0, 59.0, 0.0, 0.0, 26.0, 24.96788922064519, 0.3203569358535758, 0.0, 1.0, 23917.22087498418], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3697200.0000, 
sim time next is 3697800.0000, 
raw observation next is [3.833333333333333, 59.66666666666667, 0.0, 0.0, 26.0, 24.95485123965786, 0.3164376957352332, 0.0, 1.0, 32581.71863045832], 
processed observation next is [0.0, 0.8260869565217391, 0.5687903970452447, 0.5966666666666667, 0.0, 0.0, 0.6666666666666666, 0.579570936638155, 0.6054792319117445, 0.0, 1.0, 0.15515104109742056], 
reward next is 0.8448, 
noisyNet noise sample is [array([-0.29992014], dtype=float32), 1.0301689]. 
=============================================
[2019-04-03 22:41:02,984] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.0155409e-12 2.6556859e-06 1.3795811e-11 3.4100950e-08 5.9545499e-07
 4.7484445e-13 9.9999666e-01], sum to 1.0000
[2019-04-03 22:41:02,984] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4687
[2019-04-03 22:41:03,023] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 51.0, 0.0, 0.0, 26.0, 25.20351168954283, 0.4436208110084355, 0.0, 1.0, 20877.03038807977], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3873600.0000, 
sim time next is 3874200.0000, 
raw observation next is [0.6666666666666667, 52.5, 0.0, 0.0, 26.0, 25.12715634946045, 0.4413147172790832, 0.0, 1.0, 65800.72633635027], 
processed observation next is [1.0, 0.8695652173913043, 0.4810710987996307, 0.525, 0.0, 0.0, 0.6666666666666666, 0.5939296957883707, 0.6471049057596944, 0.0, 1.0, 0.3133367920778584], 
reward next is 0.6867, 
noisyNet noise sample is [array([-0.9779168], dtype=float32), 0.8191588]. 
=============================================
[2019-04-03 22:41:15,397] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.4882772e-11 2.2312405e-03 8.3809387e-10 1.6344345e-07 3.7842907e-04
 5.4323580e-11 9.9739021e-01], sum to 1.0000
[2019-04-03 22:41:15,397] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0781
[2019-04-03 22:41:15,477] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.66666666666667, 56.33333333333333, 94.33333333333333, 486.3333333333333, 26.0, 26.11435087617321, 0.4727619206263241, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4005600.0000, 
sim time next is 4006200.0000, 
raw observation next is [-11.33333333333333, 54.66666666666667, 95.66666666666666, 528.6666666666667, 26.0, 26.25907882164697, 0.4852952858493607, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.14866112650046176, 0.5466666666666667, 0.31888888888888883, 0.5841620626151014, 0.6666666666666666, 0.6882565684705808, 0.6617650952831202, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.44534522], dtype=float32), -0.6417407]. 
=============================================
[2019-04-03 22:41:16,833] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.7922686e-12 9.1921123e-05 3.2977570e-12 3.4510862e-08 7.4077298e-06
 4.4607991e-13 9.9990070e-01], sum to 1.0000
[2019-04-03 22:41:16,836] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4037
[2019-04-03 22:41:16,900] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.166666666666666, 40.66666666666667, 114.3333333333333, 792.6666666666667, 26.0, 26.55444375753664, 0.5844407455009332, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4013400.0000, 
sim time next is 4014000.0000, 
raw observation next is [-8.0, 40.0, 115.5, 798.5, 26.0, 26.55372568638396, 0.5789127388764141, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.24099722991689754, 0.4, 0.385, 0.8823204419889503, 0.6666666666666666, 0.7128104738653299, 0.6929709129588048, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.27139446], dtype=float32), -0.008914157]. 
=============================================
[2019-04-03 22:41:16,921] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[83.443085]
 [83.38319 ]
 [83.20097 ]
 [83.050095]
 [82.95133 ]], R is [[83.63362885]
 [83.79729462]
 [83.95932007]
 [84.11972809]
 [84.27853394]].
[2019-04-03 22:41:42,630] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.9108264e-11 2.2298694e-03 5.0015642e-10 7.7757733e-08 6.0736715e-05
 1.1570526e-10 9.9770927e-01], sum to 1.0000
[2019-04-03 22:41:42,631] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.2300
[2019-04-03 22:41:42,665] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 49.0, 0.0, 0.0, 26.0, 25.35516635469298, 0.3243307560762079, 0.0, 1.0, 39807.46443128338], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4257000.0000, 
sim time next is 4257600.0000, 
raw observation next is [3.0, 49.0, 0.0, 0.0, 26.0, 25.34907396775345, 0.3233527724916394, 0.0, 1.0, 39413.3451343027], 
processed observation next is [0.0, 0.2608695652173913, 0.5457063711911359, 0.49, 0.0, 0.0, 0.6666666666666666, 0.6124228306461209, 0.6077842574972131, 0.0, 1.0, 0.1876825958776319], 
reward next is 0.8123, 
noisyNet noise sample is [array([-1.0271931], dtype=float32), 0.74665505]. 
=============================================
[2019-04-03 22:41:48,281] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.3214562e-12 4.6788286e-05 1.9276289e-11 6.5507981e-09 7.9816027e-06
 2.0628111e-12 9.9994516e-01], sum to 1.0000
[2019-04-03 22:41:48,282] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9946
[2019-04-03 22:41:48,310] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.25, 74.5, 0.0, 0.0, 26.0, 25.5330812923055, 0.4099946900186339, 0.0, 1.0, 19306.39495275544], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4324200.0000, 
sim time next is 4324800.0000, 
raw observation next is [4.300000000000001, 74.0, 0.0, 0.0, 26.0, 25.57729045491084, 0.4210795077389792, 0.0, 1.0, 18742.09200348783], 
processed observation next is [1.0, 0.043478260869565216, 0.5817174515235458, 0.74, 0.0, 0.0, 0.6666666666666666, 0.6314408712425701, 0.6403598359129931, 0.0, 1.0, 0.08924805715946585], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.7557654], dtype=float32), 0.29528967]. 
=============================================
[2019-04-03 22:41:48,531] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.5094885e-11 1.4552416e-03 9.5587663e-11 2.7996288e-07 2.1951259e-03
 6.2351973e-12 9.9634922e-01], sum to 1.0000
[2019-04-03 22:41:48,532] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1101
[2019-04-03 22:41:48,544] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.2, 64.0, 0.0, 0.0, 26.0, 25.33682397749876, 0.3548104099056906, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4298400.0000, 
sim time next is 4299000.0000, 
raw observation next is [6.166666666666667, 64.83333333333334, 0.0, 0.0, 26.0, 25.26867563881269, 0.3386822169304834, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.782608695652174, 0.6334256694367498, 0.6483333333333334, 0.0, 0.0, 0.6666666666666666, 0.6057229699010577, 0.6128940723101611, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8826003], dtype=float32), 2.2211561]. 
=============================================
[2019-04-03 22:41:48,548] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[82.7615  ]
 [83.92351 ]
 [84.42234 ]
 [84.988686]
 [85.63449 ]], R is [[82.2853241 ]
 [82.46247101]
 [82.6378479 ]
 [82.81147003]
 [82.98335266]].
[2019-04-03 22:41:53,486] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.8279638e-13 1.2632906e-04 5.9285674e-11 2.5127076e-08 4.9985247e-05
 5.9195361e-12 9.9982375e-01], sum to 1.0000
[2019-04-03 22:41:53,487] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2738
[2019-04-03 22:41:53,499] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.966666666666667, 70.66666666666667, 0.0, 0.0, 26.0, 25.62504619849974, 0.4146767170712226, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4332000.0000, 
sim time next is 4332600.0000, 
raw observation next is [3.95, 70.5, 0.0, 0.0, 26.0, 25.66489266540983, 0.4037470706167876, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.57202216066482, 0.705, 0.0, 0.0, 0.6666666666666666, 0.6387410554508192, 0.6345823568722625, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.06193685], dtype=float32), 0.6034715]. 
=============================================
[2019-04-03 22:41:57,892] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.3588299e-12 6.2164571e-04 1.5276665e-10 4.3944777e-08 7.3852498e-05
 2.1580883e-11 9.9930441e-01], sum to 1.0000
[2019-04-03 22:41:57,893] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1562
[2019-04-03 22:41:57,926] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 71.0, 0.0, 0.0, 26.0, 25.22008219178851, 0.3922061957712379, 0.0, 1.0, 40636.10870427138], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4516800.0000, 
sim time next is 4517400.0000, 
raw observation next is [-1.0, 71.0, 0.0, 0.0, 26.0, 25.26330770058906, 0.3934558987696254, 0.0, 1.0, 40542.33755761449], 
processed observation next is [1.0, 0.2608695652173913, 0.4349030470914128, 0.71, 0.0, 0.0, 0.6666666666666666, 0.605275641715755, 0.6311519662565418, 0.0, 1.0, 0.1930587502743547], 
reward next is 0.8069, 
noisyNet noise sample is [array([0.8245494], dtype=float32), -0.35432392]. 
=============================================
[2019-04-03 22:42:15,677] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.3714996e-10 3.5401564e-02 1.1190982e-08 1.8934386e-06 4.2005549e-03
 2.2226814e-09 9.6039593e-01], sum to 1.0000
[2019-04-03 22:42:15,678] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9200
[2019-04-03 22:42:15,702] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.166666666666667, 92.83333333333333, 0.0, 0.0, 26.0, 23.91535900343071, 0.1037318918238916, 0.0, 1.0, 41870.97826331233], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4777800.0000, 
sim time next is 4778400.0000, 
raw observation next is [-6.133333333333334, 92.66666666666667, 0.0, 0.0, 26.0, 23.88456037210125, 0.09691253262898564, 0.0, 1.0, 41899.5333149286], 
processed observation next is [0.0, 0.30434782608695654, 0.2927054478301016, 0.9266666666666667, 0.0, 0.0, 0.6666666666666666, 0.49038003100843763, 0.5323041775429952, 0.0, 1.0, 0.19952158721394572], 
reward next is 0.8005, 
noisyNet noise sample is [array([-0.5518291], dtype=float32), 0.5266368]. 
=============================================
[2019-04-03 22:42:16,769] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.06208185e-11 2.15019754e-04 8.69459366e-11 6.49704788e-08
 8.74830948e-05 7.38806802e-13 9.99697447e-01], sum to 1.0000
[2019-04-03 22:42:16,769] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4186
[2019-04-03 22:42:16,788] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.0, 24.33333333333334, 0.0, 0.0, 26.0, 27.11195916917418, 0.7913526164719119, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4990800.0000, 
sim time next is 4991400.0000, 
raw observation next is [6.0, 24.0, 0.0, 0.0, 26.0, 27.07382220938915, 0.6567655524276049, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.6288088642659281, 0.24, 0.0, 0.0, 0.6666666666666666, 0.7561518507824291, 0.7189218508092017, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0872717], dtype=float32), 0.7249264]. 
=============================================
[2019-04-03 22:42:18,192] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.9522318e-11 1.1444339e-03 6.3410949e-10 2.3485347e-07 1.4391768e-03
 1.5728658e-11 9.9741614e-01], sum to 1.0000
[2019-04-03 22:42:18,192] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0072
[2019-04-03 22:42:18,219] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.166666666666667, 65.0, 71.66666666666666, 245.0, 26.0, 25.58896378885387, 0.4230025070129811, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5039400.0000, 
sim time next is 5040000.0000, 
raw observation next is [-2.0, 65.0, 78.0, 317.0, 26.0, 25.58487790860521, 0.4292187367513505, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.40720221606648205, 0.65, 0.26, 0.35027624309392263, 0.6666666666666666, 0.6320731590504343, 0.6430729122504502, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.91997206], dtype=float32), 1.0330828]. 
=============================================
[2019-04-03 22:42:18,224] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[85.79299 ]
 [82.83501 ]
 [83.4766  ]
 [80.09506 ]
 [75.765724]], R is [[87.79852295]
 [87.92053986]
 [88.04133606]
 [88.16092682]
 [88.27931976]].
[2019-04-03 22:42:22,493] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:42:22,493] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:42:22,509] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run5
[2019-04-03 22:42:22,847] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:42:22,848] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:42:22,851] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run5
[2019-04-03 22:42:23,304] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:42:23,304] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:42:23,306] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run5
[2019-04-03 22:42:24,955] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:42:24,955] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:42:24,958] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run5
[2019-04-03 22:42:29,241] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:42:29,241] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:42:29,243] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run5
[2019-04-03 22:42:31,425] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:42:31,427] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:42:31,428] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run5
[2019-04-03 22:42:31,618] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:42:31,619] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:42:31,620] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run5
[2019-04-03 22:42:32,350] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.8210265e-13 4.6484598e-05 5.0183226e-11 2.1484093e-08 3.7998936e-05
 6.4140624e-12 9.9991548e-01], sum to 1.0000
[2019-04-03 22:42:32,350] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6339
[2019-04-03 22:42:32,386] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 54.16666666666667, 0.0, 0.0, 26.0, 25.36043055260692, 0.3892778403437605, 0.0, 1.0, 63253.78552055294], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5026200.0000, 
sim time next is 5026800.0000, 
raw observation next is [-1.0, 53.33333333333334, 0.0, 0.0, 26.0, 25.3349651927617, 0.3964859972721874, 0.0, 1.0, 48473.7438084036], 
processed observation next is [1.0, 0.17391304347826086, 0.4349030470914128, 0.5333333333333334, 0.0, 0.0, 0.6666666666666666, 0.6112470993968083, 0.6321619990907291, 0.0, 1.0, 0.23082735146858854], 
reward next is 0.7692, 
noisyNet noise sample is [array([-0.3133517], dtype=float32), -1.2116688]. 
=============================================
[2019-04-03 22:42:32,415] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:42:32,415] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:42:32,417] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run5
[2019-04-03 22:42:32,949] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:42:32,949] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:42:32,954] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run5
[2019-04-03 22:42:33,721] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:42:33,721] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:42:33,724] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run5
[2019-04-03 22:42:33,837] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:42:33,837] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:42:33,856] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run5
[2019-04-03 22:42:35,713] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:42:35,713] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:42:35,716] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run5
[2019-04-03 22:42:36,009] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:42:36,009] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:42:36,011] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run5
[2019-04-03 22:42:37,125] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:42:37,125] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:42:37,127] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run5
[2019-04-03 22:42:37,905] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:42:37,905] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:42:37,907] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run5
[2019-04-03 22:42:41,660] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:42:41,661] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:42:41,662] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run5
[2019-04-03 22:42:49,540] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3321597e-11 3.5636124e-04 1.7326008e-09 5.1285849e-07 3.3421340e-04
 3.8213233e-10 9.9930882e-01], sum to 1.0000
[2019-04-03 22:42:49,540] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8911
[2019-04-03 22:42:49,582] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.71178837462841, -0.2542553896173845, 0.0, 1.0, 44953.25353150383], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 195000.0000, 
sim time next is 195600.0000, 
raw observation next is [-8.900000000000002, 78.0, 0.0, 0.0, 26.0, 22.65610158040396, -0.2598757296748834, 0.0, 1.0, 44957.35091117842], 
processed observation next is [1.0, 0.2608695652173913, 0.2160664819944598, 0.78, 0.0, 0.0, 0.6666666666666666, 0.38800846503366326, 0.4133747567750388, 0.0, 1.0, 0.21408262338656392], 
reward next is 0.7859, 
noisyNet noise sample is [array([0.1453601], dtype=float32), 0.20052657]. 
=============================================
[2019-04-03 22:42:50,789] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.3722735e-13 7.6804041e-05 2.9809065e-11 1.4199278e-08 1.7292623e-05
 1.3688019e-11 9.9990594e-01], sum to 1.0000
[2019-04-03 22:42:50,789] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1575
[2019-04-03 22:42:50,842] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.683333333333334, 85.33333333333334, 12.0, 0.0, 26.0, 24.54705520768697, 0.1885587474897131, 0.0, 1.0, 34866.34361374741], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 60600.0000, 
sim time next is 61200.0000, 
raw observation next is [5.5, 86.0, 0.0, 0.0, 26.0, 24.54550005086362, 0.1942964111072328, 0.0, 1.0, 42213.77513613884], 
processed observation next is [0.0, 0.7391304347826086, 0.6149584487534627, 0.86, 0.0, 0.0, 0.6666666666666666, 0.5454583375719683, 0.5647654703690775, 0.0, 1.0, 0.20101797683875636], 
reward next is 0.7990, 
noisyNet noise sample is [array([-0.0574689], dtype=float32), 1.1266402]. 
=============================================
[2019-04-03 22:43:08,300] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.4515496e-13 5.3107278e-06 3.1947341e-11 3.2778566e-08 3.3700526e-06
 4.1711193e-13 9.9999130e-01], sum to 1.0000
[2019-04-03 22:43:08,301] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5535
[2019-04-03 22:43:08,356] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 26.0, 24.9345717789821, 0.254527360551221, 0.0, 1.0, 50351.05393597209], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 246600.0000, 
sim time next is 247200.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 26.0, 24.89164812783088, 0.2442620471577862, 0.0, 1.0, 46740.79993686373], 
processed observation next is [1.0, 0.8695652173913043, 0.368421052631579, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5743040106525733, 0.5814206823859287, 0.0, 1.0, 0.2225752377945892], 
reward next is 0.7774, 
noisyNet noise sample is [array([-2.2037544], dtype=float32), 1.2973809]. 
=============================================
[2019-04-03 22:43:27,891] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.1083210e-13 4.3805339e-06 3.3466351e-11 2.9776577e-09 6.2517092e-06
 1.7006178e-11 9.9998939e-01], sum to 1.0000
[2019-04-03 22:43:27,891] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2068
[2019-04-03 22:43:27,940] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 87.0, 0.0, 0.0, 26.0, 24.94341702800877, 0.289773327911177, 0.0, 1.0, 56483.26170906942], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 581400.0000, 
sim time next is 582000.0000, 
raw observation next is [-2.1, 87.0, 0.0, 0.0, 26.0, 24.95242024973768, 0.2902364223350555, 0.0, 1.0, 45156.56541811807], 
processed observation next is [0.0, 0.7391304347826086, 0.404432132963989, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5793683541448068, 0.5967454741116852, 0.0, 1.0, 0.21503126389580035], 
reward next is 0.7850, 
noisyNet noise sample is [array([0.9309318], dtype=float32), -0.10871811]. 
=============================================
[2019-04-03 22:43:27,960] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[81.84101]
 [81.30531]
 [80.98277]
 [81.04219]
 [80.86535]], R is [[82.40362549]
 [82.31062317]
 [82.25906372]
 [82.28154755]
 [82.33310699]].
[2019-04-03 22:43:28,761] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.7957764e-15 1.5458731e-07 4.7882217e-13 7.8635576e-10 1.2151901e-07
 2.2694247e-15 9.9999976e-01], sum to 1.0000
[2019-04-03 22:43:28,761] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3722
[2019-04-03 22:43:28,823] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 96.0, 0.0, 0.0, 26.0, 24.72626913955947, 0.1784726496004304, 1.0, 1.0, 102230.3681227753], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 502200.0000, 
sim time next is 502800.0000, 
raw observation next is [1.1, 96.0, 0.0, 0.0, 26.0, 24.66555811382598, 0.2037195575255476, 1.0, 1.0, 108768.6452053885], 
processed observation next is [1.0, 0.8260869565217391, 0.49307479224376743, 0.96, 0.0, 0.0, 0.6666666666666666, 0.555463176152165, 0.5679065191751825, 1.0, 1.0, 0.517945929549469], 
reward next is 0.4821, 
noisyNet noise sample is [array([-1.5754589], dtype=float32), 1.2826862]. 
=============================================
[2019-04-03 22:43:31,202] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.2072964e-12 5.4152304e-04 3.5738540e-10 3.7517339e-08 5.4763368e-05
 6.6862529e-11 9.9940360e-01], sum to 1.0000
[2019-04-03 22:43:31,203] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1402
[2019-04-03 22:43:31,239] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 65.5, 0.0, 0.0, 26.0, 23.7124307865888, -0.01235491610844162, 0.0, 1.0, 43985.30666043075], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 625800.0000, 
sim time next is 626400.0000, 
raw observation next is [-4.5, 65.0, 0.0, 0.0, 26.0, 23.72189753451286, -0.01661821261597634, 0.0, 1.0, 43895.12979217836], 
processed observation next is [0.0, 0.2608695652173913, 0.3379501385041552, 0.65, 0.0, 0.0, 0.6666666666666666, 0.47682479454273824, 0.49446059579467455, 0.0, 1.0, 0.20902442758180173], 
reward next is 0.7910, 
noisyNet noise sample is [array([-0.8138195], dtype=float32), 1.2214686]. 
=============================================
[2019-04-03 22:43:34,696] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.6259237e-15 1.1291840e-07 2.2104550e-13 2.5685920e-10 1.1032127e-07
 6.6453466e-15 9.9999976e-01], sum to 1.0000
[2019-04-03 22:43:34,700] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7187
[2019-04-03 22:43:34,724] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.3, 96.0, 0.0, 0.0, 26.0, 24.83589931883902, 0.2335312388634035, 0.0, 1.0, 40390.96628448059], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 514800.0000, 
sim time next is 515400.0000, 
raw observation next is [3.383333333333333, 96.16666666666666, 0.0, 0.0, 26.0, 24.83846333596739, 0.2335783803970969, 0.0, 1.0, 40247.04249967411], 
processed observation next is [1.0, 1.0, 0.5563250230840259, 0.9616666666666666, 0.0, 0.0, 0.6666666666666666, 0.5698719446639492, 0.5778594601323657, 0.0, 1.0, 0.19165258333178148], 
reward next is 0.8083, 
noisyNet noise sample is [array([0.3355669], dtype=float32), -2.4195917]. 
=============================================
[2019-04-03 22:43:38,818] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.6684136e-12 2.3614604e-05 4.4491133e-11 3.0418413e-08 7.7073106e-05
 9.3129792e-12 9.9989927e-01], sum to 1.0000
[2019-04-03 22:43:38,820] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0790
[2019-04-03 22:43:38,851] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 69.0, 0.0, 0.0, 26.0, 24.95534319523085, 0.2057362213578684, 0.0, 1.0, 42391.46497055359], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 680400.0000, 
sim time next is 681000.0000, 
raw observation next is [-3.4, 69.0, 0.0, 0.0, 26.0, 24.93218653322971, 0.1983825815039489, 0.0, 1.0, 42323.9410842012], 
processed observation next is [0.0, 0.9130434782608695, 0.368421052631579, 0.69, 0.0, 0.0, 0.6666666666666666, 0.5776822111024759, 0.566127527167983, 0.0, 1.0, 0.2015425765914343], 
reward next is 0.7985, 
noisyNet noise sample is [array([1.6367769], dtype=float32), 0.81206465]. 
=============================================
[2019-04-03 22:43:38,887] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[80.52597]
 [80.60134]
 [80.69782]
 [80.76401]
 [80.89075]], R is [[80.41208649]
 [80.40609741]
 [80.39987183]
 [80.39356995]
 [80.38611603]].
[2019-04-03 22:43:46,528] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.4660516e-11 1.7045445e-03 8.0772500e-10 1.5973202e-07 1.0299688e-03
 4.0741355e-10 9.9726534e-01], sum to 1.0000
[2019-04-03 22:43:46,551] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3847
[2019-04-03 22:43:46,565] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 68.0, 0.0, 0.0, 26.0, 23.80190521137781, 0.006288519104980318, 0.0, 1.0, 44395.75759693926], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 622800.0000, 
sim time next is 623400.0000, 
raw observation next is [-4.5, 67.5, 0.0, 0.0, 26.0, 23.77562291684596, 0.004009932870546361, 0.0, 1.0, 44320.82560163779], 
processed observation next is [0.0, 0.21739130434782608, 0.3379501385041552, 0.675, 0.0, 0.0, 0.6666666666666666, 0.4813019097371634, 0.5013366442901821, 0.0, 1.0, 0.2110515504839895], 
reward next is 0.7889, 
noisyNet noise sample is [array([0.06912275], dtype=float32), 0.018857181]. 
=============================================
[2019-04-03 22:43:56,917] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1319617e-13 1.2692493e-05 2.0430287e-12 7.8401410e-09 9.9973922e-06
 4.1552411e-14 9.9997723e-01], sum to 1.0000
[2019-04-03 22:43:56,917] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4120
[2019-04-03 22:43:56,934] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 47.0, 82.5, 372.5, 26.0, 25.94479421960897, 0.4267617651475511, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 745200.0000, 
sim time next is 745800.0000, 
raw observation next is [-0.09999999999999999, 46.66666666666667, 83.33333333333333, 258.6666666666666, 26.0, 25.86579965471508, 0.276112395169583, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.4598337950138504, 0.46666666666666673, 0.27777777777777773, 0.28581952117863707, 0.6666666666666666, 0.6554833045595899, 0.5920374650565277, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.05566621], dtype=float32), -0.39804634]. 
=============================================
[2019-04-03 22:44:07,160] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.2928951e-14 2.7273226e-04 1.0957882e-12 4.5628901e-10 4.0341511e-06
 2.7464352e-14 9.9972326e-01], sum to 1.0000
[2019-04-03 22:44:07,161] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.1450
[2019-04-03 22:44:07,185] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.333333333333334, 91.33333333333333, 0.0, 0.0, 26.0, 25.31538461331616, 0.4212413689741921, 0.0, 1.0, 38034.58644570113], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 952800.0000, 
sim time next is 953400.0000, 
raw observation next is [5.416666666666667, 90.16666666666667, 0.0, 0.0, 26.0, 25.31646274046922, 0.4205714182111109, 0.0, 1.0, 38043.11356783254], 
processed observation next is [1.0, 0.0, 0.6126500461680519, 0.9016666666666667, 0.0, 0.0, 0.6666666666666666, 0.609705228372435, 0.640190472737037, 0.0, 1.0, 0.1811576836563454], 
reward next is 0.8188, 
noisyNet noise sample is [array([-0.43893218], dtype=float32), 0.37962052]. 
=============================================
[2019-04-03 22:44:14,277] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.0860542e-13 1.6217816e-04 1.7644835e-11 1.8140360e-08 1.0087085e-04
 5.0338444e-12 9.9973696e-01], sum to 1.0000
[2019-04-03 22:44:14,282] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8860
[2019-04-03 22:44:14,291] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.8, 100.0, 86.0, 0.0, 26.0, 24.77696554066742, 0.448875753451608, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1260000.0000, 
sim time next is 1260600.0000, 
raw observation next is [13.8, 100.0, 83.0, 0.0, 26.0, 24.76189946286332, 0.4466410715553518, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.844875346260388, 1.0, 0.27666666666666667, 0.0, 0.6666666666666666, 0.5634916219052766, 0.6488803571851173, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.34732744], dtype=float32), -0.72548085]. 
=============================================
[2019-04-03 22:44:15,329] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.5744547e-14 1.3091051e-03 6.3970990e-12 6.2367627e-09 5.5553692e-06
 2.0746593e-13 9.9868530e-01], sum to 1.0000
[2019-04-03 22:44:15,329] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9683
[2019-04-03 22:44:15,341] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.3, 80.0, 0.0, 0.0, 26.0, 25.7354152610196, 0.5783831428259015, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1059600.0000, 
sim time next is 1060200.0000, 
raw observation next is [13.3, 80.0, 0.0, 0.0, 26.0, 25.66772719070872, 0.5789715453115704, 0.0, 1.0, 61962.83648896827], 
processed observation next is [1.0, 0.2608695652173913, 0.8310249307479226, 0.8, 0.0, 0.0, 0.6666666666666666, 0.6389772658923935, 0.6929905151038568, 0.0, 1.0, 0.29506112613794416], 
reward next is 0.7049, 
noisyNet noise sample is [array([0.3782236], dtype=float32), -1.2558452]. 
=============================================
[2019-04-03 22:44:20,659] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.4821581e-12 4.1220261e-04 4.6624687e-11 6.1999117e-08 1.3109426e-05
 2.4483452e-11 9.9957460e-01], sum to 1.0000
[2019-04-03 22:44:20,665] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3414
[2019-04-03 22:44:20,673] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [18.46666666666667, 64.33333333333334, 88.0, 0.0, 26.0, 25.05931077393005, 0.4921197692192287, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1178400.0000, 
sim time next is 1179000.0000, 
raw observation next is [18.55, 64.0, 80.0, 0.0, 26.0, 25.045771035041, 0.488817314077735, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.976454293628809, 0.64, 0.26666666666666666, 0.0, 0.6666666666666666, 0.5871475862534167, 0.6629391046925783, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9725016], dtype=float32), 0.41646072]. 
=============================================
[2019-04-03 22:44:20,685] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[84.4785 ]
 [84.60979]
 [84.76346]
 [84.93777]
 [85.11419]], R is [[84.5260849 ]
 [84.68082428]
 [84.83401489]
 [84.985672  ]
 [85.13581848]].
[2019-04-03 22:44:21,192] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.6485339e-15 3.2899436e-05 9.8332388e-13 1.6418424e-09 7.6601879e-07
 7.5434294e-14 9.9996638e-01], sum to 1.0000
[2019-04-03 22:44:21,195] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2578
[2019-04-03 22:44:21,240] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 100.0, 9.0, 0.0, 26.0, 25.41889772240422, 0.4415864414770851, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1411200.0000, 
sim time next is 1411800.0000, 
raw observation next is [-0.6, 100.0, 12.0, 0.0, 26.0, 25.36559653900025, 0.428119985412359, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.44598337950138506, 1.0, 0.04, 0.0, 0.6666666666666666, 0.6137997115833542, 0.6427066618041196, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6125834], dtype=float32), 0.03930848]. 
=============================================
[2019-04-03 22:44:21,436] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.7523658e-14 3.4823253e-05 3.9645622e-12 5.2957967e-09 3.3614018e-05
 3.8260481e-13 9.9993157e-01], sum to 1.0000
[2019-04-03 22:44:21,439] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0131
[2019-04-03 22:44:21,452] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.68333333333333, 76.0, 0.0, 0.0, 26.0, 25.64900218269987, 0.6424397943303458, 0.0, 1.0, 34483.30313114921], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1126200.0000, 
sim time next is 1126800.0000, 
raw observation next is [10.5, 77.0, 0.0, 0.0, 26.0, 25.65036193176282, 0.6422468373032667, 0.0, 1.0, 27061.03723463996], 
processed observation next is [0.0, 0.043478260869565216, 0.7534626038781165, 0.77, 0.0, 0.0, 0.6666666666666666, 0.637530160980235, 0.7140822791010889, 0.0, 1.0, 0.12886208206971408], 
reward next is 0.8711, 
noisyNet noise sample is [array([0.4197989], dtype=float32), -0.420065]. 
=============================================
[2019-04-03 22:44:23,651] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.4785521e-15 2.7851783e-07 2.6959262e-13 6.6726798e-11 1.2785999e-07
 3.4361372e-15 9.9999964e-01], sum to 1.0000
[2019-04-03 22:44:23,651] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9098
[2019-04-03 22:44:23,669] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.1666666666666667, 94.0, 95.0, 0.0, 26.0, 25.71778115595882, 0.474072985260627, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1426800.0000, 
sim time next is 1427400.0000, 
raw observation next is [0.25, 93.5, 96.0, 0.0, 26.0, 25.66790112912408, 0.4457885290068311, 1.0, 1.0, 18680.8043927615], 
processed observation next is [1.0, 0.5217391304347826, 0.46952908587257625, 0.935, 0.32, 0.0, 0.6666666666666666, 0.6389917607603399, 0.6485961763356104, 1.0, 1.0, 0.08895621139410238], 
reward next is 0.9110, 
noisyNet noise sample is [array([1.9000891], dtype=float32), 0.24195167]. 
=============================================
[2019-04-03 22:44:26,439] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2398935e-12 1.9897483e-04 6.4643443e-11 3.8102353e-08 1.3665894e-05
 9.2076451e-12 9.9978727e-01], sum to 1.0000
[2019-04-03 22:44:26,439] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6716
[2019-04-03 22:44:26,443] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.0, 100.0, 76.0, 0.0, 26.0, 23.30472534653298, 0.1230740888890166, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1245600.0000, 
sim time next is 1246200.0000, 
raw observation next is [14.9, 100.0, 76.66666666666667, 0.0, 26.0, 23.30085875325023, 0.1224842962833061, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.8753462603878118, 1.0, 0.2555555555555556, 0.0, 0.6666666666666666, 0.441738229437519, 0.540828098761102, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.15585455], dtype=float32), -0.81832904]. 
=============================================
[2019-04-03 22:44:31,717] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.0758167e-15 5.2386739e-08 2.1292894e-12 5.3674643e-10 1.5257957e-07
 5.1374900e-14 9.9999988e-01], sum to 1.0000
[2019-04-03 22:44:31,717] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0665
[2019-04-03 22:44:31,762] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 26.0, 25.41128669834539, 0.4829851146447678, 0.0, 1.0, 59419.24187177058], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1386000.0000, 
sim time next is 1386600.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 26.0, 25.30777632944009, 0.4852049479551219, 0.0, 1.0, 72306.19909039894], 
processed observation next is [1.0, 0.043478260869565216, 0.46260387811634357, 0.95, 0.0, 0.0, 0.6666666666666666, 0.6089813607866743, 0.6617349826517073, 0.0, 1.0, 0.34431523376380446], 
reward next is 0.6557, 
noisyNet noise sample is [array([0.27431196], dtype=float32), 0.7364369]. 
=============================================
[2019-04-03 22:44:33,674] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.1179889e-14 1.6836668e-06 1.6196080e-12 4.0680055e-09 1.0602402e-06
 3.4923331e-14 9.9999726e-01], sum to 1.0000
[2019-04-03 22:44:33,674] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8683
[2019-04-03 22:44:33,751] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.4166666666666667, 92.5, 94.0, 0.0, 26.0, 25.31756444656965, 0.3170492065348053, 1.0, 1.0, 16605.69518837373], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1428600.0000, 
sim time next is 1429200.0000, 
raw observation next is [0.5, 92.0, 93.0, 0.0, 26.0, 24.93872140415865, 0.413037931625526, 1.0, 1.0, 169883.6876283516], 
processed observation next is [1.0, 0.5652173913043478, 0.4764542936288089, 0.92, 0.31, 0.0, 0.6666666666666666, 0.5782267836798874, 0.637679310541842, 1.0, 1.0, 0.8089699410873885], 
reward next is 0.1910, 
noisyNet noise sample is [array([-0.40637022], dtype=float32), -0.37469074]. 
=============================================
[2019-04-03 22:44:48,784] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.5592412e-11 3.7417546e-04 1.1308584e-09 5.8027115e-07 4.5247674e-05
 3.4705411e-10 9.9957997e-01], sum to 1.0000
[2019-04-03 22:44:48,784] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5695
[2019-04-03 22:44:48,795] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.616666666666667, 78.16666666666666, 0.0, 0.0, 26.0, 23.41869652185456, -0.06698592155754672, 0.0, 1.0, 47109.3492635877], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1839000.0000, 
sim time next is 1839600.0000, 
raw observation next is [-6.7, 78.0, 0.0, 0.0, 26.0, 23.38500275954588, -0.07418306208310112, 0.0, 1.0, 47142.55689489856], 
processed observation next is [0.0, 0.30434782608695654, 0.2770083102493075, 0.78, 0.0, 0.0, 0.6666666666666666, 0.4487502299621567, 0.47527231263896624, 0.0, 1.0, 0.22448836616618362], 
reward next is 0.7755, 
noisyNet noise sample is [array([1.3384999], dtype=float32), -0.6414938]. 
=============================================
[2019-04-03 22:45:16,937] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.3558083e-13 3.4896388e-05 7.6505954e-11 1.9979968e-08 3.0995423e-07
 4.2333021e-12 9.9996471e-01], sum to 1.0000
[2019-04-03 22:45:16,937] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9872
[2019-04-03 22:45:16,951] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.5, 90.16666666666667, 0.0, 0.0, 26.0, 24.33002248514952, 0.1566966284545257, 0.0, 1.0, 43830.69704096475], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2087400.0000, 
sim time next is 2088000.0000, 
raw observation next is [-5.6, 91.0, 0.0, 0.0, 26.0, 24.41168876910437, 0.1649026159851507, 0.0, 1.0, 43575.6144060193], 
processed observation next is [1.0, 0.17391304347826086, 0.30747922437673136, 0.91, 0.0, 0.0, 0.6666666666666666, 0.5343073974253642, 0.5549675386617169, 0.0, 1.0, 0.20750292574294904], 
reward next is 0.7925, 
noisyNet noise sample is [array([0.16712879], dtype=float32), 0.26833254]. 
=============================================
[2019-04-03 22:45:16,955] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[76.80988]
 [76.89298]
 [76.97713]
 [77.05712]
 [77.13418]], R is [[76.77355957]
 [76.79710388]
 [76.82221985]
 [76.84745026]
 [76.87277985]].
[2019-04-03 22:45:18,044] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-03 22:45:18,050] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-03 22:45:18,050] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:45:18,052] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run7
[2019-04-03 22:45:18,069] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-03 22:45:18,070] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-03 22:45:18,071] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:45:18,071] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:45:18,073] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run7
[2019-04-03 22:45:18,085] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run7
[2019-04-03 22:46:29,074] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.23184994], dtype=float32), 0.319062]
[2019-04-03 22:46:29,075] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-1.033333333333333, 58.66666666666666, 0.0, 0.0, 26.0, 25.2926247617061, 0.3341923956092336, 0.0, 1.0, 36707.54366847639]
[2019-04-03 22:46:29,075] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-03 22:46:29,085] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [6.7610843e-12 9.4307114e-05 1.5814129e-10 6.4592420e-08 3.5874298e-05
 1.7936914e-11 9.9986970e-01], sampled 0.8865571418630039
[2019-04-03 22:46:42,394] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.23184994], dtype=float32), 0.319062]
[2019-04-03 22:46:42,395] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [12.3, 77.66666666666667, 0.0, 0.0, 26.0, 25.66759265060125, 0.5887376688594642, 1.0, 1.0, 0.0]
[2019-04-03 22:46:42,395] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-03 22:46:42,395] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [5.6983430e-15 4.4492645e-06 2.0014841e-13 5.3242333e-10 1.0634768e-06
 9.7407662e-15 9.9999452e-01], sampled 0.09709655248033522
[2019-04-03 22:47:16,598] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.23184994], dtype=float32), 0.319062]
[2019-04-03 22:47:16,599] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-1.366666666666667, 54.66666666666667, 0.0, 0.0, 26.0, 25.03336721118488, 0.3185207753691872, 0.0, 1.0, 94714.18646303183]
[2019-04-03 22:47:16,599] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-03 22:47:16,601] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.3478965e-12 3.0197272e-05 2.3720038e-11 2.3240798e-08 9.9828785e-06
 2.3646666e-12 9.9995971e-01], sampled 0.546619348089278
[2019-04-03 22:47:56,231] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.23184994], dtype=float32), 0.319062]
[2019-04-03 22:47:56,232] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-4.0, 68.0, 0.0, 0.0, 26.0, 25.91452310221571, 0.6149168843155913, 1.0, 1.0, 0.0]
[2019-04-03 22:47:56,232] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-03 22:47:56,233] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.4512188e-12 1.2451746e-05 2.0648858e-11 2.3281043e-08 1.0350581e-05
 1.5473385e-12 9.9997711e-01], sampled 0.1533026049264984
[2019-04-03 22:48:22,255] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.6610 239891183.7710 1604.9478
[2019-04-03 22:48:56,222] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.3658 263473189.0111 1556.8321
[2019-04-03 22:48:58,544] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.6528 275802911.4828 1233.0757
[2019-04-03 22:48:59,595] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 600000, evaluation results [600000.0, 7241.365766613863, 263473189.01109415, 1556.8320537688296, 7353.661029661943, 239891183.77099577, 1604.947837403963, 7182.652802462828, 275802911.48280716, 1233.075658274952]
[2019-04-03 22:49:01,402] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.5129486e-12 5.1087260e-05 3.4628420e-10 1.3008701e-08 3.4631000e-06
 2.3028277e-11 9.9994540e-01], sum to 1.0000
[2019-04-03 22:49:01,402] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5676
[2019-04-03 22:49:01,415] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.7, 83.0, 0.0, 0.0, 26.0, 24.06241220357136, 0.07261119835244921, 0.0, 1.0, 43563.55716902344], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2095200.0000, 
sim time next is 2095800.0000, 
raw observation next is [-6.700000000000001, 82.16666666666667, 0.0, 0.0, 26.0, 24.05790031469976, 0.0612982478511782, 0.0, 1.0, 43596.21460823753], 
processed observation next is [1.0, 0.2608695652173913, 0.2770083102493075, 0.8216666666666668, 0.0, 0.0, 0.6666666666666666, 0.5048250262249802, 0.5204327492837261, 0.0, 1.0, 0.20760102194398825], 
reward next is 0.7924, 
noisyNet noise sample is [array([-1.3289375], dtype=float32), 0.00723896]. 
=============================================
[2019-04-03 22:49:06,866] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.5063885e-12 2.2359419e-04 1.7071967e-10 5.8846833e-08 2.1304470e-05
 6.4159325e-12 9.9975497e-01], sum to 1.0000
[2019-04-03 22:49:06,866] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3408
[2019-04-03 22:49:06,883] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.7, 90.33333333333334, 0.0, 0.0, 26.0, 24.49503058595728, 0.1457477561932128, 0.0, 1.0, 43690.62747850252], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2088600.0000, 
sim time next is 2089200.0000, 
raw observation next is [-5.8, 89.66666666666667, 0.0, 0.0, 26.0, 24.39211666715576, 0.1350052190174746, 0.0, 1.0, 43568.73127809018], 
processed observation next is [1.0, 0.17391304347826086, 0.30193905817174516, 0.8966666666666667, 0.0, 0.0, 0.6666666666666666, 0.5326763889296467, 0.5450017396724915, 0.0, 1.0, 0.20747014894328655], 
reward next is 0.7925, 
noisyNet noise sample is [array([1.4696141], dtype=float32), -1.6968945]. 
=============================================
[2019-04-03 22:49:10,285] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.8417848e-12 3.1742951e-05 4.1221287e-10 3.6975024e-08 1.3292483e-05
 2.6917697e-11 9.9995482e-01], sum to 1.0000
[2019-04-03 22:49:10,285] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6040
[2019-04-03 22:49:10,298] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.7, 78.0, 0.0, 0.0, 26.0, 24.49425654861614, 0.161493506533807, 0.0, 1.0, 42439.06301869988], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2170200.0000, 
sim time next is 2170800.0000, 
raw observation next is [-6.7, 78.0, 0.0, 0.0, 26.0, 24.49888037273671, 0.1600654918974205, 0.0, 1.0, 42362.12929025448], 
processed observation next is [1.0, 0.13043478260869565, 0.2770083102493075, 0.78, 0.0, 0.0, 0.6666666666666666, 0.5415733643947259, 0.5533551639658069, 0.0, 1.0, 0.201724425191688], 
reward next is 0.7983, 
noisyNet noise sample is [array([1.5689042], dtype=float32), -0.90095377]. 
=============================================
[2019-04-03 22:49:24,452] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.38202938e-11 1.55816949e-03 9.28422117e-10 2.06823117e-07
 1.25896710e-04 1.08516585e-10 9.98315692e-01], sum to 1.0000
[2019-04-03 22:49:24,453] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3322
[2019-04-03 22:49:24,601] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.716666666666667, 64.5, 133.0, 420.0, 26.0, 25.04547068049989, 0.2871340569814284, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2369400.0000, 
sim time next is 2370000.0000, 
raw observation next is [-2.633333333333333, 64.0, 136.0, 435.0, 26.0, 24.97732918325845, 0.2825121438706844, 0.0, 1.0, 45070.69594287476], 
processed observation next is [0.0, 0.43478260869565216, 0.38965835641735924, 0.64, 0.4533333333333333, 0.48066298342541436, 0.6666666666666666, 0.5814440986048709, 0.5941707146235614, 0.0, 1.0, 0.21462236163273693], 
reward next is 0.7854, 
noisyNet noise sample is [array([-0.87849945], dtype=float32), -1.1767234]. 
=============================================
[2019-04-03 22:49:24,608] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[75.40493]
 [75.88932]
 [76.24355]
 [76.50162]
 [76.64912]], R is [[75.10896301]
 [75.35787201]
 [75.60429382]
 [75.84825134]
 [76.08976746]].
[2019-04-03 22:49:24,731] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.9155720e-13 3.6433912e-06 4.5288664e-12 4.5790869e-09 3.2488106e-06
 6.3106624e-14 9.9999309e-01], sum to 1.0000
[2019-04-03 22:49:24,731] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6238
[2019-04-03 22:49:24,802] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.75, 71.0, 117.0, 0.0, 26.0, 25.9642075830378, 0.3852730545837105, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2197800.0000, 
sim time next is 2198400.0000, 
raw observation next is [-4.666666666666667, 71.0, 121.3333333333333, 0.0, 26.0, 25.93190664160938, 0.3783519087808544, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.3333333333333333, 0.71, 0.40444444444444433, 0.0, 0.6666666666666666, 0.6609922201341151, 0.6261173029269514, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6577378], dtype=float32), 0.5780738]. 
=============================================
[2019-04-03 22:49:31,566] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.8308699e-12 1.3581292e-04 7.4584700e-10 7.7913363e-08 1.3024191e-05
 9.6705612e-12 9.9985111e-01], sum to 1.0000
[2019-04-03 22:49:31,566] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4831
[2019-04-03 22:49:31,607] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 91.0, 0.0, 0.0, 26.0, 23.64195431450531, -0.02871170786308557, 0.0, 1.0, 43241.56110321133], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2266800.0000, 
sim time next is 2267400.0000, 
raw observation next is [-8.9, 91.0, 0.0, 0.0, 26.0, 23.5845791257484, -0.03019881099357069, 0.0, 1.0, 43230.28101855238], 
processed observation next is [1.0, 0.21739130434782608, 0.21606648199445982, 0.91, 0.0, 0.0, 0.6666666666666666, 0.4653815938123668, 0.48993372966880977, 0.0, 1.0, 0.2058584810407256], 
reward next is 0.7941, 
noisyNet noise sample is [array([-0.46880245], dtype=float32), 0.6241271]. 
=============================================
[2019-04-03 22:49:33,574] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.4095771e-11 1.8111643e-05 3.4083447e-10 1.3805983e-07 3.2761644e-04
 2.8463273e-11 9.9965417e-01], sum to 1.0000
[2019-04-03 22:49:33,574] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9977
[2019-04-03 22:49:33,622] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.4, 43.0, 0.0, 0.0, 26.0, 24.95973928872527, 0.256278156803975, 0.0, 1.0, 46633.13675813874], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2401200.0000, 
sim time next is 2401800.0000, 
raw observation next is [-2.566666666666666, 42.83333333333334, 0.0, 0.0, 26.0, 24.95671212740697, 0.2556937155692121, 0.0, 1.0, 48895.22770694947], 
processed observation next is [0.0, 0.8260869565217391, 0.39150507848568794, 0.42833333333333345, 0.0, 0.0, 0.6666666666666666, 0.5797260106172475, 0.5852312385230707, 0.0, 1.0, 0.23283441765214033], 
reward next is 0.7672, 
noisyNet noise sample is [array([-1.5643501], dtype=float32), -0.9390139]. 
=============================================
[2019-04-03 22:49:42,479] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.2342213e-10 1.3371474e-03 6.0333782e-09 5.3392705e-07 1.6618233e-04
 6.5157008e-10 9.9849617e-01], sum to 1.0000
[2019-04-03 22:49:42,479] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4039
[2019-04-03 22:49:42,553] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 43.0, 68.5, 721.0, 26.0, 25.1684667674995, 0.241653605885497, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2455200.0000, 
sim time next is 2455800.0000, 
raw observation next is [-5.050000000000001, 41.83333333333334, 71.0, 739.6666666666667, 26.0, 25.11927619299918, 0.2363331951384152, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.32271468144044324, 0.41833333333333345, 0.23666666666666666, 0.8173112338858196, 0.6666666666666666, 0.593273016083265, 0.578777731712805, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5821807], dtype=float32), 0.45535067]. 
=============================================
[2019-04-03 22:49:43,017] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.4076812e-12 3.2472220e-05 5.7180021e-11 2.3677803e-08 1.0027406e-05
 1.2661134e-11 9.9995744e-01], sum to 1.0000
[2019-04-03 22:49:43,017] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8353
[2019-04-03 22:49:43,090] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.35, 29.5, 0.0, 0.0, 26.0, 24.94139556688015, 0.2493861393920583, 0.0, 1.0, 106876.6716445727], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2489400.0000, 
sim time next is 2490000.0000, 
raw observation next is [-0.4666666666666666, 29.33333333333334, 0.0, 0.0, 26.0, 25.01811651151726, 0.2650450265096679, 0.0, 1.0, 62842.33636042243], 
processed observation next is [0.0, 0.8260869565217391, 0.44967682363804257, 0.2933333333333334, 0.0, 0.0, 0.6666666666666666, 0.5848430426264383, 0.5883483421698893, 0.0, 1.0, 0.2992492207639163], 
reward next is 0.7008, 
noisyNet noise sample is [array([-0.05084954], dtype=float32), 0.2663524]. 
=============================================
[2019-04-03 22:49:43,109] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[73.3696  ]
 [72.96543 ]
 [72.05811 ]
 [71.93003 ]
 [71.647285]], R is [[73.47280121]
 [73.22914124]
 [72.67024994]
 [72.77539825]
 [72.85185242]].
[2019-04-03 22:49:53,934] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.6476167e-11 1.7866885e-05 1.4772657e-10 1.4447863e-07 2.2776086e-04
 4.1027508e-11 9.9975425e-01], sum to 1.0000
[2019-04-03 22:49:53,934] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5714
[2019-04-03 22:49:53,996] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.233333333333333, 28.5, 89.0, 840.6666666666666, 26.0, 24.91859081379431, 0.2696375250157744, 0.0, 1.0, 18719.87687325451], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2465400.0000, 
sim time next is 2466000.0000, 
raw observation next is [1.6, 28.0, 88.5, 838.5, 26.0, 24.94172266456691, 0.2738209544375068, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.5069252077562327, 0.28, 0.295, 0.9265193370165746, 0.6666666666666666, 0.5784768887139092, 0.5912736514791689, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6787375], dtype=float32), -0.937955]. 
=============================================
[2019-04-03 22:49:54,004] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[71.88945 ]
 [71.80765 ]
 [71.698944]
 [71.4998  ]
 [71.323555]], R is [[72.16896057]
 [72.35813141]
 [72.51006317]
 [72.62872314]
 [72.78210449]].
[2019-04-03 22:50:01,570] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3228677e-15 9.8541777e-06 5.6501087e-13 1.5255404e-09 1.3598920e-07
 4.9147685e-15 9.9998999e-01], sum to 1.0000
[2019-04-03 22:50:01,570] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9998
[2019-04-03 22:50:01,640] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 45.0, 216.0, 130.0, 26.0, 26.42136004223122, 0.4347133469029609, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2644200.0000, 
sim time next is 2644800.0000, 
raw observation next is [0.5, 45.66666666666667, 205.8333333333333, 142.6666666666667, 26.0, 25.93119349378101, 0.4649407078535819, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.4764542936288089, 0.4566666666666667, 0.686111111111111, 0.15764272559852677, 0.6666666666666666, 0.6609327911484174, 0.6549802359511939, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.73581445], dtype=float32), 1.05557]. 
=============================================
[2019-04-03 22:50:20,080] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.3886339e-14 3.0294364e-06 1.5775062e-11 1.3541100e-08 2.3974369e-06
 1.8742659e-13 9.9999452e-01], sum to 1.0000
[2019-04-03 22:50:20,080] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2384
[2019-04-03 22:50:20,114] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.666666666666666, 26.0, 114.1666666666667, 0.0, 26.0, 25.85820477985626, 0.3073862905090105, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2817600.0000, 
sim time next is 2818200.0000, 
raw observation next is [6.833333333333334, 25.0, 110.3333333333333, 0.0, 26.0, 25.38729132603896, 0.3276610875354529, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.651892890120037, 0.25, 0.36777777777777765, 0.0, 0.6666666666666666, 0.6156076105032465, 0.6092203625118177, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.3396716], dtype=float32), 1.2917563]. 
=============================================
[2019-04-03 22:50:23,306] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.2906881e-14 9.2269630e-07 2.8254244e-12 4.8882765e-09 1.2498477e-07
 2.3713333e-14 9.9999893e-01], sum to 1.0000
[2019-04-03 22:50:23,306] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7678
[2019-04-03 22:50:23,358] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 59.0, 0.0, 0.0, 26.0, 25.18841126752067, 0.3965465015578132, 1.0, 1.0, 58629.55909428257], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2747400.0000, 
sim time next is 2748000.0000, 
raw observation next is [-5.0, 59.0, 0.0, 0.0, 26.0, 25.14757489129779, 0.391176039485531, 0.0, 1.0, 76014.06243184343], 
processed observation next is [1.0, 0.8260869565217391, 0.32409972299168976, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5956312409414825, 0.6303920131618437, 0.0, 1.0, 0.3619717258659211], 
reward next is 0.6380, 
noisyNet noise sample is [array([-1.6980385], dtype=float32), 0.07519765]. 
=============================================
[2019-04-03 22:50:23,368] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[84.08087]
 [84.04477]
 [83.95619]
 [83.69238]
 [83.5651 ]], R is [[79.87001801]
 [79.79212952]
 [79.69757843]
 [79.49057007]
 [79.44342804]].
[2019-04-03 22:50:24,657] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.8402343e-14 1.0713069e-05 1.8774281e-13 2.9858502e-09 8.3880519e-07
 1.8920256e-14 9.9998844e-01], sum to 1.0000
[2019-04-03 22:50:24,657] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.6809
[2019-04-03 22:50:24,677] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.5, 45.0, 142.6666666666667, 737.0, 26.0, 25.97714927833372, 0.4447784992210151, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2807400.0000, 
sim time next is 2808000.0000, 
raw observation next is [2.0, 44.0, 151.5, 724.0, 26.0, 25.92619419508869, 0.441639200138892, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.518005540166205, 0.44, 0.505, 0.8, 0.6666666666666666, 0.6605161829240576, 0.647213066712964, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7828159], dtype=float32), -0.29189283]. 
=============================================
[2019-04-03 22:50:24,695] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[90.657555]
 [90.437355]
 [90.29467 ]
 [90.21406 ]
 [90.16472 ]], R is [[90.9622879 ]
 [91.05266571]
 [91.14214325]
 [91.23072052]
 [91.31841278]].
[2019-04-03 22:50:25,219] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.36404785e-11 4.31030086e-04 2.49617860e-09 2.13090232e-07
 8.05359232e-05 2.01660230e-10 9.99488235e-01], sum to 1.0000
[2019-04-03 22:50:25,220] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3704
[2019-04-03 22:50:25,250] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.49645367619951, 0.1591783566453998, 0.0, 1.0, 40748.05502187481], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2778600.0000, 
sim time next is 2779200.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.46122163873633, 0.1528635139783321, 0.0, 1.0, 40753.3310897811], 
processed observation next is [1.0, 0.17391304347826086, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5384351365613608, 0.5509545046594441, 0.0, 1.0, 0.19406348137991], 
reward next is 0.8059, 
noisyNet noise sample is [array([0.20645194], dtype=float32), -0.833113]. 
=============================================
[2019-04-03 22:50:26,264] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.6742115e-11 5.9501170e-05 1.1358985e-09 1.7575852e-07 4.4137635e-04
 5.8146932e-11 9.9949896e-01], sum to 1.0000
[2019-04-03 22:50:26,284] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4042
[2019-04-03 22:50:26,335] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.52841864237258, 0.1735935863697132, 0.0, 1.0, 40884.82981576209], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2776800.0000, 
sim time next is 2777400.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.50462039352297, 0.1694631798025314, 0.0, 1.0, 40823.58189594799], 
processed observation next is [1.0, 0.13043478260869565, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5420516994602474, 0.5564877266008438, 0.0, 1.0, 0.19439800902832374], 
reward next is 0.8056, 
noisyNet noise sample is [array([1.4805709], dtype=float32), -0.15490544]. 
=============================================
[2019-04-03 22:50:34,121] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.6473348e-12 9.4255720e-06 3.3320222e-11 1.7793704e-08 5.8971040e-05
 1.4048163e-12 9.9993157e-01], sum to 1.0000
[2019-04-03 22:50:34,121] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9883
[2019-04-03 22:50:34,146] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.666666666666667, 65.33333333333334, 0.0, 0.0, 26.0, 25.22931550665036, 0.3525981238629425, 0.0, 1.0, 43155.42125199507], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2848800.0000, 
sim time next is 2849400.0000, 
raw observation next is [1.5, 67.0, 0.0, 0.0, 26.0, 25.22468865733192, 0.3498030718973957, 0.0, 1.0, 42724.76100815783], 
processed observation next is [1.0, 1.0, 0.5041551246537397, 0.67, 0.0, 0.0, 0.6666666666666666, 0.6020573881109934, 0.6166010239657985, 0.0, 1.0, 0.20345124289598968], 
reward next is 0.7965, 
noisyNet noise sample is [array([-0.01806241], dtype=float32), -0.11260101]. 
=============================================
[2019-04-03 22:50:35,233] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.8117184e-14 4.9950080e-07 1.4043649e-12 7.6408596e-10 2.8731856e-07
 7.9737276e-15 9.9999928e-01], sum to 1.0000
[2019-04-03 22:50:35,236] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4014
[2019-04-03 22:50:35,314] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 87.33333333333334, 0.0, 0.0, 26.0, 25.17010881129664, 0.3855424689253988, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2920800.0000, 
sim time next is 2921400.0000, 
raw observation next is [-1.0, 85.0, 0.0, 0.0, 26.0, 25.04154787872397, 0.3776836742329784, 1.0, 1.0, 62374.28884173473], 
processed observation next is [1.0, 0.8260869565217391, 0.4349030470914128, 0.85, 0.0, 0.0, 0.6666666666666666, 0.586795656560331, 0.6258945580776595, 1.0, 1.0, 0.29702042305587967], 
reward next is 0.7030, 
noisyNet noise sample is [array([-0.9657306], dtype=float32), 0.51002276]. 
=============================================
[2019-04-03 22:50:36,656] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.7959407e-11 2.7982690e-04 1.7370180e-09 5.1668962e-07 1.0149844e-03
 3.2121616e-10 9.9870467e-01], sum to 1.0000
[2019-04-03 22:50:36,672] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3241
[2019-04-03 22:50:36,689] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.99787878471326, 0.2838739300970952, 0.0, 1.0, 38296.97886870749], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3020400.0000, 
sim time next is 3021000.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.96197197134993, 0.2769486931268647, 0.0, 1.0, 38224.09640888419], 
processed observation next is [0.0, 1.0, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5801643309458274, 0.5923162310422883, 0.0, 1.0, 0.18201950670897232], 
reward next is 0.8180, 
noisyNet noise sample is [array([0.8954779], dtype=float32), -0.11027294]. 
=============================================
[2019-04-03 22:50:36,726] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[70.226295]
 [70.351845]
 [70.46861 ]
 [70.594215]
 [70.702705]], R is [[70.2157135 ]
 [70.33119202]
 [70.44515991]
 [70.5575943 ]
 [70.66851807]].
[2019-04-03 22:50:51,363] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.6065620e-14 2.1179163e-04 4.6642941e-12 4.0067314e-09 6.0041915e-05
 1.3611774e-12 9.9972814e-01], sum to 1.0000
[2019-04-03 22:50:51,365] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8758
[2019-04-03 22:50:51,383] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.5, 100.0, 0.0, 0.0, 26.0, 25.1966048999119, 0.2861746029914237, 0.0, 1.0, 54906.50319593125], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3124200.0000, 
sim time next is 3124800.0000, 
raw observation next is [2.6, 100.0, 0.0, 0.0, 26.0, 25.19710749743185, 0.2903823954649349, 0.0, 1.0, 54296.53337979856], 
processed observation next is [1.0, 0.17391304347826086, 0.5346260387811635, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5997589581193209, 0.596794131821645, 0.0, 1.0, 0.2585549208561836], 
reward next is 0.7414, 
noisyNet noise sample is [array([2.7789373], dtype=float32), 0.1061526]. 
=============================================
[2019-04-03 22:50:59,915] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.2229712e-14 3.7439363e-06 7.9552066e-13 4.5434652e-09 2.8633440e-04
 5.1998634e-14 9.9970990e-01], sum to 1.0000
[2019-04-03 22:50:59,915] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.0523575e-12 1.5973253e-06 8.3177958e-11 1.2056485e-08 5.6490881e-06
 1.1242445e-12 9.9999273e-01], sum to 1.0000
[2019-04-03 22:50:59,917] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1246
[2019-04-03 22:50:59,918] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0522
[2019-04-03 22:50:59,935] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.266666666666667, 77.0, 112.3333333333333, 801.1666666666666, 26.0, 26.54866305797056, 0.7419365752573507, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3237600.0000, 
sim time next is 3238200.0000, 
raw observation next is [-2.2, 75.5, 113.0, 811.0, 26.0, 26.59164577187602, 0.7463565871461891, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4016620498614959, 0.755, 0.37666666666666665, 0.8961325966850828, 0.6666666666666666, 0.7159704809896684, 0.7487855290487296, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.71927613], dtype=float32), 1.0367514]. 
=============================================
[2019-04-03 22:50:59,937] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.51024383449023, 0.5364081586945095, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3358800.0000, 
sim time next is 3359400.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.5939360214883, 0.5329912760266617, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6328280017906917, 0.677663758675554, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.12870786], dtype=float32), -0.6894243]. 
=============================================
[2019-04-03 22:51:04,067] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.0791253e-14 1.9094448e-06 7.7033649e-12 2.2773441e-09 1.9782006e-05
 2.1316317e-13 9.9997830e-01], sum to 1.0000
[2019-04-03 22:51:04,068] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4473
[2019-04-03 22:51:04,083] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.333333333333334, 57.33333333333334, 116.3333333333333, 800.1666666666666, 26.0, 26.36966099388933, 0.6036673320237477, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3325200.0000, 
sim time next is 3325800.0000, 
raw observation next is [-6.166666666666666, 55.66666666666666, 116.6666666666667, 802.3333333333334, 26.0, 26.37188237478832, 0.6031385605387916, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.29178208679593726, 0.5566666666666665, 0.388888888888889, 0.8865561694290977, 0.6666666666666666, 0.6976568645656934, 0.701046186846264, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3598655], dtype=float32), -1.1499918]. 
=============================================
[2019-04-03 22:51:10,007] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.5941928e-13 9.4870702e-06 2.4887490e-11 3.2558603e-08 5.0855626e-05
 2.5642238e-13 9.9993956e-01], sum to 1.0000
[2019-04-03 22:51:10,007] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9239
[2019-04-03 22:51:10,058] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 26.0, 25.05325031670651, 0.4154178017884385, 1.0, 1.0, 86740.89970068041], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3439800.0000, 
sim time next is 3440400.0000, 
raw observation next is [1.0, 79.0, 0.0, 0.0, 26.0, 24.98559838756763, 0.4260192120123503, 0.0, 1.0, 94806.70357484039], 
processed observation next is [1.0, 0.8260869565217391, 0.4903047091412743, 0.79, 0.0, 0.0, 0.6666666666666666, 0.5821331989639692, 0.6420064040041168, 0.0, 1.0, 0.45146049321352566], 
reward next is 0.5485, 
noisyNet noise sample is [array([0.42014608], dtype=float32), -0.9367353]. 
=============================================
[2019-04-03 22:51:15,610] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.1044545e-13 7.1787554e-06 5.8219631e-11 2.3393985e-08 1.5150063e-05
 5.3417357e-12 9.9997771e-01], sum to 1.0000
[2019-04-03 22:51:15,612] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0935
[2019-04-03 22:51:15,631] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.43372230111131, 0.4669856147492933, 0.0, 1.0, 18763.22715047029], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3544200.0000, 
sim time next is 3544800.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.42331593296998, 0.4558106068739242, 0.0, 1.0, 28024.46274718341], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.6, 0.0, 0.0, 0.6666666666666666, 0.6186096610808317, 0.6519368689579748, 0.0, 1.0, 0.1334498226056353], 
reward next is 0.8666, 
noisyNet noise sample is [array([1.257271], dtype=float32), -0.74153095]. 
=============================================
[2019-04-03 22:51:31,321] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.4408792e-14 9.9365670e-06 8.8076372e-13 9.0294989e-09 4.3882824e-06
 1.9822948e-14 9.9998569e-01], sum to 1.0000
[2019-04-03 22:51:31,321] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9148
[2019-04-03 22:51:31,330] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 60.0, 113.5, 798.5, 26.0, 26.57326062929243, 0.6404652817248134, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3841200.0000, 
sim time next is 3841800.0000, 
raw observation next is [-1.0, 60.0, 114.6666666666667, 806.3333333333334, 26.0, 26.5925658366967, 0.6340639953343953, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4349030470914128, 0.6, 0.38222222222222235, 0.89097605893186, 0.6666666666666666, 0.7160471530580583, 0.7113546651114651, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9022347], dtype=float32), 2.4472497]. 
=============================================
[2019-04-03 22:51:33,300] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.8332931e-12 1.2480402e-03 5.2589688e-10 4.5902604e-07 3.3972858e-04
 8.2313004e-11 9.9841177e-01], sum to 1.0000
[2019-04-03 22:51:33,301] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9116
[2019-04-03 22:51:33,312] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.86921308563347, 0.2823751785331112, 0.0, 1.0, 43978.25131803635], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3818400.0000, 
sim time next is 3819000.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.82940208049961, 0.2788505953783416, 0.0, 1.0, 43989.55736305994], 
processed observation next is [1.0, 0.17391304347826086, 0.3518005540166205, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5691168400416341, 0.5929501984594472, 0.0, 1.0, 0.2094740826812378], 
reward next is 0.7905, 
noisyNet noise sample is [array([0.4520459], dtype=float32), -0.17355348]. 
=============================================
[2019-04-03 22:51:33,329] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[75.19081 ]
 [75.295975]
 [75.3923  ]
 [75.46854 ]
 [75.54953 ]], R is [[75.12023926]
 [75.15961456]
 [75.1987915 ]
 [75.23784637]
 [75.27680206]].
[2019-04-03 22:51:40,943] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.62148461e-12 7.59890681e-05 8.86854201e-10 1.11246315e-07
 2.17682846e-05 3.87591452e-11 9.99902129e-01], sum to 1.0000
[2019-04-03 22:51:40,943] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7124
[2019-04-03 22:51:40,966] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.166666666666668, 49.66666666666667, 0.0, 0.0, 26.0, 25.38877902698252, 0.445921449068828, 0.0, 1.0, 60796.20025510197], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3967800.0000, 
sim time next is 3968400.0000, 
raw observation next is [-8.333333333333334, 50.33333333333334, 0.0, 0.0, 26.0, 25.40018642616397, 0.4330337547906083, 0.0, 1.0, 43639.89129622265], 
processed observation next is [1.0, 0.9565217391304348, 0.23176361957525393, 0.5033333333333334, 0.0, 0.0, 0.6666666666666666, 0.6166822021803308, 0.6443445849302027, 0.0, 1.0, 0.20780900617248882], 
reward next is 0.7922, 
noisyNet noise sample is [array([0.39650223], dtype=float32), -0.08494103]. 
=============================================
[2019-04-03 22:51:41,359] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.4929606e-14 3.3955712e-07 2.7421372e-12 3.1993888e-09 3.1539592e-05
 1.1562645e-13 9.9996805e-01], sum to 1.0000
[2019-04-03 22:51:41,359] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3269
[2019-04-03 22:51:41,391] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 38.0, 42.5, 352.5, 26.0, 26.5732243882371, 0.6569103423491589, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3949200.0000, 
sim time next is 3949800.0000, 
raw observation next is [-5.166666666666667, 38.5, 34.66666666666666, 291.3333333333333, 26.0, 26.33659152629602, 0.6320956054780714, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.31948291782086796, 0.385, 0.11555555555555552, 0.321915285451197, 0.6666666666666666, 0.6947159605246682, 0.7106985351593571, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2642912], dtype=float32), 0.20646055]. 
=============================================
[2019-04-03 22:51:50,404] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.9825161e-14 3.8641451e-06 5.0487734e-13 1.7193094e-09 3.9595743e-06
 3.1746805e-14 9.9999225e-01], sum to 1.0000
[2019-04-03 22:51:50,410] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9319
[2019-04-03 22:51:50,417] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 34.5, 0.0, 0.0, 26.0, 26.50098669370006, 0.6573449598580218, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4126200.0000, 
sim time next is 4126800.0000, 
raw observation next is [3.0, 35.0, 0.0, 0.0, 26.0, 26.48127553929983, 0.6545546033638159, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.5457063711911359, 0.35, 0.0, 0.0, 0.6666666666666666, 0.7067729616083192, 0.7181848677879387, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3908222], dtype=float32), -0.5039771]. 
=============================================
[2019-04-03 22:51:51,053] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.5451023e-13 5.5381493e-04 2.1676338e-11 1.3947438e-07 1.5455401e-03
 2.9128727e-11 9.9790049e-01], sum to 1.0000
[2019-04-03 22:51:51,058] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4412
[2019-04-03 22:51:51,069] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.0, 53.5, 121.0, 822.0, 26.0, 25.21092011611271, 0.39681175506728, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4275000.0000, 
sim time next is 4275600.0000, 
raw observation next is [6.333333333333333, 53.0, 120.8333333333333, 826.1666666666666, 26.0, 25.20294897825052, 0.3975644577062603, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.6380424746075716, 0.53, 0.4027777777777777, 0.9128913443830571, 0.6666666666666666, 0.6002457481875435, 0.6325214859020868, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8942065], dtype=float32), 1.9961354]. 
=============================================
[2019-04-03 22:52:02,425] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.0246908e-13 2.6754767e-04 1.9143364e-11 2.7210569e-09 4.8343678e-05
 1.2672016e-12 9.9968410e-01], sum to 1.0000
[2019-04-03 22:52:02,428] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.8229
[2019-04-03 22:52:02,443] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 80.0, 20.0, 38.66666666666666, 26.0, 25.58540320586078, 0.5010839931498138, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4434000.0000, 
sim time next is 4434600.0000, 
raw observation next is [2.0, 80.0, 39.99999999999999, 77.33333333333331, 26.0, 25.59996483231215, 0.4964528426572596, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.518005540166205, 0.8, 0.1333333333333333, 0.08545119705340698, 0.6666666666666666, 0.6333304026926792, 0.6654842808857532, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.45280528], dtype=float32), -0.15715086]. 
=============================================
[2019-04-03 22:52:03,165] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.2252856e-14 1.2259013e-05 4.2213013e-13 4.2766918e-10 1.1849072e-05
 6.0053971e-15 9.9997592e-01], sum to 1.0000
[2019-04-03 22:52:03,165] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2056
[2019-04-03 22:52:03,177] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.0, 42.0, 111.0, 728.5, 26.0, 27.23057621415582, 0.7617108895034185, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4356000.0000, 
sim time next is 4356600.0000, 
raw observation next is [10.43333333333333, 40.66666666666666, 112.3333333333333, 745.6666666666667, 26.0, 27.32991631948994, 0.787151798923655, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.7516158818097877, 0.40666666666666657, 0.37444444444444436, 0.8239410681399633, 0.6666666666666666, 0.7774930266241616, 0.7623839329745516, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9149286], dtype=float32), -0.73762846]. 
=============================================
[2019-04-03 22:52:09,135] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.6412189e-13 1.8992893e-06 3.1721271e-11 3.5433261e-09 1.4759715e-05
 5.5016663e-13 9.9998331e-01], sum to 1.0000
[2019-04-03 22:52:09,136] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2793
[2019-04-03 22:52:09,151] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.4333333333333333, 72.66666666666667, 0.0, 0.0, 26.0, 25.37946088852097, 0.4775966046069735, 0.0, 1.0, 42899.37215551726], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4491600.0000, 
sim time next is 4492200.0000, 
raw observation next is [-0.4666666666666667, 72.83333333333333, 0.0, 0.0, 26.0, 25.37204665996567, 0.4821396405822978, 0.0, 1.0, 47121.33052266898], 
processed observation next is [1.0, 1.0, 0.44967682363804257, 0.7283333333333333, 0.0, 0.0, 0.6666666666666666, 0.6143372216638058, 0.6607132135274326, 0.0, 1.0, 0.2243872882031856], 
reward next is 0.7756, 
noisyNet noise sample is [array([0.53582424], dtype=float32), -0.021443464]. 
=============================================
[2019-04-03 22:52:11,231] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.37377775e-14 5.78325089e-05 5.24038235e-12 1.64812004e-08
 8.15286039e-05 5.89038570e-14 9.99860644e-01], sum to 1.0000
[2019-04-03 22:52:11,236] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3028
[2019-04-03 22:52:11,255] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.166666666666667, 51.5, 124.3333333333333, 811.0, 26.0, 26.71590474765394, 0.6906356662473371, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4619400.0000, 
sim time next is 4620000.0000, 
raw observation next is [2.333333333333333, 51.0, 123.1666666666667, 822.0, 26.0, 26.28285366072824, 0.6831475795487941, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.5272391505078486, 0.51, 0.4105555555555557, 0.9082872928176795, 0.6666666666666666, 0.6902378050606867, 0.7277158598495981, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.9738694], dtype=float32), -0.8421686]. 
=============================================
[2019-04-03 22:52:11,260] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[93.54906 ]
 [93.57644 ]
 [93.7373  ]
 [93.734726]
 [93.85775 ]], R is [[93.54190826]
 [93.60649109]
 [93.67042542]
 [93.73371887]
 [93.79637909]].
[2019-04-03 22:52:13,382] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.8461799e-12 2.2408128e-04 7.5818032e-11 4.3963132e-07 1.0013391e-03
 5.9987492e-12 9.9877411e-01], sum to 1.0000
[2019-04-03 22:52:13,385] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5394
[2019-04-03 22:52:13,403] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.333333333333333, 47.0, 145.6666666666667, 21.33333333333333, 26.0, 26.20713959174198, 0.5690585180309197, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4549200.0000, 
sim time next is 4549800.0000, 
raw observation next is [2.166666666666667, 47.5, 138.3333333333333, 30.66666666666666, 26.0, 26.27469009615686, 0.5783388299083857, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5226223453370269, 0.475, 0.46111111111111097, 0.03388581952117863, 0.6666666666666666, 0.6895575080130717, 0.6927796099694619, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7716029], dtype=float32), -0.5380955]. 
=============================================
[2019-04-03 22:52:17,051] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.6911295e-15 5.9340650e-06 1.3947200e-13 7.3737394e-10 4.0156915e-06
 3.2727553e-15 9.9998999e-01], sum to 1.0000
[2019-04-03 22:52:17,051] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7301
[2019-04-03 22:52:17,084] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.466666666666667, 49.0, 149.6666666666667, 777.3333333333333, 26.0, 27.0201974509358, 0.7999662741913148, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4628400.0000, 
sim time next is 4629000.0000, 
raw observation next is [4.583333333333334, 49.0, 160.3333333333333, 741.6666666666667, 26.0, 27.07906669924055, 0.8191301039400335, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5895660203139428, 0.49, 0.5344444444444443, 0.8195211786372009, 0.6666666666666666, 0.756588891603379, 0.7730433679800112, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6720999], dtype=float32), -0.9744067]. 
=============================================
[2019-04-03 22:52:17,097] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[96.74919 ]
 [96.56869 ]
 [96.41172 ]
 [96.24168 ]
 [96.062706]], R is [[96.97708893]
 [97.00731659]
 [97.0372467 ]
 [97.06687164]
 [97.09620667]].
[2019-04-03 22:52:18,507] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.0267725e-13 1.7978231e-05 8.5920498e-12 1.6030944e-08 2.8953407e-05
 4.3773858e-13 9.9995303e-01], sum to 1.0000
[2019-04-03 22:52:18,508] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1425
[2019-04-03 22:52:18,565] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.8333333333333334, 78.0, 0.0, 0.0, 26.0, 25.0553656622964, 0.4208612275144064, 1.0, 1.0, 111259.8558813405], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4733400.0000, 
sim time next is 4734000.0000, 
raw observation next is [-1.0, 78.0, 0.0, 0.0, 26.0, 25.01876281217148, 0.4454711003251444, 1.0, 1.0, 96408.4466363806], 
processed observation next is [1.0, 0.8260869565217391, 0.4349030470914128, 0.78, 0.0, 0.0, 0.6666666666666666, 0.58489690101429, 0.6484903667750481, 1.0, 1.0, 0.4590878411256219], 
reward next is 0.5409, 
noisyNet noise sample is [array([1.4942584], dtype=float32), -0.40868708]. 
=============================================
[2019-04-03 22:52:18,575] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[84.42361 ]
 [83.933044]
 [83.65397 ]
 [83.51017 ]
 [83.4398  ]], R is [[84.60597992]
 [84.23011017]
 [84.11916351]
 [84.1554184 ]
 [84.19476318]].
[2019-04-03 22:52:19,696] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [4.6833316e-13 3.5651654e-05 2.2438643e-11 7.8422984e-09 5.8163550e-06
 1.7547594e-13 9.9995852e-01], sum to 1.0000
[2019-04-03 22:52:19,696] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.8319
[2019-04-03 22:52:19,738] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 78.0, 0.0, 0.0, 26.0, 25.14381775553802, 0.423794469891199, 0.0, 1.0, 26275.96041545622], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4737000.0000, 
sim time next is 4737600.0000, 
raw observation next is [-1.0, 78.0, 0.0, 0.0, 26.0, 25.04120922817234, 0.4173647038718691, 0.0, 1.0, 82620.65753302096], 
processed observation next is [1.0, 0.8695652173913043, 0.4349030470914128, 0.78, 0.0, 0.0, 0.6666666666666666, 0.5867674356810285, 0.6391215679572897, 0.0, 1.0, 0.39343170253819504], 
reward next is 0.6066, 
noisyNet noise sample is [array([-1.0082724], dtype=float32), 0.9154572]. 
=============================================
[2019-04-03 22:52:20,714] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.3461788e-11 3.4344280e-03 9.0963315e-10 9.2130017e-07 5.0880807e-04
 2.6982977e-10 9.9605584e-01], sum to 1.0000
[2019-04-03 22:52:20,717] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4578
[2019-04-03 22:52:20,733] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.40909910181964, 0.2211579152718629, 0.0, 1.0, 41156.67768069321], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4768800.0000, 
sim time next is 4769400.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.39980223686034, 0.2107546728603556, 0.0, 1.0, 41192.67398264114], 
processed observation next is [0.0, 0.17391304347826086, 0.296398891966759, 0.92, 0.0, 0.0, 0.6666666666666666, 0.5333168530716949, 0.5702515576201185, 0.0, 1.0, 0.19615559039352926], 
reward next is 0.8038, 
noisyNet noise sample is [array([1.3219186], dtype=float32), 1.3218298]. 
=============================================
[2019-04-03 22:52:34,187] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3202216e-11 1.2934746e-03 3.3292208e-10 9.3654002e-08 3.3028441e-04
 2.8333183e-11 9.9837613e-01], sum to 1.0000
[2019-04-03 22:52:34,189] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1699
[2019-04-03 22:52:34,204] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.1666666666666667, 39.16666666666666, 0.0, 0.0, 26.0, 25.38282357822653, 0.3488005244769969, 0.0, 1.0, 52741.17443210196], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4921800.0000, 
sim time next is 4922400.0000, 
raw observation next is [0.3333333333333333, 39.33333333333334, 0.0, 0.0, 26.0, 25.36951638267657, 0.3502647980471768, 0.0, 1.0, 50605.41056547196], 
processed observation next is [0.0, 1.0, 0.4718374884579871, 0.3933333333333334, 0.0, 0.0, 0.6666666666666666, 0.6141263652230474, 0.6167549326823923, 0.0, 1.0, 0.2409781455498665], 
reward next is 0.7590, 
noisyNet noise sample is [array([1.0179851], dtype=float32), 1.0873176]. 
=============================================
[2019-04-03 22:52:35,260] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.5808397e-13 1.7825008e-05 2.1607417e-11 1.0545362e-08 2.0639850e-04
 7.2054255e-13 9.9977571e-01], sum to 1.0000
[2019-04-03 22:52:35,265] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8243
[2019-04-03 22:52:35,276] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.833333333333333, 35.0, 0.0, 0.0, 26.0, 25.52723751496265, 0.4963543429951779, 0.0, 1.0, 18748.04461031932], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5008200.0000, 
sim time next is 5008800.0000, 
raw observation next is [2.666666666666667, 36.0, 0.0, 0.0, 26.0, 25.56165758183747, 0.4916326030626935, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.5364727608494922, 0.36, 0.0, 0.0, 0.6666666666666666, 0.6301381318197891, 0.6638775343542312, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.92939734], dtype=float32), 0.9449511]. 
=============================================
[2019-04-03 22:52:35,942] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.1260943e-11 1.2436452e-04 2.1640989e-09 2.9753411e-07 1.1641198e-03
 1.4021133e-10 9.9871123e-01], sum to 1.0000
[2019-04-03 22:52:35,943] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7806
[2019-04-03 22:52:35,968] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.833333333333333, 44.50000000000001, 0.0, 0.0, 26.0, 24.99552284279526, 0.2973945721337654, 0.0, 1.0, 36928.86160605744], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4903800.0000, 
sim time next is 4904400.0000, 
raw observation next is [1.666666666666667, 45.0, 0.0, 0.0, 26.0, 24.97835479977574, 0.2958173919015374, 0.0, 1.0, 42236.02370647708], 
processed observation next is [0.0, 0.782608695652174, 0.5087719298245615, 0.45, 0.0, 0.0, 0.6666666666666666, 0.5815295666479784, 0.5986057973005124, 0.0, 1.0, 0.2011239224117956], 
reward next is 0.7989, 
noisyNet noise sample is [array([-0.1852745], dtype=float32), 0.8206992]. 
=============================================
[2019-04-03 22:52:36,297] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:52:36,297] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:52:36,314] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run6
[2019-04-03 22:52:37,105] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:52:37,105] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:52:37,111] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run6
[2019-04-03 22:52:37,486] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1126581e-14 2.2863821e-06 1.3558105e-13 1.3038057e-09 7.6395423e-07
 2.4043742e-15 9.9999702e-01], sum to 1.0000
[2019-04-03 22:52:37,486] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5287
[2019-04-03 22:52:37,491] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.666666666666668, 25.33333333333333, 88.66666666666667, 751.8333333333333, 26.0, 27.23994589607431, 0.8550326415736453, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4981200.0000, 
sim time next is 4981800.0000, 
raw observation next is [8.833333333333332, 25.16666666666667, 85.33333333333334, 729.6666666666667, 26.0, 27.49876124526878, 0.8821863641473738, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.7072945521698984, 0.2516666666666667, 0.2844444444444445, 0.8062615101289136, 0.6666666666666666, 0.7915634371057317, 0.794062121382458, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.37939358], dtype=float32), 0.7765516]. 
=============================================
[2019-04-03 22:52:38,031] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.4620730e-14 1.8405812e-05 1.2033072e-12 4.8458779e-09 2.1314088e-05
 4.9124130e-14 9.9996030e-01], sum to 1.0000
[2019-04-03 22:52:38,031] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5831
[2019-04-03 22:52:38,056] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.0, 24.33333333333334, 0.0, 0.0, 26.0, 25.59233098874263, 0.6029785022973777, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4990800.0000, 
sim time next is 4991400.0000, 
raw observation next is [6.0, 24.0, 0.0, 0.0, 26.0, 25.98055998427534, 0.625325890639475, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.6288088642659281, 0.24, 0.0, 0.0, 0.6666666666666666, 0.6650466653562784, 0.7084419635464917, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.625902], dtype=float32), 0.023011077]. 
=============================================
[2019-04-03 22:52:39,524] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:52:39,524] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:52:39,528] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run6
[2019-04-03 22:52:40,965] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:52:40,965] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:52:40,968] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run6
[2019-04-03 22:52:41,209] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:52:41,209] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:52:41,213] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run6
[2019-04-03 22:52:43,193] A3C_AGENT_WORKER-Thread-19 INFO:Local step 42500, global step 678814: loss 54.4568
[2019-04-03 22:52:43,195] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 42500, global step 678814: learning rate 0.0001
[2019-04-03 22:52:44,045] A3C_AGENT_WORKER-Thread-17 INFO:Local step 42500, global step 679177: loss 50.9841
[2019-04-03 22:52:44,050] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 42500, global step 679179: learning rate 0.0001
[2019-04-03 22:52:44,765] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:52:44,765] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:52:44,770] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run6
[2019-04-03 22:52:44,877] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:52:44,877] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:52:44,882] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run6
[2019-04-03 22:52:44,951] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:52:44,952] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:52:44,955] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run6
[2019-04-03 22:52:45,050] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:52:45,050] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:52:45,056] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run6
[2019-04-03 22:52:45,165] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:52:45,165] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:52:45,169] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run6
[2019-04-03 22:52:45,270] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:52:45,271] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:52:45,292] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run6
[2019-04-03 22:52:46,609] A3C_AGENT_WORKER-Thread-7 INFO:Local step 42500, global step 679608: loss 55.7057
[2019-04-03 22:52:46,610] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 42500, global step 679608: learning rate 0.0001
[2019-04-03 22:52:48,060] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.6664036e-13 1.2515947e-05 2.1823500e-12 2.5480933e-09 1.2335773e-04
 1.4290308e-14 9.9986410e-01], sum to 1.0000
[2019-04-03 22:52:48,060] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1946
[2019-04-03 22:52:48,097] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.0, 19.0, 0.0, 0.0, 26.0, 27.47142926417436, 0.9251235576376967, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5083200.0000, 
sim time next is 5083800.0000, 
raw observation next is [9.833333333333334, 19.0, 0.0, 0.0, 26.0, 27.3933241240911, 0.9109463134541924, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.7349953831948293, 0.19, 0.0, 0.0, 0.6666666666666666, 0.7827770103409252, 0.8036487711513974, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5581691], dtype=float32), -0.848209]. 
=============================================
[2019-04-03 22:52:48,216] A3C_AGENT_WORKER-Thread-8 INFO:Local step 42500, global step 679896: loss 53.5825
[2019-04-03 22:52:48,218] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 42500, global step 679896: learning rate 0.0001
[2019-04-03 22:52:48,249] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:52:48,249] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:52:48,252] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run6
[2019-04-03 22:52:48,316] A3C_AGENT_WORKER-Thread-6 INFO:Local step 42500, global step 679908: loss 50.7109
[2019-04-03 22:52:48,329] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 42500, global step 679908: learning rate 0.0001
[2019-04-03 22:52:48,992] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:52:48,993] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:52:48,999] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run6
[2019-04-03 22:52:49,325] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:52:49,325] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:52:49,329] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run6
[2019-04-03 22:52:49,917] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:52:49,917] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:52:49,920] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run6
[2019-04-03 22:52:50,395] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:52:50,395] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:52:50,429] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run6
[2019-04-03 22:52:52,944] A3C_AGENT_WORKER-Thread-5 INFO:Local step 42500, global step 680373: loss 50.6857
[2019-04-03 22:52:52,945] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 42500, global step 680373: learning rate 0.0001
[2019-04-03 22:52:53,028] A3C_AGENT_WORKER-Thread-3 INFO:Local step 42500, global step 680383: loss 52.9477
[2019-04-03 22:52:53,029] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 42500, global step 680383: learning rate 0.0001
[2019-04-03 22:52:54,050] A3C_AGENT_WORKER-Thread-15 INFO:Local step 42500, global step 680541: loss 51.5805
[2019-04-03 22:52:54,076] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 42500, global step 680541: learning rate 0.0001
[2019-04-03 22:52:54,133] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [5.0608311e-12 4.4785747e-03 1.0027363e-10 2.7498703e-07 5.1063658e-03
 1.6118176e-11 9.9041474e-01], sum to 1.0000
[2019-04-03 22:52:54,133] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.2403
[2019-04-03 22:52:54,179] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.416666666666667, 82.66666666666667, 28.66666666666666, 0.0, 26.0, 24.54550322854908, 0.1782786068492267, 0.0, 1.0, 28689.95716807476], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 58200.0000, 
sim time next is 58800.0000, 
raw observation next is [6.233333333333333, 83.33333333333334, 23.33333333333333, 0.0, 26.0, 24.52117528320296, 0.1768928102089091, 0.0, 1.0, 50784.1768728397], 
processed observation next is [0.0, 0.6956521739130435, 0.6352723915050786, 0.8333333333333335, 0.07777777777777777, 0.0, 0.6666666666666666, 0.5434312736002468, 0.5589642700696363, 0.0, 1.0, 0.24182941368018904], 
reward next is 0.7582, 
noisyNet noise sample is [array([-0.5916164], dtype=float32), 1.0626724]. 
=============================================
[2019-04-03 22:52:54,474] A3C_AGENT_WORKER-Thread-20 INFO:Local step 42500, global step 680623: loss 49.1007
[2019-04-03 22:52:54,478] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 42500, global step 680624: learning rate 0.0001
[2019-04-03 22:52:54,481] A3C_AGENT_WORKER-Thread-4 INFO:Local step 42500, global step 680624: loss 49.4920
[2019-04-03 22:52:54,481] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 42500, global step 680624: learning rate 0.0001
[2019-04-03 22:52:54,827] A3C_AGENT_WORKER-Thread-10 INFO:Local step 42500, global step 680687: loss 61.2746
[2019-04-03 22:52:54,829] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 42500, global step 680687: learning rate 0.0001
[2019-04-03 22:52:54,941] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.3217873e-14 4.0650762e-06 2.6788795e-13 3.4060816e-09 3.3219978e-06
 2.5774511e-14 9.9999261e-01], sum to 1.0000
[2019-04-03 22:52:54,941] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6610
[2019-04-03 22:52:55,000] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.899999999999999, 61.0, 133.0, 563.3333333333334, 26.0, 25.55744865647944, 0.410068416557303, 1.0, 1.0, 44546.9131760454], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 132600.0000, 
sim time next is 133200.0000, 
raw observation next is [-7.8, 61.0, 134.5, 543.5, 26.0, 25.58335124790931, 0.4207356227131496, 1.0, 1.0, 38975.33409178795], 
processed observation next is [1.0, 0.5652173913043478, 0.24653739612188366, 0.61, 0.4483333333333333, 0.6005524861878453, 0.6666666666666666, 0.6319459373257758, 0.6402452075710499, 1.0, 1.0, 0.18559682900851404], 
reward next is 0.8144, 
noisyNet noise sample is [array([-1.1019536], dtype=float32), 1.0431563]. 
=============================================
[2019-04-03 22:52:57,455] A3C_AGENT_WORKER-Thread-2 INFO:Local step 42500, global step 681143: loss 50.2403
[2019-04-03 22:52:57,455] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 42500, global step 681143: learning rate 0.0001
[2019-04-03 22:52:57,693] A3C_AGENT_WORKER-Thread-9 INFO:Local step 42500, global step 681196: loss 51.4405
[2019-04-03 22:52:57,693] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 42500, global step 681196: learning rate 0.0001
[2019-04-03 22:52:58,126] A3C_AGENT_WORKER-Thread-18 INFO:Local step 42500, global step 681291: loss 51.2010
[2019-04-03 22:52:58,141] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 42500, global step 681293: learning rate 0.0001
[2019-04-03 22:52:58,805] A3C_AGENT_WORKER-Thread-16 INFO:Local step 42500, global step 681487: loss 49.1451
[2019-04-03 22:52:58,834] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 42500, global step 681487: learning rate 0.0001
[2019-04-03 22:52:59,163] A3C_AGENT_WORKER-Thread-14 INFO:Local step 42500, global step 681598: loss 49.7467
[2019-04-03 22:52:59,164] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 42500, global step 681598: learning rate 0.0001
[2019-04-03 22:53:05,929] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.7944356e-11 7.3987700e-05 2.1422793e-10 1.5280155e-07 1.3161750e-04
 2.0696207e-12 9.9979430e-01], sum to 1.0000
[2019-04-03 22:53:05,929] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7602
[2019-04-03 22:53:06,000] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.7, 61.0, 104.5, 75.5, 26.0, 25.60732300925154, 0.3894063847939735, 1.0, 1.0, 154356.3589756935], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 140400.0000, 
sim time next is 141000.0000, 
raw observation next is [-6.700000000000001, 61.5, 90.0, 65.33333333333333, 26.0, 25.574636971764, 0.3110214327191965, 1.0, 1.0, 100843.9331224681], 
processed observation next is [1.0, 0.6521739130434783, 0.2770083102493075, 0.615, 0.3, 0.0721915285451197, 0.6666666666666666, 0.6312197476470001, 0.6036738109063988, 1.0, 1.0, 0.48020920534508615], 
reward next is 0.5198, 
noisyNet noise sample is [array([0.39186543], dtype=float32), -0.39914602]. 
=============================================
[2019-04-03 22:53:06,004] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[70.80295 ]
 [70.80229 ]
 [71.77171 ]
 [72.74526 ]
 [73.722595]], R is [[70.34137726]
 [69.90293121]
 [70.2039032 ]
 [70.50186157]
 [70.79684448]].
[2019-04-03 22:53:13,125] A3C_AGENT_WORKER-Thread-19 INFO:Local step 43000, global step 685342: loss 0.8033
[2019-04-03 22:53:13,126] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 43000, global step 685342: learning rate 0.0001
[2019-04-03 22:53:13,427] A3C_AGENT_WORKER-Thread-17 INFO:Local step 43000, global step 685408: loss 0.7691
[2019-04-03 22:53:13,428] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 43000, global step 685408: learning rate 0.0001
[2019-04-03 22:53:17,011] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5577579e-11 2.0588105e-04 2.2968420e-09 2.5365277e-06 5.8798911e-04
 2.2947302e-10 9.9920362e-01], sum to 1.0000
[2019-04-03 22:53:17,011] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0507
[2019-04-03 22:53:17,032] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-14.5, 69.0, 0.0, 0.0, 26.0, 23.26433971492308, -0.1081242555080051, 0.0, 1.0, 47675.37601166679], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 349200.0000, 
sim time next is 349800.0000, 
raw observation next is [-14.58333333333333, 69.0, 0.0, 0.0, 26.0, 23.26888975364484, -0.1139752443985334, 0.0, 1.0, 47726.60849477619], 
processed observation next is [1.0, 0.043478260869565216, 0.05863342566943682, 0.69, 0.0, 0.0, 0.6666666666666666, 0.4390741461370699, 0.46200825186715555, 0.0, 1.0, 0.227269564260839], 
reward next is 0.7727, 
noisyNet noise sample is [array([0.07162422], dtype=float32), -1.6860973]. 
=============================================
[2019-04-03 22:53:17,619] A3C_AGENT_WORKER-Thread-7 INFO:Local step 43000, global step 686585: loss 0.6473
[2019-04-03 22:53:17,620] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 43000, global step 686585: learning rate 0.0001
[2019-04-03 22:53:19,345] A3C_AGENT_WORKER-Thread-8 INFO:Local step 43000, global step 687025: loss 0.5365
[2019-04-03 22:53:19,345] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 43000, global step 687025: learning rate 0.0001
[2019-04-03 22:53:19,951] A3C_AGENT_WORKER-Thread-6 INFO:Local step 43000, global step 687196: loss 0.4487
[2019-04-03 22:53:19,953] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 43000, global step 687196: learning rate 0.0001
[2019-04-03 22:53:22,657] A3C_AGENT_WORKER-Thread-5 INFO:Local step 43000, global step 687935: loss 0.3401
[2019-04-03 22:53:22,659] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 43000, global step 687935: learning rate 0.0001
[2019-04-03 22:53:23,480] A3C_AGENT_WORKER-Thread-3 INFO:Local step 43000, global step 688198: loss 0.3354
[2019-04-03 22:53:23,481] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 43000, global step 688198: learning rate 0.0001
[2019-04-03 22:53:24,365] A3C_AGENT_WORKER-Thread-4 INFO:Local step 43000, global step 688438: loss 0.2853
[2019-04-03 22:53:24,365] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 43000, global step 688438: learning rate 0.0001
[2019-04-03 22:53:24,642] A3C_AGENT_WORKER-Thread-15 INFO:Local step 43000, global step 688510: loss 0.2876
[2019-04-03 22:53:24,643] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 43000, global step 688510: learning rate 0.0001
[2019-04-03 22:53:24,660] A3C_AGENT_WORKER-Thread-20 INFO:Local step 43000, global step 688515: loss 0.2673
[2019-04-03 22:53:24,661] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 43000, global step 688515: learning rate 0.0001
[2019-04-03 22:53:24,772] A3C_AGENT_WORKER-Thread-10 INFO:Local step 43000, global step 688546: loss 0.2489
[2019-04-03 22:53:24,773] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 43000, global step 688546: learning rate 0.0001
[2019-04-03 22:53:26,424] A3C_AGENT_WORKER-Thread-2 INFO:Local step 43000, global step 688952: loss 0.2446
[2019-04-03 22:53:26,426] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 43000, global step 688952: learning rate 0.0001
[2019-04-03 22:53:27,072] A3C_AGENT_WORKER-Thread-9 INFO:Local step 43000, global step 689128: loss 0.2254
[2019-04-03 22:53:27,072] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 43000, global step 689128: learning rate 0.0001
[2019-04-03 22:53:27,445] A3C_AGENT_WORKER-Thread-18 INFO:Local step 43000, global step 689234: loss 0.1886
[2019-04-03 22:53:27,445] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 43000, global step 689234: learning rate 0.0001
[2019-04-03 22:53:28,478] A3C_AGENT_WORKER-Thread-16 INFO:Local step 43000, global step 689516: loss 0.2401
[2019-04-03 22:53:28,478] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 43000, global step 689516: learning rate 0.0001
[2019-04-03 22:53:28,743] A3C_AGENT_WORKER-Thread-14 INFO:Local step 43000, global step 689580: loss 0.2382
[2019-04-03 22:53:28,744] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 43000, global step 689580: learning rate 0.0001
[2019-04-03 22:53:29,093] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.1784555e-13 5.1520251e-06 5.8615932e-11 2.7910144e-08 1.7602324e-04
 5.8412406e-13 9.9981886e-01], sum to 1.0000
[2019-04-03 22:53:29,093] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5314
[2019-04-03 22:53:29,141] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.5, 42.0, 72.0, 501.3333333333333, 26.0, 26.0778011962708, 0.5181600685828213, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 314400.0000, 
sim time next is 315000.0000, 
raw observation next is [-9.5, 42.0, 70.0, 477.0, 26.0, 26.26709894267765, 0.5286570470153121, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.1994459833795014, 0.42, 0.23333333333333334, 0.5270718232044199, 0.6666666666666666, 0.6889249118898041, 0.6762190156717707, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4700702], dtype=float32), -0.3293484]. 
=============================================
[2019-04-03 22:53:29,172] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[71.876305]
 [72.07191 ]
 [72.47111 ]
 [72.3439  ]
 [71.85623 ]], R is [[71.89955902]
 [72.18056488]
 [72.45876312]
 [72.19890594]
 [71.5319519 ]].
[2019-04-03 22:53:40,441] A3C_AGENT_WORKER-Thread-19 INFO:Local step 43500, global step 692940: loss 0.2709
[2019-04-03 22:53:40,442] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 43500, global step 692941: learning rate 0.0001
[2019-04-03 22:53:41,300] A3C_AGENT_WORKER-Thread-17 INFO:Local step 43500, global step 693213: loss 0.2283
[2019-04-03 22:53:41,303] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 43500, global step 693213: learning rate 0.0001
[2019-04-03 22:53:45,144] A3C_AGENT_WORKER-Thread-7 INFO:Local step 43500, global step 694325: loss 0.3080
[2019-04-03 22:53:45,147] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 43500, global step 694325: learning rate 0.0001
[2019-04-03 22:53:47,101] A3C_AGENT_WORKER-Thread-8 INFO:Local step 43500, global step 694971: loss 0.2299
[2019-04-03 22:53:47,104] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 43500, global step 694971: learning rate 0.0001
[2019-04-03 22:53:47,449] A3C_AGENT_WORKER-Thread-6 INFO:Local step 43500, global step 695096: loss 0.1264
[2019-04-03 22:53:47,452] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 43500, global step 695096: learning rate 0.0001
[2019-04-03 22:53:49,913] A3C_AGENT_WORKER-Thread-5 INFO:Local step 43500, global step 695888: loss 0.0988
[2019-04-03 22:53:49,914] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 43500, global step 695888: learning rate 0.0001
[2019-04-03 22:53:50,538] A3C_AGENT_WORKER-Thread-3 INFO:Local step 43500, global step 696067: loss 0.0835
[2019-04-03 22:53:50,542] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 43500, global step 696067: learning rate 0.0001
[2019-04-03 22:53:51,458] A3C_AGENT_WORKER-Thread-4 INFO:Local step 43500, global step 696426: loss 0.0419
[2019-04-03 22:53:51,459] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 43500, global step 696426: learning rate 0.0001
[2019-04-03 22:53:51,554] A3C_AGENT_WORKER-Thread-15 INFO:Local step 43500, global step 696471: loss 0.0329
[2019-04-03 22:53:51,555] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 43500, global step 696471: learning rate 0.0001
[2019-04-03 22:53:51,819] A3C_AGENT_WORKER-Thread-10 INFO:Local step 43500, global step 696569: loss 0.0263
[2019-04-03 22:53:51,821] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 43500, global step 696569: learning rate 0.0001
[2019-04-03 22:53:51,859] A3C_AGENT_WORKER-Thread-20 INFO:Local step 43500, global step 696583: loss 0.0180
[2019-04-03 22:53:51,870] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 43500, global step 696583: learning rate 0.0001
[2019-04-03 22:53:52,866] A3C_AGENT_WORKER-Thread-2 INFO:Local step 43500, global step 696959: loss 0.0207
[2019-04-03 22:53:52,868] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 43500, global step 696960: learning rate 0.0001
[2019-04-03 22:53:53,506] A3C_AGENT_WORKER-Thread-18 INFO:Local step 43500, global step 697169: loss 0.0095
[2019-04-03 22:53:53,507] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 43500, global step 697169: learning rate 0.0001
[2019-04-03 22:53:53,909] A3C_AGENT_WORKER-Thread-9 INFO:Local step 43500, global step 697320: loss 0.0014
[2019-04-03 22:53:53,911] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 43500, global step 697320: learning rate 0.0001
[2019-04-03 22:53:54,815] A3C_AGENT_WORKER-Thread-16 INFO:Local step 43500, global step 697704: loss 0.0036
[2019-04-03 22:53:54,817] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 43500, global step 697706: learning rate 0.0001
[2019-04-03 22:53:55,380] A3C_AGENT_WORKER-Thread-14 INFO:Local step 43500, global step 697879: loss 0.0040
[2019-04-03 22:53:55,381] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 43500, global step 697879: learning rate 0.0001
[2019-04-03 22:54:02,779] A3C_AGENT_WORKER-Thread-4 INFO:Evaluating...
[2019-04-03 22:54:02,779] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-03 22:54:02,779] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:54:02,781] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run8
[2019-04-03 22:54:02,798] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-03 22:54:02,798] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:54:02,800] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run8
[2019-04-03 22:54:02,841] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-03 22:54:02,842] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:54:02,853] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run8
[2019-04-03 22:56:10,870] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.2617044], dtype=float32), 0.35065305]
[2019-04-03 22:56:10,870] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-1.7, 46.5, 0.0, 0.0, 26.0, 25.01493642065942, 0.1894419233239879, 0.0, 1.0, 38456.60243722765]
[2019-04-03 22:56:10,871] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-03 22:56:10,871] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [8.6541269e-12 7.7993271e-04 4.5850218e-10 9.4292773e-08 4.8233973e-04
 4.5709266e-11 9.9873763e-01], sampled 0.9549292059160258
[2019-04-03 22:56:58,909] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.2617044], dtype=float32), 0.35065305]
[2019-04-03 22:56:58,909] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [11.1, 61.0, 177.0, 631.0, 26.0, 27.44060507895362, 0.8961933719843841, 1.0, 1.0, 0.0]
[2019-04-03 22:56:58,909] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-03 22:56:58,912] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [2.4215740e-15 5.6848594e-06 8.0267417e-14 5.3654442e-10 6.3794005e-06
 2.8935166e-15 9.9998784e-01], sampled 0.04368415661027514
[2019-04-03 22:57:11,988] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.9373 239833163.3917 1605.2127
[2019-04-03 22:57:40,605] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.4791 263449394.7845 1559.4396
[2019-04-03 22:57:45,991] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.4443 275846686.6181 1233.4143
[2019-04-03 22:57:47,037] A3C_AGENT_WORKER-Thread-4 INFO:Global step: 700000, evaluation results [700000.0, 7241.4790724546365, 263449394.7845263, 1559.4396267249065, 7353.937317182356, 239833163.3917029, 1605.2126855584513, 7182.44434943782, 275846686.6180592, 1233.4143390405059]
[2019-04-03 22:57:51,085] A3C_AGENT_WORKER-Thread-19 INFO:Local step 44000, global step 701019: loss 0.0355
[2019-04-03 22:57:51,087] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 44000, global step 701019: learning rate 0.0001
[2019-04-03 22:57:52,105] A3C_AGENT_WORKER-Thread-17 INFO:Local step 44000, global step 701312: loss 0.0190
[2019-04-03 22:57:52,105] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 44000, global step 701312: learning rate 0.0001
[2019-04-03 22:57:52,183] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.69338716e-13 1.17144606e-04 1.47986803e-10 2.02364472e-08
 5.86616923e-04 3.74646104e-12 9.99296188e-01], sum to 1.0000
[2019-04-03 22:57:52,183] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8937
[2019-04-03 22:57:52,200] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.300000000000001, 71.0, 0.0, 0.0, 26.0, 23.68951954688084, -0.03347210469886253, 0.0, 1.0, 41919.77921905041], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 796800.0000, 
sim time next is 797400.0000, 
raw observation next is [-7.3, 71.0, 0.0, 0.0, 26.0, 23.63440609081136, -0.03996473787269585, 0.0, 1.0, 41992.87312203029], 
processed observation next is [1.0, 0.21739130434782608, 0.26038781163434904, 0.71, 0.0, 0.0, 0.6666666666666666, 0.4695338409009467, 0.48667842070910133, 0.0, 1.0, 0.19996606248585852], 
reward next is 0.8000, 
noisyNet noise sample is [array([-0.76114553], dtype=float32), 0.84023076]. 
=============================================
[2019-04-03 22:57:55,578] A3C_AGENT_WORKER-Thread-7 INFO:Local step 44000, global step 702134: loss 0.0278
[2019-04-03 22:57:55,579] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 44000, global step 702134: learning rate 0.0001
[2019-04-03 22:57:58,292] A3C_AGENT_WORKER-Thread-6 INFO:Local step 44000, global step 702821: loss 0.0273
[2019-04-03 22:57:58,292] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 44000, global step 702821: learning rate 0.0001
[2019-04-03 22:57:58,562] A3C_AGENT_WORKER-Thread-8 INFO:Local step 44000, global step 702876: loss 0.0317
[2019-04-03 22:57:58,562] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 44000, global step 702876: learning rate 0.0001
[2019-04-03 22:57:58,858] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [4.2267800e-15 7.2316252e-06 1.0993157e-12 1.0206675e-10 1.5751442e-05
 4.1917115e-15 9.9997699e-01], sum to 1.0000
[2019-04-03 22:57:58,858] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4214
[2019-04-03 22:57:58,920] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.0, 100.0, 0.0, 0.0, 26.0, 24.67225078103023, 0.3103696567001642, 0.0, 1.0, 168195.256806504], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 937200.0000, 
sim time next is 937800.0000, 
raw observation next is [5.0, 100.0, 0.0, 0.0, 26.0, 24.70086450921662, 0.336654431689496, 0.0, 1.0, 103112.1873685872], 
processed observation next is [1.0, 0.8695652173913043, 0.6011080332409973, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5584053757680515, 0.6122181438964986, 0.0, 1.0, 0.4910104160408914], 
reward next is 0.5090, 
noisyNet noise sample is [array([0.29576674], dtype=float32), 0.45229465]. 
=============================================
[2019-04-03 22:58:01,547] A3C_AGENT_WORKER-Thread-5 INFO:Local step 44000, global step 703729: loss 0.0781
[2019-04-03 22:58:01,548] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 44000, global step 703729: learning rate 0.0001
[2019-04-03 22:58:02,023] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.2653378e-15 2.9697051e-06 8.5714842e-14 1.8954774e-10 1.7867279e-06
 9.4031933e-16 9.9999523e-01], sum to 1.0000
[2019-04-03 22:58:02,023] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3530
[2019-04-03 22:58:02,071] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.1, 93.0, 90.0, 0.0, 26.0, 25.97486183322127, 0.3419626664910673, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 916200.0000, 
sim time next is 916800.0000, 
raw observation next is [4.2, 93.0, 84.0, 0.0, 26.0, 25.57080336630285, 0.3629803849931823, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5789473684210527, 0.93, 0.28, 0.0, 0.6666666666666666, 0.6309002805252376, 0.6209934616643941, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.106412], dtype=float32), -0.9340864]. 
=============================================
[2019-04-03 22:58:02,555] A3C_AGENT_WORKER-Thread-3 INFO:Local step 44000, global step 704033: loss 0.0379
[2019-04-03 22:58:02,556] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 44000, global step 704033: learning rate 0.0001
[2019-04-03 22:58:04,086] A3C_AGENT_WORKER-Thread-20 INFO:Local step 44000, global step 704509: loss 0.0460
[2019-04-03 22:58:04,090] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 44000, global step 704511: learning rate 0.0001
[2019-04-03 22:58:04,167] A3C_AGENT_WORKER-Thread-15 INFO:Local step 44000, global step 704538: loss 0.0302
[2019-04-03 22:58:04,168] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 44000, global step 704538: learning rate 0.0001
[2019-04-03 22:58:04,625] A3C_AGENT_WORKER-Thread-4 INFO:Local step 44000, global step 704684: loss 0.0247
[2019-04-03 22:58:04,626] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 44000, global step 704684: learning rate 0.0001
[2019-04-03 22:58:05,375] A3C_AGENT_WORKER-Thread-10 INFO:Local step 44000, global step 704906: loss 0.0255
[2019-04-03 22:58:05,378] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 44000, global step 704908: learning rate 0.0001
[2019-04-03 22:58:06,887] A3C_AGENT_WORKER-Thread-18 INFO:Local step 44000, global step 705414: loss 0.0268
[2019-04-03 22:58:06,887] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 44000, global step 705414: learning rate 0.0001
[2019-04-03 22:58:06,895] A3C_AGENT_WORKER-Thread-2 INFO:Local step 44000, global step 705418: loss 0.0288
[2019-04-03 22:58:06,896] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 44000, global step 705419: learning rate 0.0001
[2019-04-03 22:58:07,781] A3C_AGENT_WORKER-Thread-9 INFO:Local step 44000, global step 705762: loss 0.0526
[2019-04-03 22:58:07,782] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 44000, global step 705762: learning rate 0.0001
[2019-04-03 22:58:08,223] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.9391538e-16 4.2769957e-06 7.8249500e-14 2.7034572e-10 2.3197274e-06
 8.4574885e-16 9.9999344e-01], sum to 1.0000
[2019-04-03 22:58:08,223] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.7417
[2019-04-03 22:58:08,237] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 26.0, 25.97011147122982, 0.6272500351350248, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1030200.0000, 
sim time next is 1030800.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 26.0, 25.9592221264225, 0.6242927576069809, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.8614958448753465, 0.75, 0.0, 0.0, 0.6666666666666666, 0.6632685105352083, 0.7080975858689936, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.134278], dtype=float32), 0.8513767]. 
=============================================
[2019-04-03 22:58:09,161] A3C_AGENT_WORKER-Thread-16 INFO:Local step 44000, global step 706295: loss 0.0305
[2019-04-03 22:58:09,212] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 44000, global step 706315: learning rate 0.0001
[2019-04-03 22:58:09,524] A3C_AGENT_WORKER-Thread-14 INFO:Local step 44000, global step 706450: loss 0.0332
[2019-04-03 22:58:09,526] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 44000, global step 706453: learning rate 0.0001
[2019-04-03 22:58:12,063] A3C_AGENT_WORKER-Thread-19 INFO:Local step 44500, global step 707376: loss 0.6043
[2019-04-03 22:58:12,064] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 44500, global step 707376: learning rate 0.0001
[2019-04-03 22:58:12,660] A3C_AGENT_WORKER-Thread-17 INFO:Local step 44500, global step 707647: loss 0.5206
[2019-04-03 22:58:12,672] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 44500, global step 707647: learning rate 0.0001
[2019-04-03 22:58:14,789] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.5922705e-15 9.9727484e-05 4.9345234e-13 1.2557675e-09 3.8721777e-05
 1.9421039e-14 9.9986148e-01], sum to 1.0000
[2019-04-03 22:58:14,793] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0623
[2019-04-03 22:58:14,802] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.2, 77.33333333333334, 0.0, 0.0, 26.0, 25.90982350274386, 0.6021877691111638, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1052400.0000, 
sim time next is 1053000.0000, 
raw observation next is [14.1, 77.5, 0.0, 0.0, 26.0, 25.8295040775266, 0.5999565573780469, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.8531855955678671, 0.775, 0.0, 0.0, 0.6666666666666666, 0.6524586731272167, 0.6999855191260157, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3000481], dtype=float32), -0.0069594975]. 
=============================================
[2019-04-03 22:58:14,855] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[97.964134]
 [98.22253 ]
 [98.44459 ]
 [98.57014 ]
 [98.637566]], R is [[97.8015213 ]
 [97.82350922]
 [97.84527588]
 [97.86682129]
 [97.88815308]].
[2019-04-03 22:58:15,454] A3C_AGENT_WORKER-Thread-7 INFO:Local step 44500, global step 708889: loss 0.4573
[2019-04-03 22:58:15,456] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 44500, global step 708891: learning rate 0.0001
[2019-04-03 22:58:16,334] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.1671950e-13 1.0472260e-03 8.1588715e-11 6.0687646e-09 9.8461460e-05
 3.4722143e-12 9.9885428e-01], sum to 1.0000
[2019-04-03 22:58:16,335] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7769
[2019-04-03 22:58:16,357] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.0, 96.0, 0.0, 0.0, 26.0, 23.60900915761398, 0.1766954195630129, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1229400.0000, 
sim time next is 1230000.0000, 
raw observation next is [15.0, 96.0, 0.0, 0.0, 26.0, 23.62409205044009, 0.1735857660772742, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.21739130434782608, 0.8781163434903049, 0.96, 0.0, 0.0, 0.6666666666666666, 0.4686743375366742, 0.557861922025758, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.85132384], dtype=float32), 2.7692208]. 
=============================================
[2019-04-03 22:58:16,395] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[86.08481]
 [86.12075]
 [86.14979]
 [86.18387]
 [86.21802]], R is [[86.18847656]
 [86.32659149]
 [86.4633255 ]
 [86.59869385]
 [86.73270416]].
[2019-04-03 22:58:18,084] A3C_AGENT_WORKER-Thread-6 INFO:Local step 44500, global step 710009: loss 0.4594
[2019-04-03 22:58:18,086] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 44500, global step 710009: learning rate 0.0001
[2019-04-03 22:58:18,511] A3C_AGENT_WORKER-Thread-8 INFO:Local step 44500, global step 710220: loss 0.4249
[2019-04-03 22:58:18,512] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 44500, global step 710220: learning rate 0.0001
[2019-04-03 22:58:21,101] A3C_AGENT_WORKER-Thread-5 INFO:Local step 44500, global step 711289: loss 0.4904
[2019-04-03 22:58:21,103] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 44500, global step 711289: learning rate 0.0001
[2019-04-03 22:58:21,928] A3C_AGENT_WORKER-Thread-3 INFO:Local step 44500, global step 711848: loss 0.6303
[2019-04-03 22:58:21,929] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 44500, global step 711848: learning rate 0.0001
[2019-04-03 22:58:22,860] A3C_AGENT_WORKER-Thread-15 INFO:Local step 44500, global step 712430: loss 0.3768
[2019-04-03 22:58:22,861] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 44500, global step 712430: learning rate 0.0001
[2019-04-03 22:58:23,014] A3C_AGENT_WORKER-Thread-4 INFO:Local step 44500, global step 712531: loss 0.4040
[2019-04-03 22:58:23,016] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 44500, global step 712532: learning rate 0.0001
[2019-04-03 22:58:23,221] A3C_AGENT_WORKER-Thread-20 INFO:Local step 44500, global step 712652: loss 0.3798
[2019-04-03 22:58:23,222] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 44500, global step 712652: learning rate 0.0001
[2019-04-03 22:58:23,244] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.0479402e-15 4.9531468e-07 1.0727723e-13 1.4045380e-10 4.7746624e-08
 1.0797534e-14 9.9999952e-01], sum to 1.0000
[2019-04-03 22:58:23,252] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0952
[2019-04-03 22:58:23,263] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.15, 81.5, 0.0, 0.0, 26.0, 25.62829395755236, 0.6127404718260125, 0.0, 1.0, 24999.8520700094], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1146600.0000, 
sim time next is 1147200.0000, 
raw observation next is [12.33333333333333, 81.0, 0.0, 0.0, 26.0, 25.64418896594798, 0.6138293140988788, 0.0, 1.0, 18727.70189866602], 
processed observation next is [0.0, 0.2608695652173913, 0.8042474607571561, 0.81, 0.0, 0.0, 0.6666666666666666, 0.6370157471623316, 0.704609771366293, 0.0, 1.0, 0.08917953285079057], 
reward next is 0.9108, 
noisyNet noise sample is [array([-1.0346832], dtype=float32), -1.104833]. 
=============================================
[2019-04-03 22:58:23,802] A3C_AGENT_WORKER-Thread-10 INFO:Local step 44500, global step 713026: loss 0.3145
[2019-04-03 22:58:23,804] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 44500, global step 713027: learning rate 0.0001
[2019-04-03 22:58:24,548] A3C_AGENT_WORKER-Thread-2 INFO:Local step 44500, global step 713454: loss 0.1993
[2019-04-03 22:58:24,550] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 44500, global step 713454: learning rate 0.0001
[2019-04-03 22:58:24,724] A3C_AGENT_WORKER-Thread-18 INFO:Local step 44500, global step 713543: loss 0.1906
[2019-04-03 22:58:24,728] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 44500, global step 713543: learning rate 0.0001
[2019-04-03 22:58:25,563] A3C_AGENT_WORKER-Thread-9 INFO:Local step 44500, global step 714024: loss 0.2451
[2019-04-03 22:58:25,564] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 44500, global step 714024: learning rate 0.0001
[2019-04-03 22:58:26,524] A3C_AGENT_WORKER-Thread-16 INFO:Local step 44500, global step 714509: loss 0.2098
[2019-04-03 22:58:26,525] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 44500, global step 714509: learning rate 0.0001
[2019-04-03 22:58:26,753] A3C_AGENT_WORKER-Thread-14 INFO:Local step 44500, global step 714625: loss 0.2590
[2019-04-03 22:58:26,754] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 44500, global step 714625: learning rate 0.0001
[2019-04-03 22:58:31,750] A3C_AGENT_WORKER-Thread-19 INFO:Local step 45000, global step 716873: loss 0.7871
[2019-04-03 22:58:31,751] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 45000, global step 716873: learning rate 0.0001
[2019-04-03 22:58:32,413] A3C_AGENT_WORKER-Thread-17 INFO:Local step 45000, global step 717145: loss 0.7614
[2019-04-03 22:58:32,413] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 45000, global step 717145: learning rate 0.0001
[2019-04-03 22:58:34,260] A3C_AGENT_WORKER-Thread-7 INFO:Local step 45000, global step 717977: loss 0.7617
[2019-04-03 22:58:34,260] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 45000, global step 717977: learning rate 0.0001
[2019-04-03 22:58:36,089] A3C_AGENT_WORKER-Thread-6 INFO:Local step 45000, global step 718719: loss 0.7395
[2019-04-03 22:58:36,090] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 45000, global step 718719: learning rate 0.0001
[2019-04-03 22:58:36,862] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.0812477e-14 2.7116636e-07 5.9867922e-12 5.8116756e-10 1.6617282e-07
 1.9209746e-13 9.9999964e-01], sum to 1.0000
[2019-04-03 22:58:36,867] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7312
[2019-04-03 22:58:36,892] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.29622830239762, 0.4611084455235363, 0.0, 1.0, 38790.71666851523], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1399200.0000, 
sim time next is 1399800.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.31206690223276, 0.4523454330760175, 0.0, 1.0, 38681.66262494119], 
processed observation next is [1.0, 0.17391304347826086, 0.44598337950138506, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6093389085193968, 0.6507818110253392, 0.0, 1.0, 0.18419839345210093], 
reward next is 0.8158, 
noisyNet noise sample is [array([-0.3121576], dtype=float32), -0.4616851]. 
=============================================
[2019-04-03 22:58:37,024] A3C_AGENT_WORKER-Thread-8 INFO:Local step 45000, global step 719131: loss 0.7436
[2019-04-03 22:58:37,026] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 45000, global step 719131: learning rate 0.0001
[2019-04-03 22:58:38,571] A3C_AGENT_WORKER-Thread-5 INFO:Local step 45000, global step 719829: loss 0.7325
[2019-04-03 22:58:38,573] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 45000, global step 719830: learning rate 0.0001
[2019-04-03 22:58:39,408] A3C_AGENT_WORKER-Thread-3 INFO:Local step 45000, global step 720235: loss 0.7353
[2019-04-03 22:58:39,412] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 45000, global step 720236: learning rate 0.0001
[2019-04-03 22:58:40,677] A3C_AGENT_WORKER-Thread-4 INFO:Local step 45000, global step 720805: loss 0.7393
[2019-04-03 22:58:40,678] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 45000, global step 720805: learning rate 0.0001
[2019-04-03 22:58:40,748] A3C_AGENT_WORKER-Thread-20 INFO:Local step 45000, global step 720832: loss 0.7272
[2019-04-03 22:58:40,750] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 45000, global step 720832: learning rate 0.0001
[2019-04-03 22:58:40,917] A3C_AGENT_WORKER-Thread-15 INFO:Local step 45000, global step 720900: loss 0.7219
[2019-04-03 22:58:40,920] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 45000, global step 720902: learning rate 0.0001
[2019-04-03 22:58:41,982] A3C_AGENT_WORKER-Thread-2 INFO:Local step 45000, global step 721451: loss 0.7267
[2019-04-03 22:58:41,984] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 45000, global step 721452: learning rate 0.0001
[2019-04-03 22:58:42,161] A3C_AGENT_WORKER-Thread-18 INFO:Local step 45000, global step 721545: loss 0.7394
[2019-04-03 22:58:42,164] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 45000, global step 721545: learning rate 0.0001
[2019-04-03 22:58:42,182] A3C_AGENT_WORKER-Thread-10 INFO:Local step 45000, global step 721555: loss 0.7120
[2019-04-03 22:58:42,184] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 45000, global step 721558: learning rate 0.0001
[2019-04-03 22:58:43,585] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.3886196e-15 2.2048063e-07 2.1088131e-12 1.9634362e-10 2.0137038e-07
 5.1243352e-14 9.9999952e-01], sum to 1.0000
[2019-04-03 22:58:43,590] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9443
[2019-04-03 22:58:43,599] A3C_AGENT_WORKER-Thread-9 INFO:Local step 45000, global step 722261: loss 0.7122
[2019-04-03 22:58:43,601] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 45000, global step 722262: learning rate 0.0001
[2019-04-03 22:58:43,609] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.9, 82.66666666666667, 0.0, 0.0, 26.0, 25.59827398656102, 0.5185689893844188, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1575600.0000, 
sim time next is 1576200.0000, 
raw observation next is [4.95, 82.33333333333334, 0.0, 0.0, 26.0, 25.63560341146033, 0.5172790518018534, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.5997229916897507, 0.8233333333333335, 0.0, 0.0, 0.6666666666666666, 0.6363002842883608, 0.6724263506006177, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4666493], dtype=float32), 0.20080534]. 
=============================================
[2019-04-03 22:58:44,309] A3C_AGENT_WORKER-Thread-16 INFO:Local step 45000, global step 722607: loss 0.7108
[2019-04-03 22:58:44,315] A3C_AGENT_WORKER-Thread-14 INFO:Local step 45000, global step 722612: loss 0.7220
[2019-04-03 22:58:44,328] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 45000, global step 722619: learning rate 0.0001
[2019-04-03 22:58:44,330] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 45000, global step 722620: learning rate 0.0001
[2019-04-03 22:58:49,153] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [4.0210423e-14 4.8518098e-07 7.8340901e-12 2.5006410e-09 5.1691472e-06
 2.9669179e-13 9.9999440e-01], sum to 1.0000
[2019-04-03 22:58:49,153] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3056
[2019-04-03 22:58:49,221] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.466666666666667, 83.0, 124.8333333333333, 0.0, 26.0, 24.94110343126335, 0.3471836890907887, 0.0, 1.0, 46867.40142435212], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1772400.0000, 
sim time next is 1773000.0000, 
raw observation next is [-2.55, 83.0, 126.0, 0.0, 26.0, 24.94465363246887, 0.3516515446275985, 0.0, 1.0, 45027.94160100788], 
processed observation next is [0.0, 0.5217391304347826, 0.3919667590027701, 0.83, 0.42, 0.0, 0.6666666666666666, 0.5787211360390726, 0.6172171815425328, 0.0, 1.0, 0.21441876952860894], 
reward next is 0.7856, 
noisyNet noise sample is [array([1.0605617], dtype=float32), 0.23022453]. 
=============================================
[2019-04-03 22:58:49,228] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[76.10575 ]
 [76.28941 ]
 [76.49946 ]
 [76.749565]
 [77.00693 ]], R is [[75.947052  ]
 [75.96440887]
 [75.9881897 ]
 [76.03865051]
 [76.1158371 ]].
[2019-04-03 22:58:49,970] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.5941596e-15 2.1713817e-08 4.6359151e-13 5.5926508e-10 2.8803028e-08
 6.0465053e-15 1.0000000e+00], sum to 1.0000
[2019-04-03 22:58:49,971] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8515
[2019-04-03 22:58:49,987] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 26.0, 25.48564855300878, 0.5398911098718179, 0.0, 1.0, 53866.41569851194], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1720800.0000, 
sim time next is 1721400.0000, 
raw observation next is [0.4166666666666667, 92.5, 0.0, 0.0, 26.0, 25.46671527672841, 0.4941724955636695, 0.0, 1.0, 55999.19090460148], 
processed observation next is [1.0, 0.9565217391304348, 0.47414589104339805, 0.925, 0.0, 0.0, 0.6666666666666666, 0.622226273060701, 0.6647241651878898, 0.0, 1.0, 0.2666628138314356], 
reward next is 0.7333, 
noisyNet noise sample is [array([0.4423623], dtype=float32), 0.017742617]. 
=============================================
[2019-04-03 22:58:50,272] A3C_AGENT_WORKER-Thread-19 INFO:Local step 45500, global step 725429: loss 0.0173
[2019-04-03 22:58:50,275] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 45500, global step 725429: learning rate 0.0001
[2019-04-03 22:58:50,460] A3C_AGENT_WORKER-Thread-17 INFO:Local step 45500, global step 725501: loss 0.0315
[2019-04-03 22:58:50,462] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 45500, global step 725503: learning rate 0.0001
[2019-04-03 22:58:51,172] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.5131302e-15 1.8050497e-07 8.3953038e-13 2.4865546e-10 6.2532736e-06
 1.7659507e-14 9.9999356e-01], sum to 1.0000
[2019-04-03 22:58:51,176] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8727
[2019-04-03 22:58:51,198] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.6, 97.0, 0.0, 0.0, 26.0, 25.60330305623009, 0.535889661635574, 0.0, 1.0, 106573.9669905736], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1652400.0000, 
sim time next is 1653000.0000, 
raw observation next is [6.6, 97.0, 0.0, 0.0, 26.0, 25.48933549375933, 0.5397255777976143, 0.0, 1.0, 129423.131701693], 
processed observation next is [1.0, 0.13043478260869565, 0.6454293628808865, 0.97, 0.0, 0.0, 0.6666666666666666, 0.6241112911466109, 0.6799085259325381, 0.0, 1.0, 0.616300627150919], 
reward next is 0.3837, 
noisyNet noise sample is [array([2.6873317], dtype=float32), -0.53307766]. 
=============================================
[2019-04-03 22:58:51,233] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[85.60555 ]
 [85.21431 ]
 [85.32449 ]
 [85.441925]
 [85.560074]], R is [[85.6208725 ]
 [85.25717163]
 [85.40460205]
 [85.55056   ]
 [85.6950531 ]].
[2019-04-03 22:58:51,557] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.0378026e-15 1.3685983e-08 1.7466540e-13 2.2601663e-09 1.5084915e-07
 1.0125912e-14 9.9999988e-01], sum to 1.0000
[2019-04-03 22:58:51,558] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6778
[2019-04-03 22:58:51,584] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.516666666666667, 82.16666666666667, 37.66666666666667, 0.0, 26.0, 26.07920314295013, 0.4540562306290494, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1699800.0000, 
sim time next is 1700400.0000, 
raw observation next is [1.433333333333334, 83.33333333333334, 33.83333333333333, 0.0, 26.0, 25.5817407061119, 0.4656407058432579, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.502308402585411, 0.8333333333333335, 0.11277777777777777, 0.0, 0.6666666666666666, 0.6318117255093251, 0.6552135686144193, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.1818435], dtype=float32), -0.97843796]. 
=============================================
[2019-04-03 22:58:52,717] A3C_AGENT_WORKER-Thread-7 INFO:Local step 45500, global step 726283: loss 0.0230
[2019-04-03 22:58:52,724] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 45500, global step 726287: learning rate 0.0001
[2019-04-03 22:58:54,673] A3C_AGENT_WORKER-Thread-6 INFO:Local step 45500, global step 726918: loss 0.0087
[2019-04-03 22:58:54,681] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 45500, global step 726918: learning rate 0.0001
[2019-04-03 22:58:55,683] A3C_AGENT_WORKER-Thread-8 INFO:Local step 45500, global step 727199: loss 0.0126
[2019-04-03 22:58:55,687] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 45500, global step 727200: learning rate 0.0001
[2019-04-03 22:58:58,123] A3C_AGENT_WORKER-Thread-5 INFO:Local step 45500, global step 727863: loss 0.0091
[2019-04-03 22:58:58,124] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 45500, global step 727863: learning rate 0.0001
[2019-04-03 22:58:58,393] A3C_AGENT_WORKER-Thread-3 INFO:Local step 45500, global step 727946: loss 0.0093
[2019-04-03 22:58:58,393] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 45500, global step 727946: learning rate 0.0001
[2019-04-03 22:58:59,299] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.8675287e-12 4.1882922e-05 9.6892681e-11 2.5198746e-08 2.6933421e-05
 2.0238115e-11 9.9993110e-01], sum to 1.0000
[2019-04-03 22:58:59,299] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1473
[2019-04-03 22:58:59,361] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 83.66666666666667, 39.0, 0.0, 26.0, 25.219079688971, 0.3722465521888863, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1759800.0000, 
sim time next is 1760400.0000, 
raw observation next is [-1.7, 83.0, 45.5, 0.0, 26.0, 25.18858768596008, 0.3570919612026413, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.4155124653739613, 0.83, 0.15166666666666667, 0.0, 0.6666666666666666, 0.5990489738300067, 0.6190306537342137, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.21452355], dtype=float32), 1.1810472]. 
=============================================
[2019-04-03 22:59:00,091] A3C_AGENT_WORKER-Thread-20 INFO:Local step 45500, global step 728442: loss 0.0161
[2019-04-03 22:59:00,093] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 45500, global step 728442: learning rate 0.0001
[2019-04-03 22:59:00,100] A3C_AGENT_WORKER-Thread-4 INFO:Local step 45500, global step 728445: loss 0.0184
[2019-04-03 22:59:00,109] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 45500, global step 728447: learning rate 0.0001
[2019-04-03 22:59:00,865] A3C_AGENT_WORKER-Thread-15 INFO:Local step 45500, global step 728669: loss 0.0267
[2019-04-03 22:59:00,874] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 45500, global step 728669: learning rate 0.0001
[2019-04-03 22:59:01,010] A3C_AGENT_WORKER-Thread-2 INFO:Local step 45500, global step 728713: loss 0.0211
[2019-04-03 22:59:01,011] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 45500, global step 728713: learning rate 0.0001
[2019-04-03 22:59:01,684] A3C_AGENT_WORKER-Thread-10 INFO:Local step 45500, global step 728925: loss 0.0234
[2019-04-03 22:59:01,685] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 45500, global step 728925: learning rate 0.0001
[2019-04-03 22:59:01,905] A3C_AGENT_WORKER-Thread-18 INFO:Local step 45500, global step 728997: loss 0.0228
[2019-04-03 22:59:01,906] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 45500, global step 728997: learning rate 0.0001
[2019-04-03 22:59:02,722] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.4987437e-11 2.9732268e-05 1.6519650e-10 3.0822736e-08 7.4801465e-05
 9.0521375e-12 9.9989533e-01], sum to 1.0000
[2019-04-03 22:59:02,735] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3217
[2019-04-03 22:59:02,781] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 82.00000000000001, 9.999999999999998, 0.0, 26.0, 25.008350135357, 0.3261464358508387, 0.0, 1.0, 45146.38445239743], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1789800.0000, 
sim time next is 1790400.0000, 
raw observation next is [-3.9, 82.0, 0.0, 0.0, 26.0, 25.00497737990092, 0.3234504061825144, 0.0, 1.0, 46568.77504752864], 
processed observation next is [0.0, 0.7391304347826086, 0.3545706371191136, 0.82, 0.0, 0.0, 0.6666666666666666, 0.5837481149917435, 0.6078168020608382, 0.0, 1.0, 0.22175607165489827], 
reward next is 0.7782, 
noisyNet noise sample is [array([-0.7195907], dtype=float32), -0.283046]. 
=============================================
[2019-04-03 22:59:03,029] A3C_AGENT_WORKER-Thread-9 INFO:Local step 45500, global step 729343: loss 0.0218
[2019-04-03 22:59:03,036] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 45500, global step 729346: learning rate 0.0001
[2019-04-03 22:59:04,388] A3C_AGENT_WORKER-Thread-14 INFO:Local step 45500, global step 729735: loss 0.0271
[2019-04-03 22:59:04,392] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 45500, global step 729735: learning rate 0.0001
[2019-04-03 22:59:04,531] A3C_AGENT_WORKER-Thread-16 INFO:Local step 45500, global step 729777: loss 0.0295
[2019-04-03 22:59:04,532] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 45500, global step 729777: learning rate 0.0001
[2019-04-03 22:59:04,680] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [7.7523279e-11 6.5010841e-05 1.9654172e-09 3.3566835e-07 1.6275433e-05
 2.6356872e-10 9.9991834e-01], sum to 1.0000
[2019-04-03 22:59:04,680] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2831
[2019-04-03 22:59:04,698] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.366666666666667, 78.66666666666667, 0.0, 0.0, 26.0, 23.52472634768239, -0.04494099355686754, 0.0, 1.0, 47063.60783013808], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1837200.0000, 
sim time next is 1837800.0000, 
raw observation next is [-6.45, 78.5, 0.0, 0.0, 26.0, 23.48840174822771, -0.05238568438804692, 0.0, 1.0, 47069.16123658804], 
processed observation next is [0.0, 0.2608695652173913, 0.28393351800554023, 0.785, 0.0, 0.0, 0.6666666666666666, 0.4573668123523091, 0.4825381052039843, 0.0, 1.0, 0.22413886303137162], 
reward next is 0.7759, 
noisyNet noise sample is [array([-0.02094686], dtype=float32), -0.13977244]. 
=============================================
[2019-04-03 22:59:10,731] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.5537105e-12 2.9443634e-05 4.4970327e-10 7.4273970e-08 3.4188259e-05
 1.8193157e-11 9.9993622e-01], sum to 1.0000
[2019-04-03 22:59:10,731] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5522
[2019-04-03 22:59:10,779] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.483333333333334, 78.66666666666667, 0.0, 0.0, 26.0, 23.72035844825436, -0.06884632734639884, 0.0, 1.0, 45157.22825710701], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1915800.0000, 
sim time next is 1916400.0000, 
raw observation next is [-8.566666666666666, 79.33333333333334, 0.0, 0.0, 26.0, 23.69221791172737, -0.07373859846474624, 0.0, 1.0, 45241.56699154932], 
processed observation next is [1.0, 0.17391304347826086, 0.22530009233610343, 0.7933333333333334, 0.0, 0.0, 0.6666666666666666, 0.4743514926439474, 0.4754204671784179, 0.0, 1.0, 0.215436033293092], 
reward next is 0.7846, 
noisyNet noise sample is [array([-1.6439606], dtype=float32), 0.37537098]. 
=============================================
[2019-04-03 22:59:17,494] A3C_AGENT_WORKER-Thread-19 INFO:Local step 46000, global step 733580: loss 0.8487
[2019-04-03 22:59:17,496] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 46000, global step 733580: learning rate 0.0001
[2019-04-03 22:59:18,098] A3C_AGENT_WORKER-Thread-17 INFO:Local step 46000, global step 733755: loss 0.7947
[2019-04-03 22:59:18,098] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 46000, global step 733755: learning rate 0.0001
[2019-04-03 22:59:20,296] A3C_AGENT_WORKER-Thread-7 INFO:Local step 46000, global step 734364: loss 0.7539
[2019-04-03 22:59:20,297] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 46000, global step 734364: learning rate 0.0001
[2019-04-03 22:59:22,780] A3C_AGENT_WORKER-Thread-6 INFO:Local step 46000, global step 735027: loss 0.6122
[2019-04-03 22:59:22,780] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 46000, global step 735027: learning rate 0.0001
[2019-04-03 22:59:23,426] A3C_AGENT_WORKER-Thread-8 INFO:Local step 46000, global step 735233: loss 0.5838
[2019-04-03 22:59:23,427] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 46000, global step 735233: learning rate 0.0001
[2019-04-03 22:59:25,334] A3C_AGENT_WORKER-Thread-5 INFO:Local step 46000, global step 735801: loss 0.4385
[2019-04-03 22:59:25,335] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 46000, global step 735801: learning rate 0.0001
[2019-04-03 22:59:25,552] A3C_AGENT_WORKER-Thread-3 INFO:Local step 46000, global step 735862: loss 0.4134
[2019-04-03 22:59:25,569] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 46000, global step 735862: learning rate 0.0001
[2019-04-03 22:59:26,693] A3C_AGENT_WORKER-Thread-20 INFO:Local step 46000, global step 736190: loss 0.3105
[2019-04-03 22:59:26,697] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 46000, global step 736192: learning rate 0.0001
[2019-04-03 22:59:27,003] A3C_AGENT_WORKER-Thread-4 INFO:Local step 46000, global step 736276: loss 0.2470
[2019-04-03 22:59:27,004] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 46000, global step 736276: learning rate 0.0001
[2019-04-03 22:59:27,182] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.1506470e-13 1.2801365e-06 3.6281893e-12 4.8639222e-09 9.8989983e-07
 4.7230492e-13 9.9999774e-01], sum to 1.0000
[2019-04-03 22:59:27,182] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7871
[2019-04-03 22:59:27,214] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.666666666666667, 86.0, 0.0, 0.0, 26.0, 24.63242087320867, 0.2098085052664004, 0.0, 1.0, 42742.73317844255], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2082000.0000, 
sim time next is 2082600.0000, 
raw observation next is [-4.75, 86.0, 0.0, 0.0, 26.0, 24.62384610264735, 0.2016990465263108, 0.0, 1.0, 42796.58356774782], 
processed observation next is [1.0, 0.08695652173913043, 0.3310249307479225, 0.86, 0.0, 0.0, 0.6666666666666666, 0.5519871752206124, 0.5672330155087703, 0.0, 1.0, 0.2037932550845134], 
reward next is 0.7962, 
noisyNet noise sample is [array([-0.9581224], dtype=float32), 1.3743697]. 
=============================================
[2019-04-03 22:59:27,738] A3C_AGENT_WORKER-Thread-15 INFO:Local step 46000, global step 736475: loss 0.2326
[2019-04-03 22:59:27,739] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 46000, global step 736475: learning rate 0.0001
[2019-04-03 22:59:27,780] A3C_AGENT_WORKER-Thread-2 INFO:Local step 46000, global step 736485: loss 0.2482
[2019-04-03 22:59:27,780] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 46000, global step 736485: learning rate 0.0001
[2019-04-03 22:59:28,402] A3C_AGENT_WORKER-Thread-10 INFO:Local step 46000, global step 736663: loss 0.2118
[2019-04-03 22:59:28,423] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 46000, global step 736663: learning rate 0.0001
[2019-04-03 22:59:29,811] A3C_AGENT_WORKER-Thread-18 INFO:Local step 46000, global step 737044: loss 0.1738
[2019-04-03 22:59:29,813] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 46000, global step 737044: learning rate 0.0001
[2019-04-03 22:59:29,892] A3C_AGENT_WORKER-Thread-9 INFO:Local step 46000, global step 737062: loss 0.1890
[2019-04-03 22:59:29,893] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 46000, global step 737062: learning rate 0.0001
[2019-04-03 22:59:30,058] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.8212417e-14 1.4487051e-06 3.1707827e-11 6.1180545e-09 2.3605533e-06
 3.3235060e-13 9.9999619e-01], sum to 1.0000
[2019-04-03 22:59:30,064] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8759
[2019-04-03 22:59:30,082] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.299999999999999, 82.0, 0.0, 0.0, 26.0, 24.49039176644276, 0.2088146664910109, 0.0, 1.0, 42394.32130621357], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2159400.0000, 
sim time next is 2160000.0000, 
raw observation next is [-7.3, 82.0, 0.0, 0.0, 26.0, 24.44086010184872, 0.1984230516547663, 0.0, 1.0, 42429.08293234652], 
processed observation next is [1.0, 0.0, 0.26038781163434904, 0.82, 0.0, 0.0, 0.6666666666666666, 0.5367383418207267, 0.5661410172182554, 0.0, 1.0, 0.20204325205879298], 
reward next is 0.7980, 
noisyNet noise sample is [array([0.4761696], dtype=float32), 0.205665]. 
=============================================
[2019-04-03 22:59:30,087] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[74.49194]
 [74.67636]
 [74.85324]
 [75.05043]
 [75.2325 ]], R is [[74.37198639]
 [74.42638397]
 [74.48041534]
 [74.53408051]
 [74.58738708]].
[2019-04-03 22:59:31,743] A3C_AGENT_WORKER-Thread-16 INFO:Local step 46000, global step 737629: loss 0.1092
[2019-04-03 22:59:31,744] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 46000, global step 737629: learning rate 0.0001
[2019-04-03 22:59:32,284] A3C_AGENT_WORKER-Thread-14 INFO:Local step 46000, global step 737788: loss 0.0947
[2019-04-03 22:59:32,289] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 46000, global step 737789: learning rate 0.0001
[2019-04-03 22:59:33,465] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.4497362e-15 9.8732635e-08 4.9623229e-13 3.1613925e-10 3.8018766e-07
 8.0991732e-15 9.9999952e-01], sum to 1.0000
[2019-04-03 22:59:33,465] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8931
[2019-04-03 22:59:33,526] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.600000000000001, 83.0, 0.0, 0.0, 26.0, 25.03829442766529, 0.3901295062419545, 0.0, 1.0, 72001.6971062378], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2146200.0000, 
sim time next is 2146800.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 25.09447048708305, 0.3992791477660029, 0.0, 1.0, 109835.5440246133], 
processed observation next is [1.0, 0.8695652173913043, 0.30747922437673136, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5912058739235876, 0.6330930492553343, 0.0, 1.0, 0.5230264001172061], 
reward next is 0.4770, 
noisyNet noise sample is [array([0.84127855], dtype=float32), -1.815785]. 
=============================================
[2019-04-03 22:59:38,387] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.5089894e-12 1.0416588e-05 1.5797841e-10 1.2050626e-08 7.9960955e-06
 7.4383104e-12 9.9998164e-01], sum to 1.0000
[2019-04-03 22:59:38,387] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9982
[2019-04-03 22:59:38,437] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.616666666666667, 77.5, 0.0, 0.0, 26.0, 24.17331271805243, 0.09195745784479344, 0.0, 1.0, 42113.57436515755], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2175000.0000, 
sim time next is 2175600.0000, 
raw observation next is [-6.533333333333334, 77.0, 0.0, 0.0, 26.0, 24.1488636117085, 0.08516808089209482, 0.0, 1.0, 42074.23030846928], 
processed observation next is [1.0, 0.17391304347826086, 0.28162511542012925, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5124053009757082, 0.5283893602973649, 0.0, 1.0, 0.20035347765937753], 
reward next is 0.7996, 
noisyNet noise sample is [array([0.81001514], dtype=float32), -0.076929025]. 
=============================================
[2019-04-03 22:59:44,471] A3C_AGENT_WORKER-Thread-19 INFO:Local step 46500, global step 741308: loss 0.1409
[2019-04-03 22:59:44,482] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 46500, global step 741309: learning rate 0.0001
[2019-04-03 22:59:45,345] A3C_AGENT_WORKER-Thread-17 INFO:Local step 46500, global step 741596: loss 0.0965
[2019-04-03 22:59:45,347] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 46500, global step 741596: learning rate 0.0001
[2019-04-03 22:59:47,950] A3C_AGENT_WORKER-Thread-7 INFO:Local step 46500, global step 742432: loss 0.1074
[2019-04-03 22:59:47,957] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 46500, global step 742432: learning rate 0.0001
[2019-04-03 22:59:50,216] A3C_AGENT_WORKER-Thread-8 INFO:Local step 46500, global step 743150: loss 0.1535
[2019-04-03 22:59:50,231] A3C_AGENT_WORKER-Thread-6 INFO:Local step 46500, global step 743154: loss 0.1401
[2019-04-03 22:59:50,232] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 46500, global step 743154: learning rate 0.0001
[2019-04-03 22:59:50,239] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 46500, global step 743157: learning rate 0.0001
[2019-04-03 22:59:50,597] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.2462519e-13 3.1243630e-06 2.2039974e-11 2.3873408e-09 1.8347889e-06
 4.7974897e-12 9.9999511e-01], sum to 1.0000
[2019-04-03 22:59:50,598] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2844
[2019-04-03 22:59:50,637] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.3, 68.33333333333333, 0.0, 0.0, 26.0, 24.12265151173925, 0.08745158277712269, 0.0, 1.0, 41323.20927652468], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2357400.0000, 
sim time next is 2358000.0000, 
raw observation next is [-3.4, 69.0, 0.0, 0.0, 26.0, 24.10928345615132, 0.08090820450857202, 0.0, 1.0, 41374.74805384606], 
processed observation next is [0.0, 0.30434782608695654, 0.368421052631579, 0.69, 0.0, 0.0, 0.6666666666666666, 0.5091069546792767, 0.5269694015028573, 0.0, 1.0, 0.19702260978021932], 
reward next is 0.8030, 
noisyNet noise sample is [array([0.34646162], dtype=float32), 0.40849707]. 
=============================================
[2019-04-03 22:59:50,666] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[74.302   ]
 [74.351456]
 [74.40309 ]
 [74.449486]
 [74.50185 ]], R is [[74.3099823 ]
 [74.37010956]
 [74.42991638]
 [74.48941803]
 [74.54846191]].
[2019-04-03 22:59:51,899] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.4183772e-13 1.4605815e-06 4.8296641e-11 6.1762275e-09 2.2341570e-05
 1.5150722e-12 9.9997628e-01], sum to 1.0000
[2019-04-03 22:59:51,900] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9020
[2019-04-03 22:59:51,966] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 49.5, 160.0, 0.0, 26.0, 24.97501436656589, 0.2913829176499179, 0.0, 1.0, 24125.66260088992], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2385000.0000, 
sim time next is 2385600.0000, 
raw observation next is [0.0, 48.66666666666667, 147.6666666666667, 56.83333333333332, 26.0, 24.9544538773636, 0.2923813170251463, 0.0, 1.0, 38503.46011838165], 
processed observation next is [0.0, 0.6086956521739131, 0.46260387811634357, 0.4866666666666667, 0.4922222222222224, 0.06279926335174953, 0.6666666666666666, 0.5795378231136334, 0.5974604390083821, 0.0, 1.0, 0.18334981008753165], 
reward next is 0.8167, 
noisyNet noise sample is [array([-0.7055088], dtype=float32), -0.6805544]. 
=============================================
[2019-04-03 22:59:52,325] A3C_AGENT_WORKER-Thread-3 INFO:Local step 46500, global step 743855: loss 0.1138
[2019-04-03 22:59:52,328] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 46500, global step 743855: learning rate 0.0001
[2019-04-03 22:59:53,269] A3C_AGENT_WORKER-Thread-5 INFO:Local step 46500, global step 744161: loss 0.1401
[2019-04-03 22:59:53,270] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 46500, global step 744161: learning rate 0.0001
[2019-04-03 22:59:54,119] A3C_AGENT_WORKER-Thread-4 INFO:Local step 46500, global step 744411: loss 0.1600
[2019-04-03 22:59:54,120] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 46500, global step 744411: learning rate 0.0001
[2019-04-03 22:59:54,438] A3C_AGENT_WORKER-Thread-15 INFO:Local step 46500, global step 744517: loss 0.1573
[2019-04-03 22:59:54,439] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 46500, global step 744517: learning rate 0.0001
[2019-04-03 22:59:54,570] A3C_AGENT_WORKER-Thread-20 INFO:Local step 46500, global step 744558: loss 0.1606
[2019-04-03 22:59:54,571] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 46500, global step 744558: learning rate 0.0001
[2019-04-03 22:59:54,706] A3C_AGENT_WORKER-Thread-2 INFO:Local step 46500, global step 744609: loss 0.1377
[2019-04-03 22:59:54,706] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 46500, global step 744609: learning rate 0.0001
[2019-04-03 22:59:55,902] A3C_AGENT_WORKER-Thread-10 INFO:Local step 46500, global step 745077: loss 0.1665
[2019-04-03 22:59:55,904] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 46500, global step 745077: learning rate 0.0001
[2019-04-03 22:59:57,487] A3C_AGENT_WORKER-Thread-18 INFO:Local step 46500, global step 745561: loss 0.1799
[2019-04-03 22:59:57,497] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 46500, global step 745561: learning rate 0.0001
[2019-04-03 22:59:57,700] A3C_AGENT_WORKER-Thread-9 INFO:Local step 46500, global step 745634: loss 0.1622
[2019-04-03 22:59:57,701] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 46500, global step 745634: learning rate 0.0001
[2019-04-03 22:59:58,542] A3C_AGENT_WORKER-Thread-16 INFO:Local step 46500, global step 745961: loss 0.2380
[2019-04-03 22:59:58,543] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 46500, global step 745961: learning rate 0.0001
[2019-04-03 22:59:59,435] A3C_AGENT_WORKER-Thread-14 INFO:Local step 46500, global step 746322: loss 0.2518
[2019-04-03 22:59:59,437] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 46500, global step 746322: learning rate 0.0001
[2019-04-03 22:59:59,514] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.8754553e-11 3.0308633e-05 5.6344102e-10 7.7014803e-08 1.1137765e-05
 6.6833712e-11 9.9995852e-01], sum to 1.0000
[2019-04-03 22:59:59,514] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5740
[2019-04-03 22:59:59,527] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.4, 60.83333333333334, 0.0, 0.0, 26.0, 23.03151777956982, -0.1996443765584041, 0.0, 1.0, 44056.21935516662], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2443800.0000, 
sim time next is 2444400.0000, 
raw observation next is [-9.5, 61.0, 0.0, 0.0, 26.0, 22.98791673514511, -0.2090736133265978, 0.0, 1.0, 44087.85257250348], 
processed observation next is [0.0, 0.30434782608695654, 0.1994459833795014, 0.61, 0.0, 0.0, 0.6666666666666666, 0.4156597279287591, 0.4303087955578007, 0.0, 1.0, 0.2099421551071594], 
reward next is 0.7901, 
noisyNet noise sample is [array([0.6262899], dtype=float32), -0.5237548]. 
=============================================
[2019-04-03 23:00:01,391] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.6403520e-11 2.2897605e-04 7.6631373e-10 5.3563845e-08 1.8786073e-05
 1.2534711e-10 9.9975210e-01], sum to 1.0000
[2019-04-03 23:00:01,394] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1132
[2019-04-03 23:00:01,428] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.5, 58.5, 15.33333333333333, 165.3333333333333, 26.0, 22.82011477923628, -0.2236800061830001, 0.0, 1.0, 44180.10961037297], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2447400.0000, 
sim time next is 2448000.0000, 
raw observation next is [-9.5, 58.0, 21.5, 228.0, 26.0, 22.83146156358064, -0.2198925571330972, 0.0, 1.0, 44076.79573648464], 
processed observation next is [0.0, 0.34782608695652173, 0.1994459833795014, 0.58, 0.07166666666666667, 0.25193370165745854, 0.6666666666666666, 0.40262179696505324, 0.4267024809556343, 0.0, 1.0, 0.2098895035070697], 
reward next is 0.7901, 
noisyNet noise sample is [array([-0.4623347], dtype=float32), -0.5017896]. 
=============================================
[2019-04-03 23:00:01,435] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[70.61803 ]
 [70.039116]
 [69.55329 ]
 [69.727325]
 [69.89848 ]], R is [[71.25340271]
 [71.33049011]
 [71.40657043]
 [71.48193359]
 [71.55677795]].
[2019-04-03 23:00:06,706] A3C_AGENT_WORKER-Thread-19 INFO:Local step 47000, global step 749058: loss 0.2258
[2019-04-03 23:00:06,707] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 47000, global step 749058: learning rate 0.0001
[2019-04-03 23:00:06,981] A3C_AGENT_WORKER-Thread-17 INFO:Local step 47000, global step 749170: loss 0.2159
[2019-04-03 23:00:06,981] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 47000, global step 749170: learning rate 0.0001
[2019-04-03 23:00:07,114] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.5339556e-14 1.2740601e-06 1.5978120e-11 4.0894625e-09 1.0695767e-06
 2.3218802e-13 9.9999762e-01], sum to 1.0000
[2019-04-03 23:00:07,114] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2324
[2019-04-03 23:00:07,127] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.1, 54.33333333333334, 0.0, 0.0, 26.0, 24.80705310349936, 0.1474665675167543, 0.0, 1.0, 38378.4149832624], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2522400.0000, 
sim time next is 2523000.0000, 
raw observation next is [-2.2, 55.66666666666666, 0.0, 0.0, 26.0, 24.79551158743869, 0.1413091432261547, 0.0, 1.0, 38370.32712944461], 
processed observation next is [1.0, 0.17391304347826086, 0.4016620498614959, 0.5566666666666665, 0.0, 0.0, 0.6666666666666666, 0.5662926322865575, 0.5471030477420515, 0.0, 1.0, 0.18271584347354575], 
reward next is 0.8173, 
noisyNet noise sample is [array([0.55820626], dtype=float32), -1.0599984]. 
=============================================
[2019-04-03 23:00:07,130] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[81.749535]
 [81.70194 ]
 [81.66149 ]
 [81.63077 ]
 [81.60242 ]], R is [[81.79577637]
 [81.79506683]
 [81.79428101]
 [81.79341125]
 [81.79266357]].
[2019-04-03 23:00:08,215] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.2737103e-12 2.4269644e-05 4.8409049e-10 1.6367149e-08 5.5453671e-05
 1.0690242e-11 9.9992025e-01], sum to 1.0000
[2019-04-03 23:00:08,224] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3808
[2019-04-03 23:00:08,247] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-15.0, 91.0, 0.0, 0.0, 26.0, 23.51180627460512, -0.01140833228865865, 0.0, 1.0, 44500.55273092505], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2692800.0000, 
sim time next is 2693400.0000, 
raw observation next is [-15.0, 89.66666666666667, 0.0, 0.0, 26.0, 23.43901901650056, -0.01876596742700646, 0.0, 1.0, 44479.33604797773], 
processed observation next is [1.0, 0.17391304347826086, 0.04709141274238226, 0.8966666666666667, 0.0, 0.0, 0.6666666666666666, 0.45325158470837995, 0.4937446775243312, 0.0, 1.0, 0.2118063621332273], 
reward next is 0.7882, 
noisyNet noise sample is [array([-1.1576291], dtype=float32), -1.3868469]. 
=============================================
[2019-04-03 23:00:10,043] A3C_AGENT_WORKER-Thread-7 INFO:Local step 47000, global step 750274: loss 0.2457
[2019-04-03 23:00:10,044] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 47000, global step 750275: learning rate 0.0001
[2019-04-03 23:00:11,419] A3C_AGENT_WORKER-Thread-8 INFO:Local step 47000, global step 750748: loss 0.2778
[2019-04-03 23:00:11,420] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 47000, global step 750749: learning rate 0.0001
[2019-04-03 23:00:12,676] A3C_AGENT_WORKER-Thread-6 INFO:Local step 47000, global step 751180: loss 0.2517
[2019-04-03 23:00:12,676] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 47000, global step 751180: learning rate 0.0001
[2019-04-03 23:00:13,221] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.5005426e-14 2.6910797e-07 1.0020761e-13 3.3547748e-10 1.5850743e-07
 4.7666385e-15 9.9999964e-01], sum to 1.0000
[2019-04-03 23:00:13,221] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2296
[2019-04-03 23:00:13,277] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.45, 50.5, 245.0, 147.0, 26.0, 25.71924457395604, 0.2802656686946031, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2637000.0000, 
sim time next is 2637600.0000, 
raw observation next is [-1.166666666666667, 49.33333333333334, 231.5, 157.6666666666667, 26.0, 25.08760560501431, 0.313864132104004, 1.0, 1.0, 132687.4598501023], 
processed observation next is [1.0, 0.5217391304347826, 0.43028624192059095, 0.4933333333333334, 0.7716666666666666, 0.17421731123388587, 0.6666666666666666, 0.5906338004178592, 0.6046213773680013, 1.0, 1.0, 0.6318450469052491], 
reward next is 0.3682, 
noisyNet noise sample is [array([-0.9988051], dtype=float32), -0.20071979]. 
=============================================
[2019-04-03 23:00:14,554] A3C_AGENT_WORKER-Thread-3 INFO:Local step 47000, global step 751808: loss 0.3343
[2019-04-03 23:00:14,556] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 47000, global step 751809: learning rate 0.0001
[2019-04-03 23:00:14,950] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.3950146e-14 5.4670278e-07 9.2893640e-13 2.8237255e-09 7.1153977e-07
 5.0865480e-14 9.9999869e-01], sum to 1.0000
[2019-04-03 23:00:14,951] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9684
[2019-04-03 23:00:14,995] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.666666666666666, 26.0, 114.1666666666667, 0.0, 26.0, 25.85820339888743, 0.3073857031331968, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2817600.0000, 
sim time next is 2818200.0000, 
raw observation next is [6.833333333333334, 25.0, 110.3333333333333, 0.0, 26.0, 25.38729055437526, 0.3276605735810444, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.651892890120037, 0.25, 0.36777777777777765, 0.0, 0.6666666666666666, 0.6156075461979382, 0.6092201911936814, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.8224703], dtype=float32), -0.24710749]. 
=============================================
[2019-04-03 23:00:15,132] A3C_AGENT_WORKER-Thread-5 INFO:Local step 47000, global step 752024: loss 0.3310
[2019-04-03 23:00:15,145] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 47000, global step 752026: learning rate 0.0001
[2019-04-03 23:00:15,782] A3C_AGENT_WORKER-Thread-4 INFO:Local step 47000, global step 752234: loss 0.3538
[2019-04-03 23:00:15,783] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 47000, global step 752234: learning rate 0.0001
[2019-04-03 23:00:16,430] A3C_AGENT_WORKER-Thread-20 INFO:Local step 47000, global step 752426: loss 0.3568
[2019-04-03 23:00:16,430] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 47000, global step 752426: learning rate 0.0001
[2019-04-03 23:00:16,559] A3C_AGENT_WORKER-Thread-2 INFO:Local step 47000, global step 752472: loss 0.3943
[2019-04-03 23:00:16,560] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 47000, global step 752473: learning rate 0.0001
[2019-04-03 23:00:16,881] A3C_AGENT_WORKER-Thread-15 INFO:Local step 47000, global step 752581: loss 0.4158
[2019-04-03 23:00:16,881] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 47000, global step 752581: learning rate 0.0001
[2019-04-03 23:00:17,822] A3C_AGENT_WORKER-Thread-10 INFO:Local step 47000, global step 752911: loss 0.4885
[2019-04-03 23:00:17,823] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 47000, global step 752911: learning rate 0.0001
[2019-04-03 23:00:19,180] A3C_AGENT_WORKER-Thread-18 INFO:Local step 47000, global step 753373: loss 0.5822
[2019-04-03 23:00:19,198] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 47000, global step 753374: learning rate 0.0001
[2019-04-03 23:00:19,544] A3C_AGENT_WORKER-Thread-16 INFO:Local step 47000, global step 753489: loss 0.5833
[2019-04-03 23:00:19,545] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 47000, global step 753489: learning rate 0.0001
[2019-04-03 23:00:19,604] A3C_AGENT_WORKER-Thread-9 INFO:Local step 47000, global step 753508: loss 0.5155
[2019-04-03 23:00:19,608] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 47000, global step 753509: learning rate 0.0001
[2019-04-03 23:00:21,477] A3C_AGENT_WORKER-Thread-14 INFO:Local step 47000, global step 754169: loss 0.6867
[2019-04-03 23:00:21,479] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 47000, global step 754170: learning rate 0.0001
[2019-04-03 23:00:26,173] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [9.1255757e-13 5.8792280e-06 3.6657878e-11 2.8558757e-08 1.9487654e-05
 7.8836812e-13 9.9997461e-01], sum to 1.0000
[2019-04-03 23:00:26,198] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6716
[2019-04-03 23:00:26,252] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 44.0, 0.0, 0.0, 26.0, 25.48260353408074, 0.4346205361278719, 0.0, 1.0, 18756.36719757331], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2843400.0000, 
sim time next is 2844000.0000, 
raw observation next is [2.0, 44.0, 0.0, 0.0, 26.0, 25.49540489787267, 0.432169868648464, 0.0, 1.0, 18752.42857460346], 
processed observation next is [1.0, 0.9565217391304348, 0.518005540166205, 0.44, 0.0, 0.0, 0.6666666666666666, 0.6246170748227226, 0.6440566228828214, 0.0, 1.0, 0.08929727892668314], 
reward next is 0.9107, 
noisyNet noise sample is [array([0.74879134], dtype=float32), 0.42385247]. 
=============================================
[2019-04-03 23:00:26,272] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[75.639275]
 [75.713234]
 [75.75505 ]
 [75.793724]
 [75.63903 ]], R is [[75.69723511]
 [75.85094452]
 [76.00309753]
 [76.08382416]
 [76.08738708]].
[2019-04-03 23:00:29,281] A3C_AGENT_WORKER-Thread-19 INFO:Local step 47500, global step 756982: loss 0.0027
[2019-04-03 23:00:29,282] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 47500, global step 756982: learning rate 0.0001
[2019-04-03 23:00:30,071] A3C_AGENT_WORKER-Thread-17 INFO:Local step 47500, global step 757325: loss 0.0020
[2019-04-03 23:00:30,072] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 47500, global step 757325: learning rate 0.0001
[2019-04-03 23:00:33,491] A3C_AGENT_WORKER-Thread-7 INFO:Local step 47500, global step 758523: loss 0.0032
[2019-04-03 23:00:33,492] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 47500, global step 758524: learning rate 0.0001
[2019-04-03 23:00:34,687] A3C_AGENT_WORKER-Thread-8 INFO:Local step 47500, global step 758975: loss 0.0012
[2019-04-03 23:00:34,687] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 47500, global step 758975: learning rate 0.0001
[2019-04-03 23:00:35,968] A3C_AGENT_WORKER-Thread-6 INFO:Local step 47500, global step 759384: loss 0.0011
[2019-04-03 23:00:35,975] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 47500, global step 759384: learning rate 0.0001
[2019-04-03 23:00:37,609] A3C_AGENT_WORKER-Thread-3 INFO:Local step 47500, global step 759973: loss 0.0141
[2019-04-03 23:00:37,612] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 47500, global step 759973: learning rate 0.0001
[2019-04-03 23:00:38,228] A3C_AGENT_WORKER-Thread-5 INFO:Local step 47500, global step 760204: loss 0.0101
[2019-04-03 23:00:38,228] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 47500, global step 760204: learning rate 0.0001
[2019-04-03 23:00:39,022] A3C_AGENT_WORKER-Thread-2 INFO:Local step 47500, global step 760481: loss 0.0392
[2019-04-03 23:00:39,026] A3C_AGENT_WORKER-Thread-20 INFO:Local step 47500, global step 760482: loss 0.0265
[2019-04-03 23:00:39,028] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 47500, global step 760481: learning rate 0.0001
[2019-04-03 23:00:39,031] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 47500, global step 760482: learning rate 0.0001
[2019-04-03 23:00:39,200] A3C_AGENT_WORKER-Thread-4 INFO:Local step 47500, global step 760556: loss 0.0311
[2019-04-03 23:00:39,201] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 47500, global step 760556: learning rate 0.0001
[2019-04-03 23:00:40,379] A3C_AGENT_WORKER-Thread-15 INFO:Local step 47500, global step 761010: loss 0.0357
[2019-04-03 23:00:40,383] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 47500, global step 761010: learning rate 0.0001
[2019-04-03 23:00:40,447] A3C_AGENT_WORKER-Thread-10 INFO:Local step 47500, global step 761035: loss 0.0429
[2019-04-03 23:00:40,448] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 47500, global step 761036: learning rate 0.0001
[2019-04-03 23:00:42,280] A3C_AGENT_WORKER-Thread-16 INFO:Local step 47500, global step 761751: loss 0.0647
[2019-04-03 23:00:42,300] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 47500, global step 761755: learning rate 0.0001
[2019-04-03 23:00:42,396] A3C_AGENT_WORKER-Thread-18 INFO:Local step 47500, global step 761797: loss 0.0581
[2019-04-03 23:00:42,416] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 47500, global step 761804: learning rate 0.0001
[2019-04-03 23:00:42,963] A3C_AGENT_WORKER-Thread-9 INFO:Local step 47500, global step 762021: loss 0.0924
[2019-04-03 23:00:42,965] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 47500, global step 762021: learning rate 0.0001
[2019-04-03 23:00:43,788] A3C_AGENT_WORKER-Thread-14 INFO:Local step 47500, global step 762354: loss 0.1209
[2019-04-03 23:00:43,790] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 47500, global step 762355: learning rate 0.0001
[2019-04-03 23:00:44,905] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [7.2787124e-16 1.7797488e-07 2.0545547e-13 2.8623614e-11 2.9664361e-07
 2.5723309e-15 9.9999964e-01], sum to 1.0000
[2019-04-03 23:00:44,906] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.2305
[2019-04-03 23:00:44,940] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.166666666666667, 98.83333333333334, 0.0, 0.0, 26.0, 25.57084413782835, 0.6148879234403946, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3199800.0000, 
sim time next is 3200400.0000, 
raw observation next is [1.0, 100.0, 0.0, 0.0, 26.0, 25.69015318137225, 0.6134277731418734, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6408460984476875, 0.7044759243806245, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.25089905], dtype=float32), -0.30218747]. 
=============================================
[2019-04-03 23:00:45,004] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.2763674e-14 3.7678910e-07 1.6039937e-12 1.8702999e-09 6.5258411e-07
 7.7026169e-15 9.9999905e-01], sum to 1.0000
[2019-04-03 23:00:45,007] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4786
[2019-04-03 23:00:45,055] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 69.0, 0.0, 0.0, 26.0, 25.87484374886951, 0.5971602806823978, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3267600.0000, 
sim time next is 3268200.0000, 
raw observation next is [-4.0, 70.0, 0.0, 0.0, 26.0, 25.79171156620193, 0.5753719395719653, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.3518005540166205, 0.7, 0.0, 0.0, 0.6666666666666666, 0.6493092971834941, 0.6917906465239884, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2781501], dtype=float32), 0.023459345]. 
=============================================
[2019-04-03 23:00:47,713] A3C_AGENT_WORKER-Thread-19 INFO:Local step 48000, global step 764136: loss 87.8777
[2019-04-03 23:00:47,714] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 48000, global step 764136: learning rate 0.0001
[2019-04-03 23:00:48,138] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0417563e-14 2.3239214e-05 1.0367148e-12 1.5196295e-09 1.6079704e-05
 5.0946951e-14 9.9996066e-01], sum to 1.0000
[2019-04-03 23:00:48,138] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5399
[2019-04-03 23:00:48,152] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.0, 100.0, 0.0, 0.0, 26.0, 25.35530897277023, 0.3151236837328787, 0.0, 1.0, 57221.16875474429], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3133800.0000, 
sim time next is 3134400.0000, 
raw observation next is [5.333333333333334, 100.0, 0.0, 0.0, 26.0, 25.36794117606976, 0.3223340293665175, 0.0, 1.0, 53446.16133860867], 
processed observation next is [1.0, 0.2608695652173913, 0.6103416435826409, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6139950980058133, 0.6074446764555058, 0.0, 1.0, 0.2545055301838508], 
reward next is 0.7455, 
noisyNet noise sample is [array([-1.3957964], dtype=float32), -0.44380283]. 
=============================================
[2019-04-03 23:00:48,438] A3C_AGENT_WORKER-Thread-17 INFO:Local step 48000, global step 764498: loss 88.5143
[2019-04-03 23:00:48,439] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 48000, global step 764498: learning rate 0.0001
[2019-04-03 23:00:49,278] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.5040412e-17 3.7470116e-07 2.4371988e-15 7.9592131e-12 3.9022811e-07
 2.3253228e-17 9.9999928e-01], sum to 1.0000
[2019-04-03 23:00:49,280] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2395
[2019-04-03 23:00:49,289] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.0, 93.0, 113.5, 814.0, 26.0, 27.22053036406051, 0.8141158922672882, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3153600.0000, 
sim time next is 3154200.0000, 
raw observation next is [7.833333333333334, 94.16666666666666, 113.3333333333333, 817.0, 26.0, 27.24864630750663, 0.827189883118769, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.6795937211449677, 0.9416666666666665, 0.37777777777777766, 0.9027624309392265, 0.6666666666666666, 0.7707205256255524, 0.7757299610395897, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1208599], dtype=float32), -0.34776157]. 
=============================================
[2019-04-03 23:00:51,366] A3C_AGENT_WORKER-Thread-7 INFO:Local step 48000, global step 766038: loss 86.9253
[2019-04-03 23:00:51,367] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 48000, global step 766038: learning rate 0.0001
[2019-04-03 23:00:52,601] A3C_AGENT_WORKER-Thread-8 INFO:Local step 48000, global step 766645: loss 86.9439
[2019-04-03 23:00:52,602] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 48000, global step 766646: learning rate 0.0001
[2019-04-03 23:00:53,227] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.8442464e-14 6.3910392e-07 7.2748269e-13 5.3384080e-10 5.0451013e-06
 1.1912665e-14 9.9999440e-01], sum to 1.0000
[2019-04-03 23:00:53,227] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7382
[2019-04-03 23:00:53,239] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.6, 84.0, 109.6666666666667, 761.8333333333334, 26.0, 26.41044731013784, 0.7061941308942287, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3235200.0000, 
sim time next is 3235800.0000, 
raw observation next is [-2.5, 82.0, 110.3333333333333, 771.6666666666666, 26.0, 26.4440888990793, 0.7131165202170457, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.39335180055401664, 0.82, 0.36777777777777765, 0.8526703499079189, 0.6666666666666666, 0.703674074923275, 0.7377055067390152, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.919527], dtype=float32), -0.08510583]. 
=============================================
[2019-04-03 23:00:53,892] A3C_AGENT_WORKER-Thread-6 INFO:Local step 48000, global step 767262: loss 87.4464
[2019-04-03 23:00:53,892] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 48000, global step 767262: learning rate 0.0001
[2019-04-03 23:00:54,860] A3C_AGENT_WORKER-Thread-3 INFO:Local step 48000, global step 767705: loss 89.0623
[2019-04-03 23:00:54,862] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 48000, global step 767705: learning rate 0.0001
[2019-04-03 23:00:55,099] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.05533393e-13 6.16302657e-07 6.42243854e-13 2.11690221e-09
 2.81434927e-06 1.19540701e-14 9.99996543e-01], sum to 1.0000
[2019-04-03 23:00:55,099] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3154
[2019-04-03 23:00:55,124] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.833333333333334, 53.33333333333333, 115.3333333333333, 803.6666666666666, 26.0, 25.950977804727, 0.5577301322681268, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3330600.0000, 
sim time next is 3331200.0000, 
raw observation next is [-4.666666666666667, 52.66666666666667, 114.6666666666667, 801.8333333333334, 26.0, 25.94154108597878, 0.5677092541714563, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.3333333333333333, 0.5266666666666667, 0.38222222222222235, 0.8860036832412523, 0.6666666666666666, 0.6617950904982317, 0.6892364180571521, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2536192], dtype=float32), -1.2977576]. 
=============================================
[2019-04-03 23:00:55,553] A3C_AGENT_WORKER-Thread-5 INFO:Local step 48000, global step 768038: loss 89.6098
[2019-04-03 23:00:55,554] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 48000, global step 768038: learning rate 0.0001
[2019-04-03 23:00:56,234] A3C_AGENT_WORKER-Thread-2 INFO:Local step 48000, global step 768336: loss 88.4723
[2019-04-03 23:00:56,235] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 48000, global step 768336: learning rate 0.0001
[2019-04-03 23:00:56,358] A3C_AGENT_WORKER-Thread-20 INFO:Local step 48000, global step 768387: loss 90.1773
[2019-04-03 23:00:56,359] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 48000, global step 768387: learning rate 0.0001
[2019-04-03 23:00:56,508] A3C_AGENT_WORKER-Thread-4 INFO:Local step 48000, global step 768442: loss 89.6891
[2019-04-03 23:00:56,509] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 48000, global step 768442: learning rate 0.0001
[2019-04-03 23:00:56,701] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.5005607e-13 4.3413506e-04 6.0811231e-11 2.0364038e-08 5.7576195e-05
 8.6742571e-13 9.9950826e-01], sum to 1.0000
[2019-04-03 23:00:56,702] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8209
[2019-04-03 23:00:56,753] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.833333333333334, 75.83333333333334, 98.0, 542.0, 26.0, 26.12401938578328, 0.5232572375729053, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3316200.0000, 
sim time next is 3316800.0000, 
raw observation next is [-8.666666666666668, 74.66666666666667, 101.0, 578.5, 26.0, 26.19743267856054, 0.5293931375091893, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.22253000923361033, 0.7466666666666667, 0.33666666666666667, 0.6392265193370166, 0.6666666666666666, 0.6831193898800448, 0.6764643791697297, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.72362953], dtype=float32), -0.111615166]. 
=============================================
[2019-04-03 23:00:57,589] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.7418872e-13 5.4564507e-06 2.4648797e-11 8.3212237e-09 7.8566927e-07
 1.2911019e-12 9.9999368e-01], sum to 1.0000
[2019-04-03 23:00:57,589] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2558
[2019-04-03 23:00:57,605] A3C_AGENT_WORKER-Thread-15 INFO:Local step 48000, global step 768930: loss 89.6666
[2019-04-03 23:00:57,606] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 48000, global step 768930: learning rate 0.0001
[2019-04-03 23:00:57,660] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.0, 80.0, 2.0, 94.0, 26.0, 24.15939890528772, 0.2605532078176986, 1.0, 1.0, 203136.7278448355], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3310200.0000, 
sim time next is 3310800.0000, 
raw observation next is [-11.0, 81.33333333333334, 16.0, 144.3333333333333, 26.0, 24.9187993930797, 0.3549336847232984, 1.0, 1.0, 84468.95804959713], 
processed observation next is [1.0, 0.30434782608695654, 0.15789473684210528, 0.8133333333333335, 0.05333333333333334, 0.15948434622467766, 0.6666666666666666, 0.5765666160899748, 0.6183112282410995, 1.0, 1.0, 0.4022331335695101], 
reward next is 0.5978, 
noisyNet noise sample is [array([0.22412898], dtype=float32), -0.65054315]. 
=============================================
[2019-04-03 23:00:57,811] A3C_AGENT_WORKER-Thread-10 INFO:Local step 48000, global step 769026: loss 90.6069
[2019-04-03 23:00:57,811] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 48000, global step 769026: learning rate 0.0001
[2019-04-03 23:00:57,919] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2746683e-12 1.9092562e-05 1.7270210e-10 2.0136291e-08 1.8024624e-05
 1.3129909e-11 9.9996293e-01], sum to 1.0000
[2019-04-03 23:00:57,920] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3621
[2019-04-03 23:00:58,044] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.1666666666666667, 71.83333333333333, 0.0, 0.0, 26.0, 25.16504908393695, 0.3460344226371583, 0.0, 1.0, 55226.19909334219], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3481800.0000, 
sim time next is 3482400.0000, 
raw observation next is [-0.3333333333333333, 71.66666666666667, 0.0, 0.0, 26.0, 25.15201179194441, 0.3850761754663703, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.4533702677747, 0.7166666666666667, 0.0, 0.0, 0.6666666666666666, 0.5960009826620342, 0.6283587251554568, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.02567312], dtype=float32), -1.1410558]. 
=============================================
[2019-04-03 23:00:59,162] A3C_AGENT_WORKER-Thread-16 INFO:Local step 48000, global step 769634: loss 90.7352
[2019-04-03 23:00:59,163] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 48000, global step 769634: learning rate 0.0001
[2019-04-03 23:00:59,582] A3C_AGENT_WORKER-Thread-18 INFO:Local step 48000, global step 769829: loss 92.1669
[2019-04-03 23:00:59,583] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 48000, global step 769830: learning rate 0.0001
[2019-04-03 23:00:59,786] A3C_AGENT_WORKER-Thread-9 INFO:Local step 48000, global step 769935: loss 92.9782
[2019-04-03 23:00:59,789] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 48000, global step 769935: learning rate 0.0001
[2019-04-03 23:01:00,767] A3C_AGENT_WORKER-Thread-14 INFO:Local step 48000, global step 770388: loss 92.7048
[2019-04-03 23:01:00,768] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 48000, global step 770388: learning rate 0.0001
[2019-04-03 23:01:02,713] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.1775830e-13 1.4781281e-05 2.7130674e-11 4.4051948e-08 4.0096318e-05
 9.0258600e-13 9.9994516e-01], sum to 1.0000
[2019-04-03 23:01:02,722] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3488
[2019-04-03 23:01:02,740] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.40986912965273, 0.5007560686005034, 0.0, 1.0, 77150.09131051361], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3362400.0000, 
sim time next is 3363000.0000, 
raw observation next is [-4.166666666666667, 66.0, 0.0, 0.0, 26.0, 25.43405102179884, 0.4531855893553152, 0.0, 1.0, 43324.80753115234], 
processed observation next is [1.0, 0.9565217391304348, 0.3471837488457987, 0.66, 0.0, 0.0, 0.6666666666666666, 0.61950425181657, 0.6510618631184384, 0.0, 1.0, 0.20630860729120162], 
reward next is 0.7937, 
noisyNet noise sample is [array([-0.37619373], dtype=float32), -0.32915512]. 
=============================================
[2019-04-03 23:01:02,746] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[77.90591 ]
 [77.39509 ]
 [77.016846]
 [76.76601 ]
 [76.72963 ]], R is [[78.15988159]
 [78.0109024 ]
 [77.89105988]
 [77.91210938]
 [78.13298798]].
[2019-04-03 23:01:04,601] A3C_AGENT_WORKER-Thread-19 INFO:Local step 48500, global step 772203: loss 0.4474
[2019-04-03 23:01:04,614] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 48500, global step 772203: learning rate 0.0001
[2019-04-03 23:01:05,162] A3C_AGENT_WORKER-Thread-17 INFO:Local step 48500, global step 772458: loss 0.5264
[2019-04-03 23:01:05,165] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 48500, global step 772459: learning rate 0.0001
[2019-04-03 23:01:08,304] A3C_AGENT_WORKER-Thread-7 INFO:Local step 48500, global step 774060: loss 0.6967
[2019-04-03 23:01:08,305] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 48500, global step 774060: learning rate 0.0001
[2019-04-03 23:01:09,694] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.5355489e-14 2.3299681e-06 1.3858050e-12 1.2007878e-09 2.3319042e-06
 8.7963626e-15 9.9999523e-01], sum to 1.0000
[2019-04-03 23:01:09,694] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5839
[2019-04-03 23:01:09,708] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 60.0, 104.0, 720.0, 26.0, 26.13614660064907, 0.5618829339627793, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3492000.0000, 
sim time next is 3492600.0000, 
raw observation next is [0.1666666666666667, 60.16666666666666, 105.6666666666667, 736.6666666666666, 26.0, 26.21649742089709, 0.5769524173653423, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4672206832871654, 0.6016666666666666, 0.3522222222222223, 0.8139963167587476, 0.6666666666666666, 0.6847081184080906, 0.6923174724551141, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.21884659], dtype=float32), 1.3370612]. 
=============================================
[2019-04-03 23:01:09,943] A3C_AGENT_WORKER-Thread-8 INFO:Local step 48500, global step 774905: loss 0.7594
[2019-04-03 23:01:09,946] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 48500, global step 774905: learning rate 0.0001
[2019-04-03 23:01:10,296] A3C_AGENT_WORKER-Thread-6 INFO:Local step 48500, global step 775081: loss 0.7099
[2019-04-03 23:01:10,297] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 48500, global step 775081: learning rate 0.0001
[2019-04-03 23:01:12,066] A3C_AGENT_WORKER-Thread-3 INFO:Local step 48500, global step 775953: loss 0.6338
[2019-04-03 23:01:12,068] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 48500, global step 775953: learning rate 0.0001
[2019-04-03 23:01:12,229] A3C_AGENT_WORKER-Thread-5 INFO:Local step 48500, global step 776033: loss 0.6014
[2019-04-03 23:01:12,231] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 48500, global step 776033: learning rate 0.0001
[2019-04-03 23:01:13,251] A3C_AGENT_WORKER-Thread-2 INFO:Local step 48500, global step 776593: loss 0.5726
[2019-04-03 23:01:13,252] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 48500, global step 776594: learning rate 0.0001
[2019-04-03 23:01:13,464] A3C_AGENT_WORKER-Thread-4 INFO:Local step 48500, global step 776707: loss 0.5967
[2019-04-03 23:01:13,466] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 48500, global step 776708: learning rate 0.0001
[2019-04-03 23:01:13,736] A3C_AGENT_WORKER-Thread-20 INFO:Local step 48500, global step 776844: loss 0.5524
[2019-04-03 23:01:13,738] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 48500, global step 776845: learning rate 0.0001
[2019-04-03 23:01:13,921] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.0248172e-13 7.3569267e-05 5.0251227e-11 8.2788434e-09 7.3153165e-06
 6.5998548e-12 9.9991918e-01], sum to 1.0000
[2019-04-03 23:01:13,924] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8114
[2019-04-03 23:01:13,947] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.833333333333334, 70.0, 31.33333333333333, 222.3333333333333, 26.0, 24.22234811613262, 0.1954732555771531, 0.0, 1.0, 41508.00276097714], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3570600.0000, 
sim time next is 3571200.0000, 
raw observation next is [-7.0, 70.0, 45.5, 273.0, 26.0, 24.20482719922352, 0.1987798170460425, 0.0, 1.0, 41420.24117229613], 
processed observation next is [0.0, 0.34782608695652173, 0.2686980609418283, 0.7, 0.15166666666666667, 0.30165745856353593, 0.6666666666666666, 0.5170689332686266, 0.5662599390153474, 0.0, 1.0, 0.19723924367760062], 
reward next is 0.8028, 
noisyNet noise sample is [array([-0.3891798], dtype=float32), 0.6941978]. 
=============================================
[2019-04-03 23:01:14,919] A3C_AGENT_WORKER-Thread-10 INFO:Local step 48500, global step 777467: loss 0.4854
[2019-04-03 23:01:14,921] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 48500, global step 777467: learning rate 0.0001
[2019-04-03 23:01:15,124] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.8549401e-13 8.1484386e-06 7.5099323e-11 9.9820321e-09 2.8255829e-06
 5.4618614e-13 9.9998903e-01], sum to 1.0000
[2019-04-03 23:01:15,125] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.8346
[2019-04-03 23:01:15,139] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.24829371944683, 0.3021249846666655, 0.0, 1.0, 43205.35850069222], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3733200.0000, 
sim time next is 3733800.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.19136668897056, 0.2938929443262752, 0.0, 1.0, 41988.699631319], 
processed observation next is [1.0, 0.21739130434782608, 0.3795013850415513, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5992805574142134, 0.5979643147754251, 0.0, 1.0, 0.1999461887205667], 
reward next is 0.8001, 
noisyNet noise sample is [array([1.4372137], dtype=float32), -0.73525566]. 
=============================================
[2019-04-03 23:01:15,224] A3C_AGENT_WORKER-Thread-15 INFO:Local step 48500, global step 777636: loss 0.4470
[2019-04-03 23:01:15,225] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 48500, global step 777636: learning rate 0.0001
[2019-04-03 23:01:15,929] A3C_AGENT_WORKER-Thread-16 INFO:Local step 48500, global step 778058: loss 0.3809
[2019-04-03 23:01:15,930] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 48500, global step 778059: learning rate 0.0001
[2019-04-03 23:01:16,449] A3C_AGENT_WORKER-Thread-9 INFO:Local step 48500, global step 778356: loss 0.4302
[2019-04-03 23:01:16,450] A3C_AGENT_WORKER-Thread-18 INFO:Local step 48500, global step 778357: loss 0.3961
[2019-04-03 23:01:16,451] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 48500, global step 778357: learning rate 0.0001
[2019-04-03 23:01:16,452] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 48500, global step 778358: learning rate 0.0001
[2019-04-03 23:01:18,235] A3C_AGENT_WORKER-Thread-14 INFO:Local step 48500, global step 779430: loss 0.5251
[2019-04-03 23:01:18,238] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 48500, global step 779430: learning rate 0.0001
[2019-04-03 23:01:18,638] A3C_AGENT_WORKER-Thread-19 INFO:Local step 49000, global step 779640: loss 1.5118
[2019-04-03 23:01:18,649] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 49000, global step 779641: learning rate 0.0001
[2019-04-03 23:01:19,250] A3C_AGENT_WORKER-Thread-17 INFO:Local step 49000, global step 779959: loss 1.6466
[2019-04-03 23:01:19,254] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 49000, global step 779961: learning rate 0.0001
[2019-04-03 23:01:20,899] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.28761764e-14 2.07497214e-06 2.26864317e-12 2.05463180e-09
 9.44032593e-07 1.43303216e-13 9.99997020e-01], sum to 1.0000
[2019-04-03 23:01:20,906] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3859
[2019-04-03 23:01:20,957] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 74.0, 5.0, 136.0, 26.0, 25.21880631218215, 0.2887305227992982, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3742200.0000, 
sim time next is 3742800.0000, 
raw observation next is [-4.0, 73.0, 19.0, 184.8333333333333, 26.0, 25.20947614672972, 0.3100435570615359, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.3518005540166205, 0.73, 0.06333333333333334, 0.20423572744014729, 0.6666666666666666, 0.6007896788941434, 0.6033478523538452, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5883615], dtype=float32), -0.34657577]. 
=============================================
[2019-04-03 23:01:22,509] A3C_AGENT_WORKER-Thread-7 INFO:Local step 49000, global step 781601: loss 1.4154
[2019-04-03 23:01:22,509] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 49000, global step 781601: learning rate 0.0001
[2019-04-03 23:01:24,267] A3C_AGENT_WORKER-Thread-8 INFO:Local step 49000, global step 782458: loss 1.4016
[2019-04-03 23:01:24,269] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 49000, global step 782459: learning rate 0.0001
[2019-04-03 23:01:24,860] A3C_AGENT_WORKER-Thread-6 INFO:Local step 49000, global step 782716: loss 1.4855
[2019-04-03 23:01:24,860] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 49000, global step 782716: learning rate 0.0001
[2019-04-03 23:01:25,879] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.8121570e-16 1.1126355e-07 6.4907643e-15 3.5525576e-11 5.3557581e-09
 5.9778788e-17 9.9999988e-01], sum to 1.0000
[2019-04-03 23:01:25,879] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0916
[2019-04-03 23:01:25,912] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 48.0, 99.66666666666666, 760.3333333333333, 26.0, 26.95834924470662, 0.6416084354882713, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3855000.0000, 
sim time next is 3855600.0000, 
raw observation next is [2.0, 48.0, 96.5, 749.5, 26.0, 26.33246451008962, 0.676673421763102, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.518005540166205, 0.48, 0.32166666666666666, 0.8281767955801105, 0.6666666666666666, 0.6943720425074682, 0.7255578072543672, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6519177], dtype=float32), 0.16662271]. 
=============================================
[2019-04-03 23:01:26,506] A3C_AGENT_WORKER-Thread-3 INFO:Local step 49000, global step 783536: loss 1.5346
[2019-04-03 23:01:26,509] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 49000, global step 783537: learning rate 0.0001
[2019-04-03 23:01:26,799] A3C_AGENT_WORKER-Thread-5 INFO:Local step 49000, global step 783673: loss 1.5237
[2019-04-03 23:01:26,800] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 49000, global step 783673: learning rate 0.0001
[2019-04-03 23:01:27,555] A3C_AGENT_WORKER-Thread-2 INFO:Local step 49000, global step 784031: loss 1.5416
[2019-04-03 23:01:27,564] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 49000, global step 784033: learning rate 0.0001
[2019-04-03 23:01:27,986] A3C_AGENT_WORKER-Thread-4 INFO:Local step 49000, global step 784250: loss 1.4971
[2019-04-03 23:01:27,987] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 49000, global step 784250: learning rate 0.0001
[2019-04-03 23:01:28,397] A3C_AGENT_WORKER-Thread-20 INFO:Local step 49000, global step 784438: loss 1.5603
[2019-04-03 23:01:28,398] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 49000, global step 784438: learning rate 0.0001
[2019-04-03 23:01:29,340] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.4119853e-12 3.5364428e-05 7.9231094e-11 2.4135826e-08 9.0866770e-06
 3.3244442e-12 9.9995553e-01], sum to 1.0000
[2019-04-03 23:01:29,346] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8783
[2019-04-03 23:01:29,352] A3C_AGENT_WORKER-Thread-15 INFO:Local step 49000, global step 784904: loss 1.5203
[2019-04-03 23:01:29,352] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 49000, global step 784904: learning rate 0.0001
[2019-04-03 23:01:29,434] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.666666666666667, 38.66666666666667, 0.0, 0.0, 26.0, 24.45464496909299, 0.225070913233253, 0.0, 1.0, 202465.1648414407], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4087200.0000, 
sim time next is 4087800.0000, 
raw observation next is [-4.5, 37.5, 0.0, 0.0, 26.0, 24.8399637085918, 0.2702049530623995, 1.0, 1.0, 18780.11382376275], 
processed observation next is [1.0, 0.30434782608695654, 0.3379501385041552, 0.375, 0.0, 0.0, 0.6666666666666666, 0.5699969757159833, 0.5900683176874665, 1.0, 1.0, 0.08942911344648928], 
reward next is 0.9106, 
noisyNet noise sample is [array([0.605148], dtype=float32), 0.96507084]. 
=============================================
[2019-04-03 23:01:29,494] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.1138610e-13 2.0936169e-07 4.4213344e-12 7.5619749e-10 2.2684412e-06
 7.1040166e-15 9.9999750e-01], sum to 1.0000
[2019-04-03 23:01:29,494] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8831
[2019-04-03 23:01:29,501] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.833333333333333, 40.5, 0.0, 0.0, 26.0, 25.77049737231198, 0.5302733405835406, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3952200.0000, 
sim time next is 3952800.0000, 
raw observation next is [-6.0, 41.0, 0.0, 0.0, 26.0, 25.73800563565019, 0.5130861193579743, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.296398891966759, 0.41, 0.0, 0.0, 0.6666666666666666, 0.6448338029708491, 0.6710287064526582, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9511706], dtype=float32), -0.5943788]. 
=============================================
[2019-04-03 23:01:29,663] A3C_AGENT_WORKER-Thread-10 INFO:Local step 49000, global step 785059: loss 1.6481
[2019-04-03 23:01:29,664] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 49000, global step 785059: learning rate 0.0001
[2019-04-03 23:01:30,286] A3C_AGENT_WORKER-Thread-16 INFO:Local step 49000, global step 785322: loss 1.6568
[2019-04-03 23:01:30,287] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 49000, global step 785322: learning rate 0.0001
[2019-04-03 23:01:30,931] A3C_AGENT_WORKER-Thread-9 INFO:Local step 49000, global step 785604: loss 1.7016
[2019-04-03 23:01:30,942] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 49000, global step 785605: learning rate 0.0001
[2019-04-03 23:01:31,202] A3C_AGENT_WORKER-Thread-18 INFO:Local step 49000, global step 785725: loss 1.6725
[2019-04-03 23:01:31,202] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 49000, global step 785725: learning rate 0.0001
[2019-04-03 23:01:32,266] A3C_AGENT_WORKER-Thread-14 INFO:Local step 49000, global step 786263: loss 1.7353
[2019-04-03 23:01:32,267] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 49000, global step 786263: learning rate 0.0001
[2019-04-03 23:01:34,705] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.7402047e-13 4.6098135e-06 1.8122500e-11 4.2513926e-09 2.1188839e-06
 8.2663732e-13 9.9999321e-01], sum to 1.0000
[2019-04-03 23:01:34,706] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5751
[2019-04-03 23:01:34,738] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.5, 42.5, 0.0, 0.0, 26.0, 25.34001041773259, 0.4118728788799317, 0.0, 1.0, 46943.16260011796], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4152600.0000, 
sim time next is 4153200.0000, 
raw observation next is [-1.666666666666667, 43.66666666666666, 0.0, 0.0, 26.0, 25.34298833472506, 0.4084718837511751, 0.0, 1.0, 41776.58978874918], 
processed observation next is [0.0, 0.043478260869565216, 0.4164358264081256, 0.4366666666666666, 0.0, 0.0, 0.6666666666666666, 0.6119156945604217, 0.636157294583725, 0.0, 1.0, 0.19893614185118658], 
reward next is 0.8011, 
noisyNet noise sample is [array([0.7712211], dtype=float32), -0.66224074]. 
=============================================
[2019-04-03 23:01:38,009] A3C_AGENT_WORKER-Thread-19 INFO:Local step 49500, global step 787899: loss 3.8177
[2019-04-03 23:01:38,010] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 49500, global step 787899: learning rate 0.0001
[2019-04-03 23:01:39,576] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.0274712e-14 2.9143030e-06 2.3687983e-12 1.4096310e-09 7.2950598e-07
 1.4450974e-13 9.9999642e-01], sum to 1.0000
[2019-04-03 23:01:39,577] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0467
[2019-04-03 23:01:39,623] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.95, 40.16666666666667, 38.66666666666666, 292.0, 26.0, 25.3811131549315, 0.3824923370436896, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4209000.0000, 
sim time next is 4209600.0000, 
raw observation next is [1.9, 40.33333333333334, 31.33333333333333, 227.5, 26.0, 25.29683146029777, 0.35884365939101, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.7391304347826086, 0.515235457063712, 0.40333333333333343, 0.10444444444444442, 0.2513812154696133, 0.6666666666666666, 0.6080692883581476, 0.6196145531303366, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.39219347], dtype=float32), -1.1423616]. 
=============================================
[2019-04-03 23:01:39,903] A3C_AGENT_WORKER-Thread-17 INFO:Local step 49500, global step 788502: loss 4.4350
[2019-04-03 23:01:39,905] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 49500, global step 788503: learning rate 0.0001
[2019-04-03 23:01:40,031] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.3917541e-13 6.6082888e-05 1.7887562e-10 2.5340592e-08 1.9912371e-05
 4.3704098e-12 9.9991393e-01], sum to 1.0000
[2019-04-03 23:01:40,032] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4824
[2019-04-03 23:01:40,053] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 40.0, 0.0, 0.0, 26.0, 24.91863826543484, 0.2516413924599827, 0.0, 1.0, 40597.72036876536], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4074000.0000, 
sim time next is 4074600.0000, 
raw observation next is [-5.0, 40.5, 0.0, 0.0, 26.0, 24.88385400465612, 0.2478605194600993, 0.0, 1.0, 40564.03831443515], 
processed observation next is [1.0, 0.13043478260869565, 0.32409972299168976, 0.405, 0.0, 0.0, 0.6666666666666666, 0.5736545003880099, 0.5826201731533664, 0.0, 1.0, 0.19316208721159595], 
reward next is 0.8068, 
noisyNet noise sample is [array([0.17358692], dtype=float32), 0.19187903]. 
=============================================
[2019-04-03 23:01:44,078] A3C_AGENT_WORKER-Thread-7 INFO:Local step 49500, global step 789747: loss 5.0881
[2019-04-03 23:01:44,082] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 49500, global step 789748: learning rate 0.0001
[2019-04-03 23:01:47,794] A3C_AGENT_WORKER-Thread-6 INFO:Local step 49500, global step 791061: loss 5.0520
[2019-04-03 23:01:47,813] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 49500, global step 791064: learning rate 0.0001
[2019-04-03 23:01:48,151] A3C_AGENT_WORKER-Thread-8 INFO:Local step 49500, global step 791163: loss 4.8176
[2019-04-03 23:01:48,152] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 49500, global step 791163: learning rate 0.0001
[2019-04-03 23:01:50,222] A3C_AGENT_WORKER-Thread-3 INFO:Local step 49500, global step 791801: loss 5.3907
[2019-04-03 23:01:50,227] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 49500, global step 791803: learning rate 0.0001
[2019-04-03 23:01:50,706] A3C_AGENT_WORKER-Thread-5 INFO:Local step 49500, global step 791956: loss 5.4603
[2019-04-03 23:01:50,720] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 49500, global step 791956: learning rate 0.0001
[2019-04-03 23:01:51,242] A3C_AGENT_WORKER-Thread-2 INFO:Local step 49500, global step 792120: loss 5.3103
[2019-04-03 23:01:51,243] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 49500, global step 792120: learning rate 0.0001
[2019-04-03 23:01:52,730] A3C_AGENT_WORKER-Thread-4 INFO:Local step 49500, global step 792633: loss 5.0099
[2019-04-03 23:01:52,731] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 49500, global step 792633: learning rate 0.0001
[2019-04-03 23:01:53,778] A3C_AGENT_WORKER-Thread-20 INFO:Local step 49500, global step 792998: loss 5.0957
[2019-04-03 23:01:53,779] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 49500, global step 792998: learning rate 0.0001
[2019-04-03 23:01:54,284] A3C_AGENT_WORKER-Thread-10 INFO:Local step 49500, global step 793172: loss 4.8408
[2019-04-03 23:01:54,284] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 49500, global step 793172: learning rate 0.0001
[2019-04-03 23:01:54,847] A3C_AGENT_WORKER-Thread-15 INFO:Local step 49500, global step 793355: loss 4.6206
[2019-04-03 23:01:54,850] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 49500, global step 793355: learning rate 0.0001
[2019-04-03 23:01:55,584] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.7782277e-14 1.3017036e-06 4.4677083e-12 4.5397661e-10 4.1029080e-07
 3.2964861e-13 9.9999833e-01], sum to 1.0000
[2019-04-03 23:01:55,584] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0886
[2019-04-03 23:01:55,626] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.133333333333333, 68.0, 0.0, 0.0, 26.0, 25.62566102656208, 0.5171677853270233, 0.0, 1.0, 35946.51137553871], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4427400.0000, 
sim time next is 4428000.0000, 
raw observation next is [3.0, 68.0, 0.0, 0.0, 26.0, 25.5586121000588, 0.5175941867788633, 0.0, 1.0, 69629.40974690298], 
processed observation next is [1.0, 0.2608695652173913, 0.5457063711911359, 0.68, 0.0, 0.0, 0.6666666666666666, 0.6298843416715666, 0.6725313955929545, 0.0, 1.0, 0.33156861784239516], 
reward next is 0.6684, 
noisyNet noise sample is [array([-0.28345472], dtype=float32), -1.3882432]. 
=============================================
[2019-04-03 23:01:55,659] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[87.62782 ]
 [87.736465]
 [88.068726]
 [88.40389 ]
 [88.69321 ]], R is [[87.49153137]
 [87.4454422 ]
 [87.57099152]
 [87.69528198]
 [87.81832886]].
[2019-04-03 23:01:55,851] A3C_AGENT_WORKER-Thread-16 INFO:Local step 49500, global step 793607: loss 4.4422
[2019-04-03 23:01:55,852] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 49500, global step 793608: learning rate 0.0001
[2019-04-03 23:01:58,001] A3C_AGENT_WORKER-Thread-9 INFO:Local step 49500, global step 794321: loss 4.2608
[2019-04-03 23:01:58,001] A3C_AGENT_WORKER-Thread-18 INFO:Local step 49500, global step 794321: loss 4.2322
[2019-04-03 23:01:58,029] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 49500, global step 794326: learning rate 0.0001
[2019-04-03 23:01:58,029] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 49500, global step 794326: learning rate 0.0001
[2019-04-03 23:01:58,269] A3C_AGENT_WORKER-Thread-14 INFO:Local step 49500, global step 794415: loss 3.9908
[2019-04-03 23:01:58,287] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 49500, global step 794417: learning rate 0.0001
[2019-04-03 23:02:01,283] A3C_AGENT_WORKER-Thread-19 INFO:Local step 50000, global step 795452: loss 0.0420
[2019-04-03 23:02:01,298] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 50000, global step 795452: learning rate 0.0001
[2019-04-03 23:02:01,590] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.3840934e-14 7.2863472e-06 1.2415764e-11 6.3223640e-09 3.5983830e-06
 7.0125015e-13 9.9998915e-01], sum to 1.0000
[2019-04-03 23:02:01,590] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8380
[2019-04-03 23:02:01,604] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 73.0, 0.0, 0.0, 26.0, 25.27538609171772, 0.4403739142915703, 0.0, 1.0, 48166.72317881749], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4500000.0000, 
sim time next is 4500600.0000, 
raw observation next is [-0.6666666666666666, 73.0, 0.0, 0.0, 26.0, 25.28632385744752, 0.4470388131488389, 0.0, 1.0, 44364.69650657505], 
processed observation next is [1.0, 0.08695652173913043, 0.44413665743305636, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6071936547872934, 0.6490129377162797, 0.0, 1.0, 0.21126045955511927], 
reward next is 0.7887, 
noisyNet noise sample is [array([0.37395546], dtype=float32), -0.10480987]. 
=============================================
[2019-04-03 23:02:02,428] A3C_AGENT_WORKER-Thread-17 INFO:Local step 50000, global step 795867: loss 0.0311
[2019-04-03 23:02:02,452] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 50000, global step 795867: learning rate 0.0001
[2019-04-03 23:02:03,314] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.6956156e-13 8.8269648e-05 2.3151822e-11 1.1134134e-08 2.1130640e-05
 1.5640817e-12 9.9989057e-01], sum to 1.0000
[2019-04-03 23:02:03,323] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5664
[2019-04-03 23:02:03,337] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.300000000000001, 73.0, 0.0, 0.0, 26.0, 25.73193123160505, 0.4773722078536491, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4306800.0000, 
sim time next is 4307400.0000, 
raw observation next is [5.25, 73.0, 0.0, 0.0, 26.0, 25.78729451476448, 0.473415845427186, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.60803324099723, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6489412095637066, 0.657805281809062, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.49770686], dtype=float32), 0.121465705]. 
=============================================
[2019-04-03 23:02:07,030] A3C_AGENT_WORKER-Thread-7 INFO:Local step 50000, global step 797580: loss 0.0040
[2019-04-03 23:02:07,032] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 50000, global step 797580: learning rate 0.0001
[2019-04-03 23:02:10,080] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.6959654e-14 1.7559532e-05 7.3549656e-12 6.3256461e-10 7.8744670e-06
 9.6199924e-14 9.9997461e-01], sum to 1.0000
[2019-04-03 23:02:10,080] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5352
[2019-04-03 23:02:10,108] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.916666666666666, 66.16666666666667, 0.0, 0.0, 26.0, 25.66689542217449, 0.5615948010454662, 0.0, 1.0, 148432.7184830936], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4414200.0000, 
sim time next is 4414800.0000, 
raw observation next is [5.733333333333334, 66.33333333333334, 0.0, 0.0, 26.0, 25.64846365085189, 0.5732211494081069, 0.0, 1.0, 96988.92992008332], 
processed observation next is [1.0, 0.08695652173913043, 0.6214219759926132, 0.6633333333333334, 0.0, 0.0, 0.6666666666666666, 0.6373719709043243, 0.691073716469369, 0.0, 1.0, 0.461852047238492], 
reward next is 0.5381, 
noisyNet noise sample is [array([-0.607033], dtype=float32), -1.3925529]. 
=============================================
[2019-04-03 23:02:11,225] A3C_AGENT_WORKER-Thread-6 INFO:Local step 50000, global step 798753: loss 0.0097
[2019-04-03 23:02:11,238] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 50000, global step 798754: learning rate 0.0001
[2019-04-03 23:02:12,403] A3C_AGENT_WORKER-Thread-8 INFO:Local step 50000, global step 799073: loss 0.0001
[2019-04-03 23:02:12,404] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 50000, global step 799073: learning rate 0.0001
[2019-04-03 23:02:14,342] A3C_AGENT_WORKER-Thread-3 INFO:Local step 50000, global step 799513: loss 0.0057
[2019-04-03 23:02:14,344] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 50000, global step 799514: learning rate 0.0001
[2019-04-03 23:02:14,962] A3C_AGENT_WORKER-Thread-5 INFO:Local step 50000, global step 799730: loss 0.0070
[2019-04-03 23:02:14,964] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 50000, global step 799730: learning rate 0.0001
[2019-04-03 23:02:15,736] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-04-03 23:02:15,744] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-03 23:02:15,744] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:02:15,747] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run9
[2019-04-03 23:02:15,761] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-03 23:02:15,776] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:02:15,787] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run9
[2019-04-03 23:02:15,831] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-03 23:02:15,832] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:02:15,835] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run9
[2019-04-03 23:04:28,806] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.3055385], dtype=float32), 0.35619965]
[2019-04-03 23:04:28,806] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-2.378369579666667, 49.37074475, 0.0, 0.0, 26.0, 24.76195871796969, 0.1583980650111253, 0.0, 1.0, 39170.92170773476]
[2019-04-03 23:04:28,806] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-03 23:04:28,807] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.1856827e-11 4.2681940e-04 3.6758668e-10 1.0469544e-07 5.9316611e-05
 3.4855136e-11 9.9951375e-01], sampled 0.5686966239658094
[2019-04-03 23:05:31,736] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7751 239867233.2168 1605.1272
[2019-04-03 23:06:01,529] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.7079 263401340.1679 1552.0207
[2019-04-03 23:06:08,419] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-03 23:06:09,471] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 800000, evaluation results [800000.0, 7241.707903962278, 263401340.16791633, 1552.0206828969613, 7353.775079920009, 239867233.21679944, 1605.1271615780058, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-03 23:06:10,469] A3C_AGENT_WORKER-Thread-2 INFO:Local step 50000, global step 800277: loss 0.0025
[2019-04-03 23:06:10,493] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 50000, global step 800277: learning rate 0.0001
[2019-04-03 23:06:10,981] A3C_AGENT_WORKER-Thread-4 INFO:Local step 50000, global step 800424: loss 0.0051
[2019-04-03 23:06:10,997] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 50000, global step 800424: learning rate 0.0001
[2019-04-03 23:06:13,453] A3C_AGENT_WORKER-Thread-15 INFO:Local step 50000, global step 801153: loss 0.0202
[2019-04-03 23:06:13,457] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 50000, global step 801156: learning rate 0.0001
[2019-04-03 23:06:14,028] A3C_AGENT_WORKER-Thread-20 INFO:Local step 50000, global step 801314: loss 0.0181
[2019-04-03 23:06:14,085] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 50000, global step 801322: learning rate 0.0001
[2019-04-03 23:06:14,572] A3C_AGENT_WORKER-Thread-16 INFO:Local step 50000, global step 801451: loss 0.0066
[2019-04-03 23:06:14,579] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 50000, global step 801451: learning rate 0.0001
[2019-04-03 23:06:14,647] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.2899867e-14 1.5614145e-06 2.8783427e-12 9.3734365e-10 1.9494744e-06
 4.6278041e-14 9.9999654e-01], sum to 1.0000
[2019-04-03 23:06:14,647] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3105
[2019-04-03 23:06:14,662] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 48.0, 131.0, 40.0, 26.0, 26.32808918653393, 0.5840413348791883, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4550400.0000, 
sim time next is 4551000.0000, 
raw observation next is [2.0, 48.66666666666666, 123.6666666666667, 49.33333333333334, 26.0, 26.36835988168201, 0.5880210887681303, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.518005540166205, 0.4866666666666666, 0.4122222222222223, 0.05451197053406999, 0.6666666666666666, 0.6973633234735009, 0.6960070295893767, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2262817], dtype=float32), -0.30898878]. 
=============================================
[2019-04-03 23:06:14,697] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[83.54855 ]
 [83.76922 ]
 [84.091606]
 [84.627266]
 [85.66485 ]], R is [[83.63331604]
 [83.79698181]
 [83.95901489]
 [84.11942291]
 [84.27822876]].
[2019-04-03 23:06:15,279] A3C_AGENT_WORKER-Thread-10 INFO:Local step 50000, global step 801657: loss 0.0175
[2019-04-03 23:06:15,279] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 50000, global step 801657: learning rate 0.0001
[2019-04-03 23:06:16,714] A3C_AGENT_WORKER-Thread-18 INFO:Local step 50000, global step 802091: loss 0.0188
[2019-04-03 23:06:16,720] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 50000, global step 802091: learning rate 0.0001
[2019-04-03 23:06:17,701] A3C_AGENT_WORKER-Thread-9 INFO:Local step 50000, global step 802366: loss 0.0189
[2019-04-03 23:06:17,721] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 50000, global step 802370: learning rate 0.0001
[2019-04-03 23:06:17,926] A3C_AGENT_WORKER-Thread-14 INFO:Local step 50000, global step 802458: loss 0.0105
[2019-04-03 23:06:17,926] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 50000, global step 802458: learning rate 0.0001
[2019-04-03 23:06:20,717] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.1073614e-14 1.3833006e-05 1.8584708e-12 2.7568337e-09 3.0424648e-05
 4.8753807e-14 9.9995577e-01], sum to 1.0000
[2019-04-03 23:06:20,717] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5603
[2019-04-03 23:06:20,794] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 48.66666666666666, 124.0, 0.0, 26.0, 26.22901814264835, 0.5352948055530236, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4536600.0000, 
sim time next is 4537200.0000, 
raw observation next is [2.0, 49.33333333333333, 125.5, 0.0, 26.0, 26.22298841810721, 0.5089234864868791, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.518005540166205, 0.4933333333333333, 0.41833333333333333, 0.0, 0.6666666666666666, 0.6852490348422675, 0.669641162162293, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.52547103], dtype=float32), 0.927825]. 
=============================================
[2019-04-03 23:06:22,786] A3C_AGENT_WORKER-Thread-19 INFO:Local step 50500, global step 803817: loss 0.2309
[2019-04-03 23:06:22,787] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 50500, global step 803817: learning rate 0.0001
[2019-04-03 23:06:24,447] A3C_AGENT_WORKER-Thread-17 INFO:Local step 50500, global step 804322: loss 0.2478
[2019-04-03 23:06:24,454] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 50500, global step 804322: learning rate 0.0001
[2019-04-03 23:06:29,109] A3C_AGENT_WORKER-Thread-7 INFO:Local step 50500, global step 805610: loss 0.2030
[2019-04-03 23:06:29,110] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 50500, global step 805610: learning rate 0.0001
[2019-04-03 23:06:33,438] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.0074141e-15 2.4640955e-05 5.4805672e-13 7.7455559e-10 6.0454255e-07
 6.9139543e-15 9.9997473e-01], sum to 1.0000
[2019-04-03 23:06:33,439] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4041
[2019-04-03 23:06:33,483] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.833333333333333, 75.16666666666667, 155.3333333333333, 2.0, 26.0, 25.73389478703947, 0.548144616181301, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4715400.0000, 
sim time next is 4716000.0000, 
raw observation next is [2.0, 73.0, 165.5, 3.0, 26.0, 26.07369989476401, 0.5828112624887197, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.518005540166205, 0.73, 0.5516666666666666, 0.0033149171270718232, 0.6666666666666666, 0.6728083245636677, 0.6942704208295732, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1703218], dtype=float32), -0.9397113]. 
=============================================
[2019-04-03 23:06:33,496] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[92.86988 ]
 [92.544624]
 [92.327995]
 [91.37815 ]
 [90.366295]], R is [[93.21795654]
 [93.28577423]
 [93.35292053]
 [92.48168945]
 [91.62250519]].
[2019-04-03 23:06:33,572] A3C_AGENT_WORKER-Thread-6 INFO:Local step 50500, global step 806934: loss 0.1107
[2019-04-03 23:06:33,573] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 50500, global step 806934: learning rate 0.0001
[2019-04-03 23:06:34,097] A3C_AGENT_WORKER-Thread-8 INFO:Local step 50500, global step 807154: loss 0.0368
[2019-04-03 23:06:34,099] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 50500, global step 807156: learning rate 0.0001
[2019-04-03 23:06:35,346] A3C_AGENT_WORKER-Thread-5 INFO:Local step 50500, global step 807617: loss 0.0894
[2019-04-03 23:06:35,349] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 50500, global step 807617: learning rate 0.0001
[2019-04-03 23:06:35,623] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.8990295e-14 6.8098943e-06 2.8104370e-12 5.5942104e-09 9.0928484e-07
 3.0423587e-14 9.9999225e-01], sum to 1.0000
[2019-04-03 23:06:35,624] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9462
[2019-04-03 23:06:35,627] A3C_AGENT_WORKER-Thread-3 INFO:Local step 50500, global step 807749: loss 0.0841
[2019-04-03 23:06:35,628] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 50500, global step 807749: learning rate 0.0001
[2019-04-03 23:06:35,638] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 86.0, 125.5, 0.9999999999999998, 26.0, 26.25869846049559, 0.5302994047345254, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4710000.0000, 
sim time next is 4710600.0000, 
raw observation next is [1.0, 86.0, 108.0, 0.0, 26.0, 26.18661024581965, 0.5064750667987994, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.4903047091412743, 0.86, 0.36, 0.0, 0.6666666666666666, 0.6822175204849709, 0.6688250222662665, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.2026136], dtype=float32), -0.16710587]. 
=============================================
[2019-04-03 23:06:36,765] A3C_AGENT_WORKER-Thread-2 INFO:Local step 50500, global step 808263: loss 0.0594
[2019-04-03 23:06:36,765] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 50500, global step 808263: learning rate 0.0001
[2019-04-03 23:06:36,893] A3C_AGENT_WORKER-Thread-4 INFO:Local step 50500, global step 808321: loss 0.0699
[2019-04-03 23:06:36,908] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 50500, global step 808321: learning rate 0.0001
[2019-04-03 23:06:39,160] A3C_AGENT_WORKER-Thread-15 INFO:Local step 50500, global step 809263: loss 0.0556
[2019-04-03 23:06:39,161] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 50500, global step 809263: learning rate 0.0001
[2019-04-03 23:06:39,311] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.4828192e-13 2.2277566e-04 3.2887464e-11 5.4803531e-09 6.2967782e-05
 1.7188780e-12 9.9971431e-01], sum to 1.0000
[2019-04-03 23:06:39,314] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5507
[2019-04-03 23:06:39,327] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.333333333333333, 49.16666666666667, 0.0, 0.0, 26.0, 25.35315817607364, 0.3649085788132639, 0.0, 1.0, 61552.29930764345], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5033400.0000, 
sim time next is 5034000.0000, 
raw observation next is [-1.666666666666667, 52.33333333333334, 0.0, 0.0, 26.0, 25.3359177483566, 0.3581796583970292, 0.0, 1.0, 46068.74918983461], 
processed observation next is [1.0, 0.2608695652173913, 0.4164358264081256, 0.5233333333333334, 0.0, 0.0, 0.6666666666666666, 0.6113264790297167, 0.6193932194656764, 0.0, 1.0, 0.2193749961420696], 
reward next is 0.7806, 
noisyNet noise sample is [array([0.6113677], dtype=float32), 0.07979771]. 
=============================================
[2019-04-03 23:06:39,335] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[75.166214]
 [75.08887 ]
 [74.965744]
 [75.003235]
 [75.11739 ]], R is [[75.20014954]
 [75.15504456]
 [75.01276398]
 [75.06181335]
 [75.16847992]].
[2019-04-03 23:06:39,721] A3C_AGENT_WORKER-Thread-20 INFO:Local step 50500, global step 809493: loss 0.0692
[2019-04-03 23:06:39,721] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 50500, global step 809493: learning rate 0.0001
[2019-04-03 23:06:39,785] A3C_AGENT_WORKER-Thread-16 INFO:Local step 50500, global step 809519: loss 0.0614
[2019-04-03 23:06:39,786] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 50500, global step 809519: learning rate 0.0001
[2019-04-03 23:06:40,107] A3C_AGENT_WORKER-Thread-10 INFO:Local step 50500, global step 809661: loss 0.0492
[2019-04-03 23:06:40,126] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 50500, global step 809664: learning rate 0.0001
[2019-04-03 23:06:41,432] A3C_AGENT_WORKER-Thread-9 INFO:Local step 50500, global step 810254: loss 0.0758
[2019-04-03 23:06:41,434] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 50500, global step 810254: learning rate 0.0001
[2019-04-03 23:06:41,679] A3C_AGENT_WORKER-Thread-18 INFO:Local step 50500, global step 810376: loss 0.0701
[2019-04-03 23:06:41,681] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 50500, global step 810376: learning rate 0.0001
[2019-04-03 23:06:42,172] A3C_AGENT_WORKER-Thread-14 INFO:Local step 50500, global step 810639: loss 0.0674
[2019-04-03 23:06:42,188] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 50500, global step 810642: learning rate 0.0001
[2019-04-03 23:06:43,891] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:06:43,891] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:06:43,903] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run7
[2019-04-03 23:06:45,282] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.1844580e-13 3.0846897e-04 4.6619656e-11 1.3565147e-08 1.7099289e-06
 9.5787581e-13 9.9968982e-01], sum to 1.0000
[2019-04-03 23:06:45,283] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6028
[2019-04-03 23:06:45,332] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 46.0, 46.5, 280.0, 26.0, 25.2275075369352, 0.2750752202805363, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4953600.0000, 
sim time next is 4954200.0000, 
raw observation next is [-1.833333333333333, 44.83333333333334, 62.00000000000001, 373.3333333333334, 26.0, 25.18659544263375, 0.2710288370258783, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.41181902123730385, 0.4483333333333334, 0.2066666666666667, 0.412523020257827, 0.6666666666666666, 0.5988829535528124, 0.5903429456752928, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6001912], dtype=float32), 1.3875074]. 
=============================================
[2019-04-03 23:06:45,419] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:06:45,419] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:06:45,433] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run7
[2019-04-03 23:06:47,551] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:06:47,551] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:06:47,554] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run7
[2019-04-03 23:06:50,486] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2526006e-14 4.7395902e-06 4.3163444e-13 1.7352049e-09 5.0390549e-06
 9.8440839e-15 9.9999022e-01], sum to 1.0000
[2019-04-03 23:06:50,487] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9903
[2019-04-03 23:06:50,498] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.0, 23.0, 0.0, 0.0, 26.0, 26.34907858172992, 0.6538112925658339, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4993800.0000, 
sim time next is 4994400.0000, 
raw observation next is [6.0, 23.0, 0.0, 0.0, 26.0, 26.30033462450302, 0.6372437779577039, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.6288088642659281, 0.23, 0.0, 0.0, 0.6666666666666666, 0.6916945520419183, 0.7124145926525679, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3824587], dtype=float32), -0.5836648]. 
=============================================
[2019-04-03 23:06:50,794] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:06:50,794] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:06:50,803] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run7
[2019-04-03 23:06:51,322] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:06:51,322] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:06:51,326] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run7
[2019-04-03 23:06:51,435] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.1994354e-13 2.8348778e-04 3.9254034e-11 2.4981420e-08 1.2833870e-05
 5.9255655e-13 9.9970359e-01], sum to 1.0000
[2019-04-03 23:06:51,435] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0226
[2019-04-03 23:06:51,451] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.666666666666667, 40.0, 0.0, 0.0, 26.0, 25.47212734314422, 0.4712398477248672, 0.0, 1.0, 50366.66128740685], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5012400.0000, 
sim time next is 5013000.0000, 
raw observation next is [1.5, 40.0, 0.0, 0.0, 26.0, 25.54191600076905, 0.4685609985302754, 0.0, 1.0, 18747.01873276171], 
processed observation next is [1.0, 0.0, 0.5041551246537397, 0.4, 0.0, 0.0, 0.6666666666666666, 0.6284930000640875, 0.6561869995100919, 0.0, 1.0, 0.08927151777505576], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.08914018], dtype=float32), -0.03687594]. 
=============================================
[2019-04-03 23:06:51,465] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[81.34037 ]
 [81.38683 ]
 [81.39045 ]
 [81.29663 ]
 [81.374504]], R is [[81.22107697]
 [81.16902924]
 [81.10153961]
 [81.04762268]
 [81.04330444]].
[2019-04-03 23:06:52,452] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.8232887e-14 2.4011582e-05 3.1869520e-12 2.4163098e-09 1.4053504e-05
 1.1154196e-14 9.9996197e-01], sum to 1.0000
[2019-04-03 23:06:52,452] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4283
[2019-04-03 23:06:52,461] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.0, 24.33333333333334, 0.0, 0.0, 26.0, 25.59356496445047, 0.6032565213615246, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4990800.0000, 
sim time next is 4991400.0000, 
raw observation next is [6.0, 24.0, 0.0, 0.0, 26.0, 25.98180909386741, 0.6256051982851901, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.6288088642659281, 0.24, 0.0, 0.0, 0.6666666666666666, 0.6651507578222843, 0.7085350660950634, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.45406985], dtype=float32), 0.60974693]. 
=============================================
[2019-04-03 23:06:52,469] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:06:52,470] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:06:52,476] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run7
[2019-04-03 23:06:52,668] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.1938415e-12 9.5551863e-04 3.5555842e-10 2.7927618e-08 2.1069941e-04
 1.7368927e-11 9.9883372e-01], sum to 1.0000
[2019-04-03 23:06:52,668] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4105
[2019-04-03 23:06:52,687] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.7, 93.0, 0.0, 0.0, 26.0, 21.13323870819645, -0.6038190476650007, 0.0, 1.0, 40503.27892014325], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 19800.0000, 
sim time next is 20400.0000, 
raw observation next is [7.699999999999999, 93.0, 0.0, 0.0, 26.0, 21.14602321318991, -0.5992118986222611, 0.0, 1.0, 40466.90291275085], 
processed observation next is [0.0, 0.21739130434782608, 0.6759002770083103, 0.93, 0.0, 0.0, 0.6666666666666666, 0.2621686010991591, 0.3002627004592463, 0.0, 1.0, 0.19269953767976594], 
reward next is 0.8073, 
noisyNet noise sample is [array([1.8399419], dtype=float32), 0.25829434]. 
=============================================
[2019-04-03 23:06:52,845] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:06:52,845] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:06:52,869] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run7
[2019-04-03 23:06:53,789] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:06:53,789] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:06:53,792] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run7
[2019-04-03 23:06:54,345] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:06:54,345] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:06:54,350] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run7
[2019-04-03 23:06:56,933] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:06:56,933] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:06:56,936] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run7
[2019-04-03 23:06:58,777] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:06:58,777] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:06:58,781] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run7
[2019-04-03 23:06:58,987] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:06:58,987] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:06:58,990] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run7
[2019-04-03 23:06:59,249] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:06:59,249] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:06:59,267] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run7
[2019-04-03 23:06:59,960] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:06:59,960] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:06:59,964] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run7
[2019-04-03 23:07:00,573] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:07:00,573] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:07:00,576] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run7
[2019-04-03 23:07:00,622] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:07:00,622] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:07:00,626] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run7
[2019-04-03 23:07:02,071] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.8513813e-06 1.5916324e-01 1.3958687e-05 3.4222344e-04 7.4671134e-03
 1.1989715e-05 8.3299768e-01], sum to 1.0000
[2019-04-03 23:07:02,072] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0421
[2019-04-03 23:07:02,091] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [7.2, 96.0, 0.0, 0.0, 26.0, 19.51968674178698, -0.8942079630472416, 0.0, 1.0, 56332.10978983452], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 4200.0000, 
sim time next is 4800.0000, 
raw observation next is [7.200000000000001, 96.0, 0.0, 0.0, 25.0, 19.83350193217863, -0.8627742032349285, 0.0, 1.0, 48176.74377566727], 
processed observation next is [0.0, 0.043478260869565216, 0.662049861495845, 0.96, 0.0, 0.0, 0.5833333333333334, 0.1527918276815526, 0.2124085989216905, 0.0, 1.0, 0.2294130655984156], 
reward next is 0.7706, 
noisyNet noise sample is [array([-0.08057202], dtype=float32), -0.26252255]. 
=============================================
[2019-04-03 23:07:05,590] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.30700740e-13 6.95375376e-04 1.14028093e-11 3.74549369e-09
 1.07891676e-04 1.24636694e-12 9.99196708e-01], sum to 1.0000
[2019-04-03 23:07:05,590] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0295
[2019-04-03 23:07:05,656] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.699999999999999, 93.0, 78.5, 0.0, 26.0, 24.29826064559254, 0.08947310634852557, 0.0, 1.0, 18769.53021224736], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 42000.0000, 
sim time next is 42600.0000, 
raw observation next is [7.7, 93.0, 82.0, 0.0, 26.0, 24.30536076667673, 0.09161281776638376, 0.0, 1.0, 23265.72771261453], 
processed observation next is [0.0, 0.4782608695652174, 0.6759002770083103, 0.93, 0.2733333333333333, 0.0, 0.6666666666666666, 0.5254467305563942, 0.530537605922128, 0.0, 1.0, 0.11078917958387872], 
reward next is 0.8892, 
noisyNet noise sample is [array([1.9018488], dtype=float32), -1.4849181]. 
=============================================
[2019-04-03 23:07:21,832] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.05937775e-14 3.95754796e-06 4.08468994e-13 6.48943954e-10
 7.22727293e-07 1.49562708e-14 9.99995351e-01], sum to 1.0000
[2019-04-03 23:07:21,834] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4730
[2019-04-03 23:07:21,883] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.5, 44.00000000000001, 91.0, 673.3333333333333, 26.0, 25.77137750574359, 0.4708705380643021, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 306600.0000, 
sim time next is 307200.0000, 
raw observation next is [-9.5, 44.0, 93.0, 652.1666666666666, 26.0, 25.97094155626662, 0.5032931614020371, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.1994459833795014, 0.44, 0.31, 0.7206261510128913, 0.6666666666666666, 0.664245129688885, 0.6677643871340123, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.562023], dtype=float32), 1.1200143]. 
=============================================
[2019-04-03 23:07:31,684] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.0928144e-13 4.5700348e-05 3.9945005e-12 2.1666255e-08 1.6450384e-05
 2.5631924e-13 9.9993777e-01], sum to 1.0000
[2019-04-03 23:07:31,684] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5129
[2019-04-03 23:07:31,800] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.866666666666667, 45.66666666666667, 85.0, 736.8333333333334, 26.0, 24.60844397325036, 0.3296449739428961, 1.0, 1.0, 200611.3106211847], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 304800.0000, 
sim time next is 305400.0000, 
raw observation next is [-9.683333333333334, 44.83333333333334, 87.0, 715.6666666666666, 26.0, 25.06819890024882, 0.4037670495192358, 1.0, 1.0, 102076.4181119362], 
processed observation next is [1.0, 0.5217391304347826, 0.19436749769159742, 0.4483333333333334, 0.29, 0.7907918968692449, 0.6666666666666666, 0.589016575020735, 0.634589016506412, 1.0, 1.0, 0.4860781814854105], 
reward next is 0.5139, 
noisyNet noise sample is [array([-0.8406911], dtype=float32), -0.056048833]. 
=============================================
[2019-04-03 23:07:32,902] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.1575779e-13 6.3197520e-05 7.2058345e-11 2.4912620e-08 3.4787670e-05
 7.1076452e-12 9.9990201e-01], sum to 1.0000
[2019-04-03 23:07:32,902] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8772
[2019-04-03 23:07:32,947] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.966666666666667, 71.0, 0.0, 0.0, 26.0, 24.12356134092471, 0.08529654805777094, 0.0, 1.0, 44565.62647653794], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 261600.0000, 
sim time next is 262200.0000, 
raw observation next is [-6.333333333333334, 69.0, 0.0, 0.0, 26.0, 24.14935200120879, 0.08113610824002816, 0.0, 1.0, 44684.20180433836], 
processed observation next is [1.0, 0.0, 0.28716528162511545, 0.69, 0.0, 0.0, 0.6666666666666666, 0.5124460001007325, 0.5270453694133427, 0.0, 1.0, 0.21278191335399219], 
reward next is 0.7872, 
noisyNet noise sample is [array([-1.8613106], dtype=float32), -0.06271131]. 
=============================================
[2019-04-03 23:07:46,520] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.6241407e-13 5.4750271e-04 2.6873039e-12 5.3844387e-09 1.4939170e-06
 4.7011668e-13 9.9945098e-01], sum to 1.0000
[2019-04-03 23:07:46,527] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2989
[2019-04-03 23:07:46,586] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-15.6, 90.0, 32.0, 607.5, 26.0, 25.27395324895621, 0.2104488935932594, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 378000.0000, 
sim time next is 378600.0000, 
raw observation next is [-15.41666666666667, 86.0, 34.33333333333334, 651.0, 26.0, 25.3837211194508, 0.2122575465096887, 1.0, 1.0, 18801.29606230558], 
processed observation next is [1.0, 0.391304347826087, 0.0355493998153277, 0.86, 0.11444444444444447, 0.7193370165745856, 0.6666666666666666, 0.6153100932875667, 0.5707525155032296, 1.0, 1.0, 0.08952998124907419], 
reward next is 0.9105, 
noisyNet noise sample is [array([-0.7500097], dtype=float32), -0.5154918]. 
=============================================
[2019-04-03 23:07:48,490] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.1509948e-12 4.9776316e-04 6.2889791e-11 4.2148489e-08 5.4322591e-06
 5.6461502e-12 9.9949670e-01], sum to 1.0000
[2019-04-03 23:07:48,514] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3209
[2019-04-03 23:07:48,553] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 87.0, 20.5, 22.5, 26.0, 24.98320799786399, 0.3050336883395189, 0.0, 1.0, 26381.24648879339], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 579600.0000, 
sim time next is 580200.0000, 
raw observation next is [-1.8, 87.0, 0.0, 0.0, 26.0, 24.98344802229088, 0.2928499985149806, 0.0, 1.0, 32535.5878971461], 
processed observation next is [0.0, 0.7391304347826086, 0.41274238227146814, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5819540018575733, 0.5976166661716602, 0.0, 1.0, 0.15493137093879095], 
reward next is 0.8451, 
noisyNet noise sample is [array([-0.3927514], dtype=float32), 0.5456462]. 
=============================================
[2019-04-03 23:07:52,014] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.1405604e-13 4.6429347e-04 9.2816354e-12 1.0434664e-08 1.3512619e-04
 8.4741350e-13 9.9940062e-01], sum to 1.0000
[2019-04-03 23:07:52,014] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2613
[2019-04-03 23:07:52,053] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.183333333333333, 87.5, 0.0, 0.0, 26.0, 24.50739124415498, 0.1729811987212656, 0.0, 1.0, 40767.24243747846], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 539400.0000, 
sim time next is 540000.0000, 
raw observation next is [1.1, 88.0, 0.0, 0.0, 26.0, 24.49143009768368, 0.1666664677552588, 0.0, 1.0, 40777.18425266224], 
processed observation next is [0.0, 0.2608695652173913, 0.49307479224376743, 0.88, 0.0, 0.0, 0.6666666666666666, 0.5409525081403066, 0.5555554892517529, 0.0, 1.0, 0.1941770678698202], 
reward next is 0.8058, 
noisyNet noise sample is [array([-1.7766911], dtype=float32), 0.15381446]. 
=============================================
[2019-04-03 23:07:52,075] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[90.5378 ]
 [90.65344]
 [90.78761]
 [90.94051]
 [91.09955]], R is [[90.32483673]
 [90.22746277]
 [90.13105774]
 [90.0357666 ]
 [89.94166565]].
[2019-04-03 23:07:57,031] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.1933897e-14 1.1795745e-04 1.0458829e-11 1.4509641e-09 7.4266813e-05
 3.7224146e-13 9.9980778e-01], sum to 1.0000
[2019-04-03 23:07:57,032] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8571
[2019-04-03 23:07:57,091] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.91640982639554, 0.2060605583202335, 0.0, 1.0, 42814.41591019015], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 673200.0000, 
sim time next is 673800.0000, 
raw observation next is [-2.383333333333333, 62.5, 0.0, 0.0, 26.0, 24.91004702799769, 0.2045418239621081, 0.0, 1.0, 48493.4925236972], 
processed observation next is [0.0, 0.8260869565217391, 0.3965835641735919, 0.625, 0.0, 0.0, 0.6666666666666666, 0.575837252333141, 0.5681806079873694, 0.0, 1.0, 0.23092139296998665], 
reward next is 0.7691, 
noisyNet noise sample is [array([0.03042441], dtype=float32), -0.5834982]. 
=============================================
[2019-04-03 23:07:59,518] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.5114127e-13 2.3036359e-02 1.2541723e-10 9.1631073e-08 3.7400919e-04
 2.3622035e-11 9.7658950e-01], sum to 1.0000
[2019-04-03 23:07:59,521] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2790
[2019-04-03 23:07:59,554] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.783333333333333, 84.5, 0.0, 0.0, 26.0, 24.5838585668109, 0.1865913839864839, 0.0, 1.0, 40465.21719653818], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 535800.0000, 
sim time next is 536400.0000, 
raw observation next is [1.6, 85.0, 0.0, 0.0, 26.0, 24.55529193883258, 0.181636897935662, 0.0, 1.0, 40547.8028121997], 
processed observation next is [0.0, 0.21739130434782608, 0.5069252077562327, 0.85, 0.0, 0.0, 0.6666666666666666, 0.5462743282360482, 0.5605456326452206, 0.0, 1.0, 0.19308477529618903], 
reward next is 0.8069, 
noisyNet noise sample is [array([-0.21679574], dtype=float32), 0.912327]. 
=============================================
[2019-04-03 23:08:06,418] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.9285862e-12 1.3893632e-04 4.3518782e-11 5.3365348e-09 3.1136318e-05
 1.8713189e-12 9.9982989e-01], sum to 1.0000
[2019-04-03 23:08:06,420] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6223
[2019-04-03 23:08:06,470] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 87.0, 0.0, 0.0, 26.0, 24.94343050758064, 0.2897462948861418, 0.0, 1.0, 56501.25625225455], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 581400.0000, 
sim time next is 582000.0000, 
raw observation next is [-2.1, 87.0, 0.0, 0.0, 26.0, 24.95232514778273, 0.2902165512487064, 0.0, 1.0, 45230.54340499926], 
processed observation next is [0.0, 0.7391304347826086, 0.404432132963989, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5793604289818942, 0.5967388504162354, 0.0, 1.0, 0.215383540023806], 
reward next is 0.7846, 
noisyNet noise sample is [array([0.05424295], dtype=float32), 0.27606633]. 
=============================================
[2019-04-03 23:08:06,478] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[75.08031 ]
 [74.47638 ]
 [74.10339 ]
 [74.155655]
 [73.916214]], R is [[75.77108765]
 [75.74432373]
 [75.75874329]
 [75.84630585]
 [75.96231079]].
[2019-04-03 23:08:06,914] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.6921850e-12 3.5326604e-03 1.2501597e-10 1.4229928e-08 1.2438024e-04
 4.4427278e-11 9.9634296e-01], sum to 1.0000
[2019-04-03 23:08:06,916] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2016
[2019-04-03 23:08:06,944] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 75.0, 0.0, 0.0, 26.0, 24.15177616229446, 0.0899326763414444, 0.0, 1.0, 42992.33850934677], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 616200.0000, 
sim time next is 616800.0000, 
raw observation next is [-4.1, 75.0, 0.0, 0.0, 26.0, 24.18661211152649, 0.08734234614710512, 0.0, 1.0, 43142.31765798671], 
processed observation next is [0.0, 0.13043478260869565, 0.3490304709141275, 0.75, 0.0, 0.0, 0.6666666666666666, 0.5155510092938741, 0.5291141153823684, 0.0, 1.0, 0.2054396078951748], 
reward next is 0.7946, 
noisyNet noise sample is [array([-0.22123362], dtype=float32), 0.4098777]. 
=============================================
[2019-04-03 23:08:17,073] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.8099159e-16 1.7056389e-06 2.3941722e-14 1.5620268e-10 5.5498123e-07
 5.3965591e-16 9.9999774e-01], sum to 1.0000
[2019-04-03 23:08:17,074] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6908
[2019-04-03 23:08:17,114] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.2333333333333334, 54.66666666666667, 123.1666666666667, 504.0, 26.0, 25.79087381677328, 0.3737644641209288, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 735600.0000, 
sim time next is 736200.0000, 
raw observation next is [-0.04999999999999999, 53.5, 131.0, 449.0, 26.0, 25.74847572333329, 0.3607342750364853, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.461218836565097, 0.535, 0.43666666666666665, 0.49613259668508286, 0.6666666666666666, 0.6457063102777741, 0.6202447583454951, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7438957], dtype=float32), -0.13856952]. 
=============================================
[2019-04-03 23:08:17,315] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.0107512e-15 3.0431672e-04 6.3433829e-13 7.9196960e-10 5.8926500e-05
 8.4816153e-14 9.9963677e-01], sum to 1.0000
[2019-04-03 23:08:17,315] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7905
[2019-04-03 23:08:17,369] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 76.0, 19.33333333333334, 0.0, 26.0, 25.63118979813935, 0.2780892132064727, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 720600.0000, 
sim time next is 721200.0000, 
raw observation next is [-2.3, 76.0, 24.16666666666667, 0.0, 26.0, 25.59581330937592, 0.2795717699736356, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.3988919667590028, 0.76, 0.08055555555555557, 0.0, 0.6666666666666666, 0.6329844424479933, 0.5931905899912119, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9445349], dtype=float32), 0.30195752]. 
=============================================
[2019-04-03 23:08:18,793] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.5712192e-15 1.1871313e-05 2.4292696e-13 1.1443999e-09 2.3003913e-06
 8.2061382e-15 9.9998581e-01], sum to 1.0000
[2019-04-03 23:08:18,797] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1798
[2019-04-03 23:08:18,876] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 48.33333333333334, 96.0, 719.0, 26.0, 25.31615285921696, 0.3604362208786534, 1.0, 1.0, 18680.50282469799], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 739200.0000, 
sim time next is 739800.0000, 
raw observation next is [0.5, 47.5, 89.0, 773.0, 26.0, 25.358186190011, 0.3819265241842024, 1.0, 1.0, 18680.87239688835], 
processed observation next is [1.0, 0.5652173913043478, 0.4764542936288089, 0.475, 0.2966666666666667, 0.8541436464088398, 0.6666666666666666, 0.6131821825009167, 0.6273088413947342, 1.0, 1.0, 0.08895653522327786], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.68469405], dtype=float32), -0.3201868]. 
=============================================
[2019-04-03 23:08:31,401] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.11761191e-15 1.03951854e-04 4.80191191e-13 6.76195655e-10
 1.66622704e-05 3.94649387e-14 9.99879360e-01], sum to 1.0000
[2019-04-03 23:08:31,401] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9284
[2019-04-03 23:08:31,409] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.9, 90.5, 97.0, 0.0, 26.0, 25.35464604950357, 0.280689682137806, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 905400.0000, 
sim time next is 906000.0000, 
raw observation next is [2.166666666666667, 92.66666666666666, 98.16666666666667, 0.0, 26.0, 25.35283911185922, 0.2767646046698056, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.5226223453370269, 0.9266666666666665, 0.32722222222222225, 0.0, 0.6666666666666666, 0.6127365926549349, 0.5922548682232686, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5311375], dtype=float32), 0.78062385]. 
=============================================
[2019-04-03 23:08:31,413] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[94.87336 ]
 [94.75304 ]
 [94.67463 ]
 [94.657455]
 [94.693634]], R is [[95.03884888]
 [95.08846283]
 [95.13758087]
 [95.186203  ]
 [95.23434448]].
[2019-04-03 23:08:34,545] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.2909270e-15 1.8582024e-04 7.0593726e-13 3.3185193e-10 1.1479200e-06
 4.6011101e-13 9.9981302e-01], sum to 1.0000
[2019-04-03 23:08:34,549] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7888
[2019-04-03 23:08:34,556] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.8, 100.0, 80.0, 0.0, 26.0, 24.74647268549879, 0.4440944240053212, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1261200.0000, 
sim time next is 1261800.0000, 
raw observation next is [13.8, 100.0, 77.0, 0.0, 26.0, 24.73242229842806, 0.4468599953188737, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.844875346260388, 1.0, 0.25666666666666665, 0.0, 0.6666666666666666, 0.5610351915356716, 0.6489533317729579, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.46755135], dtype=float32), 0.56830186]. 
=============================================
[2019-04-03 23:08:34,930] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.3707014e-16 7.1458329e-05 3.2202407e-14 1.2427332e-11 1.9217243e-06
 7.5905476e-16 9.9992669e-01], sum to 1.0000
[2019-04-03 23:08:34,931] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9466
[2019-04-03 23:08:34,943] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.2, 83.0, 11.5, 38.0, 26.0, 25.92285537774244, 0.626536333788751, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1065600.0000, 
sim time next is 1066200.0000, 
raw observation next is [12.2, 83.0, 15.0, 48.33333333333334, 26.0, 25.90792047415956, 0.6267831296372594, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.8005540166204987, 0.83, 0.05, 0.05340699815837938, 0.6666666666666666, 0.6589933728466301, 0.7089277098790864, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.26839828], dtype=float32), 0.9230845]. 
=============================================
[2019-04-03 23:08:36,477] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.5000121e-17 9.7351904e-07 1.6917181e-14 1.8009302e-11 3.6262162e-07
 8.5892410e-17 9.9999869e-01], sum to 1.0000
[2019-04-03 23:08:36,478] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6902
[2019-04-03 23:08:36,486] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 81.0, 98.16666666666667, 0.0, 26.0, 26.74229784720519, 0.7139430578094226, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1002000.0000, 
sim time next is 1002600.0000, 
raw observation next is [14.4, 81.0, 94.0, 0.0, 26.0, 26.78434930667433, 0.7159385818604699, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.8614958448753465, 0.81, 0.31333333333333335, 0.0, 0.6666666666666666, 0.7320291088895274, 0.73864619395349, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4633213], dtype=float32), -0.9967832]. 
=============================================
[2019-04-03 23:08:40,436] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.4189655e-16 3.9573197e-06 5.0592729e-14 1.1568314e-10 1.0324310e-06
 3.7595994e-16 9.9999499e-01], sum to 1.0000
[2019-04-03 23:08:40,438] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9223
[2019-04-03 23:08:40,446] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 26.0, 25.67285467456977, 0.5673086009426139, 0.0, 1.0, 64305.63971212378], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1035000.0000, 
sim time next is 1035600.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 26.0, 25.61783889967857, 0.5717328798246362, 0.0, 1.0, 72730.25542920305], 
processed observation next is [1.0, 1.0, 0.8614958448753465, 0.75, 0.0, 0.0, 0.6666666666666666, 0.6348199083065476, 0.6905776266082121, 0.0, 1.0, 0.3463345496628717], 
reward next is 0.6537, 
noisyNet noise sample is [array([-1.3836765], dtype=float32), 0.26473102]. 
=============================================
[2019-04-03 23:08:40,490] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [7.1716057e-14 4.3766139e-04 7.9068965e-12 1.7787481e-09 1.0310130e-05
 1.0105009e-12 9.9955195e-01], sum to 1.0000
[2019-04-03 23:08:40,494] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.5767
[2019-04-03 23:08:40,499] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [18.2, 63.66666666666666, 0.0, 0.0, 26.0, 24.79610616409742, 0.4260059373242639, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1188600.0000, 
sim time next is 1189200.0000, 
raw observation next is [18.1, 64.33333333333334, 0.0, 0.0, 26.0, 24.77768406949803, 0.4268117125596472, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.782608695652174, 0.9639889196675901, 0.6433333333333334, 0.0, 0.0, 0.6666666666666666, 0.5648070057915024, 0.6422705708532157, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6821048], dtype=float32), -0.90563554]. 
=============================================
[2019-04-03 23:08:42,974] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.8969804e-15 8.5143056e-06 4.4806915e-13 2.4892366e-10 7.2752368e-06
 2.8202221e-14 9.9998426e-01], sum to 1.0000
[2019-04-03 23:08:42,993] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8326
[2019-04-03 23:08:42,999] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.18333333333333, 56.33333333333333, 0.0, 0.0, 26.0, 26.36927632811071, 0.7386756887320688, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1104600.0000, 
sim time next is 1105200.0000, 
raw observation next is [15.0, 57.0, 0.0, 0.0, 26.0, 26.27743719891262, 0.7354236127020569, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.8781163434903049, 0.57, 0.0, 0.0, 0.6666666666666666, 0.6897864332427183, 0.745141204234019, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.7329508], dtype=float32), 0.1838019]. 
=============================================
[2019-04-03 23:08:44,983] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.9647546e-14 6.8245259e-05 1.3185852e-12 2.3265348e-10 6.2108420e-06
 3.0139782e-13 9.9992549e-01], sum to 1.0000
[2019-04-03 23:08:44,986] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.9870
[2019-04-03 23:08:44,998] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.500000000000001, 100.0, 0.0, 0.0, 26.0, 25.50870965497926, 0.5889164687676194, 0.0, 1.0, 25508.20231486748], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1285800.0000, 
sim time next is 1286400.0000, 
raw observation next is [5.5, 100.0, 0.0, 0.0, 26.0, 25.48036998763846, 0.5861039481728172, 0.0, 1.0, 46098.34200309573], 
processed observation next is [0.0, 0.9130434782608695, 0.6149584487534627, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6233641656365384, 0.6953679827242724, 0.0, 1.0, 0.21951591430045583], 
reward next is 0.7805, 
noisyNet noise sample is [array([0.7794736], dtype=float32), 0.10444554]. 
=============================================
[2019-04-03 23:08:48,582] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5885130e-16 2.3217859e-07 4.7475875e-14 6.1406692e-11 3.0356091e-08
 9.2655420e-16 9.9999976e-01], sum to 1.0000
[2019-04-03 23:08:48,584] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7858
[2019-04-03 23:08:48,599] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.6, 68.0, 85.0, 701.0, 26.0, 26.15142134647602, 0.6600371844355313, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1517400.0000, 
sim time next is 1518000.0000, 
raw observation next is [9.066666666666666, 66.33333333333334, 83.33333333333334, 694.6666666666667, 26.0, 26.38150085396911, 0.6844326538871474, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.713758079409049, 0.6633333333333334, 0.2777777777777778, 0.7675874769797423, 0.6666666666666666, 0.6984584044974259, 0.7281442179623824, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6123434], dtype=float32), 0.0717204]. 
=============================================
[2019-04-03 23:08:48,606] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[89.770485]
 [90.22678 ]
 [90.738235]
 [91.29894 ]
 [91.971146]], R is [[89.41975403]
 [89.52555847]
 [89.63030243]
 [89.73400116]
 [89.83666229]].
[2019-04-03 23:08:56,985] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.9255442e-16 2.8577265e-06 9.6706687e-14 1.6158815e-10 3.8509611e-07
 1.4830048e-15 9.9999678e-01], sum to 1.0000
[2019-04-03 23:08:56,985] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3157
[2019-04-03 23:08:57,049] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.3, 96.0, 80.5, 354.0, 26.0, 26.06658063680985, 0.566375847352841, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1508400.0000, 
sim time next is 1509000.0000, 
raw observation next is [3.483333333333333, 95.5, 83.0, 472.0000000000001, 26.0, 26.13726230372536, 0.5833339467410131, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.559095106186519, 0.955, 0.27666666666666667, 0.521546961325967, 0.6666666666666666, 0.6781051919771134, 0.6944446489136711, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.18893908], dtype=float32), -1.0109861]. 
=============================================
[2019-04-03 23:08:57,065] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[94.05963]
 [93.03723]
 [92.16386]
 [91.49922]
 [91.46773]], R is [[95.07644653]
 [95.12568665]
 [95.17443085]
 [95.22268677]
 [95.27046204]].
[2019-04-03 23:08:57,667] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.2960960e-15 5.5954189e-07 1.2928727e-13 3.7261524e-10 1.4983394e-07
 5.8672024e-15 9.9999928e-01], sum to 1.0000
[2019-04-03 23:08:57,669] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4212
[2019-04-03 23:08:57,688] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.53333333333333, 59.33333333333334, 76.66666666666666, 669.3333333333333, 26.0, 26.8029821237452, 0.7572507547796441, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1520400.0000, 
sim time next is 1521000.0000, 
raw observation next is [10.8, 57.5, 75.0, 663.0, 26.0, 26.86851309681968, 0.7757880691083292, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.7617728531855957, 0.575, 0.25, 0.7325966850828729, 0.6666666666666666, 0.7390427580683067, 0.7585960230361097, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.04487975], dtype=float32), 0.08235889]. 
=============================================
[2019-04-03 23:08:57,719] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[88.52111 ]
 [88.581696]
 [88.73302 ]
 [88.92929 ]
 [89.33649 ]], R is [[88.5890274 ]
 [88.70314026]
 [88.8161087 ]
 [88.927948  ]
 [89.03866577]].
[2019-04-03 23:08:58,775] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.2582498e-16 5.3251224e-06 5.8568398e-14 1.6600302e-10 1.1410984e-06
 3.3798770e-15 9.9999344e-01], sum to 1.0000
[2019-04-03 23:08:58,775] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1395
[2019-04-03 23:08:58,799] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.7, 60.5, 0.0, 0.0, 26.0, 26.30528748103388, 0.6687289268734498, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1535400.0000, 
sim time next is 1536000.0000, 
raw observation next is [9.600000000000001, 60.66666666666667, 0.0, 0.0, 26.0, 26.19216632932653, 0.6671123343396282, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.7285318559556788, 0.6066666666666667, 0.0, 0.0, 0.6666666666666666, 0.6826805274438774, 0.7223707781132095, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3656053], dtype=float32), 1.1822708]. 
=============================================
[2019-04-03 23:08:58,816] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[90.598495]
 [90.13884 ]
 [89.57656 ]
 [88.989685]
 [88.384476]], R is [[91.06469727]
 [91.15405273]
 [91.24251556]
 [91.33009338]
 [91.41679382]].
[2019-04-03 23:09:00,899] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.2442228e-15 1.3423515e-05 5.7034125e-13 2.1206255e-10 3.8736135e-07
 2.5311428e-14 9.9998617e-01], sum to 1.0000
[2019-04-03 23:09:00,900] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0536
[2019-04-03 23:09:00,913] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.1, 96.16666666666666, 0.0, 0.0, 26.0, 25.74938300443856, 0.5711720858092414, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1649400.0000, 
sim time next is 1650000.0000, 
raw observation next is [7.0, 96.33333333333333, 0.0, 0.0, 26.0, 25.69118402517243, 0.5731964551282139, 0.0, 1.0, 34472.01465016885], 
processed observation next is [1.0, 0.08695652173913043, 0.6565096952908588, 0.9633333333333333, 0.0, 0.0, 0.6666666666666666, 0.6409320020977024, 0.691065485042738, 0.0, 1.0, 0.16415245071508977], 
reward next is 0.8358, 
noisyNet noise sample is [array([-0.09375747], dtype=float32), -0.6336201]. 
=============================================
[2019-04-03 23:09:00,918] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[88.21701]
 [88.25134]
 [88.30314]
 [88.38302]
 [88.43061]], R is [[88.30445862]
 [88.42141724]
 [88.53720093]
 [88.65183258]
 [88.76531219]].
[2019-04-03 23:09:03,203] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.6026787e-17 1.3321549e-06 9.7141254e-15 2.8414380e-11 1.5650558e-07
 3.3532706e-16 9.9999857e-01], sum to 1.0000
[2019-04-03 23:09:03,205] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3572
[2019-04-03 23:09:03,253] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.93333333333334, 52.00000000000001, 54.66666666666667, 30.83333333333334, 26.0, 27.45160436025279, 0.8666554573352293, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1614000.0000, 
sim time next is 1614600.0000, 
raw observation next is [12.75, 52.5, 50.0, 37.0, 26.0, 27.48794766318074, 0.871700810180625, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.8157894736842106, 0.525, 0.16666666666666666, 0.04088397790055249, 0.6666666666666666, 0.7906623052650618, 0.790566936726875, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.72571266], dtype=float32), -0.05832108]. 
=============================================
[2019-04-03 23:09:03,521] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.5136208e-16 4.2395004e-05 1.2097758e-13 1.3394516e-10 9.1162798e-07
 5.5485346e-15 9.9995661e-01], sum to 1.0000
[2019-04-03 23:09:03,522] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8713
[2019-04-03 23:09:03,553] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.1, 100.0, 55.66666666666666, 0.0, 26.0, 26.09507307117677, 0.532029261179989, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1504200.0000, 
sim time next is 1504800.0000, 
raw observation next is [2.2, 100.0, 60.0, 0.0, 26.0, 26.12975908724655, 0.5233572383617673, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.5235457063711911, 1.0, 0.2, 0.0, 0.6666666666666666, 0.6774799239372126, 0.6744524127872559, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7738346], dtype=float32), -0.62596923]. 
=============================================
[2019-04-03 23:09:09,278] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.49957697e-14 1.06996519e-03 1.04305635e-11 2.93870372e-09
 3.11455588e-05 8.69702368e-13 9.98898864e-01], sum to 1.0000
[2019-04-03 23:09:09,281] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0544
[2019-04-03 23:09:09,298] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.1, 96.16666666666666, 0.0, 0.0, 26.0, 25.74924314706273, 0.5711321087598112, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1649400.0000, 
sim time next is 1650000.0000, 
raw observation next is [7.0, 96.33333333333333, 0.0, 0.0, 26.0, 25.69102464310434, 0.573167748994085, 0.0, 1.0, 34581.69499191479], 
processed observation next is [1.0, 0.08695652173913043, 0.6565096952908588, 0.9633333333333333, 0.0, 0.0, 0.6666666666666666, 0.6409187202586949, 0.6910559163313618, 0.0, 1.0, 0.16467473805673707], 
reward next is 0.8353, 
noisyNet noise sample is [array([-1.5727128], dtype=float32), 0.22333063]. 
=============================================
[2019-04-03 23:09:09,312] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[88.00238 ]
 [88.04634 ]
 [88.090706]
 [88.16172 ]
 [88.18182 ]], R is [[88.07731628]
 [88.19654083]
 [88.3145752 ]
 [88.431427  ]
 [88.54711151]].
[2019-04-03 23:09:14,191] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.4602632e-11 1.6920592e-03 3.1740463e-10 4.8263335e-08 9.7086333e-05
 5.2146027e-11 9.9821079e-01], sum to 1.0000
[2019-04-03 23:09:14,191] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8101
[2019-04-03 23:09:14,268] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.7, 78.0, 67.33333333333333, 31.33333333333333, 26.0, 24.63826088208528, 0.2261688013602329, 0.0, 1.0, 84617.47002491], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1846200.0000, 
sim time next is 1846800.0000, 
raw observation next is [-6.7, 78.0, 87.5, 47.0, 26.0, 24.97961537196057, 0.2558487377079411, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.2770083102493075, 0.78, 0.2916666666666667, 0.051933701657458566, 0.6666666666666666, 0.5816346143300475, 0.5852829125693136, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.9545554], dtype=float32), -1.2632829]. 
=============================================
[2019-04-03 23:09:16,800] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [8.9961997e-15 3.6080666e-05 9.1854137e-12 4.1606443e-09 7.6271144e-06
 1.6943143e-13 9.9995625e-01], sum to 1.0000
[2019-04-03 23:09:16,802] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2474
[2019-04-03 23:09:16,815] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.1666666666666667, 94.0, 0.0, 0.0, 26.0, 25.35900573054449, 0.4797868888026264, 0.0, 1.0, 53099.31012886222], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1725600.0000, 
sim time next is 1726200.0000, 
raw observation next is [0.25, 93.5, 0.0, 0.0, 26.0, 25.35630549845631, 0.4784395861014238, 0.0, 1.0, 47229.61751061396], 
processed observation next is [1.0, 1.0, 0.46952908587257625, 0.935, 0.0, 0.0, 0.6666666666666666, 0.6130254582046927, 0.6594798620338079, 0.0, 1.0, 0.22490294052673315], 
reward next is 0.7751, 
noisyNet noise sample is [array([-0.33329478], dtype=float32), -1.258157]. 
=============================================
[2019-04-03 23:09:22,227] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.6110782e-13 2.0404837e-04 1.7366683e-11 3.0156851e-08 1.4509869e-05
 1.5457977e-11 9.9978143e-01], sum to 1.0000
[2019-04-03 23:09:22,228] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0812
[2019-04-03 23:09:22,274] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 83.0, 0.0, 0.0, 26.0, 25.01321534959631, 0.3189677347308487, 0.0, 1.0, 121082.536742704], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1797600.0000, 
sim time next is 1798200.0000, 
raw observation next is [-4.5, 83.0, 0.0, 0.0, 26.0, 25.0108139664821, 0.3319706704878382, 0.0, 1.0, 79089.3190364689], 
processed observation next is [0.0, 0.8260869565217391, 0.3379501385041552, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5842344972068417, 0.6106568901626127, 0.0, 1.0, 0.37661580493556623], 
reward next is 0.6234, 
noisyNet noise sample is [array([-0.83399194], dtype=float32), 0.047302313]. 
=============================================
[2019-04-03 23:09:34,163] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.6951984e-15 8.6071515e-05 5.2781553e-13 1.1692932e-09 2.5246715e-07
 6.2516995e-14 9.9991369e-01], sum to 1.0000
[2019-04-03 23:09:34,163] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6649
[2019-04-03 23:09:34,217] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 77.66666666666667, 154.6666666666667, 0.0, 26.0, 25.35077376345195, 0.299828147674164, 1.0, 1.0, 43344.166056068], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2032800.0000, 
sim time next is 2033400.0000, 
raw observation next is [-4.5, 78.33333333333334, 153.3333333333333, 0.0, 26.0, 25.32278711233179, 0.3100868135190956, 1.0, 1.0, 35237.46368217998], 
processed observation next is [1.0, 0.5217391304347826, 0.3379501385041552, 0.7833333333333334, 0.511111111111111, 0.0, 0.6666666666666666, 0.6102322593609824, 0.6033622711730319, 1.0, 1.0, 0.16779744610561895], 
reward next is 0.8322, 
noisyNet noise sample is [array([-1.4714786], dtype=float32), 0.21470274]. 
=============================================
[2019-04-03 23:09:37,082] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4432845e-15 6.7474117e-05 5.0848648e-13 7.4209455e-10 1.9300469e-07
 5.4441078e-14 9.9993229e-01], sum to 1.0000
[2019-04-03 23:09:37,085] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1251
[2019-04-03 23:09:37,145] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.199999999999999, 87.0, 0.0, 0.0, 26.0, 24.86899451438714, 0.237656901847128, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2014800.0000, 
sim time next is 2015400.0000, 
raw observation next is [-6.2, 87.0, 10.33333333333333, 0.0, 26.0, 25.16551242035782, 0.257418649388705, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.2908587257617729, 0.87, 0.03444444444444444, 0.0, 0.6666666666666666, 0.5971260350298184, 0.5858062164629017, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.1450614], dtype=float32), 0.93060696]. 
=============================================
[2019-04-03 23:09:50,502] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5412393e-14 7.8293306e-06 2.2016703e-12 8.4550827e-10 1.4975672e-07
 1.2314161e-13 9.9999201e-01], sum to 1.0000
[2019-04-03 23:09:50,502] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1572
[2019-04-03 23:09:50,561] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.533333333333333, 55.33333333333333, 0.0, 0.0, 26.0, 24.91133383435227, 0.3499811322821295, 0.0, 1.0, 87517.88839624675], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2317200.0000, 
sim time next is 2317800.0000, 
raw observation next is [-1.616666666666667, 55.66666666666667, 0.0, 0.0, 26.0, 24.94503431663741, 0.3609759026871309, 1.0, 1.0, 42373.75703256112], 
processed observation next is [1.0, 0.8260869565217391, 0.4178208679593721, 0.5566666666666668, 0.0, 0.0, 0.6666666666666666, 0.5787528597197843, 0.6203253008957103, 1.0, 1.0, 0.2017797953931482], 
reward next is 0.7982, 
noisyNet noise sample is [array([-0.2572046], dtype=float32), -0.7304636]. 
=============================================
[2019-04-03 23:09:51,742] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.01529104e-12 5.14898449e-04 2.92709496e-10 1.25314834e-08
 1.04307555e-05 3.64950821e-12 9.99474585e-01], sum to 1.0000
[2019-04-03 23:09:51,742] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0872
[2019-04-03 23:09:51,761] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.700000000000001, 78.0, 0.0, 0.0, 26.0, 24.51320157304164, 0.1553641359398676, 0.0, 1.0, 42295.07083054254], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2171400.0000, 
sim time next is 2172000.0000, 
raw observation next is [-6.700000000000001, 78.0, 0.0, 0.0, 26.0, 24.49793079787746, 0.1320572564136527, 0.0, 1.0, 42595.0299307151], 
processed observation next is [1.0, 0.13043478260869565, 0.2770083102493075, 0.78, 0.0, 0.0, 0.6666666666666666, 0.5414942331564548, 0.5440190854712176, 0.0, 1.0, 0.2028334758605481], 
reward next is 0.7972, 
noisyNet noise sample is [array([1.3866748], dtype=float32), 0.91472054]. 
=============================================
[2019-04-03 23:09:51,767] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[75.87969 ]
 [75.8242  ]
 [75.783966]
 [75.7298  ]
 [75.70782 ]], R is [[75.91339874]
 [75.95285797]
 [75.99160767]
 [76.02960205]
 [76.06693268]].
[2019-04-03 23:09:52,498] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.7934801e-14 1.0673933e-05 3.4382391e-12 2.3575537e-09 1.4052829e-06
 1.6675010e-13 9.9998784e-01], sum to 1.0000
[2019-04-03 23:09:52,498] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0586
[2019-04-03 23:09:52,530] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.516666666666667, 83.0, 0.0, 0.0, 26.0, 25.22880137687479, 0.4028110642084533, 0.0, 1.0, 42264.40818506441], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2152200.0000, 
sim time next is 2152800.0000, 
raw observation next is [-6.7, 83.0, 0.0, 0.0, 26.0, 25.21737653259062, 0.3976263853456311, 0.0, 1.0, 42206.82806090523], 
processed observation next is [1.0, 0.9565217391304348, 0.2770083102493075, 0.83, 0.0, 0.0, 0.6666666666666666, 0.6014480443825517, 0.6325421284485437, 0.0, 1.0, 0.20098489552812016], 
reward next is 0.7990, 
noisyNet noise sample is [array([-0.77683353], dtype=float32), 0.024701009]. 
=============================================
[2019-04-03 23:09:59,725] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.8277943e-15 7.2343477e-07 4.4685620e-13 4.8864296e-10 6.7831067e-08
 3.6185137e-15 9.9999917e-01], sum to 1.0000
[2019-04-03 23:09:59,725] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6036
[2019-04-03 23:09:59,782] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.45, 55.0, 0.0, 0.0, 26.0, 24.93775006788599, 0.3296113593192943, 0.0, 1.0, 117124.5906731118], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2316600.0000, 
sim time next is 2317200.0000, 
raw observation next is [-1.533333333333333, 55.33333333333333, 0.0, 0.0, 26.0, 24.92057153745486, 0.344458755279279, 0.0, 1.0, 83069.90827381276], 
processed observation next is [1.0, 0.8260869565217391, 0.42012927054478305, 0.5533333333333332, 0.0, 0.0, 0.6666666666666666, 0.5767142947879051, 0.614819585093093, 0.0, 1.0, 0.3955709917800608], 
reward next is 0.6044, 
noisyNet noise sample is [array([-0.9188775], dtype=float32), 0.015760424]. 
=============================================
[2019-04-03 23:10:02,537] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.2452372e-10 4.6318013e-04 1.7706874e-09 8.9252296e-08 3.4988170e-05
 2.4858912e-10 9.9950171e-01], sum to 1.0000
[2019-04-03 23:10:02,537] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.6120
[2019-04-03 23:10:02,565] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.4, 61.0, 0.0, 0.0, 26.0, 23.58951630925006, -0.06799011341064214, 0.0, 1.0, 44324.86639148896], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2433600.0000, 
sim time next is 2434200.0000, 
raw observation next is [-8.4, 61.0, 0.0, 0.0, 26.0, 23.54623345815827, -0.07687433091263658, 0.0, 1.0, 44370.879113523], 
processed observation next is [0.0, 0.17391304347826086, 0.2299168975069252, 0.61, 0.0, 0.0, 0.6666666666666666, 0.4621861215131891, 0.47437522302912116, 0.0, 1.0, 0.21128990054058572], 
reward next is 0.7887, 
noisyNet noise sample is [array([0.05656222], dtype=float32), -1.6735169]. 
=============================================
[2019-04-03 23:10:03,807] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.9244747e-13 9.7162025e-05 6.1958941e-12 4.4327022e-09 1.7199294e-06
 7.5396183e-13 9.9990118e-01], sum to 1.0000
[2019-04-03 23:10:03,807] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5744
[2019-04-03 23:10:03,849] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 57.0, 0.0, 0.0, 26.0, 24.78435628051816, 0.1293331693759008, 0.0, 1.0, 38544.0978509351], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2527200.0000, 
sim time next is 2527800.0000, 
raw observation next is [-2.383333333333333, 56.5, 0.0, 0.0, 26.0, 24.73787768261239, 0.1253927402610648, 0.0, 1.0, 38628.23626145841], 
processed observation next is [1.0, 0.2608695652173913, 0.3965835641735919, 0.565, 0.0, 0.0, 0.6666666666666666, 0.5614898068843658, 0.5417975800870216, 0.0, 1.0, 0.183943982197421], 
reward next is 0.8161, 
noisyNet noise sample is [array([0.03826522], dtype=float32), -1.4455192]. 
=============================================
[2019-04-03 23:10:24,476] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.2982151e-14 1.6393828e-05 2.9616960e-12 2.1349915e-09 2.1199371e-07
 1.7232967e-12 9.9998331e-01], sum to 1.0000
[2019-04-03 23:10:24,476] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4773
[2019-04-03 23:10:24,508] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 39.33333333333334, 0.0, 0.0, 26.0, 25.0323926041249, 0.1995700724668077, 0.0, 1.0, 39096.5384412799], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2510400.0000, 
sim time next is 2511000.0000, 
raw observation next is [-1.7, 39.0, 0.0, 0.0, 26.0, 24.98711237263582, 0.2024365524787961, 0.0, 1.0, 39055.01890209065], 
processed observation next is [1.0, 0.043478260869565216, 0.4155124653739613, 0.39, 0.0, 0.0, 0.6666666666666666, 0.5822593643863184, 0.5674788508262654, 0.0, 1.0, 0.18597628048614595], 
reward next is 0.8140, 
noisyNet noise sample is [array([-0.00118614], dtype=float32), 0.86837524]. 
=============================================
[2019-04-03 23:10:24,549] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[82.45272 ]
 [82.14175 ]
 [81.873505]
 [81.72095 ]
 [81.41363 ]], R is [[82.42462158]
 [82.41420746]
 [82.40376282]
 [82.39334869]
 [82.38306427]].
[2019-04-03 23:10:37,688] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.4179799e-15 8.7100017e-07 1.0677930e-13 1.9914131e-10 1.0226730e-08
 2.9271773e-15 9.9999917e-01], sum to 1.0000
[2019-04-03 23:10:37,688] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4754
[2019-04-03 23:10:37,741] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.6, 25.0, 83.0, 38.0, 26.0, 25.90998231325217, 0.4271371893683777, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2822400.0000, 
sim time next is 2823000.0000, 
raw observation next is [6.5, 25.5, 79.0, 50.66666666666667, 26.0, 26.00288250930605, 0.3322775688672677, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.6426592797783934, 0.255, 0.2633333333333333, 0.0559852670349908, 0.6666666666666666, 0.6669068757755042, 0.6107591896224226, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0962576], dtype=float32), -0.3639084]. 
=============================================
[2019-04-03 23:10:37,743] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[89.39951 ]
 [89.310844]
 [89.07154 ]
 [88.7725  ]
 [88.60931 ]], R is [[89.51605225]
 [89.62089539]
 [89.72468567]
 [89.82743835]
 [89.92916107]].
[2019-04-03 23:10:49,470] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.39853305e-14 2.80882978e-06 3.61516568e-12 2.96426750e-09
 2.05002561e-06 9.21034082e-14 9.99995112e-01], sum to 1.0000
[2019-04-03 23:10:49,474] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4886
[2019-04-03 23:10:49,493] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.833333333333333, 83.83333333333334, 0.0, 0.0, 26.0, 25.13413772962639, 0.3920255145849096, 0.0, 1.0, 43772.29317371535], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2933400.0000, 
sim time next is 2934000.0000, 
raw observation next is [-2.0, 85.0, 0.0, 0.0, 26.0, 25.08402640355472, 0.3830479892442958, 0.0, 1.0, 43663.39487217567], 
processed observation next is [1.0, 1.0, 0.40720221606648205, 0.85, 0.0, 0.0, 0.6666666666666666, 0.59033553362956, 0.627682663081432, 0.0, 1.0, 0.20792092796274128], 
reward next is 0.7921, 
noisyNet noise sample is [array([-0.09264008], dtype=float32), 0.014304994]. 
=============================================
[2019-04-03 23:10:49,532] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[77.7096  ]
 [77.647255]
 [77.83007 ]
 [77.77859 ]
 [77.773506]], R is [[77.79840088]
 [77.81197357]
 [77.82437897]
 [77.83289337]
 [77.83372498]].
[2019-04-03 23:11:04,035] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.5801934e-12 1.4574475e-04 8.5864919e-11 5.6388846e-08 3.2272778e-06
 5.9286582e-12 9.9985099e-01], sum to 1.0000
[2019-04-03 23:11:04,035] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2997
[2019-04-03 23:11:04,048] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 39.5, 84.0, 673.0, 26.0, 25.12339439440625, 0.3683640035908022, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3079800.0000, 
sim time next is 3080400.0000, 
raw observation next is [0.6666666666666666, 39.66666666666666, 79.5, 641.8333333333334, 26.0, 25.14815969568285, 0.3740281348459679, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.4810710987996307, 0.39666666666666656, 0.265, 0.7092081031307551, 0.6666666666666666, 0.5956799746402375, 0.624676044948656, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4991008], dtype=float32), 0.7094673]. 
=============================================
[2019-04-03 23:11:10,059] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.1485127e-12 7.5582589e-04 4.1899476e-10 2.6823363e-08 3.2242604e-06
 2.0219635e-11 9.9924099e-01], sum to 1.0000
[2019-04-03 23:11:10,060] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1191
[2019-04-03 23:11:10,100] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.833333333333334, 70.0, 0.0, 0.0, 26.0, 24.66741286061226, 0.2066539518357627, 0.0, 1.0, 37899.50282343289], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3027000.0000, 
sim time next is 3027600.0000, 
raw observation next is [-5.0, 71.0, 0.0, 0.0, 26.0, 24.625760712423, 0.1975066458077182, 0.0, 1.0, 37953.70561030667], 
processed observation next is [0.0, 0.043478260869565216, 0.32409972299168976, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5521467260352498, 0.5658355486025727, 0.0, 1.0, 0.18073193147765082], 
reward next is 0.8193, 
noisyNet noise sample is [array([-1.100961], dtype=float32), -0.8123932]. 
=============================================
[2019-04-03 23:11:18,554] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.74836511e-16 3.14872909e-06 3.57067925e-14 1.06280595e-10
 3.78344716e-08 4.47091518e-16 9.99996901e-01], sum to 1.0000
[2019-04-03 23:11:18,575] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3869
[2019-04-03 23:11:18,614] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.333333333333333, 71.0, 87.66666666666667, 699.8333333333334, 26.0, 26.29851459294293, 0.7527126705579424, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3252000.0000, 
sim time next is 3252600.0000, 
raw observation next is [-2.5, 71.0, 85.0, 686.0, 26.0, 25.60763125288084, 0.7154652767558211, 1.0, 1.0, 115882.2477001386], 
processed observation next is [1.0, 0.6521739130434783, 0.39335180055401664, 0.71, 0.2833333333333333, 0.7580110497237569, 0.6666666666666666, 0.6339692710734033, 0.7384884255852736, 1.0, 1.0, 0.5518202271435172], 
reward next is 0.4482, 
noisyNet noise sample is [array([-0.5243773], dtype=float32), 1.8369762]. 
=============================================
[2019-04-03 23:11:23,044] A3C_AGENT_WORKER-Thread-8 INFO:Evaluating...
[2019-04-03 23:11:23,052] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-03 23:11:23,053] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-03 23:11:23,054] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-03 23:11:23,067] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:11:23,067] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:11:23,069] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:11:23,073] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run10
[2019-04-03 23:11:23,102] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run10
[2019-04-03 23:11:23,118] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run10
[2019-04-03 23:12:32,426] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.2918098], dtype=float32), 0.39459732]
[2019-04-03 23:12:32,427] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [4.333333333333334, 89.66666666666666, 0.0, 0.0, 26.0, 25.22082895262792, 0.3231818473813149, 0.0, 1.0, 53164.25140344579]
[2019-04-03 23:12:32,427] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-03 23:12:32,428] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [4.6825625e-14 7.9003203e-04 6.3813434e-12 2.0990554e-09 4.0807143e-07
 9.3509087e-13 9.9920952e-01], sampled 0.7171351113316079
[2019-04-03 23:13:21,102] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.2918098], dtype=float32), 0.39459732]
[2019-04-03 23:13:21,103] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-9.396099923, 91.13399247666668, 0.0, 0.0, 26.0, 23.25277380507327, -0.1051298497888525, 0.0, 1.0, 43901.06292052974]
[2019-04-03 23:13:21,103] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-03 23:13:21,104] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [6.1664246e-12 1.6772238e-03 3.9670434e-10 5.8159920e-08 4.3581795e-06
 7.9209465e-11 9.9831831e-01], sampled 0.38119253534419495
[2019-04-03 23:14:08,593] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.2918098], dtype=float32), 0.39459732]
[2019-04-03 23:14:08,593] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-9.265723444333334, 85.06165843833334, 0.0, 0.0, 26.0, 25.43348440390681, 0.5541372701021833, 0.0, 1.0, 18761.74037514831]
[2019-04-03 23:14:08,593] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-03 23:14:08,594] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [6.1706689e-13 4.9995208e-05 1.6538507e-11 1.0313743e-08 5.2294882e-07
 1.3415374e-12 9.9994957e-01], sampled 0.19874441873031878
[2019-04-03 23:14:28,010] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.1428 240000002.2134 1605.2943
[2019-04-03 23:14:55,235] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2784 263491535.8389 1557.0207
[2019-04-03 23:14:58,623] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-03 23:14:59,646] A3C_AGENT_WORKER-Thread-8 INFO:Global step: 900000, evaluation results [900000.0, 7241.2784007671635, 263491535.8388988, 1557.0206657087865, 7353.142846602716, 240000002.21342754, 1605.294310153203, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-03 23:15:01,397] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [6.8630831e-17 1.1969408e-06 1.2358988e-15 2.0042032e-11 4.2182675e-09
 6.9137450e-17 9.9999881e-01], sum to 1.0000
[2019-04-03 23:15:01,408] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.2409
[2019-04-03 23:15:01,414] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.0, 100.0, 112.3333333333333, 811.6666666666666, 26.0, 27.31596994092423, 0.8567994168890279, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3157800.0000, 
sim time next is 3158400.0000, 
raw observation next is [7.0, 100.0, 112.1666666666667, 808.8333333333334, 26.0, 27.33095431624253, 0.8690743487260023, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.6565096952908588, 1.0, 0.373888888888889, 0.8937384898710866, 0.6666666666666666, 0.7775795263535441, 0.7896914495753341, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1722959], dtype=float32), 1.9715382]. 
=============================================
[2019-04-03 23:15:03,148] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.6367616e-15 1.0106804e-04 3.9052390e-13 3.4262060e-10 5.3800775e-08
 3.0535500e-14 9.9989891e-01], sum to 1.0000
[2019-04-03 23:15:03,148] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2042
[2019-04-03 23:15:03,230] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 92.0, 98.33333333333334, 605.8333333333334, 26.0, 26.09288116623654, 0.6313919542304365, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3230400.0000, 
sim time next is 3231000.0000, 
raw observation next is [-3.0, 92.0, 101.0, 653.0, 26.0, 26.17355036322057, 0.6515477388942067, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.3795013850415513, 0.92, 0.33666666666666667, 0.7215469613259669, 0.6666666666666666, 0.6811291969350476, 0.7171825796314022, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2093325], dtype=float32), 1.1624045]. 
=============================================
[2019-04-03 23:15:03,252] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[91.68071 ]
 [91.74765 ]
 [91.84762 ]
 [91.99315 ]
 [92.133095]], R is [[91.75590515]
 [91.83834839]
 [91.91996765]
 [92.00077057]
 [92.08076477]].
[2019-04-03 23:15:06,492] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.0779619e-14 4.9602382e-05 3.2762768e-12 1.5095688e-09 3.5582130e-07
 2.6595420e-13 9.9995005e-01], sum to 1.0000
[2019-04-03 23:15:06,501] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1600
[2019-04-03 23:15:06,532] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 48.0, 104.0, 711.0, 26.0, 26.53793255410312, 0.6002600031021287, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3405600.0000, 
sim time next is 3406200.0000, 
raw observation next is [2.166666666666667, 48.16666666666666, 105.6666666666667, 728.6666666666666, 26.0, 26.57627677055248, 0.6127604147486424, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.5226223453370269, 0.4816666666666666, 0.3522222222222223, 0.8051565377532228, 0.6666666666666666, 0.7146897308793733, 0.7042534715828808, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.34901965], dtype=float32), -0.80123156]. 
=============================================
[2019-04-03 23:15:12,372] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.7696972e-12 6.7114586e-04 2.6778693e-10 3.0277203e-08 7.3004048e-07
 6.6113233e-11 9.9932814e-01], sum to 1.0000
[2019-04-03 23:15:12,372] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5978
[2019-04-03 23:15:12,388] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.816666666666666, 76.16666666666667, 0.0, 0.0, 26.0, 24.49023517303901, 0.2247646916343202, 0.0, 1.0, 43761.61456668792], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3300600.0000, 
sim time next is 3301200.0000, 
raw observation next is [-10.0, 76.0, 0.0, 0.0, 26.0, 24.48197698787257, 0.2210465167498432, 0.0, 1.0, 43706.19970792518], 
processed observation next is [1.0, 0.21739130434782608, 0.18559556786703602, 0.76, 0.0, 0.0, 0.6666666666666666, 0.5401647489893809, 0.5736821722499478, 0.0, 1.0, 0.20812476051392945], 
reward next is 0.7919, 
noisyNet noise sample is [array([0.19884159], dtype=float32), 1.4981815]. 
=============================================
[2019-04-03 23:15:14,359] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.1876085e-15 1.7938653e-05 3.6847530e-12 4.0378714e-09 2.9406471e-07
 1.7929226e-13 9.9998176e-01], sum to 1.0000
[2019-04-03 23:15:14,359] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3860
[2019-04-03 23:15:14,370] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6666666666666666, 76.0, 0.0, 0.0, 26.0, 25.87507758351995, 0.5953962094827749, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3534000.0000, 
sim time next is 3534600.0000, 
raw observation next is [-0.8333333333333334, 77.0, 0.0, 0.0, 26.0, 25.85824447545426, 0.5809791394832112, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.43951985226223456, 0.77, 0.0, 0.0, 0.6666666666666666, 0.654853706287855, 0.6936597131610704, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1393753], dtype=float32), 0.23358393]. 
=============================================
[2019-04-03 23:15:19,665] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [7.2067943e-15 6.9443180e-05 2.4585952e-12 9.2432700e-10 1.1824273e-07
 8.9068320e-14 9.9993038e-01], sum to 1.0000
[2019-04-03 23:15:19,681] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.6598
[2019-04-03 23:15:19,700] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 26.0, 25.3848097976675, 0.4631099172252724, 0.0, 1.0, 63628.42026677614], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3460800.0000, 
sim time next is 3461400.0000, 
raw observation next is [1.0, 79.0, 0.0, 0.0, 26.0, 25.52053007970018, 0.4626791960668566, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 0.79, 0.0, 0.0, 0.6666666666666666, 0.626710839975015, 0.6542263986889522, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.27681634], dtype=float32), -0.07980924]. 
=============================================
[2019-04-03 23:15:19,955] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.3868899e-14 1.3133292e-06 1.3995214e-12 3.1200978e-10 5.0817732e-08
 1.6322833e-14 9.9999869e-01], sum to 1.0000
[2019-04-03 23:15:19,955] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6949
[2019-04-03 23:15:19,977] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.6666666666666667, 65.33333333333334, 0.0, 0.0, 26.0, 25.18439232298611, 0.4494752737074741, 0.0, 1.0, 58128.39748348399], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3526800.0000, 
sim time next is 3527400.0000, 
raw observation next is [0.3333333333333333, 68.66666666666666, 0.0, 0.0, 26.0, 25.10326200630285, 0.4463510953664027, 0.0, 1.0, 90325.98757637743], 
processed observation next is [1.0, 0.8260869565217391, 0.4718374884579871, 0.6866666666666665, 0.0, 0.0, 0.6666666666666666, 0.5919385005252374, 0.6487836984554676, 0.0, 1.0, 0.43012375036370204], 
reward next is 0.5699, 
noisyNet noise sample is [array([-0.82197875], dtype=float32), -0.01602059]. 
=============================================
[2019-04-03 23:15:27,279] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.9643745e-16 6.5155164e-06 2.2100773e-14 3.3917036e-10 2.7450437e-08
 7.9900581e-16 9.9999344e-01], sum to 1.0000
[2019-04-03 23:15:27,287] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1346
[2019-04-03 23:15:27,292] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 60.00000000000001, 90.16666666666666, 721.8333333333333, 26.0, 26.79651214100826, 0.6962644797701215, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3770400.0000, 
sim time next is 3771000.0000, 
raw observation next is [0.0, 60.0, 87.0, 711.0, 26.0, 26.82409264200371, 0.6959914853176654, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.46260387811634357, 0.6, 0.29, 0.7856353591160221, 0.6666666666666666, 0.7353410535003091, 0.7319971617725551, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.04856867], dtype=float32), 0.6507791]. 
=============================================
[2019-04-03 23:15:27,295] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[93.8459 ]
 [93.91644]
 [93.99068]
 [94.07381]
 [94.12346]], R is [[93.83895111]
 [93.9005661 ]
 [93.96156311]
 [94.02194977]
 [94.0817337 ]].
[2019-04-03 23:15:32,689] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.3976450e-16 5.2257963e-07 6.4878441e-14 5.2144698e-11 7.2639530e-09
 1.4799852e-15 9.9999952e-01], sum to 1.0000
[2019-04-03 23:15:32,697] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7145
[2019-04-03 23:15:32,713] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.5, 44.5, 18.0, 179.0, 26.0, 26.74960785275064, 0.6357126732701026, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3864600.0000, 
sim time next is 3865200.0000, 
raw observation next is [2.333333333333333, 45.66666666666667, 15.0, 149.1666666666667, 26.0, 26.44963823096612, 0.6498198985550939, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.5272391505078486, 0.4566666666666667, 0.05, 0.1648250460405157, 0.6666666666666666, 0.7041365192471766, 0.7166066328516979, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5770745], dtype=float32), 2.174033]. 
=============================================
[2019-04-03 23:15:32,739] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.9816685e-15 1.6698065e-06 5.9672265e-14 6.7542111e-11 2.4327280e-08
 2.5268503e-15 9.9999833e-01], sum to 1.0000
[2019-04-03 23:15:32,740] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8427
[2019-04-03 23:15:32,750] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 73.0, 111.6666666666667, 777.8333333333334, 26.0, 26.37534256351486, 0.5579325341717586, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3753600.0000, 
sim time next is 3754200.0000, 
raw observation next is [-3.0, 72.0, 112.3333333333333, 786.6666666666667, 26.0, 26.39238751727401, 0.5657341416148663, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.3795013850415513, 0.72, 0.37444444444444436, 0.8692449355432782, 0.6666666666666666, 0.6993656264395008, 0.6885780472049555, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.24131635], dtype=float32), -0.89684814]. 
=============================================
[2019-04-03 23:15:36,579] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.5805799e-15 1.4385508e-05 3.9360147e-13 3.7906958e-10 3.6605023e-08
 2.5620085e-14 9.9998558e-01], sum to 1.0000
[2019-04-03 23:15:36,580] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0922
[2019-04-03 23:15:36,615] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.333333333333333, 67.33333333333334, 99.33333333333333, 641.1666666666667, 26.0, 26.16041703886401, 0.5356552765641446, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3835200.0000, 
sim time next is 3835800.0000, 
raw observation next is [-3.0, 65.5, 101.0, 680.0, 26.0, 26.23505353126695, 0.5550484022219692, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.3795013850415513, 0.655, 0.33666666666666667, 0.7513812154696132, 0.6666666666666666, 0.6862544609389124, 0.6850161340739898, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.93696016], dtype=float32), 0.7908324]. 
=============================================
[2019-04-03 23:15:37,149] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.1732083e-16 1.9145079e-06 2.6448258e-14 5.2543574e-11 2.9089886e-09
 2.1294680e-16 9.9999809e-01], sum to 1.0000
[2019-04-03 23:15:37,157] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4434
[2019-04-03 23:15:37,171] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 55.5, 117.0, 835.0, 26.0, 26.6600138953349, 0.6748145171543385, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3846600.0000, 
sim time next is 3847200.0000, 
raw observation next is [0.3333333333333333, 54.0, 116.3333333333333, 833.1666666666667, 26.0, 26.64818256836011, 0.6797086673077022, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.4718374884579871, 0.54, 0.38777777777777767, 0.9206261510128915, 0.6666666666666666, 0.7206818806966758, 0.7265695557692341, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3831432], dtype=float32), 2.2540865]. 
=============================================
[2019-04-03 23:15:38,026] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.6200480e-14 1.5499935e-06 1.5723449e-12 9.6168473e-10 3.2935286e-08
 3.4623979e-14 9.9999845e-01], sum to 1.0000
[2019-04-03 23:15:38,028] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0642
[2019-04-03 23:15:38,059] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.333333333333333, 67.0, 0.0, 0.0, 26.0, 25.11960373549391, 0.4638115643804243, 0.0, 1.0, 198787.7383368569], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3788400.0000, 
sim time next is 3789000.0000, 
raw observation next is [-2.5, 68.0, 0.0, 0.0, 26.0, 25.11585148557431, 0.5037857409494001, 0.0, 1.0, 170408.7271059874], 
processed observation next is [1.0, 0.8695652173913043, 0.39335180055401664, 0.68, 0.0, 0.0, 0.6666666666666666, 0.5929876237978592, 0.6679285803164667, 0.0, 1.0, 0.8114701290761305], 
reward next is 0.1885, 
noisyNet noise sample is [array([-1.4753805], dtype=float32), -0.38271102]. 
=============================================
[2019-04-03 23:15:38,062] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[77.96727 ]
 [80.145645]
 [84.08222 ]
 [82.536545]
 [81.25498 ]], R is [[76.11177063]
 [75.4040451 ]
 [75.36580658]
 [75.30692291]
 [75.26843262]].
[2019-04-03 23:15:51,652] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.0496617e-12 7.0947994e-05 1.7275377e-10 1.3993749e-08 8.6797161e-07
 7.0665891e-12 9.9992824e-01], sum to 1.0000
[2019-04-03 23:15:51,652] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8049
[2019-04-03 23:15:51,674] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 40.5, 0.0, 0.0, 26.0, 24.88485573135924, 0.2484635304311122, 0.0, 1.0, 40562.41707420137], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4074600.0000, 
sim time next is 4075200.0000, 
raw observation next is [-5.0, 41.0, 0.0, 0.0, 26.0, 24.87294276553499, 0.244346272907025, 0.0, 1.0, 40521.71296016916], 
processed observation next is [1.0, 0.17391304347826086, 0.32409972299168976, 0.41, 0.0, 0.0, 0.6666666666666666, 0.5727452304612491, 0.5814487576356749, 0.0, 1.0, 0.19296053790556744], 
reward next is 0.8070, 
noisyNet noise sample is [array([-1.1220073], dtype=float32), 0.77698773]. 
=============================================
[2019-04-03 23:15:55,229] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.0220059e-14 2.1680678e-06 1.0789166e-12 8.5524149e-10 2.6365009e-07
 2.2360191e-14 9.9999762e-01], sum to 1.0000
[2019-04-03 23:15:55,236] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2896
[2019-04-03 23:15:55,263] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 38.0, 0.0, 0.0, 26.0, 25.92238748872528, 0.5836335841563985, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4138200.0000, 
sim time next is 4138800.0000, 
raw observation next is [1.0, 38.66666666666666, 0.0, 0.0, 26.0, 25.92030385270454, 0.578780843697972, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.4903047091412743, 0.38666666666666655, 0.0, 0.0, 0.6666666666666666, 0.6600253210587116, 0.692926947899324, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2066393], dtype=float32), -2.0638957]. 
=============================================
[2019-04-03 23:15:57,474] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.1918594e-13 3.8202925e-05 2.9154162e-11 8.2329574e-09 4.3161833e-07
 2.0367195e-13 9.9996126e-01], sum to 1.0000
[2019-04-03 23:15:57,476] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7719
[2019-04-03 23:15:57,496] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 30.0, 0.0, 0.0, 26.0, 25.62505449585553, 0.4891678610378302, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4051800.0000, 
sim time next is 4052400.0000, 
raw observation next is [-4.666666666666666, 30.33333333333333, 0.0, 0.0, 26.0, 25.56202079253569, 0.4764250063847237, 0.0, 1.0, 38038.06430455437], 
processed observation next is [1.0, 0.9130434782608695, 0.33333333333333337, 0.3033333333333333, 0.0, 0.0, 0.6666666666666666, 0.6301683993779742, 0.6588083354615746, 0.0, 1.0, 0.181133639545497], 
reward next is 0.8189, 
noisyNet noise sample is [array([-0.36040056], dtype=float32), -0.5763358]. 
=============================================
[2019-04-03 23:16:03,280] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.2914572e-13 2.3219944e-04 1.8055479e-12 3.1239991e-09 2.8582664e-07
 1.2788046e-12 9.9976760e-01], sum to 1.0000
[2019-04-03 23:16:03,286] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0876
[2019-04-03 23:16:03,296] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.5, 40.5, 160.0, 538.0, 26.0, 25.31491255678436, 0.4476686371323768, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4203000.0000, 
sim time next is 4203600.0000, 
raw observation next is [2.666666666666667, 39.33333333333333, 144.6666666666667, 540.0, 26.0, 25.35944109233535, 0.4476960123687013, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.5364727608494922, 0.3933333333333333, 0.4822222222222224, 0.5966850828729282, 0.6666666666666666, 0.6132867576946124, 0.6492320041229004, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.19953574], dtype=float32), -0.4284546]. 
=============================================
[2019-04-03 23:16:04,125] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.2471010e-14 1.2608683e-04 2.6416656e-13 2.6384470e-10 5.3107268e-08
 3.8723334e-14 9.9987388e-01], sum to 1.0000
[2019-04-03 23:16:04,125] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4604
[2019-04-03 23:16:04,140] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.0, 52.0, 120.0, 847.0, 26.0, 25.25653696986035, 0.4168525168108421, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4278600.0000, 
sim time next is 4279200.0000, 
raw observation next is [7.0, 52.0, 131.3333333333333, 825.3333333333334, 26.0, 25.26687978863998, 0.4273386909759231, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.6565096952908588, 0.52, 0.4377777777777776, 0.9119705340699816, 0.6666666666666666, 0.6055733157199983, 0.6424462303253077, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.546009], dtype=float32), -0.19900058]. 
=============================================
[2019-04-03 23:16:05,282] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.2490477e-15 7.5479191e-05 1.1415311e-12 6.7917333e-10 3.9289212e-08
 1.6279596e-13 9.9992454e-01], sum to 1.0000
[2019-04-03 23:16:05,282] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2883
[2019-04-03 23:16:05,293] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.199999999999999, 73.0, 0.0, 0.0, 26.0, 25.80578449497003, 0.4693402202357972, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4308000.0000, 
sim time next is 4308600.0000, 
raw observation next is [5.15, 73.0, 0.0, 0.0, 26.0, 25.81009644953117, 0.4612142549929398, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.605263157894737, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6508413707942641, 0.6537380849976465, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.28576973], dtype=float32), 0.46605182]. 
=============================================
[2019-04-03 23:16:07,231] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.4110680e-13 3.2718380e-03 1.2593837e-11 5.8577152e-09 4.8873318e-07
 1.6411486e-12 9.9672771e-01], sum to 1.0000
[2019-04-03 23:16:07,234] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1615
[2019-04-03 23:16:07,278] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.333333333333333, 53.33333333333334, 170.0, 118.0, 26.0, 25.68660927866797, 0.4058240980057999, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4267200.0000, 
sim time next is 4267800.0000, 
raw observation next is [3.5, 53.5, 182.0, 131.0, 26.0, 25.70410420268236, 0.4069295128259844, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.5595567867036012, 0.535, 0.6066666666666667, 0.14475138121546963, 0.6666666666666666, 0.6420086835568632, 0.6356431709419949, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.57345843], dtype=float32), 1.0217795]. 
=============================================
[2019-04-03 23:16:10,809] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.09197688e-12 5.95809380e-03 1.22862154e-11 6.65432465e-09
 1.40787324e-06 2.41359818e-12 9.94040430e-01], sum to 1.0000
[2019-04-03 23:16:10,810] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3772
[2019-04-03 23:16:10,852] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.166666666666667, 53.16666666666667, 158.0, 105.0, 26.0, 25.67631358843686, 0.3991748104794597, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4266600.0000, 
sim time next is 4267200.0000, 
raw observation next is [3.333333333333333, 53.33333333333334, 170.0, 118.0, 26.0, 25.68694790005701, 0.4058935057095864, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.5549399815327793, 0.5333333333333334, 0.5666666666666667, 0.13038674033149172, 0.6666666666666666, 0.6405789916714175, 0.6352978352365288, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-3.0285623], dtype=float32), -0.26679903]. 
=============================================
[2019-04-03 23:16:12,913] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.7159393e-13 4.2875054e-05 2.8277098e-11 2.8251156e-08 8.6587033e-07
 5.7954417e-13 9.9995625e-01], sum to 1.0000
[2019-04-03 23:16:12,914] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2409
[2019-04-03 23:16:12,930] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.58629679728526, 0.5545057039267562, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4484400.0000, 
sim time next is 4485000.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.55188100979001, 0.5445545873515641, 0.0, 1.0, 28664.19454722727], 
processed observation next is [1.0, 0.9130434782608695, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6293234174825008, 0.6815181957838546, 0.0, 1.0, 0.13649616451060606], 
reward next is 0.8635, 
noisyNet noise sample is [array([-1.1221907], dtype=float32), -0.88201475]. 
=============================================
[2019-04-03 23:16:12,934] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[74.557365]
 [74.53545 ]
 [74.520836]
 [74.47584 ]
 [74.33308 ]], R is [[74.71033478]
 [74.96323395]
 [75.21360016]
 [75.46146393]
 [75.70684814]].
[2019-04-03 23:16:22,094] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.8654585e-15 1.5866302e-05 5.5453787e-14 1.3324582e-10 1.2164181e-08
 2.0271794e-15 9.9998415e-01], sum to 1.0000
[2019-04-03 23:16:22,097] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2624
[2019-04-03 23:16:22,137] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.583333333333334, 49.0, 160.3333333333333, 741.6666666666667, 26.0, 27.07909054106896, 0.7020058406308216, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4629000.0000, 
sim time next is 4629600.0000, 
raw observation next is [4.7, 49.0, 171.0, 706.0, 26.0, 26.61699679341374, 0.7600310101007136, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.592797783933518, 0.49, 0.57, 0.7801104972375691, 0.6666666666666666, 0.7180830661178117, 0.7533436700335713, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.15009426], dtype=float32), 1.0405706]. 
=============================================
[2019-04-03 23:16:22,281] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.4179533e-13 6.7493634e-04 9.3057603e-11 6.0136456e-09 1.1392071e-06
 2.2551954e-12 9.9932384e-01], sum to 1.0000
[2019-04-03 23:16:22,281] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9205
[2019-04-03 23:16:22,301] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.9, 73.0, 0.0, 0.0, 26.0, 25.37416048285523, 0.4260809653230678, 0.0, 1.0, 45314.4659514359], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4507200.0000, 
sim time next is 4507800.0000, 
raw observation next is [-0.8833333333333333, 72.66666666666667, 0.0, 0.0, 26.0, 25.32278438790118, 0.4162087620913708, 0.0, 1.0, 44341.72553129362], 
processed observation next is [1.0, 0.17391304347826086, 0.43813481071098803, 0.7266666666666667, 0.0, 0.0, 0.6666666666666666, 0.6102320323250984, 0.638736254030457, 0.0, 1.0, 0.21115107395854105], 
reward next is 0.7888, 
noisyNet noise sample is [array([1.00222], dtype=float32), -0.25790122]. 
=============================================
[2019-04-03 23:16:28,502] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.3705992e-15 8.6293665e-05 5.7753428e-13 9.4619290e-10 1.1280664e-07
 6.1678165e-15 9.9991357e-01], sum to 1.0000
[2019-04-03 23:16:28,503] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5408
[2019-04-03 23:16:28,532] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.666666666666667, 50.0, 121.6666666666667, 837.3333333333334, 26.0, 26.68487293732435, 0.7149321184624758, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4621200.0000, 
sim time next is 4621800.0000, 
raw observation next is [2.833333333333333, 49.5, 121.3333333333333, 841.6666666666666, 26.0, 26.39274098925526, 0.7070981197890737, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.541089566020314, 0.495, 0.40444444444444433, 0.9300184162062615, 0.6666666666666666, 0.6993950824379382, 0.7356993732630245, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0034623], dtype=float32), 1.1412935]. 
=============================================
[2019-04-03 23:16:33,281] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.1857183e-15 1.1739922e-03 2.5410780e-12 2.5470892e-09 3.6426769e-08
 2.1028599e-14 9.9882597e-01], sum to 1.0000
[2019-04-03 23:16:33,281] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6375
[2019-04-03 23:16:33,304] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 92.0, 115.0, 0.0, 26.0, 26.29209200491002, 0.5613138779400605, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4699800.0000, 
sim time next is 4700400.0000, 
raw observation next is [0.0, 92.0, 130.5, 0.9999999999999998, 26.0, 26.37266909432094, 0.5678070260968933, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.46260387811634357, 0.92, 0.435, 0.0011049723756906074, 0.6666666666666666, 0.697722424526745, 0.6892690086989645, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.00508772], dtype=float32), 0.61820257]. 
=============================================
[2019-04-03 23:16:37,173] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.1806611e-13 2.4604792e-04 4.9212998e-11 9.9662927e-09 1.9340443e-06
 4.8354350e-12 9.9975199e-01], sum to 1.0000
[2019-04-03 23:16:37,177] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4414
[2019-04-03 23:16:37,194] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.3333333333333334, 45.0, 0.0, 0.0, 26.0, 25.46794388133109, 0.445047975673929, 0.0, 1.0, 18756.28353394231], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5019600.0000, 
sim time next is 5020200.0000, 
raw observation next is [0.0, 47.5, 0.0, 0.0, 26.0, 25.55141819686839, 0.4442662656264504, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.46260387811634357, 0.475, 0.0, 0.0, 0.6666666666666666, 0.6292848497390325, 0.6480887552088168, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.24119432], dtype=float32), 1.1707196]. 
=============================================
[2019-04-03 23:16:40,398] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.5467009e-12 9.8080595e-04 7.0975073e-11 1.3761712e-08 6.6359676e-06
 7.3986324e-11 9.9901247e-01], sum to 1.0000
[2019-04-03 23:16:40,400] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4252
[2019-04-03 23:16:40,420] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.2290448567761, 0.3397246908426917, 0.0, 1.0, 39682.75623472538], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4842000.0000, 
sim time next is 4842600.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.21214687365991, 0.3357440464191774, 0.0, 1.0, 39400.99839498099], 
processed observation next is [0.0, 0.043478260869565216, 0.40720221606648205, 0.6, 0.0, 0.0, 0.6666666666666666, 0.6010122394716593, 0.6119146821397258, 0.0, 1.0, 0.18762380188086186], 
reward next is 0.8124, 
noisyNet noise sample is [array([0.16308136], dtype=float32), 1.1798267]. 
=============================================
[2019-04-03 23:16:42,310] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:16:42,310] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:16:42,318] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run8
[2019-04-03 23:16:43,957] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:16:43,957] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:16:43,976] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run8
[2019-04-03 23:16:47,489] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:16:47,489] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:16:47,511] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run8
[2019-04-03 23:16:48,745] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.2534329e-15 6.4218904e-05 3.5093717e-14 6.8855927e-10 3.9238351e-08
 1.1389737e-15 9.9993575e-01], sum to 1.0000
[2019-04-03 23:16:48,746] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6689
[2019-04-03 23:16:48,761] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.333333333333334, 25.66666666666667, 94.83333333333334, 781.5, 26.0, 26.24934033309413, 0.7429635685607954, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4980000.0000, 
sim time next is 4980600.0000, 
raw observation next is [8.5, 25.5, 92.0, 774.0, 26.0, 26.79393110808671, 0.8118187756133829, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.698060941828255, 0.255, 0.30666666666666664, 0.8552486187845304, 0.6666666666666666, 0.7328275923405592, 0.7706062585377943, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2546164], dtype=float32), 0.49424952]. 
=============================================
[2019-04-03 23:16:49,692] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:16:49,693] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:16:49,696] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run8
[2019-04-03 23:16:49,720] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:16:49,723] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:16:49,738] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run8
[2019-04-03 23:16:49,927] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:16:49,928] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:16:49,931] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run8
[2019-04-03 23:16:50,386] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:16:50,387] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:16:50,407] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run8
[2019-04-03 23:16:51,643] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:16:51,644] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:16:51,648] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run8
[2019-04-03 23:16:52,788] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:16:52,788] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:16:52,791] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run8
[2019-04-03 23:16:53,110] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:16:53,110] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:16:53,115] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run8
[2019-04-03 23:16:53,765] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:16:53,765] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:16:53,768] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run8
[2019-04-03 23:16:56,637] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:16:56,637] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:16:56,641] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run8
[2019-04-03 23:16:57,493] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:16:57,493] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:16:57,496] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run8
[2019-04-03 23:16:58,535] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:16:58,536] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:16:58,545] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run8
[2019-04-03 23:16:58,776] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:16:58,776] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:16:58,782] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run8
[2019-04-03 23:16:58,821] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:16:58,821] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:16:58,842] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run8
[2019-04-03 23:17:03,266] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.9978192e-14 3.1167211e-04 3.1677313e-12 2.0821689e-09 2.5242559e-07
 2.8790788e-13 9.9968815e-01], sum to 1.0000
[2019-04-03 23:17:03,266] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2023
[2019-04-03 23:17:03,356] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.05, 84.0, 18.0, 0.0, 26.0, 24.50422910401001, 0.1860511567913048, 0.0, 1.0, 58935.20283097521], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 59400.0000, 
sim time next is 60000.0000, 
raw observation next is [5.866666666666667, 84.66666666666666, 15.0, 0.0, 26.0, 24.53955699601114, 0.1896098215059848, 0.0, 1.0, 34419.24430637214], 
processed observation next is [0.0, 0.6956521739130435, 0.6251154201292707, 0.8466666666666666, 0.05, 0.0, 0.6666666666666666, 0.5449630830009283, 0.5632032738353282, 0.0, 1.0, 0.16390116336367683], 
reward next is 0.8361, 
noisyNet noise sample is [array([0.44694403], dtype=float32), -0.18594716]. 
=============================================
[2019-04-03 23:17:03,360] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[89.04911 ]
 [89.081795]
 [89.16251 ]
 [89.391014]
 [89.68543 ]], R is [[88.88803864]
 [88.71851349]
 [88.58995056]
 [88.56777954]
 [88.59288788]].
[2019-04-03 23:17:04,866] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.0540884e-13 5.5364013e-05 1.7575791e-12 4.8028017e-09 3.0854645e-07
 2.6872437e-14 9.9994433e-01], sum to 1.0000
[2019-04-03 23:17:04,867] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2158
[2019-04-03 23:17:04,920] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.4, 68.0, 0.0, 0.0, 26.0, 24.98692253693702, 0.3143176106917797, 0.0, 1.0, 83790.47007051541], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 158400.0000, 
sim time next is 159000.0000, 
raw observation next is [-8.4, 68.0, 0.0, 0.0, 26.0, 25.01508177658311, 0.3172058431232116, 1.0, 1.0, 52893.72762804938], 
processed observation next is [1.0, 0.8695652173913043, 0.2299168975069252, 0.68, 0.0, 0.0, 0.6666666666666666, 0.5845901480485924, 0.6057352810410706, 1.0, 1.0, 0.25187489346690184], 
reward next is 0.7481, 
noisyNet noise sample is [array([-1.648423], dtype=float32), -0.47306797]. 
=============================================
[2019-04-03 23:17:04,924] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[77.229294]
 [82.40725 ]
 [82.1411  ]
 [81.91658 ]
 [82.09708 ]], R is [[78.97470093]
 [78.78594971]
 [78.47237396]
 [78.22399139]
 [78.27684784]].
[2019-04-03 23:17:07,915] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.4543680e-12 4.3337117e-03 4.0701428e-11 2.1511026e-08 4.6042478e-06
 2.9077570e-11 9.9566168e-01], sum to 1.0000
[2019-04-03 23:17:07,918] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3705
[2019-04-03 23:17:07,928] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.7, 93.0, 0.0, 0.0, 26.0, 21.31438586249307, -0.5485331347517629, 0.0, 1.0, 40271.59813724393], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 25200.0000, 
sim time next is 25800.0000, 
raw observation next is [7.7, 93.0, 0.0, 0.0, 26.0, 21.33383646848958, -0.5428710628642913, 0.0, 1.0, 40271.64230459555], 
processed observation next is [0.0, 0.30434782608695654, 0.6759002770083103, 0.93, 0.0, 0.0, 0.6666666666666666, 0.27781970570746495, 0.3190429790452362, 0.0, 1.0, 0.19176972525997882], 
reward next is 0.8082, 
noisyNet noise sample is [array([0.11142915], dtype=float32), -2.2016478]. 
=============================================
[2019-04-03 23:17:16,736] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.4312275e-14 6.8473833e-05 8.3180533e-13 3.5109387e-09 2.0589809e-07
 1.4228394e-13 9.9993134e-01], sum to 1.0000
[2019-04-03 23:17:16,738] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4875
[2019-04-03 23:17:16,789] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.466666666666667, 65.66666666666667, 30.83333333333334, 7.5, 26.0, 25.02803481863253, 0.2512005557983419, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 116400.0000, 
sim time next is 117000.0000, 
raw observation next is [-7.55, 64.5, 37.0, 9.0, 26.0, 25.16772768728775, 0.2668540931174303, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.25346260387811637, 0.645, 0.12333333333333334, 0.009944751381215469, 0.6666666666666666, 0.5973106406073126, 0.5889513643724767, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.27762997], dtype=float32), 0.731279]. 
=============================================
[2019-04-03 23:17:16,798] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[82.940254]
 [82.51124 ]
 [81.32538 ]
 [80.20152 ]
 [76.86588 ]], R is [[82.6791153 ]
 [82.85232544]
 [83.02380371]
 [83.19356537]
 [82.83802795]].
[2019-04-03 23:17:20,738] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.5976349e-15 1.4978550e-05 1.3712637e-12 3.3139931e-09 1.4925220e-07
 3.1388984e-14 9.9998486e-01], sum to 1.0000
[2019-04-03 23:17:20,739] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8525
[2019-04-03 23:17:20,815] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.5, 44.0, 89.0, 694.5, 26.0, 25.47955481038625, 0.4482820464085409, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 306000.0000, 
sim time next is 306600.0000, 
raw observation next is [-9.5, 44.00000000000001, 91.0, 673.3333333333333, 26.0, 25.77137750574359, 0.4708705380643021, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.1994459833795014, 0.44000000000000006, 0.30333333333333334, 0.7440147329650091, 0.6666666666666666, 0.6476147921452992, 0.656956846021434, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.196715], dtype=float32), 0.0978606]. 
=============================================
[2019-04-03 23:17:26,826] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.0831474e-15 1.4656227e-05 6.9357085e-13 8.4883117e-10 7.3632975e-08
 4.8033978e-14 9.9998522e-01], sum to 1.0000
[2019-04-03 23:17:26,827] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0842
[2019-04-03 23:17:26,889] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 65.0, 141.0, 0.0, 26.0, 25.23498729874644, 0.2148977834839768, 1.0, 1.0, 25024.57986803531], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 216000.0000, 
sim time next is 216600.0000, 
raw observation next is [-4.916666666666667, 65.0, 137.0, 0.0, 26.0, 25.18426835585715, 0.2227937729355375, 1.0, 1.0, 53779.6325222601], 
processed observation next is [1.0, 0.5217391304347826, 0.32640812557710064, 0.65, 0.45666666666666667, 0.0, 0.6666666666666666, 0.5986890296547625, 0.5742645909785126, 1.0, 1.0, 0.25609348820123856], 
reward next is 0.7439, 
noisyNet noise sample is [array([-0.1366024], dtype=float32), -0.34567207]. 
=============================================
[2019-04-03 23:17:27,576] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.7896042e-15 6.1406240e-06 1.8451573e-13 4.8771653e-10 1.7735283e-07
 5.0793195e-14 9.9999368e-01], sum to 1.0000
[2019-04-03 23:17:27,577] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5119
[2019-04-03 23:17:27,625] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 62.0, 37.0, 0.0, 26.0, 25.95139234597434, 0.3620471514440244, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 230400.0000, 
sim time next is 231000.0000, 
raw observation next is [-3.4, 62.5, 30.66666666666666, 0.0, 26.0, 25.88709132110593, 0.209641129080297, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.368421052631579, 0.625, 0.1022222222222222, 0.0, 0.6666666666666666, 0.6572576100921609, 0.569880376360099, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2093867], dtype=float32), -0.21157642]. 
=============================================
[2019-04-03 23:17:27,630] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[78.68677 ]
 [78.94737 ]
 [79.14192 ]
 [79.269   ]
 [79.509346]], R is [[78.65330505]
 [78.86677551]
 [79.07810974]
 [79.28733063]
 [79.49446106]].
[2019-04-03 23:17:30,961] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.8291510e-13 7.5416169e-06 2.4158394e-12 5.7872866e-09 5.9523018e-07
 3.9179581e-13 9.9999189e-01], sum to 1.0000
[2019-04-03 23:17:30,961] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4649
[2019-04-03 23:17:30,980] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.65, 70.0, 0.0, 0.0, 26.0, 24.62988864986578, 0.1944485935178513, 0.0, 1.0, 44691.29951837688], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 250200.0000, 
sim time next is 250800.0000, 
raw observation next is [-3.733333333333333, 71.66666666666667, 0.0, 0.0, 26.0, 24.58411936377963, 0.1866414739930911, 0.0, 1.0, 44319.15109682769], 
processed observation next is [1.0, 0.9130434782608695, 0.35918744228993543, 0.7166666666666667, 0.0, 0.0, 0.6666666666666666, 0.5486766136483027, 0.5622138246643636, 0.0, 1.0, 0.21104357665156043], 
reward next is 0.7890, 
noisyNet noise sample is [array([0.59617543], dtype=float32), -1.1748822]. 
=============================================
[2019-04-03 23:17:32,050] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.9786246e-11 5.5435527e-04 8.9505081e-10 1.5269053e-07 1.3546880e-05
 1.8268258e-10 9.9943191e-01], sum to 1.0000
[2019-04-03 23:17:32,050] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4344
[2019-04-03 23:17:32,080] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.2, 55.0, 0.0, 0.0, 26.0, 23.60639416237538, -0.06265457803700009, 0.0, 1.0, 45416.35234234382], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 435600.0000, 
sim time next is 436200.0000, 
raw observation next is [-11.2, 55.0, 0.0, 0.0, 26.0, 23.57856386634929, -0.06687794534550848, 0.0, 1.0, 45445.78996125834], 
processed observation next is [1.0, 0.043478260869565216, 0.15235457063711913, 0.55, 0.0, 0.0, 0.6666666666666666, 0.464880322195774, 0.4777073515514972, 0.0, 1.0, 0.21640852362503973], 
reward next is 0.7836, 
noisyNet noise sample is [array([0.3985408], dtype=float32), 1.0545374]. 
=============================================
[2019-04-03 23:17:39,908] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0653575e-12 2.6218901e-05 4.6635643e-11 1.9172251e-08 2.8335765e-07
 1.9378533e-12 9.9997354e-01], sum to 1.0000
[2019-04-03 23:17:39,918] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8147
[2019-04-03 23:17:39,963] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-14.0, 66.5, 0.0, 0.0, 26.0, 23.52995114982552, -0.05769219098995788, 0.0, 1.0, 47377.16137342355], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 346200.0000, 
sim time next is 346800.0000, 
raw observation next is [-14.1, 67.0, 0.0, 0.0, 26.0, 23.45823555139974, -0.0722566518881534, 0.0, 1.0, 47449.56974582743], 
processed observation next is [1.0, 0.0, 0.07202216066481995, 0.67, 0.0, 0.0, 0.6666666666666666, 0.4548529626166449, 0.47591444937061556, 0.0, 1.0, 0.22595033212298776], 
reward next is 0.7740, 
noisyNet noise sample is [array([0.5783999], dtype=float32), -1.0188185]. 
=============================================
[2019-04-03 23:17:52,124] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.9662940e-14 2.0161150e-02 6.4016769e-12 5.1231321e-09 8.4650759e-07
 3.3180943e-12 9.7983801e-01], sum to 1.0000
[2019-04-03 23:17:52,125] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0336
[2019-04-03 23:17:52,142] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.183333333333333, 87.5, 0.0, 0.0, 26.0, 24.50739124415498, 0.1729811987212656, 0.0, 1.0, 40767.24243747846], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 539400.0000, 
sim time next is 540000.0000, 
raw observation next is [1.1, 88.0, 0.0, 0.0, 26.0, 24.49143009768368, 0.1666664677552588, 0.0, 1.0, 40777.18425266224], 
processed observation next is [0.0, 0.2608695652173913, 0.49307479224376743, 0.88, 0.0, 0.0, 0.6666666666666666, 0.5409525081403066, 0.5555554892517529, 0.0, 1.0, 0.1941770678698202], 
reward next is 0.8058, 
noisyNet noise sample is [array([0.5532263], dtype=float32), -0.32741013]. 
=============================================
[2019-04-03 23:17:52,149] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[86.84575]
 [86.94254]
 [87.05995]
 [87.19583]
 [87.33756]], R is [[86.68870544]
 [86.62768555]
 [86.567276  ]
 [86.50762177]
 [86.44880676]].
[2019-04-03 23:18:14,741] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.2230411e-16 4.2100305e-06 3.8123052e-14 2.6470509e-10 3.4041960e-09
 2.6768729e-15 9.9999583e-01], sum to 1.0000
[2019-04-03 23:18:14,741] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8512
[2019-04-03 23:18:14,788] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 83.33333333333334, 32.33333333333333, 0.0, 26.0, 25.90329205638411, 0.3995420555451148, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 836400.0000, 
sim time next is 837000.0000, 
raw observation next is [-3.9, 84.0, 29.0, 0.0, 26.0, 25.89962546925489, 0.2951061031947936, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.3545706371191136, 0.84, 0.09666666666666666, 0.0, 0.6666666666666666, 0.6583021224379074, 0.5983687010649312, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9214049], dtype=float32), -1.0291522]. 
=============================================
[2019-04-03 23:18:14,795] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[89.130035]
 [89.27132 ]
 [89.262665]
 [89.40405 ]
 [89.64305 ]], R is [[89.04675293]
 [89.15628815]
 [89.26472473]
 [89.37207794]
 [89.47835541]].
[2019-04-03 23:18:16,234] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.8410728e-14 5.2535709e-04 5.9089756e-12 2.3986373e-09 1.9972049e-07
 3.0295598e-13 9.9947447e-01], sum to 1.0000
[2019-04-03 23:18:16,237] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4466
[2019-04-03 23:18:16,339] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.700000000000001, 69.66666666666667, 0.0, 0.0, 26.0, 23.43240226605372, -0.02009259462278101, 1.0, 1.0, 202414.3956292189], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 804000.0000, 
sim time next is 804600.0000, 
raw observation next is [-6.7, 71.0, 0.0, 0.0, 26.0, 23.73007834560853, 0.0799066750139937, 1.0, 1.0, 202742.7708674357], 
processed observation next is [1.0, 0.30434782608695654, 0.2770083102493075, 0.71, 0.0, 0.0, 0.6666666666666666, 0.47750652880071076, 0.5266355583379979, 1.0, 1.0, 0.9654417660354081], 
reward next is 0.0346, 
noisyNet noise sample is [array([0.16751653], dtype=float32), 0.15764508]. 
=============================================
[2019-04-03 23:18:25,927] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.9675937e-16 7.7749122e-05 1.5964311e-14 4.3698812e-11 2.0357868e-08
 8.5397804e-16 9.9992228e-01], sum to 1.0000
[2019-04-03 23:18:25,927] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2743
[2019-04-03 23:18:25,965] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.3, 80.0, 0.0, 0.0, 26.0, 25.62678149343117, 0.5804089955089528, 0.0, 1.0, 48121.02509847558], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1061400.0000, 
sim time next is 1062000.0000, 
raw observation next is [13.3, 80.0, 0.0, 0.0, 26.0, 25.60644802572634, 0.5862603528220877, 0.0, 1.0, 44609.7076542043], 
processed observation next is [1.0, 0.30434782608695654, 0.8310249307479226, 0.8, 0.0, 0.0, 0.6666666666666666, 0.6338706688105283, 0.6954201176073626, 0.0, 1.0, 0.21242717930573476], 
reward next is 0.7876, 
noisyNet noise sample is [array([-0.9702574], dtype=float32), 0.14541313]. 
=============================================
[2019-04-03 23:18:26,054] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[97.95171 ]
 [97.8167  ]
 [97.74317 ]
 [97.506966]
 [97.67933 ]], R is [[97.88153839]
 [97.67357635]
 [97.46723938]
 [97.19747162]
 [97.22549438]].
[2019-04-03 23:18:32,571] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.7858060e-16 1.8831639e-05 4.0054731e-14 2.4924451e-10 1.5528826e-08
 3.9726804e-14 9.9998116e-01], sum to 1.0000
[2019-04-03 23:18:32,585] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5844
[2019-04-03 23:18:32,617] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.41666666666667, 77.33333333333334, 0.0, 0.0, 26.0, 25.6529648821109, 0.6412613267238599, 0.0, 1.0, 23345.90766090225], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1127400.0000, 
sim time next is 1128000.0000, 
raw observation next is [10.33333333333333, 77.66666666666667, 0.0, 0.0, 26.0, 25.65489178615274, 0.6403223012623993, 0.0, 1.0, 22279.36500708475], 
processed observation next is [0.0, 0.043478260869565216, 0.7488457987072946, 0.7766666666666667, 0.0, 0.0, 0.6666666666666666, 0.6379076488460615, 0.7134407670874664, 0.0, 1.0, 0.10609221431945119], 
reward next is 0.8939, 
noisyNet noise sample is [array([1.2637329], dtype=float32), 0.25012398]. 
=============================================
[2019-04-03 23:18:32,674] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[91.454445]
 [91.52859 ]
 [91.51392 ]
 [91.4696  ]
 [91.38278 ]], R is [[91.22073364]
 [91.19735718]
 [91.15652466]
 [91.08074951]
 [90.9630127 ]].
[2019-04-03 23:18:33,097] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.1409523e-17 2.2766601e-05 1.9910653e-14 5.2005279e-11 2.0701014e-09
 6.3009097e-16 9.9997723e-01], sum to 1.0000
[2019-04-03 23:18:33,097] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8084
[2019-04-03 23:18:33,180] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.9, 92.16666666666667, 38.0, 0.0, 26.0, 26.02279765083031, 0.5557974636565365, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 982200.0000, 
sim time next is 982800.0000, 
raw observation next is [10.0, 92.0, 43.5, 0.0, 26.0, 26.14637123743889, 0.5740108565304533, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.739612188365651, 0.92, 0.145, 0.0, 0.6666666666666666, 0.678864269786574, 0.6913369521768177, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3928561], dtype=float32), -0.6716766]. 
=============================================
[2019-04-03 23:18:38,032] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1823214e-14 1.6481272e-05 4.2617917e-13 6.6168360e-10 1.6380708e-08
 1.4101040e-14 9.9998355e-01], sum to 1.0000
[2019-04-03 23:18:38,032] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7496
[2019-04-03 23:18:38,084] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.25, 83.0, 0.0, 0.0, 26.0, 25.35423776604974, 0.4364200679401786, 0.0, 1.0, 79699.22401627924], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 966600.0000, 
sim time next is 967200.0000, 
raw observation next is [8.433333333333334, 83.0, 0.0, 0.0, 26.0, 25.37725346011483, 0.4497889614758182, 0.0, 1.0, 52714.65082210901], 
processed observation next is [1.0, 0.17391304347826086, 0.6962142197599263, 0.83, 0.0, 0.0, 0.6666666666666666, 0.6147711216762358, 0.6499296538252727, 0.0, 1.0, 0.2510221467719477], 
reward next is 0.7490, 
noisyNet noise sample is [array([-0.09384883], dtype=float32), 0.42225322]. 
=============================================
[2019-04-03 23:18:46,869] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.2686790e-16 1.6316564e-05 6.1283098e-14 8.9430230e-10 4.3977249e-08
 3.5651607e-14 9.9998367e-01], sum to 1.0000
[2019-04-03 23:18:46,869] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9276
[2019-04-03 23:18:46,893] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.16666666666667, 78.33333333333333, 0.0, 0.0, 26.0, 25.66444083731969, 0.6367836195615608, 0.0, 1.0, 19026.53144336948], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1129200.0000, 
sim time next is 1129800.0000, 
raw observation next is [10.08333333333333, 78.66666666666667, 0.0, 0.0, 26.0, 25.65863468838921, 0.6335016934821194, 0.0, 1.0, 24076.0528859765], 
processed observation next is [0.0, 0.043478260869565216, 0.7419205909510619, 0.7866666666666667, 0.0, 0.0, 0.6666666666666666, 0.6382195573657675, 0.7111672311607066, 0.0, 1.0, 0.11464787088560238], 
reward next is 0.8854, 
noisyNet noise sample is [array([-0.20209035], dtype=float32), -1.6004766]. 
=============================================
[2019-04-03 23:18:56,496] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.9668125e-14 1.7584232e-05 1.1562110e-12 7.1498174e-10 1.2928547e-07
 6.4852454e-14 9.9998224e-01], sum to 1.0000
[2019-04-03 23:18:56,496] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0447
[2019-04-03 23:18:56,505] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [18.3, 65.0, 112.0, 0.0, 26.0, 25.05494644357268, 0.4945100701426947, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1176600.0000, 
sim time next is 1177200.0000, 
raw observation next is [18.3, 65.0, 104.0, 0.0, 26.0, 25.04218717805268, 0.4970714129738406, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.9695290858725764, 0.65, 0.3466666666666667, 0.0, 0.6666666666666666, 0.5868489315043899, 0.6656904709912802, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3156162], dtype=float32), -0.21223381]. 
=============================================
[2019-04-03 23:19:07,853] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.9224932e-15 2.7918725e-07 4.6011356e-13 1.3971213e-10 6.1668497e-09
 2.4574037e-14 9.9999976e-01], sum to 1.0000
[2019-04-03 23:19:07,854] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8349
[2019-04-03 23:19:07,902] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.7, 92.0, 0.0, 0.0, 26.0, 25.55280775737404, 0.4863068468186991, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1473000.0000, 
sim time next is 1473600.0000, 
raw observation next is [1.8, 92.0, 0.0, 0.0, 26.0, 25.5331873319775, 0.4732627465572377, 0.0, 1.0, 18745.70755522529], 
processed observation next is [1.0, 0.043478260869565216, 0.5124653739612189, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6277656109981251, 0.6577542488524125, 0.0, 1.0, 0.08926527407250139], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.26447788], dtype=float32), -0.21201672]. 
=============================================
[2019-04-03 23:19:08,964] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [7.5755937e-16 6.5502542e-07 1.5340367e-13 3.1075922e-10 7.2689277e-09
 2.5941587e-14 9.9999940e-01], sum to 1.0000
[2019-04-03 23:19:08,964] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1102
[2019-04-03 23:19:09,063] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 26.0, 25.50469363190516, 0.5008264936927699, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1385400.0000, 
sim time next is 1386000.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 26.0, 25.41131299355769, 0.4829914861237265, 0.0, 1.0, 59416.58246164225], 
processed observation next is [1.0, 0.043478260869565216, 0.46260387811634357, 0.95, 0.0, 0.0, 0.6666666666666666, 0.6176094161298075, 0.6609971620412421, 0.0, 1.0, 0.28293610696020116], 
reward next is 0.7171, 
noisyNet noise sample is [array([-1.4502014], dtype=float32), 0.14423616]. 
=============================================
[2019-04-03 23:19:09,136] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[79.170334]
 [79.342186]
 [79.49609 ]
 [79.53224 ]
 [79.48313 ]], R is [[79.20877075]
 [79.41668701]
 [79.62252045]
 [79.70209503]
 [79.71112061]].
[2019-04-03 23:19:11,947] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.5212372e-15 1.5379841e-06 2.7846220e-13 2.3047703e-10 2.3778703e-08
 2.1320881e-14 9.9999845e-01], sum to 1.0000
[2019-04-03 23:19:11,947] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1082
[2019-04-03 23:19:11,978] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.466666666666667, 98.66666666666666, 0.0, 0.0, 26.0, 25.51645383308988, 0.4679276147823516, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1492800.0000, 
sim time next is 1493400.0000, 
raw observation next is [1.283333333333333, 99.33333333333334, 0.0, 0.0, 26.0, 25.57646409873075, 0.4357440799736405, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.4981532779316713, 0.9933333333333334, 0.0, 0.0, 0.6666666666666666, 0.6313720082275625, 0.6452480266578802, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0926636], dtype=float32), 0.14738803]. 
=============================================
[2019-04-03 23:19:22,925] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.1042248e-17 1.2039320e-07 6.7364916e-15 6.5362316e-12 1.2907130e-09
 1.7144958e-16 9.9999988e-01], sum to 1.0000
[2019-04-03 23:19:22,926] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3435
[2019-04-03 23:19:23,050] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 26.0, 25.290245273735, 0.5044699554772211, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1360800.0000, 
sim time next is 1361400.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 26.0, 25.33894602099891, 0.5109045262903821, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.4764542936288089, 0.96, 0.0, 0.0, 0.6666666666666666, 0.6115788350832426, 0.6703015087634606, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.65158963], dtype=float32), -0.23516266]. 
=============================================
[2019-04-03 23:19:34,731] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.2334812e-16 1.5784103e-06 1.6202796e-13 1.8944100e-10 1.6498069e-08
 3.8588267e-15 9.9999845e-01], sum to 1.0000
[2019-04-03 23:19:34,731] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4914
[2019-04-03 23:19:34,755] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.0, 82.00000000000001, 0.0, 0.0, 26.0, 25.45988316201126, 0.5550283084019634, 0.0, 1.0, 53152.93446353033], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1555800.0000, 
sim time next is 1556400.0000, 
raw observation next is [5.0, 82.0, 0.0, 0.0, 26.0, 25.45909044762929, 0.5589669481036421, 0.0, 1.0, 42457.21795152664], 
processed observation next is [1.0, 0.0, 0.6011080332409973, 0.82, 0.0, 0.0, 0.6666666666666666, 0.6215908706357741, 0.6863223160345474, 0.0, 1.0, 0.20217722834060306], 
reward next is 0.7978, 
noisyNet noise sample is [array([-0.9943272], dtype=float32), 1.1421994]. 
=============================================
[2019-04-03 23:19:37,009] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.4212905e-17 2.4451154e-08 8.0578524e-15 5.4578009e-12 1.0974320e-09
 3.7800946e-17 1.0000000e+00], sum to 1.0000
[2019-04-03 23:19:37,034] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4786
[2019-04-03 23:19:37,065] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.16666666666667, 59.33333333333334, 0.0, 0.0, 26.0, 26.53284822198952, 0.70295643602853, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1532400.0000, 
sim time next is 1533000.0000, 
raw observation next is [10.08333333333333, 59.66666666666666, 0.0, 0.0, 26.0, 26.43657216134831, 0.699008151533376, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.7419205909510619, 0.5966666666666666, 0.0, 0.0, 0.6666666666666666, 0.7030476801123591, 0.7330027171777921, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2637264], dtype=float32), 1.3020823]. 
=============================================
[2019-04-03 23:19:37,072] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[91.03461 ]
 [90.734985]
 [90.69452 ]
 [91.0715  ]
 [91.05737 ]], R is [[91.5641861 ]
 [91.64854431]
 [91.73206329]
 [91.81474304]
 [91.89659882]].
[2019-04-03 23:19:52,612] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.34111764e-15 3.35549475e-06 1.45228190e-13 4.19343615e-10
 1.41177114e-08 1.08453834e-15 9.99996662e-01], sum to 1.0000
[2019-04-03 23:19:52,676] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0023
[2019-04-03 23:19:52,818] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 88.0, 0.0, 0.0, 26.0, 23.44688152235361, 0.3056189696940574, 1.0, 1.0, 198040.7045602175], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1709400.0000, 
sim time next is 1710000.0000, 
raw observation next is [1.1, 88.0, 0.0, 0.0, 26.0, 24.04933160176579, 0.4183857323130762, 1.0, 1.0, 199723.8606962624], 
processed observation next is [1.0, 0.8260869565217391, 0.49307479224376743, 0.88, 0.0, 0.0, 0.6666666666666666, 0.5041109668138158, 0.6394619107710254, 1.0, 1.0, 0.9510660033155351], 
reward next is 0.0489, 
noisyNet noise sample is [array([0.47742173], dtype=float32), -0.8570137]. 
=============================================
[2019-04-03 23:19:52,865] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[84.9797 ]
 [84.46468]
 [83.77365]
 [83.80756]
 [83.66634]], R is [[84.94044495]
 [84.14798737]
 [83.37069702]
 [83.43372345]
 [83.51043701]].
[2019-04-03 23:19:54,917] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.6806229e-12 5.8748707e-04 4.5597911e-11 3.1328966e-08 1.8092863e-06
 3.1773254e-12 9.9941075e-01], sum to 1.0000
[2019-04-03 23:19:54,917] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1399
[2019-04-03 23:19:54,976] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.733333333333333, 83.66666666666667, 23.5, 0.0, 26.0, 25.00007145026158, 0.3335152570494348, 0.0, 1.0, 39969.39642310943], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1788000.0000, 
sim time next is 1788600.0000, 
raw observation next is [-3.816666666666666, 82.83333333333334, 19.0, 0.0, 26.0, 25.00867076457775, 0.3312000662664439, 0.0, 1.0, 39539.95353786107], 
processed observation next is [0.0, 0.6956521739130435, 0.3568790397045245, 0.8283333333333335, 0.06333333333333334, 0.0, 0.6666666666666666, 0.5840558970481459, 0.6104000220888146, 0.0, 1.0, 0.18828549303743367], 
reward next is 0.8117, 
noisyNet noise sample is [array([-1.7783558], dtype=float32), 2.5019045]. 
=============================================
[2019-04-03 23:20:01,291] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-03 23:20:01,338] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-03 23:20:01,338] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:20:01,340] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run11
[2019-04-03 23:20:01,457] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-03 23:20:01,462] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-03 23:20:01,488] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:20:01,523] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run11
[2019-04-03 23:20:01,521] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:20:01,663] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run11
[2019-04-03 23:23:07,452] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7550 239871447.9954 1604.7371
[2019-04-03 23:23:27,191] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.5725 263429776.7709 1552.0052
[2019-04-03 23:23:31,956] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.7010 275792795.1220 1233.4063
[2019-04-03 23:23:32,978] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 1000000, evaluation results [1000000.0, 7241.572491567361, 263429776.77085403, 1552.0052359268102, 7353.7550095456645, 239871447.9954139, 1604.737059845423, 7182.700975609721, 275792795.1219553, 1233.4062756532364]
[2019-04-03 23:23:42,666] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1568089e-12 1.9623154e-04 1.8511203e-11 8.7760323e-09 6.1606238e-07
 1.8645493e-12 9.9980313e-01], sum to 1.0000
[2019-04-03 23:23:42,666] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2555
[2019-04-03 23:23:42,733] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 86.0, 0.0, 0.0, 26.0, 25.02800912316246, 0.2376211271386086, 0.0, 1.0, 45327.08381953599], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1886400.0000, 
sim time next is 1887000.0000, 
raw observation next is [-5.600000000000001, 85.5, 0.0, 0.0, 26.0, 24.99774732715017, 0.2302545124205996, 0.0, 1.0, 44909.35991123752], 
processed observation next is [0.0, 0.8695652173913043, 0.3074792243767313, 0.855, 0.0, 0.0, 0.6666666666666666, 0.5831456105958474, 0.5767515041401999, 0.0, 1.0, 0.21385409481541678], 
reward next is 0.7861, 
noisyNet noise sample is [array([1.628941], dtype=float32), 0.69915676]. 
=============================================
[2019-04-03 23:23:42,747] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[76.44907 ]
 [76.52149 ]
 [76.55088 ]
 [76.488205]
 [76.31263 ]], R is [[76.39073944]
 [76.41098785]
 [76.42159271]
 [76.39849854]
 [76.27861023]].
[2019-04-03 23:23:42,749] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.0854800e-12 1.5122486e-04 4.5441928e-11 1.9844832e-08 5.3844877e-07
 3.2570400e-12 9.9984813e-01], sum to 1.0000
[2019-04-03 23:23:42,750] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7507
[2019-04-03 23:23:42,824] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 77.66666666666667, 36.16666666666666, 0.0, 26.0, 25.00869769647963, 0.2638647949814155, 0.0, 1.0, 46272.28448411427], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1873200.0000, 
sim time next is 1873800.0000, 
raw observation next is [-4.5, 79.0, 29.0, 0.0, 26.0, 25.04283730785983, 0.2619168675028019, 0.0, 1.0, 26567.40548760088], 
processed observation next is [0.0, 0.6956521739130435, 0.3379501385041552, 0.79, 0.09666666666666666, 0.0, 0.6666666666666666, 0.5869031089883192, 0.587305622500934, 0.0, 1.0, 0.12651145470286135], 
reward next is 0.8735, 
noisyNet noise sample is [array([0.63909465], dtype=float32), 0.18506849]. 
=============================================
[2019-04-03 23:23:46,648] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.7916217e-14 5.8540383e-05 3.8819894e-13 2.6553073e-09 1.3397189e-08
 5.8671458e-14 9.9994147e-01], sum to 1.0000
[2019-04-03 23:23:46,648] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2147
[2019-04-03 23:23:46,686] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.633333333333333, 79.66666666666667, 202.3333333333333, 69.66666666666666, 26.0, 25.84980111235929, 0.3975029376913453, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2110800.0000, 
sim time next is 2111400.0000, 
raw observation next is [-7.55, 78.5, 208.0, 60.0, 26.0, 25.84745025064394, 0.3909335675169063, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.25346260387811637, 0.785, 0.6933333333333334, 0.06629834254143646, 0.6666666666666666, 0.6539541875536617, 0.6303111891723021, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.96475303], dtype=float32), -1.3789102]. 
=============================================
[2019-04-03 23:23:57,346] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.9987235e-14 4.2429524e-06 6.1377015e-12 2.1581832e-09 2.9096325e-08
 1.5310321e-13 9.9999571e-01], sum to 1.0000
[2019-04-03 23:23:57,346] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3019
[2019-04-03 23:23:57,369] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.7, 83.66666666666667, 0.0, 0.0, 26.0, 24.43500750633716, 0.1779633877250307, 0.0, 1.0, 42539.22173990975], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1987800.0000, 
sim time next is 1988400.0000, 
raw observation next is [-5.8, 84.33333333333334, 0.0, 0.0, 26.0, 24.405637359944, 0.1705307826935973, 0.0, 1.0, 42430.46455786597], 
processed observation next is [1.0, 0.0, 0.30193905817174516, 0.8433333333333334, 0.0, 0.0, 0.6666666666666666, 0.5338031133286666, 0.5568435942311991, 0.0, 1.0, 0.20204983122793319], 
reward next is 0.7980, 
noisyNet noise sample is [array([-0.07224911], dtype=float32), 0.16397655]. 
=============================================
[2019-04-03 23:23:58,740] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.7297811e-15 1.2299394e-05 1.0228909e-13 2.3687812e-09 2.1065333e-08
 1.7671230e-14 9.9998772e-01], sum to 1.0000
[2019-04-03 23:23:58,750] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3286
[2019-04-03 23:23:58,859] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.4, 79.00000000000001, 150.6666666666667, 0.0, 26.0, 24.90570432781312, 0.2581623766116531, 1.0, 1.0, 198065.1390629227], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2034600.0000, 
sim time next is 2035200.0000, 
raw observation next is [-4.300000000000001, 79.0, 149.3333333333333, 0.0, 26.0, 24.24189828603048, 0.3108668898766903, 1.0, 1.0, 200301.7797366625], 
processed observation next is [1.0, 0.5652173913043478, 0.34349030470914127, 0.79, 0.4977777777777776, 0.0, 0.6666666666666666, 0.52015819050254, 0.6036222966255634, 1.0, 1.0, 0.9538179987460118], 
reward next is 0.0462, 
noisyNet noise sample is [array([-0.29858172], dtype=float32), 0.54150945]. 
=============================================
[2019-04-03 23:23:59,257] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.8700823e-15 5.6323029e-06 9.5786443e-13 7.6997220e-10 1.0531949e-08
 4.8981347e-14 9.9999440e-01], sum to 1.0000
[2019-04-03 23:23:59,257] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5065
[2019-04-03 23:23:59,309] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 85.33333333333334, 0.0, 0.0, 26.0, 25.39646905840042, 0.432897500113697, 0.0, 1.0, 30827.17939336296], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2063400.0000, 
sim time next is 2064000.0000, 
raw observation next is [-3.9, 84.66666666666667, 0.0, 0.0, 26.0, 25.39742581294319, 0.4291521116336264, 0.0, 1.0, 36209.41150784995], 
processed observation next is [1.0, 0.9130434782608695, 0.3545706371191136, 0.8466666666666667, 0.0, 0.0, 0.6666666666666666, 0.6164521510785992, 0.6430507038778754, 0.0, 1.0, 0.17242576908499976], 
reward next is 0.8276, 
noisyNet noise sample is [array([-0.31739473], dtype=float32), 0.7773491]. 
=============================================
[2019-04-03 23:23:59,320] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[78.67738 ]
 [78.700005]
 [78.5953  ]
 [78.390625]
 [78.13423 ]], R is [[78.77779388]
 [78.84321594]
 [78.85309601]
 [78.84091949]
 [78.79013062]].
[2019-04-03 23:24:08,091] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.8102704e-15 5.6698968e-06 1.0122676e-12 1.3304836e-09 2.0471973e-07
 2.9014002e-14 9.9999404e-01], sum to 1.0000
[2019-04-03 23:24:08,091] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1176
[2019-04-03 23:24:08,133] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 25.13658860531231, 0.4101263994199397, 0.0, 1.0, 73827.09272208522], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2147400.0000, 
sim time next is 2148000.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 25.18021359989559, 0.4149572883716381, 0.0, 1.0, 53247.26357224202], 
processed observation next is [1.0, 0.8695652173913043, 0.30747922437673136, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5983511333246326, 0.6383190961238794, 0.0, 1.0, 0.25355839796305724], 
reward next is 0.7464, 
noisyNet noise sample is [array([0.23153216], dtype=float32), -1.4463636]. 
=============================================
[2019-04-03 23:24:08,145] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[78.026306]
 [78.51322 ]
 [80.1539  ]
 [82.39792 ]
 [85.03831 ]], R is [[78.20877075]
 [78.07512665]
 [77.77129364]
 [77.65071106]
 [77.28638458]].
[2019-04-03 23:24:10,053] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.34414349e-13 1.08534456e-04 1.50705889e-11 6.23169338e-09
 2.64676515e-07 1.78508727e-12 9.99891162e-01], sum to 1.0000
[2019-04-03 23:24:10,054] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2749
[2019-04-03 23:24:10,114] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.366666666666667, 76.0, 0.0, 0.0, 26.0, 24.06490886262169, 0.06490387675997412, 0.0, 1.0, 42038.95214220173], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2176800.0000, 
sim time next is 2177400.0000, 
raw observation next is [-6.283333333333333, 75.5, 0.0, 0.0, 26.0, 24.02153662052142, 0.06009803188699387, 0.0, 1.0, 42018.70273493353], 
processed observation next is [1.0, 0.17391304347826086, 0.288550323176362, 0.755, 0.0, 0.0, 0.6666666666666666, 0.501794718376785, 0.5200326772956646, 0.0, 1.0, 0.20008906064254064], 
reward next is 0.7999, 
noisyNet noise sample is [array([-1.2767271], dtype=float32), 1.9050579]. 
=============================================
[2019-04-03 23:24:15,044] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.0958258e-14 1.2082101e-05 1.0507540e-12 1.7840714e-09 2.1467626e-08
 1.9720719e-14 9.9998796e-01], sum to 1.0000
[2019-04-03 23:24:15,044] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0016
[2019-04-03 23:24:15,115] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 69.5, 0.0, 0.0, 26.0, 25.13777204761404, 0.4022660396490542, 0.0, 1.0, 91065.45751617581], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2233800.0000, 
sim time next is 2234400.0000, 
raw observation next is [-5.0, 69.0, 0.0, 0.0, 26.0, 25.1715305931551, 0.4084202687337736, 0.0, 1.0, 61583.06930773939], 
processed observation next is [1.0, 0.8695652173913043, 0.32409972299168976, 0.69, 0.0, 0.0, 0.6666666666666666, 0.5976275494295917, 0.6361400895779245, 0.0, 1.0, 0.29325271098923517], 
reward next is 0.7067, 
noisyNet noise sample is [array([-0.12184199], dtype=float32), 0.52769905]. 
=============================================
[2019-04-03 23:24:35,476] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.3804450e-15 2.6171597e-06 2.7899780e-13 8.5636720e-11 6.1797754e-09
 6.8865248e-15 9.9999738e-01], sum to 1.0000
[2019-04-03 23:24:35,490] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5965
[2019-04-03 23:24:35,532] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 56.0, 0.0, 0.0, 26.0, 25.37325981555664, 0.4120475456572439, 0.0, 1.0, 55556.97800842534], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2584800.0000, 
sim time next is 2585400.0000, 
raw observation next is [-2.983333333333333, 56.5, 0.0, 0.0, 26.0, 25.36408074404894, 0.3767452981657649, 0.0, 1.0, 49303.30715189676], 
processed observation next is [1.0, 0.9565217391304348, 0.37996306555863346, 0.565, 0.0, 0.0, 0.6666666666666666, 0.6136733953374117, 0.625581766055255, 0.0, 1.0, 0.2347776531042703], 
reward next is 0.7652, 
noisyNet noise sample is [array([-1.0257666], dtype=float32), 1.2495883]. 
=============================================
[2019-04-03 23:24:36,360] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [9.6872468e-14 4.5033208e-05 8.4935522e-12 1.9882591e-09 8.5708855e-08
 9.7743352e-13 9.9995482e-01], sum to 1.0000
[2019-04-03 23:24:36,360] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.5241
[2019-04-03 23:24:36,390] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 40.0, 0.0, 0.0, 26.0, 25.14670413796485, 0.2158136650378818, 0.0, 1.0, 39139.22036059063], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2509200.0000, 
sim time next is 2509800.0000, 
raw observation next is [-1.7, 39.66666666666667, 0.0, 0.0, 26.0, 25.0851954958809, 0.2072828587262661, 0.0, 1.0, 39122.89675183674], 
processed observation next is [1.0, 0.043478260869565216, 0.4155124653739613, 0.3966666666666667, 0.0, 0.0, 0.6666666666666666, 0.590432957990075, 0.5690942862420887, 0.0, 1.0, 0.18629950834207973], 
reward next is 0.8137, 
noisyNet noise sample is [array([0.32377785], dtype=float32), -1.2629797]. 
=============================================
[2019-04-03 23:24:37,296] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.13511648e-15 1.18016715e-05 1.47187865e-14 7.22301038e-11
 2.87397284e-09 6.27540823e-16 9.99988198e-01], sum to 1.0000
[2019-04-03 23:24:37,297] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6327
[2019-04-03 23:24:37,306] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 47.0, 182.5, 58.0, 26.0, 25.71937677261337, 0.3013500748799102, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2545200.0000, 
sim time next is 2545800.0000, 
raw observation next is [-0.3166666666666667, 45.66666666666667, 199.0, 62.33333333333333, 26.0, 25.71099739083844, 0.3000160421721145, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.45383194829178214, 0.4566666666666667, 0.6633333333333333, 0.06887661141804788, 0.6666666666666666, 0.6425831159032033, 0.6000053473907049, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4468738], dtype=float32), -0.45935342]. 
=============================================
[2019-04-03 23:24:41,655] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.4484523e-14 1.3697975e-04 1.8423253e-12 1.0883418e-09 8.0334829e-08
 8.1034483e-14 9.9986291e-01], sum to 1.0000
[2019-04-03 23:24:41,655] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7792
[2019-04-03 23:24:41,680] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 74.0, 0.0, 0.0, 26.0, 24.81892547245387, 0.2215534641534634, 0.0, 1.0, 41722.76989112359], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2601600.0000, 
sim time next is 2602200.0000, 
raw observation next is [-5.0, 74.0, 0.0, 0.0, 26.0, 24.82882980289735, 0.2116833339772745, 0.0, 1.0, 41765.23470173074], 
processed observation next is [1.0, 0.08695652173913043, 0.32409972299168976, 0.74, 0.0, 0.0, 0.6666666666666666, 0.5690691502414458, 0.5705611113257582, 0.0, 1.0, 0.19888207000824162], 
reward next is 0.8011, 
noisyNet noise sample is [array([-1.0916667], dtype=float32), -0.21204966]. 
=============================================
[2019-04-03 23:25:10,887] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.7468193e-12 4.4010227e-04 2.5374447e-11 1.3743735e-08 1.0095025e-06
 7.5584773e-12 9.9955887e-01], sum to 1.0000
[2019-04-03 23:25:10,887] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4472
[2019-04-03 23:25:10,924] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 60.00000000000001, 0.0, 0.0, 26.0, 25.0159302627938, 0.3201074074893028, 0.0, 1.0, 55944.70328489619], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3003600.0000, 
sim time next is 3004200.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.0058995219025, 0.3208773647381268, 0.0, 1.0, 48896.48988429349], 
processed observation next is [0.0, 0.782608695652174, 0.40720221606648205, 0.6, 0.0, 0.0, 0.6666666666666666, 0.5838249601585416, 0.6069591215793756, 0.0, 1.0, 0.2328404280204452], 
reward next is 0.7672, 
noisyNet noise sample is [array([1.5346006], dtype=float32), -2.1875165]. 
=============================================
[2019-04-03 23:25:22,633] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.6416646e-17 2.4420847e-06 1.2364773e-15 2.4230581e-11 2.3076187e-10
 6.2218048e-18 9.9999762e-01], sum to 1.0000
[2019-04-03 23:25:22,639] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9412
[2019-04-03 23:25:22,676] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.733333333333333, 99.33333333333334, 79.66666666666666, 649.0, 26.0, 27.14119391127963, 0.9364839226548725, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3166800.0000, 
sim time next is 3167400.0000, 
raw observation next is [6.666666666666666, 99.16666666666666, 75.33333333333334, 619.0, 26.0, 27.37106994887197, 0.9628871464417667, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.6472760849492153, 0.9916666666666666, 0.2511111111111111, 0.6839779005524862, 0.6666666666666666, 0.7809224957393308, 0.8209623821472555, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.34115362], dtype=float32), 0.7935741]. 
=============================================
[2019-04-03 23:25:40,426] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.3757157e-14 1.4574979e-04 7.2056506e-12 1.5181117e-09 6.2012539e-08
 8.1109537e-14 9.9985409e-01], sum to 1.0000
[2019-04-03 23:25:40,426] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1634
[2019-04-03 23:25:40,442] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 77.83333333333334, 0.0, 0.0, 26.0, 25.43851762406948, 0.4267606461477373, 0.0, 1.0, 76676.83516885263], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3463800.0000, 
sim time next is 3464400.0000, 
raw observation next is [1.0, 76.66666666666667, 0.0, 0.0, 26.0, 25.40022392030689, 0.4325522395843016, 0.0, 1.0, 77685.87699245302], 
processed observation next is [1.0, 0.08695652173913043, 0.4903047091412743, 0.7666666666666667, 0.0, 0.0, 0.6666666666666666, 0.6166853266922407, 0.6441840798614339, 0.0, 1.0, 0.3699327475831096], 
reward next is 0.6301, 
noisyNet noise sample is [array([1.2470536], dtype=float32), -1.7994363]. 
=============================================
[2019-04-03 23:25:43,500] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.4823679e-14 3.3507327e-05 2.4878656e-12 3.6674399e-09 3.2205076e-08
 1.3676437e-13 9.9996650e-01], sum to 1.0000
[2019-04-03 23:25:43,500] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0686
[2019-04-03 23:25:43,514] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.3866062058322, 0.4662370066585612, 0.0, 1.0, 37929.86392013041], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3543000.0000, 
sim time next is 3543600.0000, 
raw observation next is [-2.0, 60.00000000000001, 0.0, 0.0, 26.0, 25.37532692237849, 0.4708565347600633, 0.0, 1.0, 45557.13634902212], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.6000000000000001, 0.0, 0.0, 0.6666666666666666, 0.6146105768648743, 0.6569521782533544, 0.0, 1.0, 0.21693874451915296], 
reward next is 0.7831, 
noisyNet noise sample is [array([-1.4820299], dtype=float32), 1.0058175]. 
=============================================
[2019-04-03 23:25:56,866] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.7324740e-14 9.1016955e-05 1.6627780e-12 1.2874510e-09 3.4306250e-08
 1.5620346e-13 9.9990892e-01], sum to 1.0000
[2019-04-03 23:25:56,869] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4474
[2019-04-03 23:25:56,900] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.166666666666667, 62.33333333333333, 0.0, 0.0, 26.0, 25.14967584870291, 0.4326464126926756, 0.0, 1.0, 75789.48968527013], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3700200.0000, 
sim time next is 3700800.0000, 
raw observation next is [3.0, 63.0, 0.0, 0.0, 26.0, 25.41501302063403, 0.4591375529130194, 0.0, 1.0, 29232.25621517696], 
processed observation next is [0.0, 0.8695652173913043, 0.5457063711911359, 0.63, 0.0, 0.0, 0.6666666666666666, 0.6179177517195026, 0.6530458509710065, 0.0, 1.0, 0.13920122007227123], 
reward next is 0.8608, 
noisyNet noise sample is [array([0.43407878], dtype=float32), 0.46416196]. 
=============================================
[2019-04-03 23:26:00,423] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.9135442e-15 2.9280138e-06 1.2259753e-14 1.3961159e-10 8.6945995e-10
 3.6039566e-16 9.9999702e-01], sum to 1.0000
[2019-04-03 23:26:00,423] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5342
[2019-04-03 23:26:00,455] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6666666666666667, 58.5, 117.0, 830.6666666666667, 26.0, 26.62593362447512, 0.6711748041064179, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3845400.0000, 
sim time next is 3846000.0000, 
raw observation next is [-0.3333333333333334, 57.00000000000001, 117.0, 832.8333333333334, 26.0, 26.67164117318834, 0.6743821986860853, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.4533702677747, 0.5700000000000001, 0.39, 0.9202578268876612, 0.6666666666666666, 0.7226367644323618, 0.7247940662286951, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1415151], dtype=float32), 1.5334951]. 
=============================================
[2019-04-03 23:26:00,470] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[92.19287]
 [92.19605]
 [92.21304]
 [92.21159]
 [92.23473]], R is [[92.27436066]
 [92.35161591]
 [92.42810059]
 [92.50382233]
 [92.57878876]].
[2019-04-03 23:26:04,181] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [5.0335488e-15 2.0738800e-05 8.1308408e-13 3.7163753e-10 1.2153547e-08
 1.0958321e-13 9.9997926e-01], sum to 1.0000
[2019-04-03 23:26:04,182] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.9863
[2019-04-03 23:26:04,198] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 58.33333333333334, 0.0, 0.0, 26.0, 25.26466847281607, 0.4505247196142184, 0.0, 1.0, 91285.47734452382], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3883200.0000, 
sim time next is 3883800.0000, 
raw observation next is [-1.0, 59.16666666666666, 0.0, 0.0, 26.0, 25.24928701609894, 0.4550023045461655, 0.0, 1.0, 58383.4542314318], 
processed observation next is [1.0, 0.9565217391304348, 0.4349030470914128, 0.5916666666666666, 0.0, 0.0, 0.6666666666666666, 0.6041072513415783, 0.6516674348487218, 0.0, 1.0, 0.2780164487211038], 
reward next is 0.7220, 
noisyNet noise sample is [array([0.01021926], dtype=float32), 0.33360687]. 
=============================================
[2019-04-03 23:26:06,999] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.3987410e-13 1.5392483e-04 1.3155819e-10 1.0854727e-08 5.7130615e-07
 1.2672544e-11 9.9984539e-01], sum to 1.0000
[2019-04-03 23:26:06,999] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8188
[2019-04-03 23:26:07,011] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.5, 66.0, 0.0, 0.0, 26.0, 24.04086703315595, 0.08159332298541341, 0.0, 1.0, 43725.93418656204], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3990600.0000, 
sim time next is 3991200.0000, 
raw observation next is [-12.66666666666667, 67.0, 0.0, 0.0, 26.0, 23.97095304881544, 0.06718834099779013, 0.0, 1.0, 43759.12741309486], 
processed observation next is [1.0, 0.17391304347826086, 0.11172668513388727, 0.67, 0.0, 0.0, 0.6666666666666666, 0.49757942073461986, 0.52239611366593, 0.0, 1.0, 0.20837679720521363], 
reward next is 0.7916, 
noisyNet noise sample is [array([-1.2481424], dtype=float32), -1.4303216]. 
=============================================
[2019-04-03 23:26:15,580] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.5555099e-13 8.7937093e-05 1.4271561e-11 3.6508589e-09 3.8550391e-07
 6.1377449e-13 9.9991167e-01], sum to 1.0000
[2019-04-03 23:26:15,591] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.9657
[2019-04-03 23:26:15,606] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 41.0, 0.0, 0.0, 26.0, 25.03454467031123, 0.3193784430191782, 0.0, 1.0, 40793.35965450187], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4066200.0000, 
sim time next is 4066800.0000, 
raw observation next is [-6.0, 41.0, 0.0, 0.0, 26.0, 25.09904648751006, 0.3175581198005095, 0.0, 1.0, 40797.76069653118], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.41, 0.0, 0.0, 0.6666666666666666, 0.5915872072925049, 0.6058527066001699, 0.0, 1.0, 0.19427505093586275], 
reward next is 0.8057, 
noisyNet noise sample is [array([-0.15640628], dtype=float32), 0.8399872]. 
=============================================
[2019-04-03 23:26:22,134] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.3074204e-14 7.5043034e-05 2.1127538e-12 2.3542506e-09 6.6646983e-08
 4.4352339e-13 9.9992478e-01], sum to 1.0000
[2019-04-03 23:26:22,135] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3218
[2019-04-03 23:26:22,159] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.833333333333333, 45.5, 0.0, 0.0, 26.0, 25.42013893024764, 0.3460870380248397, 0.0, 1.0, 29996.95588739309], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4240200.0000, 
sim time next is 4240800.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 26.0, 25.42499621370843, 0.3441049880603342, 0.0, 1.0, 31800.10398624177], 
processed observation next is [0.0, 0.08695652173913043, 0.5457063711911359, 0.45, 0.0, 0.0, 0.6666666666666666, 0.6187496844757024, 0.6147016626867781, 0.0, 1.0, 0.1514290666011513], 
reward next is 0.8486, 
noisyNet noise sample is [array([-0.52773637], dtype=float32), 0.032163467]. 
=============================================
[2019-04-03 23:26:22,408] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.5937396e-14 6.7217497e-04 6.5336923e-13 1.2799396e-09 6.3575214e-08
 3.7060516e-13 9.9932778e-01], sum to 1.0000
[2019-04-03 23:26:22,409] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3216
[2019-04-03 23:26:22,434] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.833333333333334, 54.83333333333334, 176.3333333333333, 676.6666666666666, 26.0, 25.30452387480786, 0.3994697141212191, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4272600.0000, 
sim time next is 4273200.0000, 
raw observation next is [5.0, 55.0, 162.5, 713.0, 26.0, 25.27078159616473, 0.3981420873379148, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.6011080332409973, 0.55, 0.5416666666666666, 0.7878453038674034, 0.6666666666666666, 0.6058984663470609, 0.6327140291126382, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.05881973], dtype=float32), 0.62483025]. 
=============================================
[2019-04-03 23:26:26,401] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.0654732e-14 2.4765325e-04 1.5606709e-12 5.6541980e-09 1.7314119e-07
 7.2031817e-13 9.9975222e-01], sum to 1.0000
[2019-04-03 23:26:26,402] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4825
[2019-04-03 23:26:26,415] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.833333333333333, 47.83333333333333, 0.0, 0.0, 26.0, 25.41524508937967, 0.3594713114042716, 0.0, 1.0, 36195.2803498601], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4233000.0000, 
sim time next is 4233600.0000, 
raw observation next is [2.0, 48.0, 0.0, 0.0, 26.0, 25.42877500954341, 0.3582445735951754, 0.0, 1.0, 30646.29240663981], 
processed observation next is [0.0, 0.0, 0.518005540166205, 0.48, 0.0, 0.0, 0.6666666666666666, 0.6190645841286176, 0.6194148578650585, 0.0, 1.0, 0.14593472574590385], 
reward next is 0.8541, 
noisyNet noise sample is [array([0.17461334], dtype=float32), 0.5841901]. 
=============================================
[2019-04-03 23:26:26,489] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.9677233e-13 2.0572541e-03 1.4081541e-11 2.5755419e-08 4.6147562e-07
 8.8869065e-13 9.9794227e-01], sum to 1.0000
[2019-04-03 23:26:26,493] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8351
[2019-04-03 23:26:26,505] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 47.0, 0.0, 0.0, 26.0, 25.38563914630898, 0.3276470238287499, 0.0, 1.0, 40796.26261812315], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4253400.0000, 
sim time next is 4254000.0000, 
raw observation next is [3.0, 47.66666666666666, 0.0, 0.0, 26.0, 25.37246314451443, 0.3287764901010823, 0.0, 1.0, 47221.22335098425], 
processed observation next is [0.0, 0.21739130434782608, 0.5457063711911359, 0.47666666666666657, 0.0, 0.0, 0.6666666666666666, 0.6143719287095358, 0.6095921633670275, 0.0, 1.0, 0.22486296833802022], 
reward next is 0.7751, 
noisyNet noise sample is [array([-0.59404415], dtype=float32), -0.14576197]. 
=============================================
[2019-04-03 23:26:26,518] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[85.21281 ]
 [85.183   ]
 [85.21174 ]
 [85.271904]
 [85.29981 ]], R is [[85.20326233]
 [85.15695953]
 [85.1515274 ]
 [85.15410614]
 [85.10572815]].
[2019-04-03 23:26:30,270] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.4958881e-16 2.3718831e-05 1.7001096e-14 2.3200544e-10 7.1309025e-09
 1.2008209e-15 9.9997628e-01], sum to 1.0000
[2019-04-03 23:26:30,271] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.6260
[2019-04-03 23:26:30,287] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.383333333333333, 44.5, 109.6666666666667, 711.3333333333333, 26.0, 27.10255207555014, 0.7361271391331821, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4355400.0000, 
sim time next is 4356000.0000, 
raw observation next is [10.0, 42.0, 111.0, 728.5, 26.0, 27.2308330354852, 0.7617401068374582, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.739612188365651, 0.42, 0.37, 0.8049723756906078, 0.6666666666666666, 0.7692360862904334, 0.7539133689458194, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.21823783], dtype=float32), -0.3175177]. 
=============================================
[2019-04-03 23:26:30,320] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[99.16184]
 [98.79577]
 [98.39135]
 [97.98252]
 [97.73275]], R is [[99.43519592]
 [99.44084167]
 [99.44643402]
 [99.45197296]
 [99.45745087]].
[2019-04-03 23:26:31,119] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [9.5687989e-16 2.4180065e-06 2.3181145e-13 9.3243337e-11 7.6513214e-09
 3.2716692e-15 9.9999762e-01], sum to 1.0000
[2019-04-03 23:26:31,123] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9491
[2019-04-03 23:26:31,144] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 86.0, 196.5, 73.0, 26.0, 26.50147960688597, 0.6608297871942309, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4446000.0000, 
sim time next is 4446600.0000, 
raw observation next is [1.0, 86.0, 178.3333333333333, 48.66666666666666, 26.0, 26.51204951618539, 0.6588382827555382, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4903047091412743, 0.86, 0.5944444444444443, 0.05377532228360957, 0.6666666666666666, 0.7093374596821157, 0.7196127609185128, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.7822531], dtype=float32), 0.43895388]. 
=============================================
[2019-04-03 23:26:31,647] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.3854065e-15 6.1866274e-05 3.7114725e-12 4.1691225e-10 4.2611539e-08
 4.3949199e-14 9.9993813e-01], sum to 1.0000
[2019-04-03 23:26:31,650] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0253
[2019-04-03 23:26:31,668] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.75, 67.0, 0.0, 0.0, 26.0, 25.61884752016483, 0.5637210744817588, 0.0, 1.0, 66336.89321180373], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4419000.0000, 
sim time next is 4419600.0000, 
raw observation next is [4.666666666666667, 67.0, 0.0, 0.0, 26.0, 25.64951033837178, 0.5749676670964825, 0.0, 1.0, 27399.97272783117], 
processed observation next is [1.0, 0.13043478260869565, 0.5918744228993538, 0.67, 0.0, 0.0, 0.6666666666666666, 0.637459194864315, 0.6916558890321608, 0.0, 1.0, 0.13047606060871986], 
reward next is 0.8695, 
noisyNet noise sample is [array([-0.9592929], dtype=float32), -0.33736175]. 
=============================================
[2019-04-03 23:26:34,573] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.5978583e-15 5.5333612e-06 1.5921775e-13 8.0147261e-10 4.6513633e-09
 4.2194273e-15 9.9999452e-01], sum to 1.0000
[2019-04-03 23:26:34,574] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.6114
[2019-04-03 23:26:34,632] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 92.0, 196.0, 6.0, 26.0, 25.37491296073489, 0.471697846232259, 1.0, 1.0, 117234.3890815533], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4455000.0000, 
sim time next is 4455600.0000, 
raw observation next is [0.0, 92.0, 177.5, 5.0, 26.0, 24.64752559367314, 0.4483159071917661, 1.0, 1.0, 196265.7350384626], 
processed observation next is [1.0, 0.5652173913043478, 0.46260387811634357, 0.92, 0.5916666666666667, 0.0055248618784530384, 0.6666666666666666, 0.5539604661394284, 0.6494386357305887, 1.0, 1.0, 0.9345987382783932], 
reward next is 0.0654, 
noisyNet noise sample is [array([-0.57978296], dtype=float32), 0.10992256]. 
=============================================
[2019-04-03 23:26:34,644] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.4191124e-15 6.9178335e-05 3.6638565e-12 8.5992768e-10 1.1042144e-07
 1.3054012e-13 9.9993074e-01], sum to 1.0000
[2019-04-03 23:26:34,653] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0511
[2019-04-03 23:26:34,671] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 80.0, 0.0, 0.0, 26.0, 25.61142344079011, 0.5079762253383521, 0.0, 1.0, 18736.29211033228], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4431600.0000, 
sim time next is 4432200.0000, 
raw observation next is [2.0, 80.0, 0.0, 0.0, 26.0, 25.58052107578434, 0.4988905050065064, 0.0, 1.0, 29792.12513343415], 
processed observation next is [1.0, 0.30434782608695654, 0.518005540166205, 0.8, 0.0, 0.0, 0.6666666666666666, 0.631710089648695, 0.6662968350021689, 0.0, 1.0, 0.14186726254016263], 
reward next is 0.8581, 
noisyNet noise sample is [array([-0.04376118], dtype=float32), 0.5906514]. 
=============================================
[2019-04-03 23:26:36,161] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.2381270e-17 3.7504667e-06 4.3398440e-15 8.4280291e-12 3.2010095e-10
 2.7211739e-17 9.9999630e-01], sum to 1.0000
[2019-04-03 23:26:36,162] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3030
[2019-04-03 23:26:36,176] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.93333333333333, 28.16666666666667, 118.6666666666667, 844.6666666666667, 26.0, 28.25260508667546, 1.036907687250028, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4363800.0000, 
sim time next is 4364400.0000, 
raw observation next is [14.86666666666667, 28.33333333333334, 118.3333333333333, 848.8333333333334, 26.0, 28.32193685829134, 1.041750234749253, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.8744228993536475, 0.2833333333333334, 0.3944444444444443, 0.9379373848987109, 0.6666666666666666, 0.8601614048576117, 0.8472500782497511, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.78231], dtype=float32), -0.6633742]. 
=============================================
[2019-04-03 23:26:42,358] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [3.48704067e-15 2.17996028e-04 1.64595063e-13 7.60213448e-10
 5.23592476e-08 1.18603984e-14 9.99782026e-01], sum to 1.0000
[2019-04-03 23:26:42,380] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.5482
[2019-04-03 23:26:42,429] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 117.0, 33.0, 26.0, 26.06735149829141, 0.5223334223516364, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4525200.0000, 
sim time next is 4525800.0000, 
raw observation next is [0.1666666666666667, 70.16666666666667, 119.0, 22.0, 26.0, 26.17775582576018, 0.5282433875539664, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4672206832871654, 0.7016666666666667, 0.39666666666666667, 0.02430939226519337, 0.6666666666666666, 0.6814796521466816, 0.6760811291846555, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.61101836], dtype=float32), -0.059091587]. 
=============================================
[2019-04-03 23:26:44,136] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0027271e-13 1.3851930e-04 1.7754392e-11 3.1122065e-09 1.0707285e-07
 4.0988686e-13 9.9986136e-01], sum to 1.0000
[2019-04-03 23:26:44,136] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3606
[2019-04-03 23:26:44,152] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.8666666666666667, 71.0, 0.0, 0.0, 26.0, 25.31380539449156, 0.407085992037541, 0.0, 1.0, 41318.46616585369], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4512000.0000, 
sim time next is 4512600.0000, 
raw observation next is [-0.9, 71.0, 0.0, 0.0, 26.0, 25.30036430519793, 0.402185301200402, 0.0, 1.0, 41236.76931424804], 
processed observation next is [1.0, 0.21739130434782608, 0.43767313019390586, 0.71, 0.0, 0.0, 0.6666666666666666, 0.6083636920998275, 0.6340617670668006, 0.0, 1.0, 0.19636556816308592], 
reward next is 0.8036, 
noisyNet noise sample is [array([0.9640753], dtype=float32), -1.1605078]. 
=============================================
[2019-04-03 23:26:51,724] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.81848673e-14 1.21143494e-04 1.19218966e-11 4.64887373e-09
 7.04828338e-08 4.86298068e-13 9.99878764e-01], sum to 1.0000
[2019-04-03 23:26:51,724] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1062
[2019-04-03 23:26:51,771] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.3333333333333333, 94.66666666666667, 0.0, 0.0, 26.0, 25.63837042144327, 0.4681741599819012, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4681200.0000, 
sim time next is 4681800.0000, 
raw observation next is [-0.5, 96.0, 0.0, 0.0, 26.0, 25.60672308190665, 0.4594403639491773, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.44875346260387816, 0.96, 0.0, 0.0, 0.6666666666666666, 0.6338935901588876, 0.6531467879830591, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.88097453], dtype=float32), -0.4532737]. 
=============================================
[2019-04-03 23:26:53,411] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.7072543e-15 1.7383950e-06 1.6749007e-13 5.3191579e-10 6.1127250e-09
 1.1111742e-15 9.9999821e-01], sum to 1.0000
[2019-04-03 23:26:53,411] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9215
[2019-04-03 23:26:53,419] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.333333333333333, 52.33333333333333, 0.0, 0.0, 26.0, 25.84990570753864, 0.5883495763847781, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4650000.0000, 
sim time next is 4650600.0000, 
raw observation next is [2.166666666666667, 52.16666666666667, 0.0, 0.0, 26.0, 25.76103524284058, 0.570393819109121, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.5226223453370269, 0.5216666666666667, 0.0, 0.0, 0.6666666666666666, 0.6467529369033818, 0.6901312730363737, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.86760896], dtype=float32), -0.7758504]. 
=============================================
[2019-04-03 23:27:08,291] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.01599976e-13 5.07877267e-04 3.86016002e-12 1.07725073e-08
 1.09045700e-07 4.44203187e-13 9.99491930e-01], sum to 1.0000
[2019-04-03 23:27:08,291] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4574
[2019-04-03 23:27:08,394] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.166666666666667, 79.5, 140.6666666666667, 419.6666666666667, 26.0, 25.47096252259678, 0.4561720957054088, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4783800.0000, 
sim time next is 4784400.0000, 
raw observation next is [-5.0, 77.0, 149.0, 420.0, 26.0, 25.63673161448141, 0.4661330764688014, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.32409972299168976, 0.77, 0.49666666666666665, 0.46408839779005523, 0.6666666666666666, 0.6363943012067841, 0.6553776921562672, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.88908434], dtype=float32), 0.9762577]. 
=============================================
[2019-04-03 23:27:15,748] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:27:15,748] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:27:15,765] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run9
[2019-04-03 23:27:20,826] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.78224058e-12 2.49712802e-02 1.03632804e-10 2.45072044e-07
 4.15961267e-06 2.81884169e-11 9.75024283e-01], sum to 1.0000
[2019-04-03 23:27:20,826] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2597
[2019-04-03 23:27:20,890] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 60.0, 0.0, 0.0, 26.0, 24.98591268464502, 0.2754351724122753, 0.0, 1.0, 39195.64292337556], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4849800.0000, 
sim time next is 4850400.0000, 
raw observation next is [-3.0, 60.00000000000001, 0.0, 0.0, 26.0, 24.95577334077072, 0.2695044497271959, 0.0, 1.0, 39210.17397524807], 
processed observation next is [0.0, 0.13043478260869565, 0.3795013850415513, 0.6000000000000001, 0.0, 0.0, 0.6666666666666666, 0.5796477783975599, 0.589834816575732, 0.0, 1.0, 0.18671511416784797], 
reward next is 0.8133, 
noisyNet noise sample is [array([-0.58793277], dtype=float32), -0.30275202]. 
=============================================
[2019-04-03 23:27:21,633] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.3655787e-15 3.2867316e-05 3.2317438e-13 3.4225467e-10 1.0966579e-08
 2.9665281e-15 9.9996710e-01], sum to 1.0000
[2019-04-03 23:27:21,637] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8625
[2019-04-03 23:27:21,677] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.333333333333333, 24.66666666666667, 122.8333333333333, 861.6666666666667, 26.0, 27.12927973873859, 0.7268121926877841, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4969200.0000, 
sim time next is 4969800.0000, 
raw observation next is [6.5, 24.5, 123.0, 865.0, 26.0, 27.0879095401415, 0.7270972552659473, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.6426592797783934, 0.245, 0.41, 0.9558011049723757, 0.6666666666666666, 0.7573257950117916, 0.7423657517553157, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.460831], dtype=float32), -1.4487947]. 
=============================================
[2019-04-03 23:27:22,004] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:27:22,004] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:27:22,007] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run9
[2019-04-03 23:27:22,104] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.4789324e-13 1.6373419e-03 1.3587352e-11 9.8641264e-09 3.5741692e-07
 1.3907066e-12 9.9836224e-01], sum to 1.0000
[2019-04-03 23:27:22,104] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0948
[2019-04-03 23:27:22,176] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 65.0, 174.0, 246.0, 26.0, 25.55207852721982, 0.385889235766004, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4870800.0000, 
sim time next is 4871400.0000, 
raw observation next is [-2.833333333333333, 64.16666666666667, 185.0, 223.6666666666667, 26.0, 25.55152837804557, 0.3804238923689029, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.3841181902123731, 0.6416666666666667, 0.6166666666666667, 0.24714548802946598, 0.6666666666666666, 0.6292940315037976, 0.6268079641229677, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.17546847], dtype=float32), 0.019500876]. 
=============================================
[2019-04-03 23:27:23,915] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.4245375e-12 2.6116215e-04 2.4549578e-11 1.6059548e-08 8.6738304e-08
 1.1125627e-12 9.9973875e-01], sum to 1.0000
[2019-04-03 23:27:23,923] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6928
[2019-04-03 23:27:23,957] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.6, 44.66666666666667, 273.5, 388.3333333333334, 26.0, 25.05381467985883, 0.3636064133305146, 0.0, 1.0, 18698.6501461058], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4886400.0000, 
sim time next is 4887000.0000, 
raw observation next is [1.7, 44.5, 272.0, 388.0, 26.0, 25.07122410488255, 0.3664622543073647, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.5096952908587258, 0.445, 0.9066666666666666, 0.4287292817679558, 0.6666666666666666, 0.5892686754068791, 0.6221540847691216, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.44206238], dtype=float32), -0.022609552]. 
=============================================
[2019-04-03 23:27:24,021] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[79.54334]
 [79.68502]
 [79.87079]
 [80.00526]
 [80.16102]], R is [[79.57032013]
 [79.68557739]
 [79.7931366 ]
 [79.84477234]
 [79.95729828]].
[2019-04-03 23:27:26,746] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.9767103e-15 5.0109567e-04 2.8675627e-13 7.2039769e-10 3.2501511e-08
 5.6692389e-14 9.9949884e-01], sum to 1.0000
[2019-04-03 23:27:26,746] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2189
[2019-04-03 23:27:26,830] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.833333333333333, 44.83333333333334, 62.00000000000001, 373.3333333333334, 26.0, 25.18660057762263, 0.2710633768556437, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4954200.0000, 
sim time next is 4954800.0000, 
raw observation next is [-1.666666666666667, 43.66666666666667, 77.5, 466.6666666666667, 26.0, 25.11934407536977, 0.2968361437051864, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.4164358264081256, 0.4366666666666667, 0.25833333333333336, 0.5156537753222836, 0.6666666666666666, 0.5932786729474809, 0.5989453812350621, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7856011], dtype=float32), -1.5333533]. 
=============================================
[2019-04-03 23:27:32,385] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:27:32,385] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:27:32,390] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run9
[2019-04-03 23:27:33,202] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:27:33,203] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:27:33,234] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run9
[2019-04-03 23:27:35,533] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:27:35,533] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:27:35,537] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run9
[2019-04-03 23:27:36,498] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:27:36,498] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:27:36,512] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run9
[2019-04-03 23:27:37,411] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.1777810e-14 3.0323108e-05 1.7460352e-13 9.5340758e-10 1.3834563e-08
 1.8723462e-14 9.9996972e-01], sum to 1.0000
[2019-04-03 23:27:37,412] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9285
[2019-04-03 23:27:37,457] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.8, 61.0, 134.5, 543.5, 26.0, 25.58265106951828, 0.4206730555227859, 1.0, 1.0, 38975.93741246271], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 133200.0000, 
sim time next is 133800.0000, 
raw observation next is [-7.616666666666667, 61.0, 136.0, 523.6666666666666, 26.0, 25.64351867395322, 0.4294120627822191, 1.0, 1.0, 34619.78269232255], 
processed observation next is [1.0, 0.5652173913043478, 0.2516158818097876, 0.61, 0.4533333333333333, 0.5786372007366483, 0.6666666666666666, 0.6369598894961017, 0.6431373542607397, 1.0, 1.0, 0.1648561080586788], 
reward next is 0.8351, 
noisyNet noise sample is [array([0.36095858], dtype=float32), 0.34400943]. 
=============================================
[2019-04-03 23:27:37,585] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:27:37,585] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:27:37,588] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run9
[2019-04-03 23:27:38,131] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:27:38,131] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:27:38,134] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run9
[2019-04-03 23:27:39,162] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:27:39,162] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:27:39,166] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run9
[2019-04-03 23:27:39,629] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.7067559e-16 8.0601749e-06 9.7946189e-15 6.7738419e-11 2.1188971e-08
 4.8829663e-17 9.9999189e-01], sum to 1.0000
[2019-04-03 23:27:39,629] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7414
[2019-04-03 23:27:39,643] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.83333333333333, 17.0, 49.33333333333333, 389.6666666666666, 26.0, 29.04493006629126, 1.192137256332057, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5073000.0000, 
sim time next is 5073600.0000, 
raw observation next is [11.66666666666667, 17.0, 42.66666666666666, 340.8333333333333, 26.0, 29.17474531931808, 0.9990365024150215, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.785780240073869, 0.17, 0.1422222222222222, 0.3766114180478821, 0.6666666666666666, 0.93122877660984, 0.8330121674716738, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.80806226], dtype=float32), -0.71407217]. 
=============================================
[2019-04-03 23:27:40,693] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:27:40,693] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:27:40,696] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run9
[2019-04-03 23:27:41,099] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:27:41,099] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:27:41,105] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run9
[2019-04-03 23:27:41,334] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:27:41,335] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:27:41,338] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run9
[2019-04-03 23:27:41,440] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:27:41,440] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:27:41,444] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run9
[2019-04-03 23:27:42,053] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:27:42,053] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:27:42,057] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run9
[2019-04-03 23:27:42,143] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:27:42,143] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:27:42,147] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run9
[2019-04-03 23:27:48,533] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:27:48,533] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:27:48,548] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run9
[2019-04-03 23:28:09,710] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.7371828e-13 3.2527902e-04 9.7576704e-11 5.0641713e-08 4.2086984e-07
 3.2129163e-12 9.9967420e-01], sum to 1.0000
[2019-04-03 23:28:09,711] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8156
[2019-04-03 23:28:09,741] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.7918903075572, -0.2372427111298757, 0.0, 1.0, 44943.37009409874], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 193800.0000, 
sim time next is 194400.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.74138015727413, -0.2438855594443943, 0.0, 1.0, 44958.22572610667], 
processed observation next is [1.0, 0.2608695652173913, 0.21606648199445982, 0.78, 0.0, 0.0, 0.6666666666666666, 0.3951150131061774, 0.4187048135185352, 0.0, 1.0, 0.21408678917193652], 
reward next is 0.7859, 
noisyNet noise sample is [array([-0.63169414], dtype=float32), 1.4181657]. 
=============================================
[2019-04-03 23:28:12,852] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.3704806e-15 5.9190024e-05 3.5024566e-13 2.8296974e-09 4.1061590e-09
 3.4916142e-15 9.9994075e-01], sum to 1.0000
[2019-04-03 23:28:12,852] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1268
[2019-04-03 23:28:12,912] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 65.0, 139.0, 0.0, 26.0, 25.14385069628581, 0.2362619547458984, 1.0, 1.0, 36050.6008267436], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 219600.0000, 
sim time next is 220200.0000, 
raw observation next is [-4.316666666666666, 64.5, 142.3333333333333, 0.0, 26.0, 25.18582318401195, 0.1458756657854181, 1.0, 1.0, 18721.76290850715], 
processed observation next is [1.0, 0.5652173913043478, 0.34302862419205915, 0.645, 0.4744444444444443, 0.0, 0.6666666666666666, 0.5988185986676626, 0.5486252219284727, 1.0, 1.0, 0.08915125194527214], 
reward next is 0.9108, 
noisyNet noise sample is [array([-2.2188668], dtype=float32), -0.2839654]. 
=============================================
[2019-04-03 23:28:18,794] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.2652359e-13 2.8133846e-04 4.8302379e-11 9.9614645e-09 1.0880389e-06
 2.7529728e-12 9.9971753e-01], sum to 1.0000
[2019-04-03 23:28:18,795] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7566
[2019-04-03 23:28:18,819] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.633333333333333, 67.66666666666667, 0.0, 0.0, 26.0, 23.76604686072983, -0.01060801294873912, 0.0, 1.0, 45728.16483388095], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 269400.0000, 
sim time next is 270000.0000, 
raw observation next is [-8.9, 67.0, 0.0, 0.0, 26.0, 23.75846298507189, -0.02503167225461504, 0.0, 1.0, 45780.8829456885], 
processed observation next is [1.0, 0.13043478260869565, 0.21606648199445982, 0.67, 0.0, 0.0, 0.6666666666666666, 0.47987191542265756, 0.49165610924846165, 0.0, 1.0, 0.21800420450327856], 
reward next is 0.7820, 
noisyNet noise sample is [array([-0.24232513], dtype=float32), 0.8299989]. 
=============================================
[2019-04-03 23:28:18,842] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[72.68609 ]
 [72.881355]
 [73.070724]
 [73.22938 ]
 [73.37214 ]], R is [[72.54396057]
 [72.60076904]
 [72.65729523]
 [72.71341705]
 [72.76914978]].
[2019-04-03 23:28:23,838] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.1988678e-13 9.0395199e-04 1.6979229e-10 3.2618175e-08 2.4415431e-07
 3.6625152e-12 9.9909580e-01], sum to 1.0000
[2019-04-03 23:28:23,838] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1173
[2019-04-03 23:28:23,925] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-16.53333333333333, 80.0, 0.0, 0.0, 26.0, 22.68914621317499, -0.2505289343859654, 1.0, 1.0, 169295.3726917159], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 373200.0000, 
sim time next is 373800.0000, 
raw observation next is [-16.61666666666667, 80.5, 8.333333333333332, 192.3333333333333, 26.0, 23.0292743874449, -0.1737174884420432, 1.0, 1.0, 117085.9982290762], 
processed observation next is [1.0, 0.30434782608695654, 0.0023084025854107648, 0.805, 0.027777777777777773, 0.21252302025782682, 0.6666666666666666, 0.4191061989537417, 0.44209417051931893, 1.0, 1.0, 0.5575523725194105], 
reward next is 0.4424, 
noisyNet noise sample is [array([-0.20562898], dtype=float32), 0.5794941]. 
=============================================
[2019-04-03 23:28:24,331] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.6821742e-13 1.5696101e-05 1.5057969e-11 1.9015960e-08 1.3667803e-07
 1.8254910e-13 9.9998415e-01], sum to 1.0000
[2019-04-03 23:28:24,335] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3310
[2019-04-03 23:28:24,397] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.8, 75.83333333333334, 0.0, 0.0, 26.0, 24.87959683266, 0.2348838058102266, 0.0, 1.0, 44519.98441263328], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 334200.0000, 
sim time next is 334800.0000, 
raw observation next is [-12.8, 77.0, 0.0, 0.0, 26.0, 24.73933879770392, 0.2088424512833786, 0.0, 1.0, 46654.49769172643], 
processed observation next is [1.0, 0.9130434782608695, 0.1080332409972299, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5616115664753266, 0.5696141504277928, 0.0, 1.0, 0.2221642747225068], 
reward next is 0.7778, 
noisyNet noise sample is [array([0.51433945], dtype=float32), 0.13922484]. 
=============================================
[2019-04-03 23:28:24,788] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.9752736e-13 8.3762270e-06 9.7750023e-11 2.6248703e-08 1.6132824e-07
 9.4397949e-13 9.9999154e-01], sum to 1.0000
[2019-04-03 23:28:24,788] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4557
[2019-04-03 23:28:24,833] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-13.0, 78.66666666666667, 0.0, 0.0, 26.0, 24.53147391939792, 0.1716392515649951, 0.0, 1.0, 47496.42632474878], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 336000.0000, 
sim time next is 336600.0000, 
raw observation next is [-13.1, 79.5, 0.0, 0.0, 26.0, 24.43409989114882, 0.1530192062437922, 0.0, 1.0, 47550.24547310801], 
processed observation next is [1.0, 0.9130434782608695, 0.0997229916897507, 0.795, 0.0, 0.0, 0.6666666666666666, 0.5361749909290682, 0.5510064020812641, 0.0, 1.0, 0.22642974034813337], 
reward next is 0.7736, 
noisyNet noise sample is [array([-0.32296854], dtype=float32), 0.5298752]. 
=============================================
[2019-04-03 23:28:26,175] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.2034209e-14 3.9336323e-05 2.6748233e-12 2.5767803e-09 2.7320313e-08
 1.9388812e-13 9.9996066e-01], sum to 1.0000
[2019-04-03 23:28:26,175] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3890
[2019-04-03 23:28:26,241] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.133333333333333, 80.16666666666667, 132.6666666666667, 462.3333333333334, 26.0, 24.97524624963815, 0.3244676287222803, 0.0, 1.0, 18727.4530848633], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 564600.0000, 
sim time next is 565200.0000, 
raw observation next is [-1.2, 80.0, 134.0, 495.5, 26.0, 24.95913256890173, 0.3231891196300824, 0.0, 1.0, 35219.33212235032], 
processed observation next is [0.0, 0.5652173913043478, 0.42936288088642666, 0.8, 0.44666666666666666, 0.5475138121546961, 0.6666666666666666, 0.5799277140751441, 0.6077297065433608, 0.0, 1.0, 0.16771110534452535], 
reward next is 0.8323, 
noisyNet noise sample is [array([-0.7323238], dtype=float32), -0.15302871]. 
=============================================
[2019-04-03 23:28:32,962] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.31185913e-13 1.18975906e-04 4.17629889e-11 2.48580765e-08
 1.60503703e-07 2.48122681e-12 9.99880910e-01], sum to 1.0000
[2019-04-03 23:28:32,962] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3603
[2019-04-03 23:28:32,983] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.8, 50.0, 0.0, 0.0, 26.0, 23.18483534901611, -0.1800317027058938, 0.0, 1.0, 46010.67392690943], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 444000.0000, 
sim time next is 444600.0000, 
raw observation next is [-10.9, 50.5, 0.0, 0.0, 26.0, 23.11715966782695, -0.1991127916065451, 0.0, 1.0, 46083.0397167549], 
processed observation next is [1.0, 0.13043478260869565, 0.16066481994459833, 0.505, 0.0, 0.0, 0.6666666666666666, 0.4264299723189125, 0.43362906946448493, 0.0, 1.0, 0.21944304627026143], 
reward next is 0.7806, 
noisyNet noise sample is [array([0.05672826], dtype=float32), 1.6850928]. 
=============================================
[2019-04-03 23:28:36,320] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.7043651e-13 5.9770955e-06 5.9268174e-11 1.1172771e-08 9.7226042e-08
 6.0570017e-13 9.9999392e-01], sum to 1.0000
[2019-04-03 23:28:36,321] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0974
[2019-04-03 23:28:36,413] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.0, 53.0, 0.0, 0.0, 26.0, 23.27065759076272, -0.1395037611358415, 0.0, 1.0, 45840.96979009302], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 440400.0000, 
sim time next is 441000.0000, 
raw observation next is [-10.9, 52.0, 0.0, 0.0, 26.0, 23.26859170726678, -0.1430374311657751, 0.0, 1.0, 45871.0824455538], 
processed observation next is [1.0, 0.08695652173913043, 0.16066481994459833, 0.52, 0.0, 0.0, 0.6666666666666666, 0.43904930893889826, 0.45232085627807495, 0.0, 1.0, 0.21843372593120858], 
reward next is 0.7816, 
noisyNet noise sample is [array([0.7301468], dtype=float32), -0.5021011]. 
=============================================
[2019-04-03 23:28:36,448] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[70.07905 ]
 [70.09768 ]
 [70.111694]
 [70.0883  ]
 [70.0751  ]], R is [[70.13957977]
 [70.21989441]
 [70.29962921]
 [70.37879181]
 [70.45743561]].
[2019-04-03 23:28:37,713] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-03 23:28:37,714] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-03 23:28:37,744] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:28:37,746] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run12
[2019-04-03 23:28:37,774] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-03 23:28:37,775] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:28:37,782] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run12
[2019-04-03 23:28:37,812] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-03 23:28:37,813] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:28:37,815] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run12
[2019-04-03 23:31:48,195] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.8004 239861921.2139 1605.1037
[2019-04-03 23:32:05,619] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-03 23:32:10,138] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.4547 275844504.2563 1233.4233
[2019-04-03 23:32:11,161] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 1100000, evaluation results [1100000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.800375172087, 239861921.21386132, 1605.1037462841891, 7182.454741636738, 275844504.2562892, 1233.4232607117071]
[2019-04-03 23:32:18,910] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.0825773e-17 1.3265174e-05 6.7821745e-14 2.3621002e-10 2.1768753e-09
 2.6385066e-15 9.9998677e-01], sum to 1.0000
[2019-04-03 23:32:18,910] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4712
[2019-04-03 23:32:18,941] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.0, 89.0, 0.0, 0.0, 26.0, 24.84032124862157, 0.2544982710534985, 0.0, 1.0, 39907.85685164412], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 522000.0000, 
sim time next is 522600.0000, 
raw observation next is [4.883333333333334, 88.83333333333334, 0.0, 0.0, 26.0, 24.9671730974579, 0.2578232208025669, 0.0, 1.0, 39660.15620950432], 
processed observation next is [0.0, 0.043478260869565216, 0.597876269621422, 0.8883333333333334, 0.0, 0.0, 0.6666666666666666, 0.5805977581214915, 0.5859410736008557, 0.0, 1.0, 0.18885788671192533], 
reward next is 0.8111, 
noisyNet noise sample is [array([-1.3674129], dtype=float32), 0.59878457]. 
=============================================
[2019-04-03 23:32:20,839] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [6.3226351e-16 3.2965785e-05 7.2311860e-14 2.2511290e-10 1.2360570e-08
 6.9371494e-15 9.9996698e-01], sum to 1.0000
[2019-04-03 23:32:20,840] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.4566
[2019-04-03 23:32:20,875] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.05, 87.0, 0.0, 0.0, 26.0, 24.84643455622105, 0.2332904375082095, 0.0, 1.0, 39709.94832219808], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 527400.0000, 
sim time next is 528000.0000, 
raw observation next is [3.966666666666667, 86.66666666666666, 0.0, 0.0, 26.0, 24.82478057649949, 0.2302836829692247, 0.0, 1.0, 39739.48094218829], 
processed observation next is [0.0, 0.08695652173913043, 0.5724838411819021, 0.8666666666666666, 0.0, 0.0, 0.6666666666666666, 0.5687317147082908, 0.5767612276564082, 0.0, 1.0, 0.18923562353422996], 
reward next is 0.8108, 
noisyNet noise sample is [array([-1.8110656], dtype=float32), -0.3556382]. 
=============================================
[2019-04-03 23:32:20,892] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[89.6438  ]
 [89.97227 ]
 [90.148186]
 [90.32211 ]
 [90.590866]], R is [[89.29943085]
 [89.21734619]
 [89.13621521]
 [89.05602264]
 [88.97676849]].
[2019-04-03 23:32:27,939] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.5827942e-14 9.1968386e-06 4.9966251e-12 9.7261632e-10 3.1531364e-08
 2.8387084e-14 9.9999082e-01], sum to 1.0000
[2019-04-03 23:32:27,939] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5098
[2019-04-03 23:32:27,954] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.9, 83.0, 0.0, 0.0, 26.0, 24.87127320067276, 0.2458385678956594, 0.0, 1.0, 42981.88782238266], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 598200.0000, 
sim time next is 598800.0000, 
raw observation next is [-3.0, 83.0, 0.0, 0.0, 26.0, 24.84298607784239, 0.2441028936733608, 0.0, 1.0, 42936.62423467788], 
processed observation next is [0.0, 0.9565217391304348, 0.3795013850415513, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5702488398201991, 0.5813676312244537, 0.0, 1.0, 0.204460115403228], 
reward next is 0.7955, 
noisyNet noise sample is [array([-1.4837012], dtype=float32), -0.052424174]. 
=============================================
[2019-04-03 23:32:37,211] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.5318430e-15 1.2969528e-07 2.5622943e-14 1.9568233e-10 1.3926764e-09
 8.8026094e-16 9.9999988e-01], sum to 1.0000
[2019-04-03 23:32:37,212] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8229
[2019-04-03 23:32:37,255] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 54.0, 34.0, 2.5, 26.0, 25.97670245022265, 0.4398782571140162, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 752400.0000, 
sim time next is 753000.0000, 
raw observation next is [-2.983333333333333, 54.33333333333333, 0.0, 0.0, 26.0, 25.98316584920155, 0.4267363319019089, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.37996306555863346, 0.5433333333333333, 0.0, 0.0, 0.6666666666666666, 0.6652638207667959, 0.642245443967303, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.34214005], dtype=float32), -0.43736535]. 
=============================================
[2019-04-03 23:32:37,298] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[84.78433 ]
 [84.9227  ]
 [85.15643 ]
 [85.352325]
 [85.530945]], R is [[84.40003204]
 [84.55603027]
 [84.71047211]
 [84.86336517]
 [85.01473236]].
[2019-04-03 23:32:37,534] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.9543742e-15 5.9791603e-07 3.6780821e-13 2.7556854e-10 1.2062563e-09
 3.2241102e-16 9.9999940e-01], sum to 1.0000
[2019-04-03 23:32:37,534] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6320
[2019-04-03 23:32:37,614] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.333333333333333, 48.0, 70.83333333333334, 7.666666666666665, 26.0, 24.69680851004136, 0.3356503070868968, 1.0, 1.0, 197075.4151551395], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 750000.0000, 
sim time next is 750600.0000, 
raw observation next is [-1.7, 49.5, 68.0, 3.0, 26.0, 25.22705343771711, 0.3957530974034487, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.4155124653739613, 0.495, 0.22666666666666666, 0.0033149171270718232, 0.6666666666666666, 0.6022544531430926, 0.6319176991344829, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.25896192], dtype=float32), -0.40830275]. 
=============================================
[2019-04-03 23:32:38,515] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [9.0372850e-17 6.5577751e-06 2.2884064e-14 3.3010730e-11 9.9560216e-10
 1.5443944e-16 9.9999344e-01], sum to 1.0000
[2019-04-03 23:32:38,515] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.5558
[2019-04-03 23:32:38,572] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.516666666666667, 67.66666666666667, 126.3333333333333, 61.66666666666666, 26.0, 25.93732787991429, 0.3318164891858978, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 727800.0000, 
sim time next is 728400.0000, 
raw observation next is [-1.333333333333333, 67.33333333333334, 132.6666666666667, 64.83333333333334, 26.0, 25.88302284618719, 0.3311871862132216, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.42566943674976926, 0.6733333333333335, 0.4422222222222224, 0.07163904235727442, 0.6666666666666666, 0.656918570515599, 0.6103957287377405, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.23655093], dtype=float32), 1.0785289]. 
=============================================
[2019-04-03 23:32:41,901] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.0822854e-16 2.9229668e-06 1.9305822e-13 1.4656699e-10 5.2717439e-09
 2.2760336e-15 9.9999702e-01], sum to 1.0000
[2019-04-03 23:32:41,901] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5808
[2019-04-03 23:32:41,961] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 83.0, 0.0, 0.0, 26.0, 24.88515093282982, 0.3042607415040064, 0.0, 1.0, 33446.79891662479], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 849600.0000, 
sim time next is 850200.0000, 
raw observation next is [-3.4, 83.0, 0.0, 0.0, 26.0, 24.9909682202301, 0.3065505801490016, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.368421052631579, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5825806850191751, 0.6021835267163339, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.26690355], dtype=float32), -0.02848295]. 
=============================================
[2019-04-03 23:32:45,449] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.6338358e-15 8.5937281e-05 7.5161535e-13 5.4882976e-10 2.7245669e-08
 3.4277936e-14 9.9991405e-01], sum to 1.0000
[2019-04-03 23:32:45,451] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1371
[2019-04-03 23:32:45,468] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [18.63333333333333, 63.66666666666667, 169.1666666666667, 0.0, 26.0, 25.09159978315294, 0.5026641757849944, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1167600.0000, 
sim time next is 1168200.0000, 
raw observation next is [18.55, 64.0, 171.0, 0.0, 26.0, 25.08476925722388, 0.5049247839734886, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.976454293628809, 0.64, 0.57, 0.0, 0.6666666666666666, 0.59039743810199, 0.6683082613244963, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.57314503], dtype=float32), -0.30058858]. 
=============================================
[2019-04-03 23:32:49,288] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.3760262e-19 1.2567552e-06 2.0062134e-16 8.9286217e-13 2.7514383e-11
 3.7704675e-18 9.9999869e-01], sum to 1.0000
[2019-04-03 23:32:49,288] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6282
[2019-04-03 23:32:49,305] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.1, 86.0, 122.6666666666667, 0.0, 26.0, 26.67710026504808, 0.6900400653864273, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 993000.0000, 
sim time next is 993600.0000, 
raw observation next is [12.2, 86.0, 124.0, 0.0, 26.0, 26.69547821926884, 0.6989155232896372, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.8005540166204987, 0.86, 0.41333333333333333, 0.0, 0.6666666666666666, 0.72462318493907, 0.7329718410965458, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.929601], dtype=float32), 0.30264628]. 
=============================================
[2019-04-03 23:32:52,403] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.1879870e-15 5.5593148e-08 4.2164921e-14 3.1268937e-10 7.7573137e-10
 1.4823478e-15 1.0000000e+00], sum to 1.0000
[2019-04-03 23:32:52,409] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9869
[2019-04-03 23:32:52,467] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 83.0, 0.0, 0.0, 26.0, 25.0277087418845, 0.3156345818205799, 0.0, 1.0, 90169.49618600478], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 850800.0000, 
sim time next is 851400.0000, 
raw observation next is [-3.4, 83.0, 0.0, 0.0, 26.0, 25.08144578721981, 0.3103043109348873, 0.0, 1.0, 66292.96094828469], 
processed observation next is [1.0, 0.8695652173913043, 0.368421052631579, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5901204822683175, 0.6034347703116291, 0.0, 1.0, 0.31568076642040327], 
reward next is 0.6843, 
noisyNet noise sample is [array([2.5763497], dtype=float32), 0.12681189]. 
=============================================
[2019-04-03 23:32:55,690] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.9312609e-17 7.8002884e-08 5.9190908e-15 2.9013385e-11 1.2805947e-10
 2.6212342e-16 9.9999988e-01], sum to 1.0000
[2019-04-03 23:32:55,692] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0395
[2019-04-03 23:32:55,705] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.25, 92.5, 0.0, 0.0, 26.0, 25.31963184388946, 0.4260466376212028, 0.0, 1.0, 38029.25669262846], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 952200.0000, 
sim time next is 952800.0000, 
raw observation next is [5.333333333333334, 91.33333333333333, 0.0, 0.0, 26.0, 25.31538461331616, 0.4212413689741921, 0.0, 1.0, 38034.58644570113], 
processed observation next is [1.0, 0.0, 0.6103416435826409, 0.9133333333333333, 0.0, 0.0, 0.6666666666666666, 0.6096153844430132, 0.6404137896580641, 0.0, 1.0, 0.1811170783128625], 
reward next is 0.8189, 
noisyNet noise sample is [array([-1.2659619], dtype=float32), 0.8117049]. 
=============================================
[2019-04-03 23:32:57,866] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6975152e-16 1.1923574e-06 7.3049158e-14 3.1922850e-11 1.1059746e-09
 2.3484304e-15 9.9999881e-01], sum to 1.0000
[2019-04-03 23:32:57,869] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1144
[2019-04-03 23:32:57,879] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.1, 79.5, 31.0, 0.0, 26.0, 25.64961665473268, 0.618482035449261, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1153800.0000, 
sim time next is 1154400.0000, 
raw observation next is [14.56666666666667, 78.0, 39.66666666666666, 0.0, 26.0, 25.73413723570139, 0.6198123105069319, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.8661126500461682, 0.78, 0.13222222222222219, 0.0, 0.6666666666666666, 0.6445114363084491, 0.7066041035023106, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.50421524], dtype=float32), -1.0018282]. 
=============================================
[2019-04-03 23:32:58,717] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.6575675e-19 3.9650914e-08 1.3449228e-16 7.2131483e-13 2.3467530e-12
 9.9226990e-18 1.0000000e+00], sum to 1.0000
[2019-04-03 23:32:58,720] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2102
[2019-04-03 23:32:58,731] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.66666666666667, 78.33333333333334, 109.3333333333333, 77.99999999999999, 26.0, 27.0484472301058, 0.8494295080571866, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1073400.0000, 
sim time next is 1074000.0000, 
raw observation next is [14.03333333333333, 76.66666666666667, 111.6666666666667, 38.99999999999999, 26.0, 27.12975565542017, 0.8575010379648299, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.8513388734995383, 0.7666666666666667, 0.37222222222222234, 0.043093922651933694, 0.6666666666666666, 0.7608129712850141, 0.78583367932161, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5054224], dtype=float32), 0.06717903]. 
=============================================
[2019-04-03 23:32:58,753] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[103.0358  ]
 [103.736084]
 [104.53705 ]
 [105.287   ]
 [105.75108 ]], R is [[102.45246124]
 [102.42794037]
 [102.40366364]
 [102.37963104]
 [102.35583496]].
[2019-04-03 23:33:02,057] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.1219717e-17 5.9178365e-07 1.4748123e-14 1.3124228e-11 1.0890173e-10
 3.2047486e-16 9.9999940e-01], sum to 1.0000
[2019-04-03 23:33:02,058] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2161
[2019-04-03 23:33:02,067] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.8, 100.0, 51.0, 0.0, 26.0, 24.67255660752623, 0.4280590292174142, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1265400.0000, 
sim time next is 1266000.0000, 
raw observation next is [13.8, 100.0, 45.66666666666666, 0.0, 26.0, 24.65608911489105, 0.4245771522173507, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.844875346260388, 1.0, 0.1522222222222222, 0.0, 0.6666666666666666, 0.5546740929075874, 0.6415257174057836, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9776332], dtype=float32), 0.475164]. 
=============================================
[2019-04-03 23:33:02,081] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[89.164505]
 [89.2208  ]
 [89.28794 ]
 [89.37594 ]
 [89.47816 ]], R is [[89.21087646]
 [89.31877136]
 [89.42558289]
 [89.53132629]
 [89.63601685]].
[2019-04-03 23:33:02,650] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.1313974e-15 1.5710935e-06 1.2196152e-13 1.4687671e-10 3.0885989e-09
 2.0488221e-14 9.9999845e-01], sum to 1.0000
[2019-04-03 23:33:02,652] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9260
[2019-04-03 23:33:02,659] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [18.3, 65.0, 159.0, 0.0, 26.0, 25.04944778839881, 0.4972813177543028, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1171800.0000, 
sim time next is 1172400.0000, 
raw observation next is [18.3, 65.0, 153.8333333333333, 0.0, 26.0, 25.04139486807425, 0.4976446745629068, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.9695290858725764, 0.65, 0.5127777777777777, 0.0, 0.6666666666666666, 0.5867829056728541, 0.6658815581876356, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.4485195], dtype=float32), -1.2649249]. 
=============================================
[2019-04-03 23:33:05,865] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.65575481e-15 1.97988115e-07 1.02304755e-13 3.91576632e-11
 1.00820741e-09 3.43932434e-15 9.99999762e-01], sum to 1.0000
[2019-04-03 23:33:05,866] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6675
[2019-04-03 23:33:05,872] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [18.3, 64.66666666666667, 9.666666666666664, 0.0, 26.0, 24.91410311822124, 0.4517360514103541, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1185000.0000, 
sim time next is 1185600.0000, 
raw observation next is [18.3, 64.33333333333334, 0.0, 0.0, 26.0, 24.89327663485841, 0.446278429122271, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.7391304347826086, 0.9695290858725764, 0.6433333333333334, 0.0, 0.0, 0.6666666666666666, 0.5744397195715342, 0.6487594763740904, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5394298], dtype=float32), 0.5791265]. 
=============================================
[2019-04-03 23:33:08,536] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [4.7600005e-18 2.2562782e-08 9.4338532e-15 4.2658572e-12 6.5548879e-11
 2.3800969e-16 1.0000000e+00], sum to 1.0000
[2019-04-03 23:33:08,539] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.5410
[2019-04-03 23:33:08,588] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.6, 100.0, 80.83333333333333, 0.0, 26.0, 24.05622076363828, 0.3842208301628278, 0.0, 1.0, 64126.6824289393], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1248000.0000, 
sim time next is 1248600.0000, 
raw observation next is [14.5, 100.0, 83.66666666666667, 0.0, 26.0, 24.45726140577369, 0.4259845501270266, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.8642659279778394, 1.0, 0.2788888888888889, 0.0, 0.6666666666666666, 0.5381051171478074, 0.6419948500423421, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.59555507], dtype=float32), 0.994134]. 
=============================================
[2019-04-03 23:33:22,219] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.3286103e-17 4.2880690e-09 1.0849645e-14 2.4052302e-11 2.1830901e-10
 4.8543117e-17 1.0000000e+00], sum to 1.0000
[2019-04-03 23:33:22,221] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3122
[2019-04-03 23:33:22,258] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.516666666666667, 89.5, 0.0, 0.0, 26.0, 25.31394295776481, 0.5478551375433177, 0.0, 1.0, 50093.21972948498], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1457400.0000, 
sim time next is 1458000.0000, 
raw observation next is [1.6, 89.0, 0.0, 0.0, 26.0, 25.47349245727521, 0.559903276188389, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.5069252077562327, 0.89, 0.0, 0.0, 0.6666666666666666, 0.6227910381062675, 0.6866344253961296, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3040246], dtype=float32), 0.54128206]. 
=============================================
[2019-04-03 23:33:22,275] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[84.16723 ]
 [83.93014 ]
 [84.15116 ]
 [85.42412 ]
 [86.910835]], R is [[84.37726593]
 [84.29495239]
 [84.11299896]
 [83.65890503]
 [82.87201691]].
[2019-04-03 23:33:22,571] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.3619954e-18 7.7002643e-10 2.7658639e-15 4.9032991e-12 3.5062172e-11
 2.3879076e-17 1.0000000e+00], sum to 1.0000
[2019-04-03 23:33:22,577] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8592
[2019-04-03 23:33:22,586] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.9, 84.0, 0.0, 0.0, 26.0, 25.82975373054571, 0.6095682961377173, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1636200.0000, 
sim time next is 1636800.0000, 
raw observation next is [7.0, 83.33333333333334, 0.0, 0.0, 26.0, 25.71671463364167, 0.5972397830102607, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.6565096952908588, 0.8333333333333335, 0.0, 0.0, 0.6666666666666666, 0.6430595528034724, 0.6990799276700869, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.38534537], dtype=float32), -1.0423328]. 
=============================================
[2019-04-03 23:33:26,881] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.3221861e-19 1.7977898e-10 2.1039085e-16 3.0303277e-12 1.3527137e-11
 4.3761945e-18 1.0000000e+00], sum to 1.0000
[2019-04-03 23:33:26,882] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5027
[2019-04-03 23:33:26,901] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.766666666666667, 64.33333333333334, 0.0, 0.0, 26.0, 26.36590662888283, 0.6991366774506246, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1622400.0000, 
sim time next is 1623000.0000, 
raw observation next is [9.583333333333334, 65.16666666666666, 0.0, 0.0, 26.0, 26.24670576260927, 0.6895705040199885, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.7280701754385965, 0.6516666666666666, 0.0, 0.0, 0.6666666666666666, 0.6872254802174392, 0.7298568346733295, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.551348], dtype=float32), -0.38993487]. 
=============================================
[2019-04-03 23:33:26,918] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[89.27576]
 [94.69253]
 [94.67777]
 [94.56211]
 [94.38775]], R is [[91.56855774]
 [91.65287018]
 [91.73634338]
 [91.81897736]
 [91.90078735]].
[2019-04-03 23:33:29,970] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.6450287e-20 3.6274331e-10 8.0605799e-17 3.3918631e-13 6.9653385e-12
 2.7281153e-18 1.0000000e+00], sum to 1.0000
[2019-04-03 23:33:29,970] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6425
[2019-04-03 23:33:29,987] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.46666666666667, 50.33333333333334, 73.33333333333333, 6.166666666666665, 26.0, 27.19524624224938, 0.832735948119273, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1611600.0000, 
sim time next is 1612200.0000, 
raw observation next is [13.38333333333333, 50.66666666666666, 68.66666666666667, 12.33333333333333, 26.0, 27.28941554149878, 0.8450980262263638, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.8333333333333334, 0.5066666666666666, 0.2288888888888889, 0.013627992633517492, 0.6666666666666666, 0.774117961791565, 0.7816993420754547, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7980152], dtype=float32), -0.3382091]. 
=============================================
[2019-04-03 23:33:42,464] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.2406441e-13 4.0654322e-07 2.4069904e-12 5.2147144e-09 2.4387429e-08
 3.0975461e-13 9.9999964e-01], sum to 1.0000
[2019-04-03 23:33:42,464] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0390
[2019-04-03 23:33:42,477] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.116666666666666, 78.33333333333334, 0.0, 0.0, 26.0, 24.45067985141205, 0.1163085635620002, 0.0, 1.0, 44905.18816883703], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1896600.0000, 
sim time next is 1897200.0000, 
raw observation next is [-7.3, 79.0, 0.0, 0.0, 26.0, 24.41438041315362, 0.1078130178988159, 0.0, 1.0, 44906.43226712498], 
processed observation next is [0.0, 1.0, 0.26038781163434904, 0.79, 0.0, 0.0, 0.6666666666666666, 0.5345317010961349, 0.5359376726329387, 0.0, 1.0, 0.21384015365297612], 
reward next is 0.7862, 
noisyNet noise sample is [array([-0.77181727], dtype=float32), 1.4362025]. 
=============================================
[2019-04-03 23:33:50,197] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.6856858e-14 3.1013269e-07 9.0134318e-13 4.0988241e-10 3.0926701e-09
 1.6751237e-13 9.9999964e-01], sum to 1.0000
[2019-04-03 23:33:50,200] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2205
[2019-04-03 23:33:50,242] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 71.0, 145.0, 20.0, 26.0, 24.96492989303418, 0.2644420712683929, 0.0, 1.0, 40586.24615375356], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1861200.0000, 
sim time next is 1861800.0000, 
raw observation next is [-4.5, 71.0, 153.3333333333333, 26.66666666666667, 26.0, 24.97739671449588, 0.2679250708130732, 0.0, 1.0, 37076.58436216327], 
processed observation next is [0.0, 0.5652173913043478, 0.3379501385041552, 0.71, 0.511111111111111, 0.02946593001841621, 0.6666666666666666, 0.5814497262079902, 0.589308356937691, 0.0, 1.0, 0.1765551636293489], 
reward next is 0.8234, 
noisyNet noise sample is [array([-0.03889799], dtype=float32), -0.8002145]. 
=============================================
[2019-04-03 23:34:10,880] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4766686e-17 1.3042797e-09 6.8661057e-16 8.5075714e-12 2.0412769e-11
 2.8902835e-17 1.0000000e+00], sum to 1.0000
[2019-04-03 23:34:10,887] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0418
[2019-04-03 23:34:11,032] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.833333333333333, 70.0, 0.0, 0.0, 26.0, 24.71961015791876, 0.3423045512490415, 1.0, 1.0, 198377.3828997469], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2137200.0000, 
sim time next is 2137800.0000, 
raw observation next is [-4.916666666666667, 70.5, 0.0, 0.0, 26.0, 24.78229759381401, 0.414087109121449, 1.0, 1.0, 199951.7297985699], 
processed observation next is [1.0, 0.7391304347826086, 0.32640812557710064, 0.705, 0.0, 0.0, 0.6666666666666666, 0.5651914661511676, 0.6380290363738164, 1.0, 1.0, 0.9521510942789043], 
reward next is 0.0478, 
noisyNet noise sample is [array([0.4108193], dtype=float32), 0.8358484]. 
=============================================
[2019-04-03 23:34:14,826] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.9331499e-15 5.0554263e-07 1.2532874e-12 7.4410611e-10 6.8458190e-09
 4.0369008e-14 9.9999952e-01], sum to 1.0000
[2019-04-03 23:34:14,826] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7433
[2019-04-03 23:34:14,845] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.55, 84.0, 0.0, 0.0, 26.0, 24.28488139374885, 0.1386221362680926, 0.0, 1.0, 43990.70731228012], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2255400.0000, 
sim time next is 2256000.0000, 
raw observation next is [-7.633333333333333, 84.66666666666666, 0.0, 0.0, 26.0, 24.34793433211189, 0.1185629239874809, 0.0, 1.0, 44097.4823602363], 
processed observation next is [1.0, 0.08695652173913043, 0.2511542012927055, 0.8466666666666666, 0.0, 0.0, 0.6666666666666666, 0.5289945276759909, 0.5395209746624936, 0.0, 1.0, 0.20998801123922048], 
reward next is 0.7900, 
noisyNet noise sample is [array([-2.5847335], dtype=float32), 0.21099515]. 
=============================================
[2019-04-03 23:34:14,849] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[77.63366]
 [77.63786]
 [77.64364]
 [77.66514]
 [77.65845]], R is [[77.63683319]
 [77.65098572]
 [77.66355896]
 [77.67745209]
 [77.69121552]].
[2019-04-03 23:34:17,199] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4492032e-14 2.3369005e-07 8.1015321e-13 2.9137620e-10 5.9774687e-09
 5.1863526e-14 9.9999976e-01], sum to 1.0000
[2019-04-03 23:34:17,204] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5476
[2019-04-03 23:34:17,254] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.816666666666667, 43.83333333333334, 0.0, 0.0, 26.0, 24.94924083075227, 0.2663233140124512, 0.0, 1.0, 42184.35223119715], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2398200.0000, 
sim time next is 2398800.0000, 
raw observation next is [-1.933333333333334, 43.66666666666667, 0.0, 0.0, 26.0, 24.96729586342442, 0.2647207652860018, 0.0, 1.0, 33161.43237914468], 
processed observation next is [0.0, 0.782608695652174, 0.40904893813481075, 0.4366666666666667, 0.0, 0.0, 0.6666666666666666, 0.5806079886187018, 0.588240255095334, 0.0, 1.0, 0.15791158275783182], 
reward next is 0.8421, 
noisyNet noise sample is [array([-0.6092461], dtype=float32), -0.095067]. 
=============================================
[2019-04-03 23:34:25,095] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.3948907e-17 2.8076058e-10 3.5979800e-15 1.0230176e-11 1.6432432e-11
 1.6120707e-16 1.0000000e+00], sum to 1.0000
[2019-04-03 23:34:25,095] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3075
[2019-04-03 23:34:25,169] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.333333333333333, 49.00000000000001, 227.8333333333333, 69.83333333333333, 26.0, 25.28085895695219, 0.2401469322831959, 1.0, 1.0, 7477.258160976548], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2294400.0000, 
sim time next is 2295000.0000, 
raw observation next is [-1.15, 48.0, 221.0, 69.0, 26.0, 25.02726676133379, 0.326967804036477, 1.0, 1.0, 119782.7184267663], 
processed observation next is [1.0, 0.5652173913043478, 0.4307479224376732, 0.48, 0.7366666666666667, 0.07624309392265194, 0.6666666666666666, 0.5856055634444827, 0.608989268012159, 1.0, 1.0, 0.5703938972703156], 
reward next is 0.4296, 
noisyNet noise sample is [array([1.4823676], dtype=float32), -0.62768936]. 
=============================================
[2019-04-03 23:34:25,188] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[86.24719 ]
 [86.737465]
 [87.34983 ]
 [87.915306]
 [88.39391 ]], R is [[86.02890015]
 [86.13300323]
 [86.27167511]
 [86.3199234 ]
 [86.36769104]].
[2019-04-03 23:34:26,985] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.8562729e-17 4.0478792e-08 1.6406288e-14 2.1813742e-11 2.6976030e-10
 6.5132583e-16 1.0000000e+00], sum to 1.0000
[2019-04-03 23:34:26,987] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4321
[2019-04-03 23:34:27,055] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 55.33333333333333, 65.16666666666666, 20.5, 26.0, 25.52294028540751, 0.2556402506519534, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2536800.0000, 
sim time next is 2537400.0000, 
raw observation next is [-2.8, 55.66666666666667, 79.33333333333333, 23.0, 26.0, 25.60570036755387, 0.2596379915124051, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.38504155124653744, 0.5566666666666668, 0.2644444444444444, 0.02541436464088398, 0.6666666666666666, 0.6338083639628224, 0.5865459971708017, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.03254636], dtype=float32), -1.982472]. 
=============================================
[2019-04-03 23:34:30,133] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.3109037e-13 2.9247733e-06 1.0466476e-11 3.5114702e-09 8.7700073e-08
 1.1916388e-12 9.9999690e-01], sum to 1.0000
[2019-04-03 23:34:30,133] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2307
[2019-04-03 23:34:30,155] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.633333333333333, 54.33333333333333, 0.0, 0.0, 26.0, 23.94528603269832, 0.007191863442325225, 0.0, 1.0, 43703.93624961603], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2428800.0000, 
sim time next is 2429400.0000, 
raw observation next is [-7.716666666666667, 54.66666666666667, 0.0, 0.0, 26.0, 23.89571443479933, -0.003121688078485554, 0.0, 1.0, 43783.77285206209], 
processed observation next is [0.0, 0.08695652173913043, 0.24884579870729456, 0.5466666666666667, 0.0, 0.0, 0.6666666666666666, 0.49130953623327756, 0.4989594373071715, 0.0, 1.0, 0.2084941564383909], 
reward next is 0.7915, 
noisyNet noise sample is [array([-0.7483492], dtype=float32), 1.0727834]. 
=============================================
[2019-04-03 23:34:36,322] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.7514331e-13 2.8189229e-06 8.5102897e-12 4.8243742e-09 5.0910110e-08
 2.1715531e-12 9.9999714e-01], sum to 1.0000
[2019-04-03 23:34:36,322] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2989
[2019-04-03 23:34:36,337] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.5, 58.5, 15.33333333333333, 165.3333333333333, 26.0, 22.82011473597079, -0.2236800165292308, 0.0, 1.0, 44180.10964372585], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2447400.0000, 
sim time next is 2448000.0000, 
raw observation next is [-9.5, 58.0, 21.5, 228.0, 26.0, 22.83146152025676, -0.2198925682921214, 0.0, 1.0, 44076.79576986783], 
processed observation next is [0.0, 0.34782608695652173, 0.1994459833795014, 0.58, 0.07166666666666667, 0.25193370165745854, 0.6666666666666666, 0.40262179335473, 0.42670247723595955, 0.0, 1.0, 0.20988950366603729], 
reward next is 0.7901, 
noisyNet noise sample is [array([0.15125069], dtype=float32), 0.25430056]. 
=============================================
[2019-04-03 23:34:36,341] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[68.56938]
 [67.87097]
 [67.19404]
 [67.33435]
 [67.47645]], R is [[69.24049377]
 [69.33770752]
 [69.43371582]
 [69.52880859]
 [69.6231842 ]].
[2019-04-03 23:34:36,865] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.1864420e-14 1.0896184e-06 2.5351782e-12 2.5612676e-09 1.6568682e-08
 2.5949655e-13 9.9999893e-01], sum to 1.0000
[2019-04-03 23:34:36,865] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2856
[2019-04-03 23:34:36,881] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.1, 47.16666666666666, 0.0, 0.0, 26.0, 24.46006520182506, 0.1122105797229196, 0.0, 1.0, 43253.37196284406], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2422200.0000, 
sim time next is 2422800.0000, 
raw observation next is [-6.2, 48.0, 0.0, 0.0, 26.0, 24.40112030601695, 0.1008887097526768, 0.0, 1.0, 43320.3921944078], 
processed observation next is [0.0, 0.043478260869565216, 0.2908587257617729, 0.48, 0.0, 0.0, 0.6666666666666666, 0.5334266921680791, 0.5336295699175589, 0.0, 1.0, 0.2062875818781324], 
reward next is 0.7937, 
noisyNet noise sample is [array([1.1390209], dtype=float32), 1.6526492]. 
=============================================
[2019-04-03 23:34:44,205] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.7546034e-16 1.5499846e-07 1.8611877e-13 7.4929445e-11 2.7841434e-09
 1.0202061e-14 9.9999988e-01], sum to 1.0000
[2019-04-03 23:34:44,205] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1210
[2019-04-03 23:34:44,232] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 44.83333333333334, 0.0, 0.0, 26.0, 24.92717285944419, 0.1840250025968773, 0.0, 1.0, 38577.3335998182], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2517000.0000, 
sim time next is 2517600.0000, 
raw observation next is [-1.7, 45.66666666666667, 0.0, 0.0, 26.0, 24.98392424792704, 0.1888238785605056, 0.0, 1.0, 38511.20497061199], 
processed observation next is [1.0, 0.13043478260869565, 0.4155124653739613, 0.4566666666666667, 0.0, 0.0, 0.6666666666666666, 0.5819936873272532, 0.5629412928535019, 0.0, 1.0, 0.18338669033624755], 
reward next is 0.8166, 
noisyNet noise sample is [array([-1.3672595], dtype=float32), 0.44538617]. 
=============================================
[2019-04-03 23:35:08,301] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2913635e-16 2.6442728e-09 6.3838002e-15 3.7500884e-11 1.2201401e-10
 4.1038799e-17 1.0000000e+00], sum to 1.0000
[2019-04-03 23:35:08,317] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9865
[2019-04-03 23:35:08,354] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.333333333333333, 28.0, 139.8333333333333, 28.83333333333333, 26.0, 26.00921597780897, 0.3445065637890202, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2816400.0000, 
sim time next is 2817000.0000, 
raw observation next is [6.5, 27.0, 118.0, 0.0, 26.0, 25.56081764981903, 0.3298171294754101, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.6426592797783934, 0.27, 0.3933333333333333, 0.0, 0.6666666666666666, 0.6300681374849191, 0.60993904315847, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.29693395], dtype=float32), 1.0459232]. 
=============================================
[2019-04-03 23:35:08,362] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[85.63298 ]
 [86.39879 ]
 [87.22624 ]
 [88.083206]
 [88.81941 ]], R is [[85.00377655]
 [85.15373993]
 [85.30220032]
 [85.4491806 ]
 [85.59468842]].
[2019-04-03 23:35:12,501] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.41387595e-14 7.75643301e-08 2.82951370e-13 3.97061134e-10
 3.08918069e-09 1.52407417e-14 9.99999881e-01], sum to 1.0000
[2019-04-03 23:35:12,501] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3118
[2019-04-03 23:35:12,543] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 60.0, 98.0, 737.0, 26.0, 25.17617565376543, 0.4081533761388808, 0.0, 1.0, 18706.76182187096], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2990400.0000, 
sim time next is 2991000.0000, 
raw observation next is [-2.0, 60.0, 95.0, 723.0, 26.0, 25.1452760988774, 0.4046696643474286, 0.0, 1.0, 26529.37968332747], 
processed observation next is [0.0, 0.6086956521739131, 0.40720221606648205, 0.6, 0.31666666666666665, 0.7988950276243094, 0.6666666666666666, 0.59543967490645, 0.6348898881158095, 0.0, 1.0, 0.12633037944441652], 
reward next is 0.8737, 
noisyNet noise sample is [array([1.2289877], dtype=float32), -0.6745931]. 
=============================================
[2019-04-03 23:35:12,547] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[75.35212 ]
 [75.46655 ]
 [75.597496]
 [75.65398 ]
 [75.73306 ]], R is [[75.4446106 ]
 [75.60108185]
 [75.84506989]
 [75.99751282]
 [76.14842987]].
[2019-04-03 23:35:15,486] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.28709272e-16 8.68168826e-09 2.87760877e-14 4.99473934e-11
 1.15738225e-10 1.03818266e-15 1.00000000e+00], sum to 1.0000
[2019-04-03 23:35:15,487] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9133
[2019-04-03 23:35:15,508] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.8333333333333334, 100.0, 0.0, 0.0, 26.0, 25.7301093801475, 0.6050173104678125, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3201000.0000, 
sim time next is 3201600.0000, 
raw observation next is [0.6666666666666667, 100.0, 0.0, 0.0, 26.0, 25.73646337295755, 0.5956723410225718, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.4810710987996307, 1.0, 0.0, 0.0, 0.6666666666666666, 0.644705281079796, 0.698557447007524, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0666927], dtype=float32), 1.6265751]. 
=============================================
[2019-04-03 23:35:18,682] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.8700451e-18 1.3765636e-10 2.3594960e-15 7.3578322e-12 1.2500157e-11
 1.0342722e-17 1.0000000e+00], sum to 1.0000
[2019-04-03 23:35:18,682] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8361
[2019-04-03 23:35:18,757] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 100.0, 85.0, 0.0, 26.0, 25.87533844883768, 0.4543928189975122, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2907000.0000, 
sim time next is 2907600.0000, 
raw observation next is [2.0, 100.0, 82.66666666666667, 8.999999999999998, 26.0, 25.90470933350791, 0.456643566419865, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.518005540166205, 1.0, 0.27555555555555555, 0.009944751381215467, 0.6666666666666666, 0.6587257777923258, 0.652214522139955, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4619344], dtype=float32), -0.73690826]. 
=============================================
[2019-04-03 23:35:24,421] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.3926699e-15 3.8450487e-08 1.5117532e-13 1.5598271e-10 1.5436179e-09
 3.4783272e-15 1.0000000e+00], sum to 1.0000
[2019-04-03 23:35:24,421] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2623
[2019-04-03 23:35:24,447] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 92.0, 0.0, 0.0, 26.0, 25.12904260012204, 0.3481203821148995, 0.0, 1.0, 68931.3732155497], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3094800.0000, 
sim time next is 3095400.0000, 
raw observation next is [-1.0, 92.0, 0.0, 0.0, 26.0, 25.32102955254966, 0.3660647556368851, 0.0, 1.0, 50357.64277171336], 
processed observation next is [0.0, 0.8260869565217391, 0.4349030470914128, 0.92, 0.0, 0.0, 0.6666666666666666, 0.610085796045805, 0.6220215852122951, 0.0, 1.0, 0.2397982989129208], 
reward next is 0.7602, 
noisyNet noise sample is [array([1.664433], dtype=float32), -0.46031818]. 
=============================================
[2019-04-03 23:35:24,459] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.9350670e-14 6.0032539e-06 2.0163099e-11 5.2454898e-09 2.6807916e-08
 2.4199119e-12 9.9999404e-01], sum to 1.0000
[2019-04-03 23:35:24,460] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4541
[2019-04-03 23:35:24,490] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 72.33333333333334, 0.0, 0.0, 26.0, 23.73304636665446, -0.0154576877729091, 0.0, 1.0, 40257.19456582331], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3046800.0000, 
sim time next is 3047400.0000, 
raw observation next is [-6.0, 73.5, 0.0, 0.0, 26.0, 23.70667593914112, -0.0200800353785007, 0.0, 1.0, 40309.24722992616], 
processed observation next is [0.0, 0.2608695652173913, 0.296398891966759, 0.735, 0.0, 0.0, 0.6666666666666666, 0.4755563282617601, 0.49330665487383313, 0.0, 1.0, 0.1919487963329817], 
reward next is 0.8081, 
noisyNet noise sample is [array([-1.4734012], dtype=float32), -0.10428474]. 
=============================================
[2019-04-03 23:35:33,599] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.2582760e-19 1.1580298e-09 2.3930234e-17 6.8916352e-13 4.4576209e-12
 1.8088924e-18 1.0000000e+00], sum to 1.0000
[2019-04-03 23:35:33,616] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5410
[2019-04-03 23:35:33,643] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.0, 100.0, 106.5, 729.5, 26.0, 26.80680908484937, 0.6775445493370963, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3147600.0000, 
sim time next is 3148200.0000, 
raw observation next is [7.0, 100.0, 108.0, 746.0, 26.0, 26.89040464850358, 0.6987240073898623, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.6565096952908588, 1.0, 0.36, 0.8243093922651934, 0.6666666666666666, 0.740867054041965, 0.7329080024632875, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.10007597], dtype=float32), 1.822191]. 
=============================================
[2019-04-03 23:35:46,592] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.2230814e-15 9.4354835e-09 4.9751858e-13 4.8666404e-10 4.4388604e-09
 4.6192988e-15 1.0000000e+00], sum to 1.0000
[2019-04-03 23:35:46,593] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6032
[2019-04-03 23:35:46,615] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.5, 86.5, 0.0, 0.0, 26.0, 25.61774763136365, 0.5522434126431222, 0.0, 1.0, 18732.31382640424], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3274200.0000, 
sim time next is 3274800.0000, 
raw observation next is [-5.666666666666667, 88.33333333333333, 0.0, 0.0, 26.0, 25.54293874420682, 0.5416550601798522, 0.0, 1.0, 61588.61946391542], 
processed observation next is [1.0, 0.9130434782608695, 0.30563250230840255, 0.8833333333333333, 0.0, 0.0, 0.6666666666666666, 0.6285782286839016, 0.6805516867266174, 0.0, 1.0, 0.29327914030435914], 
reward next is 0.7067, 
noisyNet noise sample is [array([-1.3065585], dtype=float32), 1.1475695]. 
=============================================
[2019-04-03 23:35:46,799] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.0970582e-15 4.6487281e-09 1.7761781e-13 1.7681834e-10 1.5341607e-09
 2.8170609e-15 1.0000000e+00], sum to 1.0000
[2019-04-03 23:35:46,800] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.7793
[2019-04-03 23:35:46,892] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 69.0, 0.0, 0.0, 26.0, 25.87489639686132, 0.5971717201518153, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3267600.0000, 
sim time next is 3268200.0000, 
raw observation next is [-4.0, 70.0, 0.0, 0.0, 26.0, 25.79176387535801, 0.5753833251803645, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.3518005540166205, 0.7, 0.0, 0.0, 0.6666666666666666, 0.6493136562798343, 0.6917944417267882, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0211582], dtype=float32), 0.86625075]. 
=============================================
[2019-04-03 23:36:20,612] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.6462675e-14 2.1638589e-06 1.7338723e-12 3.1104813e-10 9.9563930e-09
 1.8911387e-13 9.9999785e-01], sum to 1.0000
[2019-04-03 23:36:20,612] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1140
[2019-04-03 23:36:20,634] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.833333333333333, 69.16666666666667, 0.0, 0.0, 26.0, 24.71227970987058, 0.2727831560106943, 0.0, 1.0, 40856.10797536755], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3563400.0000, 
sim time next is 3564000.0000, 
raw observation next is [-6.0, 70.0, 0.0, 0.0, 26.0, 24.66291834348707, 0.2650136085125981, 0.0, 1.0, 40864.32570228973], 
processed observation next is [0.0, 0.2608695652173913, 0.296398891966759, 0.7, 0.0, 0.0, 0.6666666666666666, 0.5552431952905893, 0.5883378695041993, 0.0, 1.0, 0.19459202715376062], 
reward next is 0.8054, 
noisyNet noise sample is [array([-0.3497009], dtype=float32), 0.395831]. 
=============================================
[2019-04-03 23:36:20,678] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[78.562805]
 [78.72118 ]
 [78.88725 ]
 [79.0737  ]
 [79.2861  ]], R is [[78.42449188]
 [78.4457016 ]
 [78.4666748 ]
 [78.48734283]
 [78.50778961]].
[2019-04-03 23:36:33,576] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.4285532e-16 1.1495144e-07 5.7000073e-14 4.5467029e-11 2.4676023e-09
 1.0342895e-14 9.9999988e-01], sum to 1.0000
[2019-04-03 23:36:33,591] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8602
[2019-04-03 23:36:33,658] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 71.0, 0.0, 0.0, 26.0, 25.36835598657715, 0.3964425070779853, 0.0, 1.0, 41842.22400168322], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3717600.0000, 
sim time next is 3718200.0000, 
raw observation next is [-3.0, 71.0, 0.0, 0.0, 26.0, 25.49748758576366, 0.4009792793832834, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.3795013850415513, 0.71, 0.0, 0.0, 0.6666666666666666, 0.6247906321469717, 0.6336597597944278, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.08587616], dtype=float32), 1.259521]. 
=============================================
[2019-04-03 23:36:39,193] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2992448e-15 4.6792965e-07 1.6849995e-13 1.2702796e-10 3.7318668e-09
 6.1313738e-15 9.9999952e-01], sum to 1.0000
[2019-04-03 23:36:39,196] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7600
[2019-04-03 23:36:39,226] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.34626222189401, 0.3496198939241311, 0.0, 1.0, 41100.35627361521], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3724200.0000, 
sim time next is 3724800.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.34337364449395, 0.3432903160621018, 0.0, 1.0, 41025.704529839], 
processed observation next is [1.0, 0.08695652173913043, 0.3795013850415513, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6119478037078293, 0.614430105354034, 0.0, 1.0, 0.1953604977611381], 
reward next is 0.8046, 
noisyNet noise sample is [array([0.54011357], dtype=float32), 1.5542331]. 
=============================================
[2019-04-03 23:37:01,186] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5395178e-14 1.7903051e-07 6.8613751e-13 1.0971116e-09 5.9198988e-09
 1.3489933e-14 9.9999976e-01], sum to 1.0000
[2019-04-03 23:37:01,186] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4337
[2019-04-03 23:37:01,231] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.166666666666667, 39.5, 53.33333333333334, 421.0, 26.0, 25.50005437139696, 0.4201272035677857, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4207800.0000, 
sim time next is 4208400.0000, 
raw observation next is [2.0, 40.0, 46.0, 356.5, 26.0, 25.42785039859876, 0.4040851151728455, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.7391304347826086, 0.518005540166205, 0.4, 0.15333333333333332, 0.3939226519337017, 0.6666666666666666, 0.6189875332165634, 0.6346950383909485, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8495907], dtype=float32), -0.48741803]. 
=============================================
[2019-04-03 23:37:03,047] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.7169473e-14 2.7157347e-07 3.3926661e-12 5.5607924e-10 1.2512339e-08
 1.3220118e-13 9.9999976e-01], sum to 1.0000
[2019-04-03 23:37:03,047] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0773
[2019-04-03 23:37:03,063] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.0, 63.00000000000001, 0.0, 0.0, 26.0, 24.34582878521737, 0.1574518595533868, 0.0, 1.0, 43588.42509113], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3986400.0000, 
sim time next is 3987000.0000, 
raw observation next is [-12.0, 63.0, 0.0, 0.0, 26.0, 24.29170067719379, 0.1472875693109435, 0.0, 1.0, 43596.35608925058], 
processed observation next is [1.0, 0.13043478260869565, 0.13019390581717452, 0.63, 0.0, 0.0, 0.6666666666666666, 0.5243083897661492, 0.5490958564369811, 0.0, 1.0, 0.207601695663098], 
reward next is 0.7924, 
noisyNet noise sample is [array([-0.5480265], dtype=float32), -0.6134667]. 
=============================================
[2019-04-03 23:37:03,167] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[70.89206]
 [70.95023]
 [70.86403]
 [70.82842]
 [70.83198]], R is [[70.92237854]
 [71.00559235]
 [71.08779144]
 [71.16884613]
 [71.2488327 ]].
[2019-04-03 23:37:05,655] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.5912498e-16 9.1100477e-10 8.7009579e-15 1.4748210e-11 8.7501707e-11
 3.9027665e-17 1.0000000e+00], sum to 1.0000
[2019-04-03 23:37:05,655] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8313
[2019-04-03 23:37:05,759] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.666666666666667, 40.0, 15.83333333333333, 140.8333333333333, 26.0, 25.69702293229609, 0.5612900615895957, 1.0, 1.0, 196225.1178760993], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3951600.0000, 
sim time next is 3952200.0000, 
raw observation next is [-5.833333333333333, 40.5, 0.0, 0.0, 26.0, 25.65646101942163, 0.5670568054386073, 1.0, 1.0, 146323.197853244], 
processed observation next is [1.0, 0.7391304347826086, 0.30101569713758086, 0.405, 0.0, 0.0, 0.6666666666666666, 0.6380384182851359, 0.6890189351462025, 1.0, 1.0, 0.6967771326344953], 
reward next is 0.3032, 
noisyNet noise sample is [array([1.4888538], dtype=float32), -0.18590336]. 
=============================================
[2019-04-03 23:37:12,447] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.3299538e-14 3.2373777e-07 7.4373225e-12 1.3522734e-09 9.9388417e-09
 2.7871333e-13 9.9999964e-01], sum to 1.0000
[2019-04-03 23:37:12,447] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4523
[2019-04-03 23:37:12,557] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-13.83333333333333, 68.0, 0.0, 0.0, 26.0, 23.42211401958189, -0.05571029908381894, 0.0, 1.0, 43091.98055258585], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3999000.0000, 
sim time next is 3999600.0000, 
raw observation next is [-14.0, 69.0, 0.0, 0.0, 26.0, 23.39417394220888, -0.06339554255597912, 0.0, 1.0, 42977.82518129573], 
processed observation next is [1.0, 0.30434782608695654, 0.07479224376731301, 0.69, 0.0, 0.0, 0.6666666666666666, 0.4495144951840733, 0.4788681524813403, 0.0, 1.0, 0.20465631038712254], 
reward next is 0.7953, 
noisyNet noise sample is [array([-0.46547273], dtype=float32), -0.840826]. 
=============================================
[2019-04-03 23:37:29,952] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1661645e-16 9.4277155e-08 2.0081015e-14 1.6428794e-11 3.6071690e-10
 1.8670489e-15 9.9999988e-01], sum to 1.0000
[2019-04-03 23:37:29,955] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8357
[2019-04-03 23:37:30,094] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.333333333333333, 53.33333333333334, 170.0, 118.0, 26.0, 25.67960804509556, 0.4078386390669975, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4267200.0000, 
sim time next is 4267800.0000, 
raw observation next is [3.5, 53.5, 182.0, 131.0, 26.0, 25.69727779652255, 0.4089265641969133, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.5595567867036012, 0.535, 0.6066666666666667, 0.14475138121546963, 0.6666666666666666, 0.6414398163768791, 0.6363088547323045, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.079506], dtype=float32), 0.60557896]. 
=============================================
[2019-04-03 23:37:30,231] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-03 23:37:30,232] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-03 23:37:30,232] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:37:30,240] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-03 23:37:30,240] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:37:30,242] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run13
[2019-04-03 23:37:30,270] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run13
[2019-04-03 23:37:30,288] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-03 23:37:30,292] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:37:30,314] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run13
[2019-04-03 23:40:29,607] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-03 23:40:48,495] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.7639 263389580.4542 1552.0485
[2019-04-03 23:40:52,668] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-03 23:40:53,692] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 1200000, evaluation results [1200000.0, 7241.763902599205, 263389580.45416775, 1552.0485098418105, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-03 23:40:57,741] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.9948144e-14 1.1309656e-07 1.4017137e-12 7.3958417e-10 8.5906624e-09
 3.8050709e-14 9.9999988e-01], sum to 1.0000
[2019-04-03 23:40:57,744] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2923
[2019-04-03 23:40:57,769] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.75, 40.83333333333334, 16.0, 108.6666666666667, 26.0, 25.04189392011346, 0.3313876467915563, 0.0, 1.0, 65900.23791365839], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4211400.0000, 
sim time next is 4212000.0000, 
raw observation next is [1.7, 41.0, 0.0, 0.0, 26.0, 25.031304833507, 0.3237445934079118, 0.0, 1.0, 46978.63046935506], 
processed observation next is [0.0, 0.782608695652174, 0.5096952908587258, 0.41, 0.0, 0.0, 0.6666666666666666, 0.5859420694589167, 0.6079148644693039, 0.0, 1.0, 0.22370776413978602], 
reward next is 0.7763, 
noisyNet noise sample is [array([1.4401627], dtype=float32), 0.40314478]. 
=============================================
[2019-04-03 23:40:57,781] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[76.6213 ]
 [76.60253]
 [76.69906]
 [77.27654]
 [77.95974]], R is [[75.70957947]
 [75.63867188]
 [75.62661743]
 [75.78140259]
 [76.02359009]].
[2019-04-03 23:40:57,955] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.5568876e-15 5.6781172e-08 1.4468346e-13 3.5474138e-10 3.5696985e-09
 8.8284076e-15 1.0000000e+00], sum to 1.0000
[2019-04-03 23:40:57,956] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8637
[2019-04-03 23:40:57,969] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.033333333333333, 68.16666666666666, 0.0, 0.0, 26.0, 25.01744459117037, 0.2943975495745514, 0.0, 1.0, 48663.91067003047], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4301400.0000, 
sim time next is 4302000.0000, 
raw observation next is [6.0, 69.0, 0.0, 0.0, 26.0, 24.96630326150372, 0.2907442377051258, 0.0, 1.0, 55146.80420370433], 
processed observation next is [0.0, 0.8260869565217391, 0.6288088642659281, 0.69, 0.0, 0.0, 0.6666666666666666, 0.5805252717919768, 0.5969147459017087, 0.0, 1.0, 0.2626038295414492], 
reward next is 0.7374, 
noisyNet noise sample is [array([-0.33073777], dtype=float32), -1.3339754]. 
=============================================
[2019-04-03 23:40:57,981] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[80.932205]
 [80.312485]
 [79.72942 ]
 [79.23541 ]
 [79.05622 ]], R is [[81.39305878]
 [81.34739685]
 [81.38480377]
 [81.48200226]
 [81.66718292]].
[2019-04-03 23:40:58,455] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.2539359e-16 1.8803501e-08 8.3377731e-15 7.5570262e-12 2.1142614e-10
 1.9336695e-16 1.0000000e+00], sum to 1.0000
[2019-04-03 23:40:58,456] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3890
[2019-04-03 23:40:58,476] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.0, 53.66666666666667, 176.6666666666667, 738.6666666666666, 26.0, 25.35491215580701, 0.4360772423698471, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4281600.0000, 
sim time next is 4282200.0000, 
raw observation next is [7.0, 54.5, 188.0, 717.0, 26.0, 25.36106963306225, 0.4393986945275611, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.6565096952908588, 0.545, 0.6266666666666667, 0.7922651933701658, 0.6666666666666666, 0.6134224694218542, 0.646466231509187, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9363811], dtype=float32), -1.4872723]. 
=============================================
[2019-04-03 23:40:59,478] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0419438e-15 9.3387321e-08 4.6427163e-14 5.1121128e-11 7.2910467e-10
 8.3223291e-15 9.9999988e-01], sum to 1.0000
[2019-04-03 23:40:59,483] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1443
[2019-04-03 23:40:59,503] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 26.0, 25.41014277907721, 0.344734533363761, 0.0, 1.0, 36889.47203391476], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4242600.0000, 
sim time next is 4243200.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 26.0, 25.40901265473968, 0.343146443317322, 0.0, 1.0, 37994.84727494366], 
processed observation next is [0.0, 0.08695652173913043, 0.5457063711911359, 0.45, 0.0, 0.0, 0.6666666666666666, 0.6174177212283066, 0.6143821477724406, 0.0, 1.0, 0.1809278441663984], 
reward next is 0.8191, 
noisyNet noise sample is [array([-0.18304522], dtype=float32), 0.18081726]. 
=============================================
[2019-04-03 23:41:06,932] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.4492408e-16 1.8330252e-09 5.2809421e-15 5.5140132e-11 8.6441056e-11
 1.7136330e-16 1.0000000e+00], sum to 1.0000
[2019-04-03 23:41:06,936] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.6210
[2019-04-03 23:41:06,960] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 88.5, 85.0, 0.0, 26.0, 26.33581025655494, 0.6245251625602335, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4458600.0000, 
sim time next is 4459200.0000, 
raw observation next is [0.0, 87.33333333333334, 82.66666666666667, 0.0, 26.0, 26.36180427825549, 0.5037399265647228, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.46260387811634357, 0.8733333333333334, 0.27555555555555555, 0.0, 0.6666666666666666, 0.6968170231879576, 0.6679133088549075, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.21454233], dtype=float32), 1.3393083]. 
=============================================
[2019-04-03 23:41:07,787] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [8.2007070e-17 1.0517267e-09 3.3011381e-15 1.2617916e-11 5.2855477e-11
 5.0539667e-17 1.0000000e+00], sum to 1.0000
[2019-04-03 23:41:07,792] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.1613
[2019-04-03 23:41:07,806] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.44504234336573, 0.5148626060936525, 1.0, 1.0, 21804.09664137168], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4471200.0000, 
sim time next is 4471800.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.5311145485748, 0.5117460545482023, 1.0, 1.0, 20350.1808182708], 
processed observation next is [1.0, 0.782608695652174, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6275928790478998, 0.6705820181827341, 1.0, 1.0, 0.09690562294414667], 
reward next is 0.9031, 
noisyNet noise sample is [array([1.5895325], dtype=float32), -1.9225534]. 
=============================================
[2019-04-03 23:41:09,743] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.6926231e-15 5.8244339e-08 1.1933091e-13 8.3783140e-11 1.1747505e-09
 9.7917847e-15 1.0000000e+00], sum to 1.0000
[2019-04-03 23:41:09,743] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.8129
[2019-04-03 23:41:09,755] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.9333333333333333, 73.0, 0.0, 0.0, 26.0, 25.38753533714316, 0.43501901690261, 0.0, 1.0, 46909.81484589176], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4506000.0000, 
sim time next is 4506600.0000, 
raw observation next is [-0.9166666666666666, 73.0, 0.0, 0.0, 26.0, 25.37348421044976, 0.4342007444057552, 0.0, 1.0, 50378.31392951412], 
processed observation next is [1.0, 0.13043478260869565, 0.4372114496768237, 0.73, 0.0, 0.0, 0.6666666666666666, 0.61445701753748, 0.6447335814685851, 0.0, 1.0, 0.2398967329976863], 
reward next is 0.7601, 
noisyNet noise sample is [array([-0.5685751], dtype=float32), -0.4805072]. 
=============================================
[2019-04-03 23:41:12,236] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.4341110e-17 2.2677350e-10 5.0635990e-16 1.5635668e-12 1.8129607e-11
 3.7362547e-17 1.0000000e+00], sum to 1.0000
[2019-04-03 23:41:12,243] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3756
[2019-04-03 23:41:12,252] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 89.66666666666667, 103.5, 0.9999999999999998, 26.0, 26.29376631326909, 0.6256801695036694, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4458000.0000, 
sim time next is 4458600.0000, 
raw observation next is [0.0, 88.5, 85.0, 0.0, 26.0, 26.33581025655494, 0.6245251625602335, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.46260387811634357, 0.885, 0.2833333333333333, 0.0, 0.6666666666666666, 0.6946508547129117, 0.7081750541867445, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.178742], dtype=float32), 1.0504618]. 
=============================================
[2019-04-03 23:41:14,924] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.3167251e-16 9.4541861e-08 5.1462086e-14 3.6698890e-11 1.0600766e-09
 6.5985395e-15 9.9999988e-01], sum to 1.0000
[2019-04-03 23:41:14,925] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2074
[2019-04-03 23:41:14,938] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.9500000000000001, 66.66666666666667, 0.0, 0.0, 26.0, 25.39501716320024, 0.4160682241486465, 0.0, 1.0, 76524.34802339422], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4589400.0000, 
sim time next is 4590000.0000, 
raw observation next is [-1.1, 67.0, 0.0, 0.0, 26.0, 25.33627715360218, 0.4222481672730278, 0.0, 1.0, 68763.56591948333], 
processed observation next is [1.0, 0.13043478260869565, 0.4321329639889197, 0.67, 0.0, 0.0, 0.6666666666666666, 0.6113564294668482, 0.6407493890910093, 0.0, 1.0, 0.32744555199753966], 
reward next is 0.6726, 
noisyNet noise sample is [array([-1.2253723], dtype=float32), -1.0205252]. 
=============================================
[2019-04-03 23:41:14,955] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[79.77851 ]
 [79.66128 ]
 [79.855484]
 [80.03891 ]
 [80.14551 ]], R is [[79.73641968]
 [79.57465363]
 [79.68961334]
 [79.80341339]
 [79.85691071]].
[2019-04-03 23:41:17,462] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.63987587e-17 3.71422915e-09 1.93491107e-14 2.06289777e-11
 2.22002486e-10 1.04891835e-16 1.00000000e+00], sum to 1.0000
[2019-04-03 23:41:17,462] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0599
[2019-04-03 23:41:17,475] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 61.0, 0.0, 0.0, 26.0, 25.34552208499587, 0.4710607307648164, 0.0, 1.0, 41422.84099416018], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4576200.0000, 
sim time next is 4576800.0000, 
raw observation next is [1.0, 61.0, 0.0, 0.0, 26.0, 25.39470425791663, 0.4737709974338726, 0.0, 1.0, 26793.37755993821], 
processed observation next is [1.0, 1.0, 0.4903047091412743, 0.61, 0.0, 0.0, 0.6666666666666666, 0.6162253548263857, 0.6579236658112909, 0.0, 1.0, 0.12758751219018194], 
reward next is 0.8724, 
noisyNet noise sample is [array([0.4731132], dtype=float32), 0.88029355]. 
=============================================
[2019-04-03 23:41:19,238] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.9952213e-17 3.3049048e-08 6.0322214e-15 1.1689068e-11 1.3515512e-10
 1.2535006e-16 1.0000000e+00], sum to 1.0000
[2019-04-03 23:41:19,238] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8126
[2019-04-03 23:41:19,291] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 92.0, 31.5, 0.0, 26.0, 25.78181411286408, 0.467574402467769, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4694400.0000, 
sim time next is 4695000.0000, 
raw observation next is [0.0, 92.0, 42.00000000000001, 0.0, 26.0, 25.72143382821222, 0.45960770593302, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.46260387811634357, 0.92, 0.14, 0.0, 0.6666666666666666, 0.643452819017685, 0.65320256864434, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0098729], dtype=float32), 0.011704182]. 
=============================================
[2019-04-03 23:41:19,306] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[92.85773 ]
 [92.77078 ]
 [92.1405  ]
 [90.43322 ]
 [87.399704]], R is [[87.34791565]
 [87.4744339 ]
 [87.5996933 ]
 [87.72369385]
 [87.84645844]].
[2019-04-03 23:41:25,828] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.6948260e-14 7.0552841e-07 1.7137627e-12 3.1842973e-10 2.7388101e-08
 1.2318292e-13 9.9999928e-01], sum to 1.0000
[2019-04-03 23:41:25,828] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7313
[2019-04-03 23:41:25,846] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.40909910181964, 0.2211579152718629, 0.0, 1.0, 41156.67768069321], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4768800.0000, 
sim time next is 4769400.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.39980223686034, 0.2107546728603556, 0.0, 1.0, 41192.67398264114], 
processed observation next is [0.0, 0.17391304347826086, 0.296398891966759, 0.92, 0.0, 0.0, 0.6666666666666666, 0.5333168530716949, 0.5702515576201185, 0.0, 1.0, 0.19615559039352926], 
reward next is 0.8038, 
noisyNet noise sample is [array([0.26435202], dtype=float32), 0.7534643]. 
=============================================
[2019-04-03 23:41:29,025] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:41:29,027] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:41:29,048] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run10
[2019-04-03 23:41:29,084] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [9.0663586e-15 1.6101481e-07 3.6846280e-13 2.5136893e-10 3.2221068e-09
 2.4243913e-14 9.9999988e-01], sum to 1.0000
[2019-04-03 23:41:29,089] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.3749
[2019-04-03 23:41:29,103] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 55.0, 0.0, 0.0, 26.0, 25.37970209879177, 0.3897163277249688, 0.0, 1.0, 52233.35679059825], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4833000.0000, 
sim time next is 4833600.0000, 
raw observation next is [-1.0, 55.0, 0.0, 0.0, 26.0, 25.36318799178657, 0.3884201963013729, 0.0, 1.0, 49950.99718044487], 
processed observation next is [0.0, 0.9565217391304348, 0.4349030470914128, 0.55, 0.0, 0.0, 0.6666666666666666, 0.6135989993155476, 0.6294733987671243, 0.0, 1.0, 0.23786189133545177], 
reward next is 0.7621, 
noisyNet noise sample is [array([0.6681398], dtype=float32), 0.53140736]. 
=============================================
[2019-04-03 23:41:31,214] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:41:31,214] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:41:31,218] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run10
[2019-04-03 23:41:36,539] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.0772960e-15 6.6146481e-07 1.8387944e-13 1.2469580e-10 4.6676045e-09
 4.3047641e-14 9.9999928e-01], sum to 1.0000
[2019-04-03 23:41:36,541] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1224
[2019-04-03 23:41:36,646] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.833333333333333, 70.0, 94.00000000000001, 208.6666666666667, 26.0, 24.36616879011169, 0.1721040760701651, 0.0, 1.0, 39078.21478767756], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4867800.0000, 
sim time next is 4868400.0000, 
raw observation next is [-3.666666666666667, 69.0, 117.5, 260.8333333333334, 26.0, 24.40235324840997, 0.2654942244284128, 0.0, 1.0, 202487.9849510309], 
processed observation next is [0.0, 0.34782608695652173, 0.3610341643582641, 0.69, 0.39166666666666666, 0.2882136279926336, 0.6666666666666666, 0.5335294373674975, 0.5884980748094709, 0.0, 1.0, 0.9642284997668138], 
reward next is 0.0358, 
noisyNet noise sample is [array([0.12126171], dtype=float32), -0.025635611]. 
=============================================
[2019-04-03 23:41:38,400] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.5373346e-15 2.8331069e-08 1.2898885e-13 3.9741863e-11 1.1786040e-09
 4.5740940e-15 1.0000000e+00], sum to 1.0000
[2019-04-03 23:41:38,402] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0340
[2019-04-03 23:41:38,433] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.1666666666666667, 44.16666666666667, 0.0, 0.0, 26.0, 25.62500837075976, 0.3375598722571613, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4929000.0000, 
sim time next is 4929600.0000, 
raw observation next is [-0.3333333333333333, 45.33333333333334, 0.0, 0.0, 26.0, 25.51478935867408, 0.3203626648565384, 0.0, 1.0, 28205.84324136395], 
processed observation next is [1.0, 0.043478260869565216, 0.4533702677747, 0.4533333333333334, 0.0, 0.0, 0.6666666666666666, 0.6262324465561733, 0.6067875549521795, 0.0, 1.0, 0.13431353924459025], 
reward next is 0.8657, 
noisyNet noise sample is [array([-1.0893922], dtype=float32), -0.2519223]. 
=============================================
[2019-04-03 23:41:38,534] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.7081041e-17 2.8995384e-09 2.8014029e-16 3.9985758e-12 2.8785684e-11
 2.4292237e-18 1.0000000e+00], sum to 1.0000
[2019-04-03 23:41:38,535] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2219
[2019-04-03 23:41:38,544] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.333333333333334, 25.66666666666667, 40.33333333333333, 360.1666666666666, 26.0, 27.5513221633741, 0.836684352814861, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4987200.0000, 
sim time next is 4987800.0000, 
raw observation next is [7.0, 25.5, 34.0, 304.0, 26.0, 27.50521080084692, 0.8255942882123627, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.6565096952908588, 0.255, 0.11333333333333333, 0.33591160220994476, 0.6666666666666666, 0.7921009000705768, 0.7751980960707875, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5537244], dtype=float32), 0.5453307]. 
=============================================
[2019-04-03 23:41:39,833] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:41:39,833] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:41:39,837] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run10
[2019-04-03 23:41:40,261] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:41:40,261] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:41:40,274] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run10
[2019-04-03 23:41:40,298] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:41:40,300] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:41:40,316] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run10
[2019-04-03 23:41:43,912] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:41:43,912] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:41:43,916] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run10
[2019-04-03 23:41:44,177] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:41:44,177] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:41:44,183] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run10
[2019-04-03 23:41:45,117] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:41:45,117] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:41:45,121] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run10
[2019-04-03 23:41:45,150] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:41:45,151] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:41:45,157] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run10
[2019-04-03 23:41:45,661] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:41:45,661] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:41:45,685] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run10
[2019-04-03 23:41:45,847] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.7418608e-19 7.5263223e-10 1.5912001e-16 8.8329647e-13 2.2365226e-11
 6.8200214e-19 1.0000000e+00], sum to 1.0000
[2019-04-03 23:41:45,848] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1157
[2019-04-03 23:41:45,881] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.0, 29.33333333333334, 123.1666666666667, 848.3333333333334, 26.0, 27.03425777672066, 0.8104373403420467, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5053200.0000, 
sim time next is 5053800.0000, 
raw observation next is [7.5, 27.66666666666666, 123.3333333333333, 851.6666666666666, 26.0, 27.25895281135847, 0.637765280829557, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.6703601108033241, 0.2766666666666666, 0.411111111111111, 0.9410681399631675, 0.6666666666666666, 0.7715794009465391, 0.7125884269431856, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.38917053], dtype=float32), -1.2384589]. 
=============================================
[2019-04-03 23:41:48,877] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:41:48,878] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:41:48,881] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run10
[2019-04-03 23:41:48,981] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:41:48,981] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:41:48,985] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run10
[2019-04-03 23:41:49,525] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:41:49,525] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:41:49,530] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run10
[2019-04-03 23:41:49,703] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:41:49,703] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:41:49,706] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run10
[2019-04-03 23:41:50,769] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:41:50,769] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:41:50,777] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run10
[2019-04-03 23:41:51,220] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:41:51,220] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:41:51,224] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run10
[2019-04-03 23:42:24,052] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.2609559e-16 5.8526584e-10 1.1904237e-14 3.1484666e-11 9.0925274e-11
 1.2571449e-16 1.0000000e+00], sum to 1.0000
[2019-04-03 23:42:24,052] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4486
[2019-04-03 23:42:24,086] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.516666666666667, 96.0, 0.0, 0.0, 26.0, 24.86220375604746, 0.235338838153769, 0.0, 1.0, 47632.57750106947], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 507000.0000, 
sim time next is 507600.0000, 
raw observation next is [1.6, 96.0, 0.0, 0.0, 26.0, 24.87121610933739, 0.2357860922663906, 0.0, 1.0, 43770.73791204755], 
processed observation next is [1.0, 0.9130434782608695, 0.5069252077562327, 0.96, 0.0, 0.0, 0.6666666666666666, 0.5726013424447824, 0.5785953640887969, 0.0, 1.0, 0.20843208529546453], 
reward next is 0.7916, 
noisyNet noise sample is [array([-0.6592965], dtype=float32), 0.22981846]. 
=============================================
[2019-04-03 23:42:34,163] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.8904906e-16 4.4731796e-09 7.1477722e-15 5.5812670e-11 1.2238227e-10
 3.1358909e-16 1.0000000e+00], sum to 1.0000
[2019-04-03 23:42:34,163] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0519
[2019-04-03 23:42:34,216] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-13.4, 60.0, 64.5, 746.5, 26.0, 25.87398962055461, 0.3450403505080835, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 385200.0000, 
sim time next is 385800.0000, 
raw observation next is [-13.3, 58.5, 62.33333333333333, 752.3333333333333, 26.0, 25.83821537888281, 0.34130228769736, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.09418282548476452, 0.585, 0.20777777777777776, 0.8313075506445672, 0.6666666666666666, 0.6531846149069009, 0.6137674292324533, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.38879818], dtype=float32), -0.32819742]. 
=============================================
[2019-04-03 23:42:52,642] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.2369996e-18 5.8481975e-09 9.1284391e-16 5.0200751e-12 2.3145789e-11
 2.2157783e-17 1.0000000e+00], sum to 1.0000
[2019-04-03 23:42:52,650] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3637
[2019-04-03 23:42:52,740] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.700000000000001, 72.33333333333334, 0.0, 0.0, 26.0, 24.3714914651032, 0.1711193231616999, 1.0, 1.0, 150411.8790991431], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 805200.0000, 
sim time next is 805800.0000, 
raw observation next is [-6.7, 73.66666666666666, 10.66666666666666, 0.0, 26.0, 24.79801380084196, 0.2403767352183723, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.2770083102493075, 0.7366666666666666, 0.035555555555555535, 0.0, 0.6666666666666666, 0.5665011500701634, 0.5801255784061241, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.249805], dtype=float32), -1.7632204]. 
=============================================
[2019-04-03 23:42:53,976] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.4534595e-15 1.5630631e-08 6.2515233e-14 6.4121243e-11 5.7583832e-10
 3.7834311e-15 1.0000000e+00], sum to 1.0000
[2019-04-03 23:42:53,982] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.5849
[2019-04-03 23:42:54,054] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.5, 60.0, 112.0, 100.0, 26.0, 24.88906778104467, 0.2267233390359283, 0.0, 1.0, 40008.96086761865], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 649800.0000, 
sim time next is 650400.0000, 
raw observation next is [-2.433333333333334, 59.66666666666666, 123.6666666666667, 98.83333333333334, 26.0, 24.88684818763705, 0.2300963601012573, 0.0, 1.0, 42425.49150000916], 
processed observation next is [0.0, 0.5217391304347826, 0.3951985226223454, 0.5966666666666666, 0.4122222222222223, 0.10920810313075507, 0.6666666666666666, 0.5739040156364208, 0.5766987867004191, 0.0, 1.0, 0.2020261500000436], 
reward next is 0.7980, 
noisyNet noise sample is [array([1.3528923], dtype=float32), 0.9610763]. 
=============================================
[2019-04-03 23:43:04,600] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.2888586e-16 4.2554802e-08 1.0352888e-13 3.2493081e-11 3.0280084e-10
 9.4263300e-16 1.0000000e+00], sum to 1.0000
[2019-04-03 23:43:04,602] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3351
[2019-04-03 23:43:04,620] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.300000000000001, 73.66666666666667, 0.0, 0.0, 26.0, 23.85230041702727, 0.007442174486992341, 0.0, 1.0, 41414.379264389], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 793200.0000, 
sim time next is 793800.0000, 
raw observation next is [-7.3, 73.0, 0.0, 0.0, 26.0, 23.80261139196135, 0.0005210455607558793, 0.0, 1.0, 41495.82828338195], 
processed observation next is [1.0, 0.17391304347826086, 0.26038781163434904, 0.73, 0.0, 0.0, 0.6666666666666666, 0.48355094933011245, 0.5001736818535852, 0.0, 1.0, 0.1975991823018188], 
reward next is 0.8024, 
noisyNet noise sample is [array([0.17414956], dtype=float32), 1.3991551]. 
=============================================
[2019-04-03 23:43:26,307] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0484337e-17 1.9980511e-09 5.2031752e-15 5.7587588e-13 5.7647432e-11
 8.4666429e-17 1.0000000e+00], sum to 1.0000
[2019-04-03 23:43:26,308] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6675
[2019-04-03 23:43:26,343] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.699999999999999, 82.0, 0.0, 0.0, 26.0, 25.46361490029802, 0.4467761501924883, 0.0, 1.0, 18759.29653333307], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 963600.0000, 
sim time next is 964200.0000, 
raw observation next is [7.7, 82.5, 0.0, 0.0, 26.0, 25.47566013077125, 0.4553487707229321, 0.0, 1.0, 18756.06842844894], 
processed observation next is [1.0, 0.13043478260869565, 0.6759002770083103, 0.825, 0.0, 0.0, 0.6666666666666666, 0.6229716775642707, 0.6517829235743107, 0.0, 1.0, 0.08931461156404256], 
reward next is 0.9107, 
noisyNet noise sample is [array([0.4918129], dtype=float32), 0.9223558]. 
=============================================
[2019-04-03 23:43:30,644] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.5043585e-17 7.0423485e-09 2.1001168e-15 8.5879966e-12 9.3225774e-11
 6.7478278e-17 1.0000000e+00], sum to 1.0000
[2019-04-03 23:43:30,647] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9632
[2019-04-03 23:43:30,685] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.6, 77.0, 0.0, 0.0, 26.0, 25.64744354767834, 0.6187837538314601, 0.0, 1.0, 21317.82871265694], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1141200.0000, 
sim time next is 1141800.0000, 
raw observation next is [11.6, 78.0, 0.0, 0.0, 26.0, 25.66381537278685, 0.6221985357629286, 0.0, 1.0, 18725.29157426547], 
processed observation next is [0.0, 0.21739130434782608, 0.7839335180055402, 0.78, 0.0, 0.0, 0.6666666666666666, 0.6386512810655708, 0.7073995119209763, 0.0, 1.0, 0.08916805511554986], 
reward next is 0.9108, 
noisyNet noise sample is [array([-0.9656746], dtype=float32), 0.8129663]. 
=============================================
[2019-04-03 23:43:36,837] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.3140335e-17 3.0688923e-08 2.4536749e-15 2.2344151e-12 4.9416925e-11
 3.2071963e-16 1.0000000e+00], sum to 1.0000
[2019-04-03 23:43:36,846] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.8503
[2019-04-03 23:43:36,866] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.1, 77.0, 0.0, 0.0, 26.0, 25.65348451677617, 0.623928973943767, 0.0, 1.0, 20445.46271672026], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1135800.0000, 
sim time next is 1136400.0000, 
raw observation next is [11.1, 77.0, 0.0, 0.0, 26.0, 25.65027738380407, 0.621555073383898, 0.0, 1.0, 23782.84557814518], 
processed observation next is [0.0, 0.13043478260869565, 0.7700831024930749, 0.77, 0.0, 0.0, 0.6666666666666666, 0.6375231153170059, 0.7071850244612993, 0.0, 1.0, 0.11325164561021515], 
reward next is 0.8867, 
noisyNet noise sample is [array([-0.25945088], dtype=float32), 0.24751967]. 
=============================================
[2019-04-03 23:43:37,028] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.9870292e-19 1.4030500e-10 1.3164918e-16 1.8973066e-13 1.6722592e-11
 4.5485785e-18 1.0000000e+00], sum to 1.0000
[2019-04-03 23:43:37,062] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9150
[2019-04-03 23:43:37,113] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.8, 63.66666666666666, 0.0, 0.0, 26.0, 25.85807146590832, 0.684409044622568, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1115400.0000, 
sim time next is 1116000.0000, 
raw observation next is [12.7, 64.0, 0.0, 0.0, 26.0, 25.83341125379014, 0.6744956281030136, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.8144044321329641, 0.64, 0.0, 0.0, 0.6666666666666666, 0.6527842711491783, 0.7248318760343379, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.48958707], dtype=float32), 0.49369526]. 
=============================================
[2019-04-03 23:43:37,209] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[92.04446 ]
 [92.562294]
 [93.03655 ]
 [93.867134]
 [94.47002 ]], R is [[91.20133209]
 [91.2893219 ]
 [91.3764267 ]
 [91.46266174]
 [91.54803467]].
[2019-04-03 23:43:37,809] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.04389664e-16 8.35721110e-08 4.60004324e-15 1.27864143e-11
 4.93143790e-11 6.58350111e-16 9.99999881e-01], sum to 1.0000
[2019-04-03 23:43:37,810] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3321
[2019-04-03 23:43:37,816] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.0, 97.33333333333333, 23.33333333333334, 0.0, 26.0, 23.40081039658282, 0.1293995890542536, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1239600.0000, 
sim time next is 1240200.0000, 
raw observation next is [15.0, 98.0, 28.0, 0.0, 26.0, 23.38170495580534, 0.1275564670962326, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.8781163434903049, 0.98, 0.09333333333333334, 0.0, 0.6666666666666666, 0.4484754129837783, 0.5425188223654108, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.17756459], dtype=float32), 0.23902585]. 
=============================================
[2019-04-03 23:43:41,402] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.9597113e-17 9.2164543e-09 5.2143221e-15 9.8350373e-12 5.3862914e-11
 4.7483624e-16 1.0000000e+00], sum to 1.0000
[2019-04-03 23:43:41,408] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1455
[2019-04-03 23:43:41,413] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [18.38333333333333, 64.66666666666667, 167.0, 0.0, 26.0, 25.09024058892759, 0.5025021718091638, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1169400.0000, 
sim time next is 1170000.0000, 
raw observation next is [18.3, 65.0, 165.0, 0.0, 26.0, 25.07908776583792, 0.5006153161919765, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.9695290858725764, 0.65, 0.55, 0.0, 0.6666666666666666, 0.5899239804864932, 0.6668717720639922, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8726614], dtype=float32), 0.75337076]. 
=============================================
[2019-04-03 23:43:41,457] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[96.04216 ]
 [96.3033  ]
 [96.54297 ]
 [96.499084]
 [95.95129 ]], R is [[95.8122406 ]
 [95.85411835]
 [95.89557648]
 [95.93662262]
 [95.97725677]].
[2019-04-03 23:43:46,312] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.65603008e-17 2.19279794e-09 2.77935989e-15 7.12946932e-12
 1.03202946e-10 6.24136293e-17 1.00000000e+00], sum to 1.0000
[2019-04-03 23:43:46,314] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4628
[2019-04-03 23:43:46,345] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.016666666666667, 96.0, 0.0, 0.0, 26.0, 24.70398736139458, 0.4487052520078108, 0.0, 1.0, 27244.39009535813], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1278600.0000, 
sim time next is 1279200.0000, 
raw observation next is [6.833333333333334, 96.0, 0.0, 0.0, 26.0, 24.71221965435647, 0.4628862448958436, 0.0, 1.0, 198762.5007759919], 
processed observation next is [0.0, 0.8260869565217391, 0.651892890120037, 0.96, 0.0, 0.0, 0.6666666666666666, 0.5593516378630392, 0.6542954149652812, 0.0, 1.0, 0.9464880989332948], 
reward next is 0.0535, 
noisyNet noise sample is [array([1.0998129], dtype=float32), 1.5020264]. 
=============================================
[2019-04-03 23:43:55,812] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.9487815e-18 6.6349634e-11 1.8778051e-16 1.3397230e-12 2.9028707e-12
 4.2948081e-19 1.0000000e+00], sum to 1.0000
[2019-04-03 23:43:55,816] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9283
[2019-04-03 23:43:55,913] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 26.0, 24.90458596037231, 0.4627027194062487, 1.0, 1.0, 84083.71554617138], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1368000.0000, 
sim time next is 1368600.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 26.0, 24.88443875626726, 0.4744060253957358, 1.0, 1.0, 70095.54980436305], 
processed observation next is [1.0, 0.8695652173913043, 0.4764542936288089, 0.96, 0.0, 0.0, 0.6666666666666666, 0.5737032296889385, 0.6581353417985786, 1.0, 1.0, 0.33378833240172884], 
reward next is 0.6662, 
noisyNet noise sample is [array([-1.098391], dtype=float32), -0.19078392]. 
=============================================
[2019-04-03 23:43:57,642] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.8100519e-18 8.0217402e-11 1.4243579e-16 6.0306859e-13 1.1009463e-12
 1.7364455e-18 1.0000000e+00], sum to 1.0000
[2019-04-03 23:43:57,642] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5758
[2019-04-03 23:43:57,672] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 107.1666666666667, 0.0, 26.0, 25.67275469348057, 0.5326804542606867, 1.0, 1.0, 42508.27961054981], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1344000.0000, 
sim time next is 1344600.0000, 
raw observation next is [1.1, 92.0, 106.0, 0.0, 26.0, 25.71353115963649, 0.5298969784318149, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.49307479224376743, 0.92, 0.35333333333333333, 0.0, 0.6666666666666666, 0.6427942633030407, 0.6766323261439383, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.01030053], dtype=float32), 0.41152316]. 
=============================================
[2019-04-03 23:44:09,652] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.0073930e-18 1.0901526e-10 2.0233707e-16 2.6519056e-12 1.0649211e-11
 1.1830784e-18 1.0000000e+00], sum to 1.0000
[2019-04-03 23:44:09,652] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7269
[2019-04-03 23:44:09,681] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.78333333333333, 56.66666666666667, 58.66666666666667, 20.66666666666667, 26.0, 26.01043173754127, 0.6791675905729869, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1529400.0000, 
sim time next is 1530000.0000, 
raw observation next is [10.5, 58.0, 44.5, 17.0, 26.0, 26.47176269894893, 0.7117291408841889, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.7534626038781165, 0.58, 0.14833333333333334, 0.01878453038674033, 0.6666666666666666, 0.7059802249124107, 0.7372430469613963, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2690514], dtype=float32), -1.1227864]. 
=============================================
[2019-04-03 23:44:09,765] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[90.79871]
 [90.95925]
 [91.02582]
 [91.18868]
 [91.33406]], R is [[90.83667755]
 [90.92831421]
 [91.01903534]
 [91.10884857]
 [91.19776154]].
[2019-04-03 23:44:21,907] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.32530984e-16 3.30365513e-09 6.08470649e-15 2.09887559e-11
 3.94295811e-11 1.05126576e-16 1.00000000e+00], sum to 1.0000
[2019-04-03 23:44:21,908] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7914
[2019-04-03 23:44:21,970] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 26.0, 25.37778900648344, 0.4685460934494231, 0.0, 1.0, 39655.30938154563], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1731000.0000, 
sim time next is 1731600.0000, 
raw observation next is [0.5, 92.0, 0.0, 0.0, 26.0, 25.37278151432364, 0.4633704071008434, 0.0, 1.0, 43751.60024673781], 
processed observation next is [0.0, 0.043478260869565216, 0.4764542936288089, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6143984595269701, 0.6544568023669478, 0.0, 1.0, 0.20834095355589435], 
reward next is 0.7917, 
noisyNet noise sample is [array([0.10837189], dtype=float32), -0.20652965]. 
=============================================
[2019-04-03 23:44:32,185] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.2995140e-17 8.4390067e-10 8.5733185e-15 2.5752431e-11 8.9634175e-11
 1.7351892e-16 1.0000000e+00], sum to 1.0000
[2019-04-03 23:44:32,218] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7278
[2019-04-03 23:44:32,302] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.816666666666666, 78.33333333333334, 0.0, 0.0, 26.0, 25.33151688927054, 0.3483495085572961, 1.0, 1.0, 24675.21412799677], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1965000.0000, 
sim time next is 1965600.0000, 
raw observation next is [-5.0, 79.0, 0.0, 0.0, 26.0, 25.37753790425285, 0.3459471264915204, 1.0, 1.0, 33762.73584754415], 
processed observation next is [1.0, 0.782608695652174, 0.32409972299168976, 0.79, 0.0, 0.0, 0.6666666666666666, 0.6147948253544042, 0.6153157088305068, 1.0, 1.0, 0.1607749326073531], 
reward next is 0.8392, 
noisyNet noise sample is [array([-0.5164487], dtype=float32), -1.1054461]. 
=============================================
[2019-04-03 23:44:33,869] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.1873323e-14 1.9699480e-07 2.4288690e-12 8.5939672e-10 5.8038805e-09
 2.1780081e-13 9.9999976e-01], sum to 1.0000
[2019-04-03 23:44:33,870] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.5996
[2019-04-03 23:44:33,899] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.133333333333333, 85.66666666666667, 0.0, 0.0, 26.0, 24.06332765438486, 0.08403182840447992, 0.0, 1.0, 46624.14415323121], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1824000.0000, 
sim time next is 1824600.0000, 
raw observation next is [-6.166666666666667, 86.33333333333333, 0.0, 0.0, 26.0, 24.0247455309228, 0.0763339759804152, 0.0, 1.0, 46684.50859387936], 
processed observation next is [0.0, 0.08695652173913043, 0.2917820867959372, 0.8633333333333333, 0.0, 0.0, 0.6666666666666666, 0.5020621275769, 0.5254446586601383, 0.0, 1.0, 0.22230718378037792], 
reward next is 0.7777, 
noisyNet noise sample is [array([1.0684779], dtype=float32), 0.7888596]. 
=============================================
[2019-04-03 23:44:34,449] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.8261464e-14 2.7683024e-07 8.9702475e-13 1.4168949e-09 7.1121482e-09
 1.0952924e-13 9.9999976e-01], sum to 1.0000
[2019-04-03 23:44:34,449] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8006
[2019-04-03 23:44:34,543] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.600000000000001, 77.5, 129.3333333333333, 65.33333333333333, 26.0, 24.93607076169066, 0.2105018772562035, 0.0, 1.0, 76125.10333373927], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1851000.0000, 
sim time next is 1851600.0000, 
raw observation next is [-5.6, 77.0, 124.6666666666667, 58.16666666666666, 26.0, 24.87151544817926, 0.2201045392711136, 0.0, 1.0, 105087.6182916491], 
processed observation next is [0.0, 0.43478260869565216, 0.30747922437673136, 0.77, 0.4155555555555557, 0.06427255985267034, 0.6666666666666666, 0.5726262873482716, 0.5733681797570379, 0.0, 1.0, 0.5004172299602337], 
reward next is 0.4996, 
noisyNet noise sample is [array([0.97683394], dtype=float32), 0.73492485]. 
=============================================
[2019-04-03 23:44:42,803] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.7798022e-14 1.7679919e-07 7.1011686e-13 1.0304210e-09 6.3076162e-09
 7.2600725e-14 9.9999988e-01], sum to 1.0000
[2019-04-03 23:44:42,803] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7619
[2019-04-03 23:44:42,889] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 77.66666666666667, 64.83333333333333, 0.0, 26.0, 25.04672624853971, 0.2626318113096386, 0.0, 1.0, 45191.44755567658], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1870800.0000, 
sim time next is 1871400.0000, 
raw observation next is [-4.5, 76.33333333333333, 57.66666666666667, 0.0, 26.0, 25.02830136689433, 0.259418259302713, 0.0, 1.0, 52481.31523078598], 
processed observation next is [0.0, 0.6521739130434783, 0.3379501385041552, 0.7633333333333333, 0.19222222222222224, 0.0, 0.6666666666666666, 0.5856917805745274, 0.5864727531009043, 0.0, 1.0, 0.24991102490850467], 
reward next is 0.7501, 
noisyNet noise sample is [array([-1.8916727], dtype=float32), -0.08896769]. 
=============================================
[2019-04-03 23:44:48,017] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.5163433e-15 2.0692099e-08 8.0984712e-14 3.1617498e-10 6.9040790e-10
 7.2950357e-15 1.0000000e+00], sum to 1.0000
[2019-04-03 23:44:48,018] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1383
[2019-04-03 23:44:48,074] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.916666666666667, 85.5, 0.0, 0.0, 26.0, 25.01088581617411, 0.247057088521619, 0.0, 1.0, 49331.25171534764], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1879800.0000, 
sim time next is 1880400.0000, 
raw observation next is [-4.833333333333334, 85.0, 0.0, 0.0, 26.0, 25.01412686548381, 0.2470434658567346, 0.0, 1.0, 42433.77974203333], 
processed observation next is [0.0, 0.782608695652174, 0.32871652816251157, 0.85, 0.0, 0.0, 0.6666666666666666, 0.5845105721236509, 0.5823478219522449, 0.0, 1.0, 0.20206561781920634], 
reward next is 0.7979, 
noisyNet noise sample is [array([1.6494766], dtype=float32), -0.39959642]. 
=============================================
[2019-04-03 23:44:50,055] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.5533296e-19 4.4605364e-09 5.1424365e-16 1.1378109e-12 8.9820677e-12
 1.4016916e-17 1.0000000e+00], sum to 1.0000
[2019-04-03 23:44:50,058] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6812
[2019-04-03 23:44:50,137] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.633333333333333, 84.83333333333334, 67.66666666666667, 373.0000000000001, 26.0, 25.52066288090869, 0.2643639711752669, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1933800.0000, 
sim time next is 1934400.0000, 
raw observation next is [-8.366666666666667, 83.66666666666667, 76.33333333333334, 461.0, 26.0, 25.5636028498919, 0.2870699257526688, 1.0, 1.0, 18744.42614321085], 
processed observation next is [1.0, 0.391304347826087, 0.23084025854108958, 0.8366666666666667, 0.2544444444444445, 0.5093922651933702, 0.6666666666666666, 0.6303002374909917, 0.5956899752508896, 1.0, 1.0, 0.08925917211052786], 
reward next is 0.9107, 
noisyNet noise sample is [array([-1.7280569], dtype=float32), -0.37452272]. 
=============================================
[2019-04-03 23:45:02,165] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.3085965e-17 1.7800335e-10 1.5149788e-15 5.2873903e-12 3.0689343e-11
 6.5296738e-17 1.0000000e+00], sum to 1.0000
[2019-04-03 23:45:02,165] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5617
[2019-04-03 23:45:02,253] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.1, 81.33333333333334, 111.3333333333333, 0.0, 26.0, 26.36680308968076, 0.4885136725098649, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2038800.0000, 
sim time next is 2039400.0000, 
raw observation next is [-4.2, 82.5, 104.0, 0.0, 26.0, 26.36771387626353, 0.4845778459231884, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.34626038781163443, 0.825, 0.3466666666666667, 0.0, 0.6666666666666666, 0.6973094896886275, 0.6615259486410628, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.16378678], dtype=float32), -0.14587578]. 
=============================================
[2019-04-03 23:45:07,185] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.5627647e-16 2.2919624e-08 6.2387189e-14 1.0220176e-10 2.9382308e-09
 2.9925064e-15 1.0000000e+00], sum to 1.0000
[2019-04-03 23:45:07,188] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7949
[2019-04-03 23:45:07,241] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 87.66666666666666, 0.0, 0.0, 26.0, 24.6442237643293, 0.2168401080445682, 0.0, 1.0, 42711.76525830389], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2079600.0000, 
sim time next is 2080200.0000, 
raw observation next is [-4.5, 86.83333333333333, 0.0, 0.0, 26.0, 24.59945381219009, 0.2088844710155276, 0.0, 1.0, 42726.77149472436], 
processed observation next is [1.0, 0.043478260869565216, 0.3379501385041552, 0.8683333333333333, 0.0, 0.0, 0.6666666666666666, 0.5499544843491743, 0.5696281570051759, 0.0, 1.0, 0.20346081664154456], 
reward next is 0.7965, 
noisyNet noise sample is [array([-0.8764113], dtype=float32), 1.3518728]. 
=============================================
[2019-04-03 23:45:11,635] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.5167977e-18 1.5057411e-10 2.2924558e-16 2.4396088e-12 1.8245741e-12
 3.6442770e-18 1.0000000e+00], sum to 1.0000
[2019-04-03 23:45:11,635] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8744
[2019-04-03 23:45:11,758] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.9666666666666668, 47.0, 204.3333333333333, 67.5, 26.0, 24.69597617636885, 0.3475088337999195, 1.0, 1.0, 198531.0323868092], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2295600.0000, 
sim time next is 2296200.0000, 
raw observation next is [-0.7833333333333332, 46.0, 187.6666666666667, 66.0, 26.0, 25.27813311453504, 0.4151256246957483, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.44090489381348114, 0.46, 0.6255555555555558, 0.07292817679558011, 0.6666666666666666, 0.6065110928779202, 0.6383752082319161, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5416297], dtype=float32), 0.8787573]. 
=============================================
[2019-04-03 23:45:13,875] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.6506340e-18 5.0074572e-10 2.1008529e-16 3.1770528e-12 4.6934188e-12
 1.9302260e-18 1.0000000e+00], sum to 1.0000
[2019-04-03 23:45:13,875] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0607
[2019-04-03 23:45:13,957] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.916666666666667, 70.5, 0.0, 0.0, 26.0, 24.78229745940432, 0.4140870647892385, 1.0, 1.0, 199951.72983791], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2137800.0000, 
sim time next is 2138400.0000, 
raw observation next is [-5.0, 71.0, 0.0, 0.0, 26.0, 25.26307088374368, 0.4763441599377025, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.32409972299168976, 0.71, 0.0, 0.0, 0.6666666666666666, 0.60525590697864, 0.6587813866459008, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5171204], dtype=float32), 0.9291774]. 
=============================================
[2019-04-03 23:45:17,911] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.4486716e-18 3.1024061e-10 3.0148941e-16 2.6493473e-12 5.9555997e-12
 1.2043118e-17 1.0000000e+00], sum to 1.0000
[2019-04-03 23:45:17,911] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5991
[2019-04-03 23:45:17,952] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.2, 69.5, 143.0, 0.0, 26.0, 25.54572033178955, 0.3521147726430722, 1.0, 1.0, 34939.25662634293], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2201400.0000, 
sim time next is 2202000.0000, 
raw observation next is [-4.1, 69.0, 140.5, 0.0, 26.0, 25.6034007370554, 0.3633589214897736, 1.0, 1.0, 27877.58213572267], 
processed observation next is [1.0, 0.4782608695652174, 0.3490304709141275, 0.69, 0.4683333333333333, 0.0, 0.6666666666666666, 0.6336167280879499, 0.6211196404965912, 1.0, 1.0, 0.1327503911224889], 
reward next is 0.8672, 
noisyNet noise sample is [array([0.4763558], dtype=float32), 0.45524752]. 
=============================================
[2019-04-03 23:45:17,982] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[88.11236 ]
 [87.98863 ]
 [87.69113 ]
 [87.101776]
 [86.70102 ]], R is [[87.98501587]
 [87.93878937]
 [87.81135559]
 [87.5717926 ]
 [87.69607544]].
[2019-04-03 23:45:32,960] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.2079166e-17 5.7357696e-10 6.2424410e-15 1.2689234e-11 3.7358529e-11
 4.2309544e-17 1.0000000e+00], sum to 1.0000
[2019-04-03 23:45:32,960] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4642
[2019-04-03 23:45:33,048] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.45, 55.0, 0.0, 0.0, 26.0, 24.93775005406111, 0.329611355895292, 0.0, 1.0, 117124.5897406473], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2316600.0000, 
sim time next is 2317200.0000, 
raw observation next is [-1.533333333333333, 55.33333333333333, 0.0, 0.0, 26.0, 24.9205715311107, 0.3444587517944526, 0.0, 1.0, 83069.90383516529], 
processed observation next is [1.0, 0.8260869565217391, 0.42012927054478305, 0.5533333333333332, 0.0, 0.0, 0.6666666666666666, 0.5767142942592249, 0.6148195839314842, 0.0, 1.0, 0.3955709706436442], 
reward next is 0.6044, 
noisyNet noise sample is [array([0.33724007], dtype=float32), -1.7811363]. 
=============================================
[2019-04-03 23:45:41,338] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.0777395e-15 3.6055090e-07 1.2912992e-12 5.2971549e-10 1.0535178e-08
 6.7607568e-14 9.9999964e-01], sum to 1.0000
[2019-04-03 23:45:41,338] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4437
[2019-04-03 23:45:41,383] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.2, 33.0, 0.0, 0.0, 26.0, 25.28047900236282, 0.2638931449116221, 0.0, 1.0, 40216.08888121878], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2498400.0000, 
sim time next is 2499000.0000, 
raw observation next is [-1.1, 33.33333333333334, 0.0, 0.0, 26.0, 25.25058943919131, 0.2590894485793602, 0.0, 1.0, 40278.8053116081], 
processed observation next is [0.0, 0.9565217391304348, 0.4321329639889197, 0.3333333333333334, 0.0, 0.0, 0.6666666666666666, 0.6042157865992758, 0.5863631495264534, 0.0, 1.0, 0.1918038348171814], 
reward next is 0.8082, 
noisyNet noise sample is [array([0.6214572], dtype=float32), -0.7382697]. 
=============================================
[2019-04-03 23:45:41,437] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[74.16275 ]
 [74.19271 ]
 [74.21575 ]
 [74.25052 ]
 [74.335724]], R is [[74.24649811]
 [74.31253052]
 [74.37826538]
 [74.44359589]
 [74.50836182]].
[2019-04-03 23:45:50,150] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.78695207e-18 5.72168801e-09 3.03537751e-16 2.06790379e-12
 1.32029734e-11 1.93175810e-18 1.00000000e+00], sum to 1.0000
[2019-04-03 23:45:50,150] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1717
[2019-04-03 23:45:50,218] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.3, 29.0, 92.0, 256.5, 26.0, 25.92260043141344, 0.3076265022715465, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2563200.0000, 
sim time next is 2563800.0000, 
raw observation next is [3.2, 29.0, 84.66666666666666, 225.0, 26.0, 25.48435634948902, 0.3352557448026914, 1.0, 1.0, 27882.59574615725], 
processed observation next is [1.0, 0.6956521739130435, 0.551246537396122, 0.29, 0.2822222222222222, 0.24861878453038674, 0.6666666666666666, 0.6236963624574182, 0.6117519149342304, 1.0, 1.0, 0.13277426545789167], 
reward next is 0.8672, 
noisyNet noise sample is [array([-0.80603164], dtype=float32), 1.7440152]. 
=============================================
[2019-04-03 23:46:17,653] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.8707660e-17 4.2409414e-09 2.3206657e-15 9.2674366e-12 1.5243565e-10
 2.4024718e-17 1.0000000e+00], sum to 1.0000
[2019-04-03 23:46:17,653] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4855
[2019-04-03 23:46:17,732] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.4, 57.5, 109.0, 788.0, 26.0, 26.12161876058272, 0.5801679936030221, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2727000.0000, 
sim time next is 2727600.0000, 
raw observation next is [-5.199999999999999, 57.0, 107.8333333333333, 778.8333333333334, 26.0, 26.28336113735246, 0.5975653108273895, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.31855955678670367, 0.57, 0.35944444444444434, 0.8605893186003684, 0.6666666666666666, 0.6902800947793718, 0.6991884369424631, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.34304774], dtype=float32), 1.0939316]. 
=============================================
[2019-04-03 23:46:22,982] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-03 23:46:22,985] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-03 23:46:22,986] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:46:22,986] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-03 23:46:22,986] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-03 23:46:22,986] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:46:22,987] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:46:22,990] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run14
[2019-04-03 23:46:23,010] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run14
[2019-04-03 23:46:23,051] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run14
[2019-04-03 23:48:00,956] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.26570866], dtype=float32), 0.3471685]
[2019-04-03 23:48:00,956] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [8.8, 59.0, 217.0, 299.0, 26.0, 26.14610698228807, 0.6152763932570291, 1.0, 1.0, 0.0]
[2019-04-03 23:48:00,956] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-03 23:48:00,957] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [6.9544581e-18 8.5045460e-10 2.1305854e-16 2.6138360e-12 1.1813328e-11
 8.1471029e-18 1.0000000e+00], sampled 0.009947458294802147
[2019-04-03 23:49:00,936] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.26570866], dtype=float32), 0.3471685]
[2019-04-03 23:49:00,937] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [14.36666666666667, 52.83333333333334, 199.0, 335.6666666666666, 26.0, 27.19991735088573, 0.9092918010450476, 1.0, 1.0, 0.0]
[2019-04-03 23:49:00,937] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-03 23:49:00,937] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [8.9213890e-19 1.5711416e-09 5.2482618e-17 4.9364023e-13 6.4965468e-12
 2.8138453e-18 1.0000000e+00], sampled 0.812352453198314
[2019-04-03 23:49:06,500] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.26570866], dtype=float32), 0.3471685]
[2019-04-03 23:49:06,500] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [2.447262748, 38.7814756, 25.70720852, 577.8585600499999, 26.0, 26.60184141981967, 0.6545023080740987, 1.0, 1.0, 0.0]
[2019-04-03 23:49:06,500] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-03 23:49:06,501] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.1590775e-16 1.2857743e-09 1.8539476e-15 1.5262876e-11 9.4764335e-11
 4.5834690e-17 1.0000000e+00], sampled 0.7447989395781586
[2019-04-03 23:49:07,724] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-03 23:49:27,993] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-03 23:49:30,351] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-03 23:49:31,373] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 1300000, evaluation results [1300000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-03 23:49:35,524] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.8048333e-17 8.2117744e-09 4.1808232e-14 4.6956508e-11 1.5102856e-10
 2.3921769e-15 1.0000000e+00], sum to 1.0000
[2019-04-03 23:49:35,525] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5617
[2019-04-03 23:49:35,545] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.86028124116183, 0.3312413121638043, 0.0, 1.0, 43336.77042553438], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2940000.0000, 
sim time next is 2940600.0000, 
raw observation next is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.83170397725888, 0.3347308395078181, 0.0, 1.0, 43323.48322472895], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.85, 0.0, 0.0, 0.6666666666666666, 0.5693086647715733, 0.6115769465026061, 0.0, 1.0, 0.20630230107013786], 
reward next is 0.7937, 
noisyNet noise sample is [array([0.534837], dtype=float32), 0.60710853]. 
=============================================
[2019-04-03 23:49:40,848] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [4.84780207e-15 1.02716065e-07 1.55583405e-13 5.12553999e-10
 1.95390415e-09 1.59117409e-14 9.99999881e-01], sum to 1.0000
[2019-04-03 23:49:40,848] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.9786
[2019-04-03 23:49:40,923] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 65.0, 256.0, 284.0, 26.0, 24.96448985456088, 0.3549425951612621, 0.0, 1.0, 49123.43056640511], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2979000.0000, 
sim time next is 2979600.0000, 
raw observation next is [-3.0, 65.0, 243.5, 351.8333333333333, 26.0, 24.96635207908687, 0.3649378369042164, 0.0, 1.0, 44526.96263569329], 
processed observation next is [0.0, 0.4782608695652174, 0.3795013850415513, 0.65, 0.8116666666666666, 0.3887661141804788, 0.6666666666666666, 0.5805293399239059, 0.6216459456347389, 0.0, 1.0, 0.2120331554080633], 
reward next is 0.7880, 
noisyNet noise sample is [array([-0.62118053], dtype=float32), 0.65436035]. 
=============================================
[2019-04-03 23:49:48,568] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [7.9409762e-16 2.6009211e-08 2.3699386e-14 2.6629058e-11 2.1514224e-10
 3.8997791e-15 1.0000000e+00], sum to 1.0000
[2019-04-03 23:49:48,569] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4782
[2019-04-03 23:49:48,601] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.166666666666667, 60.83333333333333, 0.0, 0.0, 26.0, 25.01217155059833, 0.3131507246121719, 0.0, 1.0, 36561.31839217097], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3006600.0000, 
sim time next is 3007200.0000, 
raw observation next is [-2.333333333333333, 61.66666666666667, 0.0, 0.0, 26.0, 25.00823202934442, 0.3211626715836646, 0.0, 1.0, 173720.4842518793], 
processed observation next is [0.0, 0.8260869565217391, 0.3979686057248385, 0.6166666666666667, 0.0, 0.0, 0.6666666666666666, 0.5840193357787017, 0.6070542238612215, 0.0, 1.0, 0.8272404011994252], 
reward next is 0.1728, 
noisyNet noise sample is [array([1.3730294], dtype=float32), 1.7242814]. 
=============================================
[2019-04-03 23:49:54,043] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.9882857e-18 1.3871670e-10 6.8440352e-16 9.5942395e-13 1.3572804e-11
 1.6024684e-17 1.0000000e+00], sum to 1.0000
[2019-04-03 23:49:54,045] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9308
[2019-04-03 23:49:54,096] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 97.66666666666667, 0.0, 0.0, 26.0, 25.73829588978303, 0.5991745193557421, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3190800.0000, 
sim time next is 3191400.0000, 
raw observation next is [2.0, 96.5, 0.0, 0.0, 26.0, 25.54795660734859, 0.5904382726067039, 0.0, 1.0, 164928.4768057625], 
processed observation next is [1.0, 0.9565217391304348, 0.518005540166205, 0.965, 0.0, 0.0, 0.6666666666666666, 0.6289963839457157, 0.696812757535568, 0.0, 1.0, 0.7853736990750595], 
reward next is 0.2146, 
noisyNet noise sample is [array([-0.20457938], dtype=float32), 0.059939817]. 
=============================================
[2019-04-03 23:50:02,588] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.3294361e-14 1.2466633e-07 9.1780212e-12 1.8496510e-09 5.1791802e-09
 1.4085418e-13 9.9999988e-01], sum to 1.0000
[2019-04-03 23:50:02,594] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1555
[2019-04-03 23:50:02,676] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.5243368e-15 2.8842665e-08 7.6514706e-13 3.2916775e-10 3.9308050e-09
 1.1714226e-14 1.0000000e+00], sum to 1.0000
[2019-04-03 23:50:02,679] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9849
[2019-04-03 23:50:02,695] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.0, 77.33333333333334, 0.0, 0.0, 26.0, 23.86391429302424, 0.07456711444557412, 0.0, 1.0, 44038.68699561705], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3309000.0000, 
sim time next is 3309600.0000, 
raw observation next is [-11.0, 78.66666666666667, 0.0, 0.0, 26.0, 23.79699428012074, 0.149508528095387, 1.0, 1.0, 202374.3300019872], 
processed observation next is [1.0, 0.30434782608695654, 0.15789473684210528, 0.7866666666666667, 0.0, 0.0, 0.6666666666666666, 0.48308285667672823, 0.5498361760317957, 1.0, 1.0, 0.9636872857237485], 
reward next is 0.0363, 
noisyNet noise sample is [array([1.6877266], dtype=float32), 0.43183807]. 
=============================================
[2019-04-03 23:50:02,708] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 25.0681121950617, 0.3552229490603145, 0.0, 1.0, 41167.81821213078], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3373200.0000, 
sim time next is 3373800.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 25.08963429420478, 0.3454514509156194, 0.0, 1.0, 41189.3994244978], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5908028578503984, 0.6151504836385399, 0.0, 1.0, 0.19613999725951334], 
reward next is 0.8039, 
noisyNet noise sample is [array([-0.5055299], dtype=float32), 1.2982354]. 
=============================================
[2019-04-03 23:50:03,032] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.3885114e-15 1.3350016e-08 1.0821343e-12 1.9507901e-10 2.8825271e-09
 2.1347976e-14 1.0000000e+00], sum to 1.0000
[2019-04-03 23:50:03,032] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6870
[2019-04-03 23:50:03,062] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.0, 77.0, 0.0, 0.0, 26.0, 24.83977598718884, 0.3093039836941421, 0.0, 1.0, 43908.25010039363], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3294000.0000, 
sim time next is 3294600.0000, 
raw observation next is [-8.15, 77.0, 0.0, 0.0, 26.0, 24.79025885486099, 0.2962520335436986, 0.0, 1.0, 43930.81620955041], 
processed observation next is [1.0, 0.13043478260869565, 0.2368421052631579, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5658549045717493, 0.5987506778478996, 0.0, 1.0, 0.209194362902621], 
reward next is 0.7908, 
noisyNet noise sample is [array([-0.03218632], dtype=float32), 0.7563895]. 
=============================================
[2019-04-03 23:50:04,601] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.5698869e-17 8.8854618e-10 3.7702195e-15 1.0125813e-11 1.1795187e-10
 1.7396364e-16 1.0000000e+00], sum to 1.0000
[2019-04-03 23:50:04,601] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7836
[2019-04-03 23:50:04,661] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.333333333333332, 78.16666666666667, 92.0, 469.0, 26.0, 25.9235904842106, 0.4899626009153179, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3315000.0000, 
sim time next is 3315600.0000, 
raw observation next is [-9.0, 77.0, 95.0, 505.5, 26.0, 26.02541596587943, 0.5086785713378104, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.21329639889196678, 0.77, 0.31666666666666665, 0.5585635359116022, 0.6666666666666666, 0.6687846638232857, 0.6695595237792702, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2534953], dtype=float32), 0.21468326]. 
=============================================
[2019-04-03 23:50:08,233] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.8525762e-19 6.7701320e-09 8.6139317e-16 5.5397319e-12 3.4470489e-11
 1.2119899e-17 1.0000000e+00], sum to 1.0000
[2019-04-03 23:50:08,234] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1221
[2019-04-03 23:50:08,254] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6666666666666667, 67.33333333333334, 97.16666666666666, 624.8333333333334, 26.0, 25.71669003123315, 0.4767715428384955, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3489600.0000, 
sim time next is 3490200.0000, 
raw observation next is [-0.5, 65.5, 99.0, 670.0, 26.0, 25.72918537935858, 0.4989944164272253, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.44875346260387816, 0.655, 0.33, 0.7403314917127072, 0.6666666666666666, 0.644098781613215, 0.6663314721424084, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0845442], dtype=float32), 0.039470278]. 
=============================================
[2019-04-03 23:50:10,260] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.7482513e-17 1.3115022e-08 9.6342734e-16 1.6143632e-12 1.2369079e-10
 1.6853408e-17 1.0000000e+00], sum to 1.0000
[2019-04-03 23:50:10,261] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7861
[2019-04-03 23:50:10,273] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.333333333333333, 41.5, 116.3333333333333, 820.3333333333334, 26.0, 25.59435414030743, 0.4762448429782067, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3671400.0000, 
sim time next is 3672000.0000, 
raw observation next is [4.0, 45.0, 116.5, 822.5, 26.0, 25.52372932112246, 0.4643987175481723, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.5734072022160666, 0.45, 0.3883333333333333, 0.9088397790055248, 0.6666666666666666, 0.6269774434268717, 0.6547995725160575, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.36628333], dtype=float32), -1.1615249]. 
=============================================
[2019-04-03 23:50:10,301] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[92.86582 ]
 [93.428764]
 [93.946495]
 [94.33733 ]
 [94.66662 ]], R is [[92.29769897]
 [92.37472534]
 [92.45098114]
 [92.526474  ]
 [92.60121155]].
[2019-04-03 23:50:12,711] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.7673707e-16 7.6973317e-09 1.8787455e-14 4.3841302e-11 6.5608574e-10
 3.9640490e-15 1.0000000e+00], sum to 1.0000
[2019-04-03 23:50:12,711] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.2950
[2019-04-03 23:50:12,734] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.333333333333333, 63.66666666666667, 0.0, 0.0, 26.0, 25.31609199992329, 0.4377827125396521, 0.0, 1.0, 43878.46775088506], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3547200.0000, 
sim time next is 3547800.0000, 
raw observation next is [-2.5, 65.5, 0.0, 0.0, 26.0, 25.29759628256678, 0.4333634725643812, 0.0, 1.0, 42249.19322023057], 
processed observation next is [0.0, 0.043478260869565216, 0.39335180055401664, 0.655, 0.0, 0.0, 0.6666666666666666, 0.6081330235472316, 0.6444544908547937, 0.0, 1.0, 0.20118663438205034], 
reward next is 0.7988, 
noisyNet noise sample is [array([-0.7113457], dtype=float32), -2.1134126]. 
=============================================
[2019-04-03 23:50:21,725] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.35567018e-18 5.07208164e-10 2.49852935e-16 1.01931452e-12
 1.03189115e-11 3.37290029e-18 1.00000000e+00], sum to 1.0000
[2019-04-03 23:50:21,726] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0066
[2019-04-03 23:50:21,757] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.333333333333333, 77.0, 101.8333333333333, 690.6666666666666, 26.0, 26.1539673113851, 0.4914856036017383, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3750000.0000, 
sim time next is 3750600.0000, 
raw observation next is [-3.166666666666667, 77.0, 103.6666666666667, 706.3333333333333, 26.0, 26.17058385609787, 0.5053080323227028, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.3748845798707295, 0.77, 0.34555555555555567, 0.7804788213627992, 0.6666666666666666, 0.6808819880081559, 0.6684360107742343, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.3927546], dtype=float32), -0.86392874]. 
=============================================
[2019-04-03 23:50:21,774] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.7379081e-17 6.8660803e-08 8.4161225e-16 3.3524119e-12 1.0223860e-10
 1.0776582e-16 9.9999988e-01], sum to 1.0000
[2019-04-03 23:50:21,777] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1049
[2019-04-03 23:50:21,784] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.83333333333333, 24.66666666666666, 112.6666666666667, 780.6666666666667, 26.0, 25.65735632306515, 0.495284566610926, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3667800.0000, 
sim time next is 3668400.0000, 
raw observation next is [12.0, 24.0, 113.5, 789.5, 26.0, 25.68766442348429, 0.4991903413894219, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.7950138504155125, 0.24, 0.37833333333333335, 0.8723756906077348, 0.6666666666666666, 0.6406387019570241, 0.6663967804631407, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9656114], dtype=float32), -1.3922317]. 
=============================================
[2019-04-03 23:50:22,283] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.2126434e-18 8.4604740e-10 6.0363157e-16 7.8709643e-12 4.7578393e-11
 3.0499642e-18 1.0000000e+00], sum to 1.0000
[2019-04-03 23:50:22,284] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8068
[2019-04-03 23:50:22,292] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.1666666666666666, 60.0, 112.0, 804.0, 26.0, 26.48310155917869, 0.6414986237729785, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3765000.0000, 
sim time next is 3765600.0000, 
raw observation next is [0.0, 60.0, 110.5, 797.0, 26.0, 26.57165081041976, 0.6532108564799166, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.46260387811634357, 0.6, 0.36833333333333335, 0.8806629834254144, 0.6666666666666666, 0.7143042342016468, 0.7177369521599722, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.11548521], dtype=float32), -0.20186456]. 
=============================================
[2019-04-03 23:50:31,406] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.2613880e-18 7.6242257e-10 3.6211384e-16 2.6440568e-12 9.6340731e-12
 8.0615600e-18 1.0000000e+00], sum to 1.0000
[2019-04-03 23:50:31,406] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7118
[2019-04-03 23:50:31,414] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.166666666666667, 60.0, 112.3333333333333, 790.6666666666667, 26.0, 26.57284296561863, 0.633255038399389, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3840600.0000, 
sim time next is 3841200.0000, 
raw observation next is [-1.0, 60.0, 113.5, 798.5, 26.0, 26.57345376064314, 0.6404835248795125, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4349030470914128, 0.6, 0.37833333333333335, 0.8823204419889503, 0.6666666666666666, 0.714454480053595, 0.7134945082931708, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.30929688], dtype=float32), 1.3241422]. 
=============================================
[2019-04-03 23:50:36,508] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.7284247e-16 1.4531550e-08 3.2669769e-14 4.9505414e-11 6.2195077e-10
 5.3104561e-15 1.0000000e+00], sum to 1.0000
[2019-04-03 23:50:36,508] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2217
[2019-04-03 23:50:36,526] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 46.0, 0.0, 0.0, 26.0, 25.34535674835589, 0.3996130103610825, 0.0, 1.0, 39390.36556653557], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4154400.0000, 
sim time next is 4155000.0000, 
raw observation next is [-2.166666666666667, 46.66666666666667, 0.0, 0.0, 26.0, 25.33295408909064, 0.3944931054136555, 0.0, 1.0, 39282.31894555877], 
processed observation next is [0.0, 0.08695652173913043, 0.4025854108956602, 0.46666666666666673, 0.0, 0.0, 0.6666666666666666, 0.6110795074242201, 0.6314977018045519, 0.0, 1.0, 0.18705866164551796], 
reward next is 0.8129, 
noisyNet noise sample is [array([1.9487281], dtype=float32), -1.1360664]. 
=============================================
[2019-04-03 23:50:36,534] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[78.862236]
 [78.8867  ]
 [78.71595 ]
 [78.72053 ]
 [78.50114 ]], R is [[78.98283386]
 [79.00543213]
 [79.02528381]
 [79.03640747]
 [79.0233078 ]].
[2019-04-03 23:50:42,038] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.59770536e-18 1.54968816e-10 8.70553001e-17 2.14413231e-12
 1.44998544e-11 1.96987355e-18 1.00000000e+00], sum to 1.0000
[2019-04-03 23:50:42,038] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6086
[2019-04-03 23:50:42,077] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.666666666666667, 24.0, 105.6666666666667, 800.0, 26.0, 27.13516678840183, 0.6565421692298835, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4026000.0000, 
sim time next is 4026600.0000, 
raw observation next is [-2.5, 23.0, 104.0, 794.0, 26.0, 26.47675918500377, 0.6862569173780121, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.39335180055401664, 0.23, 0.3466666666666667, 0.8773480662983425, 0.6666666666666666, 0.706396598750314, 0.7287523057926707, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.21969396], dtype=float32), 1.2764132]. 
=============================================
[2019-04-03 23:50:45,058] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.8676680e-18 6.0330885e-10 1.8104957e-16 1.3414619e-12 1.6781543e-11
 9.4451097e-18 1.0000000e+00], sum to 1.0000
[2019-04-03 23:50:45,059] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5748
[2019-04-03 23:50:45,074] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 86.0, 208.0, 88.5, 26.0, 26.35089192899696, 0.6343544034694705, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4442400.0000, 
sim time next is 4443000.0000, 
raw observation next is [1.0, 86.0, 222.3333333333333, 107.6666666666667, 26.0, 26.37033947509359, 0.6473909967388115, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4903047091412743, 0.86, 0.7411111111111109, 0.11896869244935547, 0.6666666666666666, 0.6975282895911326, 0.7157969989129372, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8637744], dtype=float32), 0.403793]. 
=============================================
[2019-04-03 23:50:45,082] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[98.64466 ]
 [98.21655 ]
 [97.62853 ]
 [97.22938 ]
 [97.436935]], R is [[98.94932556]
 [98.95983124]
 [98.9702301 ]
 [98.98052979]
 [98.99072266]].
[2019-04-03 23:50:51,972] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [5.14221886e-15 8.31701698e-07 1.73012741e-13 1.65836872e-10
 1.75508530e-09 1.14311645e-14 9.99999166e-01], sum to 1.0000
[2019-04-03 23:50:51,972] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3490
[2019-04-03 23:50:51,991] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 26.0, 25.36043118715667, 0.3203615385194852, 0.0, 1.0, 69382.40589490208], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4249800.0000, 
sim time next is 4250400.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 26.0, 25.3249831084476, 0.3206441074229428, 0.0, 1.0, 53334.42206703949], 
processed observation next is [0.0, 0.17391304347826086, 0.5457063711911359, 0.45, 0.0, 0.0, 0.6666666666666666, 0.6104152590373001, 0.6068813691409809, 0.0, 1.0, 0.2539734384144738], 
reward next is 0.7460, 
noisyNet noise sample is [array([-1.2770814], dtype=float32), 1.0759199]. 
=============================================
[2019-04-03 23:50:53,171] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.4828670e-15 7.0769494e-07 8.8234399e-14 4.2719141e-11 8.6352053e-10
 2.5408695e-14 9.9999928e-01], sum to 1.0000
[2019-04-03 23:50:53,176] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9218
[2019-04-03 23:50:53,193] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 50.0, 0.0, 0.0, 26.0, 25.19015139420882, 0.3454949924761554, 0.0, 1.0, 39483.34525369534], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4161600.0000, 
sim time next is 4162200.0000, 
raw observation next is [-3.166666666666667, 50.66666666666667, 0.0, 0.0, 26.0, 25.1872855399786, 0.3384532051761542, 0.0, 1.0, 39467.15835361406], 
processed observation next is [0.0, 0.17391304347826086, 0.3748845798707295, 0.5066666666666667, 0.0, 0.0, 0.6666666666666666, 0.5989404616648834, 0.6128177350587181, 0.0, 1.0, 0.1879388493029241], 
reward next is 0.8121, 
noisyNet noise sample is [array([-1.5554055], dtype=float32), -0.5267342]. 
=============================================
[2019-04-03 23:51:10,930] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.0096829e-19 3.8934975e-12 1.5286544e-17 1.0541969e-13 9.6530498e-13
 1.2915044e-19 1.0000000e+00], sum to 1.0000
[2019-04-03 23:51:10,930] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8188
[2019-04-03 23:51:10,965] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.66666666666667, 28.83333333333334, 117.0, 849.3333333333334, 26.0, 26.65927911508724, 0.8709415447719969, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4366200.0000, 
sim time next is 4366800.0000, 
raw observation next is [14.6, 29.0, 116.5, 847.5, 26.0, 27.12842632795309, 0.9203280789006024, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.8670360110803325, 0.29, 0.3883333333333333, 0.93646408839779, 0.6666666666666666, 0.7607021939960908, 0.8067760263002008, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.78732], dtype=float32), 0.57534194]. 
=============================================
[2019-04-03 23:51:12,663] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.0586123e-18 2.3196803e-10 3.9430705e-15 3.0082141e-12 4.5183305e-11
 3.7419173e-17 1.0000000e+00], sum to 1.0000
[2019-04-03 23:51:12,664] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7314
[2019-04-03 23:51:12,674] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.333333333333334, 63.66666666666667, 0.0, 0.0, 26.0, 25.811454408573, 0.6357725269536028, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4407600.0000, 
sim time next is 4408200.0000, 
raw observation next is [7.199999999999999, 64.0, 0.0, 0.0, 26.0, 25.82300956226997, 0.6344903323683261, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.662049861495845, 0.64, 0.0, 0.0, 0.6666666666666666, 0.6519174635224975, 0.7114967774561087, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9328614], dtype=float32), -0.6035107]. 
=============================================
[2019-04-03 23:51:15,007] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.3134903e-17 3.4355199e-09 1.8680973e-14 7.5399832e-11 1.2794571e-10
 1.4512988e-16 1.0000000e+00], sum to 1.0000
[2019-04-03 23:51:15,007] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2921
[2019-04-03 23:51:15,027] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.09999999999999999, 72.0, 0.0, 0.0, 26.0, 25.44215356016062, 0.4859025938813107, 0.0, 1.0, 69174.3681276014], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4486800.0000, 
sim time next is 4487400.0000, 
raw observation next is [-0.15, 72.0, 0.0, 0.0, 26.0, 25.39546664835329, 0.4832920733036319, 0.0, 1.0, 76774.62109910484], 
processed observation next is [1.0, 0.9565217391304348, 0.458448753462604, 0.72, 0.0, 0.0, 0.6666666666666666, 0.616288887362774, 0.6610973577678773, 0.0, 1.0, 0.36559343380526116], 
reward next is 0.6344, 
noisyNet noise sample is [array([-1.75862], dtype=float32), 0.47115245]. 
=============================================
[2019-04-03 23:51:16,446] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.5036694e-17 3.4096963e-08 4.6069475e-14 1.5504414e-11 3.8860229e-10
 1.4417643e-15 1.0000000e+00], sum to 1.0000
[2019-04-03 23:51:16,447] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2767
[2019-04-03 23:51:16,471] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.15, 67.5, 0.0, 0.0, 26.0, 25.69308751932233, 0.5308895930488968, 0.0, 1.0, 48283.84149430507], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4422600.0000, 
sim time next is 4423200.0000, 
raw observation next is [4.033333333333333, 67.66666666666666, 0.0, 0.0, 26.0, 25.63711559880442, 0.5269746698266649, 0.0, 1.0, 67693.16442561339], 
processed observation next is [1.0, 0.17391304347826086, 0.5743305632502309, 0.6766666666666665, 0.0, 0.0, 0.6666666666666666, 0.6364262999003684, 0.6756582232755549, 0.0, 1.0, 0.32234840202673043], 
reward next is 0.6777, 
noisyNet noise sample is [array([-0.950748], dtype=float32), -1.2273029]. 
=============================================
[2019-04-03 23:51:25,762] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.7950859e-17 4.0383127e-10 5.7616316e-15 6.1111528e-12 4.9703915e-11
 1.0258952e-16 1.0000000e+00], sum to 1.0000
[2019-04-03 23:51:25,762] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6238
[2019-04-03 23:51:25,874] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 61.0, 0.0, 0.0, 26.0, 25.44882421776278, 0.4764550070083706, 0.0, 1.0, 18759.14213959462], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4578600.0000, 
sim time next is 4579200.0000, 
raw observation next is [1.0, 61.0, 0.0, 0.0, 26.0, 25.51153291632533, 0.468982368761643, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.4903047091412743, 0.61, 0.0, 0.0, 0.6666666666666666, 0.6259610763604441, 0.656327456253881, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.87928474], dtype=float32), 0.79182523]. 
=============================================
[2019-04-03 23:51:35,345] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.0063986e-15 2.6856336e-08 5.0764653e-14 1.8976315e-10 1.1400719e-09
 1.2427932e-15 1.0000000e+00], sum to 1.0000
[2019-04-03 23:51:35,346] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.3633
[2019-04-03 23:51:35,405] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.6666666666666667, 82.0, 0.0, 0.0, 26.0, 25.47993652922989, 0.4499335724252133, 0.0, 1.0, 90424.54592697059], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4678800.0000, 
sim time next is 4679400.0000, 
raw observation next is [0.3333333333333333, 87.0, 0.0, 0.0, 26.0, 25.47588236450538, 0.4491138394016032, 0.0, 1.0, 60783.12573552468], 
processed observation next is [1.0, 0.13043478260869565, 0.4718374884579871, 0.87, 0.0, 0.0, 0.6666666666666666, 0.6229901970421151, 0.6497046131338677, 0.0, 1.0, 0.28944345588345083], 
reward next is 0.7106, 
noisyNet noise sample is [array([-0.41041863], dtype=float32), 0.13064641]. 
=============================================
[2019-04-03 23:51:47,375] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.1970210e-16 4.7945225e-08 2.5534593e-14 7.7644745e-11 5.1008181e-10
 1.3722451e-15 1.0000000e+00], sum to 1.0000
[2019-04-03 23:51:47,376] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5660
[2019-04-03 23:51:47,398] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.441979581261, 0.3800203191248455, 0.0, 1.0, 18761.16361679431], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4838400.0000, 
sim time next is 4839000.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.41888469210669, 0.3656264486650847, 0.0, 1.0, 34311.21762674201], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.6, 0.0, 0.0, 0.6666666666666666, 0.6182403910088908, 0.6218754828883616, 0.0, 1.0, 0.16338675060353336], 
reward next is 0.8366, 
noisyNet noise sample is [array([-0.9580263], dtype=float32), -0.55751276]. 
=============================================
[2019-04-03 23:51:47,401] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[78.9363  ]
 [79.03489 ]
 [79.003265]
 [79.035545]
 [79.21033 ]], R is [[78.93857574]
 [79.0598526 ]
 [79.02817535]
 [79.04266357]
 [79.1628952 ]].
[2019-04-03 23:51:48,225] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:51:48,225] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:51:48,228] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run11
[2019-04-03 23:51:49,742] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.9722074e-16 9.2918086e-08 1.2479846e-13 1.5109598e-10 2.2340774e-09
 6.4687402e-15 9.9999988e-01], sum to 1.0000
[2019-04-03 23:51:49,742] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8231
[2019-04-03 23:51:49,808] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.666666666666667, 78.0, 0.0, 0.0, 26.0, 24.84409356540936, 0.3221381518021444, 0.0, 1.0, 40576.16174498659], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4760400.0000, 
sim time next is 4761000.0000, 
raw observation next is [-5.0, 81.5, 0.0, 0.0, 26.0, 24.81746054665407, 0.3170467841080082, 0.0, 1.0, 40551.37566634324], 
processed observation next is [0.0, 0.08695652173913043, 0.32409972299168976, 0.815, 0.0, 0.0, 0.6666666666666666, 0.5681217122211724, 0.6056822613693361, 0.0, 1.0, 0.19310178888734877], 
reward next is 0.8069, 
noisyNet noise sample is [array([-0.5357258], dtype=float32), 1.1392021]. 
=============================================
[2019-04-03 23:51:49,902] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[77.75093]
 [77.67247]
 [77.63131]
 [77.71484]
 [77.63924]], R is [[77.78044891]
 [77.80942535]
 [77.83789825]
 [77.86578369]
 [77.89311981]].
[2019-04-03 23:51:57,333] A3C_AGENT_WORKER-Thread-19 INFO:Local step 85000, global step 1353971: loss 0.0004
[2019-04-03 23:51:57,333] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 85000, global step 1353971: learning rate 0.0001
[2019-04-03 23:51:57,348] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2223358e-15 1.7668104e-07 8.6089129e-14 9.2414923e-11 7.7191153e-10
 3.4941793e-15 9.9999988e-01], sum to 1.0000
[2019-04-03 23:51:57,386] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6362
[2019-04-03 23:51:57,447] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6666666666666666, 47.66666666666666, 0.0, 0.0, 26.0, 25.36084104193188, 0.3151774576096557, 0.0, 1.0, 86472.44183999758], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4930800.0000, 
sim time next is 4931400.0000, 
raw observation next is [-0.8333333333333334, 48.83333333333334, 0.0, 0.0, 26.0, 25.33017064108151, 0.3209922266497172, 0.0, 1.0, 58922.36756842022], 
processed observation next is [1.0, 0.043478260869565216, 0.43951985226223456, 0.48833333333333345, 0.0, 0.0, 0.6666666666666666, 0.6108475534234591, 0.606997408883239, 0.0, 1.0, 0.28058270270676294], 
reward next is 0.7194, 
noisyNet noise sample is [array([0.7615648], dtype=float32), 0.46944654]. 
=============================================
[2019-04-03 23:51:58,441] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:51:58,442] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:51:58,445] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run11
[2019-04-03 23:51:59,314] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.6899519e-15 1.6354294e-07 3.2224568e-13 9.7628232e-11 1.6570585e-09
 4.8607646e-15 9.9999988e-01], sum to 1.0000
[2019-04-03 23:51:59,314] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3024
[2019-04-03 23:51:59,326] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 50.0, 0.0, 0.0, 26.0, 25.21964353768828, 0.2749890996398851, 0.0, 1.0, 38377.76744671525], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4939200.0000, 
sim time next is 4939800.0000, 
raw observation next is [-2.0, 49.33333333333334, 0.0, 0.0, 26.0, 25.1859450047348, 0.2708520909280713, 0.0, 1.0, 38351.86405521686], 
processed observation next is [1.0, 0.17391304347826086, 0.40720221606648205, 0.4933333333333334, 0.0, 0.0, 0.6666666666666666, 0.5988287503945667, 0.5902840303093572, 0.0, 1.0, 0.18262792407246123], 
reward next is 0.8174, 
noisyNet noise sample is [array([-3.3964908], dtype=float32), 1.1539685]. 
=============================================
[2019-04-03 23:52:02,779] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.0754970e-15 3.5039427e-08 8.4978967e-14 1.3429428e-10 1.0855780e-09
 6.1196932e-15 1.0000000e+00], sum to 1.0000
[2019-04-03 23:52:02,779] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9884
[2019-04-03 23:52:02,828] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 45.0, 152.8333333333333, 404.5, 26.0, 25.11527982039378, 0.375279513956243, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4894800.0000, 
sim time next is 4895400.0000, 
raw observation next is [3.0, 45.0, 142.6666666666667, 387.0, 26.0, 25.12398559802701, 0.3731463152991172, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.5457063711911359, 0.45, 0.47555555555555573, 0.4276243093922652, 0.6666666666666666, 0.5936654665022507, 0.6243821050997057, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.16210493], dtype=float32), 0.5435632]. 
=============================================
[2019-04-03 23:52:06,552] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.8227936e-15 5.2557287e-08 1.9213345e-13 2.6337413e-10 1.7505033e-09
 1.1409304e-14 1.0000000e+00], sum to 1.0000
[2019-04-03 23:52:06,553] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4295
[2019-04-03 23:52:06,585] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 45.0, 163.0, 422.0, 26.0, 25.09898846539158, 0.3748864506133925, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4894200.0000, 
sim time next is 4894800.0000, 
raw observation next is [3.0, 45.0, 152.8333333333333, 404.5, 26.0, 25.10954666858232, 0.3734651492770633, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.5457063711911359, 0.45, 0.5094444444444443, 0.44696132596685084, 0.6666666666666666, 0.5924622223818599, 0.6244883830923544, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.38302037], dtype=float32), 1.3014035]. 
=============================================
[2019-04-03 23:52:07,794] A3C_AGENT_WORKER-Thread-17 INFO:Local step 85000, global step 1356427: loss 0.0048
[2019-04-03 23:52:07,795] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 85000, global step 1356427: learning rate 0.0001
[2019-04-03 23:52:08,518] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.1552183e-18 2.3602502e-09 1.6665331e-16 2.6179972e-12 7.4448052e-12
 2.4048454e-18 1.0000000e+00], sum to 1.0000
[2019-04-03 23:52:08,518] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1940
[2019-04-03 23:52:08,531] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.5, 28.33333333333334, 120.3333333333333, 831.0, 26.0, 26.66793071984434, 0.6278460351468115, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4965000.0000, 
sim time next is 4965600.0000, 
raw observation next is [4.0, 27.66666666666667, 121.1666666666667, 838.0, 26.0, 26.73672120774643, 0.6450105446746427, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.5734072022160666, 0.2766666666666667, 0.403888888888889, 0.9259668508287293, 0.6666666666666666, 0.728060100645536, 0.7150035148915476, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.0225088], dtype=float32), -0.43905208]. 
=============================================
[2019-04-03 23:52:11,120] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.3389278e-16 1.3641088e-07 3.3537771e-14 2.6464192e-11 4.6707221e-10
 5.5133070e-15 9.9999988e-01], sum to 1.0000
[2019-04-03 23:52:11,136] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6609
[2019-04-03 23:52:11,214] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.100000000000001, 86.0, 88.5, 0.0, 26.0, 24.37751411680038, 0.1280119335832068, 0.0, 1.0, 41044.13722389203], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 48000.0000, 
sim time next is 48600.0000, 
raw observation next is [8.0, 86.0, 87.0, 0.0, 26.0, 24.37392053506438, 0.1340224996016708, 0.0, 1.0, 45110.31609100957], 
processed observation next is [0.0, 0.5652173913043478, 0.6842105263157896, 0.86, 0.29, 0.0, 0.6666666666666666, 0.5311600445886983, 0.5446741665338902, 0.0, 1.0, 0.21481102900480747], 
reward next is 0.7852, 
noisyNet noise sample is [array([1.2977529], dtype=float32), -1.0360703]. 
=============================================
[2019-04-03 23:52:13,662] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1495908e-19 2.6945485e-10 2.2031357e-17 5.3834817e-13 1.5204639e-12
 1.7884857e-19 1.0000000e+00], sum to 1.0000
[2019-04-03 23:52:13,668] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5852
[2019-04-03 23:52:13,677] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.833333333333334, 25.66666666666667, 114.3333333333333, 846.3333333333333, 26.0, 27.02271780930804, 0.7724555807165903, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4974600.0000, 
sim time next is 4975200.0000, 
raw observation next is [8.0, 26.0, 113.0, 839.5, 26.0, 27.22100045288598, 0.8000057163522296, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.6842105263157896, 0.26, 0.37666666666666665, 0.9276243093922651, 0.6666666666666666, 0.7684167044071651, 0.7666685721174099, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6124844], dtype=float32), 0.99347943]. 
=============================================
[2019-04-03 23:52:14,261] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:52:14,261] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:52:14,264] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run11
[2019-04-03 23:52:14,909] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:52:14,909] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:52:14,913] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run11
[2019-04-03 23:52:15,088] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:52:15,088] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:52:15,092] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run11
[2019-04-03 23:52:15,723] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:52:15,724] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:52:15,743] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run11
[2019-04-03 23:52:20,377] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:52:20,377] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:52:20,380] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run11
[2019-04-03 23:52:21,129] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:52:21,129] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:52:21,215] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run11
[2019-04-03 23:52:21,420] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.6040961e-18 2.8298527e-10 2.7580343e-16 5.0646327e-12 7.0389536e-12
 1.7455117e-17 1.0000000e+00], sum to 1.0000
[2019-04-03 23:52:21,420] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4796
[2019-04-03 23:52:21,451] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.7, 61.0, 148.0, 106.0, 26.0, 25.86113289839596, 0.423027351914573, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 138600.0000, 
sim time next is 139200.0000, 
raw observation next is [-6.700000000000001, 61.0, 133.5, 95.83333333333334, 26.0, 25.80791243328371, 0.4097866034844691, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.2770083102493075, 0.61, 0.445, 0.10589318600368325, 0.6666666666666666, 0.6506593694403092, 0.636595534494823, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.31209102], dtype=float32), -1.4874649]. 
=============================================
[2019-04-03 23:52:21,473] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:52:21,473] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:52:21,476] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run11
[2019-04-03 23:52:22,868] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:52:22,868] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:52:22,875] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run11
[2019-04-03 23:52:23,204] A3C_AGENT_WORKER-Thread-2 INFO:Local step 85000, global step 1359904: loss 0.0082
[2019-04-03 23:52:23,205] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:52:23,205] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:52:23,209] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run11
[2019-04-03 23:52:23,241] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 85000, global step 1359904: learning rate 0.0001
[2019-04-03 23:52:23,265] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:52:23,266] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:52:23,298] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run11
[2019-04-03 23:52:24,894] A3C_AGENT_WORKER-Thread-14 INFO:Local step 85000, global step 1360167: loss 0.0116
[2019-04-03 23:52:24,895] A3C_AGENT_WORKER-Thread-7 INFO:Local step 85000, global step 1360167: loss 0.0098
[2019-04-03 23:52:24,897] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 85000, global step 1360167: learning rate 0.0001
[2019-04-03 23:52:24,902] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 85000, global step 1360167: learning rate 0.0001
[2019-04-03 23:52:25,031] A3C_AGENT_WORKER-Thread-3 INFO:Local step 85000, global step 1360193: loss 0.0092
[2019-04-03 23:52:25,064] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 85000, global step 1360193: learning rate 0.0001
[2019-04-03 23:52:25,433] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:52:25,433] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:52:25,472] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run11
[2019-04-03 23:52:25,777] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:52:25,777] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:52:25,805] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run11
[2019-04-03 23:52:26,400] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:52:26,400] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:52:26,404] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run11
[2019-04-03 23:52:26,889] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 23:52:26,889] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:52:26,907] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run11
[2019-04-03 23:52:30,264] A3C_AGENT_WORKER-Thread-6 INFO:Local step 85000, global step 1360634: loss 0.0007
[2019-04-03 23:52:30,265] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 85000, global step 1360634: learning rate 0.0001
[2019-04-03 23:52:31,572] A3C_AGENT_WORKER-Thread-8 INFO:Local step 85000, global step 1360792: loss 0.0004
[2019-04-03 23:52:31,598] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 85000, global step 1360795: learning rate 0.0001
[2019-04-03 23:52:32,055] A3C_AGENT_WORKER-Thread-4 INFO:Local step 85000, global step 1360846: loss 0.0005
[2019-04-03 23:52:32,055] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 85000, global step 1360846: learning rate 0.0001
[2019-04-03 23:52:35,632] A3C_AGENT_WORKER-Thread-5 INFO:Local step 85000, global step 1361405: loss 0.0022
[2019-04-03 23:52:35,633] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 85000, global step 1361405: learning rate 0.0001
[2019-04-03 23:52:35,923] A3C_AGENT_WORKER-Thread-18 INFO:Local step 85000, global step 1361477: loss 0.0005
[2019-04-03 23:52:35,923] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 85000, global step 1361477: learning rate 0.0001
[2019-04-03 23:52:35,926] A3C_AGENT_WORKER-Thread-15 INFO:Local step 85000, global step 1361477: loss 0.0012
[2019-04-03 23:52:35,927] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 85000, global step 1361477: learning rate 0.0001
[2019-04-03 23:52:36,687] A3C_AGENT_WORKER-Thread-19 INFO:Local step 85500, global step 1361674: loss 1.4844
[2019-04-03 23:52:36,688] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 85500, global step 1361674: learning rate 0.0001
[2019-04-03 23:52:36,690] A3C_AGENT_WORKER-Thread-10 INFO:Local step 85000, global step 1361675: loss 0.0001
[2019-04-03 23:52:36,694] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 85000, global step 1361676: learning rate 0.0001
[2019-04-03 23:52:36,707] A3C_AGENT_WORKER-Thread-20 INFO:Local step 85000, global step 1361676: loss 0.0000
[2019-04-03 23:52:36,709] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 85000, global step 1361676: learning rate 0.0001
[2019-04-03 23:52:36,929] A3C_AGENT_WORKER-Thread-16 INFO:Local step 85000, global step 1361723: loss 0.0000
[2019-04-03 23:52:37,006] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 85000, global step 1361735: learning rate 0.0001
[2019-04-03 23:52:37,039] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.2225998e-16 1.5524382e-08 9.7877512e-15 4.3689209e-11 2.3190522e-10
 1.0608421e-15 1.0000000e+00], sum to 1.0000
[2019-04-03 23:52:37,039] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1422
[2019-04-03 23:52:37,078] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.433333333333334, 87.0, 0.0, 0.0, 26.0, 24.64025012538022, 0.2222134304894917, 0.0, 1.0, 120366.4043113222], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 69600.0000, 
sim time next is 70200.0000, 
raw observation next is [3.25, 87.5, 0.0, 0.0, 26.0, 24.64453812820157, 0.2319213816388229, 0.0, 1.0, 75792.94237259592], 
processed observation next is [0.0, 0.8260869565217391, 0.5526315789473685, 0.875, 0.0, 0.0, 0.6666666666666666, 0.5537115106834642, 0.577307127212941, 0.0, 1.0, 0.3609187732028377], 
reward next is 0.6391, 
noisyNet noise sample is [array([-2.520174], dtype=float32), -0.91025627]. 
=============================================
[2019-04-03 23:52:38,773] A3C_AGENT_WORKER-Thread-9 INFO:Local step 85000, global step 1362079: loss 0.0004
[2019-04-03 23:52:38,791] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 85000, global step 1362079: learning rate 0.0001
[2019-04-03 23:52:38,874] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.7874825e-17 1.7497964e-08 3.7527710e-15 9.3823673e-12 4.9323136e-11
 4.6831145e-17 1.0000000e+00], sum to 1.0000
[2019-04-03 23:52:38,875] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1813
[2019-04-03 23:52:38,939] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.8, 78.0, 165.8333333333333, 30.0, 26.0, 25.29614305367684, 0.2829509360383864, 1.0, 1.0, 42170.98229992203], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 123600.0000, 
sim time next is 124200.0000, 
raw observation next is [-7.8, 80.0, 190.0, 36.0, 26.0, 25.3385779472766, 0.3008248620935681, 1.0, 1.0, 42088.22782743681], 
processed observation next is [1.0, 0.43478260869565216, 0.24653739612188366, 0.8, 0.6333333333333333, 0.039779005524861875, 0.6666666666666666, 0.6115481622730501, 0.6002749540311894, 1.0, 1.0, 0.20042013251160384], 
reward next is 0.7996, 
noisyNet noise sample is [array([0.8486984], dtype=float32), 1.001459]. 
=============================================
[2019-04-03 23:52:45,485] A3C_AGENT_WORKER-Thread-17 INFO:Local step 85500, global step 1363459: loss 2.0513
[2019-04-03 23:52:45,506] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 85500, global step 1363459: learning rate 0.0001
[2019-04-03 23:52:53,405] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.4843614e-16 4.3704276e-08 1.9000216e-14 9.2225817e-11 3.0287708e-10
 4.3309438e-16 1.0000000e+00], sum to 1.0000
[2019-04-03 23:52:53,405] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2388
[2019-04-03 23:52:53,529] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 26.0, 23.50746784772471, -0.02983135377222553, 1.0, 1.0, 158122.9218508889], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 200400.0000, 
sim time next is 201000.0000, 
raw observation next is [-8.9, 78.0, 11.33333333333333, 110.6666666666666, 26.0, 23.93666262079129, 0.06289755577489771, 1.0, 1.0, 109116.2438965528], 
processed observation next is [1.0, 0.30434782608695654, 0.21606648199445982, 0.78, 0.03777777777777777, 0.12228360957642719, 0.6666666666666666, 0.4947218850659407, 0.5209658519249659, 1.0, 1.0, 0.5196011614121562], 
reward next is 0.4804, 
noisyNet noise sample is [array([-0.45874608], dtype=float32), -0.12600595]. 
=============================================
[2019-04-03 23:52:53,573] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[81.55457]
 [76.58201]
 [78.3323 ]
 [71.61993]
 [71.68225]], R is [[84.74624634]
 [84.14582062]
 [83.3353653 ]
 [82.53847504]
 [82.49868774]].
[2019-04-03 23:53:01,019] A3C_AGENT_WORKER-Thread-2 INFO:Local step 85500, global step 1366738: loss 1.9377
[2019-04-03 23:53:01,019] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 85500, global step 1366738: learning rate 0.0001
[2019-04-03 23:53:02,041] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.08812586e-18 8.54876558e-10 1.43049529e-15 1.38026465e-11
 9.24577342e-12 2.71748071e-17 1.00000000e+00], sum to 1.0000
[2019-04-03 23:53:02,042] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2683
[2019-04-03 23:53:02,119] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.583333333333333, 62.5, 138.3333333333333, 0.0, 26.0, 25.84403032438265, 0.402392962623317, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 222600.0000, 
sim time next is 223200.0000, 
raw observation next is [-3.4, 62.0, 133.0, 0.0, 26.0, 26.04183948325052, 0.4183720964447887, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.368421052631579, 0.62, 0.44333333333333336, 0.0, 0.6666666666666666, 0.6701532902708767, 0.6394573654815963, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9556407], dtype=float32), 0.8642117]. 
=============================================
[2019-04-03 23:53:05,270] A3C_AGENT_WORKER-Thread-7 INFO:Local step 85500, global step 1367573: loss 2.4156
[2019-04-03 23:53:05,270] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 85500, global step 1367573: learning rate 0.0001
[2019-04-03 23:53:05,514] A3C_AGENT_WORKER-Thread-3 INFO:Local step 85500, global step 1367622: loss 2.4513
[2019-04-03 23:53:05,515] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 85500, global step 1367622: learning rate 0.0001
[2019-04-03 23:53:06,689] A3C_AGENT_WORKER-Thread-14 INFO:Local step 85500, global step 1367851: loss 2.4596
[2019-04-03 23:53:06,690] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 85500, global step 1367851: learning rate 0.0001
[2019-04-03 23:53:09,152] A3C_AGENT_WORKER-Thread-6 INFO:Local step 85500, global step 1368417: loss 2.9064
[2019-04-03 23:53:09,152] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 85500, global step 1368417: learning rate 0.0001
[2019-04-03 23:53:09,856] A3C_AGENT_WORKER-Thread-8 INFO:Local step 85500, global step 1368577: loss 2.9608
[2019-04-03 23:53:09,858] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 85500, global step 1368577: learning rate 0.0001
[2019-04-03 23:53:10,375] A3C_AGENT_WORKER-Thread-4 INFO:Local step 85500, global step 1368676: loss 2.9353
[2019-04-03 23:53:10,375] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 85500, global step 1368676: learning rate 0.0001
[2019-04-03 23:53:13,152] A3C_AGENT_WORKER-Thread-19 INFO:Local step 86000, global step 1369220: loss 0.0558
[2019-04-03 23:53:13,165] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 86000, global step 1369220: learning rate 0.0001
[2019-04-03 23:53:14,811] A3C_AGENT_WORKER-Thread-5 INFO:Local step 85500, global step 1369539: loss 3.6217
[2019-04-03 23:53:14,812] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 85500, global step 1369539: learning rate 0.0001
[2019-04-03 23:53:15,030] A3C_AGENT_WORKER-Thread-16 INFO:Local step 85500, global step 1369578: loss 3.7394
[2019-04-03 23:53:15,031] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 85500, global step 1369578: learning rate 0.0001
[2019-04-03 23:53:15,665] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.87662332e-16 7.67041985e-10 1.29665896e-14 5.47131125e-11
 1.51794452e-10 1.11799985e-16 1.00000000e+00], sum to 1.0000
[2019-04-03 23:53:15,665] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7090
[2019-04-03 23:53:15,763] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.0, 43.0, 48.0, 832.0, 26.0, 25.16343722392237, 0.4282634088666383, 1.0, 1.0, 183622.5172362765], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 397800.0000, 
sim time next is 398400.0000, 
raw observation next is [-9.833333333333334, 42.0, 46.16666666666666, 811.1666666666666, 26.0, 25.70203958544609, 0.4677773563238912, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.1902123730378578, 0.42, 0.15388888888888885, 0.8963167587476979, 0.6666666666666666, 0.6418366321205076, 0.6559257854412971, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5072692], dtype=float32), 0.56789154]. 
=============================================
[2019-04-03 23:53:15,827] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.22238164e-16 6.07544459e-10 1.40921846e-14 4.50315410e-11
 1.86655885e-10 8.14918950e-17 1.00000000e+00], sum to 1.0000
[2019-04-03 23:53:15,827] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1094
[2019-04-03 23:53:15,860] A3C_AGENT_WORKER-Thread-15 INFO:Local step 85500, global step 1369720: loss 3.7727
[2019-04-03 23:53:15,862] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 85500, global step 1369720: learning rate 0.0001
[2019-04-03 23:53:15,950] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.833333333333334, 42.0, 46.16666666666666, 811.1666666666666, 26.0, 25.70203958544609, 0.4677773563238912, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 398400.0000, 
sim time next is 399000.0000, 
raw observation next is [-9.666666666666666, 41.0, 44.33333333333333, 790.3333333333334, 26.0, 26.06727525897522, 0.5017612227380733, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.19482917820867962, 0.41, 0.14777777777777776, 0.8732965009208103, 0.6666666666666666, 0.6722729382479349, 0.6672537409126912, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5072692], dtype=float32), 0.56789154]. 
=============================================
[2019-04-03 23:53:15,960] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[72.93499 ]
 [73.312614]
 [72.47015 ]
 [72.841835]
 [73.21928 ]], R is [[72.87472534]
 [73.14598083]
 [72.54013062]
 [72.81472778]
 [73.08657837]].
[2019-04-03 23:53:16,433] A3C_AGENT_WORKER-Thread-20 INFO:Local step 85500, global step 1369802: loss 3.8013
[2019-04-03 23:53:16,434] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 85500, global step 1369802: learning rate 0.0001
[2019-04-03 23:53:16,651] A3C_AGENT_WORKER-Thread-18 INFO:Local step 85500, global step 1369841: loss 3.8596
[2019-04-03 23:53:16,653] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 85500, global step 1369841: learning rate 0.0001
[2019-04-03 23:53:16,675] A3C_AGENT_WORKER-Thread-10 INFO:Local step 85500, global step 1369843: loss 3.9775
[2019-04-03 23:53:16,676] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 85500, global step 1369843: learning rate 0.0001
[2019-04-03 23:53:18,464] A3C_AGENT_WORKER-Thread-9 INFO:Local step 85500, global step 1370174: loss 4.0933
[2019-04-03 23:53:18,475] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 85500, global step 1370174: learning rate 0.0001
[2019-04-03 23:53:19,370] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.8041598e-16 2.2697321e-08 1.1026473e-14 1.1249852e-10 3.5213452e-10
 2.4682998e-16 1.0000000e+00], sum to 1.0000
[2019-04-03 23:53:19,370] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8853
[2019-04-03 23:53:19,443] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.71666666666667, 68.83333333333333, 0.0, 0.0, 26.0, 25.17548648077241, 0.3336481615253, 1.0, 1.0, 78524.94347607948], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 330600.0000, 
sim time next is 331200.0000, 
raw observation next is [-12.8, 70.0, 0.0, 0.0, 26.0, 25.18780429457048, 0.3340146913082676, 0.0, 1.0, 64252.69398521925], 
processed observation next is [1.0, 0.8695652173913043, 0.1080332409972299, 0.7, 0.0, 0.0, 0.6666666666666666, 0.5989836912142067, 0.6113382304360891, 0.0, 1.0, 0.30596520945342504], 
reward next is 0.6940, 
noisyNet noise sample is [array([0.38882548], dtype=float32), -0.42656505]. 
=============================================
[2019-04-03 23:53:23,246] A3C_AGENT_WORKER-Thread-17 INFO:Local step 86000, global step 1371259: loss 0.2955
[2019-04-03 23:53:23,266] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 86000, global step 1371261: learning rate 0.0001
[2019-04-03 23:53:38,060] A3C_AGENT_WORKER-Thread-2 INFO:Local step 86000, global step 1374427: loss 0.2661
[2019-04-03 23:53:38,090] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 86000, global step 1374427: learning rate 0.0001
[2019-04-03 23:53:41,062] A3C_AGENT_WORKER-Thread-7 INFO:Local step 86000, global step 1375081: loss 0.3029
[2019-04-03 23:53:41,062] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 86000, global step 1375081: learning rate 0.0001
[2019-04-03 23:53:42,899] A3C_AGENT_WORKER-Thread-3 INFO:Local step 86000, global step 1375619: loss 0.1568
[2019-04-03 23:53:42,900] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 86000, global step 1375619: learning rate 0.0001
[2019-04-03 23:53:44,579] A3C_AGENT_WORKER-Thread-14 INFO:Local step 86000, global step 1376001: loss 0.1598
[2019-04-03 23:53:44,587] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 86000, global step 1376001: learning rate 0.0001
[2019-04-03 23:53:44,802] A3C_AGENT_WORKER-Thread-6 INFO:Local step 86000, global step 1376042: loss 0.1563
[2019-04-03 23:53:44,804] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 86000, global step 1376042: learning rate 0.0001
[2019-04-03 23:53:45,926] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.8966425e-16 3.4774772e-08 1.8745933e-14 3.8809692e-11 4.1677664e-10
 2.0238273e-15 1.0000000e+00], sum to 1.0000
[2019-04-03 23:53:45,928] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5247
[2019-04-03 23:53:45,975] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.9333333333333333, 80.66666666666667, 123.1666666666667, 352.5, 26.0, 24.97036976865495, 0.3170356544897549, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 562800.0000, 
sim time next is 563400.0000, 
raw observation next is [-1.0, 80.5, 130.0, 396.0, 26.0, 24.96650958604356, 0.3144371160871037, 0.0, 1.0, 18730.73442671009], 
processed observation next is [0.0, 0.5217391304347826, 0.4349030470914128, 0.805, 0.43333333333333335, 0.4375690607734807, 0.6666666666666666, 0.5805424655036301, 0.6048123720290346, 0.0, 1.0, 0.08919397346052424], 
reward next is 0.9108, 
noisyNet noise sample is [array([-0.94485927], dtype=float32), -0.26941055]. 
=============================================
[2019-04-03 23:53:46,670] A3C_AGENT_WORKER-Thread-8 INFO:Local step 86000, global step 1376430: loss 0.1160
[2019-04-03 23:53:46,673] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 86000, global step 1376430: learning rate 0.0001
[2019-04-03 23:53:47,338] A3C_AGENT_WORKER-Thread-4 INFO:Local step 86000, global step 1376578: loss 0.1272
[2019-04-03 23:53:47,338] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 86000, global step 1376578: learning rate 0.0001
[2019-04-03 23:53:48,152] A3C_AGENT_WORKER-Thread-19 INFO:Local step 86500, global step 1376752: loss 0.0872
[2019-04-03 23:53:48,177] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 86500, global step 1376752: learning rate 0.0001
[2019-04-03 23:53:50,560] A3C_AGENT_WORKER-Thread-5 INFO:Local step 86000, global step 1377313: loss 0.2082
[2019-04-03 23:53:50,561] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 86000, global step 1377313: learning rate 0.0001
[2019-04-03 23:53:51,115] A3C_AGENT_WORKER-Thread-16 INFO:Local step 86000, global step 1377472: loss 0.1544
[2019-04-03 23:53:51,116] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 86000, global step 1377472: learning rate 0.0001
[2019-04-03 23:53:51,744] A3C_AGENT_WORKER-Thread-20 INFO:Local step 86000, global step 1377660: loss 0.1511
[2019-04-03 23:53:51,772] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 86000, global step 1377660: learning rate 0.0001
[2019-04-03 23:53:51,780] A3C_AGENT_WORKER-Thread-15 INFO:Local step 86000, global step 1377672: loss 0.1451
[2019-04-03 23:53:51,787] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 86000, global step 1377675: learning rate 0.0001
[2019-04-03 23:53:52,308] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.3805584e-16 6.9140739e-08 5.6957036e-14 9.8874985e-11 4.7329141e-10
 5.2996692e-15 9.9999988e-01], sum to 1.0000
[2019-04-03 23:53:52,308] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6420
[2019-04-03 23:53:52,326] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 86.0, 0.0, 0.0, 26.0, 24.39486097840527, 0.1455057243631493, 0.0, 1.0, 42119.83469298471], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 609600.0000, 
sim time next is 610200.0000, 
raw observation next is [-3.9, 86.0, 0.0, 0.0, 26.0, 24.40788801318707, 0.1406210216017837, 0.0, 1.0, 42102.3437923145], 
processed observation next is [0.0, 0.043478260869565216, 0.3545706371191136, 0.86, 0.0, 0.0, 0.6666666666666666, 0.5339906677655891, 0.5468736738672613, 0.0, 1.0, 0.20048735139197382], 
reward next is 0.7995, 
noisyNet noise sample is [array([-0.1112298], dtype=float32), 0.23270737]. 
=============================================
[2019-04-03 23:53:52,990] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.6639004e-16 1.3740761e-07 5.7863694e-14 7.8015115e-11 2.5917912e-10
 2.4432045e-15 9.9999988e-01], sum to 1.0000
[2019-04-03 23:53:52,990] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8162
[2019-04-03 23:53:53,012] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.65, 70.0, 0.0, 0.0, 26.0, 24.71147059158906, 0.1608794861159111, 0.0, 1.0, 41781.43693051115], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 685800.0000, 
sim time next is 686400.0000, 
raw observation next is [-3.733333333333333, 70.33333333333333, 0.0, 0.0, 26.0, 24.71867761717925, 0.1558110137431093, 0.0, 1.0, 41699.50157116403], 
processed observation next is [0.0, 0.9565217391304348, 0.35918744228993543, 0.7033333333333333, 0.0, 0.0, 0.6666666666666666, 0.5598898014316042, 0.5519370045810365, 0.0, 1.0, 0.1985690551007811], 
reward next is 0.8014, 
noisyNet noise sample is [array([-0.01442888], dtype=float32), 0.50230527]. 
=============================================
[2019-04-03 23:53:53,127] A3C_AGENT_WORKER-Thread-18 INFO:Local step 86000, global step 1378010: loss 0.1221
[2019-04-03 23:53:53,127] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 86000, global step 1378010: learning rate 0.0001
[2019-04-03 23:53:53,561] A3C_AGENT_WORKER-Thread-10 INFO:Local step 86000, global step 1378117: loss 0.1399
[2019-04-03 23:53:53,568] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 86000, global step 1378119: learning rate 0.0001
[2019-04-03 23:53:54,227] A3C_AGENT_WORKER-Thread-9 INFO:Local step 86000, global step 1378266: loss 0.1448
[2019-04-03 23:53:54,230] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 86000, global step 1378266: learning rate 0.0001
[2019-04-03 23:53:57,476] A3C_AGENT_WORKER-Thread-17 INFO:Local step 86500, global step 1379015: loss 0.0103
[2019-04-03 23:53:57,499] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 86500, global step 1379015: learning rate 0.0001
[2019-04-03 23:54:06,273] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.4441602e-18 4.8210032e-09 1.9481367e-16 3.8956516e-12 2.6840151e-12
 3.1454724e-18 1.0000000e+00], sum to 1.0000
[2019-04-03 23:54:06,275] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4977
[2019-04-03 23:54:06,299] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 57.0, 107.5, 614.0, 26.0, 25.84235977200898, 0.3889771677855018, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 734400.0000, 
sim time next is 735000.0000, 
raw observation next is [-0.4166666666666667, 55.83333333333333, 115.3333333333333, 559.0, 26.0, 25.84071161178181, 0.3824118822356947, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.45106186518928904, 0.5583333333333332, 0.3844444444444443, 0.6176795580110497, 0.6666666666666666, 0.6533926343151508, 0.6274706274118983, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6762799], dtype=float32), 0.7514691]. 
=============================================
[2019-04-03 23:54:06,370] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[89.830864]
 [90.88496 ]
 [92.04472 ]
 [92.90007 ]
 [92.84904 ]], R is [[89.13728333]
 [89.24591064]
 [89.35345459]
 [89.45992279]
 [89.56532288]].
[2019-04-03 23:54:07,109] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.4221756e-18 2.1949101e-10 5.3508384e-16 6.9612874e-12 6.1020793e-12
 2.4714726e-18 1.0000000e+00], sum to 1.0000
[2019-04-03 23:54:07,109] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7193
[2019-04-03 23:54:07,205] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.533333333333333, 55.33333333333333, 0.0, 0.0, 26.0, 24.926293890009, 0.2794416405124366, 1.0, 1.0, 197842.2627587587], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 754800.0000, 
sim time next is 755400.0000, 
raw observation next is [-3.716666666666666, 55.66666666666667, 0.0, 0.0, 26.0, 24.79277492052832, 0.3376771307479876, 1.0, 1.0, 199395.166913503], 
processed observation next is [1.0, 0.7391304347826086, 0.3596491228070176, 0.5566666666666668, 0.0, 0.0, 0.6666666666666666, 0.5660645767106933, 0.6125590435826626, 1.0, 1.0, 0.9495007948262048], 
reward next is 0.0505, 
noisyNet noise sample is [array([1.9523591], dtype=float32), -1.134627]. 
=============================================
[2019-04-03 23:54:10,560] A3C_AGENT_WORKER-Thread-19 INFO:Local step 87000, global step 1382520: loss 0.0388
[2019-04-03 23:54:10,561] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 87000, global step 1382520: learning rate 0.0001
[2019-04-03 23:54:10,867] A3C_AGENT_WORKER-Thread-2 INFO:Local step 86500, global step 1382611: loss 0.0015
[2019-04-03 23:54:10,869] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 86500, global step 1382611: learning rate 0.0001
[2019-04-03 23:54:14,956] A3C_AGENT_WORKER-Thread-7 INFO:Local step 86500, global step 1383669: loss 0.0009
[2019-04-03 23:54:14,957] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 86500, global step 1383669: learning rate 0.0001
[2019-04-03 23:54:15,299] A3C_AGENT_WORKER-Thread-3 INFO:Local step 86500, global step 1383756: loss 0.0011
[2019-04-03 23:54:15,299] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 86500, global step 1383756: learning rate 0.0001
[2019-04-03 23:54:16,982] A3C_AGENT_WORKER-Thread-14 INFO:Local step 86500, global step 1384169: loss 0.0005
[2019-04-03 23:54:16,982] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 86500, global step 1384169: learning rate 0.0001
[2019-04-03 23:54:17,618] A3C_AGENT_WORKER-Thread-6 INFO:Local step 86500, global step 1384354: loss 0.0013
[2019-04-03 23:54:17,622] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 86500, global step 1384354: learning rate 0.0001
[2019-04-03 23:54:19,160] A3C_AGENT_WORKER-Thread-17 INFO:Local step 87000, global step 1384835: loss 0.0288
[2019-04-03 23:54:19,161] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 87000, global step 1384835: learning rate 0.0001
[2019-04-03 23:54:19,486] A3C_AGENT_WORKER-Thread-8 INFO:Local step 86500, global step 1384974: loss 0.0008
[2019-04-03 23:54:19,490] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 86500, global step 1384975: learning rate 0.0001
[2019-04-03 23:54:19,669] A3C_AGENT_WORKER-Thread-4 INFO:Local step 86500, global step 1385029: loss 0.0006
[2019-04-03 23:54:19,670] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 86500, global step 1385029: learning rate 0.0001
[2019-04-03 23:54:22,740] A3C_AGENT_WORKER-Thread-5 INFO:Local step 86500, global step 1385970: loss 0.0007
[2019-04-03 23:54:22,746] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 86500, global step 1385971: learning rate 0.0001
[2019-04-03 23:54:22,852] A3C_AGENT_WORKER-Thread-16 INFO:Local step 86500, global step 1386009: loss 0.0007
[2019-04-03 23:54:22,853] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 86500, global step 1386009: learning rate 0.0001
[2019-04-03 23:54:23,544] A3C_AGENT_WORKER-Thread-20 INFO:Local step 86500, global step 1386258: loss 0.0028
[2019-04-03 23:54:23,547] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 86500, global step 1386258: learning rate 0.0001
[2019-04-03 23:54:24,245] A3C_AGENT_WORKER-Thread-15 INFO:Local step 86500, global step 1386492: loss 0.0006
[2019-04-03 23:54:24,269] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 86500, global step 1386492: learning rate 0.0001
[2019-04-03 23:54:24,767] A3C_AGENT_WORKER-Thread-18 INFO:Local step 86500, global step 1386679: loss 0.0006
[2019-04-03 23:54:24,768] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 86500, global step 1386679: learning rate 0.0001
[2019-04-03 23:54:25,153] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.8851588e-19 3.5147107e-09 3.8165081e-16 1.6248849e-12 1.5327056e-11
 1.2806488e-17 1.0000000e+00], sum to 1.0000
[2019-04-03 23:54:25,168] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5565
[2019-04-03 23:54:25,187] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 77.0, 0.0, 0.0, 26.0, 25.9826576586971, 0.6304668977430916, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1050600.0000, 
sim time next is 1051200.0000, 
raw observation next is [14.4, 77.0, 0.0, 0.0, 26.0, 25.98122098159372, 0.6254163542340985, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.8614958448753465, 0.77, 0.0, 0.0, 0.6666666666666666, 0.6651017484661432, 0.7084721180780328, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.72041255], dtype=float32), -1.5330068]. 
=============================================
[2019-04-03 23:54:25,654] A3C_AGENT_WORKER-Thread-10 INFO:Local step 86500, global step 1387008: loss 0.0004
[2019-04-03 23:54:25,656] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 86500, global step 1387008: learning rate 0.0001
[2019-04-03 23:54:25,982] A3C_AGENT_WORKER-Thread-9 INFO:Local step 86500, global step 1387123: loss 0.0064
[2019-04-03 23:54:25,983] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 86500, global step 1387123: learning rate 0.0001
[2019-04-03 23:54:27,476] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.5511643e-17 1.2155885e-08 1.1138117e-15 2.6716548e-12 7.3278501e-11
 1.1904815e-16 1.0000000e+00], sum to 1.0000
[2019-04-03 23:54:27,485] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3831
[2019-04-03 23:54:27,510] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.6, 83.0, 0.0, 0.0, 26.0, 25.64113713318578, 0.6069644821957371, 0.0, 1.0, 38194.73090686744], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1144800.0000, 
sim time next is 1145400.0000, 
raw observation next is [11.78333333333333, 82.5, 0.0, 0.0, 26.0, 25.62688635293034, 0.6083839280570256, 0.0, 1.0, 37901.8225247876], 
processed observation next is [0.0, 0.2608695652173913, 0.789012003693444, 0.825, 0.0, 0.0, 0.6666666666666666, 0.6355738627441951, 0.7027946426856753, 0.0, 1.0, 0.18048486916565526], 
reward next is 0.8195, 
noisyNet noise sample is [array([-0.38765323], dtype=float32), 0.7236712]. 
=============================================
[2019-04-03 23:54:31,810] A3C_AGENT_WORKER-Thread-2 INFO:Local step 87000, global step 1389413: loss 0.0211
[2019-04-03 23:54:31,811] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 87000, global step 1389413: learning rate 0.0001
[2019-04-03 23:54:33,870] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.2739166e-21 7.4232807e-12 3.2036209e-19 8.7971436e-15 2.7124514e-14
 3.6964674e-21 1.0000000e+00], sum to 1.0000
[2019-04-03 23:54:33,871] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4197
[2019-04-03 23:54:33,888] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.13333333333333, 71.66666666666667, 160.6666666666667, 71.66666666666666, 26.0, 27.2601208304396, 0.8919027086466832, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1075800.0000, 
sim time next is 1076400.0000, 
raw observation next is [15.5, 70.0, 184.0, 107.5, 26.0, 27.26126824233825, 0.9081180699825321, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.8919667590027703, 0.7, 0.6133333333333333, 0.11878453038674033, 0.6666666666666666, 0.7717723535281875, 0.8027060233275107, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0714539], dtype=float32), -0.18775162]. 
=============================================
[2019-04-03 23:54:34,889] A3C_AGENT_WORKER-Thread-7 INFO:Local step 87000, global step 1390673: loss 0.0287
[2019-04-03 23:54:34,893] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 87000, global step 1390673: learning rate 0.0001
[2019-04-03 23:54:35,171] A3C_AGENT_WORKER-Thread-19 INFO:Local step 87500, global step 1390792: loss 0.0487
[2019-04-03 23:54:35,195] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 87500, global step 1390792: learning rate 0.0001
[2019-04-03 23:54:36,269] A3C_AGENT_WORKER-Thread-3 INFO:Local step 87000, global step 1391231: loss 0.0144
[2019-04-03 23:54:36,272] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 87000, global step 1391234: learning rate 0.0001
[2019-04-03 23:54:37,430] A3C_AGENT_WORKER-Thread-6 INFO:Local step 87000, global step 1391720: loss 0.0163
[2019-04-03 23:54:37,449] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 87000, global step 1391729: learning rate 0.0001
[2019-04-03 23:54:37,817] A3C_AGENT_WORKER-Thread-14 INFO:Local step 87000, global step 1391901: loss 0.0149
[2019-04-03 23:54:37,821] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 87000, global step 1391901: learning rate 0.0001
[2019-04-03 23:54:39,226] A3C_AGENT_WORKER-Thread-4 INFO:Local step 87000, global step 1392510: loss 0.0060
[2019-04-03 23:54:39,227] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 87000, global step 1392510: learning rate 0.0001
[2019-04-03 23:54:39,266] A3C_AGENT_WORKER-Thread-8 INFO:Local step 87000, global step 1392522: loss 0.0102
[2019-04-03 23:54:39,286] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 87000, global step 1392535: learning rate 0.0001
[2019-04-03 23:54:40,201] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.1661095e-19 1.5952009e-10 1.2472475e-16 2.0768012e-12 3.6828318e-12
 1.6508775e-18 1.0000000e+00], sum to 1.0000
[2019-04-03 23:54:40,234] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1733
[2019-04-03 23:54:40,243] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 124.6666666666667, 0.0, 26.0, 26.11833518701989, 0.5828239579084681, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1338000.0000, 
sim time next is 1338600.0000, 
raw observation next is [1.1, 92.0, 122.3333333333333, 0.0, 26.0, 26.05895303016255, 0.5791894115800746, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.49307479224376743, 0.92, 0.4077777777777777, 0.0, 0.6666666666666666, 0.6715794191802124, 0.6930631371933581, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0489812], dtype=float32), 0.24267338]. 
=============================================
[2019-04-03 23:54:41,720] A3C_AGENT_WORKER-Thread-5 INFO:Local step 87000, global step 1393482: loss 0.0027
[2019-04-03 23:54:41,725] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 87000, global step 1393482: learning rate 0.0001
[2019-04-03 23:54:42,342] A3C_AGENT_WORKER-Thread-17 INFO:Local step 87500, global step 1393760: loss 0.0122
[2019-04-03 23:54:42,356] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 87500, global step 1393760: learning rate 0.0001
[2019-04-03 23:54:42,798] A3C_AGENT_WORKER-Thread-16 INFO:Local step 87000, global step 1393946: loss 0.0049
[2019-04-03 23:54:42,821] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 87000, global step 1393949: learning rate 0.0001
[2019-04-03 23:54:43,636] A3C_AGENT_WORKER-Thread-20 INFO:Local step 87000, global step 1394287: loss 0.0065
[2019-04-03 23:54:43,638] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 87000, global step 1394288: learning rate 0.0001
[2019-04-03 23:54:44,414] A3C_AGENT_WORKER-Thread-15 INFO:Local step 87000, global step 1394628: loss 0.0026
[2019-04-03 23:54:44,416] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 87000, global step 1394629: learning rate 0.0001
[2019-04-03 23:54:44,512] A3C_AGENT_WORKER-Thread-18 INFO:Local step 87000, global step 1394684: loss 0.0015
[2019-04-03 23:54:44,514] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 87000, global step 1394684: learning rate 0.0001
[2019-04-03 23:54:45,066] A3C_AGENT_WORKER-Thread-9 INFO:Local step 87000, global step 1394875: loss 0.0049
[2019-04-03 23:54:45,079] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 87000, global step 1394879: learning rate 0.0001
[2019-04-03 23:54:45,517] A3C_AGENT_WORKER-Thread-10 INFO:Local step 87000, global step 1395055: loss 0.0050
[2019-04-03 23:54:45,518] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 87000, global step 1395055: learning rate 0.0001
[2019-04-03 23:54:49,082] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.3752203e-17 4.0871312e-10 1.5156089e-15 1.9437388e-12 2.3916617e-11
 6.4278769e-17 1.0000000e+00], sum to 1.0000
[2019-04-03 23:54:49,082] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4795
[2019-04-03 23:54:49,108] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.25, 95.5, 0.0, 0.0, 26.0, 25.36973625436103, 0.514055944850092, 0.0, 1.0, 67083.72682445725], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1377000.0000, 
sim time next is 1377600.0000, 
raw observation next is [0.1666666666666667, 95.33333333333333, 0.0, 0.0, 26.0, 25.32473207092447, 0.5119309860490026, 0.0, 1.0, 53948.02395297067], 
processed observation next is [1.0, 0.9565217391304348, 0.4672206832871654, 0.9533333333333333, 0.0, 0.0, 0.6666666666666666, 0.6103943392437058, 0.6706436620163342, 0.0, 1.0, 0.25689535215700315], 
reward next is 0.7431, 
noisyNet noise sample is [array([0.42830595], dtype=float32), 1.5027558]. 
=============================================
[2019-04-03 23:54:50,476] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.7861888e-18 1.7813342e-10 5.1464007e-16 2.8773154e-12 9.0201293e-12
 1.3017543e-17 1.0000000e+00], sum to 1.0000
[2019-04-03 23:54:50,478] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8156
[2019-04-03 23:54:50,491] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.2, 82.0, 0.0, 0.0, 26.0, 25.58983897186846, 0.583614068712666, 0.0, 1.0, 93419.65825317551], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1638000.0000, 
sim time next is 1638600.0000, 
raw observation next is [7.2, 82.66666666666667, 0.0, 0.0, 26.0, 25.53142439114874, 0.6088256123886064, 0.0, 1.0, 91388.33110178061], 
processed observation next is [1.0, 1.0, 0.662049861495845, 0.8266666666666667, 0.0, 0.0, 0.6666666666666666, 0.627618699262395, 0.7029418707962022, 0.0, 1.0, 0.43518252905609817], 
reward next is 0.5648, 
noisyNet noise sample is [array([-1.5328006], dtype=float32), 0.017744035]. 
=============================================
[2019-04-03 23:54:52,829] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.4827854e-17 7.7295248e-08 2.6385525e-14 4.2530715e-11 1.6985230e-10
 2.8493092e-16 9.9999988e-01], sum to 1.0000
[2019-04-03 23:54:52,840] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8400
[2019-04-03 23:54:52,857] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.2, 96.0, 0.0, 0.0, 26.0, 25.3663795348592, 0.4476490274489031, 0.0, 1.0, 57751.85816606454], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1483200.0000, 
sim time next is 1483800.0000, 
raw observation next is [2.2, 96.0, 0.0, 0.0, 26.0, 25.31714148992549, 0.4543666911725995, 0.0, 1.0, 47884.22954857634], 
processed observation next is [1.0, 0.17391304347826086, 0.5235457063711911, 0.96, 0.0, 0.0, 0.6666666666666666, 0.6097617908271241, 0.6514555637241998, 0.0, 1.0, 0.22802014070750637], 
reward next is 0.7720, 
noisyNet noise sample is [array([1.4181851], dtype=float32), -1.1973816]. 
=============================================
[2019-04-03 23:54:55,897] A3C_AGENT_WORKER-Thread-2 INFO:Local step 87500, global step 1398337: loss 0.0111
[2019-04-03 23:54:55,897] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 87500, global step 1398337: learning rate 0.0001
[2019-04-03 23:54:56,207] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.47047972e-19 6.78963899e-11 1.01850974e-16 1.11849484e-12
 2.13706093e-12 1.35360445e-18 1.00000000e+00], sum to 1.0000
[2019-04-03 23:54:56,207] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9966
[2019-04-03 23:54:56,264] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 88.0, 0.0, 0.0, 26.0, 25.60170849340847, 0.4960283233790966, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1705200.0000, 
sim time next is 1705800.0000, 
raw observation next is [1.1, 88.0, 0.0, 0.0, 26.0, 25.42191529612188, 0.4713044641874435, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.49307479224376743, 0.88, 0.0, 0.0, 0.6666666666666666, 0.6184929413434901, 0.6571014880624811, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.45322806], dtype=float32), 0.66482025]. 
=============================================
[2019-04-03 23:54:58,006] A3C_AGENT_WORKER-Thread-7 INFO:Local step 87500, global step 1399036: loss 0.0122
[2019-04-03 23:54:58,006] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 87500, global step 1399036: learning rate 0.0001
[2019-04-03 23:55:00,665] A3C_AGENT_WORKER-Thread-19 INFO:Local step 88000, global step 1399874: loss 0.0001
[2019-04-03 23:55:00,670] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 88000, global step 1399874: learning rate 0.0001
[2019-04-03 23:55:01,105] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-03 23:55:01,117] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-03 23:55:01,117] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:55:01,118] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-03 23:55:01,118] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-03 23:55:01,119] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:55:01,119] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 23:55:01,121] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run15
[2019-04-03 23:55:01,122] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run15
[2019-04-03 23:55:01,178] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run15
[2019-04-03 23:55:21,358] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.2781158], dtype=float32), 0.31753007]
[2019-04-03 23:55:21,358] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-3.533333333333333, 50.16666666666667, 0.0, 0.0, 26.0, 25.34374449139766, 0.4029479493792963, 1.0, 1.0, 0.0]
[2019-04-03 23:55:21,358] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-03 23:55:21,359] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [2.2711044e-16 6.6304033e-09 6.5441185e-15 3.2811701e-11 1.4507055e-10
 2.2649269e-16 1.0000000e+00], sampled 0.19768250230498507
[2019-04-03 23:55:54,569] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.2781158], dtype=float32), 0.31753007]
[2019-04-03 23:55:54,570] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [3.058972567666667, 75.03156905166666, 0.0, 0.0, 26.0, 24.19723987783647, 0.07669834547382595, 1.0, 1.0, 38215.01421599267]
[2019-04-03 23:55:54,570] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-03 23:55:54,571] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [4.8154511e-17 5.0830401e-08 4.6432728e-15 1.2696691e-11 8.5786524e-11
 4.3026696e-16 1.0000000e+00], sampled 0.8723264133729552
[2019-04-03 23:56:46,426] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.2781158], dtype=float32), 0.31753007]
[2019-04-03 23:56:46,427] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-3.066666666666667, 47.66666666666667, 0.0, 0.0, 26.0, 25.40435237700134, 0.406522235004729, 0.0, 1.0, 40812.75939711642]
[2019-04-03 23:56:46,427] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-03 23:56:46,427] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.7471190e-15 4.6189101e-08 6.8100039e-14 1.5416488e-10 7.8341572e-10
 5.2086762e-15 1.0000000e+00], sampled 0.5116851658103312
[2019-04-03 23:57:25,135] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.4172 239942393.0563 1605.0250
[2019-04-03 23:57:44,759] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.2781158], dtype=float32), 0.31753007]
[2019-04-03 23:57:44,759] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [4.666666666666667, 36.83333333333333, 118.3333333333333, 821.0, 26.0, 27.40088727361006, 0.5883582008384368, 1.0, 1.0, 0.0]
[2019-04-03 23:57:44,759] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-03 23:57:44,760] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [4.7698004e-17 3.8797521e-09 8.8808938e-16 1.1649561e-11 3.8080466e-11
 4.4018128e-17 1.0000000e+00], sampled 0.3010119513959473
[2019-04-03 23:57:45,314] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-03 23:57:47,805] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-03 23:57:48,829] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 1400000, evaluation results [1400000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.417175922201, 239942393.05634084, 1605.024971488376, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-03 23:57:48,883] A3C_AGENT_WORKER-Thread-3 INFO:Local step 87500, global step 1400027: loss 0.0227
[2019-04-03 23:57:48,891] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 87500, global step 1400029: learning rate 0.0001
[2019-04-03 23:57:49,244] A3C_AGENT_WORKER-Thread-6 INFO:Local step 87500, global step 1400197: loss 0.0270
[2019-04-03 23:57:49,247] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 87500, global step 1400199: learning rate 0.0001
[2019-04-03 23:57:49,310] A3C_AGENT_WORKER-Thread-14 INFO:Local step 87500, global step 1400226: loss 0.0337
[2019-04-03 23:57:49,313] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 87500, global step 1400228: learning rate 0.0001
[2019-04-03 23:57:49,711] A3C_AGENT_WORKER-Thread-8 INFO:Local step 87500, global step 1400404: loss 0.0219
[2019-04-03 23:57:49,712] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 87500, global step 1400404: learning rate 0.0001
[2019-04-03 23:57:50,528] A3C_AGENT_WORKER-Thread-4 INFO:Local step 87500, global step 1400781: loss 0.0139
[2019-04-03 23:57:50,545] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 87500, global step 1400785: learning rate 0.0001
[2019-04-03 23:57:51,549] A3C_AGENT_WORKER-Thread-5 INFO:Local step 87500, global step 1401327: loss 0.0166
[2019-04-03 23:57:51,554] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 87500, global step 1401327: learning rate 0.0001
[2019-04-03 23:57:53,107] A3C_AGENT_WORKER-Thread-16 INFO:Local step 87500, global step 1402111: loss 0.0147
[2019-04-03 23:57:53,107] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 87500, global step 1402111: learning rate 0.0001
[2019-04-03 23:57:53,554] A3C_AGENT_WORKER-Thread-20 INFO:Local step 87500, global step 1402354: loss 0.0112
[2019-04-03 23:57:53,558] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 87500, global step 1402354: learning rate 0.0001
[2019-04-03 23:57:53,650] A3C_AGENT_WORKER-Thread-17 INFO:Local step 88000, global step 1402410: loss 0.0487
[2019-04-03 23:57:53,650] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 88000, global step 1402410: learning rate 0.0001
[2019-04-03 23:57:53,857] A3C_AGENT_WORKER-Thread-18 INFO:Local step 87500, global step 1402531: loss 0.0135
[2019-04-03 23:57:53,858] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 87500, global step 1402531: learning rate 0.0001
[2019-04-03 23:57:54,130] A3C_AGENT_WORKER-Thread-9 INFO:Local step 87500, global step 1402702: loss 0.0203
[2019-04-03 23:57:54,131] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 87500, global step 1402702: learning rate 0.0001
[2019-04-03 23:57:54,227] A3C_AGENT_WORKER-Thread-15 INFO:Local step 87500, global step 1402756: loss 0.0186
[2019-04-03 23:57:54,228] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 87500, global step 1402756: learning rate 0.0001
[2019-04-03 23:57:54,345] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.0315195e-19 2.1567430e-10 5.9480804e-17 9.8912008e-14 8.1421523e-13
 6.9300644e-19 1.0000000e+00], sum to 1.0000
[2019-04-03 23:57:54,349] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7421
[2019-04-03 23:57:54,357] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.8, 60.33333333333334, 0.0, 0.0, 26.0, 26.42526027208798, 0.6814167227980673, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1534800.0000, 
sim time next is 1535400.0000, 
raw observation next is [9.7, 60.5, 0.0, 0.0, 26.0, 26.3052705011516, 0.6687042841046095, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.7313019390581719, 0.605, 0.0, 0.0, 0.6666666666666666, 0.6921058750959667, 0.7229014280348699, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.32666138], dtype=float32), -0.5905497]. 
=============================================
[2019-04-03 23:57:54,856] A3C_AGENT_WORKER-Thread-10 INFO:Local step 87500, global step 1403121: loss 0.0092
[2019-04-03 23:57:54,863] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 87500, global step 1403122: learning rate 0.0001
[2019-04-03 23:58:02,273] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4012379e-16 4.3976014e-09 5.9369335e-15 2.2262145e-11 1.3828241e-10
 1.7619094e-16 1.0000000e+00], sum to 1.0000
[2019-04-03 23:58:02,287] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4262
[2019-04-03 23:58:02,315] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.4, 71.66666666666667, 221.8333333333333, 47.66666666666666, 26.0, 25.71286419695177, 0.3160264450281958, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1941600.0000, 
sim time next is 1942200.0000, 
raw observation next is [-5.3, 70.0, 232.0, 10.0, 26.0, 25.71277030145331, 0.3022092597436412, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.31578947368421056, 0.7, 0.7733333333333333, 0.011049723756906077, 0.6666666666666666, 0.6427308584544426, 0.6007364199145471, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6466475], dtype=float32), 0.31282413]. 
=============================================
[2019-04-03 23:58:03,569] A3C_AGENT_WORKER-Thread-2 INFO:Local step 88000, global step 1406912: loss 0.0004
[2019-04-03 23:58:03,575] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 88000, global step 1406913: learning rate 0.0001
[2019-04-03 23:58:04,390] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.3463498e-15 3.9990195e-07 3.7043014e-13 1.6193602e-10 4.3347512e-09
 2.2297756e-14 9.9999964e-01], sum to 1.0000
[2019-04-03 23:58:04,391] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4071
[2019-04-03 23:58:04,407] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.033333333333333, 83.66666666666667, 0.0, 0.0, 26.0, 24.14645338929454, 0.1072433402967549, 0.0, 1.0, 46442.9040516707], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1822200.0000, 
sim time next is 1822800.0000, 
raw observation next is [-6.066666666666666, 84.33333333333334, 0.0, 0.0, 26.0, 24.13478545229791, 0.1005023957899841, 0.0, 1.0, 46498.72962711781], 
processed observation next is [0.0, 0.08695652173913043, 0.2945521698984303, 0.8433333333333334, 0.0, 0.0, 0.6666666666666666, 0.5112321210248257, 0.5335007985966613, 0.0, 1.0, 0.22142252203389434], 
reward next is 0.7786, 
noisyNet noise sample is [array([-0.53988314], dtype=float32), -1.576839]. 
=============================================
[2019-04-03 23:58:04,558] A3C_AGENT_WORKER-Thread-7 INFO:Local step 88000, global step 1407237: loss 0.0001
[2019-04-03 23:58:04,564] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 88000, global step 1407238: learning rate 0.0001
[2019-04-03 23:58:06,353] A3C_AGENT_WORKER-Thread-3 INFO:Local step 88000, global step 1407869: loss 0.0024
[2019-04-03 23:58:06,355] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 88000, global step 1407869: learning rate 0.0001
[2019-04-03 23:58:06,893] A3C_AGENT_WORKER-Thread-6 INFO:Local step 88000, global step 1408055: loss 0.0074
[2019-04-03 23:58:06,894] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 88000, global step 1408055: learning rate 0.0001
[2019-04-03 23:58:06,944] A3C_AGENT_WORKER-Thread-8 INFO:Local step 88000, global step 1408073: loss 0.0090
[2019-04-03 23:58:06,949] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 88000, global step 1408073: learning rate 0.0001
[2019-04-03 23:58:07,040] A3C_AGENT_WORKER-Thread-14 INFO:Local step 88000, global step 1408106: loss 0.0066
[2019-04-03 23:58:07,043] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 88000, global step 1408106: learning rate 0.0001
[2019-04-03 23:58:07,228] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [5.6121654e-15 8.6837923e-07 1.0771356e-12 5.8952154e-10 3.2951191e-09
 8.3026041e-14 9.9999917e-01], sum to 1.0000
[2019-04-03 23:58:07,228] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.4391
[2019-04-03 23:58:07,255] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.2, 82.33333333333334, 0.0, 0.0, 26.0, 23.85585442524051, 0.0344562550528282, 0.0, 1.0, 46910.2592121272], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1829400.0000, 
sim time next is 1830000.0000, 
raw observation next is [-6.2, 81.66666666666667, 0.0, 0.0, 26.0, 23.82328275837155, 0.02584791037438988, 0.0, 1.0, 46926.08939369299], 
processed observation next is [0.0, 0.17391304347826086, 0.2908587257617729, 0.8166666666666668, 0.0, 0.0, 0.6666666666666666, 0.48527356319762927, 0.5086159701247966, 0.0, 1.0, 0.2234575685413952], 
reward next is 0.7765, 
noisyNet noise sample is [array([0.16135661], dtype=float32), 0.63951474]. 
=============================================
[2019-04-03 23:58:07,274] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[70.589355]
 [70.658875]
 [70.72488 ]
 [70.78551 ]
 [70.82865 ]], R is [[70.5823288 ]
 [70.65312195]
 [70.72322845]
 [70.79273224]
 [70.86170959]].
[2019-04-03 23:58:08,825] A3C_AGENT_WORKER-Thread-4 INFO:Local step 88000, global step 1408612: loss 0.0002
[2019-04-03 23:58:08,827] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 88000, global step 1408612: learning rate 0.0001
[2019-04-03 23:58:09,273] A3C_AGENT_WORKER-Thread-5 INFO:Local step 88000, global step 1408735: loss 0.0003
[2019-04-03 23:58:09,274] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 88000, global step 1408735: learning rate 0.0001
[2019-04-03 23:58:10,666] A3C_AGENT_WORKER-Thread-16 INFO:Local step 88000, global step 1409134: loss 0.0017
[2019-04-03 23:58:10,666] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 88000, global step 1409134: learning rate 0.0001
[2019-04-03 23:58:12,249] A3C_AGENT_WORKER-Thread-20 INFO:Local step 88000, global step 1409629: loss 0.0034
[2019-04-03 23:58:12,255] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 88000, global step 1409629: learning rate 0.0001
[2019-04-03 23:58:12,305] A3C_AGENT_WORKER-Thread-18 INFO:Local step 88000, global step 1409643: loss 0.0032
[2019-04-03 23:58:12,305] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 88000, global step 1409643: learning rate 0.0001
[2019-04-03 23:58:12,634] A3C_AGENT_WORKER-Thread-9 INFO:Local step 88000, global step 1409757: loss 0.0025
[2019-04-03 23:58:12,634] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 88000, global step 1409757: learning rate 0.0001
[2019-04-03 23:58:12,883] A3C_AGENT_WORKER-Thread-15 INFO:Local step 88000, global step 1409851: loss 0.0013
[2019-04-03 23:58:12,883] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 88000, global step 1409851: learning rate 0.0001
[2019-04-03 23:58:13,329] A3C_AGENT_WORKER-Thread-19 INFO:Local step 88500, global step 1410022: loss 0.4629
[2019-04-03 23:58:13,330] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 88500, global step 1410023: learning rate 0.0001
[2019-04-03 23:58:13,399] A3C_AGENT_WORKER-Thread-10 INFO:Local step 88000, global step 1410044: loss 0.0033
[2019-04-03 23:58:13,400] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 88000, global step 1410044: learning rate 0.0001
[2019-04-03 23:58:13,982] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.9283308e-14 1.4385452e-07 4.5978900e-13 5.5243737e-10 1.5657057e-09
 2.0395689e-14 9.9999988e-01], sum to 1.0000
[2019-04-03 23:58:13,987] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3735
[2019-04-03 23:58:14,084] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.583333333333333, 71.0, 136.6666666666667, 13.33333333333333, 26.0, 24.95548510172592, 0.2601260615381089, 0.0, 1.0, 48453.10301496032], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1860600.0000, 
sim time next is 1861200.0000, 
raw observation next is [-4.5, 71.0, 145.0, 20.0, 26.0, 24.96492984789506, 0.2644419514795674, 0.0, 1.0, 40586.22191521862], 
processed observation next is [0.0, 0.5652173913043478, 0.3379501385041552, 0.71, 0.48333333333333334, 0.022099447513812154, 0.6666666666666666, 0.5804108206579217, 0.5881473171598558, 0.0, 1.0, 0.19326772340580298], 
reward next is 0.8067, 
noisyNet noise sample is [array([-0.88432825], dtype=float32), 0.030559966]. 
=============================================
[2019-04-03 23:58:15,398] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.9266046e-16 4.1895012e-07 1.8765145e-13 7.5803586e-10 1.3086897e-09
 1.9597618e-14 9.9999952e-01], sum to 1.0000
[2019-04-03 23:58:15,398] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8900
[2019-04-03 23:58:15,450] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 76.0, 130.6666666666667, 56.0, 26.0, 24.91511293131853, 0.2435775619203701, 0.0, 1.0, 42170.77199789142], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1852800.0000, 
sim time next is 1853400.0000, 
raw observation next is [-5.6, 75.5, 141.3333333333333, 61.0, 26.0, 24.95810488083414, 0.2482291766370296, 0.0, 1.0, 22429.25261470014], 
processed observation next is [0.0, 0.43478260869565216, 0.30747922437673136, 0.755, 0.471111111111111, 0.06740331491712707, 0.6666666666666666, 0.5798420734028449, 0.5827430588790099, 0.0, 1.0, 0.10680596483190544], 
reward next is 0.8932, 
noisyNet noise sample is [array([-1.3245622], dtype=float32), -0.7296495]. 
=============================================
[2019-04-03 23:58:15,575] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0122195e-16 2.3245268e-09 1.2600598e-15 1.2192927e-11 4.6828340e-11
 2.6613236e-17 1.0000000e+00], sum to 1.0000
[2019-04-03 23:58:15,575] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5307
[2019-04-03 23:58:15,652] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.4, 80.0, 0.0, 0.0, 26.0, 25.23365122110962, 0.3635708430886251, 1.0, 1.0, 37480.47016105506], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2144400.0000, 
sim time next is 2145000.0000, 
raw observation next is [-5.5, 81.5, 0.0, 0.0, 26.0, 25.11017556698598, 0.3580208737857551, 1.0, 1.0, 103662.2592454931], 
processed observation next is [1.0, 0.8260869565217391, 0.3102493074792244, 0.815, 0.0, 0.0, 0.6666666666666666, 0.592514630582165, 0.6193402912619184, 1.0, 1.0, 0.49362980593091954], 
reward next is 0.5064, 
noisyNet noise sample is [array([-0.35460365], dtype=float32), 0.85506016]. 
=============================================
[2019-04-03 23:58:15,669] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[82.298836]
 [77.90529 ]
 [80.40712 ]
 [85.6679  ]
 [85.5477  ]], R is [[84.61435699]
 [84.58972931]
 [84.57974243]
 [84.58278656]
 [84.61439514]].
[2019-04-03 23:58:18,191] A3C_AGENT_WORKER-Thread-17 INFO:Local step 88500, global step 1411579: loss 0.2718
[2019-04-03 23:58:18,191] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 88500, global step 1411579: learning rate 0.0001
[2019-04-03 23:58:20,568] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.1703890e-17 9.7068853e-10 4.6946742e-15 4.9477072e-12 1.5427363e-11
 5.8633745e-17 1.0000000e+00], sum to 1.0000
[2019-04-03 23:58:20,569] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1611
[2019-04-03 23:58:20,579] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.7, 83.0, 0.0, 0.0, 26.0, 24.74841197852467, 0.2414485010030502, 0.0, 1.0, 42987.69677334322], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1983000.0000, 
sim time next is 1983600.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.69257264886767, 0.2316151000703044, 0.0, 1.0, 43000.31830018537], 
processed observation next is [1.0, 1.0, 0.30747922437673136, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5577143874056393, 0.5772050333567681, 0.0, 1.0, 0.2047634204770732], 
reward next is 0.7952, 
noisyNet noise sample is [array([-0.28152713], dtype=float32), 0.17247981]. 
=============================================
[2019-04-03 23:58:23,644] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.8202332e-18 4.7539617e-09 1.9639640e-15 1.4743842e-12 1.0354873e-11
 4.9446731e-18 1.0000000e+00], sum to 1.0000
[2019-04-03 23:58:23,648] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0743
[2019-04-03 23:58:23,705] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.833333333333334, 76.33333333333334, 0.0, 0.0, 26.0, 25.31884531810352, 0.310644525204788, 1.0, 1.0, 29428.87242616868], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1966800.0000, 
sim time next is 1967400.0000, 
raw observation next is [-4.75, 75.0, 0.0, 0.0, 26.0, 25.13657335439153, 0.3018531391310755, 1.0, 1.0, 82043.39264331062], 
processed observation next is [1.0, 0.782608695652174, 0.3310249307479225, 0.75, 0.0, 0.0, 0.6666666666666666, 0.5947144461992941, 0.6006177130436918, 1.0, 1.0, 0.39068282211100297], 
reward next is 0.6093, 
noisyNet noise sample is [array([0.6168145], dtype=float32), -0.63299775]. 
=============================================
[2019-04-03 23:58:26,511] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.3205763e-16 1.6651744e-07 5.4797306e-14 5.3354012e-11 1.4569945e-10
 1.3165627e-15 9.9999988e-01], sum to 1.0000
[2019-04-03 23:58:26,511] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4714
[2019-04-03 23:58:26,526] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.1, 86.33333333333333, 0.0, 0.0, 26.0, 24.22570645521061, 0.08144634035365685, 0.0, 1.0, 41130.93401535604], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2004600.0000, 
sim time next is 2005200.0000, 
raw observation next is [-6.2, 87.0, 0.0, 0.0, 26.0, 24.21575946943488, 0.08299146802713411, 0.0, 1.0, 41126.1696474472], 
processed observation next is [1.0, 0.21739130434782608, 0.2908587257617729, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5179799557862399, 0.5276638226757114, 0.0, 1.0, 0.19583890308308188], 
reward next is 0.8042, 
noisyNet noise sample is [array([1.0423646], dtype=float32), 1.7038031]. 
=============================================
[2019-04-03 23:58:28,691] A3C_AGENT_WORKER-Thread-2 INFO:Local step 88500, global step 1415095: loss 0.3941
[2019-04-03 23:58:28,692] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 88500, global step 1415095: learning rate 0.0001
[2019-04-03 23:58:29,377] A3C_AGENT_WORKER-Thread-7 INFO:Local step 88500, global step 1415309: loss 0.3388
[2019-04-03 23:58:29,378] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 88500, global step 1415309: learning rate 0.0001
[2019-04-03 23:58:30,856] A3C_AGENT_WORKER-Thread-3 INFO:Local step 88500, global step 1415719: loss 0.3700
[2019-04-03 23:58:30,856] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 88500, global step 1415719: learning rate 0.0001
[2019-04-03 23:58:31,853] A3C_AGENT_WORKER-Thread-14 INFO:Local step 88500, global step 1416033: loss 0.3601
[2019-04-03 23:58:31,854] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 88500, global step 1416033: learning rate 0.0001
[2019-04-03 23:58:31,961] A3C_AGENT_WORKER-Thread-8 INFO:Local step 88500, global step 1416065: loss 0.2917
[2019-04-03 23:58:31,962] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 88500, global step 1416065: learning rate 0.0001
[2019-04-03 23:58:32,021] A3C_AGENT_WORKER-Thread-6 INFO:Local step 88500, global step 1416083: loss 0.3199
[2019-04-03 23:58:32,023] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 88500, global step 1416086: learning rate 0.0001
[2019-04-03 23:58:33,747] A3C_AGENT_WORKER-Thread-4 INFO:Local step 88500, global step 1416699: loss 0.2795
[2019-04-03 23:58:33,747] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 88500, global step 1416699: learning rate 0.0001
[2019-04-03 23:58:34,829] A3C_AGENT_WORKER-Thread-5 INFO:Local step 88500, global step 1417090: loss 0.2445
[2019-04-03 23:58:34,833] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 88500, global step 1417090: learning rate 0.0001
[2019-04-03 23:58:35,259] A3C_AGENT_WORKER-Thread-16 INFO:Local step 88500, global step 1417251: loss 0.2993
[2019-04-03 23:58:35,260] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 88500, global step 1417251: learning rate 0.0001
[2019-04-03 23:58:36,299] A3C_AGENT_WORKER-Thread-18 INFO:Local step 88500, global step 1417598: loss 0.1861
[2019-04-03 23:58:36,299] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 88500, global step 1417598: learning rate 0.0001
[2019-04-03 23:58:36,607] A3C_AGENT_WORKER-Thread-20 INFO:Local step 88500, global step 1417685: loss 0.1909
[2019-04-03 23:58:36,608] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 88500, global step 1417685: learning rate 0.0001
[2019-04-03 23:58:36,979] A3C_AGENT_WORKER-Thread-19 INFO:Local step 89000, global step 1417790: loss 0.0048
[2019-04-03 23:58:36,980] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 89000, global step 1417790: learning rate 0.0001
[2019-04-03 23:58:37,059] A3C_AGENT_WORKER-Thread-9 INFO:Local step 88500, global step 1417816: loss 0.1662
[2019-04-03 23:58:37,059] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 88500, global step 1417816: learning rate 0.0001
[2019-04-03 23:58:37,786] A3C_AGENT_WORKER-Thread-15 INFO:Local step 88500, global step 1418030: loss 0.1299
[2019-04-03 23:58:37,787] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 88500, global step 1418030: learning rate 0.0001
[2019-04-03 23:58:38,108] A3C_AGENT_WORKER-Thread-10 INFO:Local step 88500, global step 1418130: loss 0.1004
[2019-04-03 23:58:38,110] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 88500, global step 1418131: learning rate 0.0001
[2019-04-03 23:58:39,579] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.21956805e-17 2.83422974e-09 2.90772604e-15 6.99417390e-12
 5.19899956e-11 1.79426179e-17 1.00000000e+00], sum to 1.0000
[2019-04-03 23:58:39,579] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3606
[2019-04-03 23:58:39,600] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.966666666666667, 83.0, 0.0, 0.0, 26.0, 25.27757834205332, 0.413672457834399, 0.0, 1.0, 43086.33272279808], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2150400.0000, 
sim time next is 2151000.0000, 
raw observation next is [-6.15, 83.0, 0.0, 0.0, 26.0, 25.26787101905561, 0.4091321879713989, 0.0, 1.0, 42958.98619060393], 
processed observation next is [1.0, 0.9130434782608695, 0.29224376731301943, 0.83, 0.0, 0.0, 0.6666666666666666, 0.6056559182546343, 0.6363773959904663, 0.0, 1.0, 0.20456660090763776], 
reward next is 0.7954, 
noisyNet noise sample is [array([0.49449298], dtype=float32), 0.20690778]. 
=============================================
[2019-04-03 23:58:39,604] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[79.87871]
 [80.00571]
 [80.10662]
 [80.19456]
 [80.06313]], R is [[79.71788025]
 [79.7155304 ]
 [79.71209717]
 [79.7056427 ]
 [79.68894196]].
[2019-04-03 23:58:42,291] A3C_AGENT_WORKER-Thread-17 INFO:Local step 89000, global step 1419267: loss 0.0941
[2019-04-03 23:58:42,291] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 89000, global step 1419267: learning rate 0.0001
[2019-04-03 23:58:42,737] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1685962e-16 1.6229825e-09 1.5430223e-14 2.5185548e-11 3.4675568e-11
 1.7991839e-16 1.0000000e+00], sum to 1.0000
[2019-04-03 23:58:42,737] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3010
[2019-04-03 23:58:42,864] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.2, 75.0, 0.0, 0.0, 26.0, 24.78970726491308, 0.2670124122433631, 0.0, 1.0, 44203.4439275136], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2242800.0000, 
sim time next is 2243400.0000, 
raw observation next is [-6.283333333333333, 75.5, 0.0, 0.0, 26.0, 24.72898716201501, 0.2551157193383058, 0.0, 1.0, 44154.56163385813], 
processed observation next is [1.0, 1.0, 0.288550323176362, 0.755, 0.0, 0.0, 0.6666666666666666, 0.5607489301679175, 0.5850385731127686, 0.0, 1.0, 0.21025981730408633], 
reward next is 0.7897, 
noisyNet noise sample is [array([1.9764426], dtype=float32), -0.48528078]. 
=============================================
[2019-04-03 23:58:53,797] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [4.0969508e-17 8.6843166e-10 6.0021060e-15 5.3319606e-12 6.6834448e-11
 1.1032310e-16 1.0000000e+00], sum to 1.0000
[2019-04-03 23:58:53,797] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.7012
[2019-04-03 23:58:53,875] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 54.0, 0.0, 0.0, 26.0, 25.38781642744049, 0.4339933900514041, 0.0, 1.0, 33807.73325232392], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2322000.0000, 
sim time next is 2322600.0000, 
raw observation next is [-1.7, 54.33333333333333, 0.0, 0.0, 26.0, 25.44985306641757, 0.4321295562847838, 0.0, 1.0, 18762.95486425245], 
processed observation next is [1.0, 0.9130434782608695, 0.4155124653739613, 0.5433333333333333, 0.0, 0.0, 0.6666666666666666, 0.6208210888681309, 0.6440431854282612, 0.0, 1.0, 0.08934740411548786], 
reward next is 0.9107, 
noisyNet noise sample is [array([-1.4519984], dtype=float32), -0.34205285]. 
=============================================
[2019-04-03 23:59:01,661] A3C_AGENT_WORKER-Thread-7 INFO:Local step 89000, global step 1422982: loss 0.1068
[2019-04-03 23:59:01,677] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 89000, global step 1422982: learning rate 0.0001
[2019-04-03 23:59:03,054] A3C_AGENT_WORKER-Thread-2 INFO:Local step 89000, global step 1423257: loss 0.1021
[2019-04-03 23:59:03,055] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 89000, global step 1423257: learning rate 0.0001
[2019-04-03 23:59:07,255] A3C_AGENT_WORKER-Thread-3 INFO:Local step 89000, global step 1424055: loss 0.1765
[2019-04-03 23:59:07,341] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 89000, global step 1424055: learning rate 0.0001
[2019-04-03 23:59:08,005] A3C_AGENT_WORKER-Thread-6 INFO:Local step 89000, global step 1424207: loss 0.1749
[2019-04-03 23:59:08,071] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 89000, global step 1424211: learning rate 0.0001
[2019-04-03 23:59:08,401] A3C_AGENT_WORKER-Thread-8 INFO:Local step 89000, global step 1424270: loss 0.1684
[2019-04-03 23:59:08,448] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 89000, global step 1424270: learning rate 0.0001
[2019-04-03 23:59:08,449] A3C_AGENT_WORKER-Thread-14 INFO:Local step 89000, global step 1424276: loss 0.1648
[2019-04-03 23:59:08,465] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 89000, global step 1424282: learning rate 0.0001
[2019-04-03 23:59:09,255] A3C_AGENT_WORKER-Thread-4 INFO:Local step 89000, global step 1424424: loss 0.1064
[2019-04-03 23:59:09,265] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 89000, global step 1424424: learning rate 0.0001
[2019-04-03 23:59:09,748] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.7755998e-14 8.6087323e-07 1.4595397e-12 1.4537408e-09 3.0306710e-09
 1.9485395e-13 9.9999917e-01], sum to 1.0000
[2019-04-03 23:59:09,779] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6510
[2019-04-03 23:59:09,827] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.4, 61.0, 0.0, 0.0, 26.0, 23.35788628780906, -0.1141751997603622, 0.0, 1.0, 44379.89677296291], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2437200.0000, 
sim time next is 2437800.0000, 
raw observation next is [-8.483333333333334, 60.83333333333334, 0.0, 0.0, 26.0, 23.34730422195621, -0.1210125515594287, 0.0, 1.0, 44337.34298091124], 
processed observation next is [0.0, 0.21739130434782608, 0.2276084949215143, 0.6083333333333334, 0.0, 0.0, 0.6666666666666666, 0.4456086851630176, 0.4596624828135238, 0.0, 1.0, 0.2111302046710059], 
reward next is 0.7889, 
noisyNet noise sample is [array([0.88457924], dtype=float32), 1.5771372]. 
=============================================
[2019-04-03 23:59:11,844] A3C_AGENT_WORKER-Thread-5 INFO:Local step 89000, global step 1424920: loss 0.1332
[2019-04-03 23:59:11,849] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 89000, global step 1424920: learning rate 0.0001
[2019-04-03 23:59:13,665] A3C_AGENT_WORKER-Thread-16 INFO:Local step 89000, global step 1425272: loss 0.0603
[2019-04-03 23:59:13,698] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 89000, global step 1425272: learning rate 0.0001
[2019-04-03 23:59:14,124] A3C_AGENT_WORKER-Thread-19 INFO:Local step 89500, global step 1425366: loss 0.3911
[2019-04-03 23:59:14,124] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 89500, global step 1425366: learning rate 0.0001
[2019-04-03 23:59:15,197] A3C_AGENT_WORKER-Thread-18 INFO:Local step 89000, global step 1425589: loss 0.0285
[2019-04-03 23:59:15,199] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 89000, global step 1425589: learning rate 0.0001
[2019-04-03 23:59:16,935] A3C_AGENT_WORKER-Thread-20 INFO:Local step 89000, global step 1426033: loss 0.0868
[2019-04-03 23:59:16,943] A3C_AGENT_WORKER-Thread-9 INFO:Local step 89000, global step 1426034: loss 0.0852
[2019-04-03 23:59:16,944] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 89000, global step 1426034: learning rate 0.0001
[2019-04-03 23:59:16,963] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 89000, global step 1426033: learning rate 0.0001
[2019-04-03 23:59:17,139] A3C_AGENT_WORKER-Thread-10 INFO:Local step 89000, global step 1426075: loss 0.0782
[2019-04-03 23:59:17,140] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 89000, global step 1426075: learning rate 0.0001
[2019-04-03 23:59:18,446] A3C_AGENT_WORKER-Thread-15 INFO:Local step 89000, global step 1426311: loss 0.1087
[2019-04-03 23:59:18,448] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 89000, global step 1426312: learning rate 0.0001
[2019-04-03 23:59:18,888] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.0973928e-14 1.6840107e-06 1.8888118e-12 2.8641409e-09 1.2223965e-08
 6.3292866e-13 9.9999833e-01], sum to 1.0000
[2019-04-03 23:59:18,889] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7067
[2019-04-03 23:59:18,926] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.4, 61.0, 0.0, 0.0, 26.0, 23.39240033448608, -0.1097425287544131, 0.0, 1.0, 44407.42444230639], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2436600.0000, 
sim time next is 2437200.0000, 
raw observation next is [-8.4, 61.0, 0.0, 0.0, 26.0, 23.35788628780906, -0.1141751997603622, 0.0, 1.0, 44379.89677296291], 
processed observation next is [0.0, 0.21739130434782608, 0.2299168975069252, 0.61, 0.0, 0.0, 0.6666666666666666, 0.44649052398408823, 0.4619416000798793, 0.0, 1.0, 0.21133284177601386], 
reward next is 0.7887, 
noisyNet noise sample is [array([-0.23437248], dtype=float32), -0.06343597]. 
=============================================
[2019-04-03 23:59:21,295] A3C_AGENT_WORKER-Thread-17 INFO:Local step 89500, global step 1426946: loss 0.3189
[2019-04-03 23:59:21,296] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 89500, global step 1426946: learning rate 0.0001
[2019-04-03 23:59:25,251] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.0876291e-18 1.5715881e-09 5.3358943e-17 5.2643436e-13 1.1045471e-12
 6.3749295e-19 1.0000000e+00], sum to 1.0000
[2019-04-03 23:59:25,251] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0376
[2019-04-03 23:59:25,269] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 39.0, 225.0, 46.5, 26.0, 25.74823556748576, 0.3315675070911117, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2548800.0000, 
sim time next is 2549400.0000, 
raw observation next is [1.283333333333333, 37.5, 222.6666666666667, 38.33333333333333, 26.0, 25.76755716843693, 0.3327145453590429, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.4981532779316713, 0.375, 0.7422222222222223, 0.04235727440147329, 0.6666666666666666, 0.6472964307030775, 0.6109048484530143, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.94847584], dtype=float32), 0.7154117]. 
=============================================
[2019-04-03 23:59:29,137] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.88175513e-18 4.59987992e-10 4.14780890e-16 6.95063365e-12
 9.52917259e-12 1.34586645e-17 1.00000000e+00], sum to 1.0000
[2019-04-03 23:59:29,137] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6793
[2019-04-03 23:59:29,271] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 62.0, 0.0, 0.0, 26.0, 24.80601395118446, 0.2561451597604474, 0.0, 1.0, 41898.09662132264], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2592000.0000, 
sim time next is 2592600.0000, 
raw observation next is [-4.583333333333333, 63.0, 0.0, 0.0, 26.0, 24.76962730010422, 0.2475255266662863, 0.0, 1.0, 41906.37437272244], 
processed observation next is [1.0, 0.0, 0.3356417359187443, 0.63, 0.0, 0.0, 0.6666666666666666, 0.5641356083420183, 0.5825085088887622, 0.0, 1.0, 0.1995541636796307], 
reward next is 0.8004, 
noisyNet noise sample is [array([0.47663423], dtype=float32), 0.9333124]. 
=============================================
[2019-04-03 23:59:38,476] A3C_AGENT_WORKER-Thread-7 INFO:Local step 89500, global step 1431034: loss 0.4631
[2019-04-03 23:59:38,476] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 89500, global step 1431034: learning rate 0.0001
[2019-04-03 23:59:39,444] A3C_AGENT_WORKER-Thread-2 INFO:Local step 89500, global step 1431233: loss 0.4713
[2019-04-03 23:59:39,446] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 89500, global step 1431233: learning rate 0.0001
[2019-04-03 23:59:43,255] A3C_AGENT_WORKER-Thread-6 INFO:Local step 89500, global step 1431993: loss 0.5633
[2019-04-03 23:59:43,256] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 89500, global step 1431993: learning rate 0.0001
[2019-04-03 23:59:44,287] A3C_AGENT_WORKER-Thread-8 INFO:Local step 89500, global step 1432223: loss 0.5382
[2019-04-03 23:59:44,288] A3C_AGENT_WORKER-Thread-14 INFO:Local step 89500, global step 1432224: loss 0.5487
[2019-04-03 23:59:44,288] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 89500, global step 1432223: learning rate 0.0001
[2019-04-03 23:59:44,291] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 89500, global step 1432224: learning rate 0.0001
[2019-04-03 23:59:45,005] A3C_AGENT_WORKER-Thread-4 INFO:Local step 89500, global step 1432381: loss 0.5466
[2019-04-03 23:59:45,006] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 89500, global step 1432381: learning rate 0.0001
[2019-04-03 23:59:45,115] A3C_AGENT_WORKER-Thread-3 INFO:Local step 89500, global step 1432410: loss 0.5625
[2019-04-03 23:59:45,187] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 89500, global step 1432410: learning rate 0.0001
[2019-04-03 23:59:46,424] A3C_AGENT_WORKER-Thread-5 INFO:Local step 89500, global step 1432709: loss 0.5080
[2019-04-03 23:59:46,424] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 89500, global step 1432709: learning rate 0.0001
[2019-04-03 23:59:48,913] A3C_AGENT_WORKER-Thread-16 INFO:Local step 89500, global step 1433266: loss 0.6201
[2019-04-03 23:59:48,915] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 89500, global step 1433267: learning rate 0.0001
[2019-04-03 23:59:49,984] A3C_AGENT_WORKER-Thread-19 INFO:Local step 90000, global step 1433512: loss 0.2787
[2019-04-03 23:59:49,984] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 90000, global step 1433512: learning rate 0.0001
[2019-04-03 23:59:50,165] A3C_AGENT_WORKER-Thread-18 INFO:Local step 89500, global step 1433556: loss 0.5987
[2019-04-03 23:59:50,166] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 89500, global step 1433556: learning rate 0.0001
[2019-04-03 23:59:52,355] A3C_AGENT_WORKER-Thread-20 INFO:Local step 89500, global step 1434050: loss 0.4411
[2019-04-03 23:59:52,397] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 89500, global step 1434050: learning rate 0.0001
[2019-04-03 23:59:52,601] A3C_AGENT_WORKER-Thread-9 INFO:Local step 89500, global step 1434096: loss 0.4181
[2019-04-03 23:59:52,602] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 89500, global step 1434096: learning rate 0.0001
[2019-04-03 23:59:52,972] A3C_AGENT_WORKER-Thread-10 INFO:Local step 89500, global step 1434169: loss 0.3918
[2019-04-03 23:59:52,972] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 89500, global step 1434169: learning rate 0.0001
[2019-04-03 23:59:53,306] A3C_AGENT_WORKER-Thread-15 INFO:Local step 89500, global step 1434229: loss 0.3986
[2019-04-03 23:59:53,306] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 89500, global step 1434229: learning rate 0.0001
[2019-04-03 23:59:53,935] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.7681448e-17 3.5436893e-09 4.6447255e-15 4.7094880e-12 2.9771411e-11
 5.6335493e-17 1.0000000e+00], sum to 1.0000
[2019-04-03 23:59:53,935] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2517
[2019-04-03 23:59:53,990] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 72.0, 0.0, 0.0, 26.0, 25.11572152185867, 0.320024470670981, 0.0, 1.0, 51807.93377626468], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2854800.0000, 
sim time next is 2855400.0000, 
raw observation next is [1.0, 73.16666666666667, 0.0, 0.0, 26.0, 25.14015126385536, 0.3211881723256231, 0.0, 1.0, 50916.71805539967], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 0.7316666666666667, 0.0, 0.0, 0.6666666666666666, 0.5950126053212799, 0.607062724108541, 0.0, 1.0, 0.24246056216856987], 
reward next is 0.7575, 
noisyNet noise sample is [array([-1.2268038], dtype=float32), -0.50871515]. 
=============================================
[2019-04-03 23:59:57,492] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.1048309e-17 7.0958461e-10 5.2983531e-16 3.7069692e-12 9.2456320e-12
 9.3883535e-18 1.0000000e+00], sum to 1.0000
[2019-04-03 23:59:57,492] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6565
[2019-04-03 23:59:57,519] A3C_AGENT_WORKER-Thread-17 INFO:Local step 90000, global step 1435276: loss 0.2527
[2019-04-03 23:59:57,521] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 90000, global step 1435276: learning rate 0.0001
[2019-04-03 23:59:57,559] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.6, 25.0, 83.0, 38.0, 26.0, 25.91058907196114, 0.4272783205653634, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2822400.0000, 
sim time next is 2823000.0000, 
raw observation next is [6.5, 25.5, 79.0, 50.66666666666667, 26.0, 26.00370750318855, 0.332426486067981, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.6426592797783934, 0.255, 0.2633333333333333, 0.0559852670349908, 0.6666666666666666, 0.6669756252657125, 0.610808828689327, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1957102], dtype=float32), 0.5561531]. 
=============================================
[2019-04-03 23:59:57,590] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[84.77503]
 [84.86432]
 [85.02512]
 [85.18125]
 [85.46778]], R is [[84.74885559]
 [84.90136719]
 [85.05235291]
 [85.201828  ]
 [85.34980774]].
[2019-04-03 23:59:59,459] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.1789621e-17 7.0026707e-10 9.5084971e-15 3.9436778e-12 3.0229170e-11
 8.4160869e-17 1.0000000e+00], sum to 1.0000
[2019-04-03 23:59:59,471] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7594
[2019-04-03 23:59:59,516] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 44.0, 0.0, 0.0, 26.0, 25.43974332867863, 0.4360251709378224, 0.0, 1.0, 18761.70884112401], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2842200.0000, 
sim time next is 2842800.0000, 
raw observation next is [2.0, 44.0, 0.0, 0.0, 26.0, 25.47761000987146, 0.4339419604715138, 0.0, 1.0, 18757.33950888826], 
processed observation next is [1.0, 0.9130434782608695, 0.518005540166205, 0.44, 0.0, 0.0, 0.6666666666666666, 0.6231341674892882, 0.6446473201571713, 0.0, 1.0, 0.08932066432803934], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.29073566], dtype=float32), 0.10462445]. 
=============================================
[2019-04-03 23:59:59,730] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.8733967e-17 2.8028024e-09 1.9563147e-15 1.3337932e-11 4.0737424e-11
 1.0241859e-17 1.0000000e+00], sum to 1.0000
[2019-04-03 23:59:59,731] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2047
[2019-04-03 23:59:59,740] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.1, 27.5, 49.0, 66.0, 26.0, 25.39858942528093, 0.3696981277740017, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2825400.0000, 
sim time next is 2826000.0000, 
raw observation next is [6.0, 28.0, 38.0, 61.0, 26.0, 25.63049867203118, 0.3917184562656946, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.6288088642659281, 0.28, 0.12666666666666668, 0.06740331491712707, 0.6666666666666666, 0.6358748893359317, 0.6305728187552315, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.19176373], dtype=float32), 0.41533044]. 
=============================================
[2019-04-03 23:59:59,819] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[85.87282 ]
 [86.39279 ]
 [86.722374]
 [85.956375]
 [85.99546 ]], R is [[85.49950409]
 [85.64450836]
 [85.78806305]
 [85.08258057]
 [85.23175812]].
[2019-04-04 00:00:05,297] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1760151e-16 2.2292603e-08 3.5014333e-14 2.5992358e-11 2.2760376e-10
 9.4557178e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 00:00:05,297] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9081
[2019-04-04 00:00:05,311] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 93.0, 0.0, 0.0, 26.0, 24.92127598746496, 0.2581053946143239, 0.0, 1.0, 55708.26410397994], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2866200.0000, 
sim time next is 2866800.0000, 
raw observation next is [1.0, 93.0, 0.0, 0.0, 26.0, 24.9087736204782, 0.25053962543479, 0.0, 1.0, 55732.82498001164], 
processed observation next is [1.0, 0.17391304347826086, 0.4903047091412743, 0.93, 0.0, 0.0, 0.6666666666666666, 0.5757311350398501, 0.5835132084782634, 0.0, 1.0, 0.2653944046667221], 
reward next is 0.7346, 
noisyNet noise sample is [array([-0.9044221], dtype=float32), -0.6373427]. 
=============================================
[2019-04-04 00:00:11,541] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.9246700e-15 1.2213410e-07 2.1543302e-13 2.2452426e-10 1.6760319e-09
 1.4157676e-14 9.9999988e-01], sum to 1.0000
[2019-04-04 00:00:11,541] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5355
[2019-04-04 00:00:11,607] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 71.0, 142.3333333333333, 118.1666666666667, 26.0, 25.26350022663998, 0.3510363778375454, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2971200.0000, 
sim time next is 2971800.0000, 
raw observation next is [-4.0, 71.0, 154.0, 132.0, 26.0, 25.25196778173771, 0.3399382830297704, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.3518005540166205, 0.71, 0.5133333333333333, 0.14585635359116023, 0.6666666666666666, 0.6043306484781427, 0.6133127610099235, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0970745], dtype=float32), -1.1348552]. 
=============================================
[2019-04-04 00:00:11,736] A3C_AGENT_WORKER-Thread-2 INFO:Local step 90000, global step 1439111: loss 0.2934
[2019-04-04 00:00:11,737] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 90000, global step 1439111: learning rate 0.0001
[2019-04-04 00:00:12,046] A3C_AGENT_WORKER-Thread-7 INFO:Local step 90000, global step 1439183: loss 0.3060
[2019-04-04 00:00:12,046] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 90000, global step 1439183: learning rate 0.0001
[2019-04-04 00:00:14,629] A3C_AGENT_WORKER-Thread-6 INFO:Local step 90000, global step 1439875: loss 0.1922
[2019-04-04 00:00:14,645] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 90000, global step 1439877: learning rate 0.0001
[2019-04-04 00:00:16,087] A3C_AGENT_WORKER-Thread-8 INFO:Local step 90000, global step 1440266: loss 0.1791
[2019-04-04 00:00:16,091] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 90000, global step 1440266: learning rate 0.0001
[2019-04-04 00:00:16,920] A3C_AGENT_WORKER-Thread-14 INFO:Local step 90000, global step 1440521: loss 0.1601
[2019-04-04 00:00:16,923] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 90000, global step 1440521: learning rate 0.0001
[2019-04-04 00:00:17,117] A3C_AGENT_WORKER-Thread-4 INFO:Local step 90000, global step 1440584: loss 0.1482
[2019-04-04 00:00:17,118] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 90000, global step 1440584: learning rate 0.0001
[2019-04-04 00:00:17,160] A3C_AGENT_WORKER-Thread-3 INFO:Local step 90000, global step 1440601: loss 0.1528
[2019-04-04 00:00:17,161] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 90000, global step 1440601: learning rate 0.0001
[2019-04-04 00:00:17,885] A3C_AGENT_WORKER-Thread-5 INFO:Local step 90000, global step 1440824: loss 0.1397
[2019-04-04 00:00:17,896] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 90000, global step 1440824: learning rate 0.0001
[2019-04-04 00:00:18,320] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.5542337e-15 2.1909331e-07 7.2577733e-14 3.5767636e-10 8.8133201e-10
 1.7392729e-14 9.9999976e-01], sum to 1.0000
[2019-04-04 00:00:18,322] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3513
[2019-04-04 00:00:18,379] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 71.0, 0.0, 0.0, 26.0, 24.58324336535351, 0.1942639775639764, 0.0, 1.0, 38056.83393186396], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3028200.0000, 
sim time next is 3028800.0000, 
raw observation next is [-5.0, 71.0, 0.0, 0.0, 26.0, 24.60031698048386, 0.1895443154541918, 0.0, 1.0, 38095.48688355942], 
processed observation next is [0.0, 0.043478260869565216, 0.32409972299168976, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5500264150403217, 0.5631814384847306, 0.0, 1.0, 0.181407080397902], 
reward next is 0.8186, 
noisyNet noise sample is [array([-0.57976264], dtype=float32), -0.06765052]. 
=============================================
[2019-04-04 00:00:18,741] A3C_AGENT_WORKER-Thread-19 INFO:Local step 90500, global step 1441017: loss 0.1723
[2019-04-04 00:00:18,806] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 90500, global step 1441017: learning rate 0.0001
[2019-04-04 00:00:19,531] A3C_AGENT_WORKER-Thread-16 INFO:Local step 90000, global step 1441232: loss 0.0847
[2019-04-04 00:00:19,533] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 90000, global step 1441232: learning rate 0.0001
[2019-04-04 00:00:20,940] A3C_AGENT_WORKER-Thread-18 INFO:Local step 90000, global step 1441619: loss 0.0518
[2019-04-04 00:00:20,940] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 90000, global step 1441619: learning rate 0.0001
[2019-04-04 00:00:22,475] A3C_AGENT_WORKER-Thread-9 INFO:Local step 90000, global step 1442062: loss 0.0388
[2019-04-04 00:00:22,493] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 90000, global step 1442062: learning rate 0.0001
[2019-04-04 00:00:22,753] A3C_AGENT_WORKER-Thread-10 INFO:Local step 90000, global step 1442132: loss 0.0357
[2019-04-04 00:00:22,764] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 90000, global step 1442133: learning rate 0.0001
[2019-04-04 00:00:22,950] A3C_AGENT_WORKER-Thread-20 INFO:Local step 90000, global step 1442190: loss 0.0383
[2019-04-04 00:00:22,951] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 90000, global step 1442190: learning rate 0.0001
[2019-04-04 00:00:24,187] A3C_AGENT_WORKER-Thread-17 INFO:Local step 90500, global step 1442534: loss 0.1750
[2019-04-04 00:00:24,188] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 90500, global step 1442534: learning rate 0.0001
[2019-04-04 00:00:24,451] A3C_AGENT_WORKER-Thread-15 INFO:Local step 90000, global step 1442639: loss 0.0397
[2019-04-04 00:00:24,453] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 90000, global step 1442639: learning rate 0.0001
[2019-04-04 00:00:32,204] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.87924462e-17 3.48868978e-09 3.34879151e-14 1.32771676e-11
 1.84158189e-10 2.19325867e-16 1.00000000e+00], sum to 1.0000
[2019-04-04 00:00:32,204] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0813
[2019-04-04 00:00:32,239] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.166666666666667, 100.0, 0.0, 0.0, 26.0, 25.33738441442875, 0.5079499146183274, 0.0, 1.0, 87706.57278168337], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3211800.0000, 
sim time next is 3212400.0000, 
raw observation next is [-1.333333333333333, 100.0, 0.0, 0.0, 26.0, 25.29639825323102, 0.5054039611662144, 0.0, 1.0, 59437.98741654179], 
processed observation next is [1.0, 0.17391304347826086, 0.42566943674976926, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6080331877692515, 0.6684679870554048, 0.0, 1.0, 0.28303803531686567], 
reward next is 0.7170, 
noisyNet noise sample is [array([-1.2299913], dtype=float32), -0.79953605]. 
=============================================
[2019-04-04 00:00:32,369] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.4458985e-18 8.7330643e-11 6.6155205e-17 3.0388073e-12 2.5011044e-12
 1.1621534e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:00:32,369] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7113
[2019-04-04 00:00:32,409] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.333333333333333, 87.5, 112.3333333333333, 815.6666666666666, 26.0, 25.01980024642993, 0.5909399802617167, 1.0, 1.0, 196217.9094192961], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3244200.0000, 
sim time next is 3244800.0000, 
raw observation next is [-2.666666666666667, 90.0, 111.6666666666667, 813.8333333333334, 26.0, 25.4844489970005, 0.6860126318584049, 1.0, 1.0, 65057.4560804026], 
processed observation next is [1.0, 0.5652173913043478, 0.38873499538319484, 0.9, 0.37222222222222234, 0.8992633517495396, 0.6666666666666666, 0.6237040830833749, 0.728670877286135, 1.0, 1.0, 0.30979740990667903], 
reward next is 0.6902, 
noisyNet noise sample is [array([-0.12202195], dtype=float32), 1.2646723]. 
=============================================
[2019-04-04 00:00:34,335] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.8788162e-18 3.8901166e-10 2.4913912e-16 2.3988379e-12 2.4637894e-12
 2.6364748e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:00:34,335] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1565
[2019-04-04 00:00:34,362] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 88.0, 107.6666666666667, 735.5, 26.0, 26.39842961992017, 0.6947931754550002, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3234000.0000, 
sim time next is 3234600.0000, 
raw observation next is [-2.7, 86.0, 109.0, 752.0, 26.0, 26.41783753803377, 0.6985344871061603, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.38781163434903054, 0.86, 0.36333333333333334, 0.830939226519337, 0.6666666666666666, 0.7014864615028141, 0.7328448290353867, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.8471403], dtype=float32), 2.0520701]. 
=============================================
[2019-04-04 00:00:36,087] A3C_AGENT_WORKER-Thread-7 INFO:Local step 90500, global step 1446935: loss 0.1566
[2019-04-04 00:00:36,088] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 90500, global step 1446935: learning rate 0.0001
[2019-04-04 00:00:37,081] A3C_AGENT_WORKER-Thread-2 INFO:Local step 90500, global step 1447270: loss 0.1697
[2019-04-04 00:00:37,082] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 90500, global step 1447270: learning rate 0.0001
[2019-04-04 00:00:39,032] A3C_AGENT_WORKER-Thread-6 INFO:Local step 90500, global step 1447940: loss 0.1507
[2019-04-04 00:00:39,033] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 90500, global step 1447940: learning rate 0.0001
[2019-04-04 00:00:39,413] A3C_AGENT_WORKER-Thread-14 INFO:Local step 90500, global step 1448071: loss 0.1335
[2019-04-04 00:00:39,416] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 90500, global step 1448071: learning rate 0.0001
[2019-04-04 00:00:40,107] A3C_AGENT_WORKER-Thread-8 INFO:Local step 90500, global step 1448306: loss 0.1275
[2019-04-04 00:00:40,109] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 90500, global step 1448309: learning rate 0.0001
[2019-04-04 00:00:40,549] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.5028631e-18 2.1227612e-08 2.3710813e-15 6.0872232e-12 2.1513803e-11
 5.7334959e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:00:40,549] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7940
[2019-04-04 00:00:40,616] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.0, 80.5, 86.0, 396.0, 26.0, 25.72936242432628, 0.4591962670261521, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3313800.0000, 
sim time next is 3314400.0000, 
raw observation next is [-9.666666666666668, 79.33333333333333, 89.0, 432.5, 26.0, 25.85623035260561, 0.4722426618437292, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.19482917820867957, 0.7933333333333333, 0.2966666666666667, 0.47790055248618785, 0.6666666666666666, 0.6546858627171342, 0.6574142206145764, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.17840257], dtype=float32), 2.048109]. 
=============================================
[2019-04-04 00:00:41,095] A3C_AGENT_WORKER-Thread-5 INFO:Local step 90500, global step 1448646: loss 0.0950
[2019-04-04 00:00:41,096] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 90500, global step 1448646: learning rate 0.0001
[2019-04-04 00:00:41,761] A3C_AGENT_WORKER-Thread-4 INFO:Local step 90500, global step 1448845: loss 0.0757
[2019-04-04 00:00:41,763] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 90500, global step 1448845: learning rate 0.0001
[2019-04-04 00:00:41,777] A3C_AGENT_WORKER-Thread-3 INFO:Local step 90500, global step 1448850: loss 0.0859
[2019-04-04 00:00:41,777] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 90500, global step 1448850: learning rate 0.0001
[2019-04-04 00:00:42,272] A3C_AGENT_WORKER-Thread-19 INFO:Local step 91000, global step 1449023: loss 0.0026
[2019-04-04 00:00:42,279] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 91000, global step 1449023: learning rate 0.0001
[2019-04-04 00:00:42,978] A3C_AGENT_WORKER-Thread-16 INFO:Local step 90500, global step 1449271: loss 0.0452
[2019-04-04 00:00:42,981] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 90500, global step 1449273: learning rate 0.0001
[2019-04-04 00:00:43,846] A3C_AGENT_WORKER-Thread-18 INFO:Local step 90500, global step 1449620: loss 0.0463
[2019-04-04 00:00:43,848] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 90500, global step 1449620: learning rate 0.0001
[2019-04-04 00:00:45,950] A3C_AGENT_WORKER-Thread-10 INFO:Local step 90500, global step 1450289: loss 0.0239
[2019-04-04 00:00:45,951] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 90500, global step 1450289: learning rate 0.0001
[2019-04-04 00:00:46,114] A3C_AGENT_WORKER-Thread-9 INFO:Local step 90500, global step 1450351: loss 0.0174
[2019-04-04 00:00:46,115] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 90500, global step 1450351: learning rate 0.0001
[2019-04-04 00:00:46,289] A3C_AGENT_WORKER-Thread-20 INFO:Local step 90500, global step 1450406: loss 0.0213
[2019-04-04 00:00:46,293] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 90500, global step 1450406: learning rate 0.0001
[2019-04-04 00:00:46,640] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.2356288e-18 3.2112371e-08 8.0665515e-16 2.3268301e-12 3.6625050e-11
 3.8390220e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:00:46,641] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8529
[2019-04-04 00:00:46,726] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.333333333333332, 78.16666666666667, 92.0, 469.0, 26.0, 25.92329342027681, 0.4898977292546152, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3315000.0000, 
sim time next is 3315600.0000, 
raw observation next is [-9.0, 77.0, 95.0, 505.5, 26.0, 26.02510267299644, 0.5086039422497409, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.21329639889196678, 0.77, 0.31666666666666665, 0.5585635359116022, 0.6666666666666666, 0.6687585560830366, 0.6695346474165803, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5482304], dtype=float32), -0.7130007]. 
=============================================
[2019-04-04 00:00:46,876] A3C_AGENT_WORKER-Thread-17 INFO:Local step 91000, global step 1450609: loss 0.0536
[2019-04-04 00:00:46,877] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 91000, global step 1450609: learning rate 0.0001
[2019-04-04 00:00:46,897] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.1279754e-20 3.7780157e-11 1.5632042e-18 3.0272876e-13 3.8641394e-13
 4.2693073e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 00:00:46,897] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5072
[2019-04-04 00:00:46,936] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.333333333333333, 64.0, 80.16666666666666, 672.1666666666667, 26.0, 26.65636252127855, 0.6759470882386149, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3426000.0000, 
sim time next is 3426600.0000, 
raw observation next is [2.166666666666667, 65.5, 76.33333333333334, 640.3333333333334, 26.0, 26.70273931924915, 0.5656510575798792, 1.0, 1.0, 3736.082334046107], 
processed observation next is [1.0, 0.6521739130434783, 0.5226223453370269, 0.655, 0.2544444444444445, 0.7075506445672192, 0.6666666666666666, 0.725228276604096, 0.6885503525266264, 1.0, 1.0, 0.017790868257362414], 
reward next is 0.9822, 
noisyNet noise sample is [array([-1.7409192], dtype=float32), 0.16031496]. 
=============================================
[2019-04-04 00:00:47,599] A3C_AGENT_WORKER-Thread-15 INFO:Local step 90500, global step 1450869: loss 0.0053
[2019-04-04 00:00:47,600] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 90500, global step 1450869: learning rate 0.0001
[2019-04-04 00:00:56,490] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.3614014e-15 2.2010336e-07 1.3365266e-13 1.2212900e-10 1.8834250e-09
 8.5230022e-15 9.9999976e-01], sum to 1.0000
[2019-04-04 00:00:56,502] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4520
[2019-04-04 00:00:56,528] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.666666666666666, 67.0, 0.0, 0.0, 26.0, 24.97029658988136, 0.3352026593399478, 0.0, 1.0, 40884.11115528287], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3559200.0000, 
sim time next is 3559800.0000, 
raw observation next is [-4.833333333333334, 66.0, 0.0, 0.0, 26.0, 24.94985404733506, 0.3251060399933215, 0.0, 1.0, 40867.2268146779], 
processed observation next is [0.0, 0.17391304347826086, 0.32871652816251157, 0.66, 0.0, 0.0, 0.6666666666666666, 0.5791545039445882, 0.6083686799977738, 0.0, 1.0, 0.1946058419746567], 
reward next is 0.8054, 
noisyNet noise sample is [array([0.6912921], dtype=float32), -0.59437376]. 
=============================================
[2019-04-04 00:00:58,260] A3C_AGENT_WORKER-Thread-7 INFO:Local step 91000, global step 1454737: loss 0.0030
[2019-04-04 00:00:58,261] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 91000, global step 1454737: learning rate 0.0001
[2019-04-04 00:00:59,297] A3C_AGENT_WORKER-Thread-2 INFO:Local step 91000, global step 1455054: loss 0.0020
[2019-04-04 00:00:59,300] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 91000, global step 1455054: learning rate 0.0001
[2019-04-04 00:01:02,307] A3C_AGENT_WORKER-Thread-8 INFO:Local step 91000, global step 1456044: loss 0.0024
[2019-04-04 00:01:02,325] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 91000, global step 1456046: learning rate 0.0001
[2019-04-04 00:01:02,337] A3C_AGENT_WORKER-Thread-14 INFO:Local step 91000, global step 1456050: loss 0.0030
[2019-04-04 00:01:02,337] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 91000, global step 1456050: learning rate 0.0001
[2019-04-04 00:01:02,439] A3C_AGENT_WORKER-Thread-6 INFO:Local step 91000, global step 1456093: loss 0.0022
[2019-04-04 00:01:02,440] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 91000, global step 1456093: learning rate 0.0001
[2019-04-04 00:01:03,147] A3C_AGENT_WORKER-Thread-19 INFO:Local step 91500, global step 1456339: loss 0.3644
[2019-04-04 00:01:03,162] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 91500, global step 1456340: learning rate 0.0001
[2019-04-04 00:01:04,736] A3C_AGENT_WORKER-Thread-4 INFO:Local step 91000, global step 1456902: loss 0.0012
[2019-04-04 00:01:04,738] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 91000, global step 1456904: learning rate 0.0001
[2019-04-04 00:01:04,850] A3C_AGENT_WORKER-Thread-3 INFO:Local step 91000, global step 1456945: loss 0.0009
[2019-04-04 00:01:04,852] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 91000, global step 1456947: learning rate 0.0001
[2019-04-04 00:01:05,328] A3C_AGENT_WORKER-Thread-5 INFO:Local step 91000, global step 1457096: loss 0.0008
[2019-04-04 00:01:05,335] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 91000, global step 1457096: learning rate 0.0001
[2019-04-04 00:01:06,516] A3C_AGENT_WORKER-Thread-16 INFO:Local step 91000, global step 1457443: loss 0.0009
[2019-04-04 00:01:06,518] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 91000, global step 1457443: learning rate 0.0001
[2019-04-04 00:01:08,287] A3C_AGENT_WORKER-Thread-18 INFO:Local step 91000, global step 1458045: loss 0.0037
[2019-04-04 00:01:08,287] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 91000, global step 1458045: learning rate 0.0001
[2019-04-04 00:01:09,079] A3C_AGENT_WORKER-Thread-17 INFO:Local step 91500, global step 1458326: loss 0.4775
[2019-04-04 00:01:09,080] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 91500, global step 1458326: learning rate 0.0001
[2019-04-04 00:01:09,479] A3C_AGENT_WORKER-Thread-10 INFO:Local step 91000, global step 1458489: loss 0.0017
[2019-04-04 00:01:09,480] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 91000, global step 1458489: learning rate 0.0001
[2019-04-04 00:01:09,807] A3C_AGENT_WORKER-Thread-9 INFO:Local step 91000, global step 1458605: loss 0.0007
[2019-04-04 00:01:09,809] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 91000, global step 1458605: learning rate 0.0001
[2019-04-04 00:01:10,335] A3C_AGENT_WORKER-Thread-20 INFO:Local step 91000, global step 1458806: loss 0.0007
[2019-04-04 00:01:10,335] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 91000, global step 1458806: learning rate 0.0001
[2019-04-04 00:01:10,854] A3C_AGENT_WORKER-Thread-15 INFO:Local step 91000, global step 1458968: loss 0.0004
[2019-04-04 00:01:10,854] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 91000, global step 1458968: learning rate 0.0001
[2019-04-04 00:01:15,738] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.2716129e-16 8.4495451e-09 5.0659969e-15 4.9206618e-11 1.1375083e-10
 2.5711434e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 00:01:15,738] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2750
[2019-04-04 00:01:15,771] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.666666666666667, 53.0, 67.83333333333333, 552.5, 26.0, 25.50174911250342, 0.4736384600501942, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3687600.0000, 
sim time next is 3688200.0000, 
raw observation next is [4.5, 54.5, 64.0, 522.0, 26.0, 25.49465389744457, 0.4674638167414282, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.5872576177285319, 0.545, 0.21333333333333335, 0.5767955801104973, 0.6666666666666666, 0.6245544914537143, 0.6558212722471427, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8497519], dtype=float32), 0.89418226]. 
=============================================
[2019-04-04 00:01:19,249] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.3316228e-18 3.6728742e-10 4.7861676e-17 1.3769509e-12 5.9680286e-12
 1.0613059e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:01:19,250] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7313
[2019-04-04 00:01:19,295] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 32.0, 117.5, 792.5, 26.0, 26.6718038180883, 0.6277394064909655, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4100400.0000, 
sim time next is 4101000.0000, 
raw observation next is [-0.6666666666666667, 31.33333333333334, 118.6666666666667, 800.3333333333334, 26.0, 26.69806236327727, 0.6356959628252002, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.44413665743305636, 0.3133333333333334, 0.39555555555555566, 0.8843462246777164, 0.6666666666666666, 0.7248385302731059, 0.7118986542750667, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3562893], dtype=float32), 0.14678046]. 
=============================================
[2019-04-04 00:01:19,320] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[91.987   ]
 [92.001015]
 [92.02638 ]
 [92.11375 ]
 [92.08157 ]], R is [[92.07453918]
 [92.15379333]
 [92.23225403]
 [92.30992889]
 [92.38683319]].
[2019-04-04 00:01:22,136] A3C_AGENT_WORKER-Thread-2 INFO:Local step 91500, global step 1462550: loss 0.6250
[2019-04-04 00:01:22,136] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 91500, global step 1462550: learning rate 0.0001
[2019-04-04 00:01:22,642] A3C_AGENT_WORKER-Thread-7 INFO:Local step 91500, global step 1462724: loss 0.6354
[2019-04-04 00:01:22,644] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 91500, global step 1462724: learning rate 0.0001
[2019-04-04 00:01:26,391] A3C_AGENT_WORKER-Thread-8 INFO:Local step 91500, global step 1463779: loss 0.5400
[2019-04-04 00:01:26,392] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 91500, global step 1463779: learning rate 0.0001
[2019-04-04 00:01:26,696] A3C_AGENT_WORKER-Thread-6 INFO:Local step 91500, global step 1463869: loss 0.5001
[2019-04-04 00:01:26,696] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 91500, global step 1463869: learning rate 0.0001
[2019-04-04 00:01:26,738] A3C_AGENT_WORKER-Thread-14 INFO:Local step 91500, global step 1463880: loss 0.5192
[2019-04-04 00:01:26,738] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 91500, global step 1463880: learning rate 0.0001
[2019-04-04 00:01:29,795] A3C_AGENT_WORKER-Thread-4 INFO:Local step 91500, global step 1464687: loss 0.3710
[2019-04-04 00:01:29,796] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 91500, global step 1464687: learning rate 0.0001
[2019-04-04 00:01:30,337] A3C_AGENT_WORKER-Thread-3 INFO:Local step 91500, global step 1464814: loss 0.3545
[2019-04-04 00:01:30,337] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 91500, global step 1464814: learning rate 0.0001
[2019-04-04 00:01:30,419] A3C_AGENT_WORKER-Thread-19 INFO:Local step 92000, global step 1464830: loss 0.0465
[2019-04-04 00:01:30,436] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 92000, global step 1464830: learning rate 0.0001
[2019-04-04 00:01:31,232] A3C_AGENT_WORKER-Thread-5 INFO:Local step 91500, global step 1465029: loss 0.3319
[2019-04-04 00:01:31,233] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 91500, global step 1465029: learning rate 0.0001
[2019-04-04 00:01:32,308] A3C_AGENT_WORKER-Thread-16 INFO:Local step 91500, global step 1465329: loss 0.2664
[2019-04-04 00:01:32,309] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 91500, global step 1465329: learning rate 0.0001
[2019-04-04 00:01:34,724] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.8709572e-15 3.4844884e-08 4.6951982e-13 2.6924163e-10 2.4174482e-09
 1.7572764e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 00:01:34,726] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.2533
[2019-04-04 00:01:34,730] A3C_AGENT_WORKER-Thread-18 INFO:Local step 91500, global step 1465941: loss 0.1661
[2019-04-04 00:01:34,731] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 91500, global step 1465941: learning rate 0.0001
[2019-04-04 00:01:34,739] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-13.83333333333333, 68.0, 0.0, 0.0, 26.0, 23.42211401958189, -0.05571029908381894, 0.0, 1.0, 43091.98055258585], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3999000.0000, 
sim time next is 3999600.0000, 
raw observation next is [-14.0, 69.0, 0.0, 0.0, 26.0, 23.39417394220888, -0.06339554255597912, 0.0, 1.0, 42977.82518129573], 
processed observation next is [1.0, 0.30434782608695654, 0.07479224376731301, 0.69, 0.0, 0.0, 0.6666666666666666, 0.4495144951840733, 0.4788681524813403, 0.0, 1.0, 0.20465631038712254], 
reward next is 0.7953, 
noisyNet noise sample is [array([-0.07393856], dtype=float32), -0.49492487]. 
=============================================
[2019-04-04 00:01:34,911] A3C_AGENT_WORKER-Thread-10 INFO:Local step 91500, global step 1465979: loss 0.1590
[2019-04-04 00:01:34,912] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 91500, global step 1465979: learning rate 0.0001
[2019-04-04 00:01:35,592] A3C_AGENT_WORKER-Thread-9 INFO:Local step 91500, global step 1466135: loss 0.1379
[2019-04-04 00:01:35,592] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 91500, global step 1466135: learning rate 0.0001
[2019-04-04 00:01:35,995] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.4484827e-16 5.6573692e-09 4.7905954e-14 4.7288159e-11 1.0890849e-09
 1.2219442e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 00:01:35,996] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8156
[2019-04-04 00:01:36,023] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.0, 58.0, 0.0, 0.0, 26.0, 24.85356004560278, 0.3175795989767824, 0.0, 1.0, 44114.88235627265], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3974400.0000, 
sim time next is 3975000.0000, 
raw observation next is [-10.16666666666667, 58.0, 0.0, 0.0, 26.0, 24.81347924787479, 0.3063339177221197, 0.0, 1.0, 44108.30086904349], 
processed observation next is [1.0, 0.0, 0.18097876269621416, 0.58, 0.0, 0.0, 0.6666666666666666, 0.5677899373228991, 0.6021113059073732, 0.0, 1.0, 0.21003952794782613], 
reward next is 0.7900, 
noisyNet noise sample is [array([0.6009979], dtype=float32), -0.091051474]. 
=============================================
[2019-04-04 00:01:36,066] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[74.224525]
 [74.2827  ]
 [74.45206 ]
 [74.63631 ]
 [74.81175 ]], R is [[74.16150665]
 [74.20982361]
 [74.25767517]
 [74.30510712]
 [74.35212708]].
[2019-04-04 00:01:36,131] A3C_AGENT_WORKER-Thread-20 INFO:Local step 91500, global step 1466260: loss 0.1373
[2019-04-04 00:01:36,138] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 91500, global step 1466260: learning rate 0.0001
[2019-04-04 00:01:37,670] A3C_AGENT_WORKER-Thread-15 INFO:Local step 91500, global step 1466642: loss 0.0659
[2019-04-04 00:01:37,671] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 91500, global step 1466642: learning rate 0.0001
[2019-04-04 00:01:39,046] A3C_AGENT_WORKER-Thread-17 INFO:Local step 92000, global step 1467011: loss 0.0649
[2019-04-04 00:01:39,049] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 92000, global step 1467011: learning rate 0.0001
[2019-04-04 00:01:44,687] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.6091065e-15 1.7638878e-07 5.5207417e-13 4.3815737e-10 1.8765511e-09
 1.7919456e-14 9.9999988e-01], sum to 1.0000
[2019-04-04 00:01:44,699] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0578
[2019-04-04 00:01:44,726] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-13.83333333333333, 68.0, 0.0, 0.0, 26.0, 23.42211401958189, -0.05571029908381894, 0.0, 1.0, 43091.98055258585], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3999000.0000, 
sim time next is 3999600.0000, 
raw observation next is [-14.0, 69.0, 0.0, 0.0, 26.0, 23.39417394220888, -0.06339554255597912, 0.0, 1.0, 42977.82518129573], 
processed observation next is [1.0, 0.30434782608695654, 0.07479224376731301, 0.69, 0.0, 0.0, 0.6666666666666666, 0.4495144951840733, 0.4788681524813403, 0.0, 1.0, 0.20465631038712254], 
reward next is 0.7953, 
noisyNet noise sample is [array([0.17950644], dtype=float32), -1.5992802]. 
=============================================
[2019-04-04 00:01:48,562] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.07291241e-16 1.35372495e-08 2.87727944e-14 7.80216725e-11
 2.34480463e-10 2.12928646e-15 1.00000000e+00], sum to 1.0000
[2019-04-04 00:01:48,562] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9745
[2019-04-04 00:01:48,647] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 41.0, 0.0, 0.0, 26.0, 25.12340783229716, 0.3374208911349341, 0.0, 1.0, 40818.53046298795], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4066200.0000, 
sim time next is 4066800.0000, 
raw observation next is [-6.0, 41.0, 0.0, 0.0, 26.0, 25.18714140761337, 0.3354875598597832, 0.0, 1.0, 40816.78917764825], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.41, 0.0, 0.0, 0.6666666666666666, 0.5989284506344476, 0.6118291866199277, 0.0, 1.0, 0.19436566275070596], 
reward next is 0.8056, 
noisyNet noise sample is [array([-0.26409653], dtype=float32), 0.6787165]. 
=============================================
[2019-04-04 00:01:53,353] A3C_AGENT_WORKER-Thread-2 INFO:Local step 92000, global step 1470559: loss 0.0346
[2019-04-04 00:01:53,388] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 92000, global step 1470559: learning rate 0.0001
[2019-04-04 00:01:53,711] A3C_AGENT_WORKER-Thread-7 INFO:Local step 92000, global step 1470634: loss 0.0439
[2019-04-04 00:01:53,712] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 92000, global step 1470634: learning rate 0.0001
[2019-04-04 00:01:58,208] A3C_AGENT_WORKER-Thread-8 INFO:Local step 92000, global step 1471844: loss 0.0397
[2019-04-04 00:01:58,229] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 92000, global step 1471844: learning rate 0.0001
[2019-04-04 00:01:58,902] A3C_AGENT_WORKER-Thread-14 INFO:Local step 92000, global step 1472053: loss 0.0344
[2019-04-04 00:01:58,907] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 92000, global step 1472053: learning rate 0.0001
[2019-04-04 00:01:59,399] A3C_AGENT_WORKER-Thread-19 INFO:Local step 92500, global step 1472190: loss 10.4022
[2019-04-04 00:01:59,400] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 92500, global step 1472190: learning rate 0.0001
[2019-04-04 00:01:59,553] A3C_AGENT_WORKER-Thread-6 INFO:Local step 92000, global step 1472245: loss 0.0372
[2019-04-04 00:01:59,557] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 92000, global step 1472245: learning rate 0.0001
[2019-04-04 00:02:01,354] A3C_AGENT_WORKER-Thread-4 INFO:Local step 92000, global step 1472731: loss 0.0407
[2019-04-04 00:02:01,355] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 92000, global step 1472731: learning rate 0.0001
[2019-04-04 00:02:03,304] A3C_AGENT_WORKER-Thread-5 INFO:Local step 92000, global step 1473170: loss 0.0227
[2019-04-04 00:02:03,396] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 92000, global step 1473170: learning rate 0.0001
[2019-04-04 00:02:03,485] A3C_AGENT_WORKER-Thread-3 INFO:Local step 92000, global step 1473206: loss 0.0270
[2019-04-04 00:02:03,491] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 92000, global step 1473206: learning rate 0.0001
[2019-04-04 00:02:04,697] A3C_AGENT_WORKER-Thread-16 INFO:Local step 92000, global step 1473576: loss 0.0141
[2019-04-04 00:02:04,702] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 92000, global step 1473576: learning rate 0.0001
[2019-04-04 00:02:04,705] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.1064531e-17 8.1535717e-10 4.5179077e-15 6.4758494e-12 6.8066122e-11
 7.9525826e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:02:04,705] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4106
[2019-04-04 00:02:04,720] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.09999999999999999, 72.0, 0.0, 0.0, 26.0, 25.44214549780003, 0.4859006752333563, 0.0, 1.0, 69171.53954693575], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4486800.0000, 
sim time next is 4487400.0000, 
raw observation next is [-0.15, 72.0, 0.0, 0.0, 26.0, 25.39546477772312, 0.4832900525748008, 0.0, 1.0, 76770.08975617538], 
processed observation next is [1.0, 0.9565217391304348, 0.458448753462604, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6162887314769266, 0.6610966841916003, 0.0, 1.0, 0.36557185598178754], 
reward next is 0.6344, 
noisyNet noise sample is [array([-0.46569553], dtype=float32), -1.3253489]. 
=============================================
[2019-04-04 00:02:06,230] A3C_AGENT_WORKER-Thread-18 INFO:Local step 92000, global step 1474026: loss 0.0202
[2019-04-04 00:02:06,232] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 92000, global step 1474026: learning rate 0.0001
[2019-04-04 00:02:06,936] A3C_AGENT_WORKER-Thread-9 INFO:Local step 92000, global step 1474232: loss 0.0152
[2019-04-04 00:02:06,937] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 92000, global step 1474232: learning rate 0.0001
[2019-04-04 00:02:06,987] A3C_AGENT_WORKER-Thread-10 INFO:Local step 92000, global step 1474249: loss 0.0185
[2019-04-04 00:02:06,988] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 92000, global step 1474249: learning rate 0.0001
[2019-04-04 00:02:08,251] A3C_AGENT_WORKER-Thread-20 INFO:Local step 92000, global step 1474609: loss 0.0140
[2019-04-04 00:02:08,252] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 92000, global step 1474609: learning rate 0.0001
[2019-04-04 00:02:08,563] A3C_AGENT_WORKER-Thread-17 INFO:Local step 92500, global step 1474688: loss 10.0541
[2019-04-04 00:02:08,591] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 92500, global step 1474699: learning rate 0.0001
[2019-04-04 00:02:09,185] A3C_AGENT_WORKER-Thread-15 INFO:Local step 92000, global step 1474854: loss 0.0152
[2019-04-04 00:02:09,186] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 92000, global step 1474854: learning rate 0.0001
[2019-04-04 00:02:09,576] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1959434e-16 8.6936225e-08 5.4975169e-14 4.1670742e-10 1.3535021e-09
 1.2175338e-14 9.9999988e-01], sum to 1.0000
[2019-04-04 00:02:09,576] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8304
[2019-04-04 00:02:09,587] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 48.0, 0.0, 0.0, 26.0, 25.42894863510603, 0.3497558857824769, 0.0, 1.0, 32339.73601528629], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4236600.0000, 
sim time next is 4237200.0000, 
raw observation next is [2.0, 48.0, 0.0, 0.0, 26.0, 25.42184105846313, 0.3463198504921512, 0.0, 1.0, 39228.68421117693], 
processed observation next is [0.0, 0.043478260869565216, 0.518005540166205, 0.48, 0.0, 0.0, 0.6666666666666666, 0.6184867548719275, 0.6154399501640504, 0.0, 1.0, 0.18680325814846158], 
reward next is 0.8132, 
noisyNet noise sample is [array([-0.15413749], dtype=float32), -1.0322732]. 
=============================================
[2019-04-04 00:02:14,772] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.8526655e-16 1.6574080e-07 6.3038957e-14 4.8627678e-11 3.2928138e-10
 3.3243013e-15 9.9999988e-01], sum to 1.0000
[2019-04-04 00:02:14,797] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0875
[2019-04-04 00:02:14,846] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.583333333333333, 69.33333333333333, 0.0, 0.0, 26.0, 25.34906279926992, 0.4012923571129307, 0.0, 1.0, 66916.7976746004], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4594200.0000, 
sim time next is 4594800.0000, 
raw observation next is [-1.666666666666667, 69.66666666666667, 0.0, 0.0, 26.0, 25.30860791023184, 0.3976936661997867, 0.0, 1.0, 50085.8463520092], 
processed observation next is [1.0, 0.17391304347826086, 0.4164358264081256, 0.6966666666666668, 0.0, 0.0, 0.6666666666666666, 0.6090506591859866, 0.632564555399929, 0.0, 1.0, 0.23850403024766284], 
reward next is 0.7615, 
noisyNet noise sample is [array([-1.1539267], dtype=float32), -0.21741407]. 
=============================================
[2019-04-04 00:02:21,800] A3C_AGENT_WORKER-Thread-2 INFO:Local step 92500, global step 1478612: loss 11.3040
[2019-04-04 00:02:21,801] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 92500, global step 1478612: learning rate 0.0001
[2019-04-04 00:02:22,167] A3C_AGENT_WORKER-Thread-7 INFO:Local step 92500, global step 1478709: loss 11.3046
[2019-04-04 00:02:22,168] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 92500, global step 1478709: learning rate 0.0001
[2019-04-04 00:02:26,337] A3C_AGENT_WORKER-Thread-8 INFO:Local step 92500, global step 1479904: loss 11.6643
[2019-04-04 00:02:26,352] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 92500, global step 1479904: learning rate 0.0001
[2019-04-04 00:02:26,357] A3C_AGENT_WORKER-Thread-6 INFO:Local step 92500, global step 1479912: loss 11.6336
[2019-04-04 00:02:26,357] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 92500, global step 1479912: learning rate 0.0001
[2019-04-04 00:02:26,564] A3C_AGENT_WORKER-Thread-14 INFO:Local step 92500, global step 1479959: loss 11.5620
[2019-04-04 00:02:26,566] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 92500, global step 1479959: learning rate 0.0001
[2019-04-04 00:02:27,765] A3C_AGENT_WORKER-Thread-19 INFO:Local step 93000, global step 1480326: loss 1.5999
[2019-04-04 00:02:27,766] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 93000, global step 1480326: learning rate 0.0001
[2019-04-04 00:02:28,312] A3C_AGENT_WORKER-Thread-4 INFO:Local step 92500, global step 1480506: loss 11.4741
[2019-04-04 00:02:28,312] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 92500, global step 1480506: learning rate 0.0001
[2019-04-04 00:02:28,458] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.0911135e-16 9.8129043e-09 4.3340965e-14 2.8834535e-11 5.8639771e-10
 3.6546541e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 00:02:28,484] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2481
[2019-04-04 00:02:28,505] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 72.5, 0.0, 0.0, 26.0, 25.18660931636487, 0.3679650803421738, 0.0, 1.0, 36216.78423529637], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4599000.0000, 
sim time next is 4599600.0000, 
raw observation next is [-2.4, 73.0, 0.0, 0.0, 26.0, 25.15105987524018, 0.3702678520645696, 0.0, 1.0, 36212.1703024438], 
processed observation next is [1.0, 0.21739130434782608, 0.39612188365650974, 0.73, 0.0, 0.0, 0.6666666666666666, 0.5959216562700149, 0.6234226173548566, 0.0, 1.0, 0.17243890620211336], 
reward next is 0.8276, 
noisyNet noise sample is [array([0.36745325], dtype=float32), 0.43181112]. 
=============================================
[2019-04-04 00:02:29,526] A3C_AGENT_WORKER-Thread-5 INFO:Local step 92500, global step 1480892: loss 11.4521
[2019-04-04 00:02:29,527] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 92500, global step 1480892: learning rate 0.0001
[2019-04-04 00:02:30,892] A3C_AGENT_WORKER-Thread-3 INFO:Local step 92500, global step 1481290: loss 11.7152
[2019-04-04 00:02:30,892] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 92500, global step 1481290: learning rate 0.0001
[2019-04-04 00:02:31,147] A3C_AGENT_WORKER-Thread-16 INFO:Local step 92500, global step 1481360: loss 11.6390
[2019-04-04 00:02:31,147] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 92500, global step 1481360: learning rate 0.0001
[2019-04-04 00:02:33,227] A3C_AGENT_WORKER-Thread-10 INFO:Local step 92500, global step 1481917: loss 11.6916
[2019-04-04 00:02:33,228] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 92500, global step 1481917: learning rate 0.0001
[2019-04-04 00:02:33,711] A3C_AGENT_WORKER-Thread-18 INFO:Local step 92500, global step 1482036: loss 11.8482
[2019-04-04 00:02:33,712] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 92500, global step 1482036: learning rate 0.0001
[2019-04-04 00:02:34,893] A3C_AGENT_WORKER-Thread-9 INFO:Local step 92500, global step 1482302: loss 11.6949
[2019-04-04 00:02:34,893] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 92500, global step 1482302: learning rate 0.0001
[2019-04-04 00:02:36,468] A3C_AGENT_WORKER-Thread-15 INFO:Local step 92500, global step 1482758: loss 11.8595
[2019-04-04 00:02:36,475] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 92500, global step 1482758: learning rate 0.0001
[2019-04-04 00:02:36,742] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.0817589e-16 2.3140572e-08 5.7906757e-14 1.0590858e-10 2.6599425e-10
 3.0726107e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 00:02:36,742] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2839
[2019-04-04 00:02:36,786] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.775557561562891e-17, 64.33333333333333, 0.0, 0.0, 26.0, 25.40221222060823, 0.4462014255884449, 0.0, 1.0, 48201.30963978072], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4585200.0000, 
sim time next is 4585800.0000, 
raw observation next is [-0.1, 64.66666666666667, 0.0, 0.0, 26.0, 25.44804225992674, 0.4456283725215116, 0.0, 1.0, 18759.26139212958], 
processed observation next is [1.0, 0.043478260869565216, 0.4598337950138504, 0.6466666666666667, 0.0, 0.0, 0.6666666666666666, 0.6206701883272284, 0.6485427908405038, 0.0, 1.0, 0.08932981615299801], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.31283844], dtype=float32), 1.2604485]. 
=============================================
[2019-04-04 00:02:36,876] A3C_AGENT_WORKER-Thread-20 INFO:Local step 92500, global step 1482885: loss 11.7249
[2019-04-04 00:02:36,876] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 92500, global step 1482885: learning rate 0.0001
[2019-04-04 00:02:36,968] A3C_AGENT_WORKER-Thread-17 INFO:Local step 93000, global step 1482919: loss 1.2424
[2019-04-04 00:02:36,969] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 93000, global step 1482919: learning rate 0.0001
[2019-04-04 00:02:43,850] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.7734224e-17 3.4289538e-09 4.8833953e-14 5.9134954e-11 1.4554503e-10
 7.4578652e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 00:02:43,869] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8959
[2019-04-04 00:02:43,880] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 25.52991068543052, 0.519044465696218, 0.0, 1.0, 18747.82384191774], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4668600.0000, 
sim time next is 4669200.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 25.60812298805486, 0.5233839428619577, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.518005540166205, 0.52, 0.0, 0.0, 0.6666666666666666, 0.6340102490045716, 0.6744613142873193, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.9666915], dtype=float32), -2.418173]. 
=============================================
[2019-04-04 00:02:50,076] A3C_AGENT_WORKER-Thread-2 INFO:Local step 93000, global step 1486828: loss 1.2069
[2019-04-04 00:02:50,078] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 93000, global step 1486828: learning rate 0.0001
[2019-04-04 00:02:50,597] A3C_AGENT_WORKER-Thread-7 INFO:Local step 93000, global step 1486951: loss 1.1444
[2019-04-04 00:02:50,598] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 93000, global step 1486951: learning rate 0.0001
[2019-04-04 00:02:53,397] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:02:53,397] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:02:53,400] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run12
[2019-04-04 00:02:53,790] A3C_AGENT_WORKER-Thread-6 INFO:Local step 93000, global step 1487891: loss 1.2803
[2019-04-04 00:02:53,790] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 93000, global step 1487891: learning rate 0.0001
[2019-04-04 00:02:53,799] A3C_AGENT_WORKER-Thread-14 INFO:Local step 93000, global step 1487892: loss 1.2960
[2019-04-04 00:02:53,800] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 93000, global step 1487892: learning rate 0.0001
[2019-04-04 00:02:54,099] A3C_AGENT_WORKER-Thread-8 INFO:Local step 93000, global step 1487957: loss 1.2185
[2019-04-04 00:02:54,133] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 93000, global step 1487957: learning rate 0.0001
[2019-04-04 00:02:54,181] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.3637621e-16 7.2638350e-08 3.2383272e-14 2.2502845e-10 1.4586552e-09
 4.0981893e-15 9.9999988e-01], sum to 1.0000
[2019-04-04 00:02:54,182] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3667
[2019-04-04 00:02:54,239] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.333333333333333, 42.0, 162.0, 795.6666666666667, 26.0, 25.07793484478888, 0.4279364307060512, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4796400.0000, 
sim time next is 4797000.0000, 
raw observation next is [1.5, 41.5, 170.0, 787.0, 26.0, 25.08925776170359, 0.4318014553906663, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.5041551246537397, 0.415, 0.5666666666666667, 0.8696132596685083, 0.6666666666666666, 0.5907714801419658, 0.6439338184635554, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.17827003], dtype=float32), -0.2564188]. 
=============================================
[2019-04-04 00:02:54,248] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[81.11507 ]
 [80.99296 ]
 [80.843834]
 [80.63419 ]
 [80.30395 ]], R is [[81.41173553]
 [81.5976181 ]
 [81.7816391 ]
 [81.91928101]
 [82.00170898]].
[2019-04-04 00:02:56,370] A3C_AGENT_WORKER-Thread-4 INFO:Local step 93000, global step 1488609: loss 1.1183
[2019-04-04 00:02:56,371] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 93000, global step 1488609: learning rate 0.0001
[2019-04-04 00:02:57,217] A3C_AGENT_WORKER-Thread-5 INFO:Local step 93000, global step 1488864: loss 1.0769
[2019-04-04 00:02:57,221] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 93000, global step 1488864: learning rate 0.0001
[2019-04-04 00:02:58,230] A3C_AGENT_WORKER-Thread-16 INFO:Local step 93000, global step 1489083: loss 0.9702
[2019-04-04 00:02:58,234] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 93000, global step 1489083: learning rate 0.0001
[2019-04-04 00:02:58,373] A3C_AGENT_WORKER-Thread-3 INFO:Local step 93000, global step 1489121: loss 0.8926
[2019-04-04 00:02:58,374] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 93000, global step 1489121: learning rate 0.0001
[2019-04-04 00:03:00,277] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.4929990e-15 8.5482192e-08 7.9836589e-14 1.8590297e-10 5.3059940e-10
 2.3906624e-15 9.9999988e-01], sum to 1.0000
[2019-04-04 00:03:00,277] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.2954
[2019-04-04 00:03:00,286] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 45.0, 163.0, 422.0, 26.0, 25.10344058485146, 0.3766864098369182, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4894200.0000, 
sim time next is 4894800.0000, 
raw observation next is [3.0, 45.0, 152.8333333333333, 404.5, 26.0, 25.11527982039378, 0.375279513956243, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.5457063711911359, 0.45, 0.5094444444444443, 0.44696132596685084, 0.6666666666666666, 0.5929399850328151, 0.6250931713187476, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2523997], dtype=float32), 0.7236934]. 
=============================================
[2019-04-04 00:03:00,315] A3C_AGENT_WORKER-Thread-18 INFO:Local step 93000, global step 1489688: loss 0.8334
[2019-04-04 00:03:00,316] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 93000, global step 1489688: learning rate 0.0001
[2019-04-04 00:03:01,138] A3C_AGENT_WORKER-Thread-10 INFO:Local step 93000, global step 1489919: loss 0.8887
[2019-04-04 00:03:01,139] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 93000, global step 1489919: learning rate 0.0001
[2019-04-04 00:03:01,189] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.6633130e-16 2.8886213e-08 6.4333537e-15 1.1739787e-11 1.4465333e-10
 1.3112049e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 00:03:01,189] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1221
[2019-04-04 00:03:01,259] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.333333333333333, 67.0, 152.0, 290.6666666666667, 26.0, 25.28709075522744, 0.3763758335705075, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4869600.0000, 
sim time next is 4870200.0000, 
raw observation next is [-3.166666666666667, 66.0, 163.0, 268.3333333333333, 26.0, 25.50151193662753, 0.3888737652850258, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.3748845798707295, 0.66, 0.5433333333333333, 0.29650092081031304, 0.6666666666666666, 0.6251259947189608, 0.6296245884283419, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.93998015], dtype=float32), -0.8342173]. 
=============================================
[2019-04-04 00:03:01,323] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.6571238e-16 3.2368799e-08 1.7354292e-14 4.3424028e-11 2.8567843e-10
 1.7385823e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 00:03:01,332] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6164
[2019-04-04 00:03:01,357] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 37.33333333333334, 0.0, 0.0, 26.0, 25.52817468101323, 0.3826905681541129, 0.0, 1.0, 18748.12911374942], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4912800.0000, 
sim time next is 4913400.0000, 
raw observation next is [1.0, 36.66666666666666, 0.0, 0.0, 26.0, 25.51404348507529, 0.3759304856843562, 0.0, 1.0, 26619.79313274628], 
processed observation next is [0.0, 0.8695652173913043, 0.4903047091412743, 0.3666666666666666, 0.0, 0.0, 0.6666666666666666, 0.6261702904229409, 0.6253101618947854, 0.0, 1.0, 0.12676091967974418], 
reward next is 0.8732, 
noisyNet noise sample is [array([-1.1945094], dtype=float32), -1.8862158]. 
=============================================
[2019-04-04 00:03:01,697] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:03:01,698] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:03:01,703] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run12
[2019-04-04 00:03:01,763] A3C_AGENT_WORKER-Thread-9 INFO:Local step 93000, global step 1490098: loss 0.7603
[2019-04-04 00:03:01,764] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 93000, global step 1490098: learning rate 0.0001
[2019-04-04 00:03:03,285] A3C_AGENT_WORKER-Thread-20 INFO:Local step 93000, global step 1490525: loss 0.8486
[2019-04-04 00:03:03,329] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 93000, global step 1490525: learning rate 0.0001
[2019-04-04 00:03:03,540] A3C_AGENT_WORKER-Thread-15 INFO:Local step 93000, global step 1490599: loss 0.7939
[2019-04-04 00:03:03,542] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 93000, global step 1490599: learning rate 0.0001
[2019-04-04 00:03:10,125] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0649615e-15 1.4527981e-07 4.5547770e-14 3.2256822e-11 4.4763093e-10
 1.7987935e-15 9.9999988e-01], sum to 1.0000
[2019-04-04 00:03:10,129] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6100
[2019-04-04 00:03:10,141] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.7, 93.0, 0.0, 0.0, 26.0, 21.12870018126839, -0.6022778255115232, 0.0, 1.0, 40511.75665130172], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 19200.0000, 
sim time next is 19800.0000, 
raw observation next is [7.7, 93.0, 0.0, 0.0, 26.0, 21.15439888979542, -0.5988358517197008, 0.0, 1.0, 40451.06442776605], 
processed observation next is [0.0, 0.21739130434782608, 0.6759002770083103, 0.93, 0.0, 0.0, 0.6666666666666666, 0.26286657414961834, 0.3003880494267664, 0.0, 1.0, 0.19262411632269547], 
reward next is 0.8074, 
noisyNet noise sample is [array([-0.5130749], dtype=float32), -0.79326606]. 
=============================================
[2019-04-04 00:03:15,133] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:03:15,133] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:03:15,137] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run12
[2019-04-04 00:03:15,162] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:03:15,163] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:03:15,177] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run12
[2019-04-04 00:03:17,930] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:03:17,930] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:03:17,938] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run12
[2019-04-04 00:03:18,381] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:03:18,382] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:03:18,402] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run12
[2019-04-04 00:03:18,713] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:03:18,713] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:03:18,721] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run12
[2019-04-04 00:03:21,018] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.0559948e-15 1.2853954e-07 5.9763631e-13 2.1544155e-10 4.6144930e-10
 2.3893780e-14 9.9999988e-01], sum to 1.0000
[2019-04-04 00:03:21,018] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6417
[2019-04-04 00:03:21,039] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.87181863882638, -0.2035249782267531, 0.0, 1.0, 44739.83403770169], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 191400.0000, 
sim time next is 192000.0000, 
raw observation next is [-8.900000000000002, 78.0, 0.0, 0.0, 26.0, 22.88579223529666, -0.2069961487408042, 0.0, 1.0, 44806.42913440012], 
processed observation next is [1.0, 0.21739130434782608, 0.2160664819944598, 0.78, 0.0, 0.0, 0.6666666666666666, 0.40714935294138827, 0.43100128375306523, 0.0, 1.0, 0.21336394825904817], 
reward next is 0.7866, 
noisyNet noise sample is [array([1.2638614], dtype=float32), 0.6019541]. 
=============================================
[2019-04-04 00:03:21,113] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[74.96052]
 [75.01456]
 [75.07742]
 [75.14546]
 [75.22395]], R is [[74.95582581]
 [74.9932251 ]
 [75.03064728]
 [75.06807709]
 [75.10552216]].
[2019-04-04 00:03:21,276] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:03:21,276] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:03:21,280] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run12
[2019-04-04 00:03:21,932] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:03:21,932] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:03:21,935] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run12
[2019-04-04 00:03:23,285] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:03:23,286] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:03:23,305] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run12
[2019-04-04 00:03:23,609] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:03:23,609] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:03:23,615] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run12
[2019-04-04 00:03:25,396] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:03:25,396] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:03:25,399] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run12
[2019-04-04 00:03:26,049] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:03:26,049] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:03:26,052] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run12
[2019-04-04 00:03:26,717] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:03:26,717] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:03:26,720] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run12
[2019-04-04 00:03:28,043] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:03:28,043] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:03:28,065] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:03:28,065] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:03:28,067] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run12
[2019-04-04 00:03:28,121] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run12
[2019-04-04 00:03:31,689] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.1729313e-06 9.5256455e-03 7.6712904e-06 2.5776651e-04 1.2046896e-04
 2.5789093e-06 9.9008358e-01], sum to 1.0000
[2019-04-04 00:03:31,689] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7086
[2019-04-04 00:03:31,719] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.6, 95.5, 0.0, 0.0, 25.0, 18.90128101974305, -0.9919784346224004, 0.0, 1.0, 109105.3263111149], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1800.0000, 
sim time next is 2400.0000, 
raw observation next is [4.8, 95.66666666666667, 0.0, 0.0, 26.0, 19.20649486772959, -0.951296632291249, 0.0, 1.0, 66411.83679401851], 
processed observation next is [0.0, 0.0, 0.5955678670360112, 0.9566666666666667, 0.0, 0.0, 0.6666666666666666, 0.10054123897746574, 0.18290112256958369, 0.0, 1.0, 0.3162468418762786], 
reward next is 0.6838, 
noisyNet noise sample is [array([-1.7188234], dtype=float32), 1.4468083]. 
=============================================
[2019-04-04 00:03:35,771] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.6392377e-16 7.8510972e-08 3.7668426e-14 2.3949755e-11 7.1638091e-11
 9.4468843e-16 9.9999988e-01], sum to 1.0000
[2019-04-04 00:03:35,771] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0313
[2019-04-04 00:03:35,825] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.3, 80.33333333333334, 0.0, 0.0, 26.0, 23.96191895123049, 0.0548743181485688, 0.0, 1.0, 43544.37549899466], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 100200.0000, 
sim time next is 100800.0000, 
raw observation next is [-3.4, 79.0, 0.0, 0.0, 26.0, 23.91509121279585, 0.04971586908928834, 0.0, 1.0, 43613.45514814612], 
processed observation next is [1.0, 0.17391304347826086, 0.368421052631579, 0.79, 0.0, 0.0, 0.6666666666666666, 0.49292426773298753, 0.516571956363096, 0.0, 1.0, 0.20768311975307674], 
reward next is 0.7923, 
noisyNet noise sample is [array([0.10537965], dtype=float32), 0.41870415]. 
=============================================
[2019-04-04 00:03:47,144] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-04 00:03:47,147] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 00:03:47,148] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:03:47,153] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 00:03:47,160] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 00:03:47,160] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:03:47,166] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:03:47,173] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run16
[2019-04-04 00:03:47,252] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run16
[2019-04-04 00:03:47,333] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run16
[2019-04-04 00:04:38,483] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.31118938], dtype=float32), 0.29801285]
[2019-04-04 00:04:38,483] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [1.016666666666667, 76.33333333333333, 28.66666666666667, 249.0, 26.0, 24.94609802686933, 0.3453216867361095, 0.0, 1.0, 49264.43366438947]
[2019-04-04 00:04:38,483] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 00:04:38,484] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [2.0795965e-15 4.1298506e-08 7.7405964e-14 1.9532773e-10 4.9539445e-10
 4.0389186e-15 1.0000000e+00], sampled 0.2923635283201491
[2019-04-04 00:05:53,901] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 00:06:21,455] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.7639 263389580.4542 1552.0485
[2019-04-04 00:06:24,815] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 00:06:25,844] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 1500000, evaluation results [1500000.0, 7241.763902599205, 263389580.45416775, 1552.0485098418105, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 00:06:30,750] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.8569690e-16 3.2236316e-09 6.1119479e-15 5.0877757e-11 1.5108821e-10
 1.0653389e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 00:06:30,776] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8066
[2019-04-04 00:06:30,889] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.9, 66.33333333333334, 32.66666666666666, 9.999999999999998, 26.0, 25.97936080298592, 0.4363945538992374, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 145200.0000, 
sim time next is 145800.0000, 
raw observation next is [-7.0, 67.5, 27.0, 3.0, 26.0, 25.96476521913496, 0.2760877491028652, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.2686980609418283, 0.675, 0.09, 0.0033149171270718232, 0.6666666666666666, 0.6637304349279134, 0.592029249700955, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.730241], dtype=float32), -1.1791098]. 
=============================================
[2019-04-04 00:06:53,077] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.6781246e-15 7.8759177e-07 9.1210665e-13 9.4786357e-10 1.7842411e-09
 1.9588455e-14 9.9999917e-01], sum to 1.0000
[2019-04-04 00:06:53,077] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6516
[2019-04-04 00:06:53,119] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-16.2, 78.0, 0.0, 0.0, 26.0, 21.7826081811242, -0.4751985559176858, 0.0, 1.0, 49027.46503632655], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 369000.0000, 
sim time next is 369600.0000, 
raw observation next is [-16.2, 78.0, 0.0, 0.0, 26.0, 21.70973333539626, -0.493362995174224, 0.0, 1.0, 49150.14669542631], 
processed observation next is [1.0, 0.2608695652173913, 0.013850415512465375, 0.78, 0.0, 0.0, 0.6666666666666666, 0.3091444446163549, 0.33554566827525867, 0.0, 1.0, 0.23404831759726813], 
reward next is 0.7660, 
noisyNet noise sample is [array([1.4224788], dtype=float32), 0.83872354]. 
=============================================
[2019-04-04 00:06:56,607] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.69079595e-16 1.00192366e-09 4.33025223e-15 2.72903054e-11
 1.12339416e-10 1.48001352e-16 1.00000000e+00], sum to 1.0000
[2019-04-04 00:06:56,607] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5192
[2019-04-04 00:06:56,678] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.0, 36.66666666666666, 0.0, 0.0, 26.0, 25.86782381641483, 0.3854921802980939, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 407400.0000, 
sim time next is 408000.0000, 
raw observation next is [-9.100000000000001, 37.33333333333334, 0.0, 0.0, 26.0, 25.85811952447198, 0.2129631684409733, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.21052631578947364, 0.3733333333333334, 0.0, 0.0, 0.6666666666666666, 0.6548432937059984, 0.5709877228136578, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.17456], dtype=float32), 0.2274768]. 
=============================================
[2019-04-04 00:06:56,682] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[73.81744]
 [75.44282]
 [76.09391]
 [76.51142]
 [76.96307]], R is [[73.68232727]
 [73.94550323]
 [74.20604706]
 [74.46398926]
 [74.71935272]].
[2019-04-04 00:07:20,003] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.7126469e-17 4.6297941e-08 8.0794593e-15 2.2018569e-11 1.2314279e-10
 6.6456744e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 00:07:20,003] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6966
[2019-04-04 00:07:20,040] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 69.0, 0.0, 0.0, 26.0, 24.8106457976279, 0.182789050022776, 0.0, 1.0, 42057.22227401144], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 683400.0000, 
sim time next is 684000.0000, 
raw observation next is [-3.4, 69.0, 0.0, 0.0, 26.0, 24.82335430505099, 0.1772772369142697, 0.0, 1.0, 41969.42287775488], 
processed observation next is [0.0, 0.9565217391304348, 0.368421052631579, 0.69, 0.0, 0.0, 0.6666666666666666, 0.5686128587542493, 0.5590924123047566, 0.0, 1.0, 0.1998543946559756], 
reward next is 0.8001, 
noisyNet noise sample is [array([0.60074455], dtype=float32), 0.017265454]. 
=============================================
[2019-04-04 00:07:20,064] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[80.17736 ]
 [80.20516 ]
 [80.20519 ]
 [80.22924 ]
 [80.294815]], R is [[80.16996765]
 [80.16799927]
 [80.16564941]
 [80.16300201]
 [80.16010284]].
[2019-04-04 00:07:27,273] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [4.4241014e-17 3.5119317e-08 6.5767526e-15 4.6171060e-11 6.2790662e-11
 2.9114206e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 00:07:27,274] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.8265
[2019-04-04 00:07:27,316] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 76.0, 0.0, 0.0, 26.0, 24.32036863187057, 0.03489167738768649, 0.0, 1.0, 41789.22055217635], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 713400.0000, 
sim time next is 714000.0000, 
raw observation next is [-2.3, 76.0, 0.0, 0.0, 26.0, 24.27379970172349, 0.02651654320908862, 0.0, 1.0, 41867.59078610584], 
processed observation next is [1.0, 0.2608695652173913, 0.3988919667590028, 0.76, 0.0, 0.0, 0.6666666666666666, 0.5228166418102909, 0.5088388477363629, 0.0, 1.0, 0.19936947993383733], 
reward next is 0.8006, 
noisyNet noise sample is [array([1.936059], dtype=float32), 0.72414684]. 
=============================================
[2019-04-04 00:07:27,376] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[83.03369 ]
 [83.0264  ]
 [83.00485 ]
 [82.987656]
 [82.96976 ]], R is [[82.98558807]
 [82.9567337 ]
 [82.92841339]
 [82.90052795]
 [82.87303162]].
[2019-04-04 00:07:42,180] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.5677213e-17 1.9490924e-08 6.2842535e-15 1.2327009e-11 9.4983249e-12
 6.3684218e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:07:42,180] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5399
[2019-04-04 00:07:42,221] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 24.43789531069389, 0.1560709328318241, 0.0, 1.0, 38573.8085993506], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 888000.0000, 
sim time next is 888600.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 24.52043046833471, 0.1619084406770875, 0.0, 1.0, 38492.44643714612], 
processed observation next is [1.0, 0.2608695652173913, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.543369205694559, 0.5539694802256958, 0.0, 1.0, 0.1832973639864101], 
reward next is 0.8167, 
noisyNet noise sample is [array([-1.1052166], dtype=float32), 1.7586621]. 
=============================================
[2019-04-04 00:07:42,684] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.25473968e-16 1.09386576e-07 2.83129368e-14 4.41113812e-11
 2.28880567e-10 4.55563277e-16 9.99999881e-01], sum to 1.0000
[2019-04-04 00:07:42,685] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9612
[2019-04-04 00:07:42,803] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.700000000000001, 68.33333333333334, 0.0, 0.0, 26.0, 23.4486376638184, -0.08720836730051527, 0.0, 1.0, 42002.06598549861], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 803400.0000, 
sim time next is 804000.0000, 
raw observation next is [-6.700000000000001, 69.66666666666667, 0.0, 0.0, 26.0, 23.43240226605372, -0.02009259462278101, 1.0, 1.0, 202414.3956292189], 
processed observation next is [1.0, 0.30434782608695654, 0.2770083102493075, 0.6966666666666668, 0.0, 0.0, 0.6666666666666666, 0.4527001888378101, 0.493302468459073, 1.0, 1.0, 0.9638780744248518], 
reward next is 0.0361, 
noisyNet noise sample is [array([-1.2276582], dtype=float32), 0.97770095]. 
=============================================
[2019-04-04 00:07:42,848] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[79.50698 ]
 [79.502045]
 [79.51791 ]
 [79.53629 ]
 [79.55699 ]], R is [[85.08018494]
 [85.02937317]
 [84.97894287]
 [84.92896271]
 [84.87934113]].
[2019-04-04 00:08:08,277] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.3210072e-20 1.6583954e-10 5.0230960e-18 2.2795749e-13 1.1464084e-13
 1.8713091e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:08:08,302] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3446
[2019-04-04 00:08:08,337] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [18.46666666666667, 55.33333333333334, 172.6666666666667, 52.83333333333332, 26.0, 27.21294778288656, 0.9252396672923503, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1084800.0000, 
sim time next is 1085400.0000, 
raw observation next is [18.55, 55.0, 171.0, 0.0, 26.0, 27.36178897806007, 0.9432855010360114, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.976454293628809, 0.55, 0.57, 0.0, 0.6666666666666666, 0.7801490815050057, 0.8144285003453371, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.419776], dtype=float32), 1.0871507]. 
=============================================
[2019-04-04 00:08:10,048] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.84269656e-19 1.16114111e-10 1.05582326e-16 4.17120358e-13
 2.04256881e-12 3.19797977e-19 1.00000000e+00], sum to 1.0000
[2019-04-04 00:08:10,049] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4393
[2019-04-04 00:08:10,061] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.266666666666667, 91.0, 0.0, 0.0, 26.0, 25.57279945081726, 0.5418755941620879, 0.0, 1.0, 18741.96032006044], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1460400.0000, 
sim time next is 1461000.0000, 
raw observation next is [1.183333333333333, 91.5, 0.0, 0.0, 26.0, 25.52563006730549, 0.5417824215380899, 0.0, 1.0, 40198.02822588547], 
processed observation next is [1.0, 0.9130434782608695, 0.49538319482917825, 0.915, 0.0, 0.0, 0.6666666666666666, 0.6271358389421241, 0.6805941405126966, 0.0, 1.0, 0.19141918202802605], 
reward next is 0.8086, 
noisyNet noise sample is [array([0.19587837], dtype=float32), -0.97901547]. 
=============================================
[2019-04-04 00:08:10,127] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[87.15632]
 [87.19388]
 [87.29451]
 [87.35278]
 [87.45079]], R is [[87.2082901 ]
 [87.24695587]
 [87.37448883]
 [87.50074768]
 [87.62574005]].
[2019-04-04 00:08:19,311] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.7558104e-17 4.5560252e-08 8.8462708e-16 5.4116932e-12 1.3676648e-11
 3.6667715e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:08:19,317] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9160
[2019-04-04 00:08:19,326] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [17.75, 66.0, 130.0, 0.0, 26.0, 25.32669251546525, 0.5227851515011661, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1161000.0000, 
sim time next is 1161600.0000, 
raw observation next is [17.93333333333333, 65.66666666666666, 135.0, 0.0, 26.0, 25.27444320487836, 0.5228058507681536, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.9593721144967682, 0.6566666666666666, 0.45, 0.0, 0.6666666666666666, 0.60620360040653, 0.6742686169227179, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4565624], dtype=float32), 0.018225553]. 
=============================================
[2019-04-04 00:08:50,035] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.3916625e-18 1.3379420e-10 7.3670431e-16 5.9254715e-12 2.0735358e-12
 2.4342978e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:08:50,035] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4247
[2019-04-04 00:08:50,113] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.35, 90.5, 0.0, 0.0, 26.0, 24.98537416015688, 0.4961658579151891, 0.0, 1.0, 128543.7826799539], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1456200.0000, 
sim time next is 1456800.0000, 
raw observation next is [1.433333333333333, 90.0, 0.0, 0.0, 26.0, 25.10493229092007, 0.5231709894785996, 0.0, 1.0, 71120.4492216831], 
processed observation next is [1.0, 0.8695652173913043, 0.502308402585411, 0.9, 0.0, 0.0, 0.6666666666666666, 0.5920776909100057, 0.6743903298262, 0.0, 1.0, 0.33866880581753855], 
reward next is 0.6613, 
noisyNet noise sample is [array([-0.1721741], dtype=float32), -1.2870467]. 
=============================================
[2019-04-04 00:08:58,278] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.4646311e-20 1.2790081e-10 1.2017510e-17 3.8509672e-14 3.6376956e-13
 2.1910060e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:08:58,278] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9305
[2019-04-04 00:08:58,309] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.35, 54.0, 87.0, 28.0, 26.0, 27.24863838788803, 0.7000800744772828, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1528200.0000, 
sim time next is 1528800.0000, 
raw observation next is [11.06666666666667, 55.33333333333333, 72.83333333333333, 24.33333333333334, 26.0, 26.7128197582625, 0.7404489738739426, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.7691597414589106, 0.5533333333333332, 0.24277777777777776, 0.026887661141804794, 0.6666666666666666, 0.7260683131885418, 0.7468163246246475, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.95203954], dtype=float32), 1.0450879]. 
=============================================
[2019-04-04 00:09:13,042] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1804867e-17 5.5667471e-10 7.0356907e-16 1.8646400e-11 5.6290094e-12
 3.8432987e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:09:13,042] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6609
[2019-04-04 00:09:13,158] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.566666666666666, 63.0, 132.8333333333333, 0.0, 26.0, 25.66003501195884, 0.3377456549579564, 1.0, 1.0, 23106.17931630252], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1950000.0000, 
sim time next is 1950600.0000, 
raw observation next is [-3.483333333333333, 62.5, 128.6666666666667, 0.0, 26.0, 25.69665176071489, 0.3450545576041857, 1.0, 1.0, 23003.9806099329], 
processed observation next is [1.0, 0.5652173913043478, 0.3661126500461681, 0.625, 0.42888888888888904, 0.0, 0.6666666666666666, 0.6413876467262408, 0.6150181858680619, 1.0, 1.0, 0.10954276480920429], 
reward next is 0.8905, 
noisyNet noise sample is [array([0.767927], dtype=float32), -1.0290095]. 
=============================================
[2019-04-04 00:09:16,830] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.2330885e-19 7.1653716e-10 3.0138805e-17 5.5745691e-13 7.4136552e-13
 1.3959973e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:09:16,832] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1516
[2019-04-04 00:09:16,982] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 88.0, 0.0, 0.0, 26.0, 23.44710134580161, 0.3057654375826873, 1.0, 1.0, 198046.2192543068], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1709400.0000, 
sim time next is 1710000.0000, 
raw observation next is [1.1, 88.0, 0.0, 0.0, 26.0, 24.05002153471001, 0.4185959065501003, 1.0, 1.0, 199729.2167776584], 
processed observation next is [1.0, 0.8260869565217391, 0.49307479224376743, 0.88, 0.0, 0.0, 0.6666666666666666, 0.504168461225834, 0.6395319688500335, 1.0, 1.0, 0.9510915084650401], 
reward next is 0.0489, 
noisyNet noise sample is [array([1.4671148], dtype=float32), 0.1773075]. 
=============================================
[2019-04-04 00:09:16,986] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[86.22107]
 [85.81779]
 [85.4407 ]
 [85.56849]
 [85.58578]], R is [[86.03730774]
 [85.2338562 ]
 [84.44568634]
 [84.49488068]
 [84.56098175]].
[2019-04-04 00:09:21,086] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1620351e-15 3.1410372e-07 1.6467109e-13 1.5761917e-10 3.5322750e-10
 1.1916546e-14 9.9999964e-01], sum to 1.0000
[2019-04-04 00:09:21,087] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3251
[2019-04-04 00:09:21,105] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.283333333333333, 87.0, 0.0, 0.0, 26.0, 24.88535198898978, 0.3384855960510033, 0.0, 1.0, 43869.36444674501], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1750200.0000, 
sim time next is 1750800.0000, 
raw observation next is [-1.366666666666667, 87.0, 0.0, 0.0, 26.0, 24.85728994280371, 0.3330268283482395, 0.0, 1.0, 43916.59201584577], 
processed observation next is [0.0, 0.2608695652173913, 0.42474607571560485, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5714408285669759, 0.6110089427827465, 0.0, 1.0, 0.2091266286468846], 
reward next is 0.7909, 
noisyNet noise sample is [array([0.93565905], dtype=float32), -0.44553334]. 
=============================================
[2019-04-04 00:09:24,042] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.3344455e-15 8.5320117e-08 1.2648865e-13 6.6947482e-11 1.6530739e-10
 3.1880834e-15 9.9999988e-01], sum to 1.0000
[2019-04-04 00:09:24,042] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1938
[2019-04-04 00:09:24,085] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 87.0, 0.0, 0.0, 26.0, 24.75683393378661, 0.3139666278372978, 0.0, 1.0, 44023.1710937966], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1753200.0000, 
sim time next is 1753800.0000, 
raw observation next is [-1.7, 87.0, 0.0, 0.0, 26.0, 24.75132803783238, 0.3095296845501519, 0.0, 1.0, 44030.00307459341], 
processed observation next is [0.0, 0.30434782608695654, 0.4155124653739613, 0.87, 0.0, 0.0, 0.6666666666666666, 0.562610669819365, 0.6031765615167173, 0.0, 1.0, 0.20966668130758767], 
reward next is 0.7903, 
noisyNet noise sample is [array([1.3112911], dtype=float32), -0.80271214]. 
=============================================
[2019-04-04 00:09:32,661] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.8182793e-16 6.7528532e-08 7.2509117e-14 7.9970829e-11 6.0184452e-10
 3.3835625e-15 9.9999988e-01], sum to 1.0000
[2019-04-04 00:09:32,661] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6770
[2019-04-04 00:09:32,734] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 86.0, 0.0, 0.0, 26.0, 24.99327819333553, 0.2849815177625476, 0.0, 1.0, 45857.23276615205], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1806600.0000, 
sim time next is 1807200.0000, 
raw observation next is [-5.0, 86.0, 0.0, 0.0, 26.0, 24.95225502935209, 0.2753514089930717, 0.0, 1.0, 45836.41650810099], 
processed observation next is [0.0, 0.9565217391304348, 0.32409972299168976, 0.86, 0.0, 0.0, 0.6666666666666666, 0.579354585779341, 0.5917838029976906, 0.0, 1.0, 0.21826865003857615], 
reward next is 0.7817, 
noisyNet noise sample is [array([-0.40121225], dtype=float32), 0.38824415]. 
=============================================
[2019-04-04 00:10:05,086] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.4106611e-16 2.3543524e-08 6.9426236e-14 2.9329757e-11 2.0933714e-10
 6.9544655e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 00:10:05,087] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6012
[2019-04-04 00:10:05,202] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.800000000000001, 78.16666666666667, 0.0, 0.0, 26.0, 23.77552735957714, 0.01076023100295946, 0.0, 1.0, 43331.41048358567], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2099400.0000, 
sim time next is 2100000.0000, 
raw observation next is [-6.9, 78.33333333333334, 0.0, 0.0, 26.0, 23.73381054300751, 0.09466526859594608, 1.0, 1.0, 202393.2886562294], 
processed observation next is [1.0, 0.30434782608695654, 0.27146814404432135, 0.7833333333333334, 0.0, 0.0, 0.6666666666666666, 0.4778175452506259, 0.531555089531982, 1.0, 1.0, 0.9637775650296637], 
reward next is 0.0362, 
noisyNet noise sample is [array([-1.1789551], dtype=float32), -1.0512029]. 
=============================================
[2019-04-04 00:10:05,262] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[75.83943 ]
 [75.933525]
 [76.04821 ]
 [76.17027 ]
 [76.29623 ]], R is [[80.9884491 ]
 [80.972229  ]
 [80.95565033]
 [80.93881226]
 [80.92182922]].
[2019-04-04 00:10:09,283] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.9745161e-17 3.5705565e-09 7.9359228e-15 2.3320252e-11 4.2565288e-11
 1.7418343e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 00:10:09,283] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8891
[2019-04-04 00:10:09,365] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.3, 82.0, 0.0, 0.0, 26.0, 24.44086010184872, 0.1984230516547663, 0.0, 1.0, 42429.08293234652], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2160000.0000, 
sim time next is 2160600.0000, 
raw observation next is [-7.3, 81.50000000000001, 0.0, 0.0, 26.0, 24.39418791201546, 0.1886713168485513, 0.0, 1.0, 42462.76034636118], 
processed observation next is [1.0, 0.0, 0.26038781163434904, 0.8150000000000002, 0.0, 0.0, 0.6666666666666666, 0.532848992667955, 0.5628904389495171, 0.0, 1.0, 0.20220362069695802], 
reward next is 0.7978, 
noisyNet noise sample is [array([-0.44450748], dtype=float32), -0.10336417]. 
=============================================
[2019-04-04 00:10:10,219] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [9.0879174e-18 2.0538817e-09 3.8213592e-16 4.2575661e-12 1.2378891e-11
 1.1220747e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:10:10,235] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.7487
[2019-04-04 00:10:10,273] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.916666666666667, 71.0, 112.0, 150.3333333333333, 26.0, 26.02138990067376, 0.4083472868590949, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2196600.0000, 
sim time next is 2197200.0000, 
raw observation next is [-4.833333333333334, 71.0, 114.5, 75.16666666666664, 26.0, 25.99344212866596, 0.3980989078212284, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.32871652816251157, 0.71, 0.38166666666666665, 0.08305709023941066, 0.6666666666666666, 0.6661201773888301, 0.6326996359404095, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.52693146], dtype=float32), -0.0055361856]. 
=============================================
[2019-04-04 00:10:16,097] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [7.5937649e-17 1.0201782e-08 7.5990172e-15 1.8480342e-11 5.2510715e-11
 1.3484006e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 00:10:16,098] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.8828
[2019-04-04 00:10:16,144] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.800000000000001, 76.16666666666667, 0.0, 0.0, 26.0, 24.3642465515727, 0.1604414110810506, 0.0, 1.0, 44138.60339142177], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2250600.0000, 
sim time next is 2251200.0000, 
raw observation next is [-6.9, 77.33333333333334, 0.0, 0.0, 26.0, 24.30406196161853, 0.1538236176686053, 0.0, 1.0, 44091.22152438267], 
processed observation next is [1.0, 0.043478260869565216, 0.27146814404432135, 0.7733333333333334, 0.0, 0.0, 0.6666666666666666, 0.5253384968015441, 0.5512745392228684, 0.0, 1.0, 0.2099581977351556], 
reward next is 0.7900, 
noisyNet noise sample is [array([1.1276875], dtype=float32), -0.18612626]. 
=============================================
[2019-04-04 00:10:24,944] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.9538683e-14 6.0526247e-07 9.0138422e-13 1.7429429e-09 4.0637333e-09
 1.1908596e-13 9.9999940e-01], sum to 1.0000
[2019-04-04 00:10:24,944] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9380
[2019-04-04 00:10:24,966] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.2, 60.5, 0.0, 0.0, 26.0, 23.11099747463686, -0.1810983650668982, 0.0, 1.0, 44035.53124332401], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2442600.0000, 
sim time next is 2443200.0000, 
raw observation next is [-9.3, 60.66666666666667, 0.0, 0.0, 26.0, 23.07434229935446, -0.1901171284715862, 0.0, 1.0, 44038.86235208395], 
processed observation next is [0.0, 0.2608695652173913, 0.20498614958448752, 0.6066666666666667, 0.0, 0.0, 0.6666666666666666, 0.42286185827953826, 0.4366276238428046, 0.0, 1.0, 0.2097088683432569], 
reward next is 0.7903, 
noisyNet noise sample is [array([0.787953], dtype=float32), 0.06747112]. 
=============================================
[2019-04-04 00:10:28,523] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [8.0900943e-16 1.4386330e-07 1.3804843e-13 1.3524201e-10 3.1383054e-10
 9.5166608e-15 9.9999988e-01], sum to 1.0000
[2019-04-04 00:10:28,524] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0420
[2019-04-04 00:10:28,568] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.466666666666667, 63.0, 0.0, 0.0, 26.0, 24.53399347836289, 0.1920901211838017, 0.0, 1.0, 39755.88521956332], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2344800.0000, 
sim time next is 2345400.0000, 
raw observation next is [-2.55, 63.5, 0.0, 0.0, 26.0, 24.52150073818959, 0.1860453205060347, 0.0, 1.0, 39862.47935494516], 
processed observation next is [0.0, 0.13043478260869565, 0.3919667590027701, 0.635, 0.0, 0.0, 0.6666666666666666, 0.5434583948491326, 0.5620151068353448, 0.0, 1.0, 0.1898213302616436], 
reward next is 0.8102, 
noisyNet noise sample is [array([-0.4408823], dtype=float32), 1.6218747]. 
=============================================
[2019-04-04 00:10:42,593] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [5.3743431e-16 4.2459618e-08 2.7735135e-15 2.4278520e-11 1.0181885e-10
 8.1966140e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 00:10:42,606] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.7450
[2019-04-04 00:10:42,638] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.7833333333333333, 35.5, 0.0, 0.0, 26.0, 25.13965045487438, 0.2398398535041896, 0.0, 1.0, 39944.98434834379], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2502600.0000, 
sim time next is 2503200.0000, 
raw observation next is [-0.9666666666666667, 36.0, 0.0, 0.0, 26.0, 25.13442353306431, 0.2362241604483756, 0.0, 1.0, 39826.3370697416], 
processed observation next is [0.0, 1.0, 0.43582640812557716, 0.36, 0.0, 0.0, 0.6666666666666666, 0.5945352944220259, 0.5787413868161252, 0.0, 1.0, 0.1896492241416267], 
reward next is 0.8104, 
noisyNet noise sample is [array([1.880729], dtype=float32), 1.0489624]. 
=============================================
[2019-04-04 00:10:42,803] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.12029498e-17 3.95338624e-08 1.63070084e-15 6.81462352e-12
 1.06857995e-10 5.54889696e-17 1.00000000e+00], sum to 1.0000
[2019-04-04 00:10:42,803] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7439
[2019-04-04 00:10:42,888] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.6, 28.0, 88.5, 838.5, 26.0, 24.94172266057019, 0.2738209550159147, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2466000.0000, 
sim time next is 2466600.0000, 
raw observation next is [1.7, 27.83333333333334, 88.0, 836.3333333333334, 26.0, 24.96081604033909, 0.2741734925257777, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.5096952908587258, 0.2783333333333334, 0.29333333333333333, 0.9241252302025783, 0.6666666666666666, 0.5800680033615908, 0.5913911641752593, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.13646135], dtype=float32), 0.92737365]. 
=============================================
[2019-04-04 00:10:43,040] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.9515142e-18 1.0999179e-08 3.5224312e-15 7.1146218e-12 1.0777638e-11
 3.4618299e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:10:43,040] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3196
[2019-04-04 00:10:43,117] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 71.0, 0.0, 0.0, 26.0, 24.88616492047424, 0.2471875592370374, 0.0, 1.0, 41686.42731971607], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2597400.0000, 
sim time next is 2598000.0000, 
raw observation next is [-5.0, 72.0, 0.0, 0.0, 26.0, 24.88408758334603, 0.2457724636409285, 0.0, 1.0, 41671.28803202201], 
processed observation next is [1.0, 0.043478260869565216, 0.32409972299168976, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5736739652788359, 0.5819241545469761, 0.0, 1.0, 0.1984347049143905], 
reward next is 0.8016, 
noisyNet noise sample is [array([0.64963835], dtype=float32), 0.07141999]. 
=============================================
[2019-04-04 00:10:43,127] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[83.14671]
 [83.05029]
 [82.99512]
 [82.94628]
 [82.95026]], R is [[83.19114685]
 [83.16072845]
 [83.13043213]
 [83.10020447]
 [83.07010651]].
[2019-04-04 00:10:54,211] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.6145323e-17 3.5607293e-09 1.0436584e-15 5.2346790e-12 1.0053358e-11
 2.3319156e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:10:54,211] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9090
[2019-04-04 00:10:54,250] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.566666666666666, 68.33333333333333, 102.8333333333333, 121.6666666666667, 26.0, 25.87453131451238, 0.3648260994315942, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2626800.0000, 
sim time next is 2627400.0000, 
raw observation next is [-5.283333333333333, 66.66666666666667, 112.6666666666667, 152.3333333333333, 26.0, 25.86916515470329, 0.3641132432547693, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.31625115420129274, 0.6666666666666667, 0.37555555555555564, 0.16832412523020251, 0.6666666666666666, 0.6557637628919407, 0.6213710810849231, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.82874286], dtype=float32), -0.4064972]. 
=============================================
[2019-04-04 00:10:55,553] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.2258084e-18 9.3488828e-10 5.8850208e-17 2.6534690e-12 1.0082695e-12
 9.2677574e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:10:55,553] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8417
[2019-04-04 00:10:55,609] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 47.0, 204.5, 179.0, 26.0, 24.92923088517456, 0.3889045023036898, 1.0, 1.0, 65585.86101070858], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2638800.0000, 
sim time next is 2639400.0000, 
raw observation next is [-0.4166666666666667, 46.33333333333334, 191.0, 189.6666666666667, 26.0, 25.46031412401397, 0.4455401773117213, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.45106186518928904, 0.46333333333333343, 0.6366666666666667, 0.20957642725598533, 0.6666666666666666, 0.6216928436678307, 0.6485133924372405, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2669621], dtype=float32), 0.86864287]. 
=============================================
[2019-04-04 00:11:08,559] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.0173495e-18 1.8444399e-10 5.3113852e-16 2.5323008e-12 6.6877515e-12
 3.6070720e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:11:08,559] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4625
[2019-04-04 00:11:08,588] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 50.0, 37.00000000000001, 320.3333333333334, 26.0, 25.90765175488876, 0.4586791579018502, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2739000.0000, 
sim time next is 2739600.0000, 
raw observation next is [-3.0, 50.0, 28.5, 253.5, 26.0, 25.59610119693251, 0.4543076756240847, 1.0, 1.0, 172573.9780704277], 
processed observation next is [1.0, 0.7391304347826086, 0.3795013850415513, 0.5, 0.095, 0.28011049723756903, 0.6666666666666666, 0.633008433077709, 0.6514358918746949, 1.0, 1.0, 0.8217808479544176], 
reward next is 0.1782, 
noisyNet noise sample is [array([0.37633073], dtype=float32), -1.4223207]. 
=============================================
[2019-04-04 00:11:11,485] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.5949863e-18 2.4683733e-10 9.9621154e-16 2.2817379e-12 9.6862162e-12
 1.7364728e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:11:11,485] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1612
[2019-04-04 00:11:11,505] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 44.0, 0.0, 0.0, 26.0, 25.15869245857756, 0.4052999204445781, 0.0, 1.0, 56724.37810128005], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2840400.0000, 
sim time next is 2841000.0000, 
raw observation next is [2.0, 44.00000000000001, 0.0, 0.0, 26.0, 25.25234987385802, 0.4173193977542015, 0.0, 1.0, 51553.42887649449], 
processed observation next is [1.0, 0.9130434782608695, 0.518005540166205, 0.44000000000000006, 0.0, 0.0, 0.6666666666666666, 0.6043624894881683, 0.6391064659180672, 0.0, 1.0, 0.24549251845949754], 
reward next is 0.7545, 
noisyNet noise sample is [array([-1.1744229], dtype=float32), -1.1844932]. 
=============================================
[2019-04-04 00:11:11,593] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[78.94281]
 [78.61594]
 [78.36103]
 [77.73135]
 [76.72757]], R is [[79.23988342]
 [79.17736816]
 [79.04493713]
 [78.7240448 ]
 [78.04633331]].
[2019-04-04 00:11:13,403] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.1948018e-19 3.0329214e-10 3.1564961e-17 1.2689083e-12 3.1821456e-13
 2.0611746e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:11:13,404] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2066
[2019-04-04 00:11:13,427] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.0, 100.0, 103.5, 696.5, 26.0, 26.64464233686327, 0.6339443979195666, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3146400.0000, 
sim time next is 3147000.0000, 
raw observation next is [7.0, 100.0, 105.0, 713.0, 26.0, 26.70806740674579, 0.6552142855542773, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.6565096952908588, 1.0, 0.35, 0.7878453038674034, 0.6666666666666666, 0.7256722838954826, 0.7184047618514257, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0476146], dtype=float32), 2.3547225]. 
=============================================
[2019-04-04 00:11:13,436] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[97.79699]
 [97.9262 ]
 [98.08002]
 [98.17356]
 [98.01398]], R is [[97.73472595]
 [97.75737762]
 [97.77980804]
 [97.80200958]
 [97.82398987]].
[2019-04-04 00:11:13,783] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1228295e-16 2.9235452e-09 1.7586947e-14 4.1437867e-11 1.9028942e-10
 1.3657324e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 00:11:13,783] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9742
[2019-04-04 00:11:13,845] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 25.11292592749771, 0.3410352894301952, 0.0, 1.0, 46240.85351669978], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2760000.0000, 
sim time next is 2760600.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 25.04383666646008, 0.3294190374465294, 0.0, 1.0, 45928.37957805395], 
processed observation next is [1.0, 0.9565217391304348, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5869863888716734, 0.6098063458155097, 0.0, 1.0, 0.21870656941930453], 
reward next is 0.7813, 
noisyNet noise sample is [array([-0.11625199], dtype=float32), 0.59677935]. 
=============================================
[2019-04-04 00:11:15,188] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.9968161e-17 1.0664121e-09 5.0831675e-15 1.9971918e-11 1.8820589e-11
 7.1086625e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:11:15,188] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0527
[2019-04-04 00:11:15,238] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 85.00000000000001, 0.0, 0.0, 26.0, 24.94581008323555, 0.3475193666307905, 0.0, 1.0, 43352.75264622162], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2938200.0000, 
sim time next is 2938800.0000, 
raw observation next is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.90434066450486, 0.3399474377342013, 0.0, 1.0, 43355.78700594074], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.85, 0.0, 0.0, 0.6666666666666666, 0.5753617220420718, 0.6133158125780671, 0.0, 1.0, 0.20645612859971782], 
reward next is 0.7935, 
noisyNet noise sample is [array([0.40379584], dtype=float32), -1.3980509]. 
=============================================
[2019-04-04 00:11:26,747] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.6589648e-18 6.2830980e-09 5.0889883e-15 1.2999173e-11 2.7224382e-11
 1.0742762e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 00:11:26,747] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2921
[2019-04-04 00:11:26,855] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.5, 96.5, 0.0, 0.0, 26.0, 24.73168222363366, 0.2261876861687102, 0.0, 1.0, 55379.38884654026], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2874600.0000, 
sim time next is 2875200.0000, 
raw observation next is [1.666666666666667, 95.33333333333334, 0.0, 0.0, 26.0, 24.77915244644671, 0.238348122571261, 0.0, 1.0, 54858.57438206342], 
processed observation next is [1.0, 0.2608695652173913, 0.5087719298245615, 0.9533333333333335, 0.0, 0.0, 0.6666666666666666, 0.5649293705372257, 0.5794493741904203, 0.0, 1.0, 0.2612313065812544], 
reward next is 0.7388, 
noisyNet noise sample is [array([0.56170094], dtype=float32), 1.4471157]. 
=============================================
[2019-04-04 00:11:37,668] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.1531898e-16 7.9578957e-09 2.4075387e-15 1.1975536e-11 5.9462837e-11
 1.5088817e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 00:11:37,679] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5084
[2019-04-04 00:11:37,761] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 60.0, 104.0, 767.3333333333334, 26.0, 25.11105607057395, 0.4129695866293375, 0.0, 1.0, 18712.95613905228], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2988600.0000, 
sim time next is 2989200.0000, 
raw observation next is [-2.0, 60.00000000000001, 102.5, 759.1666666666667, 26.0, 25.1486207520684, 0.4187886686817798, 0.0, 1.0, 18711.19869482028], 
processed observation next is [0.0, 0.6086956521739131, 0.40720221606648205, 0.6000000000000001, 0.3416666666666667, 0.8388581952117865, 0.6666666666666666, 0.5957183960057, 0.6395962228939266, 0.0, 1.0, 0.08910094616581085], 
reward next is 0.9109, 
noisyNet noise sample is [array([0.7429359], dtype=float32), -0.18918167]. 
=============================================
[2019-04-04 00:11:41,517] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.8489965e-15 2.5210321e-07 7.6538731e-14 3.1535927e-11 7.9799445e-10
 1.0913277e-14 9.9999976e-01], sum to 1.0000
[2019-04-04 00:11:41,517] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9580
[2019-04-04 00:11:41,577] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 71.0, 0.0, 0.0, 26.0, 24.46806262929402, 0.1602401886798432, 0.0, 1.0, 38373.37254575094], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3030600.0000, 
sim time next is 3031200.0000, 
raw observation next is [-5.0, 71.0, 0.0, 0.0, 26.0, 24.42346484431128, 0.1540426015147497, 0.0, 1.0, 38521.25483205608], 
processed observation next is [0.0, 0.08695652173913043, 0.32409972299168976, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5352887370259399, 0.5513475338382499, 0.0, 1.0, 0.18343454681931468], 
reward next is 0.8166, 
noisyNet noise sample is [array([-0.65787065], dtype=float32), 1.2681192]. 
=============================================
[2019-04-04 00:11:56,149] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.5236744e-20 8.3905682e-10 1.8580379e-17 3.7707410e-13 2.4172602e-12
 9.5705593e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:11:56,149] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.0606
[2019-04-04 00:11:56,185] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.0, 100.0, 14.66666666666666, 133.6666666666667, 26.0, 25.47584166156738, 0.3369582446015421, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3138000.0000, 
sim time next is 3138600.0000, 
raw observation next is [6.0, 100.0, 28.33333333333333, 185.3333333333333, 26.0, 25.46780416280599, 0.3297081341090468, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.6288088642659281, 1.0, 0.09444444444444443, 0.20478821362799257, 0.6666666666666666, 0.6223170135671658, 0.6099027113696823, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9194591], dtype=float32), 1.5698143]. 
=============================================
[2019-04-04 00:12:12,416] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.8971565e-17 5.7226346e-10 6.9873099e-15 1.8045293e-11 1.3335624e-10
 2.0526486e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 00:12:12,417] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0666
[2019-04-04 00:12:12,451] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 55.0, 0.0, 0.0, 26.0, 25.54313407110579, 0.4948263132301917, 0.0, 1.0, 27165.43980060415], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3352800.0000, 
sim time next is 3353400.0000, 
raw observation next is [-3.0, 55.0, 0.0, 0.0, 26.0, 25.45363112439814, 0.4767692660647609, 0.0, 1.0, 25685.07034706496], 
processed observation next is [1.0, 0.8260869565217391, 0.3795013850415513, 0.55, 0.0, 0.0, 0.6666666666666666, 0.6211359270331783, 0.6589230886882537, 0.0, 1.0, 0.12230985879554743], 
reward next is 0.8777, 
noisyNet noise sample is [array([-1.222468], dtype=float32), -1.4921618]. 
=============================================
[2019-04-04 00:12:29,165] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0851604e-19 1.4870269e-10 7.0277976e-18 1.2875386e-13 9.9117340e-13
 1.1346212e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:12:29,165] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6593
[2019-04-04 00:12:29,211] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 49.0, 109.6666666666667, 795.6666666666666, 26.0, 26.40693778187495, 0.6293754430979085, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3419400.0000, 
sim time next is 3420000.0000, 
raw observation next is [3.0, 49.0, 108.0, 790.5, 26.0, 26.47928142825629, 0.5327992663547778, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.49, 0.36, 0.8734806629834254, 0.6666666666666666, 0.7066067856880242, 0.6775997554515927, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7539926], dtype=float32), -0.0117887845]. 
=============================================
[2019-04-04 00:12:29,254] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[92.16696 ]
 [92.094864]
 [92.17128 ]
 [92.22133 ]
 [92.3599  ]], R is [[92.18247223]
 [92.26065063]
 [92.33804321]
 [92.41466522]
 [92.49051666]].
[2019-04-04 00:12:31,649] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.4925356e-19 1.2157658e-10 3.5082133e-17 1.2984793e-12 2.2241083e-12
 9.6904695e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:12:31,649] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3934
[2019-04-04 00:12:31,672] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 56.16666666666666, 115.6666666666667, 818.6666666666667, 26.0, 25.18743664129374, 0.5275577351396238, 1.0, 1.0, 65786.76487610437], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3499800.0000, 
sim time next is 3500400.0000, 
raw observation next is [2.0, 55.33333333333334, 115.8333333333333, 820.8333333333334, 26.0, 25.62551801954501, 0.5505783075491518, 1.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.5217391304347826, 0.518005540166205, 0.5533333333333335, 0.386111111111111, 0.9069981583793739, 0.6666666666666666, 0.635459834962084, 0.6835261025163839, 1.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.1914074], dtype=float32), 0.075361885]. 
=============================================
[2019-04-04 00:12:35,302] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.0403535e-15 1.8029276e-07 4.5166080e-14 4.6750954e-11 1.1799848e-09
 6.3018239e-15 9.9999976e-01], sum to 1.0000
[2019-04-04 00:12:35,306] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6216
[2019-04-04 00:12:35,340] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 70.0, 0.0, 0.0, 26.0, 24.66291834348707, 0.2650136085125981, 0.0, 1.0, 40864.32570228973], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3564000.0000, 
sim time next is 3564600.0000, 
raw observation next is [-6.0, 70.0, 0.0, 0.0, 26.0, 24.63275055162215, 0.2558893033570594, 0.0, 1.0, 40872.05357827726], 
processed observation next is [0.0, 0.2608695652173913, 0.296398891966759, 0.7, 0.0, 0.0, 0.6666666666666666, 0.552729212635179, 0.5852964344523531, 0.0, 1.0, 0.19462882656322503], 
reward next is 0.8054, 
noisyNet noise sample is [array([0.5948961], dtype=float32), -1.7075626]. 
=============================================
[2019-04-04 00:12:41,196] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.5384707e-17 2.9934533e-08 1.0172057e-15 9.1593278e-12 5.1201345e-11
 8.9498944e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:12:41,198] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3956
[2019-04-04 00:12:41,210] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.5, 29.0, 89.0, 403.0, 26.0, 25.57836968608911, 0.4251244586619747, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3659400.0000, 
sim time next is 3660000.0000, 
raw observation next is [10.0, 28.0, 91.0, 446.3333333333334, 26.0, 25.6045740301603, 0.4311587351745405, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.739612188365651, 0.28, 0.30333333333333334, 0.49318600368324134, 0.6666666666666666, 0.6337145025133584, 0.6437195783915135, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.38941735], dtype=float32), -0.35617822]. 
=============================================
[2019-04-04 00:12:41,222] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[89.84148]
 [89.44271]
 [89.1458 ]
 [88.84312]
 [88.41168]], R is [[90.13986206]
 [90.23846436]
 [90.33608246]
 [90.432724  ]
 [90.43909454]].
[2019-04-04 00:12:42,782] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.4654383e-17 5.5435665e-09 2.3315446e-15 5.1859636e-12 5.5838011e-11
 4.3433004e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:12:42,785] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3163
[2019-04-04 00:12:42,798] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.166666666666667, 57.5, 47.66666666666667, 403.0000000000001, 26.0, 25.45390552141603, 0.4495975277196586, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3689400.0000, 
sim time next is 3690000.0000, 
raw observation next is [4.0, 59.0, 39.5, 343.5, 26.0, 25.44486223756817, 0.4365503331484107, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.7391304347826086, 0.5734072022160666, 0.59, 0.13166666666666665, 0.37955801104972375, 0.6666666666666666, 0.6204051864640142, 0.6455167777161369, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.765461], dtype=float32), -0.88453794]. 
=============================================
[2019-04-04 00:12:42,813] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[87.24648 ]
 [87.962654]
 [88.57346 ]
 [88.86594 ]
 [89.18051 ]], R is [[86.67025757]
 [86.80355835]
 [86.93552399]
 [87.06616974]
 [87.19551086]].
[2019-04-04 00:12:46,814] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.4179260e-18 8.0641804e-09 8.2271564e-16 2.9334681e-11 1.8886216e-11
 1.2765280e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 00:12:46,815] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8412
[2019-04-04 00:12:46,826] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.3333333333333333, 70.33333333333333, 0.0, 0.0, 26.0, 25.42037969986173, 0.4093641586891285, 0.0, 1.0, 76936.83655967977], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3707400.0000, 
sim time next is 3708000.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.4145538759984, 0.4130819472555836, 0.0, 1.0, 56812.84450081544], 
processed observation next is [0.0, 0.9565217391304348, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6178794896665334, 0.6376939824185278, 0.0, 1.0, 0.2705373547657878], 
reward next is 0.7295, 
noisyNet noise sample is [array([0.3583309], dtype=float32), -1.1447964]. 
=============================================
[2019-04-04 00:12:46,844] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[84.41466]
 [84.36019]
 [84.23131]
 [84.18631]
 [84.31236]], R is [[84.27275085]
 [84.06365967]
 [83.79694366]
 [83.56534576]
 [83.50031281]].
[2019-04-04 00:12:47,043] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.9985461e-17 3.7041104e-10 6.5487601e-16 3.5379501e-12 6.2663689e-12
 1.1442063e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:12:47,044] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3215
[2019-04-04 00:12:47,081] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.666666666666667, 69.0, 0.0, 0.0, 26.0, 25.25396058641791, 0.5336668167166897, 0.0, 1.0, 91195.8267630407], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3789600.0000, 
sim time next is 3790200.0000, 
raw observation next is [-2.833333333333333, 70.0, 0.0, 0.0, 26.0, 25.45993119012686, 0.5496448392210641, 0.0, 1.0, 18758.37875354252], 
processed observation next is [1.0, 0.8695652173913043, 0.3841181902123731, 0.7, 0.0, 0.0, 0.6666666666666666, 0.6216609325105716, 0.6832149464070213, 0.0, 1.0, 0.08932561311210725], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.5737397], dtype=float32), 0.8260187]. 
=============================================
[2019-04-04 00:12:51,944] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.9873904e-19 3.2477222e-11 2.9164767e-17 3.7922055e-13 1.6959412e-12
 2.8167113e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:12:51,945] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.3035
[2019-04-04 00:12:51,956] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.5, 46.5, 87.0, 717.0, 26.0, 26.39875434975885, 0.7016466939276803, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3857400.0000, 
sim time next is 3858000.0000, 
raw observation next is [2.666666666666667, 46.0, 83.16666666666666, 689.3333333333333, 26.0, 26.6443361617717, 0.7220095177900901, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5364727608494922, 0.46, 0.2772222222222222, 0.7616942909760589, 0.6666666666666666, 0.7203613468143084, 0.7406698392633634, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0846728], dtype=float32), 1.8961262]. 
=============================================
[2019-04-04 00:12:51,973] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[91.80421 ]
 [91.92707 ]
 [92.13329 ]
 [92.5498  ]
 [92.795135]], R is [[91.70865631]
 [91.79157257]
 [91.87365723]
 [91.95491791]
 [92.03536987]].
[2019-04-04 00:12:52,874] A3C_AGENT_WORKER-Thread-4 INFO:Evaluating...
[2019-04-04 00:12:52,875] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 00:12:52,875] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:12:52,875] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 00:12:52,876] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:12:52,877] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 00:12:52,877] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:12:52,880] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run17
[2019-04-04 00:12:52,880] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run17
[2019-04-04 00:12:52,924] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run17
[2019-04-04 00:13:04,645] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.33400175], dtype=float32), 0.2723433]
[2019-04-04 00:13:04,646] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-5.0, 65.0, 141.0, 0.0, 26.0, 25.23467359040765, 0.2148334751170541, 1.0, 1.0, 25191.66745773939]
[2019-04-04 00:13:04,646] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 00:13:04,647] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.8879138e-17 4.9299294e-09 9.4562949e-16 9.7204926e-12 1.2451495e-11
 5.4519395e-17 1.0000000e+00], sampled 0.3695895067480959
[2019-04-04 00:14:22,194] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.33400175], dtype=float32), 0.2723433]
[2019-04-04 00:14:22,194] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-3.0, 84.0, 0.0, 0.0, 26.0, 24.40211551958336, 0.215412316050482, 0.0, 1.0, 42941.33072778645]
[2019-04-04 00:14:22,194] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 00:14:22,195] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [4.0515081e-16 1.3951504e-07 3.8743511e-14 6.0712706e-11 2.6586744e-10
 4.5188556e-15 9.9999988e-01], sampled 0.599402042827752
[2019-04-04 00:14:58,205] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 00:15:27,140] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.6417 263415234.9355 1551.9605
[2019-04-04 00:15:36,992] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 00:15:38,030] A3C_AGENT_WORKER-Thread-4 INFO:Global step: 1600000, evaluation results [1600000.0, 7241.641738402564, 263415234.93545747, 1551.9605289518147, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 00:15:42,103] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.2204596e-17 9.2011332e-10 3.0429729e-15 2.2411430e-11 3.8730349e-11
 1.7435960e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 00:15:42,125] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6738
[2019-04-04 00:15:42,138] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.32955473815348, 0.4405118194438956, 0.0, 1.0, 47019.08499189231], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3888600.0000, 
sim time next is 3889200.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.33607999920739, 0.4396284035335473, 0.0, 1.0, 42354.78905017174], 
processed observation next is [1.0, 0.0, 0.40720221606648205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.611339999933949, 0.6465428011778491, 0.0, 1.0, 0.2016894716674845], 
reward next is 0.7983, 
noisyNet noise sample is [array([0.8090818], dtype=float32), -1.167348]. 
=============================================
[2019-04-04 00:15:42,806] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.8651328e-16 2.0739877e-07 1.2316600e-14 1.2698607e-10 1.8062239e-10
 2.0781528e-15 9.9999976e-01], sum to 1.0000
[2019-04-04 00:15:42,806] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7844
[2019-04-04 00:15:42,820] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 26.0, 25.39729169432064, 0.3437339918954256, 0.0, 1.0, 37962.19218055241], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4247400.0000, 
sim time next is 4248000.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 26.0, 25.47031442092987, 0.3417794953387102, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.17391304347826086, 0.5457063711911359, 0.45, 0.0, 0.0, 0.6666666666666666, 0.6225262017441558, 0.6139264984462368, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.8329245], dtype=float32), -1.247085]. 
=============================================
[2019-04-04 00:15:42,827] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[83.32936 ]
 [83.352135]
 [83.32567 ]
 [83.268326]
 [83.29663 ]], R is [[83.27649689]
 [83.26296234]
 [83.20774078]
 [83.13170624]
 [83.13918304]].
[2019-04-04 00:15:42,851] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.4496108e-17 2.8230669e-09 1.7152035e-15 1.3578345e-11 3.2254675e-11
 1.0172827e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 00:15:42,851] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0860
[2019-04-04 00:15:42,862] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.36280365464358, 0.4358770496944626, 0.0, 1.0, 39843.79311490045], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3891000.0000, 
sim time next is 3891600.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.36896433177301, 0.4373781268379474, 0.0, 1.0, 39745.86841568379], 
processed observation next is [1.0, 0.043478260869565216, 0.40720221606648205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6140803609810842, 0.6457927089459825, 0.0, 1.0, 0.18926604007468473], 
reward next is 0.8107, 
noisyNet noise sample is [array([-1.3935457], dtype=float32), -1.2249113]. 
=============================================
[2019-04-04 00:15:44,932] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4563726e-17 2.2677880e-09 2.0786608e-15 6.8079717e-12 6.4723916e-12
 1.6879861e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 00:15:44,932] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8592
[2019-04-04 00:15:44,977] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.25, 75.16666666666666, 0.0, 0.0, 26.0, 25.51039960871919, 0.40162095126299, 0.0, 1.0, 35013.25104896644], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4323000.0000, 
sim time next is 4323600.0000, 
raw observation next is [4.2, 75.0, 0.0, 0.0, 26.0, 25.50350591352824, 0.4048620937261207, 0.0, 1.0, 37295.7139293152], 
processed observation next is [1.0, 0.043478260869565216, 0.5789473684210527, 0.75, 0.0, 0.0, 0.6666666666666666, 0.6252921594606867, 0.6349540312420402, 0.0, 1.0, 0.17759863775864382], 
reward next is 0.8224, 
noisyNet noise sample is [array([3.176291], dtype=float32), 1.5196568]. 
=============================================
[2019-04-04 00:15:48,018] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.6116018e-17 3.9561129e-10 1.5643997e-15 5.1126373e-12 1.9854101e-11
 6.9771510e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:15:48,019] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.6900
[2019-04-04 00:15:48,029] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 35.5, 0.0, 0.0, 26.0, 26.45873456321669, 0.6189459304682, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4127400.0000, 
sim time next is 4128000.0000, 
raw observation next is [3.0, 36.0, 0.0, 0.0, 26.0, 26.26676405451683, 0.5913193098379929, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.5457063711911359, 0.36, 0.0, 0.0, 0.6666666666666666, 0.6888970045430692, 0.6971064366126644, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8743772], dtype=float32), -1.0753721]. 
=============================================
[2019-04-04 00:15:48,100] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[82.76972 ]
 [82.60162 ]
 [82.37542 ]
 [82.379295]
 [82.717766]], R is [[83.03010559]
 [83.19980621]
 [83.36780548]
 [83.53412628]
 [83.69878387]].
[2019-04-04 00:15:56,069] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.7613358e-18 1.3325696e-08 2.3487557e-15 2.8859499e-12 9.2670654e-12
 7.0589952e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:15:56,071] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0288
[2019-04-04 00:15:56,090] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.383333333333333, 67.16666666666667, 0.0, 0.0, 26.0, 25.71200708213254, 0.5545382376036295, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4421400.0000, 
sim time next is 4422000.0000, 
raw observation next is [4.266666666666667, 67.33333333333334, 0.0, 0.0, 26.0, 25.70604074441254, 0.5464218602730851, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.5807940904893814, 0.6733333333333335, 0.0, 0.0, 0.6666666666666666, 0.6421700620343783, 0.6821406200910284, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.77607536], dtype=float32), 0.43439853]. 
=============================================
[2019-04-04 00:15:56,143] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[85.93211]
 [86.17234]
 [86.36771]
 [86.58706]
 [86.6953 ]], R is [[85.84999847]
 [85.99150085]
 [86.13158417]
 [86.2702713 ]
 [86.27709198]].
[2019-04-04 00:15:57,396] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.3671444e-16 3.8365581e-08 2.6563983e-14 6.9155154e-11 3.0647418e-10
 1.6534387e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 00:15:57,396] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0133
[2019-04-04 00:15:57,464] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 35.33333333333334, 0.0, 0.0, 26.0, 24.87510000995376, 0.2264746308187372, 0.0, 1.0, 40207.41238272389], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4080000.0000, 
sim time next is 4080600.0000, 
raw observation next is [-4.0, 36.0, 0.0, 0.0, 26.0, 24.82899811774097, 0.2207144359255359, 0.0, 1.0, 40190.81714542563], 
processed observation next is [1.0, 0.21739130434782608, 0.3518005540166205, 0.36, 0.0, 0.0, 0.6666666666666666, 0.5690831764784141, 0.5735714786418453, 0.0, 1.0, 0.19138484354964586], 
reward next is 0.8086, 
noisyNet noise sample is [array([0.09279905], dtype=float32), -1.6201743]. 
=============================================
[2019-04-04 00:15:57,718] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.9440883e-18 1.5292700e-09 7.8620354e-16 3.0040341e-12 4.9488018e-12
 6.4671520e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:15:57,718] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3277
[2019-04-04 00:15:57,730] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.4666666666666667, 72.83333333333333, 0.0, 0.0, 26.0, 25.34527711291272, 0.482460122858537, 0.0, 1.0, 43459.59159681219], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4492200.0000, 
sim time next is 4492800.0000, 
raw observation next is [-0.5, 73.0, 0.0, 0.0, 26.0, 25.39262974445863, 0.4796055132198228, 0.0, 1.0, 32301.32722331606], 
processed observation next is [1.0, 0.0, 0.44875346260387816, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6160524787048859, 0.6598685044066076, 0.0, 1.0, 0.15381584392055267], 
reward next is 0.8462, 
noisyNet noise sample is [array([-1.1654804], dtype=float32), -2.8020759]. 
=============================================
[2019-04-04 00:16:13,501] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.8776760e-20 6.6223332e-11 3.5568708e-18 1.7700736e-13 1.3412488e-13
 3.8937194e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 00:16:13,502] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9849
[2019-04-04 00:16:13,519] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.5, 31.0, 155.0, 735.5, 26.0, 28.49458839630955, 1.127169817276631, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4370400.0000, 
sim time next is 4371000.0000, 
raw observation next is [14.4, 31.16666666666667, 168.3333333333333, 700.0, 26.0, 28.55432813556805, 1.144310275668714, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.8614958448753465, 0.3116666666666667, 0.561111111111111, 0.7734806629834254, 0.6666666666666666, 0.8795273446306707, 0.881436758556238, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.14291687], dtype=float32), 1.1386024]. 
=============================================
[2019-04-04 00:16:13,540] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[96.484   ]
 [96.36537 ]
 [96.27353 ]
 [96.20831 ]
 [96.284325]], R is [[96.64750671]
 [96.68103027]
 [96.71421814]
 [96.74707794]
 [96.77960968]].
[2019-04-04 00:16:35,384] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.8081284e-16 1.7363483e-08 1.3944964e-14 2.4073413e-11 1.7327338e-10
 8.3789643e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 00:16:35,384] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5801
[2019-04-04 00:16:35,407] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 43.5, 0.0, 0.0, 26.0, 25.12713939175858, 0.3479835325957756, 0.0, 1.0, 152652.7670043305], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4908600.0000, 
sim time next is 4909200.0000, 
raw observation next is [1.0, 42.33333333333333, 0.0, 0.0, 26.0, 25.17728557210885, 0.3679739124701962, 0.0, 1.0, 82808.14351107727], 
processed observation next is [0.0, 0.8260869565217391, 0.4903047091412743, 0.4233333333333333, 0.0, 0.0, 0.6666666666666666, 0.5981071310090709, 0.6226579708233987, 0.0, 1.0, 0.3943244929098918], 
reward next is 0.6057, 
noisyNet noise sample is [array([-0.5762091], dtype=float32), -0.55923116]. 
=============================================
[2019-04-04 00:16:40,433] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.0838052e-19 2.5905511e-10 5.3242235e-17 6.0226048e-13 3.1613364e-12
 8.0608177e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:16:40,433] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1639
[2019-04-04 00:16:40,481] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.800000000000001, 47.0, 52.83333333333333, 145.3333333333333, 26.0, 27.51018954326894, 0.821107692278347, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4641600.0000, 
sim time next is 4642200.0000, 
raw observation next is [4.6, 47.5, 40.0, 145.0, 26.0, 27.31500589531953, 0.77448369262021, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.590027700831025, 0.475, 0.13333333333333333, 0.16022099447513813, 0.6666666666666666, 0.7762504912766275, 0.7581612308734034, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4254363], dtype=float32), -1.441559]. 
=============================================
[2019-04-04 00:16:41,784] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.19810446e-19 8.65114404e-12 9.15269888e-18 5.37096951e-13
 8.92757819e-13 1.05381375e-19 1.00000000e+00], sum to 1.0000
[2019-04-04 00:16:41,785] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7540
[2019-04-04 00:16:41,810] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.0, 26.0, 53.0, 472.5, 26.0, 26.99257240869654, 0.8274615153686081, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4986000.0000, 
sim time next is 4986600.0000, 
raw observation next is [7.666666666666667, 25.83333333333334, 46.66666666666666, 416.3333333333333, 26.0, 27.30291447063754, 0.8570768473741047, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.674976915974146, 0.2583333333333334, 0.15555555555555553, 0.460036832412523, 0.6666666666666666, 0.7752428725531283, 0.7856922824580349, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8307526], dtype=float32), -1.2716964]. 
=============================================
[2019-04-04 00:16:43,655] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.08285883e-16 2.29029737e-08 1.04833939e-14 1.08529115e-11
 5.94164717e-11 3.29729783e-16 1.00000000e+00], sum to 1.0000
[2019-04-04 00:16:43,658] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5910
[2019-04-04 00:16:43,671] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.666666666666667, 67.0, 0.0, 0.0, 26.0, 25.68174780301176, 0.476614721421624, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4677000.0000, 
sim time next is 4677600.0000, 
raw observation next is [1.333333333333333, 72.0, 0.0, 0.0, 26.0, 25.6546704228174, 0.4591525256196293, 0.0, 1.0, 18728.50836786075], 
processed observation next is [1.0, 0.13043478260869565, 0.4995383194829178, 0.72, 0.0, 0.0, 0.6666666666666666, 0.63788920190145, 0.6530508418732098, 0.0, 1.0, 0.0891833731802893], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.9746128], dtype=float32), 1.3076034]. 
=============================================
[2019-04-04 00:16:45,572] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [3.5267804e-19 2.0614278e-10 6.9606454e-17 8.5002681e-13 7.8490122e-13
 1.2374692e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:16:45,626] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.8115
[2019-04-04 00:16:45,641] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 86.0, 117.0, 0.0, 26.0, 25.93163624918936, 0.4583046922945902, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4711800.0000, 
sim time next is 4712400.0000, 
raw observation next is [1.0, 86.0, 121.5, 0.0, 26.0, 25.77872005909615, 0.4399982179333503, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.4903047091412743, 0.86, 0.405, 0.0, 0.6666666666666666, 0.6482266715913457, 0.6466660726444501, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.135651], dtype=float32), -1.7833403]. 
=============================================
[2019-04-04 00:16:47,097] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:16:47,097] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:16:47,100] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run13
[2019-04-04 00:16:48,910] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.7822100e-15 2.6738523e-08 1.0640875e-13 8.9146385e-11 6.2193301e-10
 3.4487188e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 00:16:48,917] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8258
[2019-04-04 00:16:48,929] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.666666666666667, 36.0, 41.16666666666666, 245.6666666666667, 26.0, 25.09755942413934, 0.383782935596045, 0.0, 1.0, 42377.37100275412], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4814400.0000, 
sim time next is 4815000.0000, 
raw observation next is [2.5, 37.0, 33.0, 185.0, 26.0, 25.09861011997645, 0.3755835057929132, 0.0, 1.0, 28215.38616152038], 
processed observation next is [0.0, 0.7391304347826086, 0.5318559556786704, 0.37, 0.11, 0.20441988950276244, 0.6666666666666666, 0.5915508433313708, 0.625194501930971, 0.0, 1.0, 0.1343589817215256], 
reward next is 0.8656, 
noisyNet noise sample is [array([-0.94721687], dtype=float32), -0.22801694]. 
=============================================
[2019-04-04 00:16:48,939] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[77.85495 ]
 [78.1068  ]
 [78.536835]
 [79.07504 ]
 [79.73323 ]], R is [[77.66003418]
 [77.68164062]
 [77.78683472]
 [77.92001343]
 [78.14081573]].
[2019-04-04 00:16:51,110] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:16:51,110] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:16:51,134] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run13
[2019-04-04 00:16:51,279] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [4.18294013e-16 1.03360094e-07 6.70342298e-15 9.50836979e-11
 3.11677101e-10 2.30711637e-15 9.99999881e-01], sum to 1.0000
[2019-04-04 00:16:51,279] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8958
[2019-04-04 00:16:51,293] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 40.0, 0.0, 0.0, 26.0, 25.39888793174308, 0.3468598452756706, 0.0, 1.0, 30644.85527136343], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4924800.0000, 
sim time next is 4925400.0000, 
raw observation next is [0.8333333333333334, 40.5, 0.0, 0.0, 26.0, 25.38992145763738, 0.341697843945526, 0.0, 1.0, 38554.32950505528], 
processed observation next is [1.0, 0.0, 0.4856879039704525, 0.405, 0.0, 0.0, 0.6666666666666666, 0.6158267881364484, 0.6138992813151753, 0.0, 1.0, 0.183592045262168], 
reward next is 0.8164, 
noisyNet noise sample is [array([-0.61637455], dtype=float32), -0.6534581]. 
=============================================
[2019-04-04 00:17:06,001] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.9179796e-18 2.4737052e-09 4.2761641e-15 8.1221939e-12 1.4245144e-10
 1.0720942e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 00:17:06,001] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2224
[2019-04-04 00:17:06,031] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.5, 37.0, 0.0, 0.0, 26.0, 25.57448281256838, 0.4820348529306581, 0.0, 1.0, 18740.87278252707], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5009400.0000, 
sim time next is 5010000.0000, 
raw observation next is [2.333333333333333, 38.0, 0.0, 0.0, 26.0, 25.56356478032794, 0.4740847153818115, 0.0, 1.0, 24097.61471102163], 
processed observation next is [1.0, 1.0, 0.5272391505078486, 0.38, 0.0, 0.0, 0.6666666666666666, 0.6302970650273284, 0.6580282384606039, 0.0, 1.0, 0.11475054624296015], 
reward next is 0.8852, 
noisyNet noise sample is [array([-1.6614898], dtype=float32), 0.7773487]. 
=============================================
[2019-04-04 00:17:06,094] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[78.02458 ]
 [78.156364]
 [78.436424]
 [78.58509 ]
 [78.55101 ]], R is [[78.06938934]
 [78.19945526]
 [78.41746521]
 [78.54401398]
 [78.51696777]].
[2019-04-04 00:17:06,385] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:17:06,386] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:17:06,392] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run13
[2019-04-04 00:17:07,455] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [9.9233756e-19 4.8629314e-09 5.6055772e-16 3.0852077e-12 5.1143148e-12
 4.9843861e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:17:07,457] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.1072
[2019-04-04 00:17:07,529] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 39.0, 100.5, 638.5, 26.0, 25.70440736829734, 0.4102391934557207, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4957200.0000, 
sim time next is 4957800.0000, 
raw observation next is [-0.6666666666666667, 37.5, 103.0, 664.6666666666667, 26.0, 25.91213708046043, 0.4324335869512148, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.44413665743305636, 0.375, 0.3433333333333333, 0.734438305709024, 0.6666666666666666, 0.6593447567050358, 0.6441445289837383, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.82557046], dtype=float32), 0.5754331]. 
=============================================
[2019-04-04 00:17:08,196] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.7260579e-17 5.4254059e-09 5.5452206e-15 5.8277337e-12 1.8500798e-11
 7.8048673e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:17:08,196] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9072
[2019-04-04 00:17:08,210] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 35.0, 0.0, 0.0, 26.0, 25.48954817783453, 0.4745768902132596, 0.0, 1.0, 137034.8830698556], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5006400.0000, 
sim time next is 5007000.0000, 
raw observation next is [3.0, 34.5, 0.0, 0.0, 26.0, 25.45295327963407, 0.4857392677365042, 0.0, 1.0, 98207.07175405325], 
processed observation next is [1.0, 0.9565217391304348, 0.5457063711911359, 0.345, 0.0, 0.0, 0.6666666666666666, 0.6210794399695058, 0.6619130892455014, 0.0, 1.0, 0.4676527226383488], 
reward next is 0.5323, 
noisyNet noise sample is [array([-0.18742724], dtype=float32), -0.8830738]. 
=============================================
[2019-04-04 00:17:08,213] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[79.63329]
 [79.26863]
 [78.87716]
 [79.13672]
 [79.31624]], R is [[79.52022552]
 [79.07247162]
 [78.73062897]
 [78.94332123]
 [79.15389252]].
[2019-04-04 00:17:13,455] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:17:13,456] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:17:13,461] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run13
[2019-04-04 00:17:13,634] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:17:13,635] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:17:13,638] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run13
[2019-04-04 00:17:13,891] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:17:13,891] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:17:13,897] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run13
[2019-04-04 00:17:16,185] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:17:16,185] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:17:16,189] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run13
[2019-04-04 00:17:19,216] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:17:19,216] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:17:19,220] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run13
[2019-04-04 00:17:19,521] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:17:19,521] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:17:19,525] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run13
[2019-04-04 00:17:19,921] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:17:19,921] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:17:19,925] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run13
[2019-04-04 00:17:20,307] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:17:20,307] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:17:20,315] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run13
[2019-04-04 00:17:21,699] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:17:21,699] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:17:21,712] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run13
[2019-04-04 00:17:22,232] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.3358208e-18 1.0583715e-09 1.2967894e-15 7.0689431e-12 1.6899685e-11
 1.5385295e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:17:22,233] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7537
[2019-04-04 00:17:22,276] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 76.16666666666667, 0.0, 0.0, 26.0, 24.49248231954316, 0.1665445783476293, 0.0, 1.0, 44191.81142438296], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 252600.0000, 
sim time next is 253200.0000, 
raw observation next is [-3.9, 77.33333333333334, 0.0, 0.0, 26.0, 24.46117997356117, 0.1593161444588466, 0.0, 1.0, 44179.42109850749], 
processed observation next is [1.0, 0.9565217391304348, 0.3545706371191136, 0.7733333333333334, 0.0, 0.0, 0.6666666666666666, 0.5384316644634307, 0.5531053814862822, 0.0, 1.0, 0.21037819570717853], 
reward next is 0.7896, 
noisyNet noise sample is [array([-0.48128346], dtype=float32), -0.9703619]. 
=============================================
[2019-04-04 00:17:22,419] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:17:22,419] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:17:22,586] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run13
[2019-04-04 00:17:23,181] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:17:23,181] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:17:23,184] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run13
[2019-04-04 00:17:23,760] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:17:23,760] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:17:23,764] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run13
[2019-04-04 00:17:23,863] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:17:23,863] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:17:23,886] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run13
[2019-04-04 00:17:28,375] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.6297939e-17 1.7407312e-09 7.7379060e-15 2.6889201e-11 8.9994436e-11
 6.0509784e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 00:17:28,376] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3922
[2019-04-04 00:17:28,406] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.966666666666667, 71.0, 0.0, 0.0, 26.0, 24.12413343229557, 0.08545579177673328, 0.0, 1.0, 44565.29230686714], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 261600.0000, 
sim time next is 262200.0000, 
raw observation next is [-6.333333333333334, 69.0, 0.0, 0.0, 26.0, 24.14993192803506, 0.0812965499261787, 0.0, 1.0, 44683.84938549842], 
processed observation next is [1.0, 0.0, 0.28716528162511545, 0.69, 0.0, 0.0, 0.6666666666666666, 0.5124943273362549, 0.5270988499753929, 0.0, 1.0, 0.2127802351690401], 
reward next is 0.7872, 
noisyNet noise sample is [array([0.2950762], dtype=float32), -0.01702882]. 
=============================================
[2019-04-04 00:17:41,512] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.30541745e-17 1.14275895e-08 4.12435114e-14 1.10343384e-11
 2.83852768e-11 4.32386179e-16 1.00000000e+00], sum to 1.0000
[2019-04-04 00:17:41,517] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7355
[2019-04-04 00:17:41,540] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.9, 85.66666666666667, 0.0, 0.0, 26.0, 24.05860520030572, 0.08599278635322184, 0.0, 1.0, 43086.98739205402], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 97800.0000, 
sim time next is 98400.0000, 
raw observation next is [-3.0, 84.33333333333334, 0.0, 0.0, 26.0, 24.08479104639365, 0.08299541897174625, 0.0, 1.0, 43209.92480693283], 
processed observation next is [1.0, 0.13043478260869565, 0.3795013850415513, 0.8433333333333334, 0.0, 0.0, 0.6666666666666666, 0.5070659205328042, 0.5276651396572488, 0.0, 1.0, 0.20576154669968014], 
reward next is 0.7942, 
noisyNet noise sample is [array([-1.3192497], dtype=float32), 0.5307855]. 
=============================================
[2019-04-04 00:17:44,750] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.8967754e-16 2.4026479e-08 4.8150543e-14 8.8214408e-11 5.6848595e-11
 1.3584638e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 00:17:44,751] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2033
[2019-04-04 00:17:44,764] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.58742151399065, -0.2746099196665856, 0.0, 1.0, 44979.86242635983], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 197400.0000, 
sim time next is 198000.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.60008549849249, -0.2845155169865929, 0.0, 1.0, 44982.86998327757], 
processed observation next is [1.0, 0.30434782608695654, 0.21606648199445982, 0.78, 0.0, 0.0, 0.6666666666666666, 0.3833404582077075, 0.40516149433780235, 0.0, 1.0, 0.21420414277751224], 
reward next is 0.7858, 
noisyNet noise sample is [array([0.85546726], dtype=float32), 0.32233572]. 
=============================================
[2019-04-04 00:17:44,792] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[69.93012 ]
 [69.96625 ]
 [70.011024]
 [70.05627 ]
 [70.100235]], R is [[69.97838593]
 [70.06441498]
 [70.14959717]
 [70.23399353]
 [70.31757355]].
[2019-04-04 00:17:55,146] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.3359008e-17 4.9000970e-10 2.6300602e-15 7.1865790e-12 2.8225150e-11
 4.3749282e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:17:55,147] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2469
[2019-04-04 00:17:55,211] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.4, 71.0, 0.0, 0.0, 26.0, 23.87452314386739, 0.04052335222094285, 0.0, 1.0, 44644.84152318563], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 172200.0000, 
sim time next is 172800.0000, 
raw observation next is [-8.4, 71.0, 0.0, 0.0, 26.0, 23.83105400442642, 0.03034964524917983, 0.0, 1.0, 44589.87354125576], 
processed observation next is [1.0, 0.0, 0.2299168975069252, 0.71, 0.0, 0.0, 0.6666666666666666, 0.485921167035535, 0.5101165484163933, 0.0, 1.0, 0.21233273114883694], 
reward next is 0.7877, 
noisyNet noise sample is [array([1.6399204], dtype=float32), 1.0498856]. 
=============================================
[2019-04-04 00:18:12,736] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.4469475e-17 5.2835469e-09 1.9417720e-15 6.4356706e-12 7.5661985e-12
 8.8964870e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:18:12,736] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7795
[2019-04-04 00:18:12,833] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-15.96666666666667, 87.0, 27.33333333333334, 520.5, 26.0, 24.70494592415706, 0.1476465443764829, 1.0, 1.0, 83906.18360625481], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 376800.0000, 
sim time next is 377400.0000, 
raw observation next is [-15.78333333333333, 88.5, 29.66666666666666, 564.0, 26.0, 25.11539644650247, 0.1851591983035835, 1.0, 1.0, 19926.94606657642], 
processed observation next is [1.0, 0.34782608695652173, 0.02539242843951994, 0.885, 0.09888888888888887, 0.6232044198895028, 0.6666666666666666, 0.5929497038752057, 0.5617197327678611, 1.0, 1.0, 0.09489021936464961], 
reward next is 0.9051, 
noisyNet noise sample is [array([-0.83408904], dtype=float32), 0.31708515]. 
=============================================
[2019-04-04 00:18:34,762] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1881529e-18 1.2781813e-10 1.2161818e-16 6.3167543e-13 1.5532821e-12
 1.5801894e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:18:34,773] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1040
[2019-04-04 00:18:34,805] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 63.0, 0.0, 0.0, 26.0, 24.84645286189493, 0.2676150435246745, 0.0, 1.0, 43632.70865133088], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 769200.0000, 
sim time next is 769800.0000, 
raw observation next is [-6.1, 63.5, 0.0, 0.0, 26.0, 24.79616971235574, 0.2580818966531057, 0.0, 1.0, 43411.36911083456], 
processed observation next is [1.0, 0.9130434782608695, 0.29362880886426596, 0.635, 0.0, 0.0, 0.6666666666666666, 0.5663474760296451, 0.5860272988843686, 0.0, 1.0, 0.20672080528968836], 
reward next is 0.7933, 
noisyNet noise sample is [array([1.2022454], dtype=float32), 0.28100222]. 
=============================================
[2019-04-04 00:18:34,947] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1196516e-16 4.4366764e-09 1.3316915e-14 5.0992760e-12 4.2978412e-11
 2.2068799e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 00:18:34,948] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4164
[2019-04-04 00:18:34,967] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.7, 50.0, 0.0, 0.0, 26.0, 23.19006142319212, -0.1619924066852084, 0.0, 1.0, 45942.88737060973], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 442200.0000, 
sim time next is 442800.0000, 
raw observation next is [-10.6, 49.0, 0.0, 0.0, 26.0, 23.18807021745657, -0.1704510504962111, 0.0, 1.0, 45968.28828525393], 
processed observation next is [1.0, 0.13043478260869565, 0.1689750692520776, 0.49, 0.0, 0.0, 0.6666666666666666, 0.4323391847880475, 0.44318298316792964, 0.0, 1.0, 0.21889661088216159], 
reward next is 0.7811, 
noisyNet noise sample is [array([0.33241042], dtype=float32), 0.14651944]. 
=============================================
[2019-04-04 00:18:45,265] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.9217551e-17 1.3710623e-08 1.6512578e-15 2.6456610e-12 2.7902011e-11
 2.6791199e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 00:18:45,266] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5912
[2019-04-04 00:18:45,336] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.433333333333334, 86.0, 0.0, 0.0, 26.0, 24.50872272416177, 0.1739376340721569, 0.0, 1.0, 40685.57899845464], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 537600.0000, 
sim time next is 538200.0000, 
raw observation next is [1.35, 86.5, 0.0, 0.0, 26.0, 24.48483456272314, 0.1694155659677331, 0.0, 1.0, 40736.36389142635], 
processed observation next is [0.0, 0.21739130434782608, 0.5000000000000001, 0.865, 0.0, 0.0, 0.6666666666666666, 0.5404028802269284, 0.5564718553225777, 0.0, 1.0, 0.19398268519726833], 
reward next is 0.8060, 
noisyNet noise sample is [array([-0.6978513], dtype=float32), -0.50157905]. 
=============================================
[2019-04-04 00:18:48,363] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.0083645e-17 1.7000287e-08 2.7497366e-15 5.3733901e-12 4.6178106e-11
 7.7509854e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:18:48,363] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7791
[2019-04-04 00:18:48,459] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.383333333333333, 87.0, 0.0, 0.0, 26.0, 24.94835072935565, 0.2920147482467101, 0.0, 1.0, 45085.4781974746], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 583800.0000, 
sim time next is 584400.0000, 
raw observation next is [-2.466666666666667, 87.0, 0.0, 0.0, 26.0, 24.98547093945752, 0.2895676880150406, 0.0, 1.0, 25056.09180961876], 
processed observation next is [0.0, 0.782608695652174, 0.39427516158818104, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5821225782881267, 0.5965225626716802, 0.0, 1.0, 0.11931472290294648], 
reward next is 0.8807, 
noisyNet noise sample is [array([-0.17926843], dtype=float32), 0.5153189]. 
=============================================
[2019-04-04 00:18:56,181] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.5856442e-18 1.1940931e-08 1.7647691e-15 4.0596701e-12 2.5423198e-11
 6.7665928e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:18:56,182] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0109
[2019-04-04 00:18:56,224] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 87.0, 0.0, 0.0, 26.0, 25.06646592313293, 0.313667360846816, 0.0, 1.0, 47249.40750470583], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 589800.0000, 
sim time next is 590400.0000, 
raw observation next is [-2.8, 87.0, 0.0, 0.0, 26.0, 25.13094809005833, 0.3121999667977531, 0.0, 1.0, 44577.65224341236], 
processed observation next is [0.0, 0.8695652173913043, 0.38504155124653744, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5942456741715274, 0.604066655599251, 0.0, 1.0, 0.21227453449243983], 
reward next is 0.7877, 
noisyNet noise sample is [array([-1.2213205], dtype=float32), -0.15456171]. 
=============================================
[2019-04-04 00:19:12,706] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.5607973e-20 1.6886666e-11 7.8498001e-18 4.3906339e-14 1.4149802e-13
 4.3016759e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 00:19:12,707] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4481
[2019-04-04 00:19:12,766] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.8, 60.0, 0.0, 0.0, 26.0, 25.78807563138178, 0.6564299987168375, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1108800.0000, 
sim time next is 1109400.0000, 
raw observation next is [13.71666666666667, 60.33333333333333, 0.0, 0.0, 26.0, 25.72265724002167, 0.6450260674558499, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.8425669436749772, 0.6033333333333333, 0.0, 0.0, 0.6666666666666666, 0.6435547700018059, 0.7150086891519499, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7812565], dtype=float32), -0.60399204]. 
=============================================
[2019-04-04 00:19:29,537] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.9470477e-19 4.2453988e-10 8.7768548e-17 7.0918976e-13 1.2895964e-12
 5.2890761e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:19:29,539] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6459
[2019-04-04 00:19:29,590] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.3, 82.33333333333334, 0.0, 0.0, 26.0, 24.83370966923195, 0.2567677284626809, 0.0, 1.0, 41560.48373720986], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 857400.0000, 
sim time next is 858000.0000, 
raw observation next is [-3.2, 81.66666666666667, 0.0, 0.0, 26.0, 24.81111438259661, 0.2513255749680333, 0.0, 1.0, 41484.55922090947], 
processed observation next is [1.0, 0.9565217391304348, 0.37396121883656513, 0.8166666666666668, 0.0, 0.0, 0.6666666666666666, 0.5675928652163842, 0.5837751916560111, 0.0, 1.0, 0.1975455200995689], 
reward next is 0.8025, 
noisyNet noise sample is [array([0.03792709], dtype=float32), 0.11273307]. 
=============================================
[2019-04-04 00:19:29,637] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[83.133934]
 [83.133934]
 [83.07038 ]
 [83.0653  ]
 [83.104546]], R is [[83.11968994]
 [83.0905838 ]
 [83.06148529]
 [83.03263855]
 [83.0030365 ]].
[2019-04-04 00:19:30,941] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.0464014e-21 1.3095680e-11 4.4095561e-19 1.3324131e-14 1.4269317e-14
 4.9618033e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 00:19:30,941] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8625
[2019-04-04 00:19:30,963] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.11666666666667, 81.83333333333333, 110.6666666666667, 0.0, 26.0, 26.54773120096637, 0.6813825032078479, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1000200.0000, 
sim time next is 1000800.0000, 
raw observation next is [14.4, 81.0, 106.5, 0.0, 26.0, 26.61579783310861, 0.6913163659584974, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.8614958448753465, 0.81, 0.355, 0.0, 0.6666666666666666, 0.7179831527590507, 0.7304387886528324, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.23674643], dtype=float32), -0.6979063]. 
=============================================
[2019-04-04 00:19:37,133] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.3611738e-20 9.2201899e-11 2.5796795e-17 4.8231706e-14 6.4784278e-13
 5.0653878e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:19:37,133] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7368
[2019-04-04 00:19:37,173] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.05, 85.5, 0.0, 0.0, 26.0, 25.38946180451292, 0.4329433135132863, 0.0, 1.0, 28525.54359071035], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 955800.0000, 
sim time next is 956400.0000, 
raw observation next is [6.233333333333333, 84.33333333333334, 0.0, 0.0, 26.0, 25.36186257300121, 0.4492530389472041, 0.0, 1.0, 42729.41757613902], 
processed observation next is [1.0, 0.043478260869565216, 0.6352723915050786, 0.8433333333333334, 0.0, 0.0, 0.6666666666666666, 0.613488547750101, 0.6497510129824013, 0.0, 1.0, 0.20347341702923344], 
reward next is 0.7965, 
noisyNet noise sample is [array([-1.9586866], dtype=float32), -0.45297915]. 
=============================================
[2019-04-04 00:19:37,705] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.8408421e-20 4.8023127e-11 3.7136315e-18 5.7812953e-14 7.3372095e-14
 1.7056794e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 00:19:37,705] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2327
[2019-04-04 00:19:37,734] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [18.71666666666667, 54.33333333333333, 160.6666666666667, 0.0, 26.0, 27.13214309712052, 0.9009332964752269, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1086600.0000, 
sim time next is 1087200.0000, 
raw observation next is [18.8, 54.0, 155.5, 0.0, 26.0, 26.56257305671717, 0.8770586970082886, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.9833795013850417, 0.54, 0.5183333333333333, 0.0, 0.6666666666666666, 0.7135477547264308, 0.792352899002763, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.40506798], dtype=float32), -0.36463422]. 
=============================================
[2019-04-04 00:19:41,937] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.8586905e-18 4.0492827e-09 2.6580275e-16 1.6295094e-12 2.4104369e-12
 1.0730019e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:19:41,940] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6223
[2019-04-04 00:19:41,951] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [18.63333333333333, 63.66666666666667, 71.5, 0.0, 26.0, 25.03153388163404, 0.4916902128388969, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1179600.0000, 
sim time next is 1180200.0000, 
raw observation next is [18.71666666666667, 63.33333333333333, 63.00000000000001, 0.0, 26.0, 25.05951985498358, 0.4901742175069504, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.981071098799631, 0.6333333333333333, 0.21000000000000002, 0.0, 0.6666666666666666, 0.5882933212486318, 0.6633914058356501, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.162981], dtype=float32), -0.6085259]. 
=============================================
[2019-04-04 00:19:51,856] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.6381160e-20 2.0330472e-11 5.1420976e-18 4.8309596e-14 1.4232140e-13
 2.8530994e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 00:19:51,856] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2247
[2019-04-04 00:19:51,893] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.56666666666667, 53.0, 41.83333333333334, 30.83333333333334, 26.0, 27.51160435628253, 0.7542803724095822, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1615200.0000, 
sim time next is 1615800.0000, 
raw observation next is [12.38333333333333, 53.5, 33.66666666666667, 24.66666666666667, 26.0, 27.00002913858884, 0.764495754323134, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.8056325023084026, 0.535, 0.11222222222222224, 0.027255985267034995, 0.6666666666666666, 0.7500024282157366, 0.7548319181077113, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.69640315], dtype=float32), -0.56744236]. 
=============================================
[2019-04-04 00:19:55,509] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.0670112e-19 1.9068945e-10 3.8935024e-17 2.7110459e-13 4.0522799e-13
 8.5534730e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:19:55,509] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7072
[2019-04-04 00:19:55,513] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 94.33333333333333, 0.0, 26.0, 25.73526231480488, 0.5418785984617771, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1345800.0000, 
sim time next is 1346400.0000, 
raw observation next is [1.1, 92.0, 88.5, 0.0, 26.0, 25.78259654046934, 0.5445550069742632, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.49307479224376743, 0.92, 0.295, 0.0, 0.6666666666666666, 0.6485497117057782, 0.6815183356580877, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0749573], dtype=float32), 0.9642857]. 
=============================================
[2019-04-04 00:20:00,215] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.2346395e-17 1.4415680e-09 9.3764091e-16 1.1900056e-11 2.1110447e-11
 7.9783502e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:20:00,221] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8093
[2019-04-04 00:20:00,275] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.183333333333333, 92.0, 0.0, 0.0, 26.0, 25.41026722667187, 0.5278991594501431, 0.0, 1.0, 21569.52710518911], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1320600.0000, 
sim time next is 1321200.0000, 
raw observation next is [1.1, 92.0, 0.0, 0.0, 26.0, 25.41176259642193, 0.516568451603789, 0.0, 1.0, 28962.00930823965], 
processed observation next is [1.0, 0.30434782608695654, 0.49307479224376743, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6176468830351608, 0.6721894838679296, 0.0, 1.0, 0.13791433003923642], 
reward next is 0.8621, 
noisyNet noise sample is [array([0.89624643], dtype=float32), -1.0347968]. 
=============================================
[2019-04-04 00:20:00,833] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0604808e-17 5.6514349e-10 1.5017161e-15 4.1700280e-12 4.9377707e-12
 1.1421000e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:20:00,842] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4417
[2019-04-04 00:20:00,888] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.3333333333333333, 93.0, 0.0, 0.0, 26.0, 25.35972591543482, 0.4775027574178108, 0.0, 1.0, 44091.09238731928], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1726800.0000, 
sim time next is 1727400.0000, 
raw observation next is [0.4166666666666667, 92.5, 0.0, 0.0, 26.0, 25.35711319317116, 0.4754876445726777, 0.0, 1.0, 43508.28728098986], 
processed observation next is [1.0, 1.0, 0.47414589104339805, 0.925, 0.0, 0.0, 0.6666666666666666, 0.6130927660975966, 0.6584958815242259, 0.0, 1.0, 0.207182320385666], 
reward next is 0.7928, 
noisyNet noise sample is [array([-1.9745381], dtype=float32), -0.53549325]. 
=============================================
[2019-04-04 00:20:04,899] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.0936110e-19 1.3357657e-12 1.3608067e-17 2.5825400e-13 3.2890617e-13
 3.4182337e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:20:04,899] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9853
[2019-04-04 00:20:04,945] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 26.0, 24.9670653035517, 0.4576913349279345, 1.0, 1.0, 67410.6078861627], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1367400.0000, 
sim time next is 1368000.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 26.0, 24.90458592281151, 0.462702716108531, 1.0, 1.0, 84083.72690126527], 
processed observation next is [1.0, 0.8695652173913043, 0.4764542936288089, 0.96, 0.0, 0.0, 0.6666666666666666, 0.5753821602342925, 0.6542342387028436, 1.0, 1.0, 0.4003986995298346], 
reward next is 0.5996, 
noisyNet noise sample is [array([0.57931054], dtype=float32), 1.502247]. 
=============================================
[2019-04-04 00:20:04,948] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[85.13191 ]
 [80.306854]
 [81.02858 ]
 [82.80491 ]
 [87.61825 ]], R is [[86.9741745 ]
 [86.78343201]
 [86.82653046]
 [86.95826721]
 [87.08868408]].
[2019-04-04 00:20:05,383] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.87023833e-18 8.44909387e-09 1.14630885e-15 2.48818609e-12
 1.14997924e-11 1.06904353e-16 1.00000000e+00], sum to 1.0000
[2019-04-04 00:20:05,384] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1474
[2019-04-04 00:20:05,404] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.2, 94.0, 0.0, 0.0, 26.0, 25.38766949577047, 0.453111672173585, 0.0, 1.0, 63667.65719639319], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1479600.0000, 
sim time next is 1480200.0000, 
raw observation next is [2.2, 94.33333333333334, 0.0, 0.0, 26.0, 25.32673858067381, 0.4493741489283034, 0.0, 1.0, 57490.20287322671], 
processed observation next is [1.0, 0.13043478260869565, 0.5235457063711911, 0.9433333333333335, 0.0, 0.0, 0.6666666666666666, 0.6105615483894841, 0.6497913829761012, 0.0, 1.0, 0.2737628708248891], 
reward next is 0.7262, 
noisyNet noise sample is [array([0.9938948], dtype=float32), -1.1708485]. 
=============================================
[2019-04-04 00:20:07,581] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.5580999e-19 4.2276599e-11 2.5069692e-16 4.3350661e-13 1.0109405e-12
 1.0636689e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:20:07,582] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3871
[2019-04-04 00:20:07,629] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 0.0, 0.0, 26.0, 25.52021423161152, 0.5393788645385545, 0.0, 1.0, 39372.69452971256], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1461600.0000, 
sim time next is 1462200.0000, 
raw observation next is [1.183333333333333, 92.0, 0.0, 0.0, 26.0, 25.48441481318878, 0.4892138130461203, 0.0, 1.0, 54954.58142123643], 
processed observation next is [1.0, 0.9565217391304348, 0.49538319482917825, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6237012344323984, 0.6630712710153734, 0.0, 1.0, 0.2616884829582687], 
reward next is 0.7383, 
noisyNet noise sample is [array([-0.5018951], dtype=float32), -0.086068556]. 
=============================================
[2019-04-04 00:20:11,761] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0070114e-18 6.1672681e-11 1.2297576e-16 3.4865670e-13 1.6491578e-12
 7.4591946e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:20:11,762] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9204
[2019-04-04 00:20:11,782] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 26.0, 25.34763734681208, 0.4814296059223523, 0.0, 1.0, 39093.40110908009], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1469400.0000, 
sim time next is 1470000.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 26.0, 25.33959642884006, 0.4791974912599666, 0.0, 1.0, 37764.99355200079], 
processed observation next is [1.0, 0.0, 0.5069252077562327, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6116330357366717, 0.6597324970866555, 0.0, 1.0, 0.1798333026285752], 
reward next is 0.8202, 
noisyNet noise sample is [array([0.01780676], dtype=float32), 0.85330415]. 
=============================================
[2019-04-04 00:20:11,785] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[83.81227]
 [83.75597]
 [83.70019]
 [83.68828]
 [83.6978 ]], R is [[83.85414886]
 [83.82945251]
 [83.78944397]
 [83.72943878]
 [83.72451019]].
[2019-04-04 00:20:12,116] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [6.7835924e-19 6.1510214e-11 5.0082157e-16 8.1829270e-13 3.6520257e-12
 8.1473160e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:20:12,117] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0906
[2019-04-04 00:20:12,154] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 88.66666666666667, 0.0, 0.0, 26.0, 25.67982727454175, 0.5446334146949029, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1714200.0000, 
sim time next is 1714800.0000, 
raw observation next is [0.9000000000000001, 89.33333333333334, 0.0, 0.0, 26.0, 25.62408920592033, 0.5397870150145042, 0.0, 1.0, 57340.81635273551], 
processed observation next is [1.0, 0.8695652173913043, 0.48753462603878117, 0.8933333333333334, 0.0, 0.0, 0.6666666666666666, 0.6353407671600276, 0.6799290050048348, 0.0, 1.0, 0.27305150644159765], 
reward next is 0.7269, 
noisyNet noise sample is [array([-0.43115115], dtype=float32), 0.74303436]. 
=============================================
[2019-04-04 00:20:16,233] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0358861e-19 2.9341116e-11 1.6902470e-17 1.8514370e-13 1.3680338e-13
 5.6712340e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:20:16,234] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2154
[2019-04-04 00:20:16,258] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.45, 75.0, 0.0, 0.0, 26.0, 25.32673613788881, 0.576251819410613, 0.0, 1.0, 196217.9094192961], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1629000.0000, 
sim time next is 1629600.0000, 
raw observation next is [7.366666666666667, 75.33333333333333, 0.0, 0.0, 26.0, 25.30068056117317, 0.6029087173239025, 0.0, 1.0, 197762.5127839971], 
processed observation next is [1.0, 0.8695652173913043, 0.6666666666666667, 0.7533333333333333, 0.0, 0.0, 0.6666666666666666, 0.6083900467644309, 0.7009695724413009, 0.0, 1.0, 0.9417262513523672], 
reward next is 0.0583, 
noisyNet noise sample is [array([-0.7062611], dtype=float32), -0.896182]. 
=============================================
[2019-04-04 00:20:16,755] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.9717295e-18 1.2709055e-10 2.0323596e-16 5.7855337e-13 4.3395192e-12
 4.4829818e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:20:16,760] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4811
[2019-04-04 00:20:16,769] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.866666666666667, 80.0, 0.0, 0.0, 26.0, 25.69904672548848, 0.5628863582583202, 0.0, 1.0, 33349.12372654081], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1550400.0000, 
sim time next is 1551000.0000, 
raw observation next is [5.683333333333334, 81.0, 0.0, 0.0, 26.0, 25.59568713654894, 0.5537021150848046, 0.0, 1.0, 84001.09449476651], 
processed observation next is [1.0, 0.9565217391304348, 0.6200369344413666, 0.81, 0.0, 0.0, 0.6666666666666666, 0.6329739280457449, 0.6845673716949349, 0.0, 1.0, 0.40000521187984056], 
reward next is 0.6000, 
noisyNet noise sample is [array([-1.3163431], dtype=float32), 1.3067877]. 
=============================================
[2019-04-04 00:20:16,787] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[84.762535]
 [84.84164 ]
 [85.00143 ]
 [85.09731 ]
 [85.16296 ]], R is [[84.50427246]
 [84.50042725]
 [84.65542603]
 [84.80887604]
 [84.96078491]].
[2019-04-04 00:20:21,368] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.5239658e-19 2.4080394e-11 7.8670713e-17 3.4104357e-13 6.3276191e-13
 2.9165088e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:20:21,368] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3623
[2019-04-04 00:20:21,393] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.1, 82.66666666666667, 0.0, 0.0, 26.0, 25.67220544148928, 0.5864123950220548, 0.0, 1.0, 70871.9775822074], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1637400.0000, 
sim time next is 1638000.0000, 
raw observation next is [7.2, 82.0, 0.0, 0.0, 26.0, 25.5898401825248, 0.583613518286071, 0.0, 1.0, 93418.84051648348], 
processed observation next is [1.0, 1.0, 0.662049861495845, 0.82, 0.0, 0.0, 0.6666666666666666, 0.6324866818770666, 0.6945378394286904, 0.0, 1.0, 0.4448516215070642], 
reward next is 0.5551, 
noisyNet noise sample is [array([-0.1557564], dtype=float32), -0.6239584]. 
=============================================
[2019-04-04 00:20:21,418] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[83.77569 ]
 [83.70257 ]
 [83.812744]
 [83.90207 ]
 [83.998665]], R is [[83.8217392 ]
 [83.64603424]
 [83.80957794]
 [83.97148132]
 [84.13176727]].
[2019-04-04 00:20:23,991] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.05304018e-16 1.01871205e-08 2.65667191e-14 7.39819525e-11
 8.27462543e-11 1.15754485e-15 1.00000000e+00], sum to 1.0000
[2019-04-04 00:20:23,991] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5136
[2019-04-04 00:20:24,049] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 83.0, 119.0, 0.0, 26.0, 24.96439456273545, 0.3463579267881324, 0.0, 1.0, 54828.90168155248], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1776600.0000, 
sim time next is 1777200.0000, 
raw observation next is [-2.8, 83.0, 115.6666666666667, 0.0, 26.0, 24.96167245328473, 0.3575402757344004, 0.0, 1.0, 49751.14215381105], 
processed observation next is [0.0, 0.5652173913043478, 0.38504155124653744, 0.83, 0.38555555555555565, 0.0, 0.6666666666666666, 0.5801393711070609, 0.6191800919114668, 0.0, 1.0, 0.23691020073243357], 
reward next is 0.7631, 
noisyNet noise sample is [array([2.3879986], dtype=float32), -1.013515]. 
=============================================
[2019-04-04 00:20:27,825] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.8930393e-18 1.3663650e-09 1.3420228e-15 2.1448195e-12 2.2908358e-11
 6.4812572e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:20:27,825] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7893
[2019-04-04 00:20:27,849] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.4, 91.66666666666667, 0.0, 0.0, 26.0, 25.34224321497358, 0.455689660952909, 0.0, 1.0, 43483.40263982167], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1732800.0000, 
sim time next is 1733400.0000, 
raw observation next is [0.35, 91.5, 0.0, 0.0, 26.0, 25.32528733129514, 0.456620992497031, 0.0, 1.0, 43096.09348582705], 
processed observation next is [0.0, 0.043478260869565216, 0.47229916897506935, 0.915, 0.0, 0.0, 0.6666666666666666, 0.6104406109412617, 0.6522069974990103, 0.0, 1.0, 0.20521949278965262], 
reward next is 0.7948, 
noisyNet noise sample is [array([0.36981615], dtype=float32), 0.64835477]. 
=============================================
[2019-04-04 00:20:51,013] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1174615e-18 4.4612827e-10 9.9674701e-17 5.6545957e-13 1.7940200e-12
 2.2673798e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:20:51,014] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1478
[2019-04-04 00:20:51,098] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 79.0, 133.3333333333333, 0.0, 26.0, 26.13050848547026, 0.4760679570549104, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2037000.0000, 
sim time next is 2037600.0000, 
raw observation next is [-3.9, 79.0, 126.0, 0.0, 26.0, 26.29660625242786, 0.4902889879887155, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3545706371191136, 0.79, 0.42, 0.0, 0.6666666666666666, 0.6913838543689884, 0.6634296626629052, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.40348807], dtype=float32), 0.598144]. 
=============================================
[2019-04-04 00:21:14,610] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.4071390e-17 1.2211372e-09 2.5296997e-15 6.2264091e-12 2.8500886e-11
 1.0437136e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 00:21:14,615] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9462
[2019-04-04 00:21:14,639] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.100000000000001, 68.5, 0.0, 0.0, 26.0, 25.2830008714883, 0.4138140058949927, 0.0, 1.0, 45955.38822626014], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2236200.0000, 
sim time next is 2236800.0000, 
raw observation next is [-5.2, 69.0, 0.0, 0.0, 26.0, 25.29225120302735, 0.4128961862251061, 0.0, 1.0, 45578.27459623531], 
processed observation next is [1.0, 0.9130434782608695, 0.31855955678670367, 0.69, 0.0, 0.0, 0.6666666666666666, 0.6076876002522793, 0.6376320620750354, 0.0, 1.0, 0.21703940283921574], 
reward next is 0.7830, 
noisyNet noise sample is [array([0.42324033], dtype=float32), 1.8047715]. 
=============================================
[2019-04-04 00:21:31,382] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.11052490e-15 9.93325813e-08 1.13373964e-13 7.38687236e-11
 6.91966096e-10 2.93884519e-14 9.99999881e-01], sum to 1.0000
[2019-04-04 00:21:31,383] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1064
[2019-04-04 00:21:31,397] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.0, 60.16666666666666, 0.0, 0.0, 26.0, 23.18829477966062, -0.1628461744738972, 0.0, 1.0, 44068.93863906941], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2441400.0000, 
sim time next is 2442000.0000, 
raw observation next is [-9.100000000000001, 60.33333333333334, 0.0, 0.0, 26.0, 23.15244956823958, -0.1726565225755133, 0.0, 1.0, 44042.25480666911], 
processed observation next is [0.0, 0.2608695652173913, 0.21052631578947364, 0.6033333333333334, 0.0, 0.0, 0.6666666666666666, 0.42937079735329825, 0.4424478258081623, 0.0, 1.0, 0.20972502288890052], 
reward next is 0.7903, 
noisyNet noise sample is [array([1.0445704], dtype=float32), -0.6941513]. 
=============================================
[2019-04-04 00:21:31,423] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[73.445526]
 [73.539085]
 [73.63485 ]
 [73.7196  ]
 [73.799126]], R is [[73.41033173]
 [73.46637726]
 [73.5216217 ]
 [73.57606506]
 [73.62975311]].
[2019-04-04 00:21:34,223] A3C_AGENT_WORKER-Thread-7 INFO:Evaluating...
[2019-04-04 00:21:34,225] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 00:21:34,226] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 00:21:34,226] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:21:34,226] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 00:21:34,226] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:21:34,226] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:21:34,230] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run18
[2019-04-04 00:21:34,230] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run18
[2019-04-04 00:21:34,273] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run18
[2019-04-04 00:22:01,221] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.3465125], dtype=float32), 0.25686282]
[2019-04-04 00:22:01,222] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-10.4, 51.0, 0.0, 0.0, 26.0, 25.14499815105681, 0.1987324180448634, 0.0, 1.0, 59998.79482489491]
[2019-04-04 00:22:01,222] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 00:22:01,223] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [7.2555116e-16 2.4745228e-08 2.4464327e-14 4.7845942e-11 2.0949133e-10
 1.6807136e-15 1.0000000e+00], sampled 0.8778055450120554
[2019-04-04 00:22:21,219] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.3465125], dtype=float32), 0.25686282]
[2019-04-04 00:22:21,219] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [7.766666666666667, 77.33333333333334, 0.0, 0.0, 26.0, 25.49352018806353, 0.541071571427124, 0.0, 1.0, 44677.64371401613]
[2019-04-04 00:22:21,220] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 00:22:21,220] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [2.2650283e-18 5.7580896e-09 3.6341742e-16 9.3057625e-13 7.4001109e-12
 2.6290945e-17 1.0000000e+00], sampled 0.86211311158008
[2019-04-04 00:22:22,985] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.3465125], dtype=float32), 0.25686282]
[2019-04-04 00:22:22,985] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [13.8, 49.0, 145.0, 0.0, 26.0, 27.41829750723956, 0.8731876628273803, 1.0, 1.0, 0.0]
[2019-04-04 00:22:22,985] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 00:22:22,986] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [5.6709286e-20 6.2018661e-11 2.6192728e-18 6.5336957e-14 1.4912424e-13
 7.3607914e-20 1.0000000e+00], sampled 0.44870013722087865
[2019-04-04 00:23:25,629] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.3465125], dtype=float32), 0.25686282]
[2019-04-04 00:23:25,629] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-0.179877587, 52.16066457333334, 0.0, 0.0, 26.0, 25.21318131934686, 0.3657838500962386, 0.0, 1.0, 52348.58880518761]
[2019-04-04 00:23:25,629] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 00:23:25,630] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.32841350e-17 5.52264945e-09 1.26573489e-15 4.24630947e-12
 2.17300657e-11 1.10769265e-16 1.00000000e+00], sampled 0.7882843480396154
[2019-04-04 00:24:00,892] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 00:24:29,072] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.7639 263389580.4542 1552.0485
[2019-04-04 00:24:36,808] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 00:24:37,842] A3C_AGENT_WORKER-Thread-7 INFO:Global step: 1700000, evaluation results [1700000.0, 7241.763902599205, 263389580.45416775, 1552.0485098418105, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 00:24:45,142] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.7635480e-17 1.2225658e-09 7.8954906e-15 1.0968324e-11 2.3505350e-11
 2.0058172e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:24:45,144] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0016
[2019-04-04 00:24:45,183] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.333333333333333, 71.0, 0.0, 0.0, 26.0, 24.80390855491619, 0.2849329283326669, 0.0, 1.0, 44404.60226436695], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2677200.0000, 
sim time next is 2677800.0000, 
raw observation next is [-6.666666666666667, 71.5, 0.0, 0.0, 26.0, 24.76123577651911, 0.2715041754771783, 0.0, 1.0, 44401.07385268424], 
processed observation next is [1.0, 1.0, 0.27793167128347185, 0.715, 0.0, 0.0, 0.6666666666666666, 0.563436314709926, 0.5905013918257261, 0.0, 1.0, 0.2114336850127821], 
reward next is 0.7886, 
noisyNet noise sample is [array([-1.4238535], dtype=float32), -1.9777412]. 
=============================================
[2019-04-04 00:25:01,366] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.3566387e-19 3.2049297e-11 2.9142969e-17 1.0458600e-12 2.1642861e-12
 9.3559928e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:25:01,366] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7753
[2019-04-04 00:25:01,404] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.666666666666667, 63.16666666666667, 112.6666666666667, 793.0, 26.0, 25.9332442247323, 0.4757844382074192, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2722200.0000, 
sim time next is 2722800.0000, 
raw observation next is [-7.333333333333334, 62.33333333333334, 112.8333333333333, 796.0, 26.0, 25.9437117050253, 0.4666115158403307, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.2594644506001847, 0.6233333333333334, 0.376111111111111, 0.8795580110497238, 0.6666666666666666, 0.6619759754187751, 0.655537171946777, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6779157], dtype=float32), -1.0732403]. 
=============================================
[2019-04-04 00:25:03,808] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [7.1302810e-17 2.8872277e-08 9.7572944e-15 3.1352053e-11 2.3695535e-11
 4.4560476e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 00:25:03,808] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.4903
[2019-04-04 00:25:03,842] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.0, 64.0, 0.0, 0.0, 26.0, 24.22830141162368, 0.0949242276940074, 0.0, 1.0, 41278.50875527797], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2785800.0000, 
sim time next is 2786400.0000, 
raw observation next is [-7.0, 64.0, 0.0, 0.0, 26.0, 24.24598780273309, 0.09743972765427285, 0.0, 1.0, 41264.94182764633], 
processed observation next is [1.0, 0.2608695652173913, 0.2686980609418283, 0.64, 0.0, 0.0, 0.6666666666666666, 0.5204989835610908, 0.532479909218091, 0.0, 1.0, 0.19649972298879206], 
reward next is 0.8035, 
noisyNet noise sample is [array([-0.6019884], dtype=float32), 0.30670056]. 
=============================================
[2019-04-04 00:25:12,981] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [9.9839759e-19 4.6863544e-11 1.7373616e-16 1.1173498e-12 2.5221685e-12
 1.8105974e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:25:12,981] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7408
[2019-04-04 00:25:13,001] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 100.0, 151.6666666666667, 0.0, 26.0, 25.39682945652768, 0.338529879753328, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2900400.0000, 
sim time next is 2901000.0000, 
raw observation next is [2.0, 100.0, 139.3333333333333, 0.0, 26.0, 25.41603693524968, 0.3416881387318187, 1.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.5652173913043478, 0.518005540166205, 1.0, 0.46444444444444427, 0.0, 0.6666666666666666, 0.6180030779374732, 0.6138960462439396, 1.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-3.0820434], dtype=float32), -1.2505889]. 
=============================================
[2019-04-04 00:25:13,028] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[83.02076]
 [83.31641]
 [83.39057]
 [83.55106]
 [83.81685]], R is [[82.78420258]
 [82.95635986]
 [83.12680054]
 [83.29553223]
 [83.46257782]].
[2019-04-04 00:25:17,259] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.9143583e-17 1.7268610e-10 1.5476254e-15 6.6857110e-12 1.1222714e-11
 1.1394542e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:25:17,259] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.3775
[2019-04-04 00:25:17,325] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 78.0, 0.0, 0.0, 26.0, 24.91735521974622, 0.4025858968592665, 0.0, 1.0, 49588.62481319209], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2923200.0000, 
sim time next is 2923800.0000, 
raw observation next is [-1.0, 79.16666666666667, 0.0, 0.0, 26.0, 24.95814174938439, 0.4062593651049748, 1.0, 1.0, 18966.80501071285], 
processed observation next is [1.0, 0.8695652173913043, 0.4349030470914128, 0.7916666666666667, 0.0, 0.0, 0.6666666666666666, 0.5798451457820324, 0.6354197883683249, 1.0, 1.0, 0.09031811909863262], 
reward next is 0.9097, 
noisyNet noise sample is [array([0.31757843], dtype=float32), 0.17726377]. 
=============================================
[2019-04-04 00:25:18,506] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.8464844e-16 6.6921409e-08 3.4817404e-14 4.1751873e-11 2.9427608e-10
 7.2412194e-15 9.9999988e-01], sum to 1.0000
[2019-04-04 00:25:18,506] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2399
[2019-04-04 00:25:18,624] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 63.16666666666667, 55.66666666666668, 262.6666666666667, 26.0, 23.59627182223154, -0.01032683484091365, 0.0, 1.0, 40458.15007326171], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3053400.0000, 
sim time next is 3054000.0000, 
raw observation next is [-6.0, 62.33333333333334, 69.33333333333334, 310.8333333333334, 26.0, 23.66078790069373, 0.07859192734328342, 0.0, 1.0, 202451.661015601], 
processed observation next is [0.0, 0.34782608695652173, 0.296398891966759, 0.6233333333333334, 0.23111111111111116, 0.343462246777164, 0.6666666666666666, 0.4717323250578109, 0.5261973091144277, 0.0, 1.0, 0.9640555286457191], 
reward next is 0.0359, 
noisyNet noise sample is [array([0.25341678], dtype=float32), -1.346291]. 
=============================================
[2019-04-04 00:25:18,631] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[76.122406]
 [75.80199 ]
 [75.23126 ]
 [74.3481  ]
 [73.24995 ]], R is [[76.36054993]
 [76.40428925]
 [76.44669342]
 [76.48813629]
 [76.52893066]].
[2019-04-04 00:25:20,856] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0918433e-15 3.3047729e-07 1.5787046e-13 1.5448949e-10 6.7000766e-10
 7.3802641e-15 9.9999964e-01], sum to 1.0000
[2019-04-04 00:25:20,856] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3344
[2019-04-04 00:25:20,872] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 23.93577044360276, 0.03571129357473884, 0.0, 1.0, 40210.89980072431], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3041400.0000, 
sim time next is 3042000.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 23.90984109903956, 0.02958920391666374, 0.0, 1.0, 40198.08265195469], 
processed observation next is [0.0, 0.21739130434782608, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.49248675825329674, 0.5098630679722213, 0.0, 1.0, 0.19141944119978424], 
reward next is 0.8086, 
noisyNet noise sample is [array([-1.3965452], dtype=float32), -0.45247492]. 
=============================================
[2019-04-04 00:25:20,876] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[74.945274]
 [74.95446 ]
 [74.96929 ]
 [74.99253 ]
 [75.030045]], R is [[74.99604797]
 [75.05460358]
 [75.11258698]
 [75.17006683]
 [75.22711945]].
[2019-04-04 00:25:36,866] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [3.9287704e-18 1.8452149e-09 6.6662658e-16 1.5762719e-12 7.0517463e-12
 7.9810554e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:25:36,866] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.1069
[2019-04-04 00:25:36,882] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.8, 100.0, 0.0, 0.0, 26.0, 25.24630562746479, 0.2965724311340123, 0.0, 1.0, 53902.00188430025], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3126600.0000, 
sim time next is 3127200.0000, 
raw observation next is [2.866666666666667, 100.0, 0.0, 0.0, 26.0, 25.26377895063481, 0.2957462034563267, 0.0, 1.0, 53836.05666584253], 
processed observation next is [1.0, 0.17391304347826086, 0.5420129270544783, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6053149125529007, 0.5985820678187755, 0.0, 1.0, 0.25636217459925015], 
reward next is 0.7436, 
noisyNet noise sample is [array([2.0418406], dtype=float32), -0.83757627]. 
=============================================
[2019-04-04 00:25:38,112] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.7422920e-17 3.8685277e-09 1.2871150e-15 7.2294458e-12 3.0071830e-11
 9.7760989e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:25:38,117] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6394
[2019-04-04 00:25:38,147] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 100.0, 0.0, 0.0, 26.0, 25.3673781325299, 0.3279947354101889, 0.0, 1.0, 61303.26493639628], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3107400.0000, 
sim time next is 3108000.0000, 
raw observation next is [0.0, 100.0, 0.0, 0.0, 26.0, 25.32411882882785, 0.3255890385534634, 0.0, 1.0, 51076.45258798439], 
processed observation next is [0.0, 1.0, 0.46260387811634357, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6103432357356541, 0.6085296795178211, 0.0, 1.0, 0.24322120279992568], 
reward next is 0.7568, 
noisyNet noise sample is [array([0.95440525], dtype=float32), 0.43048272]. 
=============================================
[2019-04-04 00:25:38,172] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[83.71698 ]
 [83.53368 ]
 [83.518486]
 [83.58788 ]
 [83.63575 ]], R is [[83.7720871 ]
 [83.6424408 ]
 [83.66033173]
 [83.73438263]
 [83.74977112]].
[2019-04-04 00:25:38,674] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.4995594e-20 1.6410444e-11 1.7425379e-17 9.2303809e-14 4.7075099e-13
 6.1237176e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 00:25:38,674] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7888
[2019-04-04 00:25:38,691] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.666666666666667, 100.0, 0.0, 0.0, 26.0, 26.96058116415801, 0.8595270516187212, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3175800.0000, 
sim time next is 3176400.0000, 
raw observation next is [5.333333333333333, 100.0, 0.0, 0.0, 26.0, 26.8723854724347, 0.847197045528478, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.6103416435826409, 1.0, 0.0, 0.0, 0.6666666666666666, 0.739365456036225, 0.7823990151761593, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.00229039], dtype=float32), -1.4639627]. 
=============================================
[2019-04-04 00:25:40,449] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.2545291e-16 1.7382902e-08 2.4982078e-14 3.0329315e-11 6.1187957e-11
 1.6493252e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 00:25:40,463] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4941
[2019-04-04 00:25:40,494] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.0, 76.0, 0.0, 0.0, 26.0, 24.00304061050497, 0.1044438493067807, 0.0, 1.0, 43905.13170116688], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3306600.0000, 
sim time next is 3307200.0000, 
raw observation next is [-11.0, 76.0, 0.0, 0.0, 26.0, 23.91304142896867, 0.09461149700302111, 0.0, 1.0, 43971.82634365459], 
processed observation next is [1.0, 0.2608695652173913, 0.15789473684210528, 0.76, 0.0, 0.0, 0.6666666666666666, 0.4927534524140557, 0.5315371656676737, 0.0, 1.0, 0.20938964925549805], 
reward next is 0.7906, 
noisyNet noise sample is [array([2.3051147], dtype=float32), -1.4356161]. 
=============================================
[2019-04-04 00:25:41,382] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.1490728e-17 1.0155469e-10 3.4536426e-15 9.8778624e-12 6.2608018e-12
 8.5955123e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:25:41,382] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8449
[2019-04-04 00:25:41,420] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.666666666666667, 88.33333333333333, 0.0, 0.0, 26.0, 25.54293728127535, 0.5416550425477212, 0.0, 1.0, 61589.93179350624], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3274800.0000, 
sim time next is 3275400.0000, 
raw observation next is [-5.833333333333333, 90.16666666666667, 0.0, 0.0, 26.0, 25.47357703164096, 0.5392773512662085, 0.0, 1.0, 85564.41175837441], 
processed observation next is [1.0, 0.9130434782608695, 0.30101569713758086, 0.9016666666666667, 0.0, 0.0, 0.6666666666666666, 0.62279808597008, 0.6797591170887362, 0.0, 1.0, 0.4074495798017829], 
reward next is 0.5926, 
noisyNet noise sample is [array([0.22253017], dtype=float32), -0.15459]. 
=============================================
[2019-04-04 00:25:50,930] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.6235702e-19 5.7926647e-10 9.6365483e-17 9.1071324e-13 1.8656706e-12
 1.4509209e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:25:50,931] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7985
[2019-04-04 00:25:50,975] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.9999999999999999, 52.0, 100.6666666666667, 675.6666666666666, 26.0, 26.42454892195974, 0.5762938295222073, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3404400.0000, 
sim time next is 3405000.0000, 
raw observation next is [1.5, 50.0, 102.3333333333333, 693.3333333333334, 26.0, 26.48737067929524, 0.5839664800785161, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.5041551246537397, 0.5, 0.341111111111111, 0.7661141804788214, 0.6666666666666666, 0.7072808899412699, 0.6946554933595054, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4901586], dtype=float32), -0.6634317]. 
=============================================
[2019-04-04 00:25:50,999] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[89.03254 ]
 [88.950806]
 [88.80857 ]
 [88.795395]
 [88.83004 ]], R is [[89.07939148]
 [89.18859863]
 [89.29671478]
 [89.40374756]
 [89.50971222]].
[2019-04-04 00:25:52,387] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2743130e-18 3.4974607e-11 4.1182140e-17 4.0467881e-13 2.7431508e-12
 3.2461111e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:25:52,387] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6373
[2019-04-04 00:25:52,405] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 67.0, 68.66666666666666, 576.6666666666667, 26.0, 25.40827773801312, 0.5397824388132918, 1.0, 1.0, 96132.37790658609], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3427800.0000, 
sim time next is 3428400.0000, 
raw observation next is [2.0, 67.0, 64.83333333333333, 544.8333333333333, 26.0, 25.80720855871357, 0.5913245051941072, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.518005540166205, 0.67, 0.2161111111111111, 0.602025782688766, 0.6666666666666666, 0.6506007132261308, 0.6971081683980357, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.75216293], dtype=float32), -0.72374016]. 
=============================================
[2019-04-04 00:25:56,249] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.0889347e-20 4.2722756e-11 3.9133046e-18 6.5491666e-14 8.0956335e-13
 1.9411819e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 00:25:56,252] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3825
[2019-04-04 00:25:56,261] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 60.0, 99.66666666666666, 754.3333333333333, 26.0, 26.67866088023206, 0.6741252954594135, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3768600.0000, 
sim time next is 3769200.0000, 
raw observation next is [0.0, 60.0, 96.5, 743.5, 26.0, 26.72314536089698, 0.6799097403920027, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.46260387811634357, 0.6, 0.32166666666666666, 0.8215469613259668, 0.6666666666666666, 0.7269287800747483, 0.7266365801306676, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.91721654], dtype=float32), 0.09773229]. 
=============================================
[2019-04-04 00:25:58,557] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.0727768e-17 1.4362801e-08 6.2383706e-15 2.3891256e-12 5.0583700e-11
 1.3193474e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 00:25:58,559] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6950
[2019-04-04 00:25:58,575] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.1666666666666666, 71.16666666666666, 0.0, 0.0, 26.0, 25.24967894764472, 0.3885077954651557, 0.0, 1.0, 42275.45729647216], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3473400.0000, 
sim time next is 3474000.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.22227008058188, 0.3830110376495086, 0.0, 1.0, 41941.58722307802], 
processed observation next is [1.0, 0.21739130434782608, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6018558400484899, 0.6276703458831695, 0.0, 1.0, 0.19972184391941916], 
reward next is 0.8003, 
noisyNet noise sample is [array([0.29961404], dtype=float32), 0.3356188]. 
=============================================
[2019-04-04 00:25:58,588] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[82.74309]
 [82.78863]
 [82.84267]
 [82.9068 ]
 [82.90222]], R is [[82.60826111]
 [82.58087158]
 [82.54698944]
 [82.49087524]
 [82.37245178]].
[2019-04-04 00:26:00,353] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.0853672e-19 3.4686882e-11 7.6725833e-18 5.5739311e-13 6.7051058e-13
 2.1830553e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:26:00,354] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5793
[2019-04-04 00:26:00,367] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 49.0, 83.66666666666667, 660.0, 26.0, 26.13836256634142, 0.6832205617690782, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3512400.0000, 
sim time next is 3513000.0000, 
raw observation next is [3.0, 49.0, 79.33333333333333, 633.0, 26.0, 26.47633653554588, 0.7138646554304161, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5457063711911359, 0.49, 0.2644444444444444, 0.6994475138121546, 0.6666666666666666, 0.7063613779621566, 0.7379548851434721, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8789752], dtype=float32), 0.3871023]. 
=============================================
[2019-04-04 00:26:00,420] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[89.39585 ]
 [89.65844 ]
 [89.905846]
 [90.0826  ]
 [90.21427 ]], R is [[89.2368927 ]
 [89.3445282 ]
 [89.36212921]
 [89.46850586]
 [89.57382202]].
[2019-04-04 00:26:05,819] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.0362775e-17 1.4043955e-08 2.3255487e-15 2.9495364e-11 4.9206150e-11
 3.7836745e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 00:26:05,820] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7717
[2019-04-04 00:26:05,830] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 54.16666666666666, 0.0, 0.0, 26.0, 25.31789215335619, 0.367592918719642, 0.0, 1.0, 38574.90154910761], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3625800.0000, 
sim time next is 3626400.0000, 
raw observation next is [1.0, 48.33333333333334, 0.0, 0.0, 26.0, 25.31639464522011, 0.3707691383639962, 0.0, 1.0, 38478.60943869619], 
processed observation next is [0.0, 1.0, 0.4903047091412743, 0.48333333333333345, 0.0, 0.0, 0.6666666666666666, 0.6096995537683426, 0.6235897127879987, 0.0, 1.0, 0.1832314735176009], 
reward next is 0.8168, 
noisyNet noise sample is [array([-1.0308968], dtype=float32), -0.09830079]. 
=============================================
[2019-04-04 00:26:07,793] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.4819152e-20 1.2433393e-10 1.8273199e-18 1.1657973e-13 4.5698081e-13
 1.3268405e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:26:07,801] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2823
[2019-04-04 00:26:07,817] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 55.33333333333334, 115.8333333333333, 820.8333333333334, 26.0, 25.62551801954501, 0.5505783075491518, 1.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3500400.0000, 
sim time next is 3501000.0000, 
raw observation next is [2.0, 54.5, 116.0, 823.0, 26.0, 25.82078561305109, 0.5748248955394444, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.518005540166205, 0.545, 0.38666666666666666, 0.9093922651933701, 0.6666666666666666, 0.6517321344209241, 0.6916082985131481, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.16534846], dtype=float32), -1.4930428]. 
=============================================
[2019-04-04 00:26:07,860] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[89.99013 ]
 [90.018135]
 [89.9784  ]
 [90.06365 ]
 [90.117226]], R is [[89.9317627 ]
 [89.9434967 ]
 [89.73078918]
 [89.83348083]
 [89.93515015]].
[2019-04-04 00:26:13,664] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.2553939e-18 5.8649280e-10 4.2065109e-16 1.3461779e-12 8.8972735e-12
 5.4173568e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:26:13,665] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6924
[2019-04-04 00:26:13,672] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.0, 45.66666666666667, 101.1666666666667, 764.1666666666667, 26.0, 25.50049207200338, 0.4858114260102029, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3681600.0000, 
sim time next is 3682200.0000, 
raw observation next is [6.0, 46.33333333333334, 98.33333333333334, 752.3333333333333, 26.0, 25.49571053286211, 0.4852664115450744, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.6288088642659281, 0.46333333333333343, 0.32777777777777783, 0.8313075506445672, 0.6666666666666666, 0.6246425444051757, 0.6617554705150248, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.10244683], dtype=float32), 0.990318]. 
=============================================
[2019-04-04 00:26:14,889] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.7690566e-18 1.1574954e-09 5.8024335e-16 6.2223612e-12 7.0716267e-12
 3.7793303e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:26:14,890] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3948
[2019-04-04 00:26:14,915] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 63.0, 0.0, 0.0, 26.0, 25.41548259371854, 0.4591830747408079, 0.0, 1.0, 28987.82009188908], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3700800.0000, 
sim time next is 3701400.0000, 
raw observation next is [2.833333333333333, 62.83333333333333, 0.0, 0.0, 26.0, 25.60354462122162, 0.4708633506296424, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.541089566020314, 0.6283333333333333, 0.0, 0.0, 0.6666666666666666, 0.6336287184351349, 0.6569544502098809, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.80731595], dtype=float32), -0.82785857]. 
=============================================
[2019-04-04 00:26:16,107] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.2860051e-16 3.4758592e-08 4.0167014e-15 1.2280146e-11 8.9746058e-11
 4.7582626e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 00:26:16,111] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.2164
[2019-04-04 00:26:16,136] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.54077134286621, 0.40788440018609, 0.0, 1.0, 18746.09412486411], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3617400.0000, 
sim time next is 3618000.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.50356455952674, 0.40226706980678, 0.0, 1.0, 37384.40733853273], 
processed observation next is [0.0, 0.9130434782608695, 0.4349030470914128, 0.42, 0.0, 0.0, 0.6666666666666666, 0.6252970466272284, 0.6340890232689267, 0.0, 1.0, 0.17802098732634633], 
reward next is 0.8220, 
noisyNet noise sample is [array([-0.56996423], dtype=float32), -0.9377429]. 
=============================================
[2019-04-04 00:26:16,142] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[80.25549]
 [80.27839]
 [80.43721]
 [80.66077]
 [80.80913]], R is [[80.38661957]
 [80.49349213]
 [80.68856049]
 [80.88167572]
 [81.07286072]].
[2019-04-04 00:26:17,448] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.1040745e-18 1.0120937e-09 1.6757065e-16 5.7701934e-13 8.6664374e-12
 1.0192672e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:26:17,449] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5913
[2019-04-04 00:26:17,469] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.5, 54.5, 64.0, 522.0, 26.0, 25.49433936147977, 0.4675428400757573, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3688200.0000, 
sim time next is 3688800.0000, 
raw observation next is [4.333333333333334, 56.0, 55.83333333333334, 462.5, 26.0, 25.47945002848898, 0.4568034243897802, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.58264081255771, 0.56, 0.18611111111111114, 0.511049723756906, 0.6666666666666666, 0.6232875023740817, 0.6522678081299268, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9860385], dtype=float32), -1.1458639]. 
=============================================
[2019-04-04 00:26:23,999] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.692363e-17 5.209527e-09 8.173446e-16 2.214318e-12 6.696064e-11
 4.248713e-17 1.000000e+00], sum to 1.0000
[2019-04-04 00:26:23,999] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9457
[2019-04-04 00:26:24,080] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.166666666666667, 62.16666666666667, 0.0, 0.0, 26.0, 25.72351749553625, 0.4454905924076447, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3703800.0000, 
sim time next is 3704400.0000, 
raw observation next is [2.0, 62.0, 0.0, 0.0, 26.0, 25.67541099334624, 0.4376604230534706, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.9130434782608695, 0.518005540166205, 0.62, 0.0, 0.0, 0.6666666666666666, 0.6396175827788534, 0.6458868076844902, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.00936252], dtype=float32), -0.8950986]. 
=============================================
[2019-04-04 00:26:25,333] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.2682720e-17 1.6266247e-08 8.2967496e-15 1.1492930e-11 7.6243234e-11
 1.7787791e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 00:26:25,336] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6623
[2019-04-04 00:26:25,357] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.666666666666667, 69.0, 0.0, 0.0, 26.0, 25.39165496539485, 0.3927933686152232, 0.0, 1.0, 30903.77033816755], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3901200.0000, 
sim time next is 3901800.0000, 
raw observation next is [-2.833333333333333, 70.0, 0.0, 0.0, 26.0, 25.36530403340042, 0.3838554382341715, 0.0, 1.0, 45320.2063180511], 
processed observation next is [1.0, 0.13043478260869565, 0.3841181902123731, 0.7, 0.0, 0.0, 0.6666666666666666, 0.6137753361167017, 0.6279518127447238, 0.0, 1.0, 0.21581050627643383], 
reward next is 0.7842, 
noisyNet noise sample is [array([-0.00363304], dtype=float32), -0.637624]. 
=============================================
[2019-04-04 00:26:27,591] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.8226237e-18 3.1232458e-10 7.3771106e-16 1.7007939e-12 6.3908028e-12
 2.0491816e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:26:27,600] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7044
[2019-04-04 00:26:27,611] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 57.5, 0.0, 0.0, 26.0, 25.37166594154025, 0.4469849254585362, 0.0, 1.0, 150280.4522877796], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3882600.0000, 
sim time next is 3883200.0000, 
raw observation next is [-1.0, 58.33333333333334, 0.0, 0.0, 26.0, 25.26587303829786, 0.4508119074885713, 0.0, 1.0, 91782.9292964003], 
processed observation next is [1.0, 0.9565217391304348, 0.4349030470914128, 0.5833333333333335, 0.0, 0.0, 0.6666666666666666, 0.6054894198581549, 0.6502706358295237, 0.0, 1.0, 0.43706156807809665], 
reward next is 0.5629, 
noisyNet noise sample is [array([0.76179826], dtype=float32), 0.13494377]. 
=============================================
[2019-04-04 00:26:31,223] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.1400243e-16 2.1943503e-09 5.3163138e-15 1.2638124e-11 6.5793003e-11
 4.2672137e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:26:31,223] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3396
[2019-04-04 00:26:31,241] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.0, 49.0, 0.0, 0.0, 26.0, 25.41331197138265, 0.4958125170503582, 0.0, 1.0, 60129.24111769332], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3967200.0000, 
sim time next is 3967800.0000, 
raw observation next is [-8.166666666666668, 49.66666666666667, 0.0, 0.0, 26.0, 25.40039873450786, 0.4470061553218342, 0.0, 1.0, 57014.64574861499], 
processed observation next is [1.0, 0.9565217391304348, 0.2363804247460757, 0.4966666666666667, 0.0, 0.0, 0.6666666666666666, 0.6166998945423217, 0.6490020517739447, 0.0, 1.0, 0.2714983130886428], 
reward next is 0.7285, 
noisyNet noise sample is [array([0.24474992], dtype=float32), -0.08230135]. 
=============================================
[2019-04-04 00:26:32,443] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.0168941e-17 1.1845792e-08 3.3380510e-15 2.6679477e-12 8.9881901e-11
 2.2359965e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 00:26:32,445] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2368
[2019-04-04 00:26:32,536] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.666666666666667, 43.33333333333334, 102.6666666666667, 602.6666666666667, 26.0, 25.56444776750092, 0.4193915110162199, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4180200.0000, 
sim time next is 4180800.0000, 
raw observation next is [-3.333333333333333, 41.66666666666667, 105.3333333333333, 631.3333333333333, 26.0, 25.52410676820093, 0.422973496585226, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.37026777469990774, 0.41666666666666674, 0.351111111111111, 0.6976058931860036, 0.6666666666666666, 0.6270088973500775, 0.6409911655284087, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4262675], dtype=float32), 0.12228805]. 
=============================================
[2019-04-04 00:26:34,163] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.3933539e-19 1.4465912e-10 9.1484397e-18 8.1907774e-14 2.0775371e-13
 9.2130104e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 00:26:34,164] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3325
[2019-04-04 00:26:34,191] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.333333333333333, 61.83333333333333, 104.0, 711.6666666666666, 26.0, 26.39740304411487, 0.5804476922698273, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3837000.0000, 
sim time next is 3837600.0000, 
raw observation next is [-2.0, 60.0, 105.5, 727.5, 26.0, 26.42990178295419, 0.590694916482538, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.40720221606648205, 0.6, 0.3516666666666667, 0.8038674033149171, 0.6666666666666666, 0.7024918152461824, 0.6968983054941793, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0777781], dtype=float32), 1.1371864]. 
=============================================
[2019-04-04 00:26:39,393] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.1047123e-18 2.4021596e-10 1.7353546e-16 5.9037410e-13 2.4722063e-12
 3.5306571e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:26:39,395] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9782
[2019-04-04 00:26:39,446] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.666666666666666, 43.66666666666667, 0.0, 0.0, 26.0, 25.5749373153522, 0.5331132582481731, 1.0, 1.0, 79609.53915348771], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3958800.0000, 
sim time next is 3959400.0000, 
raw observation next is [-6.833333333333334, 44.33333333333333, 0.0, 0.0, 26.0, 25.56894520502246, 0.5356341692579053, 0.0, 1.0, 57114.12798528703], 
processed observation next is [1.0, 0.8260869565217391, 0.27331486611265005, 0.4433333333333333, 0.0, 0.0, 0.6666666666666666, 0.6307454337518715, 0.6785447230859685, 0.0, 1.0, 0.27197203802517633], 
reward next is 0.7280, 
noisyNet noise sample is [array([1.0147666], dtype=float32), -1.145904]. 
=============================================
[2019-04-04 00:26:41,664] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0385941e-16 1.5598108e-08 2.6268728e-14 7.2178309e-12 1.5839073e-10
 1.3746238e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 00:26:41,666] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2949
[2019-04-04 00:26:41,699] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.0, 63.0, 0.0, 0.0, 26.0, 24.26272556757615, 0.1657580356239948, 0.0, 1.0, 43748.98248055012], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3984600.0000, 
sim time next is 3985200.0000, 
raw observation next is [-12.0, 63.0, 0.0, 0.0, 26.0, 24.2750914149667, 0.1694885039151215, 0.0, 1.0, 43695.10638557497], 
processed observation next is [1.0, 0.13043478260869565, 0.13019390581717452, 0.63, 0.0, 0.0, 0.6666666666666666, 0.5229242845805583, 0.5564961679717072, 0.0, 1.0, 0.20807193516940461], 
reward next is 0.7919, 
noisyNet noise sample is [array([1.8552386], dtype=float32), -1.3650509]. 
=============================================
[2019-04-04 00:26:45,865] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [7.8970794e-17 2.9126141e-09 1.9746466e-15 1.3060757e-11 9.0074975e-11
 5.5674066e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:26:45,865] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2211
[2019-04-04 00:26:45,894] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.5, 47.0, 0.0, 0.0, 26.0, 25.49136482627049, 0.5031581938710973, 0.0, 1.0, 42044.96048438217], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3965400.0000, 
sim time next is 3966000.0000, 
raw observation next is [-7.666666666666666, 47.66666666666666, 0.0, 0.0, 26.0, 25.47045719148074, 0.4985495699031844, 0.0, 1.0, 50848.31184936313], 
processed observation next is [1.0, 0.9130434782608695, 0.25023084025854114, 0.47666666666666657, 0.0, 0.0, 0.6666666666666666, 0.6225380992900617, 0.6661831899677281, 0.0, 1.0, 0.2421348183303006], 
reward next is 0.7579, 
noisyNet noise sample is [array([-2.511031], dtype=float32), -0.754633]. 
=============================================
[2019-04-04 00:26:45,902] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[77.73527 ]
 [77.75473 ]
 [77.87813 ]
 [77.907745]
 [77.58603 ]], R is [[77.76390076]
 [77.78604889]
 [77.84732819]
 [77.89912415]
 [77.87953186]].
[2019-04-04 00:26:46,114] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.8254796e-19 1.2647704e-10 1.3497843e-17 2.3840671e-13 9.3406869e-13
 3.4298977e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:26:46,114] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7109
[2019-04-04 00:26:46,138] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.6666666666666667, 28.66666666666667, 120.6666666666667, 824.3333333333334, 26.0, 26.29944083894983, 0.621438890441193, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4103400.0000, 
sim time next is 4104000.0000, 
raw observation next is [1.0, 28.0, 120.5, 828.5, 26.0, 26.54601884201412, 0.4077076637400346, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.4903047091412743, 0.28, 0.40166666666666667, 0.9154696132596685, 0.6666666666666666, 0.71216823683451, 0.6359025545800115, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9947601], dtype=float32), 0.17494005]. 
=============================================
[2019-04-04 00:26:46,150] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[90.6908 ]
 [90.75829]
 [90.83296]
 [91.11288]
 [91.22574]], R is [[90.69078827]
 [90.78388214]
 [90.87604523]
 [90.96728516]
 [91.05760956]].
[2019-04-04 00:26:53,225] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.0707137e-16 2.7812046e-08 2.7660500e-14 1.3687739e-11 2.2079645e-10
 1.5980686e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 00:26:53,229] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2603
[2019-04-04 00:26:53,255] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 50.0, 0.0, 0.0, 26.0, 25.22047730385176, 0.3603786617007894, 0.0, 1.0, 39473.65542659967], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4158600.0000, 
sim time next is 4159200.0000, 
raw observation next is [-3.0, 50.0, 0.0, 0.0, 26.0, 25.19654905970373, 0.3611528742591841, 0.0, 1.0, 39489.9949385464], 
processed observation next is [0.0, 0.13043478260869565, 0.3795013850415513, 0.5, 0.0, 0.0, 0.6666666666666666, 0.5997124216419776, 0.6203842914197281, 0.0, 1.0, 0.18804759494545903], 
reward next is 0.8120, 
noisyNet noise sample is [array([1.0837588], dtype=float32), -2.0248697]. 
=============================================
[2019-04-04 00:27:01,373] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [5.4408394e-19 2.6624690e-11 5.6602388e-17 4.8871345e-13 1.2970536e-12
 6.5202336e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:27:01,373] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3200
[2019-04-04 00:27:01,408] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.666666666666668, 60.33333333333333, 0.0, 0.0, 26.0, 26.63987165946278, 0.8002309816854529, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4398000.0000, 
sim time next is 4398600.0000, 
raw observation next is [9.533333333333333, 60.66666666666666, 0.0, 0.0, 26.0, 26.57958332990581, 0.7885294691371835, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.7266851338873501, 0.6066666666666666, 0.0, 0.0, 0.6666666666666666, 0.7149652774921508, 0.7628431563790611, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.15359072], dtype=float32), 0.5042381]. 
=============================================
[2019-04-04 00:27:05,731] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.9952654e-17 5.1376112e-09 2.3826325e-15 4.8226883e-12 6.7543124e-11
 1.6636811e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 00:27:05,731] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7036
[2019-04-04 00:27:05,745] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.833333333333333, 33.33333333333334, 150.0, 787.6666666666667, 26.0, 25.11710093255336, 0.397348690780887, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4193400.0000, 
sim time next is 4194000.0000, 
raw observation next is [2.0, 34.0, 166.0, 758.0, 26.0, 25.11158890221975, 0.3983350646166371, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.518005540166205, 0.34, 0.5533333333333333, 0.8375690607734807, 0.6666666666666666, 0.5926324085183126, 0.6327783548722123, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7704212], dtype=float32), 0.38010713]. 
=============================================
[2019-04-04 00:27:05,756] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[82.38785]
 [82.13902]
 [81.99754]
 [81.96144]
 [81.95377]], R is [[82.80739594]
 [82.97932434]
 [83.1495285 ]
 [83.31803131]
 [83.48484802]].
[2019-04-04 00:27:14,899] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0983217e-17 3.4245811e-09 2.1746400e-15 3.3685132e-12 2.4194062e-11
 6.2141851e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:27:14,910] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7869
[2019-04-04 00:27:14,934] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.300000000000001, 74.0, 0.0, 0.0, 26.0, 25.57726594220734, 0.421043477359569, 0.0, 1.0, 18742.10101957603], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4324800.0000, 
sim time next is 4325400.0000, 
raw observation next is [4.35, 73.5, 0.0, 0.0, 26.0, 25.68874446386926, 0.4318304200702539, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.5831024930747922, 0.735, 0.0, 0.0, 0.6666666666666666, 0.6407287053224383, 0.6439434733567513, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.45366263], dtype=float32), -1.2917216]. 
=============================================
[2019-04-04 00:27:15,149] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.1998426e-18 1.5607557e-10 4.1628993e-16 5.9289276e-13 4.6589911e-12
 6.7791719e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:27:15,149] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2143
[2019-04-04 00:27:15,178] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 48.0, 122.5, 0.0, 26.0, 26.19201331313189, 0.5356320453702064, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4536000.0000, 
sim time next is 4536600.0000, 
raw observation next is [2.0, 48.66666666666666, 124.0, 0.0, 26.0, 26.20560012144517, 0.4272645924947922, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.518005540166205, 0.4866666666666666, 0.41333333333333333, 0.0, 0.6666666666666666, 0.6838000101204308, 0.6424215308315974, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.78712666], dtype=float32), -0.43882915]. 
=============================================
[2019-04-04 00:27:18,035] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4645783e-18 6.8595782e-11 1.1275423e-16 2.4984236e-13 8.4809476e-13
 5.5893618e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:27:18,035] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1368
[2019-04-04 00:27:18,040] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.6, 58.5, 0.0, 0.0, 26.0, 27.02016634304054, 0.876022059706817, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4393800.0000, 
sim time next is 4394400.0000, 
raw observation next is [10.46666666666667, 58.66666666666666, 0.0, 0.0, 26.0, 26.96362796085408, 0.8685628850196615, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.7525392428439522, 0.5866666666666666, 0.0, 0.0, 0.6666666666666666, 0.74696899673784, 0.7895209616732205, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.05021397], dtype=float32), -0.3344324]. 
=============================================
[2019-04-04 00:27:21,615] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.6380282e-17 1.1765181e-08 1.0149426e-14 4.9151065e-12 3.6493954e-11
 9.1927678e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:27:21,615] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9344
[2019-04-04 00:27:21,626] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.666666666666667, 74.5, 0.0, 0.0, 26.0, 25.23867797010406, 0.3665980920614327, 0.0, 1.0, 36150.44146280901], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4601400.0000, 
sim time next is 4602000.0000, 
raw observation next is [-2.733333333333333, 75.0, 0.0, 0.0, 26.0, 25.18203071255111, 0.3530940246603462, 0.0, 1.0, 36178.89103377672], 
processed observation next is [1.0, 0.2608695652173913, 0.3868882733148662, 0.75, 0.0, 0.0, 0.6666666666666666, 0.5985025593792592, 0.6176980082201154, 0.0, 1.0, 0.17228043349417488], 
reward next is 0.8277, 
noisyNet noise sample is [array([0.6888538], dtype=float32), 1.3981979]. 
=============================================
[2019-04-04 00:27:21,683] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[78.6267 ]
 [78.6569 ]
 [78.68694]
 [78.71067]
 [78.75268]], R is [[78.61990356]
 [78.66156006]
 [78.70276642]
 [78.74343872]
 [78.78356171]].
[2019-04-04 00:27:22,265] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.9353039e-18 3.5632974e-09 1.2867565e-15 5.8907748e-12 5.5785819e-12
 3.6102814e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:27:22,268] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2574
[2019-04-04 00:27:22,294] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.775557561562891e-17, 64.33333333333333, 0.0, 0.0, 26.0, 25.40221222060823, 0.4462014255884449, 0.0, 1.0, 48201.30963978072], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4585200.0000, 
sim time next is 4585800.0000, 
raw observation next is [-0.1, 64.66666666666667, 0.0, 0.0, 26.0, 25.44804225992674, 0.4456283725215116, 0.0, 1.0, 18759.26139212958], 
processed observation next is [1.0, 0.043478260869565216, 0.4598337950138504, 0.6466666666666667, 0.0, 0.0, 0.6666666666666666, 0.6206701883272284, 0.6485427908405038, 0.0, 1.0, 0.08932981615299801], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.33239344], dtype=float32), 0.19493243]. 
=============================================
[2019-04-04 00:27:29,185] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.3329181e-19 2.1267580e-11 4.5833615e-18 1.9873668e-13 5.0809725e-13
 3.7385016e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 00:27:29,186] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.2441
[2019-04-04 00:27:29,205] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.166666666666667, 48.83333333333334, 197.6666666666667, 285.6666666666666, 26.0, 27.4466787446656, 0.8911250838499223, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4633800.0000, 
sim time next is 4634400.0000, 
raw observation next is [5.333333333333334, 47.66666666666667, 196.3333333333333, 207.3333333333333, 26.0, 27.61421784729993, 0.6720813777408031, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.6103416435826409, 0.47666666666666674, 0.6544444444444443, 0.22909760589318595, 0.6666666666666666, 0.8011848206083275, 0.724027125913601, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2724653], dtype=float32), -0.42546603]. 
=============================================
[2019-04-04 00:27:29,859] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.5974431e-18 1.4162792e-09 9.2172474e-16 7.3410245e-12 1.4732802e-11
 4.5073508e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:27:29,863] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6112
[2019-04-04 00:27:29,875] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 34.5, 0.0, 0.0, 26.0, 25.45294275252224, 0.4857613442229466, 0.0, 1.0, 98206.9516709091], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5007000.0000, 
sim time next is 5007600.0000, 
raw observation next is [3.0, 34.0, 0.0, 0.0, 26.0, 25.47045678493127, 0.4960486467781511, 0.0, 1.0, 50726.153600201], 
processed observation next is [1.0, 1.0, 0.5457063711911359, 0.34, 0.0, 0.0, 0.6666666666666666, 0.622538065410939, 0.6653495489260504, 0.0, 1.0, 0.24155311238190955], 
reward next is 0.7584, 
noisyNet noise sample is [array([0.65555316], dtype=float32), 0.32563862]. 
=============================================
[2019-04-04 00:27:34,024] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:27:34,024] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:27:34,030] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run14
[2019-04-04 00:27:35,489] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:27:35,489] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:27:35,495] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run14
[2019-04-04 00:27:40,569] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [3.6183884e-17 3.0720180e-09 4.4758801e-15 3.0778136e-12 4.5782930e-11
 1.0269132e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 00:27:40,571] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.7559
[2019-04-04 00:27:40,632] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 43.0, 0.0, 0.0, 26.0, 24.99440725981882, 0.3295306629265488, 0.0, 1.0, 40510.10504205435], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4820400.0000, 
sim time next is 4821000.0000, 
raw observation next is [1.0, 43.66666666666667, 0.0, 0.0, 26.0, 24.98031205761268, 0.3299543235867029, 0.0, 1.0, 40225.65613315458], 
processed observation next is [0.0, 0.8260869565217391, 0.4903047091412743, 0.4366666666666667, 0.0, 0.0, 0.6666666666666666, 0.5816926714677232, 0.609984774528901, 0.0, 1.0, 0.1915507434912123], 
reward next is 0.8084, 
noisyNet noise sample is [array([1.9020624], dtype=float32), 0.6406145]. 
=============================================
[2019-04-04 00:27:40,676] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[78.455   ]
 [78.15756 ]
 [77.8072  ]
 [77.461075]
 [77.21875 ]], R is [[78.65226746]
 [78.67284393]
 [78.73835754]
 [78.85738373]
 [78.97834778]].
[2019-04-04 00:27:45,969] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:27:45,969] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:27:45,975] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run14
[2019-04-04 00:27:46,776] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.1776240e-19 1.7812144e-11 2.0275653e-17 2.1440412e-13 7.1194781e-13
 1.7347563e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:27:46,789] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5107
[2019-04-04 00:27:46,823] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.0, 23.0, 0.0, 0.0, 26.0, 26.32320555404392, 0.6622999484455909, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4993200.0000, 
sim time next is 4993800.0000, 
raw observation next is [6.0, 23.0, 0.0, 0.0, 26.0, 26.34907858172992, 0.6538112925658339, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.6288088642659281, 0.23, 0.0, 0.0, 0.6666666666666666, 0.6957565484774934, 0.7179370975219447, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3471447], dtype=float32), -0.41430113]. 
=============================================
[2019-04-04 00:27:48,137] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.8308484e-19 2.4154940e-09 6.0822174e-17 4.3429447e-13 8.4273804e-13
 3.7272399e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:27:48,138] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7704
[2019-04-04 00:27:48,180] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 59.0, 90.66666666666667, 461.0, 26.0, 25.61730160755225, 0.4790286066700783, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5041200.0000, 
sim time next is 5041800.0000, 
raw observation next is [-0.5, 56.0, 97.0, 533.0, 26.0, 25.84084293952325, 0.5218854067675468, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.44875346260387816, 0.56, 0.3233333333333333, 0.5889502762430939, 0.6666666666666666, 0.6534035782936041, 0.673961802255849, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([3.1507788], dtype=float32), 1.7720113]. 
=============================================
[2019-04-04 00:27:49,192] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:27:49,193] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:27:49,206] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run14
[2019-04-04 00:27:50,732] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:27:50,733] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:27:50,736] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run14
[2019-04-04 00:27:50,864] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:27:50,864] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:27:50,868] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run14
[2019-04-04 00:27:52,024] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:27:52,024] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:27:52,042] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run14
[2019-04-04 00:27:52,717] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:27:52,717] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:27:52,725] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run14
[2019-04-04 00:27:53,365] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:27:53,366] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:27:53,388] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run14
[2019-04-04 00:27:55,300] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:27:55,300] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:27:55,304] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run14
[2019-04-04 00:27:56,879] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [4.4558369e-19 1.6687393e-10 4.0789414e-16 4.5483121e-13 2.9016362e-12
 1.2756943e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:27:56,888] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.1049
[2019-04-04 00:27:56,915] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.15, 32.5, 0.0, 0.0, 26.0, 25.82291266057922, 0.5294697539613323, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5097000.0000, 
sim time next is 5097600.0000, 
raw observation next is [8.1, 35.0, 0.0, 0.0, 26.0, 25.75518798652967, 0.5119665944646544, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.6869806094182825, 0.35, 0.0, 0.0, 0.6666666666666666, 0.6462656655441391, 0.6706555314882182, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.24489], dtype=float32), 2.1387057]. 
=============================================
[2019-04-04 00:27:58,010] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:27:58,010] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:27:58,014] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run14
[2019-04-04 00:27:59,214] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:27:59,214] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:27:59,220] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run14
[2019-04-04 00:27:59,893] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:27:59,893] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:27:59,898] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run14
[2019-04-04 00:28:00,021] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:28:00,021] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:28:00,025] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run14
[2019-04-04 00:28:00,363] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.3451812e-08 1.6187245e-03 8.1052427e-08 7.3854626e-06 3.9657048e-06
 5.7519522e-08 9.9836975e-01], sum to 1.0000
[2019-04-04 00:28:00,363] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3366
[2019-04-04 00:28:00,412] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.8, 95.66666666666667, 0.0, 0.0, 26.0, 19.20649486772959, -0.951296632291249, 0.0, 1.0, 66411.83679401851], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2400.0000, 
sim time next is 3000.0000, 
raw observation next is [6.0, 95.83333333333333, 0.0, 0.0, 26.0, 19.46234971485661, -0.9170681454360184, 0.0, 1.0, 52033.994786758], 
processed observation next is [0.0, 0.0, 0.6288088642659281, 0.9583333333333333, 0.0, 0.0, 0.6666666666666666, 0.12186247623805091, 0.19431061818799386, 0.0, 1.0, 0.24778092755599046], 
reward next is 0.7522, 
noisyNet noise sample is [array([-0.35958657], dtype=float32), -0.6914883]. 
=============================================
[2019-04-04 00:28:00,417] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[41.314163]
 [34.71058 ]
 [27.80579 ]
 [20.716753]
 [13.128622]], R is [[47.2129631 ]
 [47.42458725]
 [47.43078995]
 [47.07211685]
 [46.65673828]].
[2019-04-04 00:28:00,519] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:28:00,520] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:28:00,538] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:28:00,538] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:28:00,543] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run14
[2019-04-04 00:28:00,602] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run14
[2019-04-04 00:28:05,118] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[84.791725]], R is [[1.]].
[2019-04-04 00:28:50,185] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.0531543e-20 3.8472288e-11 1.6718300e-17 5.0945457e-14 1.5767432e-13
 4.2330164e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:28:50,188] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2974
[2019-04-04 00:28:50,218] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.333333333333333, 93.33333333333333, 0.0, 0.0, 26.0, 24.85587807631568, 0.2350739289640343, 0.0, 1.0, 41341.22002644189], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 510000.0000, 
sim time next is 510600.0000, 
raw observation next is [2.516666666666667, 92.66666666666667, 0.0, 0.0, 26.0, 24.86288085410577, 0.2349144622708176, 0.0, 1.0, 41214.13123128313], 
processed observation next is [1.0, 0.9130434782608695, 0.5323176361957526, 0.9266666666666667, 0.0, 0.0, 0.6666666666666666, 0.5719067378421476, 0.5783048207569392, 0.0, 1.0, 0.1962577677680149], 
reward next is 0.8037, 
noisyNet noise sample is [array([1.638737], dtype=float32), 0.4154792]. 
=============================================
[2019-04-04 00:28:51,473] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.72705321e-20 5.12377848e-11 1.26643875e-17 3.39157007e-14
 3.19168723e-14 5.39927831e-20 1.00000000e+00], sum to 1.0000
[2019-04-04 00:28:51,474] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9086
[2019-04-04 00:28:51,494] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.8, 97.0, 0.0, 0.0, 26.0, 24.88323567034572, 0.2391238128371389, 0.0, 1.0, 39791.34591855376], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 518400.0000, 
sim time next is 519000.0000, 
raw observation next is [4.0, 95.66666666666667, 0.0, 0.0, 26.0, 24.87334989098082, 0.2378214817555314, 0.0, 1.0, 39750.3246770894], 
processed observation next is [0.0, 0.0, 0.5734072022160666, 0.9566666666666667, 0.0, 0.0, 0.6666666666666666, 0.5727791575817349, 0.5792738272518437, 0.0, 1.0, 0.18928726036709237], 
reward next is 0.8107, 
noisyNet noise sample is [array([-0.45430326], dtype=float32), -1.5337728]. 
=============================================
[2019-04-04 00:28:51,511] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[87.96924 ]
 [88.041374]
 [87.9776  ]
 [87.91263 ]
 [87.80307 ]], R is [[87.86381531]
 [87.79569244]
 [87.7279892 ]
 [87.66059113]
 [87.59345245]].
[2019-04-04 00:28:57,062] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.3332239e-18 1.1032687e-09 2.0663185e-16 1.2962367e-13 1.1867319e-12
 2.1653736e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:28:57,063] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3447
[2019-04-04 00:28:57,080] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 87.0, 0.0, 0.0, 26.0, 24.62047973466025, 0.1875227367105159, 0.0, 1.0, 42316.78990517924], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 604800.0000, 
sim time next is 605400.0000, 
raw observation next is [-3.483333333333333, 86.83333333333333, 0.0, 0.0, 26.0, 24.58890114530038, 0.1806908636789315, 0.0, 1.0, 42300.56292959822], 
processed observation next is [0.0, 0.0, 0.3661126500461681, 0.8683333333333333, 0.0, 0.0, 0.6666666666666666, 0.5490750954416983, 0.5602302878929771, 0.0, 1.0, 0.2014312520457058], 
reward next is 0.7986, 
noisyNet noise sample is [array([-0.575779], dtype=float32), 0.50332683]. 
=============================================
[2019-04-04 00:29:02,070] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.7671116e-17 3.8319343e-09 6.6547562e-16 3.0089601e-12 5.0521289e-12
 1.1807441e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 00:29:02,089] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2380
[2019-04-04 00:29:02,142] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.566666666666667, 60.33333333333334, 108.1666666666667, 89.66666666666667, 26.0, 24.89113090865252, 0.2253088351468503, 0.0, 1.0, 34787.18876028684], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 649200.0000, 
sim time next is 649800.0000, 
raw observation next is [-2.5, 60.0, 112.0, 100.0, 26.0, 24.88906791404057, 0.226723332761997, 0.0, 1.0, 40008.832728658], 
processed observation next is [0.0, 0.5217391304347826, 0.39335180055401664, 0.6, 0.37333333333333335, 0.11049723756906077, 0.6666666666666666, 0.5740889928367142, 0.5755744442539991, 0.0, 1.0, 0.19051825108884762], 
reward next is 0.8095, 
noisyNet noise sample is [array([1.9753067], dtype=float32), 0.7895023]. 
=============================================
[2019-04-04 00:29:12,931] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.2538205e-17 1.5810739e-09 1.9935911e-15 4.6116119e-12 1.0679009e-11
 5.3292425e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:29:12,945] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9236
[2019-04-04 00:29:12,976] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.8, 73.5, 0.0, 0.0, 26.0, 23.8994677868201, 0.04185673211411756, 0.0, 1.0, 41388.60063178797], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 787800.0000, 
sim time next is 788400.0000, 
raw observation next is [-7.8, 74.0, 0.0, 0.0, 26.0, 23.87076509924893, 0.04655306624437628, 0.0, 1.0, 41332.08139678056], 
processed observation next is [1.0, 0.13043478260869565, 0.24653739612188366, 0.74, 0.0, 0.0, 0.6666666666666666, 0.4892304249374109, 0.5155176887481254, 0.0, 1.0, 0.19681943522276457], 
reward next is 0.8032, 
noisyNet noise sample is [array([0.5109613], dtype=float32), -0.0044103055]. 
=============================================
[2019-04-04 00:29:17,649] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.6110226e-20 4.6733312e-11 9.5379442e-18 7.4585127e-14 1.3005508e-13
 1.9800584e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:29:17,650] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0813
[2019-04-04 00:29:17,692] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.066666666666666, 72.33333333333333, 90.83333333333333, 0.0, 26.0, 25.58023974330369, 0.2901077341818395, 1.0, 1.0, 23045.82511502284], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 816000.0000, 
sim time next is 816600.0000, 
raw observation next is [-4.783333333333333, 71.66666666666667, 94.66666666666666, 0.0, 26.0, 25.571792270669, 0.2966686033994522, 1.0, 1.0, 18714.79128171975], 
processed observation next is [1.0, 0.43478260869565216, 0.3301015697137581, 0.7166666666666667, 0.31555555555555553, 0.0, 0.6666666666666666, 0.6309826892224167, 0.5988895344664841, 1.0, 1.0, 0.089118053722475], 
reward next is 0.9109, 
noisyNet noise sample is [array([-0.02482243], dtype=float32), -0.13878332]. 
=============================================
[2019-04-04 00:29:24,822] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.2291361e-20 5.7064346e-11 1.2075129e-18 4.3913207e-14 9.6998507e-14
 3.2236014e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 00:29:24,832] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7051
[2019-04-04 00:29:24,842] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 26.0, 26.01296306613964, 0.6443886641604732, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1038600.0000, 
sim time next is 1039200.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 26.0, 26.10142924071922, 0.6387332914151563, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.8614958448753465, 0.75, 0.0, 0.0, 0.6666666666666666, 0.6751191033932683, 0.7129110971383854, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.13203634], dtype=float32), 0.8845523]. 
=============================================
[2019-04-04 00:29:24,952] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2289251e-20 6.9509287e-11 5.6910018e-18 3.3865668e-14 7.5390371e-14
 2.3617423e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:29:24,954] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7709
[2019-04-04 00:29:24,978] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.416666666666666, 83.16666666666667, 0.0, 0.0, 26.0, 25.45991181375604, 0.4564619513085434, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 957000.0000, 
sim time next is 957600.0000, 
raw observation next is [6.6, 82.0, 0.0, 0.0, 26.0, 25.55978519492045, 0.4567112556129091, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.6454293628808865, 0.82, 0.0, 0.0, 0.6666666666666666, 0.6299820995767043, 0.652237085204303, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1602709], dtype=float32), -0.8840772]. 
=============================================
[2019-04-04 00:29:26,362] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [4.5992575e-21 4.5013997e-12 4.8158688e-19 9.2964568e-15 1.2474105e-14
 7.3891180e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 00:29:26,364] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.6489
[2019-04-04 00:29:26,372] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.03333333333333, 76.66666666666667, 111.6666666666667, 38.99999999999999, 26.0, 27.12968058150722, 0.8574915747213345, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1074000.0000, 
sim time next is 1074600.0000, 
raw observation next is [14.4, 75.0, 114.0, 0.0, 26.0, 27.17288658777657, 0.8638857986559252, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.8614958448753465, 0.75, 0.38, 0.0, 0.6666666666666666, 0.7644072156480476, 0.7879619328853084, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([3.1753595], dtype=float32), 0.4390579]. 
=============================================
[2019-04-04 00:29:31,067] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.6691119e-21 2.4729606e-12 1.9916259e-19 4.6934028e-15 2.2268821e-14
 2.1299216e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 00:29:31,072] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2558
[2019-04-04 00:29:31,100] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.5, 76.0, 30.33333333333334, 0.0, 26.0, 24.56184510024849, 0.5328652612311544, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1009200.0000, 
sim time next is 1009800.0000, 
raw observation next is [15.5, 76.5, 25.0, 0.0, 26.0, 25.94885173989908, 0.6332677426625071, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.8919667590027703, 0.765, 0.08333333333333333, 0.0, 0.6666666666666666, 0.6624043116582566, 0.711089247554169, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.187894], dtype=float32), 0.2364377]. 
=============================================
[2019-04-04 00:29:36,073] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.1508593e-21 1.1765611e-11 5.7056743e-18 6.6017917e-14 1.7424219e-13
 3.1051727e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 00:29:36,075] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4864
[2019-04-04 00:29:36,092] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.516666666666667, 92.0, 0.0, 0.0, 26.0, 25.31672357982757, 0.4875627932713815, 0.0, 1.0, 46622.74766997824], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1464600.0000, 
sim time next is 1465200.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 26.0, 25.33615245134857, 0.4938482177562659, 0.0, 1.0, 41248.08623204422], 
processed observation next is [1.0, 1.0, 0.5069252077562327, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6113460376123809, 0.664616072585422, 0.0, 1.0, 0.1964194582478296], 
reward next is 0.8036, 
noisyNet noise sample is [array([0.15524346], dtype=float32), -0.6482247]. 
=============================================
[2019-04-04 00:29:39,269] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.1632500e-21 3.4851661e-12 8.2251040e-19 5.1894185e-15 4.1618557e-14
 3.1174911e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 00:29:39,273] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0262
[2019-04-04 00:29:39,283] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.26666666666667, 61.16666666666666, 78.33333333333334, 675.6666666666667, 26.0, 26.74008258594818, 0.7442441529906914, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1519800.0000, 
sim time next is 1520400.0000, 
raw observation next is [10.53333333333333, 59.33333333333334, 76.66666666666666, 669.3333333333333, 26.0, 26.80298189756736, 0.7572507310185529, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.7543859649122806, 0.5933333333333334, 0.25555555555555554, 0.7395948434622467, 0.6666666666666666, 0.73358182479728, 0.7524169103395176, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.05028524], dtype=float32), 0.3631155]. 
=============================================
[2019-04-04 00:29:39,346] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-04 00:29:39,346] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 00:29:39,346] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 00:29:39,347] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:29:39,347] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 00:29:39,347] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:29:39,347] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:29:39,354] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run19
[2019-04-04 00:29:39,382] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run19
[2019-04-04 00:29:39,400] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run19
[2019-04-04 00:31:01,149] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.3631012], dtype=float32), 0.23033527]
[2019-04-04 00:31:01,150] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-4.336045829, 69.57996938, 0.0, 0.0, 26.0, 24.55384939841017, 0.1927881372343946, 0.0, 1.0, 45370.02355139989]
[2019-04-04 00:31:01,150] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 00:31:01,151] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [5.1739535e-18 6.6212247e-10 2.8607558e-16 8.1551768e-13 5.0645941e-12
 1.9349968e-17 1.0000000e+00], sampled 0.470757003222114
[2019-04-04 00:32:36,860] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 00:32:56,767] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.3631012], dtype=float32), 0.23033527]
[2019-04-04 00:32:56,767] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [1.5, 72.5, 196.0, 6.0, 26.0, 26.4626642180841, 0.6170966292465944, 1.0, 1.0, 0.0]
[2019-04-04 00:32:56,767] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 00:32:56,768] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [6.1716527e-20 2.6512900e-11 2.8428684e-18 4.8522455e-14 8.5584880e-14
 1.1217789e-19 1.0000000e+00], sampled 0.8172648986069616
[2019-04-04 00:33:08,727] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.7639 263389580.4542 1552.0485
[2019-04-04 00:33:11,040] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.3631012], dtype=float32), 0.23033527]
[2019-04-04 00:33:11,040] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [0.9092413273333333, 32.27251991999999, 105.6842205933333, 590.2776119, 26.0, 25.67877634059038, 0.3992394185526528, 1.0, 1.0, 0.0]
[2019-04-04 00:33:11,040] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 00:33:11,041] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [6.6852498e-20 3.1243064e-10 3.9686422e-18 2.6098032e-14 3.0725802e-13
 3.3748421e-19 1.0000000e+00], sampled 0.06575402394424323
[2019-04-04 00:33:13,962] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 00:33:14,992] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 1800000, evaluation results [1800000.0, 7241.763902599205, 263389580.45416775, 1552.0485098418105, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 00:33:16,621] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [4.0715045e-19 4.6696091e-10 2.3925213e-17 7.8573961e-14 2.4131681e-13
 6.8175502e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:33:16,621] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.7367
[2019-04-04 00:33:16,625] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [18.3, 65.0, 165.0, 0.0, 26.0, 25.07908776583792, 0.5006153161919765, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1170000.0000, 
sim time next is 1170600.0000, 
raw observation next is [18.3, 65.0, 163.0, 0.0, 26.0, 25.06707635928555, 0.4990340065878165, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.9695290858725764, 0.65, 0.5433333333333333, 0.0, 0.6666666666666666, 0.5889230299404625, 0.6663446688626055, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9135837], dtype=float32), -0.49327433]. 
=============================================
[2019-04-04 00:33:17,939] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.8520282e-19 1.5859691e-10 2.1312426e-17 9.9358435e-14 1.9570444e-13
 2.4827444e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:33:17,939] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6158
[2019-04-04 00:33:17,952] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.766666666666667, 97.33333333333334, 0.0, 0.0, 26.0, 25.44249041392994, 0.5830632159532242, 0.0, 1.0, 45420.59364063265], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1294800.0000, 
sim time next is 1295400.0000, 
raw observation next is [4.583333333333334, 96.66666666666666, 0.0, 0.0, 26.0, 25.42635101509288, 0.5823489811567071, 0.0, 1.0, 48808.11025012079], 
processed observation next is [0.0, 1.0, 0.5895660203139428, 0.9666666666666666, 0.0, 0.0, 0.6666666666666666, 0.6188625845910734, 0.6941163270522357, 0.0, 1.0, 0.23241957261962282], 
reward next is 0.7676, 
noisyNet noise sample is [array([1.4794891], dtype=float32), -1.0007579]. 
=============================================
[2019-04-04 00:33:26,657] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.5294519e-19 2.7027069e-10 1.0149526e-17 2.3044977e-13 3.7847123e-13
 3.4479488e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:33:26,673] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9240
[2019-04-04 00:33:26,694] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.8, 100.0, 92.0, 0.0, 26.0, 24.807981013721, 0.4533110297562424, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1258800.0000, 
sim time next is 1259400.0000, 
raw observation next is [13.8, 100.0, 89.0, 0.0, 26.0, 24.79224152592196, 0.4509893656227959, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.844875346260388, 1.0, 0.2966666666666667, 0.0, 0.6666666666666666, 0.5660201271601633, 0.6503297885409319, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5810725], dtype=float32), -0.4656691]. 
=============================================
[2019-04-04 00:33:36,247] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.0550497e-20 1.0553709e-11 4.2587551e-18 1.9147410e-14 2.7449590e-14
 4.3448515e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 00:33:36,247] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6168
[2019-04-04 00:33:36,253] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 110.6666666666667, 0.0, 26.0, 25.77284884871523, 0.5261102579567318, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1342200.0000, 
sim time next is 1342800.0000, 
raw observation next is [1.1, 92.0, 109.5, 0.0, 26.0, 25.73038716062585, 0.5138884770530555, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.49307479224376743, 0.92, 0.365, 0.0, 0.6666666666666666, 0.6441989300521541, 0.6712961590176851, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.00529996], dtype=float32), 0.032408714]. 
=============================================
[2019-04-04 00:33:40,366] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.4380585e-20 3.1791987e-12 1.6514632e-18 9.5334117e-15 4.4928349e-14
 1.3767352e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 00:33:40,366] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3934
[2019-04-04 00:33:40,382] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.800000000000001, 83.0, 100.0, 700.0, 26.0, 25.86404210075208, 0.5672829693584897, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1513800.0000, 
sim time next is 1514400.0000, 
raw observation next is [6.266666666666667, 79.66666666666667, 97.5, 700.1666666666667, 26.0, 25.04896042811094, 0.4998282251462601, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.6361957525392429, 0.7966666666666667, 0.325, 0.7736648250460406, 0.6666666666666666, 0.5874133690092451, 0.6666094083820867, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.1921713], dtype=float32), 0.91301346]. 
=============================================
[2019-04-04 00:33:48,479] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.0168561e-19 2.7054184e-11 9.0814720e-18 1.3202532e-13 1.6722639e-13
 1.6186185e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:33:48,481] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7429
[2019-04-04 00:33:48,525] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 85.33333333333333, 91.0, 0.0, 26.0, 25.86052618885372, 0.5240802013103147, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1684200.0000, 
sim time next is 1684800.0000, 
raw observation next is [1.1, 84.0, 95.0, 0.0, 26.0, 25.86053518993326, 0.5222310383418596, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.49307479224376743, 0.84, 0.31666666666666665, 0.0, 0.6666666666666666, 0.6550445991611049, 0.6740770127806198, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.03232812], dtype=float32), 0.39852577]. 
=============================================
[2019-04-04 00:34:03,884] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.6776153e-18 4.2010093e-09 1.4322208e-15 2.8043596e-12 1.1722111e-11
 1.4889495e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 00:34:03,884] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.1388
[2019-04-04 00:34:03,915] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.2, 89.66666666666667, 0.0, 0.0, 26.0, 25.19614758419054, 0.4151683833572427, 0.0, 1.0, 43074.70186700125], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1740000.0000, 
sim time next is 1740600.0000, 
raw observation next is [-0.3, 89.0, 0.0, 0.0, 26.0, 25.17291966113001, 0.4097999240423305, 0.0, 1.0, 43110.46512040404], 
processed observation next is [0.0, 0.13043478260869565, 0.4542936288088643, 0.89, 0.0, 0.0, 0.6666666666666666, 0.5977433050941675, 0.6365999746807768, 0.0, 1.0, 0.20528792914478114], 
reward next is 0.7947, 
noisyNet noise sample is [array([-0.01156719], dtype=float32), 0.27629283]. 
=============================================
[2019-04-04 00:34:12,083] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.1947023e-17 1.1018765e-09 8.7736164e-15 5.2210779e-12 5.2770257e-11
 4.9102050e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 00:34:12,084] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2364
[2019-04-04 00:34:12,124] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.300000000000001, 79.66666666666667, 0.0, 0.0, 26.0, 24.1052778876759, 0.05266449111506832, 0.0, 1.0, 45079.62538300424], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1902000.0000, 
sim time next is 1902600.0000, 
raw observation next is [-7.3, 78.5, 0.0, 0.0, 26.0, 24.13183527723151, 0.05253248866487997, 0.0, 1.0, 45088.71466653408], 
processed observation next is [1.0, 0.0, 0.26038781163434904, 0.785, 0.0, 0.0, 0.6666666666666666, 0.5109862731026258, 0.51751082955496, 0.0, 1.0, 0.2147081650787337], 
reward next is 0.7853, 
noisyNet noise sample is [array([-0.34389722], dtype=float32), -0.17796952]. 
=============================================
[2019-04-04 00:34:15,369] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.3727050e-19 4.8496714e-11 6.9835451e-17 2.1413521e-13 1.0911126e-12
 1.5184885e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:34:15,369] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0271
[2019-04-04 00:34:15,450] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.7, 67.33333333333334, 141.0, 0.0, 26.0, 25.93600586512335, 0.3566261445148478, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2123400.0000, 
sim time next is 2124000.0000, 
raw observation next is [-5.6, 68.0, 137.0, 0.0, 26.0, 25.54703555204397, 0.3979217548194876, 1.0, 1.0, 53824.00896565457], 
processed observation next is [1.0, 0.6086956521739131, 0.30747922437673136, 0.68, 0.45666666666666667, 0.0, 0.6666666666666666, 0.6289196293369974, 0.6326405849398292, 1.0, 1.0, 0.2563048045983551], 
reward next is 0.7437, 
noisyNet noise sample is [array([-0.11151511], dtype=float32), -0.2042363]. 
=============================================
[2019-04-04 00:34:15,472] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[81.33355 ]
 [81.2768  ]
 [81.20781 ]
 [81.255226]
 [81.3525  ]], R is [[81.47480011]
 [81.66004944]
 [81.84345245]
 [81.88545227]
 [81.91750336]].
[2019-04-04 00:34:15,974] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.6660521e-19 6.2934540e-11 3.0458729e-17 8.4702135e-13 3.6638986e-13
 1.6178221e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:34:15,974] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8909
[2019-04-04 00:34:16,041] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.733333333333333, 77.66666666666667, 156.6666666666667, 288.1666666666667, 26.0, 25.7841991557119, 0.3383310351228041, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1938000.0000, 
sim time next is 1938600.0000, 
raw observation next is [-6.449999999999999, 77.0, 171.0, 236.0, 26.0, 25.8354234578566, 0.3337896242697374, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.28393351800554023, 0.77, 0.57, 0.26077348066298345, 0.6666666666666666, 0.6529519548213832, 0.6112632080899124, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0255497], dtype=float32), -1.7941786]. 
=============================================
[2019-04-04 00:34:30,727] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.6004889e-17 6.2774408e-10 1.6859727e-15 3.4012845e-12 1.6027086e-11
 1.5254804e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 00:34:30,734] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0177
[2019-04-04 00:34:30,772] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.533333333333333, 84.33333333333333, 0.0, 0.0, 26.0, 24.18206743030889, 0.08848944376720148, 0.0, 1.0, 43424.79155368249], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2094000.0000, 
sim time next is 2094600.0000, 
raw observation next is [-6.616666666666667, 83.66666666666667, 0.0, 0.0, 26.0, 24.13590745055667, 0.07610600722437852, 0.0, 1.0, 43500.84823573153], 
processed observation next is [1.0, 0.21739130434782608, 0.2793167128347184, 0.8366666666666667, 0.0, 0.0, 0.6666666666666666, 0.5113256208797224, 0.5253686690747928, 0.0, 1.0, 0.20714689636062633], 
reward next is 0.7929, 
noisyNet noise sample is [array([1.248918], dtype=float32), 1.3986564]. 
=============================================
[2019-04-04 00:34:34,050] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.6935855e-18 1.0888170e-09 1.0482709e-15 2.8772770e-12 5.1573971e-12
 6.7765382e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:34:34,050] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6978
[2019-04-04 00:34:34,077] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.33841238473986, 0.109205251045296, 0.0, 1.0, 41260.30711373613], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2000400.0000, 
sim time next is 2001000.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.30740450772384, 0.1005486121797591, 0.0, 1.0, 41257.72252390526], 
processed observation next is [1.0, 0.13043478260869565, 0.30747922437673136, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5256170423103201, 0.5335162040599197, 0.0, 1.0, 0.1964653453519298], 
reward next is 0.8035, 
noisyNet noise sample is [array([0.10936537], dtype=float32), -0.08495917]. 
=============================================
[2019-04-04 00:34:34,106] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[79.44728 ]
 [79.442184]
 [79.416595]
 [79.38623 ]
 [79.39938 ]], R is [[79.44483948]
 [79.45391083]
 [79.46278381]
 [79.47122955]
 [79.47926331]].
[2019-04-04 00:34:40,692] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.4989142e-18 6.8603834e-09 3.2997657e-15 2.2244817e-12 1.3693980e-11
 7.1197083e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:34:40,692] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0657
[2019-04-04 00:34:40,726] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 86.0, 0.0, 0.0, 26.0, 24.49343608205756, 0.1772573502056327, 0.0, 1.0, 43118.15466584141], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2084400.0000, 
sim time next is 2085000.0000, 
raw observation next is [-5.100000000000001, 86.83333333333334, 0.0, 0.0, 26.0, 24.47069031061058, 0.1740438330499393, 0.0, 1.0, 43217.85449254519], 
processed observation next is [1.0, 0.13043478260869565, 0.32132963988919666, 0.8683333333333334, 0.0, 0.0, 0.6666666666666666, 0.5392241925508815, 0.5580146110166465, 0.0, 1.0, 0.20579930710735805], 
reward next is 0.7942, 
noisyNet noise sample is [array([0.78860056], dtype=float32), -0.49922776]. 
=============================================
[2019-04-04 00:34:40,739] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[80.003   ]
 [80.12644 ]
 [80.27554 ]
 [80.408005]
 [80.531876]], R is [[79.89699554]
 [79.89270782]
 [79.88898468]
 [79.885849  ]
 [79.88319397]].
[2019-04-04 00:34:53,901] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.9517509e-17 1.2729608e-09 3.6759206e-15 3.7955073e-12 2.3941583e-11
 1.7736702e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 00:34:53,904] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6853
[2019-04-04 00:34:53,938] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 91.00000000000001, 0.0, 0.0, 26.0, 23.76246506681927, -0.003680383970683784, 0.0, 1.0, 43366.54977951405], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2265000.0000, 
sim time next is 2265600.0000, 
raw observation next is [-8.900000000000002, 91.0, 0.0, 0.0, 26.0, 23.69355740815719, -0.00880450846326535, 0.0, 1.0, 43320.51480658642], 
processed observation next is [1.0, 0.21739130434782608, 0.2160664819944598, 0.91, 0.0, 0.0, 0.6666666666666666, 0.4744631173464325, 0.49706516384557825, 0.0, 1.0, 0.20628816574564962], 
reward next is 0.7937, 
noisyNet noise sample is [array([2.360723], dtype=float32), 1.1613607]. 
=============================================
[2019-04-04 00:34:53,958] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0763096e-18 2.2180145e-10 5.0992237e-17 6.5746649e-13 1.1225106e-12
 4.1374044e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:34:53,966] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1499
[2019-04-04 00:34:54,017] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 71.0, 130.0, 0.0, 26.0, 25.74196719899886, 0.3400307316729342, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2199600.0000, 
sim time next is 2200200.0000, 
raw observation next is [-4.4, 70.5, 134.3333333333333, 0.0, 26.0, 25.62609016342897, 0.330406473753433, 1.0, 1.0, 75904.51236002636], 
processed observation next is [1.0, 0.4782608695652174, 0.3407202216066482, 0.705, 0.4477777777777776, 0.0, 0.6666666666666666, 0.6355075136190808, 0.6101354912511443, 1.0, 1.0, 0.3614500588572684], 
reward next is 0.6385, 
noisyNet noise sample is [array([0.62912023], dtype=float32), -1.3343339]. 
=============================================
[2019-04-04 00:35:05,853] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.2175108e-20 3.3248349e-11 3.9498258e-17 2.7289133e-13 4.7744339e-13
 5.5871873e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:35:05,855] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5963
[2019-04-04 00:35:05,872] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.3666666666666668, 43.66666666666667, 121.8333333333333, 35.0, 26.0, 25.57042682770993, 0.3951465266647939, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2302800.0000, 
sim time next is 2303400.0000, 
raw observation next is [0.1833333333333333, 43.83333333333334, 105.6666666666667, 28.0, 26.0, 25.76705454115535, 0.4111506453445448, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.46768236380424755, 0.4383333333333334, 0.3522222222222223, 0.030939226519337018, 0.6666666666666666, 0.6472545450962791, 0.6370502151148483, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.08688521], dtype=float32), -0.50584835]. 
=============================================
[2019-04-04 00:35:26,562] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.7278394e-19 8.1668552e-12 5.5382571e-18 1.8691175e-13 1.9218366e-13
 8.9136963e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 00:35:26,563] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2623
[2019-04-04 00:35:26,610] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.4, 55.0, 102.0, 733.0, 26.0, 26.71360305330706, 0.6604479534888882, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2730600.0000, 
sim time next is 2731200.0000, 
raw observation next is [-4.266666666666667, 54.66666666666667, 99.33333333333333, 713.1666666666667, 26.0, 26.75427156693235, 0.6644031453334301, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3444136657433057, 0.5466666666666667, 0.3311111111111111, 0.7880294659300185, 0.6666666666666666, 0.7295226305776957, 0.7214677151111434, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2008158], dtype=float32), 0.34041765]. 
=============================================
[2019-04-04 00:35:29,904] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [3.8098111e-19 8.9128704e-11 1.6449439e-17 2.6155103e-13 7.8777788e-13
 8.7284657e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:35:29,905] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.7781
[2019-04-04 00:35:29,956] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.0, 68.0, 116.1666666666667, 691.8333333333334, 26.0, 26.08157597999655, 0.4731931516495056, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2716800.0000, 
sim time next is 2717400.0000, 
raw observation next is [-9.5, 66.0, 115.3333333333333, 709.6666666666666, 26.0, 26.07939661712038, 0.4789665873910186, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.1994459833795014, 0.66, 0.3844444444444443, 0.7841620626151012, 0.6666666666666666, 0.6732830514266984, 0.6596555291303395, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0015564], dtype=float32), 0.5834923]. 
=============================================
[2019-04-04 00:35:37,691] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.3903266e-22 1.0967142e-13 6.0637677e-20 1.5976115e-15 1.3145310e-14
 5.7083435e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 00:35:37,695] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2980
[2019-04-04 00:35:37,703] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.833333333333334, 94.16666666666666, 113.6666666666667, 811.0, 26.0, 27.16585768630426, 0.8023743668491043, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3153000.0000, 
sim time next is 3153600.0000, 
raw observation next is [8.0, 93.0, 113.5, 814.0, 26.0, 27.22060185370187, 0.8141174447292379, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.6842105263157896, 0.93, 0.37833333333333335, 0.8994475138121547, 0.6666666666666666, 0.7683834878084891, 0.7713724815764126, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3878314], dtype=float32), -2.0351765]. 
=============================================
[2019-04-04 00:35:46,827] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.6063918e-17 2.1857547e-08 7.9402834e-15 6.5031422e-12 7.9202131e-11
 7.1806456e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 00:35:46,828] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1682
[2019-04-04 00:35:46,841] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 72.66666666666667, 0.0, 0.0, 26.0, 23.67241380046867, -0.04020547106976639, 0.0, 1.0, 40691.85788034041], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3050400.0000, 
sim time next is 3051000.0000, 
raw observation next is [-6.0, 70.5, 0.0, 0.0, 26.0, 23.63688093836484, -0.04756457574431707, 0.0, 1.0, 40782.63756015559], 
processed observation next is [0.0, 0.30434782608695654, 0.296398891966759, 0.705, 0.0, 0.0, 0.6666666666666666, 0.46974007819707, 0.48414514141856096, 0.0, 1.0, 0.1942030360007409], 
reward next is 0.8058, 
noisyNet noise sample is [array([2.3319378], dtype=float32), -2.9931948]. 
=============================================
[2019-04-04 00:35:46,846] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[76.03708]
 [76.06681]
 [76.04218]
 [75.97387]
 [75.90465]], R is [[76.01218414]
 [76.05828857]
 [76.10440826]
 [76.15055847]
 [76.1966095 ]].
[2019-04-04 00:35:57,196] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.1083482e-19 1.5778007e-11 1.0224410e-17 1.5915356e-14 1.5258639e-13
 1.7882469e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:35:57,196] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5219
[2019-04-04 00:35:57,251] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 92.0, 85.0, 370.0, 26.0, 25.59862912786325, 0.5619432672238318, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3227400.0000, 
sim time next is 3228000.0000, 
raw observation next is [-3.0, 92.0, 87.66666666666667, 417.1666666666667, 26.0, 25.78166205702883, 0.5760984425395571, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.3795013850415513, 0.92, 0.2922222222222222, 0.4609576427255985, 0.6666666666666666, 0.6484718380857357, 0.6920328141798523, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.68905205], dtype=float32), 0.25618038]. 
=============================================
[2019-04-04 00:35:57,259] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[89.22018 ]
 [89.03044 ]
 [88.9735  ]
 [88.88943 ]
 [88.602486]], R is [[89.30979919]
 [89.41670227]
 [89.52253723]
 [89.62731171]
 [89.73104095]].
[2019-04-04 00:35:57,840] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.1010473e-19 4.7011240e-12 7.9506681e-18 2.8178362e-14 1.1449072e-13
 4.3309183e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 00:35:57,842] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1888
[2019-04-04 00:35:57,855] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.666666666666667, 71.0, 80.66666666666667, 656.8333333333334, 26.0, 26.18362660758967, 0.7733598661251562, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3253200.0000, 
sim time next is 3253800.0000, 
raw observation next is [-2.833333333333333, 71.0, 76.33333333333333, 627.6666666666666, 26.0, 26.58954287743919, 0.8037845349363048, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.3841181902123731, 0.71, 0.2544444444444444, 0.6935543278084714, 0.6666666666666666, 0.7157952397865991, 0.7679281783121016, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.1471324], dtype=float32), -0.85744435]. 
=============================================
[2019-04-04 00:36:05,514] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.5686506e-18 1.8334044e-11 1.2027468e-16 5.2424699e-13 1.6248974e-12
 1.0009182e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:36:05,520] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3961
[2019-04-04 00:36:05,537] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 69.0, 0.0, 0.0, 26.0, 25.96600333033758, 0.604250429308392, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3260400.0000, 
sim time next is 3261000.0000, 
raw observation next is [-4.0, 67.0, 0.0, 0.0, 26.0, 25.76630327704773, 0.6012899040734571, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.3518005540166205, 0.67, 0.0, 0.0, 0.6666666666666666, 0.6471919397539775, 0.7004299680244856, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.10748672], dtype=float32), 1.7907617]. 
=============================================
[2019-04-04 00:36:05,555] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[76.11294 ]
 [77.062485]
 [77.725464]
 [78.30914 ]
 [79.00486 ]], R is [[76.48388672]
 [76.71904755]
 [76.95185852]
 [77.18234253]
 [77.41052246]].
[2019-04-04 00:36:09,668] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.8073532e-19 4.2951698e-11 9.6256731e-17 4.4869794e-13 1.3939740e-12
 1.9477781e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:36:09,671] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9281
[2019-04-04 00:36:09,685] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 26.0, 25.43988817483739, 0.4883295703801847, 0.0, 1.0, 36367.76728815391], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3277200.0000, 
sim time next is 3277800.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 26.0, 25.38549108110419, 0.4807854311932431, 0.0, 1.0, 65436.79895824954], 
processed observation next is [1.0, 0.9565217391304348, 0.296398891966759, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6154575900920157, 0.6602618103977477, 0.0, 1.0, 0.31160380456309306], 
reward next is 0.6884, 
noisyNet noise sample is [array([-0.4942837], dtype=float32), 0.35083297]. 
=============================================
[2019-04-04 00:36:10,210] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.10998829e-18 7.01717739e-10 4.77333142e-17 1.04400985e-13
 2.67496919e-12 1.30730230e-18 1.00000000e+00], sum to 1.0000
[2019-04-04 00:36:10,222] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8104
[2019-04-04 00:36:10,232] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.333333333333334, 31.0, 115.1666666666667, 807.1666666666666, 26.0, 25.68359359334551, 0.5004311870657622, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3669600.0000, 
sim time next is 3670200.0000, 
raw observation next is [8.0, 34.5, 116.0, 816.0, 26.0, 25.6867894692689, 0.4954896167419909, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.6842105263157896, 0.345, 0.38666666666666666, 0.901657458563536, 0.6666666666666666, 0.6405657891057416, 0.6651632055806637, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.29350653], dtype=float32), 1.0663743]. 
=============================================
[2019-04-04 00:36:17,015] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3707251e-19 4.6420780e-11 3.8408676e-17 9.0677472e-14 6.6047423e-13
 3.5134597e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:36:17,025] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6887
[2019-04-04 00:36:17,063] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 55.0, 0.0, 0.0, 26.0, 25.27338732833458, 0.4484047795710936, 1.0, 1.0, 25226.23884930118], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3354600.0000, 
sim time next is 3355200.0000, 
raw observation next is [-3.0, 55.0, 0.0, 0.0, 26.0, 25.19576632769245, 0.4400204761462605, 1.0, 1.0, 46934.86064121489], 
processed observation next is [1.0, 0.8695652173913043, 0.3795013850415513, 0.55, 0.0, 0.0, 0.6666666666666666, 0.5996471939743708, 0.6466734920487535, 1.0, 1.0, 0.22349933638673755], 
reward next is 0.7765, 
noisyNet noise sample is [array([-2.2380266], dtype=float32), -1.5669688]. 
=============================================
[2019-04-04 00:36:17,676] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.7275416e-19 3.0989388e-11 3.1946178e-17 4.5726308e-14 9.1084876e-13
 1.2026905e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:36:17,677] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5793
[2019-04-04 00:36:17,690] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.166666666666667, 72.0, 0.0, 0.0, 26.0, 25.1750656163288, 0.3996792064539759, 0.0, 1.0, 42964.40326943688], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3366600.0000, 
sim time next is 3367200.0000, 
raw observation next is [-5.333333333333334, 73.0, 0.0, 0.0, 26.0, 25.13921906281267, 0.3912398715698881, 0.0, 1.0, 42057.87736541917], 
processed observation next is [1.0, 1.0, 0.31486611265004616, 0.73, 0.0, 0.0, 0.6666666666666666, 0.5949349219010559, 0.6304132905232961, 0.0, 1.0, 0.20027560650199605], 
reward next is 0.7997, 
noisyNet noise sample is [array([-1.8306205], dtype=float32), -0.20842957]. 
=============================================
[2019-04-04 00:36:19,745] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.4162204e-20 7.9638353e-12 3.2632603e-17 3.5611413e-14 7.8268419e-13
 2.9259012e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:36:19,746] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8869
[2019-04-04 00:36:19,782] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 26.0, 24.98559883959826, 0.4260191682615519, 0.0, 1.0, 94808.19138469317], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3440400.0000, 
sim time next is 3441000.0000, 
raw observation next is [1.0, 79.0, 0.0, 0.0, 26.0, 24.98235813754905, 0.4360535231200394, 1.0, 1.0, 59151.31660126976], 
processed observation next is [1.0, 0.8260869565217391, 0.4903047091412743, 0.79, 0.0, 0.0, 0.6666666666666666, 0.5818631781290874, 0.6453511743733465, 1.0, 1.0, 0.28167293619652267], 
reward next is 0.7183, 
noisyNet noise sample is [array([0.9041727], dtype=float32), 0.26043138]. 
=============================================
[2019-04-04 00:36:19,789] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[82.57015 ]
 [85.108604]
 [82.222466]
 [85.91432 ]
 [85.99848 ]], R is [[85.71983337]
 [85.41116333]
 [85.14398956]
 [85.10625458]
 [85.25519562]].
[2019-04-04 00:36:26,723] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.28992849e-19 2.30334113e-10 1.08438415e-16 1.00804408e-13
 1.50245192e-12 1.71282738e-18 1.00000000e+00], sum to 1.0000
[2019-04-04 00:36:26,723] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7829
[2019-04-04 00:36:26,778] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 73.0, 75.0, 380.1666666666667, 26.0, 25.18966495568438, 0.3319790735082151, 1.0, 1.0, 9360.018379773128], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3745200.0000, 
sim time next is 3745800.0000, 
raw observation next is [-4.0, 74.0, 89.0, 429.0, 26.0, 25.33781093243289, 0.357667297229692, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.3518005540166205, 0.74, 0.2966666666666667, 0.4740331491712707, 0.6666666666666666, 0.6114842443694076, 0.6192224324098973, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.40499112], dtype=float32), 1.4328303]. 
=============================================
[2019-04-04 00:36:31,392] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [4.1264654e-19 9.5641751e-11 4.3762636e-17 1.4474446e-13 2.7522175e-12
 3.4115007e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:36:31,394] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.0465
[2019-04-04 00:36:31,405] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.5, 54.5, 64.0, 522.0, 26.0, 25.49433936147977, 0.4675428400757573, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3688200.0000, 
sim time next is 3688800.0000, 
raw observation next is [4.333333333333334, 56.0, 55.83333333333334, 462.5, 26.0, 25.47945002848898, 0.4568034243897802, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.58264081255771, 0.56, 0.18611111111111114, 0.511049723756906, 0.6666666666666666, 0.6232875023740817, 0.6522678081299268, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6285816], dtype=float32), 1.31109]. 
=============================================
[2019-04-04 00:36:32,612] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.4013723e-17 8.7066571e-10 6.3659232e-16 2.9654973e-13 1.4737973e-11
 2.1761396e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:36:32,613] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2036
[2019-04-04 00:36:32,624] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.51459192253589, 0.4347668004065986, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3615600.0000, 
sim time next is 3616200.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.55556298747651, 0.4279205503542847, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.4349030470914128, 0.42, 0.0, 0.0, 0.6666666666666666, 0.6296302489563758, 0.6426401834514283, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.33470604], dtype=float32), -1.0591843]. 
=============================================
[2019-04-04 00:36:36,588] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.6366982e-20 3.1049722e-12 1.8395382e-18 1.7866151e-14 1.9502589e-13
 1.7026631e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 00:36:36,591] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8538
[2019-04-04 00:36:36,622] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 26.0, 109.0, 812.0, 26.0, 27.03802574200939, 0.7538693042481698, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4024800.0000, 
sim time next is 4025400.0000, 
raw observation next is [-2.833333333333333, 25.0, 107.3333333333333, 806.0, 26.0, 27.10277762250811, 0.761332981350582, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3841181902123731, 0.25, 0.3577777777777777, 0.8906077348066298, 0.6666666666666666, 0.7585648018756759, 0.753777660450194, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9691955], dtype=float32), 0.28721115]. 
=============================================
[2019-04-04 00:36:48,551] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.05512060e-19 1.01718106e-10 7.26187683e-17 1.63194707e-13
 2.86884340e-12 1.71191283e-18 1.00000000e+00], sum to 1.0000
[2019-04-04 00:36:48,554] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0663
[2019-04-04 00:36:48,566] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.1666666666666667, 42.83333333333334, 0.0, 0.0, 26.0, 25.41518001187557, 0.466814402811439, 0.0, 1.0, 25778.5042910089], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4144200.0000, 
sim time next is 4144800.0000, 
raw observation next is [-0.3333333333333333, 42.66666666666667, 0.0, 0.0, 26.0, 25.46299009960076, 0.4642135127175153, 0.0, 1.0, 18760.34526621253], 
processed observation next is [1.0, 1.0, 0.4533702677747, 0.4266666666666667, 0.0, 0.0, 0.6666666666666666, 0.6219158416333966, 0.6547378375725051, 0.0, 1.0, 0.08933497745815491], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.00437303], dtype=float32), 1.6721205]. 
=============================================
[2019-04-04 00:36:57,357] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.2351853e-17 4.9932773e-09 1.1751951e-15 8.8320551e-13 1.0636320e-11
 6.6716815e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:36:57,361] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5786
[2019-04-04 00:36:57,384] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.166666666666667, 38.5, 0.0, 0.0, 26.0, 25.16051225477801, 0.3040764354264973, 0.0, 1.0, 40753.90800941098], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4071000.0000, 
sim time next is 4071600.0000, 
raw observation next is [-5.0, 38.0, 0.0, 0.0, 26.0, 25.12574387029275, 0.3016187506528106, 0.0, 1.0, 40716.38076442805], 
processed observation next is [1.0, 0.13043478260869565, 0.32409972299168976, 0.38, 0.0, 0.0, 0.6666666666666666, 0.5938119891910626, 0.6005395835509368, 0.0, 1.0, 0.1938875274496574], 
reward next is 0.8061, 
noisyNet noise sample is [array([-0.05820196], dtype=float32), -1.3396147]. 
=============================================
[2019-04-04 00:36:58,426] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.5122174e-17 3.8453178e-09 1.8812753e-16 1.5532525e-12 1.1452928e-11
 1.4818883e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:36:58,426] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3159
[2019-04-04 00:36:58,446] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.166666666666667, 30.66666666666666, 118.3333333333333, 838.6666666666667, 26.0, 25.14240252721122, 0.3964594335961941, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4191000.0000, 
sim time next is 4191600.0000, 
raw observation next is [1.333333333333333, 31.33333333333334, 118.1666666666667, 842.8333333333334, 26.0, 25.12847171465608, 0.3982056410699101, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.4995383194829178, 0.3133333333333334, 0.393888888888889, 0.9313075506445673, 0.6666666666666666, 0.5940393095546733, 0.63273521368997, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.72606885], dtype=float32), -0.37088102]. 
=============================================
[2019-04-04 00:37:12,721] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.1466701e-18 1.6102111e-09 7.2050931e-16 7.6079009e-13 3.2323942e-12
 2.0366659e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:37:12,722] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0080
[2019-04-04 00:37:12,740] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.9666666666666667, 71.0, 0.0, 0.0, 26.0, 25.24585278218523, 0.4073567325072334, 0.0, 1.0, 41021.84778830699], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4513800.0000, 
sim time next is 4514400.0000, 
raw observation next is [-1.0, 71.0, 0.0, 0.0, 26.0, 25.29220254574548, 0.4022244695476115, 0.0, 1.0, 40910.09037089845], 
processed observation next is [1.0, 0.2608695652173913, 0.4349030470914128, 0.71, 0.0, 0.0, 0.6666666666666666, 0.6076835454787899, 0.6340748231825372, 0.0, 1.0, 0.19480995414713548], 
reward next is 0.8052, 
noisyNet noise sample is [array([-0.15628728], dtype=float32), 0.60837466]. 
=============================================
[2019-04-04 00:37:13,284] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.2957099e-19 2.3707616e-10 9.4695385e-17 2.1974493e-13 1.7760333e-12
 1.7268540e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:37:13,284] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1083
[2019-04-04 00:37:13,365] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.8666666666666667, 72.33333333333333, 18.5, 11.0, 26.0, 25.59855651400763, 0.4296890230639438, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4520400.0000, 
sim time next is 4521000.0000, 
raw observation next is [-0.8333333333333334, 72.66666666666667, 36.99999999999999, 22.0, 26.0, 25.50966555730325, 0.4238374539411739, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.43951985226223456, 0.7266666666666667, 0.12333333333333331, 0.02430939226519337, 0.6666666666666666, 0.6258054631086042, 0.6412791513137246, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0217053], dtype=float32), -0.8032612]. 
=============================================
[2019-04-04 00:37:13,392] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[86.86298 ]
 [85.69414 ]
 [84.172485]
 [80.30655 ]
 [80.297455]], R is [[87.77915192]
 [87.90135956]
 [88.0223465 ]
 [88.14212036]
 [88.06817627]].
[2019-04-04 00:37:15,774] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [9.9555664e-19 2.4550304e-09 5.6360238e-16 9.0488158e-13 9.2877329e-12
 2.3870972e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:37:15,774] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7770
[2019-04-04 00:37:15,786] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.65, 66.0, 0.0, 0.0, 26.0, 25.48852872229572, 0.4416219415675993, 0.0, 1.0, 18753.48719977286], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4588200.0000, 
sim time next is 4588800.0000, 
raw observation next is [-0.8, 66.33333333333333, 0.0, 0.0, 26.0, 25.50706256863821, 0.4168635556688815, 0.0, 1.0, 18751.73822521675], 
processed observation next is [1.0, 0.08695652173913043, 0.4404432132963989, 0.6633333333333333, 0.0, 0.0, 0.6666666666666666, 0.6255885473865176, 0.6389545185562938, 0.0, 1.0, 0.0892939915486512], 
reward next is 0.9107, 
noisyNet noise sample is [array([2.255914], dtype=float32), 0.084226094]. 
=============================================
[2019-04-04 00:37:20,138] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.1625504e-21 2.1868481e-11 5.4926612e-19 1.9737137e-14 5.6622126e-14
 5.9964811e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 00:37:20,138] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6166
[2019-04-04 00:37:20,203] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 52.0, 125.5, 800.0, 26.0, 26.74981740876595, 0.4754606231419446, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4618800.0000, 
sim time next is 4619400.0000, 
raw observation next is [2.166666666666667, 51.5, 124.3333333333333, 811.0, 26.0, 26.71587722394503, 0.690628771921279, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.5226223453370269, 0.515, 0.41444444444444434, 0.8961325966850828, 0.6666666666666666, 0.726323101995419, 0.7302095906404263, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2960149], dtype=float32), -0.6486047]. 
=============================================
[2019-04-04 00:37:20,380] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.5276654e-20 2.7827921e-11 7.0392507e-19 1.5354361e-14 3.3369642e-14
 5.8983144e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 00:37:20,382] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3910
[2019-04-04 00:37:20,399] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.86666666666667, 39.33333333333334, 113.6666666666667, 762.8333333333333, 26.0, 27.42574006971326, 0.8112348956679583, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4357200.0000, 
sim time next is 4357800.0000, 
raw observation next is [11.3, 38.0, 115.0, 780.0, 26.0, 27.51706465695024, 0.8249138767541431, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.7756232686980611, 0.38, 0.38333333333333336, 0.861878453038674, 0.6666666666666666, 0.79308872141252, 0.7749712922513811, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8614886], dtype=float32), -1.4389477]. 
=============================================
[2019-04-04 00:37:20,888] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.1114871e-21 1.3933109e-10 3.2127645e-18 1.3067515e-14 7.1796667e-14
 6.0188052e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 00:37:20,890] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7535
[2019-04-04 00:37:20,934] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.333333333333333, 67.33333333333334, 157.1666666666667, 452.6666666666667, 26.0, 26.07045614542209, 0.5386947664320477, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4612800.0000, 
sim time next is 4613400.0000, 
raw observation next is [-1.0, 65.5, 164.0, 509.0, 26.0, 26.10797153804938, 0.5696292845503337, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4349030470914128, 0.655, 0.5466666666666666, 0.5624309392265193, 0.6666666666666666, 0.6756642948374484, 0.6898764281834445, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9960466], dtype=float32), -0.740709]. 
=============================================
[2019-04-04 00:37:31,606] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.6123975e-18 4.3841908e-10 7.1960563e-16 2.3296365e-12 6.1070398e-12
 5.4821375e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:37:31,606] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5647
[2019-04-04 00:37:31,628] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.92045630052909, 0.3589987058520723, 0.0, 1.0, 41050.6348964639], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4756200.0000, 
sim time next is 4756800.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.93694701388418, 0.3599985792749528, 0.0, 1.0, 40948.84559212266], 
processed observation next is [0.0, 0.043478260869565216, 0.3518005540166205, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5780789178236816, 0.6199995264249843, 0.0, 1.0, 0.19499450281963174], 
reward next is 0.8050, 
noisyNet noise sample is [array([0.79334265], dtype=float32), 0.3129642]. 
=============================================
[2019-04-04 00:37:33,329] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.1725915e-19 4.7011891e-11 2.1163933e-16 3.5122778e-13 1.8915045e-12
 9.9253060e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:37:33,330] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6960
[2019-04-04 00:37:33,343] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 61.0, 0.0, 0.0, 26.0, 25.44882421776278, 0.4764550070083706, 0.0, 1.0, 18759.14213959462], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4578600.0000, 
sim time next is 4579200.0000, 
raw observation next is [1.0, 61.0, 0.0, 0.0, 26.0, 25.51153291632533, 0.468982368761643, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.4903047091412743, 0.61, 0.0, 0.0, 0.6666666666666666, 0.6259610763604441, 0.656327456253881, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.068929], dtype=float32), 0.17380926]. 
=============================================
[2019-04-04 00:37:35,357] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.4564518e-18 1.5178323e-09 1.7252097e-16 4.1276981e-13 7.9654157e-12
 2.1850070e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:37:35,358] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0804
[2019-04-04 00:37:35,375] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.833333333333333, 59.16666666666666, 0.0, 0.0, 26.0, 25.38710764350263, 0.383683233206179, 0.0, 1.0, 50627.1191802789], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4837800.0000, 
sim time next is 4838400.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.441979581261, 0.3800203191248455, 0.0, 1.0, 18761.16361679431], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.6, 0.0, 0.0, 0.6666666666666666, 0.6201649651050832, 0.6266734397082818, 0.0, 1.0, 0.08933887436568719], 
reward next is 0.9107, 
noisyNet noise sample is [array([0.4103153], dtype=float32), 0.18812212]. 
=============================================
[2019-04-04 00:37:35,654] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:37:35,654] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:37:35,658] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run15
[2019-04-04 00:37:38,879] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.7182221e-18 4.9884226e-09 2.4423939e-15 4.8518433e-12 3.0839147e-11
 2.2440116e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 00:37:38,894] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5864
[2019-04-04 00:37:38,909] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 70.0, 46.99999999999999, 104.3333333333333, 26.0, 24.37199317192231, 0.1502253678384365, 0.0, 1.0, 39465.42780529632], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4866600.0000, 
sim time next is 4867200.0000, 
raw observation next is [-4.0, 71.0, 70.5, 156.5, 26.0, 24.3637369658134, 0.1585826907811038, 0.0, 1.0, 39300.42875272663], 
processed observation next is [0.0, 0.34782608695652173, 0.3518005540166205, 0.71, 0.235, 0.17292817679558012, 0.6666666666666666, 0.5303114138177832, 0.5528608969270347, 0.0, 1.0, 0.18714489882250776], 
reward next is 0.8129, 
noisyNet noise sample is [array([-0.716319], dtype=float32), -0.90005904]. 
=============================================
[2019-04-04 00:37:39,657] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:37:39,657] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:37:39,661] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run15
[2019-04-04 00:37:48,505] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:37:48,505] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:37:48,508] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run15
[2019-04-04 00:37:54,605] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [7.3155349e-18 3.0894920e-10 1.7334691e-16 1.1235431e-12 5.3229782e-12
 1.1324429e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:37:54,605] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1895
[2019-04-04 00:37:54,627] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 37.0, 97.0, 727.0, 26.0, 25.18501721192654, 0.4432620315263363, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4807800.0000, 
sim time next is 4808400.0000, 
raw observation next is [3.0, 37.0, 94.5, 697.3333333333334, 26.0, 25.18534690642576, 0.4426012744189542, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.5457063711911359, 0.37, 0.315, 0.7705340699815838, 0.6666666666666666, 0.5987789088688134, 0.6475337581396514, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.84685713], dtype=float32), -2.060879]. 
=============================================
[2019-04-04 00:37:54,813] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-04-04 00:37:54,814] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 00:37:54,814] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:37:54,815] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 00:37:54,815] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:37:54,816] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 00:37:54,816] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:37:54,824] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run20
[2019-04-04 00:37:54,847] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run20
[2019-04-04 00:37:54,899] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run20
[2019-04-04 00:40:22,704] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.3639963], dtype=float32), 0.2224825]
[2019-04-04 00:40:22,704] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [13.21666666666667, 92.16666666666667, 164.6666666666667, 72.66666666666666, 26.0, 25.38653391876308, 0.4503298881439647, 0.0, 1.0, 0.0]
[2019-04-04 00:40:22,704] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 00:40:22,706] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.6397486e-19 1.8740287e-10 1.6771125e-17 6.0903180e-14 3.8680254e-13
 1.1788076e-18 1.0000000e+00], sampled 0.4043923251069562
[2019-04-04 00:40:54,689] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 00:41:17,635] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 00:41:20,999] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 00:41:22,021] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 1900000, evaluation results [1900000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 00:41:23,510] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:41:23,510] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:41:23,514] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run15
[2019-04-04 00:41:23,913] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:41:23,913] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:41:23,921] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run15
[2019-04-04 00:41:24,674] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:41:24,674] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:41:24,678] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run15
[2019-04-04 00:41:24,816] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:41:24,816] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:41:24,819] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run15
[2019-04-04 00:41:30,670] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:41:30,671] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:41:30,674] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run15
[2019-04-04 00:41:31,309] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.1994002e-17 8.4489162e-09 4.1387703e-15 1.2344367e-12 1.2140157e-11
 4.9127906e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 00:41:31,310] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3240
[2019-04-04 00:41:31,329] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.7, 93.0, 0.0, 0.0, 26.0, 20.95819133430535, -0.6477444344119792, 0.0, 1.0, 41015.6418398758], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 15600.0000, 
sim time next is 16200.0000, 
raw observation next is [7.7, 93.0, 0.0, 0.0, 26.0, 20.97869491672216, -0.6434250047815583, 0.0, 1.0, 40917.80676487424], 
processed observation next is [0.0, 0.17391304347826086, 0.6759002770083103, 0.93, 0.0, 0.0, 0.6666666666666666, 0.24822457639351322, 0.28552499840614726, 0.0, 1.0, 0.19484669888035355], 
reward next is 0.8052, 
noisyNet noise sample is [array([0.9060459], dtype=float32), -0.30174896]. 
=============================================
[2019-04-04 00:41:31,705] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:41:31,705] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:41:31,708] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run15
[2019-04-04 00:41:33,418] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.4916833e-18 2.1030293e-09 6.5577317e-17 1.9466429e-13 2.9909532e-12
 9.3424094e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:41:33,418] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5269
[2019-04-04 00:41:33,524] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.7, 93.0, 14.0, 0.0, 26.0, 21.53066495178111, -0.4934649752698488, 0.0, 1.0, 40197.1557840123], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 29400.0000, 
sim time next is 30000.0000, 
raw observation next is [7.7, 93.0, 17.5, 0.0, 26.0, 21.56089873928766, -0.4186038387525526, 0.0, 1.0, 132515.1166631253], 
processed observation next is [0.0, 0.34782608695652173, 0.6759002770083103, 0.93, 0.058333333333333334, 0.0, 0.6666666666666666, 0.29674156160730486, 0.3604653870824825, 0.0, 1.0, 0.6310243650625015], 
reward next is 0.3690, 
noisyNet noise sample is [array([-0.73520696], dtype=float32), -0.3575631]. 
=============================================
[2019-04-04 00:41:33,529] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[82.75566]
 [82.62991]
 [82.46795]
 [82.28298]
 [82.26594]], R is [[82.84371185]
 [82.82386017]
 [82.80403137]
 [82.78430939]
 [82.76475525]].
[2019-04-04 00:41:34,158] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:41:34,158] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:41:34,177] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run15
[2019-04-04 00:41:34,613] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:41:34,613] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:41:34,616] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run15
[2019-04-04 00:41:35,277] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:41:35,277] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:41:35,280] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run15
[2019-04-04 00:41:39,685] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:41:39,685] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:41:39,689] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run15
[2019-04-04 00:41:40,002] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:41:40,003] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:41:40,025] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run15
[2019-04-04 00:41:41,609] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:41:41,609] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:41:41,640] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run15
[2019-04-04 00:41:41,701] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:41:41,701] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:41:41,747] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run15
[2019-04-04 00:41:48,121] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.9533681e-17 5.2036477e-09 6.6178244e-15 2.2951118e-12 8.6697437e-12
 2.9474706e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 00:41:48,121] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6127
[2019-04-04 00:41:48,150] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.74138015727413, -0.2438855594443943, 0.0, 1.0, 44958.22572610667], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 194400.0000, 
sim time next is 195000.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.71178837462841, -0.2542553896173845, 0.0, 1.0, 44953.25353150383], 
processed observation next is [1.0, 0.2608695652173913, 0.21606648199445982, 0.78, 0.0, 0.0, 0.6666666666666666, 0.39264903121903405, 0.4152482034608718, 0.0, 1.0, 0.21406311205478015], 
reward next is 0.7859, 
noisyNet noise sample is [array([-0.6581164], dtype=float32), -0.77581316]. 
=============================================
[2019-04-04 00:41:48,180] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[71.68272 ]
 [71.72    ]
 [71.76295 ]
 [71.801865]
 [71.838425]], R is [[71.71668243]
 [71.78543091]
 [71.85355377]
 [71.92119598]
 [71.98838043]].
[2019-04-04 00:41:48,821] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1981668e-17 1.0055496e-09 2.9280845e-15 4.7075573e-12 1.2527773e-11
 2.3169141e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 00:41:48,821] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5001
[2019-04-04 00:41:48,842] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 26.0, 23.40492987506879, -0.0921209691309605, 0.0, 1.0, 44036.07026832593], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 183600.0000, 
sim time next is 184200.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 26.0, 23.36110842084121, -0.1025044071657327, 0.0, 1.0, 44088.86109029114], 
processed observation next is [1.0, 0.13043478260869565, 0.21606648199445982, 0.78, 0.0, 0.0, 0.6666666666666666, 0.44675903507010073, 0.46583186427808915, 0.0, 1.0, 0.20994695757281495], 
reward next is 0.7901, 
noisyNet noise sample is [array([-1.6080462], dtype=float32), -0.22134145]. 
=============================================
[2019-04-04 00:41:49,308] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.0476447e-20 2.3273137e-12 7.9584966e-18 5.0339636e-14 1.1529056e-13
 1.1623612e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:41:49,308] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5486
[2019-04-04 00:41:49,361] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.9, 47.66666666666667, 53.83333333333334, 877.8333333333334, 26.0, 26.13525092232159, 0.4980424047096623, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 394800.0000, 
sim time next is 395400.0000, 
raw observation next is [-10.7, 46.83333333333334, 52.66666666666667, 868.6666666666666, 26.0, 26.26040099820711, 0.5108370374702098, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.1662049861495845, 0.46833333333333343, 0.17555555555555558, 0.9598526703499078, 0.6666666666666666, 0.6883667498505925, 0.6702790124900698, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.72056353], dtype=float32), 1.0462074]. 
=============================================
[2019-04-04 00:41:58,831] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.63140939e-19 5.90274184e-12 1.34179614e-17 2.03273487e-13
 2.12155367e-13 3.56503315e-19 1.00000000e+00], sum to 1.0000
[2019-04-04 00:41:58,832] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9803
[2019-04-04 00:41:58,894] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.6, 49.0, 94.5, 708.0, 26.0, 25.91247653886598, 0.4385979486473259, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 302400.0000, 
sim time next is 303000.0000, 
raw observation next is [-10.41666666666667, 48.16666666666667, 90.66666666666667, 724.6666666666666, 26.0, 26.00756785108211, 0.4440284567401139, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.17405355493998145, 0.4816666666666667, 0.3022222222222222, 0.8007366482504603, 0.6666666666666666, 0.6672973209235092, 0.6480094855800379, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.20273273], dtype=float32), 0.08474216]. 
=============================================
[2019-04-04 00:41:58,913] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[78.404305]
 [78.88828 ]
 [79.4094  ]
 [79.162994]
 [78.426056]], R is [[78.28181458]
 [78.49900055]
 [78.71401215]
 [78.27131653]
 [77.54159546]].
[2019-04-04 00:41:59,292] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.6811208e-18 1.5857747e-11 3.9622156e-17 6.9464822e-13 1.5949460e-13
 1.2256538e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:41:59,294] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8756
[2019-04-04 00:41:59,343] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.7, 61.0, 148.0, 106.0, 26.0, 25.85933657541719, 0.4225229923978378, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 138600.0000, 
sim time next is 139200.0000, 
raw observation next is [-6.700000000000001, 61.0, 133.5, 95.83333333333334, 26.0, 25.80619661077535, 0.4092654970739417, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.2770083102493075, 0.61, 0.445, 0.10589318600368325, 0.6666666666666666, 0.650516384231279, 0.6364218323579806, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8563492], dtype=float32), -1.825108]. 
=============================================
[2019-04-04 00:42:10,143] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.2909846e-19 2.2758019e-11 2.3586117e-17 1.5202833e-13 1.6694021e-13
 1.2892899e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:42:10,143] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1378
[2019-04-04 00:42:10,310] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.133333333333334, 64.0, 145.6666666666667, 0.0, 26.0, 24.72218954334707, 0.2360052485151558, 1.0, 1.0, 199538.1360595971], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 220800.0000, 
sim time next is 221400.0000, 
raw observation next is [-3.95, 63.5, 149.0, 0.0, 26.0, 24.3818742310923, 0.2867877150808245, 1.0, 1.0, 200978.6543490316], 
processed observation next is [1.0, 0.5652173913043478, 0.3531855955678671, 0.635, 0.49666666666666665, 0.0, 0.6666666666666666, 0.531822852591025, 0.5955959050269415, 1.0, 1.0, 0.9570412111858647], 
reward next is 0.0430, 
noisyNet noise sample is [array([-0.5862096], dtype=float32), 1.0908469]. 
=============================================
[2019-04-04 00:42:12,662] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4831543e-18 3.6790263e-12 7.7180292e-17 1.9530731e-13 4.3330324e-13
 4.4065969e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:42:12,662] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6700
[2019-04-04 00:42:12,711] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.900000000000002, 37.33333333333334, 23.66666666666666, 453.6666666666667, 26.0, 25.60099869448844, 0.448602468048809, 1.0, 1.0, 116910.4217151053], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 404400.0000, 
sim time next is 405000.0000, 
raw observation next is [-8.9, 37.0, 21.0, 403.0, 26.0, 25.90203791551682, 0.4764561615414877, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.21606648199445982, 0.37, 0.07, 0.4453038674033149, 0.6666666666666666, 0.6585031596264018, 0.6588187205138293, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.02677249], dtype=float32), -0.07967192]. 
=============================================
[2019-04-04 00:42:12,717] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[76.14881 ]
 [75.851494]
 [75.32627 ]
 [75.670876]
 [76.159904]], R is [[76.1157608 ]
 [75.79788971]
 [75.14007568]
 [75.38867188]
 [75.63478851]].
[2019-04-04 00:42:16,321] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1368659e-18 2.9033775e-10 3.0467113e-16 5.1319111e-13 8.1257380e-13
 3.0656180e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:42:16,322] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5090
[2019-04-04 00:42:16,428] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.55, 68.5, 0.0, 0.0, 26.0, 22.70203702887871, -0.1458374444346605, 1.0, 1.0, 203423.4331543393], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 286200.0000, 
sim time next is 286800.0000, 
raw observation next is [-12.63333333333333, 69.0, 0.0, 0.0, 26.0, 23.29202247530414, -0.02481986599102237, 1.0, 1.0, 164685.916841178], 
processed observation next is [1.0, 0.30434782608695654, 0.11265004616805181, 0.69, 0.0, 0.0, 0.6666666666666666, 0.4410018729420117, 0.4917267113363259, 1.0, 1.0, 0.7842186516246571], 
reward next is 0.2158, 
noisyNet noise sample is [array([0.179582], dtype=float32), -0.58996093]. 
=============================================
[2019-04-04 00:42:26,305] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.2741164e-21 2.8335350e-11 1.4707200e-18 1.8827740e-14 1.5355942e-14
 5.3839968e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 00:42:26,306] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0229
[2019-04-04 00:42:26,396] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-15.6, 90.0, 32.0, 607.5, 26.0, 25.27369705323454, 0.2104426870125358, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 378000.0000, 
sim time next is 378600.0000, 
raw observation next is [-15.41666666666667, 86.0, 34.33333333333334, 651.0, 26.0, 25.38356487944959, 0.2122684626246538, 1.0, 1.0, 18801.31329496738], 
processed observation next is [1.0, 0.391304347826087, 0.0355493998153277, 0.86, 0.11444444444444447, 0.7193370165745856, 0.6666666666666666, 0.6152970732874659, 0.5707561542082179, 1.0, 1.0, 0.08953006330936848], 
reward next is 0.9105, 
noisyNet noise sample is [array([0.35287383], dtype=float32), 0.5890379]. 
=============================================
[2019-04-04 00:42:30,719] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.3793276e-19 2.5394799e-11 5.9791566e-17 3.5600700e-13 1.3507535e-12
 2.2434530e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:42:30,719] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2594
[2019-04-04 00:42:30,770] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.55, 87.0, 0.0, 0.0, 26.0, 24.98198441126151, 0.2824392845723409, 0.0, 1.0, 35185.53869925345], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 585000.0000, 
sim time next is 585600.0000, 
raw observation next is [-2.633333333333333, 87.0, 0.0, 0.0, 26.0, 24.96217262825176, 0.278284999005222, 0.0, 1.0, 50628.45749733503], 
processed observation next is [0.0, 0.782608695652174, 0.38965835641735924, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5801810523543134, 0.592761666335074, 0.0, 1.0, 0.24108789284445253], 
reward next is 0.7589, 
noisyNet noise sample is [array([0.42872176], dtype=float32), -0.26458025]. 
=============================================
[2019-04-04 00:42:32,062] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.9369463e-17 2.1743645e-09 1.7736711e-15 5.3863398e-12 3.5616204e-12
 1.5266738e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 00:42:32,063] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8401
[2019-04-04 00:42:32,098] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 67.0, 0.0, 0.0, 26.0, 23.77506389384082, -0.0005536446398306019, 0.0, 1.0, 44230.8868791183], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 624000.0000, 
sim time next is 624600.0000, 
raw observation next is [-4.5, 66.5, 0.0, 0.0, 26.0, 23.75423209237816, -0.005786277350948717, 0.0, 1.0, 44146.15497804353], 
processed observation next is [0.0, 0.21739130434782608, 0.3379501385041552, 0.665, 0.0, 0.0, 0.6666666666666666, 0.47951934103151334, 0.4980712408830171, 0.0, 1.0, 0.2102197856097311], 
reward next is 0.7898, 
noisyNet noise sample is [array([0.3356432], dtype=float32), -0.015720904]. 
=============================================
[2019-04-04 00:42:41,154] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.0062548e-18 1.6977036e-10 1.3618357e-16 9.4828973e-13 4.8389390e-13
 1.3392546e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:42:41,155] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8542
[2019-04-04 00:42:41,211] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 85.66666666666667, 88.16666666666666, 134.6666666666667, 26.0, 24.79973332181223, 0.269471391471996, 0.0, 1.0, 44224.46355040994], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 555600.0000, 
sim time next is 556200.0000, 
raw observation next is [-0.6, 85.0, 77.0, 141.0, 26.0, 24.82087438287883, 0.2764568168466149, 0.0, 1.0, 30507.55761369906], 
processed observation next is [0.0, 0.43478260869565216, 0.44598337950138506, 0.85, 0.25666666666666665, 0.1558011049723757, 0.6666666666666666, 0.5684061985732359, 0.5921522722822049, 0.0, 1.0, 0.14527408387475743], 
reward next is 0.8547, 
noisyNet noise sample is [array([-1.5064579], dtype=float32), -0.05255043]. 
=============================================
[2019-04-04 00:42:41,742] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.6384856e-18 9.4682671e-11 1.8704841e-16 6.8902815e-13 8.8478367e-13
 1.9781880e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:42:41,742] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8021
[2019-04-04 00:42:41,769] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 86.0, 0.0, 0.0, 26.0, 24.46248971573678, 0.1525586964270502, 0.0, 1.0, 42162.44315111593], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 608400.0000, 
sim time next is 609000.0000, 
raw observation next is [-3.9, 86.0, 0.0, 0.0, 26.0, 24.42832901466321, 0.1458366793930325, 0.0, 1.0, 42138.93995359495], 
processed observation next is [0.0, 0.043478260869565216, 0.3545706371191136, 0.86, 0.0, 0.0, 0.6666666666666666, 0.5356940845552675, 0.5486122264643442, 0.0, 1.0, 0.2006616188266426], 
reward next is 0.7993, 
noisyNet noise sample is [array([-0.21193738], dtype=float32), -0.48122972]. 
=============================================
[2019-04-04 00:42:41,779] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[78.41693 ]
 [78.50202 ]
 [78.577446]
 [78.60519 ]
 [78.64603 ]], R is [[78.35972595]
 [78.37535095]
 [78.39070129]
 [78.40575409]
 [78.42047119]].
[2019-04-04 00:42:46,278] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.2727183e-20 4.8920260e-11 3.9758250e-18 2.7524981e-14 7.4776285e-14
 4.3845639e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:42:46,278] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4556
[2019-04-04 00:42:46,333] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.366666666666667, 75.0, 36.83333333333333, 0.0, 26.0, 25.75384755964628, 0.3308074057168242, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 808800.0000, 
sim time next is 809400.0000, 
raw observation next is [-6.283333333333333, 75.0, 41.66666666666666, 0.0, 26.0, 25.871694127301, 0.3152053168449394, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.288550323176362, 0.75, 0.13888888888888887, 0.0, 0.6666666666666666, 0.6559745106084168, 0.6050684389483131, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.54276407], dtype=float32), 0.47500804]. 
=============================================
[2019-04-04 00:42:48,878] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.5063482e-20 5.1667927e-11 1.1453111e-17 4.1248705e-14 1.5428936e-13
 2.6722264e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:42:48,881] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3552
[2019-04-04 00:42:48,933] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.283333333333333, 75.0, 41.66666666666666, 0.0, 26.0, 25.871694127301, 0.3152053168449394, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 809400.0000, 
sim time next is 810000.0000, 
raw observation next is [-6.2, 75.0, 46.5, 0.0, 26.0, 25.79265591348689, 0.3061719363941869, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.2908587257617729, 0.75, 0.155, 0.0, 0.6666666666666666, 0.6493879927905741, 0.6020573121313956, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.01349363], dtype=float32), 1.4878539]. 
=============================================
[2019-04-04 00:42:48,936] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[86.28059 ]
 [86.53094 ]
 [86.74478 ]
 [87.003716]
 [87.182594]], R is [[86.21524048]
 [86.35308838]
 [86.48955536]
 [86.62466431]
 [86.75841522]].
[2019-04-04 00:43:01,969] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.7044333e-22 7.4508244e-13 1.6755952e-19 2.0495747e-15 3.0597344e-15
 1.7535033e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 00:43:01,969] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9597
[2019-04-04 00:43:01,994] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.8, 93.0, 98.66666666666666, 0.0, 26.0, 25.12938002059843, 0.2450991073317225, 1.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 911400.0000, 
sim time next is 912000.0000, 
raw observation next is [3.8, 93.0, 97.33333333333333, 0.0, 26.0, 25.0751531164931, 0.2471599682775044, 1.0, 1.0, 43331.07017422162], 
processed observation next is [1.0, 0.5652173913043478, 0.5678670360110805, 0.93, 0.3244444444444444, 0.0, 0.6666666666666666, 0.5895960930410918, 0.5823866560925014, 1.0, 1.0, 0.20633842940105532], 
reward next is 0.7937, 
noisyNet noise sample is [array([0.72655356], dtype=float32), -1.4152447]. 
=============================================
[2019-04-04 00:43:02,017] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[92.40569]
 [92.39539]
 [92.42896]
 [92.36245]
 [92.33503]], R is [[92.36713409]
 [92.35451508]
 [92.43096924]
 [92.41770935]
 [92.49353027]].
[2019-04-04 00:43:07,034] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.4972710e-20 8.9679666e-11 8.2777999e-18 5.0721102e-14 2.1318446e-13
 7.7240295e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:43:07,048] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1553
[2019-04-04 00:43:07,064] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.1, 77.0, 0.0, 0.0, 26.0, 25.64412992676976, 0.620283745692998, 0.0, 1.0, 27155.03564725012], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1137000.0000, 
sim time next is 1137600.0000, 
raw observation next is [11.1, 77.0, 0.0, 0.0, 26.0, 25.64021223631035, 0.6215964882919224, 0.0, 1.0, 27595.72466398921], 
processed observation next is [0.0, 0.17391304347826086, 0.7700831024930749, 0.77, 0.0, 0.0, 0.6666666666666666, 0.6366843530258626, 0.7071988294306407, 0.0, 1.0, 0.1314082126856629], 
reward next is 0.8686, 
noisyNet noise sample is [array([-0.85042316], dtype=float32), 0.7231206]. 
=============================================
[2019-04-04 00:43:11,973] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.7702992e-22 3.3918953e-13 1.6720251e-20 1.9306485e-15 1.0614735e-15
 2.3444595e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 00:43:11,974] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5706
[2019-04-04 00:43:11,986] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [19.4, 49.0, 82.5, 0.0, 26.0, 26.72357767076501, 0.9032164576194494, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1093200.0000, 
sim time next is 1093800.0000, 
raw observation next is [19.4, 49.0, 73.0, 0.0, 26.0, 27.21418040627201, 0.945011141919245, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 1.0, 0.49, 0.24333333333333335, 0.0, 0.6666666666666666, 0.7678483671893342, 0.8150037139730816, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.587948], dtype=float32), -0.4558601]. 
=============================================
[2019-04-04 00:43:16,950] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1430522e-21 2.4374985e-11 2.1376332e-18 9.5756185e-15 1.5117785e-14
 6.9597859e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 00:43:16,953] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5172
[2019-04-04 00:43:16,966] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.333333333333334, 80.0, 0.0, 0.0, 26.0, 25.50746662137492, 0.4861641453599388, 0.0, 1.0, 69794.36335325766], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1579200.0000, 
sim time next is 1579800.0000, 
raw observation next is [5.416666666666667, 79.5, 0.0, 0.0, 26.0, 25.46960393664853, 0.4883101750919177, 0.0, 1.0, 66943.86124637657], 
processed observation next is [1.0, 0.2608695652173913, 0.6126500461680519, 0.795, 0.0, 0.0, 0.6666666666666666, 0.6224669947207108, 0.6627700583639725, 0.0, 1.0, 0.31878029164941224], 
reward next is 0.6812, 
noisyNet noise sample is [array([-0.5656765], dtype=float32), 0.11038819]. 
=============================================
[2019-04-04 00:43:18,713] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.3360931e-21 2.7642333e-12 6.1955240e-20 1.4182914e-15 1.2317442e-15
 8.3502486e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 00:43:18,714] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0145
[2019-04-04 00:43:18,721] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [18.9, 53.16666666666666, 150.3333333333333, 0.0, 26.0, 27.08472886264396, 0.9199157518446119, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1087800.0000, 
sim time next is 1088400.0000, 
raw observation next is [19.0, 52.33333333333334, 145.1666666666667, 0.0, 26.0, 27.40726132940654, 0.9538065979128548, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.9889196675900279, 0.5233333333333334, 0.48388888888888903, 0.0, 0.6666666666666666, 0.7839384441172118, 0.8179355326376183, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.508119], dtype=float32), -1.3700517]. 
=============================================
[2019-04-04 00:43:20,787] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.0403346e-20 9.5028369e-12 1.4394870e-18 2.3730687e-14 7.0398596e-14
 1.9429609e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:43:20,790] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1479
[2019-04-04 00:43:20,798] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.73333333333333, 100.0, 15.83333333333333, 0.0, 26.0, 24.59580846656353, 0.4221732093195605, 0.0, 1.0, 27474.39249671723], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1269600.0000, 
sim time next is 1270200.0000, 
raw observation next is [12.46666666666667, 100.0, 12.66666666666667, 0.0, 26.0, 24.5933314745675, 0.4229884100821404, 0.0, 1.0, 27371.22896073134], 
processed observation next is [0.0, 0.6956521739130435, 0.8079409048938138, 1.0, 0.04222222222222223, 0.0, 0.6666666666666666, 0.5494442895472916, 0.6409961366940468, 0.0, 1.0, 0.1303391855272921], 
reward next is 0.8697, 
noisyNet noise sample is [array([-1.1300809], dtype=float32), 1.0919491]. 
=============================================
[2019-04-04 00:43:22,508] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.4660749e-20 4.6742762e-11 3.3483889e-18 3.5543556e-14 1.1954141e-13
 3.7009157e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:43:22,508] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3132
[2019-04-04 00:43:22,521] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.18333333333333, 78.66666666666667, 0.0, 0.0, 26.0, 25.63773052378107, 0.6303088874697831, 0.0, 1.0, 31971.31022739505], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1131000.0000, 
sim time next is 1131600.0000, 
raw observation next is [10.36666666666667, 78.33333333333334, 0.0, 0.0, 26.0, 25.63120373595473, 0.6301708437828754, 0.0, 1.0, 31093.24951255308], 
processed observation next is [0.0, 0.08695652173913043, 0.7497691597414591, 0.7833333333333334, 0.0, 0.0, 0.6666666666666666, 0.6359336446628943, 0.7100569479276251, 0.0, 1.0, 0.14806309291691944], 
reward next is 0.8519, 
noisyNet noise sample is [array([0.220045], dtype=float32), 1.8816236]. 
=============================================
[2019-04-04 00:43:22,679] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.5354724e-20 1.2948395e-11 3.2124827e-18 2.8994812e-14 6.0458278e-14
 3.3052517e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:43:22,684] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8526
[2019-04-04 00:43:22,707] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.3, 92.0, 0.0, 0.0, 26.0, 25.31454294019242, 0.5607908740655531, 0.0, 1.0, 55433.2456679676], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1303200.0000, 
sim time next is 1303800.0000, 
raw observation next is [3.2, 92.0, 0.0, 0.0, 26.0, 25.34618829632669, 0.5733324819729883, 0.0, 1.0, 45025.7289642205], 
processed observation next is [1.0, 0.08695652173913043, 0.551246537396122, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6121823580272242, 0.6911108273243295, 0.0, 1.0, 0.21440823316295476], 
reward next is 0.7856, 
noisyNet noise sample is [array([-0.41101885], dtype=float32), -1.4185178]. 
=============================================
[2019-04-04 00:43:24,423] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.8409957e-21 1.1160207e-12 6.2981599e-19 1.1976431e-14 6.5101813e-15
 2.0012742e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 00:43:24,424] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.1596
[2019-04-04 00:43:24,446] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 106.0, 0.0, 26.0, 25.71353115963649, 0.5298969784318149, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1344600.0000, 
sim time next is 1345200.0000, 
raw observation next is [1.1, 92.0, 100.1666666666667, 0.0, 26.0, 25.69319787048423, 0.5343136338309148, 1.0, 1.0, 26896.59772116946], 
processed observation next is [1.0, 0.5652173913043478, 0.49307479224376743, 0.92, 0.333888888888889, 0.0, 0.6666666666666666, 0.6410998225403525, 0.678104544610305, 1.0, 1.0, 0.12807903676747362], 
reward next is 0.8719, 
noisyNet noise sample is [array([-1.3241203], dtype=float32), 0.06280659]. 
=============================================
[2019-04-04 00:43:26,004] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.5926881e-20 1.2184186e-11 1.7338913e-17 1.1338804e-13 1.8525567e-13
 8.8300745e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:43:26,006] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8042
[2019-04-04 00:43:26,021] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.8, 92.0, 0.0, 0.0, 26.0, 25.53233881240478, 0.4730685558888039, 0.0, 1.0, 18745.7086354989], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1473600.0000, 
sim time next is 1474200.0000, 
raw observation next is [1.9, 92.0, 0.0, 0.0, 26.0, 25.46828280023246, 0.465669243548722, 0.0, 1.0, 56136.43757299529], 
processed observation next is [1.0, 0.043478260869565216, 0.515235457063712, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6223569000193715, 0.6552230811829073, 0.0, 1.0, 0.26731636939521564], 
reward next is 0.7327, 
noisyNet noise sample is [array([0.3790623], dtype=float32), -1.9142916]. 
=============================================
[2019-04-04 00:43:26,208] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.1446312e-22 1.4428800e-13 4.9090714e-20 9.2765428e-16 1.0035578e-15
 3.0959675e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 00:43:26,209] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1783
[2019-04-04 00:43:26,223] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.483333333333333, 95.5, 83.0, 472.0000000000001, 26.0, 26.13726231911924, 0.583333948388435, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1509000.0000, 
sim time next is 1509600.0000, 
raw observation next is [3.666666666666667, 95.0, 85.5, 590.0, 26.0, 26.19653379552953, 0.6130434644323003, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.564173591874423, 0.95, 0.285, 0.6519337016574586, 0.6666666666666666, 0.6830444829607941, 0.7043478214774335, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.4095032], dtype=float32), 0.73704934]. 
=============================================
[2019-04-04 00:43:26,782] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.2376404e-20 1.9524606e-11 7.3661922e-18 5.0109534e-14 1.0330382e-13
 2.5500507e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:43:26,804] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0986
[2019-04-04 00:43:26,823] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.633333333333334, 92.66666666666667, 0.0, 0.0, 26.0, 25.60140710580352, 0.5646036972630579, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1300800.0000, 
sim time next is 1301400.0000, 
raw observation next is [3.55, 92.5, 0.0, 0.0, 26.0, 25.49840342098388, 0.5544002975734662, 0.0, 1.0, 64969.9138923942], 
processed observation next is [1.0, 0.043478260869565216, 0.5609418282548477, 0.925, 0.0, 0.0, 0.6666666666666666, 0.6248669517486567, 0.6848000991911554, 0.0, 1.0, 0.3093805423447343], 
reward next is 0.6906, 
noisyNet noise sample is [array([0.7687998], dtype=float32), -1.1842382]. 
=============================================
[2019-04-04 00:43:29,934] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.6301236e-18 8.5090032e-11 2.2845379e-16 7.6957012e-13 4.3746556e-12
 6.2193994e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:43:29,934] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0939
[2019-04-04 00:43:29,994] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 82.0, 0.0, 0.0, 26.0, 25.0017997077469, 0.3222421425672485, 0.0, 1.0, 47178.55048034742], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1791000.0000, 
sim time next is 1791600.0000, 
raw observation next is [-3.899999999999999, 82.0, 0.0, 0.0, 26.0, 25.00036037881996, 0.3212889931978523, 0.0, 1.0, 46589.60942866735], 
processed observation next is [0.0, 0.7391304347826086, 0.35457063711911363, 0.82, 0.0, 0.0, 0.6666666666666666, 0.5833633649016633, 0.6070963310659508, 0.0, 1.0, 0.22185528299365403], 
reward next is 0.7781, 
noisyNet noise sample is [array([0.9195679], dtype=float32), -1.9203146]. 
=============================================
[2019-04-04 00:43:32,077] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.7377615e-21 1.1392267e-12 5.8397524e-19 1.1642324e-14 3.3627642e-14
 8.7995999e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 00:43:32,083] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7309
[2019-04-04 00:43:32,092] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.2, 84.0, 0.0, 0.0, 26.0, 25.65652957759394, 0.604656001609848, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1639800.0000, 
sim time next is 1640400.0000, 
raw observation next is [7.2, 84.66666666666666, 0.0, 0.0, 26.0, 25.64535006403376, 0.6022782529451448, 0.0, 1.0, 18729.81385330287], 
processed observation next is [1.0, 1.0, 0.662049861495845, 0.8466666666666666, 0.0, 0.0, 0.6666666666666666, 0.6371125053361467, 0.7007594176483817, 0.0, 1.0, 0.08918958977763271], 
reward next is 0.9108, 
noisyNet noise sample is [array([-0.66900164], dtype=float32), -0.16042593]. 
=============================================
[2019-04-04 00:43:32,106] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.7140965e-22 1.8317807e-13 1.9314467e-20 1.6887083e-15 1.9051010e-15
 3.9151033e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 00:43:32,109] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1129
[2019-04-04 00:43:32,118] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.13333333333333, 59.66666666666667, 213.3333333333333, 222.1666666666667, 26.0, 26.94132035141762, 0.7923946706538979, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1596000.0000, 
sim time next is 1596600.0000, 
raw observation next is [10.5, 59.0, 216.0, 249.0, 26.0, 26.99750847200023, 0.8087422834896266, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.7534626038781165, 0.59, 0.72, 0.2751381215469613, 0.6666666666666666, 0.7497923726666859, 0.7695807611632088, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.37538525], dtype=float32), 0.56134546]. 
=============================================
[2019-04-04 00:43:34,182] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.6880655e-21 5.0895177e-13 9.9471012e-19 4.3278186e-14 3.0981365e-14
 1.5715134e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 00:43:34,183] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0562
[2019-04-04 00:43:34,196] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.966666666666667, 75.33333333333333, 0.0, 0.0, 26.0, 26.17129154636807, 0.7103570542355199, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1546800.0000, 
sim time next is 1547400.0000, 
raw observation next is [6.783333333333333, 75.66666666666667, 0.0, 0.0, 26.0, 26.15124887371511, 0.7012561136544452, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.6505078485687905, 0.7566666666666667, 0.0, 0.0, 0.6666666666666666, 0.6792707394762593, 0.7337520378848151, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.7736484], dtype=float32), 0.6121047]. 
=============================================
[2019-04-04 00:43:43,484] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.9417426e-21 3.8036384e-12 4.0191455e-19 5.1239344e-15 1.1366037e-14
 4.5874827e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 00:43:43,485] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4759
[2019-04-04 00:43:43,511] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.7, 68.0, 157.5, 112.0, 26.0, 26.60594582571135, 0.6990164461399156, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1591200.0000, 
sim time next is 1591800.0000, 
raw observation next is [7.983333333333333, 66.83333333333334, 171.6666666666667, 104.0, 26.0, 26.68247703995801, 0.7088023296087097, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.6837488457987074, 0.6683333333333334, 0.5722222222222224, 0.11491712707182321, 0.6666666666666666, 0.7235397533298341, 0.7362674432029032, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2518314], dtype=float32), 1.3195802]. 
=============================================
[2019-04-04 00:43:47,337] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2195960e-17 1.9355426e-10 2.9638530e-16 9.4104282e-13 3.1518958e-12
 3.6481871e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:43:47,338] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6515
[2019-04-04 00:43:47,395] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.333333333333334, 78.0, 127.8333333333333, 78.33333333333334, 26.0, 25.19558446674432, 0.2666911306621206, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1848000.0000, 
sim time next is 1848600.0000, 
raw observation next is [-6.15, 78.0, 148.0, 94.0, 26.0, 25.18804859057562, 0.2623632594079757, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.29224376731301943, 0.78, 0.49333333333333335, 0.10386740331491713, 0.6666666666666666, 0.5990040492146349, 0.5874544198026586, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0780952], dtype=float32), -0.08631811]. 
=============================================
[2019-04-04 00:44:01,957] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.2552071e-19 2.2485203e-11 4.1423987e-17 9.2452698e-14 3.7641589e-13
 5.7329867e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:44:01,957] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0568
[2019-04-04 00:44:01,996] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.45577426068806, 0.1849205436057038, 0.0, 1.0, 42651.76270200649], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1987200.0000, 
sim time next is 1987800.0000, 
raw observation next is [-5.7, 83.66666666666667, 0.0, 0.0, 26.0, 24.43500750633716, 0.1779633877250307, 0.0, 1.0, 42539.22173990975], 
processed observation next is [1.0, 0.0, 0.30470914127423826, 0.8366666666666667, 0.0, 0.0, 0.6666666666666666, 0.5362506255280968, 0.5593211292416769, 0.0, 1.0, 0.2025677225709988], 
reward next is 0.7974, 
noisyNet noise sample is [array([1.2089518], dtype=float32), 1.2859939]. 
=============================================
[2019-04-04 00:44:07,597] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.3048014e-17 7.9028933e-10 7.8317429e-16 5.4136032e-12 1.0499459e-11
 1.9489618e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 00:44:07,598] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1885
[2019-04-04 00:44:07,681] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 80.33333333333334, 91.0, 14.0, 26.0, 25.06343778036232, 0.2678156654580011, 0.0, 1.0, 38997.46850895903], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1869600.0000, 
sim time next is 1870200.0000, 
raw observation next is [-4.5, 79.0, 72.0, 0.0, 26.0, 25.04392411495846, 0.2652101956151175, 0.0, 1.0, 51357.32567829097], 
processed observation next is [0.0, 0.6521739130434783, 0.3379501385041552, 0.79, 0.24, 0.0, 0.6666666666666666, 0.5869936762465384, 0.5884033985383725, 0.0, 1.0, 0.24455869370614747], 
reward next is 0.7554, 
noisyNet noise sample is [array([-0.30081573], dtype=float32), -0.04154537]. 
=============================================
[2019-04-04 00:44:14,529] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.3118432e-19 1.1478564e-10 1.8868309e-16 3.0273798e-13 1.0457123e-12
 5.8971534e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:44:14,529] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7516
[2019-04-04 00:44:14,571] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.833333333333333, 86.0, 0.0, 0.0, 26.0, 24.59123995785789, 0.1883832160428728, 0.0, 1.0, 42891.04508016923], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2083200.0000, 
sim time next is 2083800.0000, 
raw observation next is [-4.916666666666667, 86.0, 0.0, 0.0, 26.0, 24.51644019784693, 0.1818475053119027, 0.0, 1.0, 43007.56555757757], 
processed observation next is [1.0, 0.08695652173913043, 0.32640812557710064, 0.86, 0.0, 0.0, 0.6666666666666666, 0.5430366831539107, 0.5606158351039675, 0.0, 1.0, 0.20479793122655984], 
reward next is 0.7952, 
noisyNet noise sample is [array([-0.24924889], dtype=float32), -1.2339344]. 
=============================================
[2019-04-04 00:44:17,380] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.6244647e-18 3.5483341e-10 4.5372664e-16 1.4461539e-12 2.9763902e-12
 9.9053778e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:44:17,381] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7956
[2019-04-04 00:44:17,450] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.616666666666667, 83.66666666666667, 0.0, 0.0, 26.0, 24.13590745055667, 0.07610600722437852, 0.0, 1.0, 43500.84823573153], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2094600.0000, 
sim time next is 2095200.0000, 
raw observation next is [-6.7, 83.0, 0.0, 0.0, 26.0, 24.06241220357136, 0.07261119835244921, 0.0, 1.0, 43563.55716902344], 
processed observation next is [1.0, 0.2608695652173913, 0.2770083102493075, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5052010169642799, 0.5242037327841497, 0.0, 1.0, 0.20744551032868305], 
reward next is 0.7926, 
noisyNet noise sample is [array([1.8261522], dtype=float32), 0.9110985]. 
=============================================
[2019-04-04 00:44:26,947] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.6854605e-20 2.4327842e-11 1.6994851e-18 5.5949650e-14 1.8716923e-14
 4.0348584e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 00:44:26,947] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4320
[2019-04-04 00:44:27,000] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.100000000000001, 71.33333333333334, 278.8333333333334, 94.16666666666667, 26.0, 25.7180197090333, 0.423551739471784, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2114400.0000, 
sim time next is 2115000.0000, 
raw observation next is [-7.0, 69.5, 293.0, 101.0, 26.0, 25.79703971553368, 0.4347274317410854, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.2686980609418283, 0.695, 0.9766666666666667, 0.11160220994475138, 0.6666666666666666, 0.6497533096278065, 0.6449091439136951, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2308311], dtype=float32), -0.49139392]. 
=============================================
[2019-04-04 00:44:27,045] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[86.23288 ]
 [86.09122 ]
 [85.77901 ]
 [85.667046]
 [85.24994 ]], R is [[86.29179382]
 [86.42887878]
 [85.91346741]
 [86.05433655]
 [86.19379425]].
[2019-04-04 00:44:27,745] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3400920e-18 1.3179285e-09 7.2405223e-17 2.1517269e-13 1.1307537e-12
 4.0933401e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:44:27,745] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1974
[2019-04-04 00:44:27,797] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 65.0, 130.0, 405.0, 26.0, 25.10843577683385, 0.2964394507876238, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2368800.0000, 
sim time next is 2369400.0000, 
raw observation next is [-2.716666666666667, 64.5, 133.0, 420.0, 26.0, 25.04547068049989, 0.2871340569814284, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.3873499538319483, 0.645, 0.44333333333333336, 0.46408839779005523, 0.6666666666666666, 0.5871225567083241, 0.5957113523271428, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2627087], dtype=float32), -1.1357489]. 
=============================================
[2019-04-04 00:44:29,383] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.6374077e-20 8.3219255e-13 1.9495472e-18 4.4930063e-14 3.0813348e-14
 1.0435124e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 00:44:29,384] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4415
[2019-04-04 00:44:29,439] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 82.00000000000001, 45.0, 0.0, 26.0, 25.86807795953405, 0.4128267450396905, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2045400.0000, 
sim time next is 2046000.0000, 
raw observation next is [-3.9, 82.0, 38.5, 0.0, 26.0, 25.8488272490607, 0.403931253757506, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.3545706371191136, 0.82, 0.12833333333333333, 0.0, 0.6666666666666666, 0.6540689374217251, 0.6346437512525019, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.12302044], dtype=float32), -0.9131498]. 
=============================================
[2019-04-04 00:44:29,485] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[85.08794 ]
 [85.16905 ]
 [85.265724]
 [85.083664]
 [84.884796]], R is [[85.04231262]
 [85.1918869 ]
 [85.33996582]
 [85.48656464]
 [84.68731689]].
[2019-04-04 00:44:38,644] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.66941297e-20 4.73289463e-11 2.51829595e-18 1.31373895e-14
 1.61957375e-14 3.09634867e-20 1.00000000e+00], sum to 1.0000
[2019-04-04 00:44:38,645] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0125
[2019-04-04 00:44:38,711] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.3, 75.0, 250.5, 80.5, 26.0, 25.75074458529394, 0.3830382097966343, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2113200.0000, 
sim time next is 2113800.0000, 
raw observation next is [-7.199999999999999, 73.16666666666667, 264.6666666666666, 87.33333333333334, 26.0, 25.69906095462655, 0.400113733365512, 1.0, 1.0, 136736.3840194246], 
processed observation next is [1.0, 0.4782608695652174, 0.26315789473684215, 0.7316666666666667, 0.8822222222222219, 0.09650092081031308, 0.6666666666666666, 0.6415884128855458, 0.6333712444551707, 1.0, 1.0, 0.6511256381877362], 
reward next is 0.3489, 
noisyNet noise sample is [array([-1.1169193], dtype=float32), -1.2282602]. 
=============================================
[2019-04-04 00:44:47,511] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.3891572e-19 1.0662116e-11 3.4411719e-17 2.1129601e-13 3.2348598e-13
 2.5639600e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:44:47,511] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5264
[2019-04-04 00:44:47,577] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.5, 70.5, 0.0, 0.0, 26.0, 25.24350301546981, 0.4005872186574309, 0.0, 1.0, 44673.05609532035], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2238600.0000, 
sim time next is 2239200.0000, 
raw observation next is [-5.6, 71.0, 0.0, 0.0, 26.0, 25.22140280834477, 0.3965567542977177, 0.0, 1.0, 44551.76223069547], 
processed observation next is [1.0, 0.9565217391304348, 0.30747922437673136, 0.71, 0.0, 0.0, 0.6666666666666666, 0.6017835673620642, 0.6321855847659059, 0.0, 1.0, 0.2121512487175975], 
reward next is 0.7878, 
noisyNet noise sample is [array([-1.3514799], dtype=float32), 0.3484782]. 
=============================================
[2019-04-04 00:44:52,960] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.8574923e-17 5.5242272e-10 8.9511625e-16 1.8025056e-12 4.3744387e-12
 5.7700500e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:44:52,961] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5848
[2019-04-04 00:44:53,010] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.666666666666667, 43.0, 0.0, 0.0, 26.0, 24.93861783447537, 0.2229316747108338, 0.0, 1.0, 42972.89626739178], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2413200.0000, 
sim time next is 2413800.0000, 
raw observation next is [-4.75, 42.5, 0.0, 0.0, 26.0, 24.89207616204424, 0.2125057676489288, 0.0, 1.0, 42991.61512267384], 
processed observation next is [0.0, 0.9565217391304348, 0.3310249307479225, 0.425, 0.0, 0.0, 0.6666666666666666, 0.5743396801703534, 0.5708352558829762, 0.0, 1.0, 0.20472197677463733], 
reward next is 0.7953, 
noisyNet noise sample is [array([-0.584285], dtype=float32), 0.7594704]. 
=============================================
[2019-04-04 00:44:57,672] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.8886057e-20 1.3558754e-11 1.2343111e-17 1.8628678e-13 9.1079962e-14
 2.2532121e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:44:57,672] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8570
[2019-04-04 00:44:57,761] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.983333333333333, 79.5, 126.6666666666667, 42.66666666666667, 26.0, 25.6486401670879, 0.3179417178435578, 1.0, 1.0, 18732.41801364534], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2281800.0000, 
sim time next is 2282400.0000, 
raw observation next is [-6.7, 78.0, 139.5, 44.5, 26.0, 25.62989757317027, 0.325842559357997, 1.0, 1.0, 18730.00607466905], 
processed observation next is [1.0, 0.43478260869565216, 0.2770083102493075, 0.78, 0.465, 0.049171270718232046, 0.6666666666666666, 0.635824797764189, 0.6086141864526656, 1.0, 1.0, 0.08919050511747167], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.621966], dtype=float32), -1.320012]. 
=============================================
[2019-04-04 00:45:11,310] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.3726413e-19 1.0524908e-10 4.2172738e-17 2.7589329e-13 1.4584084e-12
 1.7191311e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:45:11,310] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8824
[2019-04-04 00:45:11,337] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.76105796464427, 0.2224831998535401, 0.0, 1.0, 41709.76996661215], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2772000.0000, 
sim time next is 2772600.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.68591684260981, 0.2206171635632613, 0.0, 1.0, 41617.31813220982], 
processed observation next is [1.0, 0.08695652173913043, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5571597368841509, 0.5735390545210871, 0.0, 1.0, 0.1981777053914753], 
reward next is 0.8018, 
noisyNet noise sample is [array([-1.8938417], dtype=float32), 0.047417793]. 
=============================================
[2019-04-04 00:45:11,973] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.4766346e-19 1.7296334e-10 1.2033755e-16 2.7920776e-13 1.4577047e-12
 3.9451858e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:45:11,973] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6051
[2019-04-04 00:45:12,007] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 43.0, 0.0, 0.0, 26.0, 24.94857546907565, 0.178931114052672, 0.0, 1.0, 38639.71611160116], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2515800.0000, 
sim time next is 2516400.0000, 
raw observation next is [-1.7, 44.0, 0.0, 0.0, 26.0, 24.92560882941622, 0.1766751921566819, 0.0, 1.0, 38626.28616648732], 
processed observation next is [1.0, 0.13043478260869565, 0.4155124653739613, 0.44, 0.0, 0.0, 0.6666666666666666, 0.5771340691180183, 0.558891730718894, 0.0, 1.0, 0.183934696030892], 
reward next is 0.8161, 
noisyNet noise sample is [array([1.2453203], dtype=float32), 0.9849713]. 
=============================================
[2019-04-04 00:45:15,180] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [6.0343924e-20 2.2329336e-11 1.4766855e-17 5.8333323e-14 2.2054946e-13
 2.3172640e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:45:15,198] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5960
[2019-04-04 00:45:15,215] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 25.3419254212427, 0.3884692412779556, 0.0, 1.0, 48232.8365881076], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2758200.0000, 
sim time next is 2758800.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 25.31916490018376, 0.3721843052808311, 0.0, 1.0, 47779.57686888866], 
processed observation next is [1.0, 0.9565217391304348, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.6099304083486468, 0.6240614350936103, 0.0, 1.0, 0.22752179461375552], 
reward next is 0.7725, 
noisyNet noise sample is [array([-0.96202916], dtype=float32), 2.3338418]. 
=============================================
[2019-04-04 00:45:20,585] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.1963626e-18 3.7802522e-10 2.8491028e-16 6.1480823e-13 8.8763182e-13
 5.7157390e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:45:20,585] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1266
[2019-04-04 00:45:20,601] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.5, 77.33333333333333, 0.0, 0.0, 26.0, 24.52460153606611, 0.1586903278930824, 0.0, 1.0, 42318.73160740716], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2605800.0000, 
sim time next is 2606400.0000, 
raw observation next is [-5.6, 78.0, 0.0, 0.0, 26.0, 24.50090725705032, 0.1483883423438173, 0.0, 1.0, 42411.62275264397], 
processed observation next is [1.0, 0.17391304347826086, 0.30747922437673136, 0.78, 0.0, 0.0, 0.6666666666666666, 0.5417422714208598, 0.5494627807812724, 0.0, 1.0, 0.20196010834592365], 
reward next is 0.7980, 
noisyNet noise sample is [array([-0.07695091], dtype=float32), -0.86525995]. 
=============================================
[2019-04-04 00:45:21,522] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.1714224e-20 4.5473261e-12 1.4780775e-17 4.5586541e-14 9.0717263e-14
 4.4960125e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:45:21,522] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9257
[2019-04-04 00:45:21,535] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.466666666666666, 67.66666666666667, 0.0, 0.0, 26.0, 25.42180531203391, 0.4396589798987451, 0.0, 1.0, 57519.66473177991], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2670000.0000, 
sim time next is 2670600.0000, 
raw observation next is [-2.783333333333333, 68.33333333333333, 0.0, 0.0, 26.0, 25.39660613237269, 0.4445530259216826, 0.0, 1.0, 63430.97901814056], 
processed observation next is [1.0, 0.9130434782608695, 0.3855032317636196, 0.6833333333333332, 0.0, 0.0, 0.6666666666666666, 0.6163838443643908, 0.6481843419738942, 0.0, 1.0, 0.3020522810387646], 
reward next is 0.6979, 
noisyNet noise sample is [array([-1.8146048], dtype=float32), -0.7748734]. 
=============================================
[2019-04-04 00:45:54,287] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.9709701e-18 6.3846171e-11 9.9509821e-17 5.6767059e-13 3.7108953e-12
 9.9286439e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:45:54,289] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0288
[2019-04-04 00:45:54,339] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 55.0, 65.0, 538.3333333333334, 26.0, 25.17184822680989, 0.3976545228936872, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2995800.0000, 
sim time next is 2996400.0000, 
raw observation next is [-1.0, 55.0, 60.5, 506.1666666666666, 26.0, 25.15504157409245, 0.3867889432776109, 0.0, 1.0, 18700.26861021547], 
processed observation next is [0.0, 0.6956521739130435, 0.4349030470914128, 0.55, 0.20166666666666666, 0.5593001841620625, 0.6666666666666666, 0.5962534645077042, 0.6289296477592036, 0.0, 1.0, 0.08904889814388318], 
reward next is 0.9110, 
noisyNet noise sample is [array([2.1319208], dtype=float32), 0.7525574]. 
=============================================
[2019-04-04 00:46:02,018] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.3743333e-18 1.1516481e-10 1.0765120e-16 4.3011170e-13 2.6172432e-12
 7.7258603e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:46:02,021] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.2852
[2019-04-04 00:46:02,037] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.333333333333333, 65.0, 0.0, 0.0, 26.0, 25.34081732087324, 0.364394144025678, 0.0, 1.0, 40590.77773862823], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3012000.0000, 
sim time next is 3012600.0000, 
raw observation next is [-3.416666666666667, 65.0, 0.0, 0.0, 26.0, 25.30424348613305, 0.3596199422679962, 0.0, 1.0, 40219.52650276611], 
processed observation next is [0.0, 0.8695652173913043, 0.36795937211449675, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6086869571777541, 0.6198733140893321, 0.0, 1.0, 0.1915215547750767], 
reward next is 0.8085, 
noisyNet noise sample is [array([-1.0468315], dtype=float32), -1.311432]. 
=============================================
[2019-04-04 00:46:05,893] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.1895657e-20 2.9877219e-11 1.0955015e-17 5.3301656e-14 7.1357064e-14
 8.1381529e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:46:05,895] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3182
[2019-04-04 00:46:05,936] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.86367884426795, 0.3370372271014519, 0.0, 1.0, 43350.77912403444], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2939400.0000, 
sim time next is 2940000.0000, 
raw observation next is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.86028124116183, 0.3312413121638043, 0.0, 1.0, 43336.77042553438], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.85, 0.0, 0.0, 0.6666666666666666, 0.5716901034301524, 0.6104137707212681, 0.0, 1.0, 0.20636557345492562], 
reward next is 0.7936, 
noisyNet noise sample is [array([-0.19354583], dtype=float32), -0.25047547]. 
=============================================
[2019-04-04 00:46:05,981] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[82.01317 ]
 [82.1001  ]
 [82.20028 ]
 [82.388084]
 [82.643036]], R is [[81.92282104]
 [81.89716339]
 [81.87173462]
 [81.84657288]
 [81.82169342]].
[2019-04-04 00:46:11,166] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.49055610e-18 1.11233266e-10 2.50126497e-17 3.29794433e-13
 9.84306386e-13 1.61724915e-18 1.00000000e+00], sum to 1.0000
[2019-04-04 00:46:11,167] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8839
[2019-04-04 00:46:11,241] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.666666666666667, 54.33333333333334, 110.1666666666667, 797.3333333333334, 26.0, 25.16557538836452, 0.3435617218844387, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3064800.0000, 
sim time next is 3065400.0000, 
raw observation next is [-3.5, 54.5, 111.0, 805.0, 26.0, 25.15223174268277, 0.337292357226428, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.36565096952908593, 0.545, 0.37, 0.8895027624309392, 0.6666666666666666, 0.5960193118902307, 0.6124307857421426, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.02886205], dtype=float32), -0.6696366]. 
=============================================
[2019-04-04 00:46:21,802] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0233960e-19 2.5077898e-10 8.0837979e-17 7.4534075e-14 3.3991684e-13
 3.9035092e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:46:21,803] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4707
[2019-04-04 00:46:21,836] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.40535583874352, 0.5063131132530877, 0.0, 1.0, 102357.694019527], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3211200.0000, 
sim time next is 3211800.0000, 
raw observation next is [-1.166666666666667, 100.0, 0.0, 0.0, 26.0, 25.33738441442875, 0.5079499146183274, 0.0, 1.0, 87706.57278168337], 
processed observation next is [1.0, 0.17391304347826086, 0.43028624192059095, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6114487012023959, 0.6693166382061091, 0.0, 1.0, 0.4176503465794446], 
reward next is 0.5823, 
noisyNet noise sample is [array([-0.70687217], dtype=float32), 0.441071]. 
=============================================
[2019-04-04 00:46:26,413] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1386454e-18 3.2989920e-11 1.7081442e-16 4.1675772e-13 7.1754809e-13
 1.0615124e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:46:26,413] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5215
[2019-04-04 00:46:26,437] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.833333333333333, 76.0, 0.0, 0.0, 26.0, 25.06010969907096, 0.3687588391246023, 0.0, 1.0, 41565.88720247196], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3369000.0000, 
sim time next is 3369600.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 25.02234619193356, 0.3636279260487965, 0.0, 1.0, 41525.74974122704], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5851955159944634, 0.6212093086829321, 0.0, 1.0, 0.19774166543441446], 
reward next is 0.8023, 
noisyNet noise sample is [array([-0.14820895], dtype=float32), 0.5173208]. 
=============================================
[2019-04-04 00:46:26,795] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1960290e-18 4.2095397e-11 6.2422444e-17 3.1411516e-13 5.0176123e-13
 2.2844832e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:46:26,795] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9310
[2019-04-04 00:46:26,814] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 25.08963429420478, 0.3454514509156194, 0.0, 1.0, 41189.3994244978], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3373800.0000, 
sim time next is 3374400.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 25.03649961977214, 0.3349439289010403, 0.0, 1.0, 41203.24934384927], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.586374968314345, 0.6116479763003467, 0.0, 1.0, 0.1962059492564251], 
reward next is 0.8038, 
noisyNet noise sample is [array([-0.45056653], dtype=float32), -1.5854533]. 
=============================================
[2019-04-04 00:46:30,407] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.1195991e-18 1.7091630e-10 2.9651534e-16 2.3070925e-13 1.9030556e-12
 1.1370313e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:46:30,423] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0042
[2019-04-04 00:46:30,454] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 92.0, 0.0, 0.0, 26.0, 25.1133044821517, 0.4380164394707038, 0.0, 1.0, 41073.69052313782], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3222000.0000, 
sim time next is 3222600.0000, 
raw observation next is [-3.0, 92.0, 0.0, 0.0, 26.0, 25.08054898332288, 0.4437333789208893, 0.0, 1.0, 41116.9311549214], 
processed observation next is [1.0, 0.30434782608695654, 0.3795013850415513, 0.92, 0.0, 0.0, 0.6666666666666666, 0.5900457486102401, 0.6479111263069631, 0.0, 1.0, 0.19579491026153048], 
reward next is 0.8042, 
noisyNet noise sample is [array([-2.5756803], dtype=float32), 1.0336734]. 
=============================================
[2019-04-04 00:46:33,639] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.0620146e-18 1.7403021e-10 3.3860404e-16 1.5257912e-13 2.6582458e-12
 6.4601743e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:46:33,640] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.7484
[2019-04-04 00:46:33,684] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.82780537830124, 0.2769812020430907, 0.0, 1.0, 41144.1849516783], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3382200.0000, 
sim time next is 3382800.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.85077324029807, 0.274203041431712, 0.0, 1.0, 41187.59130488877], 
processed observation next is [1.0, 0.13043478260869565, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5708977700248393, 0.5914010138105706, 0.0, 1.0, 0.19613138716613698], 
reward next is 0.8039, 
noisyNet noise sample is [array([-0.39448357], dtype=float32), -1.5435283]. 
=============================================
[2019-04-04 00:46:37,796] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.1611144e-19 1.3128693e-11 2.4114609e-17 4.0293815e-13 2.9052899e-13
 2.0733030e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:46:37,796] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0597
[2019-04-04 00:46:37,835] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 25.46506110412335, 0.4952731682540423, 1.0, 1.0, 51195.49877175008], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3524400.0000, 
sim time next is 3525000.0000, 
raw observation next is [1.666666666666667, 55.33333333333334, 0.0, 0.0, 26.0, 25.41433161819646, 0.4844254525448883, 1.0, 1.0, 34545.19273090016], 
processed observation next is [1.0, 0.8260869565217391, 0.5087719298245615, 0.5533333333333335, 0.0, 0.0, 0.6666666666666666, 0.6178609681830384, 0.6614751508482961, 1.0, 1.0, 0.16450091776619125], 
reward next is 0.8355, 
noisyNet noise sample is [array([0.2931486], dtype=float32), -0.81345737]. 
=============================================
[2019-04-04 00:46:37,840] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[81.20695 ]
 [80.90289 ]
 [80.64289 ]
 [80.64828 ]
 [80.449814]], R is [[81.28903198]
 [81.23235321]
 [80.98960114]
 [81.17970276]
 [81.36790466]].
[2019-04-04 00:46:40,912] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.1389117e-20 3.6674293e-13 3.7837697e-19 3.2097026e-14 1.5048709e-14
 1.8529944e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 00:46:40,913] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6499
[2019-04-04 00:46:40,947] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 67.0, 64.83333333333333, 544.8333333333333, 26.0, 25.80720855871357, 0.5913245051941072, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3428400.0000, 
sim time next is 3429000.0000, 
raw observation next is [2.0, 67.0, 61.0, 513.0, 26.0, 26.13147025168256, 0.6157556568732404, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.518005540166205, 0.67, 0.20333333333333334, 0.5668508287292817, 0.6666666666666666, 0.6776225209735468, 0.7052518856244134, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.53981525], dtype=float32), -1.80252]. 
=============================================
[2019-04-04 00:46:40,999] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[86.27489 ]
 [86.580536]
 [86.73715 ]
 [87.23464 ]
 [87.58435 ]], R is [[86.09529114]
 [86.23433685]
 [85.91422272]
 [86.05508423]
 [86.17674255]].
[2019-04-04 00:46:43,963] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.1219538e-20 6.1268005e-13 2.6212806e-19 2.0964919e-14 6.4168095e-15
 2.0489274e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 00:46:43,963] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.4652
[2019-04-04 00:46:43,987] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 49.0, 97.33333333333334, 749.6666666666667, 26.0, 26.7435183132021, 0.7223573288437098, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3509400.0000, 
sim time next is 3510000.0000, 
raw observation next is [3.0, 49.0, 95.0, 734.0, 26.0, 26.75395516488047, 0.73161404399916, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5457063711911359, 0.49, 0.31666666666666665, 0.8110497237569061, 0.6666666666666666, 0.7294962637400392, 0.74387134799972, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6917682], dtype=float32), -0.33706093]. 
=============================================
[2019-04-04 00:46:44,029] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[89.084335]
 [89.10771 ]
 [89.14492 ]
 [89.016365]
 [88.971306]], R is [[89.18231964]
 [89.29049683]
 [89.39759064]
 [89.50361633]
 [89.60858154]].
[2019-04-04 00:46:47,227] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.0464154e-18 2.6559580e-10 7.6779156e-17 8.7402159e-14 6.0454501e-13
 3.6441380e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:46:47,227] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.1000
[2019-04-04 00:46:47,258] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 68.0, 0.0, 0.0, 26.0, 25.23821151948068, 0.4063593037827806, 0.0, 1.0, 40818.07926413833], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3551400.0000, 
sim time next is 3552000.0000, 
raw observation next is [-3.0, 67.0, 0.0, 0.0, 26.0, 25.20603502547294, 0.399094652250062, 0.0, 1.0, 40824.64950916447], 
processed observation next is [0.0, 0.08695652173913043, 0.3795013850415513, 0.67, 0.0, 0.0, 0.6666666666666666, 0.6005029187894116, 0.6330315507500207, 0.0, 1.0, 0.19440309290078317], 
reward next is 0.8056, 
noisyNet noise sample is [array([-1.4387207], dtype=float32), -0.6142594]. 
=============================================
[2019-04-04 00:46:47,285] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[83.23567 ]
 [83.19397 ]
 [83.12862 ]
 [83.018745]
 [83.027084]], R is [[83.27451324]
 [83.24739838]
 [83.2203598 ]
 [83.19308472]
 [83.16525269]].
[2019-04-04 00:46:50,383] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-04 00:46:50,384] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 00:46:50,384] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 00:46:50,384] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:46:50,384] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:46:50,385] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 00:46:50,390] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:46:50,444] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run21
[2019-04-04 00:46:50,719] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run21
[2019-04-04 00:46:50,814] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run21
[2019-04-04 00:47:48,110] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.34384263], dtype=float32), 0.20029314]
[2019-04-04 00:47:48,111] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [0.9097180234999998, 84.42351006000001, 109.4038029333333, 0.0, 26.0, 25.29721791988548, 0.2527521564537599, 1.0, 1.0, 0.0]
[2019-04-04 00:47:48,111] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 00:47:48,112] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.3077985e-19 2.0675696e-11 8.1191797e-18 9.4246862e-14 1.1707173e-13
 4.3587321e-19 1.0000000e+00], sampled 0.5577473281737264
[2019-04-04 00:49:43,202] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 00:50:02,619] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 00:50:07,315] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 00:50:08,337] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 2000000, evaluation results [2000000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 00:50:10,404] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.5729724e-18 6.6818641e-11 2.3529781e-16 4.7432371e-13 1.0266215e-12
 6.6231938e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:50:10,406] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9599
[2019-04-04 00:50:10,419] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.36898981755879, 0.3379578099860137, 0.0, 1.0, 43903.92484502491], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3730800.0000, 
sim time next is 3731400.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.40303255625536, 0.3295352875338045, 0.0, 1.0, 25810.01274321187], 
processed observation next is [1.0, 0.17391304347826086, 0.3795013850415513, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6169193796879467, 0.6098450958446014, 0.0, 1.0, 0.1229048225867232], 
reward next is 0.8771, 
noisyNet noise sample is [array([0.90617466], dtype=float32), -0.12575246]. 
=============================================
[2019-04-04 00:50:22,237] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.4350405e-18 5.3596769e-11 4.0634867e-17 1.9448280e-13 1.2479782e-12
 2.4774760e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:50:22,237] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2742
[2019-04-04 00:50:22,256] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.0, 59.0, 0.0, 0.0, 26.0, 24.96789006163709, 0.3203569391271602, 0.0, 1.0, 23918.14426993166], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3697200.0000, 
sim time next is 3697800.0000, 
raw observation next is [3.833333333333333, 59.66666666666667, 0.0, 0.0, 26.0, 24.95485126446658, 0.3164377864700692, 0.0, 1.0, 32582.52425026905], 
processed observation next is [0.0, 0.8260869565217391, 0.5687903970452447, 0.5966666666666667, 0.0, 0.0, 0.6666666666666666, 0.5795709387055483, 0.6054792621566897, 0.0, 1.0, 0.15515487738223357], 
reward next is 0.8448, 
noisyNet noise sample is [array([-0.03937384], dtype=float32), 0.6345391]. 
=============================================
[2019-04-04 00:50:24,939] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.2876378e-19 3.3148219e-12 9.1399239e-19 3.8364074e-14 5.2801388e-14
 1.1200466e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 00:50:24,940] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2602
[2019-04-04 00:50:24,967] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.333333333333333, 50.00000000000001, 113.6666666666667, 825.8333333333334, 26.0, 26.66097397205181, 0.5838884549287223, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3849600.0000, 
sim time next is 3850200.0000, 
raw observation next is [1.5, 49.5, 113.0, 824.0, 26.0, 26.10716292885248, 0.6300162273755254, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5041551246537397, 0.495, 0.37666666666666665, 0.9104972375690608, 0.6666666666666666, 0.6755969107377066, 0.7100054091251752, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2219083], dtype=float32), -0.45612386]. 
=============================================
[2019-04-04 00:50:25,536] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.2454308e-18 8.6586925e-11 2.6808783e-16 1.9569139e-13 8.8047061e-13
 3.1710134e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:50:25,536] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4489
[2019-04-04 00:50:25,553] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 72.0, 0.0, 0.0, 26.0, 25.19470527163882, 0.350766648706373, 0.0, 1.0, 51478.30591240621], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3811800.0000, 
sim time next is 3812400.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 26.0, 25.12611084809078, 0.3484268146778178, 0.0, 1.0, 46257.0685683475], 
processed observation next is [1.0, 0.13043478260869565, 0.3518005540166205, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5938425706742315, 0.6161422715592726, 0.0, 1.0, 0.22027175508736907], 
reward next is 0.7797, 
noisyNet noise sample is [array([-1.3785222], dtype=float32), -0.8716523]. 
=============================================
[2019-04-04 00:50:33,812] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.3716877e-20 7.1086165e-11 8.2198385e-18 1.7016975e-14 6.5607188e-14
 2.4573605e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:50:33,813] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9460
[2019-04-04 00:50:33,861] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.333333333333333, 36.66666666666666, 94.0, 504.0, 26.0, 25.89728179235071, 0.4577334877616163, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4092000.0000, 
sim time next is 4092600.0000, 
raw observation next is [-3.166666666666667, 37.33333333333334, 96.0, 539.0, 26.0, 26.12780531471229, 0.4570973908542486, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.3748845798707295, 0.3733333333333334, 0.32, 0.5955801104972376, 0.6666666666666666, 0.6773171095593575, 0.6523657969514162, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3541564], dtype=float32), -0.8661803]. 
=============================================
[2019-04-04 00:50:36,398] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.1341408e-18 1.2188933e-10 1.4039774e-16 5.5921088e-13 2.1614024e-12
 2.3664421e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:50:36,402] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0873
[2019-04-04 00:50:36,415] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 39.0, 0.0, 0.0, 26.0, 24.96376259008952, 0.3224538832028171, 0.0, 1.0, 40600.54388396289], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4062600.0000, 
sim time next is 4063200.0000, 
raw observation next is [-6.0, 39.66666666666666, 0.0, 0.0, 26.0, 24.97784708251739, 0.3163078937949389, 0.0, 1.0, 40618.80143239567], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.39666666666666656, 0.0, 0.0, 0.6666666666666666, 0.5814872568764491, 0.605435964598313, 0.0, 1.0, 0.19342286396378888], 
reward next is 0.8066, 
noisyNet noise sample is [array([-1.0216134], dtype=float32), -1.1742198]. 
=============================================
[2019-04-04 00:50:41,421] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.9318813e-20 3.4488688e-12 1.4070427e-18 3.2654198e-14 1.0254116e-13
 4.0966944e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 00:50:41,428] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3354
[2019-04-04 00:50:41,438] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.50596883668213, 0.5029114661760536, 1.0, 1.0, 19950.57455695306], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4472400.0000, 
sim time next is 4473000.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.45626731300642, 0.4883042269961293, 1.0, 1.0, 19890.27472151391], 
processed observation next is [1.0, 0.782608695652174, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6213556094172018, 0.6627680756653764, 1.0, 1.0, 0.094715593911971], 
reward next is 0.9053, 
noisyNet noise sample is [array([0.5799574], dtype=float32), 0.62558246]. 
=============================================
[2019-04-04 00:50:41,443] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[84.48101 ]
 [84.13854 ]
 [84.09497 ]
 [84.8196  ]
 [84.985725]], R is [[84.8159256 ]
 [84.87276459]
 [84.92713165]
 [84.97402954]
 [85.00184631]].
[2019-04-04 00:50:42,307] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.2379947e-21 1.2467030e-12 9.7956879e-19 1.5165032e-14 3.6834020e-15
 1.9302092e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 00:50:42,308] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7929
[2019-04-04 00:50:42,344] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 92.0, 164.6666666666667, 4.0, 26.0, 25.44752109312567, 0.5431178613526367, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4453800.0000, 
sim time next is 4454400.0000, 
raw observation next is [0.0, 92.0, 180.3333333333333, 5.0, 26.0, 25.6379703121101, 0.5825591993241876, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.46260387811634357, 0.92, 0.6011111111111109, 0.0055248618784530384, 0.6666666666666666, 0.6364975260091749, 0.6941863997747292, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.32475153], dtype=float32), -0.063178815]. 
=============================================
[2019-04-04 00:50:44,528] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.7017816e-18 9.7304033e-11 1.2153511e-17 2.0338363e-13 7.4846244e-13
 7.3128525e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:50:44,529] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6174
[2019-04-04 00:50:44,545] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.5, 32.0, 118.0, 847.0, 26.0, 25.0798418743478, 0.3974696684404031, 0.0, 1.0, 18695.8491437736], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4192200.0000, 
sim time next is 4192800.0000, 
raw observation next is [1.666666666666667, 32.66666666666666, 134.0, 817.3333333333334, 26.0, 25.07532997619406, 0.3990934951235127, 0.0, 1.0, 18695.43213449893], 
processed observation next is [0.0, 0.5217391304347826, 0.5087719298245615, 0.32666666666666655, 0.44666666666666666, 0.9031307550644567, 0.6666666666666666, 0.5896108313495049, 0.633031165041171, 0.0, 1.0, 0.08902586730713775], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.71521074], dtype=float32), 0.7048792]. 
=============================================
[2019-04-04 00:50:47,528] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0331192e-20 1.2373338e-12 6.6811233e-19 1.2083448e-14 1.8177190e-14
 6.2391366e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 00:50:47,528] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0261
[2019-04-04 00:50:47,556] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 29.66666666666667, 116.6666666666667, 831.8333333333334, 26.0, 26.39374158290374, 0.6366224411783182, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4108800.0000, 
sim time next is 4109400.0000, 
raw observation next is [3.0, 30.0, 116.0, 830.0, 26.0, 26.47188451440882, 0.5482515630635704, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5457063711911359, 0.3, 0.38666666666666666, 0.9171270718232044, 0.6666666666666666, 0.705990376200735, 0.6827505210211902, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7728437], dtype=float32), 0.3534697]. 
=============================================
[2019-04-04 00:50:51,865] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.1557553e-18 1.4216650e-09 1.4537480e-16 5.5359477e-13 2.6654859e-12
 2.4963464e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:50:51,866] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2245
[2019-04-04 00:50:51,883] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 26.0, 25.3924182460747, 0.3315340888278402, 0.0, 1.0, 51230.50379604812], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4246200.0000, 
sim time next is 4246800.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 26.0, 25.38960856574069, 0.3332044969452264, 0.0, 1.0, 46744.34777309195], 
processed observation next is [0.0, 0.13043478260869565, 0.5457063711911359, 0.45, 0.0, 0.0, 0.6666666666666666, 0.6158007138117242, 0.6110681656484088, 0.0, 1.0, 0.2225921322528188], 
reward next is 0.7774, 
noisyNet noise sample is [array([0.09098475], dtype=float32), -0.410706]. 
=============================================
[2019-04-04 00:50:51,908] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [5.6551586e-20 6.0461683e-12 7.7306620e-19 1.4404842e-14 3.5962668e-14
 1.8600917e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 00:50:51,910] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3423
[2019-04-04 00:50:51,927] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 60.0, 146.5, 638.0, 26.0, 26.44661745943677, 0.6264626186598731, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4615200.0000, 
sim time next is 4615800.0000, 
raw observation next is [0.3333333333333333, 58.66666666666666, 140.6666666666667, 681.0, 26.0, 26.49634010286344, 0.6442891209490954, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4718374884579871, 0.5866666666666666, 0.468888888888889, 0.7524861878453039, 0.6666666666666666, 0.7080283419052865, 0.7147630403163651, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1337119], dtype=float32), -1.5150476]. 
=============================================
[2019-04-04 00:50:56,928] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.0482105e-19 9.9655999e-11 2.5629946e-17 2.6074013e-13 1.2916568e-12
 1.8986122e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:50:56,930] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9499
[2019-04-04 00:50:56,960] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 55.0, 0.0, 0.0, 26.0, 25.42018563747419, 0.398769494746913, 0.0, 1.0, 26283.29589231608], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4831800.0000, 
sim time next is 4832400.0000, 
raw observation next is [-1.0, 55.0, 0.0, 0.0, 26.0, 25.4021136260202, 0.392895754940746, 0.0, 1.0, 40883.64912220329], 
processed observation next is [0.0, 0.9565217391304348, 0.4349030470914128, 0.55, 0.0, 0.0, 0.6666666666666666, 0.61684280216835, 0.6309652516469154, 0.0, 1.0, 0.1946840434390633], 
reward next is 0.8053, 
noisyNet noise sample is [array([0.6396661], dtype=float32), 0.8310924]. 
=============================================
[2019-04-04 00:51:01,262] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.0483556e-20 5.3278432e-12 1.0779825e-17 6.2552448e-14 4.4396280e-14
 4.4666776e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:51:01,262] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9276
[2019-04-04 00:51:01,284] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.7, 62.0, 0.0, 0.0, 26.0, 25.4372086035494, 0.4575492384960835, 0.0, 1.0, 44877.94352404569], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4581000.0000, 
sim time next is 4581600.0000, 
raw observation next is [0.6000000000000001, 62.33333333333334, 0.0, 0.0, 26.0, 25.44949104116888, 0.4587839950303709, 0.0, 1.0, 31299.44606638633], 
processed observation next is [1.0, 0.0, 0.479224376731302, 0.6233333333333334, 0.0, 0.0, 0.6666666666666666, 0.6207909200974067, 0.6529279983434569, 0.0, 1.0, 0.14904498126850632], 
reward next is 0.8510, 
noisyNet noise sample is [array([0.59755564], dtype=float32), 0.58260167]. 
=============================================
[2019-04-04 00:51:07,605] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.2603157e-18 7.0714989e-10 1.4642223e-16 1.0562926e-12 1.3331694e-12
 2.0310411e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:51:07,608] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3272
[2019-04-04 00:51:07,619] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 50.0, 0.0, 0.0, 26.0, 25.35105733881791, 0.3477685240163191, 0.0, 1.0, 45565.18796800071], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4932000.0000, 
sim time next is 4932600.0000, 
raw observation next is [-1.0, 50.0, 0.0, 0.0, 26.0, 25.54949225135038, 0.3474200590953562, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.4349030470914128, 0.5, 0.0, 0.0, 0.6666666666666666, 0.6291243542791983, 0.6158066863651187, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.7269293], dtype=float32), -0.3964493]. 
=============================================
[2019-04-04 00:51:12,998] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:51:12,998] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:51:13,003] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run16
[2019-04-04 00:51:14,733] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.2635463e-20 2.2455993e-13 2.1211992e-19 5.9787516e-15 1.3303257e-14
 1.2842223e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 00:51:14,734] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3106
[2019-04-04 00:51:14,761] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 49.0, 121.0, 846.0, 26.0, 26.72577509599062, 0.7441834707921245, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4622400.0000, 
sim time next is 4623000.0000, 
raw observation next is [3.166666666666667, 49.0, 120.6666666666667, 850.3333333333334, 26.0, 26.95974007265307, 0.6546392736390853, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.5503231763619576, 0.49, 0.4022222222222223, 0.9395948434622469, 0.6666666666666666, 0.7466450060544224, 0.7182130912130283, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0994624], dtype=float32), 0.66267574]. 
=============================================
[2019-04-04 00:51:14,773] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[89.05758 ]
 [89.009895]
 [89.111015]
 [89.194664]
 [89.43484 ]], R is [[89.05725861]
 [89.16668701]
 [89.27502441]
 [89.38227844]
 [89.48845673]].
[2019-04-04 00:51:15,759] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.7327059e-20 6.7713066e-13 4.6570488e-18 1.7348632e-14 3.0874530e-14
 3.9539505e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 00:51:15,759] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1313
[2019-04-04 00:51:15,808] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.2, 48.5, 26.66666666666667, 96.66666666666667, 26.0, 26.73049007624926, 0.7494901823288034, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4643400.0000, 
sim time next is 4644000.0000, 
raw observation next is [4.0, 49.0, 0.0, 0.0, 26.0, 26.26706104113046, 0.6962123187442159, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.5734072022160666, 0.49, 0.0, 0.0, 0.6666666666666666, 0.6889217534275384, 0.7320707729147387, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.92880464], dtype=float32), 0.17720161]. 
=============================================
[2019-04-04 00:51:15,838] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[83.61449 ]
 [83.92283 ]
 [84.37401 ]
 [84.62132 ]
 [84.698906]], R is [[82.36562347]
 [82.5419693 ]
 [82.71655273]
 [82.88938904]
 [83.06049347]].
[2019-04-04 00:51:16,788] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1684110e-20 7.2787262e-13 1.1809005e-18 1.1698552e-14 9.3481995e-15
 6.6560849e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 00:51:16,796] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0622
[2019-04-04 00:51:16,855] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 78.0, 0.0, 0.0, 26.0, 25.01826159268358, 0.4458151396379726, 1.0, 1.0, 95487.84026258503], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4734000.0000, 
sim time next is 4734600.0000, 
raw observation next is [-1.0, 78.0, 0.0, 0.0, 26.0, 25.07691521782707, 0.4731473005908312, 1.0, 1.0, 25704.85573417582], 
processed observation next is [1.0, 0.8260869565217391, 0.4349030470914128, 0.78, 0.0, 0.0, 0.6666666666666666, 0.5897429348189226, 0.6577157668636104, 1.0, 1.0, 0.12240407492464676], 
reward next is 0.8776, 
noisyNet noise sample is [array([-1.0308259], dtype=float32), 0.26277018]. 
=============================================
[2019-04-04 00:51:17,288] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:51:17,288] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:51:17,293] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run16
[2019-04-04 00:51:17,331] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0102666e-18 9.3658381e-10 1.3798558e-16 3.1093799e-13 1.0962400e-12
 6.8096523e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:51:17,334] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0341
[2019-04-04 00:51:17,355] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.066666666666667, 46.66666666666667, 281.6666666666666, 362.6666666666667, 26.0, 25.0530032667596, 0.3570284974588975, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4882200.0000, 
sim time next is 4882800.0000, 
raw observation next is [1.133333333333333, 46.33333333333334, 281.3333333333334, 376.3333333333333, 26.0, 25.09101525188339, 0.3629240998646095, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.49399815327793173, 0.46333333333333343, 0.937777777777778, 0.4158379373848987, 0.6666666666666666, 0.5909179376569492, 0.6209746999548699, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.09668388], dtype=float32), 0.036798116]. 
=============================================
[2019-04-04 00:51:19,605] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:51:19,605] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:51:19,628] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run16
[2019-04-04 00:51:21,487] A3C_AGENT_WORKER-Thread-19 INFO:Local step 127500, global step 2033645: loss 0.0044
[2019-04-04 00:51:21,487] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 127500, global step 2033645: learning rate 0.0001
[2019-04-04 00:51:21,617] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.5996137e-20 4.0803580e-12 5.1347079e-18 3.1206015e-14 5.0158875e-14
 2.7044050e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 00:51:21,617] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7764
[2019-04-04 00:51:21,680] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 78.0, 0.0, 0.0, 26.0, 25.2274755260652, 0.4408655647366916, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4736400.0000, 
sim time next is 4737000.0000, 
raw observation next is [-1.0, 78.0, 0.0, 0.0, 26.0, 25.14371949618502, 0.423772834711716, 0.0, 1.0, 26466.84550367529], 
processed observation next is [1.0, 0.8260869565217391, 0.4349030470914128, 0.78, 0.0, 0.0, 0.6666666666666666, 0.5953099580154184, 0.641257611570572, 0.0, 1.0, 0.126032597636549], 
reward next is 0.8740, 
noisyNet noise sample is [array([-0.29042044], dtype=float32), 2.3495655]. 
=============================================
[2019-04-04 00:51:21,683] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[82.02702 ]
 [82.07161 ]
 [82.087204]
 [82.2129  ]
 [82.232346]], R is [[78.54608154]
 [78.76062012]
 [78.97301483]
 [79.18328857]
 [79.2690506 ]].
[2019-04-04 00:51:26,246] A3C_AGENT_WORKER-Thread-17 INFO:Local step 127500, global step 2035444: loss 0.0057
[2019-04-04 00:51:26,248] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 127500, global step 2035444: learning rate 0.0001
[2019-04-04 00:51:28,702] A3C_AGENT_WORKER-Thread-7 INFO:Local step 127500, global step 2036492: loss 0.0053
[2019-04-04 00:51:28,704] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 127500, global step 2036492: learning rate 0.0001
[2019-04-04 00:51:29,981] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:51:29,981] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:51:29,984] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run16
[2019-04-04 00:51:30,123] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:51:30,124] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:51:30,148] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run16
[2019-04-04 00:51:30,967] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:51:30,967] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:51:30,984] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run16
[2019-04-04 00:51:31,699] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:51:31,700] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:51:31,703] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run16
[2019-04-04 00:51:35,609] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:51:35,609] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:51:35,617] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run16
[2019-04-04 00:51:35,648] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:51:35,648] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:51:35,668] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run16
[2019-04-04 00:51:36,385] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:51:36,385] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:51:36,388] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run16
[2019-04-04 00:51:38,333] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:51:38,333] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:51:38,336] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run16
[2019-04-04 00:51:39,285] A3C_AGENT_WORKER-Thread-14 INFO:Local step 127500, global step 2039689: loss 0.0080
[2019-04-04 00:51:39,288] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 127500, global step 2039689: learning rate 0.0001
[2019-04-04 00:51:39,309] A3C_AGENT_WORKER-Thread-2 INFO:Local step 127500, global step 2039694: loss 0.0074
[2019-04-04 00:51:39,309] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 127500, global step 2039694: learning rate 0.0001
[2019-04-04 00:51:40,143] A3C_AGENT_WORKER-Thread-6 INFO:Local step 127500, global step 2039897: loss 0.0081
[2019-04-04 00:51:40,143] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 127500, global step 2039897: learning rate 0.0001
[2019-04-04 00:51:40,295] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:51:40,296] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:51:40,303] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run16
[2019-04-04 00:51:41,569] A3C_AGENT_WORKER-Thread-5 INFO:Local step 127500, global step 2040104: loss 0.0081
[2019-04-04 00:51:41,569] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 127500, global step 2040104: learning rate 0.0001
[2019-04-04 00:51:41,963] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:51:41,964] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:51:41,968] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run16
[2019-04-04 00:51:43,826] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.6264859e-16 5.8284648e-09 1.9018346e-14 3.3889048e-11 2.2879188e-11
 5.6628146e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 00:51:43,826] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5949
[2019-04-04 00:51:43,844] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.2, 96.0, 0.0, 0.0, 26.0, 20.36229974774753, -0.778071066756464, 0.0, 1.0, 43319.686348712], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 7800.0000, 
sim time next is 8400.0000, 
raw observation next is [7.200000000000001, 96.0, 0.0, 0.0, 26.0, 20.41049389423034, -0.7644837452068821, 0.0, 1.0, 43070.62376306211], 
processed observation next is [0.0, 0.08695652173913043, 0.662049861495845, 0.96, 0.0, 0.0, 0.6666666666666666, 0.20087449118586154, 0.24517208493103929, 0.0, 1.0, 0.20509820839553386], 
reward next is 0.7949, 
noisyNet noise sample is [array([1.8912137], dtype=float32), -0.8911468]. 
=============================================
[2019-04-04 00:51:45,265] A3C_AGENT_WORKER-Thread-3 INFO:Local step 127500, global step 2040964: loss 0.0088
[2019-04-04 00:51:45,266] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 127500, global step 2040964: learning rate 0.0001
[2019-04-04 00:51:45,394] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.6762529e-20 7.8500899e-13 3.2429936e-18 5.7238861e-15 7.3734604e-14
 2.8663757e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 00:51:45,394] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6669
[2019-04-04 00:51:45,409] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.6, 19.33333333333334, 0.0, 0.0, 26.0, 26.7042213066757, 0.6972689219657463, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5091600.0000, 
sim time next is 5092200.0000, 
raw observation next is [8.55, 19.5, 0.0, 0.0, 26.0, 26.53887574132229, 0.670699141549532, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.6994459833795015, 0.195, 0.0, 0.0, 0.6666666666666666, 0.7115729784435242, 0.7235663805165107, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.23125742], dtype=float32), -0.40772042]. 
=============================================
[2019-04-04 00:51:45,803] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:51:45,804] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:51:45,807] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run16
[2019-04-04 00:51:45,963] A3C_AGENT_WORKER-Thread-8 INFO:Local step 127500, global step 2041137: loss 0.0087
[2019-04-04 00:51:45,964] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 127500, global step 2041137: learning rate 0.0001
[2019-04-04 00:51:46,041] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:51:46,041] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:51:46,044] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run16
[2019-04-04 00:51:46,219] A3C_AGENT_WORKER-Thread-18 INFO:Local step 127500, global step 2041179: loss 0.0082
[2019-04-04 00:51:46,220] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 127500, global step 2041179: learning rate 0.0001
[2019-04-04 00:51:46,579] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 00:51:46,579] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:51:46,582] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run16
[2019-04-04 00:51:47,865] A3C_AGENT_WORKER-Thread-4 INFO:Local step 127500, global step 2041430: loss 0.0069
[2019-04-04 00:51:47,865] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 127500, global step 2041430: learning rate 0.0001
[2019-04-04 00:51:50,924] A3C_AGENT_WORKER-Thread-15 INFO:Local step 127500, global step 2041991: loss 0.0078
[2019-04-04 00:51:50,926] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 127500, global step 2041991: learning rate 0.0001
[2019-04-04 00:51:51,895] A3C_AGENT_WORKER-Thread-19 INFO:Local step 128000, global step 2042188: loss 0.2819
[2019-04-04 00:51:51,897] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 128000, global step 2042188: learning rate 0.0001
[2019-04-04 00:51:52,071] A3C_AGENT_WORKER-Thread-16 INFO:Local step 127500, global step 2042230: loss 0.0104
[2019-04-04 00:51:52,072] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 127500, global step 2042230: learning rate 0.0001
[2019-04-04 00:51:55,156] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.0231895e-18 3.4866905e-11 1.2648654e-16 1.3405020e-13 2.5971122e-13
 4.2398383e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:51:55,156] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9638
[2019-04-04 00:51:55,181] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 26.0, 23.08932940744918, -0.1513679740907117, 0.0, 1.0, 44266.14068746554], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 187200.0000, 
sim time next is 187800.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 26.0, 23.08898998626671, -0.1589418719373603, 0.0, 1.0, 44279.72124114567], 
processed observation next is [1.0, 0.17391304347826086, 0.21606648199445982, 0.78, 0.0, 0.0, 0.6666666666666666, 0.4240824988555592, 0.4470193760208799, 0.0, 1.0, 0.210855815434027], 
reward next is 0.7891, 
noisyNet noise sample is [array([-0.60113513], dtype=float32), -1.1776286]. 
=============================================
[2019-04-04 00:51:55,735] A3C_AGENT_WORKER-Thread-9 INFO:Local step 127500, global step 2043294: loss 0.0276
[2019-04-04 00:51:55,736] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 127500, global step 2043294: learning rate 0.0001
[2019-04-04 00:51:55,742] A3C_AGENT_WORKER-Thread-10 INFO:Local step 127500, global step 2043298: loss 0.0288
[2019-04-04 00:51:55,743] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 127500, global step 2043298: learning rate 0.0001
[2019-04-04 00:51:56,152] A3C_AGENT_WORKER-Thread-20 INFO:Local step 127500, global step 2043397: loss 0.0304
[2019-04-04 00:51:56,153] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 127500, global step 2043397: learning rate 0.0001
[2019-04-04 00:51:56,663] A3C_AGENT_WORKER-Thread-17 INFO:Local step 128000, global step 2043495: loss 0.3503
[2019-04-04 00:51:56,664] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 128000, global step 2043495: learning rate 0.0001
[2019-04-04 00:52:00,135] A3C_AGENT_WORKER-Thread-7 INFO:Local step 128000, global step 2044309: loss 0.4295
[2019-04-04 00:52:00,137] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 128000, global step 2044310: learning rate 0.0001
[2019-04-04 00:52:06,599] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3108129e-18 2.4874008e-10 6.9994981e-16 2.1499382e-13 3.3975498e-12
 2.2625339e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:52:06,599] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5295
[2019-04-04 00:52:06,712] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.92349736331452, -0.1938747755881511, 0.0, 1.0, 44493.17285049838], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 189600.0000, 
sim time next is 190200.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.88744151424562, -0.1961068043343705, 0.0, 1.0, 44575.34330968801], 
processed observation next is [1.0, 0.17391304347826086, 0.21606648199445982, 0.78, 0.0, 0.0, 0.6666666666666666, 0.4072867928538016, 0.4346310652218765, 0.0, 1.0, 0.2122635395699429], 
reward next is 0.7877, 
noisyNet noise sample is [array([-0.6259936], dtype=float32), -1.3667431]. 
=============================================
[2019-04-04 00:52:10,258] A3C_AGENT_WORKER-Thread-2 INFO:Local step 128000, global step 2047283: loss 0.5157
[2019-04-04 00:52:10,260] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 128000, global step 2047283: learning rate 0.0001
[2019-04-04 00:52:10,393] A3C_AGENT_WORKER-Thread-14 INFO:Local step 128000, global step 2047328: loss 0.5925
[2019-04-04 00:52:10,393] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 128000, global step 2047328: learning rate 0.0001
[2019-04-04 00:52:10,759] A3C_AGENT_WORKER-Thread-6 INFO:Local step 128000, global step 2047437: loss 0.5610
[2019-04-04 00:52:10,773] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 128000, global step 2047437: learning rate 0.0001
[2019-04-04 00:52:12,139] A3C_AGENT_WORKER-Thread-5 INFO:Local step 128000, global step 2047805: loss 0.5582
[2019-04-04 00:52:12,139] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 128000, global step 2047805: learning rate 0.0001
[2019-04-04 00:52:16,130] A3C_AGENT_WORKER-Thread-3 INFO:Local step 128000, global step 2048795: loss 0.5286
[2019-04-04 00:52:16,130] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 128000, global step 2048795: learning rate 0.0001
[2019-04-04 00:52:16,876] A3C_AGENT_WORKER-Thread-18 INFO:Local step 128000, global step 2048990: loss 0.5764
[2019-04-04 00:52:16,877] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 128000, global step 2048990: learning rate 0.0001
[2019-04-04 00:52:16,919] A3C_AGENT_WORKER-Thread-8 INFO:Local step 128000, global step 2049007: loss 0.6082
[2019-04-04 00:52:16,925] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 128000, global step 2049007: learning rate 0.0001
[2019-04-04 00:52:17,740] A3C_AGENT_WORKER-Thread-4 INFO:Local step 128000, global step 2049303: loss 0.7704
[2019-04-04 00:52:17,743] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 128000, global step 2049303: learning rate 0.0001
[2019-04-04 00:52:17,900] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.6705094e-18 9.6901015e-11 2.5151304e-16 4.2385231e-13 2.0359138e-12
 6.4792520e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:52:17,905] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.1287
[2019-04-04 00:52:17,938] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-15.0, 69.0, 0.0, 0.0, 26.0, 23.20465457082566, -0.1682197647735297, 0.0, 1.0, 48611.32968001892], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 353400.0000, 
sim time next is 354000.0000, 
raw observation next is [-15.0, 69.0, 0.0, 0.0, 26.0, 23.08023406681285, -0.1852696629006809, 0.0, 1.0, 48829.27456300599], 
processed observation next is [1.0, 0.08695652173913043, 0.04709141274238226, 0.69, 0.0, 0.0, 0.6666666666666666, 0.4233528389010708, 0.438243445699773, 0.0, 1.0, 0.23252035506193328], 
reward next is 0.7675, 
noisyNet noise sample is [array([-0.6550964], dtype=float32), 1.1819888]. 
=============================================
[2019-04-04 00:52:17,945] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[73.15843]
 [73.1963 ]
 [73.21861]
 [73.21028]
 [73.16075]], R is [[73.13039398]
 [73.16761017]
 [73.20550537]
 [73.24391937]
 [73.28273773]].
[2019-04-04 00:52:18,940] A3C_AGENT_WORKER-Thread-19 INFO:Local step 128500, global step 2049706: loss 0.8129
[2019-04-04 00:52:18,943] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 128500, global step 2049707: learning rate 0.0001
[2019-04-04 00:52:20,786] A3C_AGENT_WORKER-Thread-15 INFO:Local step 128000, global step 2050196: loss 0.8236
[2019-04-04 00:52:20,787] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 128000, global step 2050196: learning rate 0.0001
[2019-04-04 00:52:22,120] A3C_AGENT_WORKER-Thread-16 INFO:Local step 128000, global step 2050529: loss 0.8835
[2019-04-04 00:52:22,120] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 128000, global step 2050529: learning rate 0.0001
[2019-04-04 00:52:23,392] A3C_AGENT_WORKER-Thread-17 INFO:Local step 128500, global step 2050882: loss 0.7862
[2019-04-04 00:52:23,392] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 128500, global step 2050882: learning rate 0.0001
[2019-04-04 00:52:24,914] A3C_AGENT_WORKER-Thread-20 INFO:Local step 128000, global step 2051294: loss 0.8390
[2019-04-04 00:52:24,917] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 128000, global step 2051294: learning rate 0.0001
[2019-04-04 00:52:25,156] A3C_AGENT_WORKER-Thread-9 INFO:Local step 128000, global step 2051369: loss 0.8168
[2019-04-04 00:52:25,156] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 128000, global step 2051369: learning rate 0.0001
[2019-04-04 00:52:25,354] A3C_AGENT_WORKER-Thread-10 INFO:Local step 128000, global step 2051443: loss 0.7805
[2019-04-04 00:52:25,366] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 128000, global step 2051445: learning rate 0.0001
[2019-04-04 00:52:25,631] A3C_AGENT_WORKER-Thread-7 INFO:Local step 128500, global step 2051530: loss 0.7607
[2019-04-04 00:52:25,632] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 128500, global step 2051530: learning rate 0.0001
[2019-04-04 00:52:35,752] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.5714661e-21 1.4606820e-13 2.1617329e-19 6.8500198e-15 3.7817861e-15
 4.6743415e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 00:52:35,752] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7523
[2019-04-04 00:52:35,842] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.633333333333333, 56.33333333333333, 0.0, 0.0, 26.0, 25.06787072106033, 0.2990073418884059, 1.0, 1.0, 45505.01498298821], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 762000.0000, 
sim time next is 762600.0000, 
raw observation next is [-4.816666666666666, 57.16666666666667, 0.0, 0.0, 26.0, 24.98154187192152, 0.2987186756393406, 1.0, 1.0, 96870.94378082962], 
processed observation next is [1.0, 0.8260869565217391, 0.32917820867959374, 0.5716666666666668, 0.0, 0.0, 0.6666666666666666, 0.5817951559934601, 0.5995728918797801, 1.0, 1.0, 0.4612902084801411], 
reward next is 0.5387, 
noisyNet noise sample is [array([0.8303175], dtype=float32), 0.68157446]. 
=============================================
[2019-04-04 00:52:37,314] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.2820192e-19 1.8622773e-10 9.2926886e-17 7.0178055e-14 2.3949918e-13
 8.0698980e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:52:37,314] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6251
[2019-04-04 00:52:37,327] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 71.5, 0.0, 0.0, 26.0, 23.89929049976794, 0.02500865469683334, 0.0, 1.0, 44399.16851028537], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 621000.0000, 
sim time next is 621600.0000, 
raw observation next is [-4.5, 70.33333333333333, 0.0, 0.0, 26.0, 23.86334312901314, 0.01831678605294676, 0.0, 1.0, 44443.80314036187], 
processed observation next is [0.0, 0.17391304347826086, 0.3379501385041552, 0.7033333333333333, 0.0, 0.0, 0.6666666666666666, 0.48861192741776155, 0.5061055953509822, 0.0, 1.0, 0.211637157811247], 
reward next is 0.7884, 
noisyNet noise sample is [array([0.33423758], dtype=float32), 2.266372]. 
=============================================
[2019-04-04 00:52:38,816] A3C_AGENT_WORKER-Thread-6 INFO:Local step 128500, global step 2054922: loss 0.6654
[2019-04-04 00:52:38,816] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 128500, global step 2054922: learning rate 0.0001
[2019-04-04 00:52:39,086] A3C_AGENT_WORKER-Thread-2 INFO:Local step 128500, global step 2054967: loss 0.6594
[2019-04-04 00:52:39,087] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 128500, global step 2054967: learning rate 0.0001
[2019-04-04 00:52:39,113] A3C_AGENT_WORKER-Thread-14 INFO:Local step 128500, global step 2054972: loss 0.6705
[2019-04-04 00:52:39,113] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 128500, global step 2054972: learning rate 0.0001
[2019-04-04 00:52:41,577] A3C_AGENT_WORKER-Thread-5 INFO:Local step 128500, global step 2055443: loss 0.6263
[2019-04-04 00:52:41,598] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 128500, global step 2055443: learning rate 0.0001
[2019-04-04 00:52:46,022] A3C_AGENT_WORKER-Thread-3 INFO:Local step 128500, global step 2056560: loss 0.6350
[2019-04-04 00:52:46,037] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 128500, global step 2056561: learning rate 0.0001
[2019-04-04 00:52:46,561] A3C_AGENT_WORKER-Thread-8 INFO:Local step 128500, global step 2056690: loss 0.6385
[2019-04-04 00:52:46,563] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 128500, global step 2056690: learning rate 0.0001
[2019-04-04 00:52:47,488] A3C_AGENT_WORKER-Thread-18 INFO:Local step 128500, global step 2056938: loss 0.6439
[2019-04-04 00:52:47,489] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 128500, global step 2056938: learning rate 0.0001
[2019-04-04 00:52:48,280] A3C_AGENT_WORKER-Thread-19 INFO:Local step 129000, global step 2057159: loss 5.9467
[2019-04-04 00:52:48,281] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 129000, global step 2057159: learning rate 0.0001
[2019-04-04 00:52:48,976] A3C_AGENT_WORKER-Thread-4 INFO:Local step 128500, global step 2057304: loss 0.6377
[2019-04-04 00:52:48,976] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 128500, global step 2057304: learning rate 0.0001
[2019-04-04 00:52:52,669] A3C_AGENT_WORKER-Thread-15 INFO:Local step 128500, global step 2058185: loss 0.6584
[2019-04-04 00:52:52,669] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 128500, global step 2058185: learning rate 0.0001
[2019-04-04 00:52:54,638] A3C_AGENT_WORKER-Thread-17 INFO:Local step 129000, global step 2058706: loss 5.9341
[2019-04-04 00:52:54,639] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 129000, global step 2058706: learning rate 0.0001
[2019-04-04 00:52:55,070] A3C_AGENT_WORKER-Thread-16 INFO:Local step 128500, global step 2058809: loss 0.6303
[2019-04-04 00:52:55,071] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 128500, global step 2058810: learning rate 0.0001
[2019-04-04 00:52:57,372] A3C_AGENT_WORKER-Thread-20 INFO:Local step 128500, global step 2059476: loss 0.5981
[2019-04-04 00:52:57,372] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 128500, global step 2059476: learning rate 0.0001
[2019-04-04 00:52:57,404] A3C_AGENT_WORKER-Thread-9 INFO:Local step 128500, global step 2059486: loss 0.5981
[2019-04-04 00:52:57,404] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 128500, global step 2059486: learning rate 0.0001
[2019-04-04 00:52:57,947] A3C_AGENT_WORKER-Thread-7 INFO:Local step 129000, global step 2059627: loss 6.0455
[2019-04-04 00:52:57,947] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 129000, global step 2059627: learning rate 0.0001
[2019-04-04 00:52:58,536] A3C_AGENT_WORKER-Thread-10 INFO:Local step 128500, global step 2059757: loss 0.6014
[2019-04-04 00:52:58,549] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 128500, global step 2059757: learning rate 0.0001
[2019-04-04 00:53:11,072] A3C_AGENT_WORKER-Thread-19 INFO:Local step 129500, global step 2062928: loss 0.1633
[2019-04-04 00:53:11,072] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 129500, global step 2062928: learning rate 0.0001
[2019-04-04 00:53:12,472] A3C_AGENT_WORKER-Thread-2 INFO:Local step 129000, global step 2063245: loss 6.2313
[2019-04-04 00:53:12,472] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 129000, global step 2063245: learning rate 0.0001
[2019-04-04 00:53:12,546] A3C_AGENT_WORKER-Thread-14 INFO:Local step 129000, global step 2063263: loss 6.2253
[2019-04-04 00:53:12,546] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 129000, global step 2063263: learning rate 0.0001
[2019-04-04 00:53:13,880] A3C_AGENT_WORKER-Thread-6 INFO:Local step 129000, global step 2063582: loss 6.1176
[2019-04-04 00:53:13,882] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 129000, global step 2063582: learning rate 0.0001
[2019-04-04 00:53:15,399] A3C_AGENT_WORKER-Thread-5 INFO:Local step 129000, global step 2063961: loss 6.0776
[2019-04-04 00:53:15,400] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 129000, global step 2063961: learning rate 0.0001
[2019-04-04 00:53:19,136] A3C_AGENT_WORKER-Thread-3 INFO:Local step 129000, global step 2064941: loss 6.0460
[2019-04-04 00:53:19,136] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 129000, global step 2064941: learning rate 0.0001
[2019-04-04 00:53:19,241] A3C_AGENT_WORKER-Thread-17 INFO:Local step 129500, global step 2064958: loss 0.0943
[2019-04-04 00:53:19,251] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 129500, global step 2064959: learning rate 0.0001
[2019-04-04 00:53:21,167] A3C_AGENT_WORKER-Thread-8 INFO:Local step 129000, global step 2065458: loss 5.9961
[2019-04-04 00:53:21,167] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 129000, global step 2065458: learning rate 0.0001
[2019-04-04 00:53:22,104] A3C_AGENT_WORKER-Thread-18 INFO:Local step 129000, global step 2065719: loss 5.9958
[2019-04-04 00:53:22,105] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 129000, global step 2065719: learning rate 0.0001
[2019-04-04 00:53:22,806] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.6068502e-22 1.3927515e-12 1.3563070e-19 1.1822772e-15 3.5166580e-15
 4.9627282e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 00:53:22,806] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6387
[2019-04-04 00:53:22,886] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 84.0, 0.0, 0.0, 26.0, 24.92604202028652, 0.2699186004031498, 1.0, 1.0, 129037.7485034698], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 844200.0000, 
sim time next is 844800.0000, 
raw observation next is [-3.899999999999999, 84.66666666666666, 0.0, 0.0, 26.0, 24.80948289449796, 0.3005154191156528, 1.0, 1.0, 153271.379853048], 
processed observation next is [1.0, 0.782608695652174, 0.35457063711911363, 0.8466666666666666, 0.0, 0.0, 0.6666666666666666, 0.56745690787483, 0.6001718063718843, 1.0, 1.0, 0.7298637135859429], 
reward next is 0.2701, 
noisyNet noise sample is [array([-0.00061951], dtype=float32), 0.710782]. 
=============================================
[2019-04-04 00:53:23,228] A3C_AGENT_WORKER-Thread-7 INFO:Local step 129500, global step 2066033: loss 0.0946
[2019-04-04 00:53:23,231] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 129500, global step 2066033: learning rate 0.0001
[2019-04-04 00:53:23,272] A3C_AGENT_WORKER-Thread-4 INFO:Local step 129000, global step 2066046: loss 5.9376
[2019-04-04 00:53:23,276] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 129000, global step 2066047: learning rate 0.0001
[2019-04-04 00:53:26,601] A3C_AGENT_WORKER-Thread-15 INFO:Local step 129000, global step 2067057: loss 6.0228
[2019-04-04 00:53:26,611] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 129000, global step 2067058: learning rate 0.0001
[2019-04-04 00:53:29,079] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.9054541e-21 8.2620837e-12 5.6677950e-19 4.3449123e-15 4.4565537e-14
 1.1135928e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:53:29,080] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4016
[2019-04-04 00:53:29,089] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.1, 77.0, 0.0, 0.0, 26.0, 25.6437720731439, 0.6282781818989502, 0.0, 1.0, 20036.27574748692], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1134000.0000, 
sim time next is 1134600.0000, 
raw observation next is [11.1, 77.0, 0.0, 0.0, 26.0, 25.64770453532875, 0.6267191414565438, 0.0, 1.0, 19953.75630117104], 
processed observation next is [0.0, 0.13043478260869565, 0.7700831024930749, 0.77, 0.0, 0.0, 0.6666666666666666, 0.6373087112773957, 0.7089063804855146, 0.0, 1.0, 0.09501788714843352], 
reward next is 0.9050, 
noisyNet noise sample is [array([0.2278548], dtype=float32), -0.97402906]. 
=============================================
[2019-04-04 00:53:29,587] A3C_AGENT_WORKER-Thread-16 INFO:Local step 129000, global step 2067969: loss 6.0970
[2019-04-04 00:53:29,587] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 129000, global step 2067969: learning rate 0.0001
[2019-04-04 00:53:30,448] A3C_AGENT_WORKER-Thread-9 INFO:Local step 129000, global step 2068260: loss 6.1144
[2019-04-04 00:53:30,449] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 129000, global step 2068260: learning rate 0.0001
[2019-04-04 00:53:31,553] A3C_AGENT_WORKER-Thread-20 INFO:Local step 129000, global step 2068562: loss 6.1138
[2019-04-04 00:53:31,557] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 129000, global step 2068562: learning rate 0.0001
[2019-04-04 00:53:32,221] A3C_AGENT_WORKER-Thread-10 INFO:Local step 129000, global step 2068758: loss 6.0728
[2019-04-04 00:53:32,222] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 129000, global step 2068758: learning rate 0.0001
[2019-04-04 00:53:36,317] A3C_AGENT_WORKER-Thread-14 INFO:Local step 129500, global step 2070207: loss 0.0454
[2019-04-04 00:53:36,318] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 129500, global step 2070207: learning rate 0.0001
[2019-04-04 00:53:36,797] A3C_AGENT_WORKER-Thread-2 INFO:Local step 129500, global step 2070366: loss 0.0403
[2019-04-04 00:53:36,799] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 129500, global step 2070366: learning rate 0.0001
[2019-04-04 00:53:38,245] A3C_AGENT_WORKER-Thread-6 INFO:Local step 129500, global step 2070850: loss 0.0484
[2019-04-04 00:53:38,257] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 129500, global step 2070853: learning rate 0.0001
[2019-04-04 00:53:39,773] A3C_AGENT_WORKER-Thread-5 INFO:Local step 129500, global step 2071262: loss 0.0339
[2019-04-04 00:53:39,784] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 129500, global step 2071262: learning rate 0.0001
[2019-04-04 00:53:40,160] A3C_AGENT_WORKER-Thread-19 INFO:Local step 130000, global step 2071401: loss 0.1188
[2019-04-04 00:53:40,161] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 130000, global step 2071401: learning rate 0.0001
[2019-04-04 00:53:40,280] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1780288e-20 1.4887026e-12 5.4111576e-19 2.3791813e-15 3.6947184e-14
 5.2686342e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 00:53:40,286] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8049
[2019-04-04 00:53:40,317] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.0, 96.66666666666666, 0.0, 0.0, 26.0, 25.20577800406416, 0.5795225045278707, 0.0, 1.0, 41817.76492747151], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1282200.0000, 
sim time next is 1282800.0000, 
raw observation next is [5.9, 97.33333333333333, 0.0, 0.0, 26.0, 25.31268299776696, 0.5923362786962912, 0.0, 1.0, 40687.42214815506], 
processed observation next is [0.0, 0.8695652173913043, 0.626038781163435, 0.9733333333333333, 0.0, 0.0, 0.6666666666666666, 0.6093902498139133, 0.6974454262320972, 0.0, 1.0, 0.19374962927692888], 
reward next is 0.8063, 
noisyNet noise sample is [array([0.4826538], dtype=float32), 0.07314727]. 
=============================================
[2019-04-04 00:53:42,640] A3C_AGENT_WORKER-Thread-3 INFO:Local step 129500, global step 2072268: loss 0.0447
[2019-04-04 00:53:42,646] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 129500, global step 2072271: learning rate 0.0001
[2019-04-04 00:53:44,773] A3C_AGENT_WORKER-Thread-8 INFO:Local step 129500, global step 2072912: loss 0.0423
[2019-04-04 00:53:44,773] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 129500, global step 2072912: learning rate 0.0001
[2019-04-04 00:53:46,603] A3C_AGENT_WORKER-Thread-4 INFO:Local step 129500, global step 2073411: loss 0.0486
[2019-04-04 00:53:46,603] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 129500, global step 2073411: learning rate 0.0001
[2019-04-04 00:53:46,815] A3C_AGENT_WORKER-Thread-18 INFO:Local step 129500, global step 2073462: loss 0.0577
[2019-04-04 00:53:46,828] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 129500, global step 2073462: learning rate 0.0001
[2019-04-04 00:53:47,355] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.6094325e-22 3.7820621e-14 2.7541715e-19 2.6058126e-15 4.6961607e-15
 2.2276984e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 00:53:47,355] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8200
[2019-04-04 00:53:47,388] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.16666666666667, 53.0, 0.0, 26.0, 25.70682445484368, 0.5131441649732681, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1350600.0000, 
sim time next is 1351200.0000, 
raw observation next is [1.1, 92.33333333333334, 48.5, 0.0, 26.0, 25.70665467015125, 0.5107668824383351, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.49307479224376743, 0.9233333333333335, 0.16166666666666665, 0.0, 0.6666666666666666, 0.642221222512604, 0.670255627479445, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5973599], dtype=float32), -0.45562345]. 
=============================================
[2019-04-04 00:53:47,543] A3C_AGENT_WORKER-Thread-17 INFO:Local step 130000, global step 2073698: loss 0.1472
[2019-04-04 00:53:47,547] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 130000, global step 2073698: learning rate 0.0001
[2019-04-04 00:53:47,979] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.5596463e-22 3.5245134e-12 8.9117247e-20 3.4589279e-16 2.1721279e-15
 7.5062238e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 00:53:47,980] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9643
[2019-04-04 00:53:48,066] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [17.7, 67.0, 0.0, 0.0, 26.0, 24.55932522064283, 0.3747685535063565, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1197600.0000, 
sim time next is 1198200.0000, 
raw observation next is [17.7, 67.0, 0.0, 0.0, 26.0, 24.56310063450866, 0.3752251825763632, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.9529085872576178, 0.67, 0.0, 0.0, 0.6666666666666666, 0.5469250528757218, 0.6250750608587877, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.02661489], dtype=float32), 0.22504552]. 
=============================================
[2019-04-04 00:53:50,459] A3C_AGENT_WORKER-Thread-7 INFO:Local step 130000, global step 2074608: loss 0.1492
[2019-04-04 00:53:50,460] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 130000, global step 2074608: learning rate 0.0001
[2019-04-04 00:53:50,858] A3C_AGENT_WORKER-Thread-15 INFO:Local step 129500, global step 2074743: loss 0.0417
[2019-04-04 00:53:50,858] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 129500, global step 2074743: learning rate 0.0001
[2019-04-04 00:53:53,558] A3C_AGENT_WORKER-Thread-16 INFO:Local step 129500, global step 2075523: loss 0.0423
[2019-04-04 00:53:53,560] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 129500, global step 2075523: learning rate 0.0001
[2019-04-04 00:53:53,922] A3C_AGENT_WORKER-Thread-9 INFO:Local step 129500, global step 2075650: loss 0.0317
[2019-04-04 00:53:53,935] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 129500, global step 2075650: learning rate 0.0001
[2019-04-04 00:53:57,220] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.5355477e-21 1.1018580e-12 1.4407882e-19 1.4265282e-15 2.9834108e-15
 4.2312683e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 00:53:57,221] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8236
[2019-04-04 00:53:57,249] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.9, 98.66666666666667, 0.0, 0.0, 26.0, 24.5841763306522, 0.42735952483161, 0.0, 1.0, 45616.4650487844], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1272000.0000, 
sim time next is 1272600.0000, 
raw observation next is [10.25, 98.0, 0.0, 0.0, 26.0, 24.59008743476455, 0.4330187011210192, 0.0, 1.0, 45080.17091632891], 
processed observation next is [0.0, 0.7391304347826086, 0.7465373961218837, 0.98, 0.0, 0.0, 0.6666666666666666, 0.5491739528970457, 0.6443395670403397, 0.0, 1.0, 0.21466748055394722], 
reward next is 0.7853, 
noisyNet noise sample is [array([-0.82756674], dtype=float32), -0.3771691]. 
=============================================
[2019-04-04 00:53:57,502] A3C_AGENT_WORKER-Thread-20 INFO:Local step 129500, global step 2076497: loss 0.0319
[2019-04-04 00:53:57,509] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 129500, global step 2076497: learning rate 0.0001
[2019-04-04 00:53:57,738] A3C_AGENT_WORKER-Thread-10 INFO:Local step 129500, global step 2076561: loss 0.0337
[2019-04-04 00:53:57,740] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 129500, global step 2076561: learning rate 0.0001
[2019-04-04 00:54:05,975] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4340376e-20 3.0988531e-13 2.3197505e-18 5.3892505e-15 7.2834957e-15
 4.3738535e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 00:54:05,975] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0306
[2019-04-04 00:54:06,062] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.516666666666667, 89.5, 0.0, 0.0, 26.0, 25.54640666472951, 0.5594023834784344, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1458600.0000, 
sim time next is 1459200.0000, 
raw observation next is [1.433333333333334, 90.0, 0.0, 0.0, 26.0, 25.57926635995268, 0.5604650887250195, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.502308402585411, 0.9, 0.0, 0.0, 0.6666666666666666, 0.6316055299960567, 0.6868216962416732, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.26560345], dtype=float32), 0.048295077]. 
=============================================
[2019-04-04 00:54:06,345] A3C_AGENT_WORKER-Thread-14 INFO:Local step 130000, global step 2078649: loss 0.1958
[2019-04-04 00:54:06,373] A3C_AGENT_WORKER-Thread-2 INFO:Local step 130000, global step 2078653: loss 0.1983
[2019-04-04 00:54:06,390] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 130000, global step 2078653: learning rate 0.0001
[2019-04-04 00:54:06,390] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 130000, global step 2078653: learning rate 0.0001
[2019-04-04 00:54:08,915] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.5425215e-20 3.7156569e-11 6.0453196e-18 8.2668315e-15 4.1655566e-14
 6.8974365e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 00:54:08,955] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8505
[2019-04-04 00:54:08,987] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.4, 86.0, 0.0, 0.0, 26.0, 25.60156706412787, 0.5257040620735105, 0.0, 1.0, 18736.71446041186], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1566000.0000, 
sim time next is 1566600.0000, 
raw observation next is [4.433333333333334, 85.83333333333334, 0.0, 0.0, 26.0, 25.58568150761173, 0.5271026122787691, 0.0, 1.0, 24102.24390413803], 
processed observation next is [1.0, 0.13043478260869565, 0.5854108956602032, 0.8583333333333334, 0.0, 0.0, 0.6666666666666666, 0.632140125634311, 0.6757008707595897, 0.0, 1.0, 0.11477259001970491], 
reward next is 0.8852, 
noisyNet noise sample is [array([0.5597412], dtype=float32), 0.731538]. 
=============================================
[2019-04-04 00:54:09,486] A3C_AGENT_WORKER-Thread-5 INFO:Local step 130000, global step 2079457: loss 0.1612
[2019-04-04 00:54:09,486] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 130000, global step 2079457: learning rate 0.0001
[2019-04-04 00:54:10,053] A3C_AGENT_WORKER-Thread-6 INFO:Local step 130000, global step 2079594: loss 0.1664
[2019-04-04 00:54:10,054] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 130000, global step 2079594: learning rate 0.0001
[2019-04-04 00:54:10,896] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [9.8397956e-21 1.7750038e-12 5.2293539e-18 1.6372776e-14 6.0755478e-14
 1.2046322e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:54:10,896] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6244
[2019-04-04 00:54:10,934] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.24129424529551, 0.4589079373845548, 0.0, 1.0, 45630.27990173167], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1396800.0000, 
sim time next is 1397400.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.27507672706881, 0.4608899176000655, 0.0, 1.0, 41068.09431772098], 
processed observation next is [1.0, 0.17391304347826086, 0.44598337950138506, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6062563939224009, 0.6536299725333552, 0.0, 1.0, 0.1955623538939094], 
reward next is 0.8044, 
noisyNet noise sample is [array([-2.2451355], dtype=float32), 0.45832217]. 
=============================================
[2019-04-04 00:54:12,190] A3C_AGENT_WORKER-Thread-19 INFO:Local step 130500, global step 2080125: loss 0.0041
[2019-04-04 00:54:12,192] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 130500, global step 2080125: learning rate 0.0001
[2019-04-04 00:54:13,956] A3C_AGENT_WORKER-Thread-3 INFO:Local step 130000, global step 2080570: loss 0.1814
[2019-04-04 00:54:13,982] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 130000, global step 2080570: learning rate 0.0001
[2019-04-04 00:54:16,650] A3C_AGENT_WORKER-Thread-8 INFO:Local step 130000, global step 2081162: loss 0.1358
[2019-04-04 00:54:16,652] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 130000, global step 2081162: learning rate 0.0001
[2019-04-04 00:54:17,533] A3C_AGENT_WORKER-Thread-18 INFO:Local step 130000, global step 2081382: loss 0.1336
[2019-04-04 00:54:17,534] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 130000, global step 2081382: learning rate 0.0001
[2019-04-04 00:54:18,735] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.7136095e-22 7.5742770e-14 8.7093368e-20 5.6502752e-16 5.0565723e-16
 1.4318777e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 00:54:18,735] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7586
[2019-04-04 00:54:18,812] A3C_AGENT_WORKER-Thread-4 INFO:Local step 130000, global step 2081743: loss 0.1902
[2019-04-04 00:54:18,824] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 78.0, 0.0, 26.0, 25.70369721346077, 0.5094847271639827, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1433400.0000, 
sim time next is 1434000.0000, 
raw observation next is [1.1, 92.0, 75.0, 0.0, 26.0, 25.03959585405082, 0.461845957885219, 1.0, 1.0, 19055.41202426924], 
processed observation next is [1.0, 0.6086956521739131, 0.49307479224376743, 0.92, 0.25, 0.0, 0.6666666666666666, 0.5866329878375683, 0.6539486526284063, 1.0, 1.0, 0.09074005725842496], 
reward next is 0.9093, 
noisyNet noise sample is [array([-1.0444863], dtype=float32), -1.0413855]. 
=============================================
[2019-04-04 00:54:18,836] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 130000, global step 2081743: learning rate 0.0001
[2019-04-04 00:54:18,847] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[85.40112 ]
 [85.60122 ]
 [85.74067 ]
 [85.79597 ]
 [85.825905]], R is [[85.2048645 ]
 [85.35281372]
 [85.49928284]
 [85.64428711]
 [85.7878418 ]].
[2019-04-04 00:54:19,277] A3C_AGENT_WORKER-Thread-17 INFO:Local step 130500, global step 2081859: loss 0.0001
[2019-04-04 00:54:19,278] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 130500, global step 2081859: learning rate 0.0001
[2019-04-04 00:54:22,046] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.6820891e-19 2.3632978e-11 1.6411583e-17 7.8756717e-14 5.7219678e-14
 5.9689177e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:54:22,046] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6456
[2019-04-04 00:54:22,075] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.733333333333334, 80.66666666666667, 0.0, 0.0, 26.0, 23.64073059260665, -0.0847732708615251, 0.0, 1.0, 45300.62290424366], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1917600.0000, 
sim time next is 1918200.0000, 
raw observation next is [-8.816666666666666, 81.33333333333334, 0.0, 0.0, 26.0, 23.6238452081551, -0.08296313703098428, 0.0, 1.0, 45285.87087959125], 
processed observation next is [1.0, 0.17391304347826086, 0.21837488457987075, 0.8133333333333335, 0.0, 0.0, 0.6666666666666666, 0.4686537673462583, 0.4723456209896719, 0.0, 1.0, 0.21564700418852978], 
reward next is 0.7844, 
noisyNet noise sample is [array([0.02344576], dtype=float32), -1.6219572]. 
=============================================
[2019-04-04 00:54:22,456] A3C_AGENT_WORKER-Thread-15 INFO:Local step 130000, global step 2082756: loss 0.1417
[2019-04-04 00:54:22,456] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 130000, global step 2082756: learning rate 0.0001
[2019-04-04 00:54:23,297] A3C_AGENT_WORKER-Thread-7 INFO:Local step 130500, global step 2082965: loss 0.0014
[2019-04-04 00:54:23,305] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 130500, global step 2082965: learning rate 0.0001
[2019-04-04 00:54:25,513] A3C_AGENT_WORKER-Thread-16 INFO:Local step 130000, global step 2083466: loss 0.1086
[2019-04-04 00:54:25,527] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 130000, global step 2083466: learning rate 0.0001
[2019-04-04 00:54:26,343] A3C_AGENT_WORKER-Thread-9 INFO:Local step 130000, global step 2083678: loss 0.1587
[2019-04-04 00:54:26,363] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 130000, global step 2083678: learning rate 0.0001
[2019-04-04 00:54:28,251] A3C_AGENT_WORKER-Thread-20 INFO:Local step 130000, global step 2084172: loss 0.1331
[2019-04-04 00:54:28,330] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 130000, global step 2084172: learning rate 0.0001
[2019-04-04 00:54:29,788] A3C_AGENT_WORKER-Thread-10 INFO:Local step 130000, global step 2084565: loss 0.1392
[2019-04-04 00:54:29,837] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 130000, global step 2084565: learning rate 0.0001
[2019-04-04 00:54:38,892] A3C_AGENT_WORKER-Thread-2 INFO:Local step 130500, global step 2086661: loss 0.0169
[2019-04-04 00:54:38,892] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 130500, global step 2086661: learning rate 0.0001
[2019-04-04 00:54:38,936] A3C_AGENT_WORKER-Thread-14 INFO:Local step 130500, global step 2086672: loss 0.0154
[2019-04-04 00:54:38,957] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 130500, global step 2086672: learning rate 0.0001
[2019-04-04 00:54:42,023] A3C_AGENT_WORKER-Thread-5 INFO:Local step 130500, global step 2087335: loss 0.0238
[2019-04-04 00:54:42,023] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 130500, global step 2087335: learning rate 0.0001
[2019-04-04 00:54:43,437] A3C_AGENT_WORKER-Thread-6 INFO:Local step 130500, global step 2087611: loss 0.0142
[2019-04-04 00:54:43,438] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 130500, global step 2087611: learning rate 0.0001
[2019-04-04 00:54:45,222] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.0798596e-18 5.9099212e-11 7.2212140e-17 2.5658514e-13 5.1229528e-13
 1.0267599e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:54:45,238] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5012
[2019-04-04 00:54:45,346] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.199999999999999, 72.33333333333333, 173.3333333333333, 67.5, 26.0, 24.9533912057362, 0.2616909529673233, 0.0, 1.0, 46737.89656451304], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1856400.0000, 
sim time next is 1857000.0000, 
raw observation next is [-5.1, 71.66666666666667, 162.6666666666667, 54.00000000000001, 26.0, 24.9633582471834, 0.2639561247342531, 0.0, 1.0, 40201.99411507563], 
processed observation next is [0.0, 0.4782608695652174, 0.3213296398891967, 0.7166666666666667, 0.5422222222222224, 0.059668508287292824, 0.6666666666666666, 0.58027985393195, 0.5879853749114177, 0.0, 1.0, 0.19143806721464585], 
reward next is 0.8086, 
noisyNet noise sample is [array([0.37808943], dtype=float32), 0.9453559]. 
=============================================
[2019-04-04 00:54:45,363] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[76.96555 ]
 [77.5671  ]
 [77.72455 ]
 [77.924065]
 [78.043175]], R is [[76.30615234]
 [76.32052612]
 [76.30482483]
 [76.29740906]
 [76.34152222]].
[2019-04-04 00:54:47,456] A3C_AGENT_WORKER-Thread-3 INFO:Local step 130500, global step 2088399: loss 0.0196
[2019-04-04 00:54:47,505] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 130500, global step 2088399: learning rate 0.0001
[2019-04-04 00:54:50,197] A3C_AGENT_WORKER-Thread-8 INFO:Local step 130500, global step 2088972: loss 0.0153
[2019-04-04 00:54:50,197] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 130500, global step 2088972: learning rate 0.0001
[2019-04-04 00:54:50,242] A3C_AGENT_WORKER-Thread-19 INFO:Local step 131000, global step 2088983: loss 0.4278
[2019-04-04 00:54:50,243] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 131000, global step 2088983: learning rate 0.0001
[2019-04-04 00:54:50,698] A3C_AGENT_WORKER-Thread-18 INFO:Local step 130500, global step 2089098: loss 0.0225
[2019-04-04 00:54:50,700] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 130500, global step 2089098: learning rate 0.0001
[2019-04-04 00:54:52,193] A3C_AGENT_WORKER-Thread-4 INFO:Local step 130500, global step 2089381: loss 0.0263
[2019-04-04 00:54:52,196] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 130500, global step 2089381: learning rate 0.0001
[2019-04-04 00:54:57,234] A3C_AGENT_WORKER-Thread-15 INFO:Local step 130500, global step 2090381: loss 0.0102
[2019-04-04 00:54:57,245] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 130500, global step 2090381: learning rate 0.0001
[2019-04-04 00:54:57,555] A3C_AGENT_WORKER-Thread-17 INFO:Local step 131000, global step 2090448: loss 0.3248
[2019-04-04 00:54:57,556] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 131000, global step 2090448: learning rate 0.0001
[2019-04-04 00:54:59,907] A3C_AGENT_WORKER-Thread-9 INFO:Local step 130500, global step 2090945: loss 0.0229
[2019-04-04 00:54:59,908] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 130500, global step 2090945: learning rate 0.0001
[2019-04-04 00:55:00,433] A3C_AGENT_WORKER-Thread-16 INFO:Local step 130500, global step 2091064: loss 0.0221
[2019-04-04 00:55:00,434] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 130500, global step 2091064: learning rate 0.0001
[2019-04-04 00:55:02,893] A3C_AGENT_WORKER-Thread-20 INFO:Local step 130500, global step 2091562: loss 0.0087
[2019-04-04 00:55:02,911] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 130500, global step 2091562: learning rate 0.0001
[2019-04-04 00:55:03,450] A3C_AGENT_WORKER-Thread-10 INFO:Local step 130500, global step 2091674: loss 0.0102
[2019-04-04 00:55:03,457] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 130500, global step 2091674: learning rate 0.0001
[2019-04-04 00:55:04,047] A3C_AGENT_WORKER-Thread-7 INFO:Local step 131000, global step 2091774: loss 0.2141
[2019-04-04 00:55:04,048] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 131000, global step 2091774: learning rate 0.0001
[2019-04-04 00:55:19,521] A3C_AGENT_WORKER-Thread-2 INFO:Local step 131000, global step 2094767: loss 0.2234
[2019-04-04 00:55:19,525] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 131000, global step 2094767: learning rate 0.0001
[2019-04-04 00:55:19,753] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.1972678e-18 2.6523383e-10 1.1186440e-16 1.6327223e-13 7.2716632e-13
 1.9501210e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:55:19,796] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9913
[2019-04-04 00:55:19,928] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 69.0, 25.33333333333334, 0.0, 26.0, 24.04225058040408, 0.06965190163121854, 0.0, 1.0, 41994.14846296566], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2362200.0000, 
sim time next is 2362800.0000, 
raw observation next is [-3.4, 69.0, 31.16666666666667, 0.0, 26.0, 24.06963236808048, 0.1323117145651304, 0.0, 1.0, 202421.3760289082], 
processed observation next is [0.0, 0.34782608695652173, 0.368421052631579, 0.69, 0.1038888888888889, 0.0, 0.6666666666666666, 0.50580269734004, 0.5441039048550435, 0.0, 1.0, 0.9639113144233724], 
reward next is 0.0361, 
noisyNet noise sample is [array([0.10063169], dtype=float32), 1.163817]. 
=============================================
[2019-04-04 00:55:20,054] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.1122735e-20 1.1570567e-12 3.0444775e-18 8.3601949e-15 1.3690146e-14
 3.3066752e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 00:55:20,054] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1165
[2019-04-04 00:55:20,150] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.55879681494132, 0.2055965407761513, 0.0, 1.0, 42907.51386197519], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1985400.0000, 
sim time next is 1986000.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.52134156251967, 0.1978180376921181, 0.0, 1.0, 42839.33155015157], 
processed observation next is [1.0, 1.0, 0.30747922437673136, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5434451302099724, 0.5659393458973727, 0.0, 1.0, 0.20399681690548366], 
reward next is 0.7960, 
noisyNet noise sample is [array([0.49297], dtype=float32), -0.4853853]. 
=============================================
[2019-04-04 00:55:20,193] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[80.2742  ]
 [80.297905]
 [80.27409 ]
 [80.1911  ]
 [79.98751 ]], R is [[80.30597687]
 [80.29859161]
 [80.29104614]
 [80.28343201]
 [80.27583313]].
[2019-04-04 00:55:20,297] A3C_AGENT_WORKER-Thread-14 INFO:Local step 131000, global step 2094961: loss 0.1741
[2019-04-04 00:55:20,301] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 131000, global step 2094961: learning rate 0.0001
[2019-04-04 00:55:22,264] A3C_AGENT_WORKER-Thread-5 INFO:Local step 131000, global step 2095367: loss 0.1822
[2019-04-04 00:55:22,318] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 131000, global step 2095367: learning rate 0.0001
[2019-04-04 00:55:22,911] A3C_AGENT_WORKER-Thread-6 INFO:Local step 131000, global step 2095511: loss 0.2016
[2019-04-04 00:55:22,911] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 131000, global step 2095511: learning rate 0.0001
[2019-04-04 00:55:23,484] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.48001727e-20 4.87416010e-12 1.07543604e-17 2.54882209e-14
 6.21543900e-14 2.52712880e-19 1.00000000e+00], sum to 1.0000
[2019-04-04 00:55:23,484] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9560
[2019-04-04 00:55:23,527] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.299999999999999, 79.5, 0.0, 0.0, 26.0, 24.46262983067114, 0.1903912408624214, 0.0, 1.0, 42453.99146300081], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2163000.0000, 
sim time next is 2163600.0000, 
raw observation next is [-7.3, 79.0, 0.0, 0.0, 26.0, 24.46131348602582, 0.1797361216175922, 0.0, 1.0, 42484.89424388644], 
processed observation next is [1.0, 0.043478260869565216, 0.26038781163434904, 0.79, 0.0, 0.0, 0.6666666666666666, 0.5384427905021516, 0.5599120405391974, 0.0, 1.0, 0.20230902020898306], 
reward next is 0.7977, 
noisyNet noise sample is [array([1.4089541], dtype=float32), -1.3538165]. 
=============================================
[2019-04-04 00:55:28,200] A3C_AGENT_WORKER-Thread-3 INFO:Local step 131000, global step 2096594: loss 0.1718
[2019-04-04 00:55:28,201] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 131000, global step 2096594: learning rate 0.0001
[2019-04-04 00:55:28,806] A3C_AGENT_WORKER-Thread-19 INFO:Local step 131500, global step 2096704: loss 0.0040
[2019-04-04 00:55:28,807] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 131500, global step 2096704: learning rate 0.0001
[2019-04-04 00:55:30,704] A3C_AGENT_WORKER-Thread-18 INFO:Local step 131000, global step 2097063: loss 0.1678
[2019-04-04 00:55:30,705] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 131000, global step 2097063: learning rate 0.0001
[2019-04-04 00:55:32,619] A3C_AGENT_WORKER-Thread-8 INFO:Local step 131000, global step 2097454: loss 0.1926
[2019-04-04 00:55:32,620] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 131000, global step 2097454: learning rate 0.0001
[2019-04-04 00:55:32,822] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0097426e-18 1.5951058e-11 4.0586378e-17 2.1243638e-13 1.8112930e-13
 2.9162863e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:55:32,822] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7385
[2019-04-04 00:55:32,870] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.933333333333334, 43.66666666666667, 0.0, 0.0, 26.0, 24.96709907776706, 0.2645328193126386, 0.0, 1.0, 33224.26308223402], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2398800.0000, 
sim time next is 2399400.0000, 
raw observation next is [-2.05, 43.5, 0.0, 0.0, 26.0, 24.96943627435964, 0.2607026122171117, 0.0, 1.0, 36860.13989581777], 
processed observation next is [0.0, 0.782608695652174, 0.40581717451523547, 0.435, 0.0, 0.0, 0.6666666666666666, 0.5807863561966368, 0.5869008707390372, 0.0, 1.0, 0.17552447569437032], 
reward next is 0.8245, 
noisyNet noise sample is [array([-0.10832332], dtype=float32), -0.16828309]. 
=============================================
[2019-04-04 00:55:32,957] A3C_AGENT_WORKER-Thread-4 INFO:Local step 131000, global step 2097530: loss 0.2084
[2019-04-04 00:55:32,958] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 131000, global step 2097530: learning rate 0.0001
[2019-04-04 00:55:33,628] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.3640630e-19 4.0700769e-11 3.8232526e-17 1.1743381e-13 4.9973263e-13
 2.8091478e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:55:33,629] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6848
[2019-04-04 00:55:33,663] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 42.0, 0.0, 0.0, 26.0, 25.15746206337576, 0.286818246293717, 0.0, 1.0, 43326.67371916811], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2406000.0000, 
sim time next is 2406600.0000, 
raw observation next is [-3.4, 42.0, 0.0, 0.0, 26.0, 25.15863164123137, 0.2810747596082467, 0.0, 1.0, 43113.74704714974], 
processed observation next is [0.0, 0.8695652173913043, 0.368421052631579, 0.42, 0.0, 0.0, 0.6666666666666666, 0.5965526367692808, 0.5936915865360822, 0.0, 1.0, 0.20530355736737974], 
reward next is 0.7947, 
noisyNet noise sample is [array([0.18235351], dtype=float32), -1.3605062]. 
=============================================
[2019-04-04 00:55:36,676] A3C_AGENT_WORKER-Thread-17 INFO:Local step 131500, global step 2098360: loss 0.0091
[2019-04-04 00:55:36,678] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 131500, global step 2098360: learning rate 0.0001
[2019-04-04 00:55:38,021] A3C_AGENT_WORKER-Thread-15 INFO:Local step 131000, global step 2098635: loss 0.1912
[2019-04-04 00:55:38,022] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 131000, global step 2098635: learning rate 0.0001
[2019-04-04 00:55:40,360] A3C_AGENT_WORKER-Thread-9 INFO:Local step 131000, global step 2099051: loss 0.1591
[2019-04-04 00:55:40,361] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 131000, global step 2099051: learning rate 0.0001
[2019-04-04 00:55:41,545] A3C_AGENT_WORKER-Thread-16 INFO:Local step 131000, global step 2099267: loss 0.1399
[2019-04-04 00:55:41,570] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 131000, global step 2099267: learning rate 0.0001
[2019-04-04 00:55:41,746] A3C_AGENT_WORKER-Thread-7 INFO:Local step 131500, global step 2099320: loss 0.0100
[2019-04-04 00:55:41,748] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 131500, global step 2099320: learning rate 0.0001
[2019-04-04 00:55:44,884] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-04 00:55:44,906] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 00:55:44,906] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:55:44,907] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 00:55:44,907] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:55:44,908] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 00:55:44,908] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 00:55:44,910] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run22
[2019-04-04 00:55:44,958] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run22
[2019-04-04 00:55:45,011] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run22
[2019-04-04 00:56:55,415] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.34453174], dtype=float32), 0.21217766]
[2019-04-04 00:56:55,415] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [2.716666666666667, 80.5, 0.0, 0.0, 26.0, 25.05679582611616, 0.2828947980578806, 0.0, 1.0, 0.0]
[2019-04-04 00:56:55,416] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 00:56:55,416] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.1006526e-19 7.2902621e-12 8.6935127e-18 2.5594812e-14 8.6171941e-14
 4.2299333e-19 1.0000000e+00], sampled 0.9487160367947148
[2019-04-04 00:58:24,914] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 00:58:45,620] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 00:58:48,701] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 00:58:49,724] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 2100000, evaluation results [2100000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 00:58:49,822] A3C_AGENT_WORKER-Thread-20 INFO:Local step 131000, global step 2100042: loss 0.1311
[2019-04-04 00:58:49,823] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 131000, global step 2100042: learning rate 0.0001
[2019-04-04 00:58:50,076] A3C_AGENT_WORKER-Thread-10 INFO:Local step 131000, global step 2100131: loss 0.1247
[2019-04-04 00:58:50,077] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 131000, global step 2100131: learning rate 0.0001
[2019-04-04 00:58:57,259] A3C_AGENT_WORKER-Thread-2 INFO:Local step 131500, global step 2102437: loss 0.0143
[2019-04-04 00:58:57,260] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 131500, global step 2102437: learning rate 0.0001
[2019-04-04 00:58:58,934] A3C_AGENT_WORKER-Thread-14 INFO:Local step 131500, global step 2103018: loss 0.0240
[2019-04-04 00:58:58,935] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 131500, global step 2103018: learning rate 0.0001
[2019-04-04 00:58:59,129] A3C_AGENT_WORKER-Thread-5 INFO:Local step 131500, global step 2103081: loss 0.0179
[2019-04-04 00:58:59,133] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 131500, global step 2103081: learning rate 0.0001
[2019-04-04 00:59:00,627] A3C_AGENT_WORKER-Thread-6 INFO:Local step 131500, global step 2103583: loss 0.0116
[2019-04-04 00:59:00,629] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 131500, global step 2103583: learning rate 0.0001
[2019-04-04 00:59:02,474] A3C_AGENT_WORKER-Thread-19 INFO:Local step 132000, global step 2104089: loss 0.0173
[2019-04-04 00:59:02,474] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 132000, global step 2104089: learning rate 0.0001
[2019-04-04 00:59:03,529] A3C_AGENT_WORKER-Thread-3 INFO:Local step 131500, global step 2104461: loss 0.0088
[2019-04-04 00:59:03,530] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 131500, global step 2104461: learning rate 0.0001
[2019-04-04 00:59:04,848] A3C_AGENT_WORKER-Thread-18 INFO:Local step 131500, global step 2104966: loss 0.0047
[2019-04-04 00:59:04,849] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 131500, global step 2104966: learning rate 0.0001
[2019-04-04 00:59:04,912] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.7890998e-19 6.6652369e-12 6.9161497e-18 6.9135920e-14 1.1603446e-13
 2.1862315e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:59:04,913] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6530
[2019-04-04 00:59:04,975] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.633333333333333, 64.0, 136.0, 435.0, 26.0, 24.97732918325845, 0.2825121438706844, 0.0, 1.0, 45070.69594287476], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2370000.0000, 
sim time next is 2370600.0000, 
raw observation next is [-2.55, 63.5, 139.0, 450.0, 26.0, 24.91671519974378, 0.2886576858217527, 0.0, 1.0, 73411.03920198038], 
processed observation next is [0.0, 0.43478260869565216, 0.3919667590027701, 0.635, 0.4633333333333333, 0.4972375690607735, 0.6666666666666666, 0.5763929333119817, 0.5962192286072509, 0.0, 1.0, 0.34957637715228757], 
reward next is 0.6504, 
noisyNet noise sample is [array([-1.4307523], dtype=float32), -0.63857615]. 
=============================================
[2019-04-04 00:59:06,118] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.3644931e-18 1.3778664e-10 1.1778065e-16 9.1197645e-14 1.9568018e-13
 7.8099556e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:59:06,119] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6980
[2019-04-04 00:59:06,144] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 69.0, 0.0, 0.0, 26.0, 24.07386371971666, 0.07363287888751012, 0.0, 1.0, 41440.29263440978], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2358600.0000, 
sim time next is 2359200.0000, 
raw observation next is [-3.4, 69.0, 0.0, 0.0, 26.0, 24.04178089789336, 0.06650827454482156, 0.0, 1.0, 41520.05577114707], 
processed observation next is [0.0, 0.30434782608695654, 0.368421052631579, 0.69, 0.0, 0.0, 0.6666666666666666, 0.5034817414911134, 0.5221694248482739, 0.0, 1.0, 0.19771455129117652], 
reward next is 0.8023, 
noisyNet noise sample is [array([0.70213735], dtype=float32), 0.89424294]. 
=============================================
[2019-04-04 00:59:06,318] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.2483106e-18 2.6574679e-10 3.5632574e-16 8.7156492e-13 2.1511248e-12
 3.2574895e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 00:59:06,319] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3097
[2019-04-04 00:59:06,352] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.58333333333333, 81.83333333333334, 0.0, 0.0, 26.0, 23.96966267230423, 0.08782126841914435, 0.0, 1.0, 44478.09683173716], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2688600.0000, 
sim time next is 2689200.0000, 
raw observation next is [-12.9, 83.0, 0.0, 0.0, 26.0, 23.91404964319495, 0.07046155879200171, 0.0, 1.0, 44496.80085001457], 
processed observation next is [1.0, 0.13043478260869565, 0.10526315789473682, 0.83, 0.0, 0.0, 0.6666666666666666, 0.49283747026624586, 0.5234871862640006, 0.0, 1.0, 0.21188952785721227], 
reward next is 0.7881, 
noisyNet noise sample is [array([-1.4253647], dtype=float32), -0.1187107]. 
=============================================
[2019-04-04 00:59:06,475] A3C_AGENT_WORKER-Thread-8 INFO:Local step 131500, global step 2105589: loss 0.0049
[2019-04-04 00:59:06,476] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 131500, global step 2105589: learning rate 0.0001
[2019-04-04 00:59:06,533] A3C_AGENT_WORKER-Thread-4 INFO:Local step 131500, global step 2105612: loss 0.0045
[2019-04-04 00:59:06,538] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 131500, global step 2105612: learning rate 0.0001
[2019-04-04 00:59:07,607] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.6000714e-20 7.6732175e-13 6.7815224e-19 2.3958573e-15 9.2758400e-15
 7.4219443e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 00:59:07,607] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8000
[2019-04-04 00:59:07,635] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 48.33333333333334, 133.5, 43.0, 26.0, 25.81121380911717, 0.3013550059277703, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2542800.0000, 
sim time next is 2543400.0000, 
raw observation next is [-0.8999999999999999, 48.0, 133.0, 45.0, 26.0, 25.79521464578109, 0.3001929568921549, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.43767313019390586, 0.48, 0.44333333333333336, 0.049723756906077346, 0.6666666666666666, 0.6496012204817575, 0.6000643189640517, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.13885535], dtype=float32), 0.27021372]. 
=============================================
[2019-04-04 00:59:07,695] A3C_AGENT_WORKER-Thread-17 INFO:Local step 132000, global step 2106014: loss 0.0229
[2019-04-04 00:59:07,695] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 132000, global step 2106014: learning rate 0.0001
[2019-04-04 00:59:10,062] A3C_AGENT_WORKER-Thread-15 INFO:Local step 131500, global step 2106822: loss 0.0053
[2019-04-04 00:59:10,063] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 131500, global step 2106822: learning rate 0.0001
[2019-04-04 00:59:10,350] A3C_AGENT_WORKER-Thread-7 INFO:Local step 132000, global step 2106926: loss 0.0266
[2019-04-04 00:59:10,350] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 132000, global step 2106926: learning rate 0.0001
[2019-04-04 00:59:11,481] A3C_AGENT_WORKER-Thread-9 INFO:Local step 131500, global step 2107335: loss 0.0092
[2019-04-04 00:59:11,482] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 131500, global step 2107335: learning rate 0.0001
[2019-04-04 00:59:12,880] A3C_AGENT_WORKER-Thread-16 INFO:Local step 131500, global step 2107825: loss 0.0043
[2019-04-04 00:59:12,884] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 131500, global step 2107828: learning rate 0.0001
[2019-04-04 00:59:14,170] A3C_AGENT_WORKER-Thread-20 INFO:Local step 131500, global step 2108326: loss 0.0083
[2019-04-04 00:59:14,173] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 131500, global step 2108327: learning rate 0.0001
[2019-04-04 00:59:14,345] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1441184e-18 3.0304564e-11 3.6409298e-17 7.7810967e-14 8.6986765e-13
 1.6849929e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:59:14,348] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0491
[2019-04-04 00:59:14,404] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.7333333333333335, 28.66666666666667, 0.0, 0.0, 26.0, 24.92754968502823, 0.2144898493446322, 0.0, 1.0, 28515.69409870524], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2485200.0000, 
sim time next is 2485800.0000, 
raw observation next is [0.55, 29.0, 0.0, 0.0, 26.0, 24.91554849102632, 0.2113485216110403, 0.0, 1.0, 41154.06603762173], 
processed observation next is [0.0, 0.782608695652174, 0.4778393351800555, 0.29, 0.0, 0.0, 0.6666666666666666, 0.5762957075855267, 0.57044950720368, 0.0, 1.0, 0.19597174303629394], 
reward next is 0.8040, 
noisyNet noise sample is [array([0.2737356], dtype=float32), 1.7779636]. 
=============================================
[2019-04-04 00:59:14,621] A3C_AGENT_WORKER-Thread-10 INFO:Local step 131500, global step 2108475: loss 0.0076
[2019-04-04 00:59:14,622] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 131500, global step 2108475: learning rate 0.0001
[2019-04-04 00:59:15,058] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [4.0448118e-21 5.8833465e-14 1.9458018e-19 1.6815408e-15 6.3902050e-16
 3.7698769e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 00:59:15,058] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.1446
[2019-04-04 00:59:15,069] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.733333333333333, 28.66666666666667, 178.8333333333333, 405.3333333333334, 26.0, 25.30308667457226, 0.314852574383751, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2553600.0000, 
sim time next is 2554200.0000, 
raw observation next is [3.0, 28.0, 171.0, 482.0, 26.0, 25.41834293453547, 0.3428142724455684, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5457063711911359, 0.28, 0.57, 0.532596685082873, 0.6666666666666666, 0.6181952445446225, 0.6142714241485229, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.39191684], dtype=float32), 2.1668646]. 
=============================================
[2019-04-04 00:59:19,707] A3C_AGENT_WORKER-Thread-2 INFO:Local step 132000, global step 2110419: loss 0.0197
[2019-04-04 00:59:19,708] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 132000, global step 2110419: learning rate 0.0001
[2019-04-04 00:59:21,382] A3C_AGENT_WORKER-Thread-14 INFO:Local step 132000, global step 2110993: loss 0.0158
[2019-04-04 00:59:21,397] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 132000, global step 2110993: learning rate 0.0001
[2019-04-04 00:59:21,436] A3C_AGENT_WORKER-Thread-5 INFO:Local step 132000, global step 2111012: loss 0.0153
[2019-04-04 00:59:21,444] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 132000, global step 2111012: learning rate 0.0001
[2019-04-04 00:59:21,614] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1869693e-20 8.2830650e-13 9.5737729e-19 2.9642617e-15 5.4795506e-15
 2.3576455e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 00:59:21,614] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0577
[2019-04-04 00:59:21,653] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.266666666666667, 53.66666666666667, 121.8333333333333, 30.5, 26.0, 25.65561870697538, 0.2824691061694298, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2539200.0000, 
sim time next is 2539800.0000, 
raw observation next is [-2.0, 52.5, 136.0, 33.0, 26.0, 25.70124486553331, 0.2930395433123135, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.40720221606648205, 0.525, 0.4533333333333333, 0.036464088397790057, 0.6666666666666666, 0.6417704054611093, 0.5976798477707712, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.64033395], dtype=float32), 0.020298997]. 
=============================================
[2019-04-04 00:59:23,139] A3C_AGENT_WORKER-Thread-6 INFO:Local step 132000, global step 2111646: loss 0.0122
[2019-04-04 00:59:23,140] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 132000, global step 2111646: learning rate 0.0001
[2019-04-04 00:59:24,401] A3C_AGENT_WORKER-Thread-19 INFO:Local step 132500, global step 2112108: loss 0.0167
[2019-04-04 00:59:24,404] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 132500, global step 2112108: learning rate 0.0001
[2019-04-04 00:59:25,279] A3C_AGENT_WORKER-Thread-3 INFO:Local step 132000, global step 2112470: loss 0.0114
[2019-04-04 00:59:25,279] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 132000, global step 2112470: learning rate 0.0001
[2019-04-04 00:59:26,225] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.40245784e-21 7.61553364e-14 1.05173346e-19 2.32902000e-15
 3.97190478e-15 1.81123029e-21 1.00000000e+00], sum to 1.0000
[2019-04-04 00:59:26,226] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9651
[2019-04-04 00:59:26,292] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.8, 58.5, 110.3333333333333, 791.6666666666666, 26.0, 25.80143069086825, 0.5242403938711852, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2725800.0000, 
sim time next is 2726400.0000, 
raw observation next is [-5.6, 58.0, 109.6666666666667, 789.8333333333334, 26.0, 25.99450057383382, 0.5529674977247893, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.30747922437673136, 0.58, 0.3655555555555557, 0.872744014732965, 0.6666666666666666, 0.6662083811528184, 0.6843224992415964, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.91449463], dtype=float32), -0.0352897]. 
=============================================
[2019-04-04 00:59:27,076] A3C_AGENT_WORKER-Thread-18 INFO:Local step 132000, global step 2113034: loss 0.0141
[2019-04-04 00:59:27,077] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 132000, global step 2113034: learning rate 0.0001
[2019-04-04 00:59:28,461] A3C_AGENT_WORKER-Thread-8 INFO:Local step 132000, global step 2113496: loss 0.0323
[2019-04-04 00:59:28,462] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 132000, global step 2113496: learning rate 0.0001
[2019-04-04 00:59:28,649] A3C_AGENT_WORKER-Thread-4 INFO:Local step 132000, global step 2113559: loss 0.0306
[2019-04-04 00:59:28,651] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 132000, global step 2113560: learning rate 0.0001
[2019-04-04 00:59:29,909] A3C_AGENT_WORKER-Thread-17 INFO:Local step 132500, global step 2114023: loss 0.0217
[2019-04-04 00:59:29,911] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 132500, global step 2114023: learning rate 0.0001
[2019-04-04 00:59:31,606] A3C_AGENT_WORKER-Thread-15 INFO:Local step 132000, global step 2114706: loss 0.0092
[2019-04-04 00:59:31,608] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 132000, global step 2114706: learning rate 0.0001
[2019-04-04 00:59:31,982] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.8005027e-21 3.6966236e-13 2.5971041e-19 6.3991327e-16 1.2805183e-15
 3.0633921e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 00:59:31,982] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5981
[2019-04-04 00:59:32,041] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 100.0, 90.0, 0.0, 26.0, 24.24680208438114, 0.3102725364111186, 1.0, 1.0, 197582.0178649458], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2903400.0000, 
sim time next is 2904000.0000, 
raw observation next is [2.0, 100.0, 89.16666666666666, 0.0, 26.0, 24.95547864635611, 0.3729836070923357, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.518005540166205, 1.0, 0.29722222222222217, 0.0, 0.6666666666666666, 0.5796232205296757, 0.6243278690307785, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0361706], dtype=float32), -0.1323606]. 
=============================================
[2019-04-04 00:59:32,051] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[82.80258 ]
 [82.393524]
 [82.15299 ]
 [82.61796 ]
 [82.98323 ]], R is [[83.04161835]
 [82.27033997]
 [81.51530457]
 [81.65567017]
 [81.75016022]].
[2019-04-04 00:59:32,163] A3C_AGENT_WORKER-Thread-7 INFO:Local step 132500, global step 2114924: loss 0.0186
[2019-04-04 00:59:32,164] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 132500, global step 2114924: learning rate 0.0001
[2019-04-04 00:59:33,652] A3C_AGENT_WORKER-Thread-9 INFO:Local step 132000, global step 2115472: loss 0.0106
[2019-04-04 00:59:33,653] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 132000, global step 2115472: learning rate 0.0001
[2019-04-04 00:59:34,301] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.2871495e-19 2.3575709e-11 4.7585689e-17 2.2184003e-13 4.0820296e-13
 1.3453576e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:59:34,302] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8146
[2019-04-04 00:59:34,364] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.9333333333333333, 90.33333333333334, 0.0, 0.0, 26.0, 25.00179716450701, 0.2774128086375748, 0.0, 1.0, 34697.94369682778], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3091800.0000, 
sim time next is 3092400.0000, 
raw observation next is [-1.0, 92.0, 0.0, 0.0, 26.0, 24.99748039798364, 0.27530477630989, 0.0, 1.0, 37543.39411578916], 
processed observation next is [0.0, 0.8260869565217391, 0.4349030470914128, 0.92, 0.0, 0.0, 0.6666666666666666, 0.5831233664986367, 0.5917682587699633, 0.0, 1.0, 0.17877806721804362], 
reward next is 0.8212, 
noisyNet noise sample is [array([-0.7324518], dtype=float32), -0.5983435]. 
=============================================
[2019-04-04 00:59:34,994] A3C_AGENT_WORKER-Thread-16 INFO:Local step 132000, global step 2115967: loss 0.0106
[2019-04-04 00:59:34,999] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 132000, global step 2115968: learning rate 0.0001
[2019-04-04 00:59:35,586] A3C_AGENT_WORKER-Thread-20 INFO:Local step 132000, global step 2116201: loss 0.0098
[2019-04-04 00:59:35,586] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 132000, global step 2116201: learning rate 0.0001
[2019-04-04 00:59:36,186] A3C_AGENT_WORKER-Thread-10 INFO:Local step 132000, global step 2116455: loss 0.0089
[2019-04-04 00:59:36,188] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 132000, global step 2116455: learning rate 0.0001
[2019-04-04 00:59:36,356] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [8.5821038e-19 1.1422625e-11 8.2717439e-17 1.2883026e-13 2.8912487e-13
 2.0586689e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:59:36,357] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6805
[2019-04-04 00:59:36,359] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2372178e-20 1.3942133e-12 2.8274458e-18 1.0489795e-14 1.7506624e-14
 6.7167169e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 00:59:36,363] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2927
[2019-04-04 00:59:36,370] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.62905551039858, 0.2045100241606967, 0.0, 1.0, 41273.40194359444], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2774400.0000, 
sim time next is 2775000.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.63972806748424, 0.1953581263165057, 0.0, 1.0, 41150.38282318524], 
processed observation next is [1.0, 0.08695652173913043, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5533106722903532, 0.5651193754388352, 0.0, 1.0, 0.1959542039199297], 
reward next is 0.8040, 
noisyNet noise sample is [array([1.0641308], dtype=float32), 0.5290692]. 
=============================================
[2019-04-04 00:59:36,379] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 100.0, 0.0, 0.0, 26.0, 25.28959355688119, 0.3264403287357778, 0.0, 1.0, 54132.98238778719], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3128400.0000, 
sim time next is 3129000.0000, 
raw observation next is [3.166666666666667, 100.0, 0.0, 0.0, 26.0, 25.45843763995906, 0.308996056003706, 0.0, 1.0, 18760.79927051768], 
processed observation next is [1.0, 0.21739130434782608, 0.5503231763619576, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6215364699965885, 0.6029986853345687, 0.0, 1.0, 0.08933713938341753], 
reward next is 0.9107, 
noisyNet noise sample is [array([1.0141009], dtype=float32), 0.86354244]. 
=============================================
[2019-04-04 00:59:36,385] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[80.71573]
 [80.71643]
 [80.73717]
 [80.66977]
 [80.63325]], R is [[80.70067596]
 [80.63589478]
 [80.57341003]
 [80.51131439]
 [80.44952393]].
[2019-04-04 00:59:36,392] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[77.0782  ]
 [77.075424]
 [77.0746  ]
 [77.070724]
 [77.05798 ]], R is [[77.1155014 ]
 [77.14780426]
 [77.179245  ]
 [77.20985413]
 [77.23957825]].
[2019-04-04 00:59:37,935] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.8263215e-20 4.9733638e-13 7.6782635e-18 5.0461913e-14 1.2230275e-13
 8.1102565e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 00:59:37,935] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7516
[2019-04-04 00:59:37,949] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 61.5, 0.0, 0.0, 26.0, 25.36814895002933, 0.442780105730884, 0.0, 1.0, 67315.21772803208], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2755800.0000, 
sim time next is 2756400.0000, 
raw observation next is [-6.0, 60.66666666666666, 0.0, 0.0, 26.0, 25.34786031254403, 0.4407071039054395, 0.0, 1.0, 57475.21611992049], 
processed observation next is [1.0, 0.9130434782608695, 0.296398891966759, 0.6066666666666666, 0.0, 0.0, 0.6666666666666666, 0.6123216927120024, 0.6469023679684799, 0.0, 1.0, 0.2736915053329547], 
reward next is 0.7263, 
noisyNet noise sample is [array([0.39223635], dtype=float32), -0.65145737]. 
=============================================
[2019-04-04 00:59:40,352] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.0328580e-20 4.5623206e-12 2.2569823e-17 1.4303269e-14 2.9833893e-14
 6.7776429e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:59:40,359] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9059
[2019-04-04 00:59:40,382] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 86.0, 0.0, 0.0, 26.0, 25.07543929589623, 0.2961217797604782, 0.0, 1.0, 53651.30025832925], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2862000.0000, 
sim time next is 2862600.0000, 
raw observation next is [1.0, 87.16666666666667, 0.0, 0.0, 26.0, 25.08633935751141, 0.3048864958962967, 0.0, 1.0, 52562.95092755607], 
processed observation next is [1.0, 0.13043478260869565, 0.4903047091412743, 0.8716666666666667, 0.0, 0.0, 0.6666666666666666, 0.5905282797926175, 0.6016288319654323, 0.0, 1.0, 0.2502997663216956], 
reward next is 0.7497, 
noisyNet noise sample is [array([0.05283119], dtype=float32), 0.86367226]. 
=============================================
[2019-04-04 00:59:41,468] A3C_AGENT_WORKER-Thread-2 INFO:Local step 132500, global step 2118572: loss 0.0023
[2019-04-04 00:59:41,469] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 132500, global step 2118572: learning rate 0.0001
[2019-04-04 00:59:41,588] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.4890941e-20 3.2596587e-13 2.4936137e-18 1.5286160e-14 2.9336203e-14
 1.4244582e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 00:59:41,588] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5728
[2019-04-04 00:59:41,626] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 89.66666666666667, 0.0, 0.0, 26.0, 25.15347097762542, 0.4126455196517979, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2920200.0000, 
sim time next is 2920800.0000, 
raw observation next is [-1.0, 87.33333333333334, 0.0, 0.0, 26.0, 25.16909837404203, 0.3853225876765645, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.4349030470914128, 0.8733333333333334, 0.0, 0.0, 0.6666666666666666, 0.5974248645035024, 0.6284408625588548, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6402206], dtype=float32), -0.26066962]. 
=============================================
[2019-04-04 00:59:41,990] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1633776e-18 8.1435740e-11 4.8527193e-17 9.5754039e-14 3.3887202e-13
 4.2036674e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:59:41,997] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1562
[2019-04-04 00:59:42,009] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.87559494075398, 0.2537325317946082, 0.0, 1.0, 37973.33131504781], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3023400.0000, 
sim time next is 3024000.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.84008398536729, 0.246376479061709, 0.0, 1.0, 37931.16879212891], 
processed observation next is [0.0, 0.0, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5700069987806075, 0.5821254930205697, 0.0, 1.0, 0.18062461329585197], 
reward next is 0.8194, 
noisyNet noise sample is [array([0.89747757], dtype=float32), 0.5103812]. 
=============================================
[2019-04-04 00:59:42,043] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[79.22549]
 [79.20064]
 [79.16684]
 [79.1307 ]
 [79.10558]], R is [[79.2454834 ]
 [79.27220154]
 [79.29843903]
 [79.32414246]
 [79.34922791]].
[2019-04-04 00:59:42,281] A3C_AGENT_WORKER-Thread-5 INFO:Local step 132500, global step 2118928: loss 0.0016
[2019-04-04 00:59:42,284] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 132500, global step 2118930: learning rate 0.0001
[2019-04-04 00:59:42,568] A3C_AGENT_WORKER-Thread-14 INFO:Local step 132500, global step 2119060: loss 0.0023
[2019-04-04 00:59:42,570] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 132500, global step 2119060: learning rate 0.0001
[2019-04-04 00:59:43,332] A3C_AGENT_WORKER-Thread-19 INFO:Local step 133000, global step 2119366: loss 0.1351
[2019-04-04 00:59:43,348] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 133000, global step 2119366: learning rate 0.0001
[2019-04-04 00:59:44,951] A3C_AGENT_WORKER-Thread-6 INFO:Local step 132500, global step 2119950: loss 0.0027
[2019-04-04 00:59:44,952] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 132500, global step 2119951: learning rate 0.0001
[2019-04-04 00:59:45,099] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.1526891e-19 9.0671862e-12 1.1193904e-17 3.2901081e-14 1.5006926e-13
 2.0889675e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 00:59:45,104] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.3634
[2019-04-04 00:59:45,162] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.166666666666667, 66.0, 204.0, 110.6666666666666, 26.0, 24.97633688472051, 0.331389476273013, 0.0, 1.0, 18747.26841677], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2976600.0000, 
sim time next is 2977200.0000, 
raw observation next is [-3.0, 65.0, 217.0, 154.0, 26.0, 24.97850470787592, 0.3390372724516872, 0.0, 1.0, 18743.28119188454], 
processed observation next is [0.0, 0.4782608695652174, 0.3795013850415513, 0.65, 0.7233333333333334, 0.17016574585635358, 0.6666666666666666, 0.58154205898966, 0.6130124241505625, 0.0, 1.0, 0.08925371996135495], 
reward next is 0.9107, 
noisyNet noise sample is [array([-1.1285064], dtype=float32), -0.6187861]. 
=============================================
[2019-04-04 00:59:46,214] A3C_AGENT_WORKER-Thread-3 INFO:Local step 132500, global step 2120479: loss 0.0010
[2019-04-04 00:59:46,217] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 132500, global step 2120480: learning rate 0.0001
[2019-04-04 00:59:48,069] A3C_AGENT_WORKER-Thread-18 INFO:Local step 132500, global step 2121196: loss 0.0009
[2019-04-04 00:59:48,071] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 132500, global step 2121196: learning rate 0.0001
[2019-04-04 00:59:48,458] A3C_AGENT_WORKER-Thread-17 INFO:Local step 133000, global step 2121364: loss 0.0870
[2019-04-04 00:59:48,461] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 133000, global step 2121364: learning rate 0.0001
[2019-04-04 00:59:49,032] A3C_AGENT_WORKER-Thread-8 INFO:Local step 132500, global step 2121639: loss 0.0014
[2019-04-04 00:59:49,033] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 132500, global step 2121639: learning rate 0.0001
[2019-04-04 00:59:49,559] A3C_AGENT_WORKER-Thread-4 INFO:Local step 132500, global step 2121882: loss 0.0046
[2019-04-04 00:59:49,562] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 132500, global step 2121884: learning rate 0.0001
[2019-04-04 00:59:50,153] A3C_AGENT_WORKER-Thread-7 INFO:Local step 133000, global step 2122204: loss 0.0650
[2019-04-04 00:59:50,155] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 133000, global step 2122204: learning rate 0.0001
[2019-04-04 00:59:51,844] A3C_AGENT_WORKER-Thread-15 INFO:Local step 132500, global step 2122967: loss 0.0025
[2019-04-04 00:59:51,846] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 132500, global step 2122967: learning rate 0.0001
[2019-04-04 00:59:52,837] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.9060851e-20 3.0343996e-12 2.7999898e-18 7.8964586e-14 2.1286783e-14
 2.2619189e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 00:59:52,837] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0166
[2019-04-04 00:59:52,886] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.333333333333333, 56.66666666666667, 78.5, 634.8333333333334, 26.0, 25.12767716858184, 0.411679020090328, 0.0, 1.0, 18708.72778748984], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2994000.0000, 
sim time next is 2994600.0000, 
raw observation next is [-1.166666666666667, 55.83333333333334, 74.0, 602.6666666666667, 26.0, 25.13784346567646, 0.4096038993193159, 0.0, 1.0, 18706.99855834664], 
processed observation next is [0.0, 0.6521739130434783, 0.43028624192059095, 0.5583333333333335, 0.24666666666666667, 0.6659300184162064, 0.6666666666666666, 0.5948202888063717, 0.6365346331064387, 0.0, 1.0, 0.08908094551593639], 
reward next is 0.9109, 
noisyNet noise sample is [array([1.0020227], dtype=float32), 0.7036813]. 
=============================================
[2019-04-04 00:59:53,970] A3C_AGENT_WORKER-Thread-9 INFO:Local step 132500, global step 2123975: loss 0.0006
[2019-04-04 00:59:53,971] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 132500, global step 2123975: learning rate 0.0001
[2019-04-04 00:59:54,088] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.4999311e-21 1.3151286e-13 1.4625510e-19 2.2891713e-15 1.0185064e-15
 1.2770605e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 00:59:54,089] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9115
[2019-04-04 00:59:54,105] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 73.33333333333334, 114.3333333333333, 819.0, 26.0, 26.66906271743749, 0.7655589425682305, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3240600.0000, 
sim time next is 3241200.0000, 
raw observation next is [-2.0, 75.66666666666667, 114.6666666666667, 821.0, 26.0, 26.69584129301002, 0.7607995940254341, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.40720221606648205, 0.7566666666666667, 0.38222222222222235, 0.907182320441989, 0.6666666666666666, 0.7246534410841683, 0.7535998646751447, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8433717], dtype=float32), -1.2754707]. 
=============================================
[2019-04-04 00:59:55,061] A3C_AGENT_WORKER-Thread-16 INFO:Local step 132500, global step 2124549: loss 0.0031
[2019-04-04 00:59:55,063] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 132500, global step 2124551: learning rate 0.0001
[2019-04-04 00:59:55,576] A3C_AGENT_WORKER-Thread-20 INFO:Local step 132500, global step 2124812: loss 0.0050
[2019-04-04 00:59:55,605] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 132500, global step 2124813: learning rate 0.0001
[2019-04-04 00:59:56,236] A3C_AGENT_WORKER-Thread-10 INFO:Local step 132500, global step 2125150: loss 0.0025
[2019-04-04 00:59:56,237] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 132500, global step 2125150: learning rate 0.0001
[2019-04-04 00:59:58,414] A3C_AGENT_WORKER-Thread-2 INFO:Local step 133000, global step 2126190: loss 0.1127
[2019-04-04 00:59:58,418] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 133000, global step 2126190: learning rate 0.0001
[2019-04-04 00:59:59,046] A3C_AGENT_WORKER-Thread-5 INFO:Local step 133000, global step 2126521: loss 0.0993
[2019-04-04 00:59:59,048] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 133000, global step 2126523: learning rate 0.0001
[2019-04-04 00:59:59,416] A3C_AGENT_WORKER-Thread-19 INFO:Local step 133500, global step 2126710: loss 2.0443
[2019-04-04 00:59:59,419] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 133500, global step 2126711: learning rate 0.0001
[2019-04-04 00:59:59,725] A3C_AGENT_WORKER-Thread-14 INFO:Local step 133000, global step 2126862: loss 0.1306
[2019-04-04 00:59:59,727] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 133000, global step 2126864: learning rate 0.0001
[2019-04-04 01:00:01,897] A3C_AGENT_WORKER-Thread-6 INFO:Local step 133000, global step 2127743: loss 0.1596
[2019-04-04 01:00:01,898] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 133000, global step 2127743: learning rate 0.0001
[2019-04-04 01:00:02,696] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [9.4146318e-20 5.6982665e-11 1.2105575e-17 5.6123441e-14 8.3813063e-14
 2.0701351e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 01:00:02,696] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0888
[2019-04-04 01:00:02,847] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.666666666666667, 70.0, 73.83333333333334, 374.3333333333334, 26.0, 24.19498466147514, 0.2920640948446717, 0.0, 1.0, 202438.3946901009], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3572400.0000, 
sim time next is 3573000.0000, 
raw observation next is [-6.5, 70.0, 88.0, 425.0, 26.0, 24.58190874133776, 0.4087603479271034, 0.0, 1.0, 202114.1663368627], 
processed observation next is [0.0, 0.34782608695652173, 0.28254847645429365, 0.7, 0.29333333333333333, 0.4696132596685083, 0.6666666666666666, 0.54849239511148, 0.6362534493090345, 0.0, 1.0, 0.9624484111279176], 
reward next is 0.0376, 
noisyNet noise sample is [array([0.35981104], dtype=float32), -0.9657347]. 
=============================================
[2019-04-04 01:00:02,923] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[84.430725]
 [83.39895 ]
 [83.09022 ]
 [82.69113 ]
 [82.02058 ]], R is [[84.97320557]
 [84.15948486]
 [84.12122345]
 [84.0827713 ]
 [84.04428864]].
[2019-04-04 01:00:03,793] A3C_AGENT_WORKER-Thread-3 INFO:Local step 133000, global step 2128320: loss 0.1406
[2019-04-04 01:00:03,808] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 133000, global step 2128320: learning rate 0.0001
[2019-04-04 01:00:06,247] A3C_AGENT_WORKER-Thread-17 INFO:Local step 133500, global step 2129047: loss 2.0550
[2019-04-04 01:00:06,249] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 133500, global step 2129047: learning rate 0.0001
[2019-04-04 01:00:06,266] A3C_AGENT_WORKER-Thread-18 INFO:Local step 133000, global step 2129053: loss 0.1526
[2019-04-04 01:00:06,267] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 133000, global step 2129053: learning rate 0.0001
[2019-04-04 01:00:07,954] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [9.1362890e-21 3.9675929e-13 7.1493665e-19 8.1512183e-15 9.3069956e-15
 6.4172304e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:00:07,954] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.9462
[2019-04-04 01:00:07,986] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.666666666666666, 54.00000000000001, 117.6666666666667, 808.8333333333334, 26.0, 26.36468254809888, 0.5898986169718059, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3327600.0000, 
sim time next is 3328200.0000, 
raw observation next is [-5.5, 54.0, 118.0, 811.0, 26.0, 26.24678252315418, 0.5773457359469171, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.3102493074792244, 0.54, 0.3933333333333333, 0.8961325966850828, 0.6666666666666666, 0.687231876929515, 0.6924485786489724, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.46227834], dtype=float32), -0.20287439]. 
=============================================
[2019-04-04 01:00:08,820] A3C_AGENT_WORKER-Thread-4 INFO:Local step 133000, global step 2129766: loss 0.1432
[2019-04-04 01:00:08,823] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 133000, global step 2129766: learning rate 0.0001
[2019-04-04 01:00:08,937] A3C_AGENT_WORKER-Thread-8 INFO:Local step 133000, global step 2129817: loss 0.1329
[2019-04-04 01:00:08,938] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 133000, global step 2129817: learning rate 0.0001
[2019-04-04 01:00:08,953] A3C_AGENT_WORKER-Thread-7 INFO:Local step 133500, global step 2129822: loss 2.1911
[2019-04-04 01:00:08,955] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 133500, global step 2129822: learning rate 0.0001
[2019-04-04 01:00:14,074] A3C_AGENT_WORKER-Thread-15 INFO:Local step 133000, global step 2131211: loss 0.1613
[2019-04-04 01:00:14,075] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 133000, global step 2131211: learning rate 0.0001
[2019-04-04 01:00:18,382] A3C_AGENT_WORKER-Thread-9 INFO:Local step 133000, global step 2132330: loss 0.1810
[2019-04-04 01:00:18,382] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 133000, global step 2132330: learning rate 0.0001
[2019-04-04 01:00:19,652] A3C_AGENT_WORKER-Thread-16 INFO:Local step 133000, global step 2132703: loss 0.1311
[2019-04-04 01:00:19,652] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 133000, global step 2132703: learning rate 0.0001
[2019-04-04 01:00:21,855] A3C_AGENT_WORKER-Thread-20 INFO:Local step 133000, global step 2133353: loss 0.1350
[2019-04-04 01:00:21,856] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 133000, global step 2133354: learning rate 0.0001
[2019-04-04 01:00:22,151] A3C_AGENT_WORKER-Thread-10 INFO:Local step 133000, global step 2133453: loss 0.1214
[2019-04-04 01:00:22,152] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 133000, global step 2133453: learning rate 0.0001
[2019-04-04 01:00:22,297] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.1411631e-18 1.0921006e-10 3.4509525e-17 1.0141884e-13 4.7096474e-13
 4.1649284e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 01:00:22,297] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0974
[2019-04-04 01:00:22,329] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.166666666666667, 65.83333333333334, 0.0, 0.0, 26.0, 24.85047182274906, 0.3041705778640142, 0.0, 1.0, 40876.26200981422], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3561000.0000, 
sim time next is 3561600.0000, 
raw observation next is [-5.333333333333334, 66.66666666666667, 0.0, 0.0, 26.0, 24.79758124861742, 0.2939922341627513, 0.0, 1.0, 40881.9426082577], 
processed observation next is [0.0, 0.21739130434782608, 0.31486611265004616, 0.6666666666666667, 0.0, 0.0, 0.6666666666666666, 0.5664651040514516, 0.5979974113875838, 0.0, 1.0, 0.19467591718217953], 
reward next is 0.8053, 
noisyNet noise sample is [array([-0.7657187], dtype=float32), -0.8248729]. 
=============================================
[2019-04-04 01:00:24,152] A3C_AGENT_WORKER-Thread-2 INFO:Local step 133500, global step 2134035: loss 2.2313
[2019-04-04 01:00:24,152] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 133500, global step 2134035: learning rate 0.0001
[2019-04-04 01:00:25,242] A3C_AGENT_WORKER-Thread-5 INFO:Local step 133500, global step 2134350: loss 2.4032
[2019-04-04 01:00:25,245] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 133500, global step 2134350: learning rate 0.0001
[2019-04-04 01:00:26,483] A3C_AGENT_WORKER-Thread-19 INFO:Local step 134000, global step 2134695: loss 0.3998
[2019-04-04 01:00:26,489] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 134000, global step 2134695: learning rate 0.0001
[2019-04-04 01:00:27,717] A3C_AGENT_WORKER-Thread-14 INFO:Local step 133500, global step 2135016: loss 2.4410
[2019-04-04 01:00:27,718] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 133500, global step 2135016: learning rate 0.0001
[2019-04-04 01:00:30,225] A3C_AGENT_WORKER-Thread-6 INFO:Local step 133500, global step 2135597: loss 2.3075
[2019-04-04 01:00:30,278] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 133500, global step 2135597: learning rate 0.0001
[2019-04-04 01:00:33,345] A3C_AGENT_WORKER-Thread-3 INFO:Local step 133500, global step 2136271: loss 2.0291
[2019-04-04 01:00:33,369] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 133500, global step 2136271: learning rate 0.0001
[2019-04-04 01:00:35,142] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.4517610e-19 1.5580542e-11 1.4564948e-17 2.6102611e-14 9.1292323e-14
 2.6770522e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 01:00:35,144] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2730
[2019-04-04 01:00:35,230] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.666666666666667, 47.33333333333333, 0.0, 0.0, 26.0, 25.39973984182495, 0.3907412616166638, 0.0, 1.0, 55724.77812974722], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3620400.0000, 
sim time next is 3621000.0000, 
raw observation next is [-1.833333333333333, 48.66666666666667, 0.0, 0.0, 26.0, 25.3903768558415, 0.3934375113267485, 0.0, 1.0, 49334.50732912542], 
processed observation next is [0.0, 0.9130434782608695, 0.41181902123730385, 0.4866666666666667, 0.0, 0.0, 0.6666666666666666, 0.6158647379867915, 0.6311458371089161, 0.0, 1.0, 0.23492622537678773], 
reward next is 0.7651, 
noisyNet noise sample is [array([-0.24661075], dtype=float32), 0.47043368]. 
=============================================
[2019-04-04 01:00:35,288] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[85.59055 ]
 [85.545425]
 [85.5211  ]
 [85.49513 ]
 [85.46724 ]], R is [[85.50396729]
 [85.38356781]
 [85.24497223]
 [85.12028503]
 [85.04712677]].
[2019-04-04 01:00:35,320] A3C_AGENT_WORKER-Thread-17 INFO:Local step 134000, global step 2136745: loss 0.2705
[2019-04-04 01:00:35,322] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 134000, global step 2136745: learning rate 0.0001
[2019-04-04 01:00:36,362] A3C_AGENT_WORKER-Thread-18 INFO:Local step 133500, global step 2137082: loss 2.2496
[2019-04-04 01:00:36,363] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 133500, global step 2137082: learning rate 0.0001
[2019-04-04 01:00:37,726] A3C_AGENT_WORKER-Thread-7 INFO:Local step 134000, global step 2137478: loss 0.3077
[2019-04-04 01:00:37,727] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 134000, global step 2137478: learning rate 0.0001
[2019-04-04 01:00:39,321] A3C_AGENT_WORKER-Thread-8 INFO:Local step 133500, global step 2137909: loss 2.1454
[2019-04-04 01:00:39,321] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 133500, global step 2137909: learning rate 0.0001
[2019-04-04 01:00:40,224] A3C_AGENT_WORKER-Thread-4 INFO:Local step 133500, global step 2138150: loss 2.0777
[2019-04-04 01:00:40,228] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 133500, global step 2138150: learning rate 0.0001
[2019-04-04 01:00:45,665] A3C_AGENT_WORKER-Thread-15 INFO:Local step 133500, global step 2139410: loss 2.0992
[2019-04-04 01:00:45,682] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 133500, global step 2139410: learning rate 0.0001
[2019-04-04 01:00:46,074] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.8195172e-19 2.4300015e-11 3.1795301e-17 3.7028879e-14 1.4440005e-13
 3.9540449e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 01:00:46,074] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4975
[2019-04-04 01:00:46,115] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 70.0, 0.0, 0.0, 26.0, 24.58594590051771, 0.2455349537048087, 0.0, 1.0, 40900.74388155824], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3565200.0000, 
sim time next is 3565800.0000, 
raw observation next is [-6.0, 70.0, 0.0, 0.0, 26.0, 24.53775429480983, 0.2356392945129711, 0.0, 1.0, 40956.02231922626], 
processed observation next is [0.0, 0.2608695652173913, 0.296398891966759, 0.7, 0.0, 0.0, 0.6666666666666666, 0.5448128579008191, 0.5785464315043237, 0.0, 1.0, 0.19502867771060123], 
reward next is 0.8050, 
noisyNet noise sample is [array([-0.6931008], dtype=float32), 0.4863566]. 
=============================================
[2019-04-04 01:00:49,534] A3C_AGENT_WORKER-Thread-9 INFO:Local step 133500, global step 2140479: loss 2.1027
[2019-04-04 01:00:49,534] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 133500, global step 2140479: learning rate 0.0001
[2019-04-04 01:00:51,821] A3C_AGENT_WORKER-Thread-16 INFO:Local step 133500, global step 2141041: loss 2.0480
[2019-04-04 01:00:51,840] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 133500, global step 2141041: learning rate 0.0001
[2019-04-04 01:00:53,192] A3C_AGENT_WORKER-Thread-2 INFO:Local step 134000, global step 2141475: loss 0.3728
[2019-04-04 01:00:53,194] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 134000, global step 2141475: learning rate 0.0001
[2019-04-04 01:00:53,435] A3C_AGENT_WORKER-Thread-20 INFO:Local step 133500, global step 2141549: loss 2.0596
[2019-04-04 01:00:53,437] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 133500, global step 2141549: learning rate 0.0001
[2019-04-04 01:00:54,013] A3C_AGENT_WORKER-Thread-10 INFO:Local step 133500, global step 2141722: loss 2.1660
[2019-04-04 01:00:54,017] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 133500, global step 2141722: learning rate 0.0001
[2019-04-04 01:00:55,503] A3C_AGENT_WORKER-Thread-5 INFO:Local step 134000, global step 2142148: loss 0.3055
[2019-04-04 01:00:55,505] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 134000, global step 2142148: learning rate 0.0001
[2019-04-04 01:00:57,782] A3C_AGENT_WORKER-Thread-14 INFO:Local step 134000, global step 2142756: loss 0.3675
[2019-04-04 01:00:57,783] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 134000, global step 2142756: learning rate 0.0001
[2019-04-04 01:01:00,301] A3C_AGENT_WORKER-Thread-6 INFO:Local step 134000, global step 2143374: loss 0.3400
[2019-04-04 01:01:00,302] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 134000, global step 2143374: learning rate 0.0001
[2019-04-04 01:01:00,711] A3C_AGENT_WORKER-Thread-19 INFO:Local step 134500, global step 2143445: loss 0.0182
[2019-04-04 01:01:00,716] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 134500, global step 2143445: learning rate 0.0001
[2019-04-04 01:01:02,978] A3C_AGENT_WORKER-Thread-3 INFO:Local step 134000, global step 2143986: loss 0.3922
[2019-04-04 01:01:02,979] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 134000, global step 2143986: learning rate 0.0001
[2019-04-04 01:01:05,513] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [9.4261677e-20 1.1962204e-11 1.7317034e-17 3.5305719e-14 1.5235549e-13
 1.1637460e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 01:01:05,513] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.7384
[2019-04-04 01:01:05,571] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.666666666666667, 71.0, 0.0, 0.0, 26.0, 25.21442428862093, 0.3415776827169387, 0.0, 1.0, 41100.13441992427], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3907200.0000, 
sim time next is 3907800.0000, 
raw observation next is [-5.0, 68.0, 0.0, 0.0, 26.0, 25.15215612320444, 0.3251548556823612, 0.0, 1.0, 41241.26252569679], 
processed observation next is [1.0, 0.21739130434782608, 0.32409972299168976, 0.68, 0.0, 0.0, 0.6666666666666666, 0.5960130102670366, 0.6083849518941203, 0.0, 1.0, 0.19638696440807993], 
reward next is 0.8036, 
noisyNet noise sample is [array([-0.32539475], dtype=float32), 0.4355033]. 
=============================================
[2019-04-04 01:01:05,829] A3C_AGENT_WORKER-Thread-17 INFO:Local step 134500, global step 2144715: loss 0.0132
[2019-04-04 01:01:05,887] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 134500, global step 2144715: learning rate 0.0001
[2019-04-04 01:01:07,953] A3C_AGENT_WORKER-Thread-18 INFO:Local step 134000, global step 2145204: loss 0.4439
[2019-04-04 01:01:07,954] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 134000, global step 2145204: learning rate 0.0001
[2019-04-04 01:01:09,425] A3C_AGENT_WORKER-Thread-8 INFO:Local step 134000, global step 2145566: loss 0.4017
[2019-04-04 01:01:09,427] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 134000, global step 2145567: learning rate 0.0001
[2019-04-04 01:01:10,251] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1569554e-19 5.2530324e-12 5.2605258e-18 1.8834745e-14 7.4663129e-14
 2.6961657e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 01:01:10,252] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0227
[2019-04-04 01:01:10,320] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 77.0, 0.0, 0.0, 26.0, 25.2667299339836, 0.4023148131331706, 0.0, 1.0, 44178.81176154229], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3806400.0000, 
sim time next is 3807000.0000, 
raw observation next is [-4.0, 77.0, 0.0, 0.0, 26.0, 25.30862239525782, 0.4026604670861376, 0.0, 1.0, 44162.28474514392], 
processed observation next is [1.0, 0.043478260869565216, 0.3518005540166205, 0.77, 0.0, 0.0, 0.6666666666666666, 0.6090518662714851, 0.6342201556953792, 0.0, 1.0, 0.21029659402449488], 
reward next is 0.7897, 
noisyNet noise sample is [array([-1.837366], dtype=float32), -0.8461424]. 
=============================================
[2019-04-04 01:01:10,436] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[79.727684]
 [79.6757  ]
 [79.630905]
 [79.636765]
 [79.67017 ]], R is [[79.77692413]
 [79.76878357]
 [79.76083374]
 [79.7533493 ]
 [79.74648285]].
[2019-04-04 01:01:11,384] A3C_AGENT_WORKER-Thread-4 INFO:Local step 134000, global step 2145985: loss 0.4322
[2019-04-04 01:01:11,393] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 134000, global step 2145985: learning rate 0.0001
[2019-04-04 01:01:11,859] A3C_AGENT_WORKER-Thread-7 INFO:Local step 134500, global step 2146093: loss 0.0448
[2019-04-04 01:01:11,868] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 134500, global step 2146093: learning rate 0.0001
[2019-04-04 01:01:14,094] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1794908e-19 7.0265018e-13 3.8917632e-18 2.0704629e-14 8.6390815e-14
 3.0265379e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 01:01:14,094] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6113
[2019-04-04 01:01:14,129] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 35.5, 23.0, 57.0, 26.0, 26.636558246562, 0.6274873714718255, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4123800.0000, 
sim time next is 4124400.0000, 
raw observation next is [3.0, 35.0, 19.16666666666667, 47.5, 26.0, 26.50335725151315, 0.6804277224355384, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.5457063711911359, 0.35, 0.0638888888888889, 0.052486187845303865, 0.6666666666666666, 0.7086131042927626, 0.7268092408118462, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.25619537], dtype=float32), 0.40529984]. 
=============================================
[2019-04-04 01:01:15,019] A3C_AGENT_WORKER-Thread-15 INFO:Local step 134000, global step 2146807: loss 0.4171
[2019-04-04 01:01:15,021] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 134000, global step 2146807: learning rate 0.0001
[2019-04-04 01:01:15,992] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.3434069e-22 5.0338673e-14 1.5815393e-19 4.8460253e-16 1.3833139e-15
 1.4432843e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:01:15,993] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8994
[2019-04-04 01:01:16,039] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.7, 41.0, 9.0, 0.0, 26.0, 27.42327086521936, 0.924039725014833, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4383000.0000, 
sim time next is 4383600.0000, 
raw observation next is [12.6, 42.0, 7.5, 0.0, 26.0, 27.44628682231164, 0.9950756068299594, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.8116343490304709, 0.42, 0.025, 0.0, 0.6666666666666666, 0.7871905685259701, 0.8316918689433198, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.48901644], dtype=float32), -2.1278915]. 
=============================================
[2019-04-04 01:01:18,187] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.6984537e-19 1.3299370e-11 3.1872536e-17 5.5534475e-14 1.3806371e-13
 6.3046021e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 01:01:18,246] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1492
[2019-04-04 01:01:18,279] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.333333333333334, 36.33333333333333, 0.0, 0.0, 26.0, 24.84516892751611, 0.2286926485071905, 0.0, 1.0, 40387.37840132401], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4077600.0000, 
sim time next is 4078200.0000, 
raw observation next is [-4.166666666666667, 35.16666666666667, 0.0, 0.0, 26.0, 24.78929686555816, 0.2317953975124562, 0.0, 1.0, 40362.70866115568], 
processed observation next is [1.0, 0.17391304347826086, 0.3471837488457987, 0.35166666666666674, 0.0, 0.0, 0.6666666666666666, 0.5657747387965134, 0.5772651325041521, 0.0, 1.0, 0.19220337457693182], 
reward next is 0.8078, 
noisyNet noise sample is [array([0.49603307], dtype=float32), -0.4794428]. 
=============================================
[2019-04-04 01:01:22,720] A3C_AGENT_WORKER-Thread-9 INFO:Local step 134000, global step 2148564: loss 0.5393
[2019-04-04 01:01:22,721] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 134000, global step 2148564: learning rate 0.0001
[2019-04-04 01:01:23,477] A3C_AGENT_WORKER-Thread-16 INFO:Local step 134000, global step 2148767: loss 0.4957
[2019-04-04 01:01:23,482] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 134000, global step 2148767: learning rate 0.0001
[2019-04-04 01:01:23,926] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.01498599e-20 1.07008552e-13 1.04992956e-19 1.22332479e-15
 1.87079911e-15 6.17470738e-22 1.00000000e+00], sum to 1.0000
[2019-04-04 01:01:23,927] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0934
[2019-04-04 01:01:23,937] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.166666666666667, 31.66666666666667, 109.3333333333333, 806.0, 26.0, 26.63457215665083, 0.6937527007454812, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4111800.0000, 
sim time next is 4112400.0000, 
raw observation next is [3.333333333333333, 32.33333333333334, 107.6666666666667, 800.0, 26.0, 26.86202403625707, 0.7233813160644081, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5549399815327793, 0.3233333333333334, 0.358888888888889, 0.8839779005524862, 0.6666666666666666, 0.7385020030214223, 0.7411271053548028, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6919689], dtype=float32), 1.5765593]. 
=============================================
[2019-04-04 01:01:25,003] A3C_AGENT_WORKER-Thread-20 INFO:Local step 134000, global step 2149229: loss 0.5400
[2019-04-04 01:01:25,003] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 134000, global step 2149229: learning rate 0.0001
[2019-04-04 01:01:25,323] A3C_AGENT_WORKER-Thread-10 INFO:Local step 134000, global step 2149301: loss 0.5981
[2019-04-04 01:01:25,324] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 134000, global step 2149301: learning rate 0.0001
[2019-04-04 01:01:25,449] A3C_AGENT_WORKER-Thread-2 INFO:Local step 134500, global step 2149335: loss 0.0533
[2019-04-04 01:01:25,470] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 134500, global step 2149335: learning rate 0.0001
[2019-04-04 01:01:30,396] A3C_AGENT_WORKER-Thread-5 INFO:Local step 134500, global step 2150378: loss 0.1079
[2019-04-04 01:01:30,409] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 134500, global step 2150378: learning rate 0.0001
[2019-04-04 01:01:33,433] A3C_AGENT_WORKER-Thread-14 INFO:Local step 134500, global step 2150948: loss 0.0609
[2019-04-04 01:01:33,433] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 134500, global step 2150948: learning rate 0.0001
[2019-04-04 01:01:33,664] A3C_AGENT_WORKER-Thread-19 INFO:Local step 135000, global step 2151009: loss 0.2286
[2019-04-04 01:01:33,666] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 135000, global step 2151009: learning rate 0.0001
[2019-04-04 01:01:34,679] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.9407724e-21 2.0614875e-13 2.1915576e-19 3.3226534e-15 1.9892382e-15
 1.0721405e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:01:34,706] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6628
[2019-04-04 01:01:34,717] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 45.0, 208.5, 62.5, 26.0, 26.09374122446678, 0.5745268200569723, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4546800.0000, 
sim time next is 4547400.0000, 
raw observation next is [2.833333333333333, 45.5, 190.0, 45.66666666666666, 26.0, 26.27172900311142, 0.5899486937475796, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.541089566020314, 0.455, 0.6333333333333333, 0.05046040515653774, 0.6666666666666666, 0.6893107502592851, 0.6966495645825265, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.47305706], dtype=float32), 0.64551127]. 
=============================================
[2019-04-04 01:01:35,547] A3C_AGENT_WORKER-Thread-6 INFO:Local step 134500, global step 2151492: loss 0.0915
[2019-04-04 01:01:35,550] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 134500, global step 2151493: learning rate 0.0001
[2019-04-04 01:01:38,145] A3C_AGENT_WORKER-Thread-17 INFO:Local step 135000, global step 2152181: loss 0.2378
[2019-04-04 01:01:38,146] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 135000, global step 2152181: learning rate 0.0001
[2019-04-04 01:01:38,267] A3C_AGENT_WORKER-Thread-3 INFO:Local step 134500, global step 2152221: loss 0.0716
[2019-04-04 01:01:38,272] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 134500, global step 2152221: learning rate 0.0001
[2019-04-04 01:01:43,801] A3C_AGENT_WORKER-Thread-18 INFO:Local step 134500, global step 2153459: loss 0.0527
[2019-04-04 01:01:43,824] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 134500, global step 2153465: learning rate 0.0001
[2019-04-04 01:01:44,455] A3C_AGENT_WORKER-Thread-7 INFO:Local step 135000, global step 2153631: loss 0.1901
[2019-04-04 01:01:44,458] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 135000, global step 2153631: learning rate 0.0001
[2019-04-04 01:01:44,652] A3C_AGENT_WORKER-Thread-8 INFO:Local step 134500, global step 2153678: loss 0.0306
[2019-04-04 01:01:44,655] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 134500, global step 2153678: learning rate 0.0001
[2019-04-04 01:01:45,556] A3C_AGENT_WORKER-Thread-4 INFO:Local step 134500, global step 2153965: loss 0.0563
[2019-04-04 01:01:45,561] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 134500, global step 2153965: learning rate 0.0001
[2019-04-04 01:01:50,627] A3C_AGENT_WORKER-Thread-15 INFO:Local step 134500, global step 2155205: loss 0.0238
[2019-04-04 01:01:50,680] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 134500, global step 2155218: learning rate 0.0001
[2019-04-04 01:01:54,712] A3C_AGENT_WORKER-Thread-9 INFO:Local step 134500, global step 2156433: loss 0.0309
[2019-04-04 01:01:54,715] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 134500, global step 2156434: learning rate 0.0001
[2019-04-04 01:01:56,779] A3C_AGENT_WORKER-Thread-2 INFO:Local step 135000, global step 2157089: loss 0.2230
[2019-04-04 01:01:56,782] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 135000, global step 2157089: learning rate 0.0001
[2019-04-04 01:01:57,573] A3C_AGENT_WORKER-Thread-16 INFO:Local step 134500, global step 2157324: loss 0.0016
[2019-04-04 01:01:57,573] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 134500, global step 2157324: learning rate 0.0001
[2019-04-04 01:01:58,370] A3C_AGENT_WORKER-Thread-20 INFO:Local step 134500, global step 2157532: loss 0.0007
[2019-04-04 01:01:58,373] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 134500, global step 2157532: learning rate 0.0001
[2019-04-04 01:01:59,887] A3C_AGENT_WORKER-Thread-10 INFO:Local step 134500, global step 2157959: loss 0.0274
[2019-04-04 01:01:59,888] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 134500, global step 2157959: learning rate 0.0001
[2019-04-04 01:02:01,921] A3C_AGENT_WORKER-Thread-5 INFO:Local step 135000, global step 2158553: loss 0.2438
[2019-04-04 01:02:01,929] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 135000, global step 2158553: learning rate 0.0001
[2019-04-04 01:02:03,102] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.6866624e-19 5.3123487e-12 1.9905266e-17 7.1523577e-14 3.2260116e-13
 5.9751826e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 01:02:03,107] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1437
[2019-04-04 01:02:03,203] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.266666666666667, 63.33333333333334, 16.0, 152.0, 26.0, 25.39445219585863, 0.3787305941344465, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4297800.0000, 
sim time next is 4298400.0000, 
raw observation next is [6.2, 64.0, 0.0, 0.0, 26.0, 25.34250421633029, 0.3538878023453524, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.782608695652174, 0.6343490304709142, 0.64, 0.0, 0.0, 0.6666666666666666, 0.6118753513608576, 0.6179626007817841, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7666789], dtype=float32), -0.010067901]. 
=============================================
[2019-04-04 01:02:03,684] A3C_AGENT_WORKER-Thread-14 INFO:Local step 135000, global step 2159109: loss 0.2707
[2019-04-04 01:02:03,709] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 135000, global step 2159117: learning rate 0.0001
[2019-04-04 01:02:03,818] A3C_AGENT_WORKER-Thread-19 INFO:Local step 135500, global step 2159149: loss 0.0006
[2019-04-04 01:02:03,869] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 135500, global step 2159162: learning rate 0.0001
[2019-04-04 01:02:04,394] A3C_AGENT_WORKER-Thread-6 INFO:Local step 135000, global step 2159345: loss 0.2508
[2019-04-04 01:02:04,395] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 135000, global step 2159346: learning rate 0.0001
[2019-04-04 01:02:06,512] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.7684674e-20 8.4373246e-12 1.0945992e-17 5.5262811e-14 4.7988353e-14
 2.5609202e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 01:02:06,512] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6503
[2019-04-04 01:02:06,557] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.1, 73.0, 0.0, 0.0, 26.0, 25.50169170808444, 0.3564580509204561, 0.0, 1.0, 41800.18459727104], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4343400.0000, 
sim time next is 4344000.0000, 
raw observation next is [3.033333333333333, 73.66666666666667, 0.0, 0.0, 26.0, 25.41277053232727, 0.3583224584454687, 0.0, 1.0, 85519.45668198958], 
processed observation next is [1.0, 0.2608695652173913, 0.5466297322253002, 0.7366666666666667, 0.0, 0.0, 0.6666666666666666, 0.6177308776939391, 0.619440819481823, 0.0, 1.0, 0.4072355080094742], 
reward next is 0.5928, 
noisyNet noise sample is [array([1.9770637], dtype=float32), 1.6041604]. 
=============================================
[2019-04-04 01:02:06,688] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[82.573875]
 [82.538666]
 [82.61739 ]
 [82.757706]
 [82.79459 ]], R is [[82.51083374]
 [82.48667908]
 [82.57254791]
 [82.74682617]
 [82.83006287]].
[2019-04-04 01:02:07,502] A3C_AGENT_WORKER-Thread-3 INFO:Local step 135000, global step 2160210: loss 0.2899
[2019-04-04 01:02:07,579] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 135000, global step 2160221: learning rate 0.0001
[2019-04-04 01:02:09,581] A3C_AGENT_WORKER-Thread-17 INFO:Local step 135500, global step 2160752: loss 0.0002
[2019-04-04 01:02:09,582] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 135500, global step 2160752: learning rate 0.0001
[2019-04-04 01:02:11,223] A3C_AGENT_WORKER-Thread-18 INFO:Local step 135000, global step 2161175: loss 0.2413
[2019-04-04 01:02:11,228] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 135000, global step 2161175: learning rate 0.0001
[2019-04-04 01:02:12,258] A3C_AGENT_WORKER-Thread-4 INFO:Local step 135000, global step 2161494: loss 0.1885
[2019-04-04 01:02:12,261] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 135000, global step 2161494: learning rate 0.0001
[2019-04-04 01:02:12,532] A3C_AGENT_WORKER-Thread-8 INFO:Local step 135000, global step 2161628: loss 0.1877
[2019-04-04 01:02:12,535] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 135000, global step 2161628: learning rate 0.0001
[2019-04-04 01:02:13,011] A3C_AGENT_WORKER-Thread-7 INFO:Local step 135500, global step 2161777: loss 0.0084
[2019-04-04 01:02:13,014] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 135500, global step 2161778: learning rate 0.0001
[2019-04-04 01:02:17,088] A3C_AGENT_WORKER-Thread-15 INFO:Local step 135000, global step 2162952: loss 0.2245
[2019-04-04 01:02:17,103] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 135000, global step 2162952: learning rate 0.0001
[2019-04-04 01:02:22,036] A3C_AGENT_WORKER-Thread-9 INFO:Local step 135000, global step 2164333: loss 0.2781
[2019-04-04 01:02:22,053] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 135000, global step 2164333: learning rate 0.0001
[2019-04-04 01:02:24,173] A3C_AGENT_WORKER-Thread-20 INFO:Local step 135000, global step 2164955: loss 0.2346
[2019-04-04 01:02:24,173] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 135000, global step 2164955: learning rate 0.0001
[2019-04-04 01:02:24,995] A3C_AGENT_WORKER-Thread-16 INFO:Local step 135000, global step 2165200: loss 0.2041
[2019-04-04 01:02:24,996] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 135000, global step 2165200: learning rate 0.0001
[2019-04-04 01:02:25,530] A3C_AGENT_WORKER-Thread-2 INFO:Local step 135500, global step 2165390: loss 0.0004
[2019-04-04 01:02:25,530] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 135500, global step 2165390: learning rate 0.0001
[2019-04-04 01:02:26,233] A3C_AGENT_WORKER-Thread-10 INFO:Local step 135000, global step 2165595: loss 0.2169
[2019-04-04 01:02:26,235] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 135000, global step 2165595: learning rate 0.0001
[2019-04-04 01:02:28,339] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:02:28,339] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:02:28,345] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run17
[2019-04-04 01:02:29,676] A3C_AGENT_WORKER-Thread-5 INFO:Local step 135500, global step 2166557: loss 0.0003
[2019-04-04 01:02:29,677] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 135500, global step 2166557: learning rate 0.0001
[2019-04-04 01:02:31,206] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0266286e-21 6.8964704e-14 1.8886488e-20 1.8930104e-15 5.2175260e-15
 3.0404673e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:02:31,206] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6358
[2019-04-04 01:02:31,221] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.33333333333333, 19.66666666666667, 112.1666666666667, 825.8333333333333, 26.0, 28.10828845635121, 1.027266345899885, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5062800.0000, 
sim time next is 5063400.0000, 
raw observation next is [11.5, 19.5, 111.0, 819.0, 26.0, 28.29822797726312, 1.057243774114189, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.7811634349030472, 0.195, 0.37, 0.9049723756906077, 0.6666666666666666, 0.8581856647719267, 0.8524145913713963, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.8208175], dtype=float32), -2.1508367]. 
=============================================
[2019-04-04 01:02:32,051] A3C_AGENT_WORKER-Thread-14 INFO:Local step 135500, global step 2167179: loss 0.0005
[2019-04-04 01:02:32,061] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 135500, global step 2167179: learning rate 0.0001
[2019-04-04 01:02:32,332] A3C_AGENT_WORKER-Thread-6 INFO:Local step 135500, global step 2167260: loss 0.0009
[2019-04-04 01:02:32,332] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 135500, global step 2167260: learning rate 0.0001
[2019-04-04 01:02:32,975] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.27734495e-21 2.65132263e-13 6.47280100e-19 1.63080651e-15
 1.11886285e-14 1.32663665e-21 1.00000000e+00], sum to 1.0000
[2019-04-04 01:02:32,975] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0513
[2019-04-04 01:02:32,982] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.2242671e-19 6.2676753e-11 4.1747448e-17 1.4925287e-13 2.6922738e-13
 2.7972355e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 01:02:32,982] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8185
[2019-04-04 01:02:33,005] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.866666666666667, 43.5, 143.0, 141.0, 26.0, 26.96688639955699, 0.809846351441932, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4637400.0000, 
sim time next is 4638000.0000, 
raw observation next is [5.733333333333333, 44.0, 130.0, 144.0, 26.0, 26.269570764432, 0.7449678769019701, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.6214219759926132, 0.44, 0.43333333333333335, 0.1591160220994475, 0.6666666666666666, 0.6891308970360001, 0.74832262563399, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7019759], dtype=float32), -0.18409282]. 
=============================================
[2019-04-04 01:02:33,009] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[84.84634 ]
 [85.22085 ]
 [85.393135]
 [85.29039 ]
 [85.27023 ]], R is [[84.45278168]
 [84.60825348]
 [84.76216888]
 [84.91455078]
 [85.0654068 ]].
[2019-04-04 01:02:33,016] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.8573458960503, 0.3338363210814979, 0.0, 1.0, 40684.60155556609], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4759200.0000, 
sim time next is 4759800.0000, 
raw observation next is [-4.333333333333334, 74.5, 0.0, 0.0, 26.0, 24.85604479035872, 0.3295606677831162, 0.0, 1.0, 40622.08453138981], 
processed observation next is [0.0, 0.08695652173913043, 0.3425669436749769, 0.745, 0.0, 0.0, 0.6666666666666666, 0.5713370658632266, 0.6098535559277054, 0.0, 1.0, 0.1934384977685229], 
reward next is 0.8066, 
noisyNet noise sample is [array([-1.0422592], dtype=float32), 1.4317964]. 
=============================================
[2019-04-04 01:02:34,869] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:02:34,869] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:02:34,872] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run17
[2019-04-04 01:02:35,671] A3C_AGENT_WORKER-Thread-3 INFO:Local step 135500, global step 2168242: loss 0.0011
[2019-04-04 01:02:35,671] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 135500, global step 2168242: learning rate 0.0001
[2019-04-04 01:02:37,160] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:02:37,160] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:02:37,201] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run17
[2019-04-04 01:02:39,075] A3C_AGENT_WORKER-Thread-8 INFO:Local step 135500, global step 2169120: loss 0.0003
[2019-04-04 01:02:39,076] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 135500, global step 2169120: learning rate 0.0001
[2019-04-04 01:02:39,191] A3C_AGENT_WORKER-Thread-18 INFO:Local step 135500, global step 2169147: loss 0.0020
[2019-04-04 01:02:39,196] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 135500, global step 2169147: learning rate 0.0001
[2019-04-04 01:02:40,140] A3C_AGENT_WORKER-Thread-4 INFO:Local step 135500, global step 2169343: loss 0.0010
[2019-04-04 01:02:40,143] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 135500, global step 2169343: learning rate 0.0001
[2019-04-04 01:02:44,456] A3C_AGENT_WORKER-Thread-15 INFO:Local step 135500, global step 2170489: loss 0.0002
[2019-04-04 01:02:44,456] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 135500, global step 2170489: learning rate 0.0001
[2019-04-04 01:02:45,226] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.3967572e-22 4.5949250e-14 1.8399856e-20 5.6865589e-16 7.5717562e-16
 3.4849555e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:02:45,226] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4366
[2019-04-04 01:02:45,262] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.0, 26.0, 113.0, 839.5, 26.0, 27.22100045288598, 0.8000057163522296, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4975200.0000, 
sim time next is 4975800.0000, 
raw observation next is [8.0, 26.0, 111.6666666666667, 832.6666666666667, 26.0, 27.35274114775165, 0.819647650715981, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.6842105263157896, 0.26, 0.37222222222222234, 0.9200736648250462, 0.6666666666666666, 0.7793950956459709, 0.7732158835719937, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4482917], dtype=float32), 2.019859]. 
=============================================
[2019-04-04 01:02:49,647] A3C_AGENT_WORKER-Thread-9 INFO:Local step 135500, global step 2171874: loss 0.0004
[2019-04-04 01:02:49,648] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 135500, global step 2171874: learning rate 0.0001
[2019-04-04 01:02:50,125] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:02:50,125] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:02:50,131] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run17
[2019-04-04 01:02:51,571] A3C_AGENT_WORKER-Thread-20 INFO:Local step 135500, global step 2172417: loss 0.0012
[2019-04-04 01:02:51,571] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 135500, global step 2172417: learning rate 0.0001
[2019-04-04 01:02:51,986] A3C_AGENT_WORKER-Thread-16 INFO:Local step 135500, global step 2172540: loss 0.0002
[2019-04-04 01:02:51,987] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 135500, global step 2172540: learning rate 0.0001
[2019-04-04 01:02:53,712] A3C_AGENT_WORKER-Thread-10 INFO:Local step 135500, global step 2172979: loss 0.0007
[2019-04-04 01:02:53,714] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 135500, global step 2172981: learning rate 0.0001
[2019-04-04 01:02:53,976] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:02:53,977] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:02:53,984] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run17
[2019-04-04 01:02:56,069] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:02:56,069] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:02:56,073] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run17
[2019-04-04 01:02:58,209] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:02:58,209] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:02:58,231] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run17
[2019-04-04 01:02:58,410] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.12822091e-21 1.36368256e-14 6.49912318e-20 6.52835715e-16
 3.59345385e-15 3.08195864e-21 1.00000000e+00], sum to 1.0000
[2019-04-04 01:02:58,411] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2924
[2019-04-04 01:02:58,431] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.5, 18.0, 0.0, 0.0, 26.0, 27.76012808678516, 0.9772095793829733, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5081400.0000, 
sim time next is 5082000.0000, 
raw observation next is [10.33333333333333, 18.33333333333333, 0.0, 0.0, 26.0, 27.66467744440866, 0.9606290719494811, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.7488457987072946, 0.1833333333333333, 0.0, 0.0, 0.6666666666666666, 0.8053897870340551, 0.820209690649827, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0227604], dtype=float32), 0.5240218]. 
=============================================
[2019-04-04 01:02:58,582] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[85.39237 ]
 [83.58831 ]
 [86.75858 ]
 [86.764465]
 [86.54623 ]], R is [[83.44934082]
 [83.61484528]
 [83.77869415]
 [83.94091034]
 [84.10150146]].
[2019-04-04 01:03:01,357] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:03:01,357] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:03:01,369] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run17
[2019-04-04 01:03:04,109] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:03:04,109] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:03:04,124] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run17
[2019-04-04 01:03:04,825] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:03:04,825] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:03:04,828] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run17
[2019-04-04 01:03:06,780] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:03:06,780] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:03:06,795] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run17
[2019-04-04 01:03:11,722] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:03:11,723] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:03:11,726] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run17
[2019-04-04 01:03:15,317] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:03:15,317] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:03:15,320] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run17
[2019-04-04 01:03:19,441] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:03:19,441] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:03:19,469] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run17
[2019-04-04 01:03:19,981] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:03:19,981] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:03:19,984] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run17
[2019-04-04 01:03:20,641] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:03:20,641] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:03:20,645] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run17
[2019-04-04 01:03:29,718] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.1408953e-20 1.2065559e-13 1.0877264e-18 1.2353635e-14 9.2438374e-15
 5.3799496e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:03:29,719] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2622
[2019-04-04 01:03:29,778] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.3, 49.33333333333334, 55.5, 890.0, 26.0, 25.66773353306255, 0.4391123930782942, 1.0, 1.0, 67477.09107082966], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 393600.0000, 
sim time next is 394200.0000, 
raw observation next is [-11.1, 48.5, 55.0, 887.0, 26.0, 25.91599278457692, 0.4737028192429443, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.1551246537396122, 0.485, 0.18333333333333332, 0.980110497237569, 0.6666666666666666, 0.6596660653814098, 0.6579009397476481, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9052755], dtype=float32), -1.0985416]. 
=============================================
[2019-04-04 01:03:47,165] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.7783442e-21 8.4904002e-13 4.4758195e-19 1.4857842e-15 1.8440939e-15
 1.4022190e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 01:03:47,166] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9879
[2019-04-04 01:03:47,241] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.816666666666666, 78.0, 22.66666666666667, 203.3333333333333, 26.0, 24.75431909910569, 0.154670295104421, 1.0, 1.0, 50896.79900477489], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 202200.0000, 
sim time next is 202800.0000, 
raw observation next is [-8.733333333333334, 78.0, 28.33333333333334, 249.6666666666667, 26.0, 24.86640509812969, 0.1958899502138549, 1.0, 1.0, 9406.838600260175], 
processed observation next is [1.0, 0.34782608695652173, 0.22068328716528163, 0.78, 0.09444444444444447, 0.27587476979742176, 0.6666666666666666, 0.5722004248441408, 0.5652966500712849, 1.0, 1.0, 0.04479446952504845], 
reward next is 0.9552, 
noisyNet noise sample is [array([-0.9695174], dtype=float32), -2.3484716]. 
=============================================
[2019-04-04 01:03:52,922] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2278807e-19 4.8647605e-12 3.0426677e-17 7.2750027e-14 1.1150499e-13
 9.9914050e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 01:03:52,922] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9699
[2019-04-04 01:03:53,004] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.87906889299419, -0.213399903010743, 0.0, 1.0, 44857.09711590107], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 192600.0000, 
sim time next is 193200.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.8585127078933, -0.2261886497509884, 0.0, 1.0, 44902.32238924186], 
processed observation next is [1.0, 0.21739130434782608, 0.21606648199445982, 0.78, 0.0, 0.0, 0.6666666666666666, 0.4048760589911084, 0.4246037834163372, 0.0, 1.0, 0.21382058280591362], 
reward next is 0.7862, 
noisyNet noise sample is [array([-0.74422574], dtype=float32), -1.3827478]. 
=============================================
[2019-04-04 01:03:59,918] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [8.8638138e-21 5.3153662e-13 1.4844335e-18 6.2517350e-15 1.1973919e-14
 5.1277734e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 01:03:59,918] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.5079
[2019-04-04 01:04:00,053] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-16.53333333333333, 80.0, 0.0, 0.0, 26.0, 22.68914621317499, -0.2505289343859654, 1.0, 1.0, 169295.3726917159], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 373200.0000, 
sim time next is 373800.0000, 
raw observation next is [-16.61666666666667, 80.5, 8.333333333333332, 192.3333333333333, 26.0, 23.0292743874449, -0.1737174884420432, 1.0, 1.0, 117085.9982290762], 
processed observation next is [1.0, 0.30434782608695654, 0.0023084025854107648, 0.805, 0.027777777777777773, 0.21252302025782682, 0.6666666666666666, 0.4191061989537417, 0.44209417051931893, 1.0, 1.0, 0.5575523725194105], 
reward next is 0.4424, 
noisyNet noise sample is [array([0.24007386], dtype=float32), -0.77509046]. 
=============================================
[2019-04-04 01:04:08,702] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.4808480e-22 7.1587864e-14 2.2191850e-19 3.1862722e-15 1.9176275e-15
 8.3436981e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:04:08,702] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3945
[2019-04-04 01:04:08,818] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.55, 96.5, 0.0, 0.0, 26.0, 24.83403332451908, 0.2344556379286678, 0.0, 1.0, 40012.19174868377], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 516600.0000, 
sim time next is 517200.0000, 
raw observation next is [3.633333333333333, 96.66666666666666, 0.0, 0.0, 26.0, 24.84276599367649, 0.2405629281880052, 0.0, 1.0, 39923.88670582083], 
processed observation next is [1.0, 1.0, 0.5632502308402586, 0.9666666666666666, 0.0, 0.0, 0.6666666666666666, 0.5702304994730408, 0.5801876427293351, 0.0, 1.0, 0.1901137462181944], 
reward next is 0.8099, 
noisyNet noise sample is [array([-0.7497318], dtype=float32), -0.8075667]. 
=============================================
[2019-04-04 01:04:15,784] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.3360573e-21 1.2622356e-13 2.2139171e-19 1.3477592e-14 1.6986127e-15
 1.5953168e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:04:15,784] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5372
[2019-04-04 01:04:15,843] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 37.66666666666667, 26.33333333333333, 504.3333333333333, 26.0, 25.48937205171736, 0.390125224903618, 1.0, 1.0, 188965.6461299974], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 403800.0000, 
sim time next is 404400.0000, 
raw observation next is [-8.900000000000002, 37.33333333333334, 23.66666666666666, 453.6666666666667, 26.0, 25.60099869448844, 0.448602468048809, 1.0, 1.0, 116910.4217151053], 
processed observation next is [1.0, 0.6956521739130435, 0.2160664819944598, 0.3733333333333334, 0.07888888888888887, 0.5012891344383057, 0.6666666666666666, 0.6334165578740366, 0.6495341560162696, 1.0, 1.0, 0.5567162938814538], 
reward next is 0.4433, 
noisyNet noise sample is [array([-1.5902708], dtype=float32), 0.74236816]. 
=============================================
[2019-04-04 01:04:29,754] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.7328099e-20 6.0299760e-12 6.5599298e-18 2.2665109e-14 3.4591030e-14
 2.5961237e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 01:04:29,755] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8287
[2019-04-04 01:04:29,805] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.95, 43.5, 0.0, 0.0, 26.0, 22.46722476407589, -0.3177993102963966, 0.0, 1.0, 46761.43961469897], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 455400.0000, 
sim time next is 456000.0000, 
raw observation next is [-8.766666666666666, 43.33333333333333, 0.0, 0.0, 26.0, 22.57152653193335, -0.3209873624956726, 0.0, 1.0, 46407.7108348789], 
processed observation next is [1.0, 0.2608695652173913, 0.2197599261311173, 0.4333333333333333, 0.0, 0.0, 0.6666666666666666, 0.38096054432777926, 0.3930042125014424, 0.0, 1.0, 0.22098909921370904], 
reward next is 0.7790, 
noisyNet noise sample is [array([-0.14003748], dtype=float32), 1.0831006]. 
=============================================
[2019-04-04 01:04:29,816] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[80.62608 ]
 [80.45748 ]
 [80.306725]
 [80.18957 ]
 [80.09083 ]], R is [[80.78131104]
 [80.75082397]
 [80.72198486]
 [80.69306946]
 [80.66409302]].
[2019-04-04 01:04:34,725] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.2966512e-22 1.2632417e-14 1.9620272e-20 6.5902356e-16 6.3317502e-16
 3.8456227e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:04:34,726] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5325
[2019-04-04 01:04:34,814] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.483333333333333, 83.5, 0.0, 0.0, 26.0, 24.79364615788631, 0.2901229035576882, 1.0, 1.0, 135502.0819221461], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 849000.0000, 
sim time next is 849600.0000, 
raw observation next is [-3.4, 83.0, 0.0, 0.0, 26.0, 24.88515099502428, 0.3042608046724737, 0.0, 1.0, 33446.91716482838], 
processed observation next is [1.0, 0.8695652173913043, 0.368421052631579, 0.83, 0.0, 0.0, 0.6666666666666666, 0.57376258291869, 0.601420268224158, 0.0, 1.0, 0.15927103411823038], 
reward next is 0.8407, 
noisyNet noise sample is [array([0.04777334], dtype=float32), 0.90321326]. 
=============================================
[2019-04-04 01:04:40,302] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.4328390e-19 3.7251920e-12 6.0899452e-18 2.1652417e-14 1.4259584e-13
 2.6647442e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 01:04:40,302] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2878
[2019-04-04 01:04:40,457] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.7, 54.5, 45.66666666666666, 22.66666666666666, 26.0, 24.89981641480934, 0.2202678779812681, 0.0, 1.0, 31231.28690816047], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 663000.0000, 
sim time next is 663600.0000, 
raw observation next is [-0.8, 55.00000000000001, 36.33333333333333, 18.83333333333333, 26.0, 24.8986967784748, 0.2153159390080196, 0.0, 1.0, 37991.63021781467], 
processed observation next is [0.0, 0.6956521739130435, 0.4404432132963989, 0.55, 0.1211111111111111, 0.020810313075506442, 0.6666666666666666, 0.5748913982062334, 0.5717719796693399, 0.0, 1.0, 0.18091252484673653], 
reward next is 0.8191, 
noisyNet noise sample is [array([-1.4656926], dtype=float32), -0.9685769]. 
=============================================
[2019-04-04 01:04:54,961] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.6074856e-21 6.3823702e-13 3.7099480e-19 1.2619413e-14 5.3681988e-15
 2.1181336e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 01:04:54,962] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8096
[2019-04-04 01:04:55,002] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 80.0, 0.0, 0.0, 26.0, 24.72534146979959, 0.2306545002519585, 0.0, 1.0, 39648.97664565075], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 866400.0000, 
sim time next is 867000.0000, 
raw observation next is [-2.3, 80.0, 0.0, 0.0, 26.0, 24.78194655720925, 0.2267974490107881, 0.0, 1.0, 39550.80090927865], 
processed observation next is [1.0, 0.0, 0.3988919667590028, 0.8, 0.0, 0.0, 0.6666666666666666, 0.5651622131007707, 0.5755991496702627, 0.0, 1.0, 0.1883371471870412], 
reward next is 0.8117, 
noisyNet noise sample is [array([-1.763365], dtype=float32), -0.012868112]. 
=============================================
[2019-04-04 01:04:55,047] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[84.69521 ]
 [84.605415]
 [84.539825]
 [84.46314 ]
 [84.40011 ]], R is [[84.70436096]
 [84.66851044]
 [84.63237762]
 [84.59599304]
 [84.55936432]].
[2019-04-04 01:05:06,478] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-04 01:05:06,489] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 01:05:06,489] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:05:06,492] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run23
[2019-04-04 01:05:06,541] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 01:05:06,551] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:05:06,553] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run23
[2019-04-04 01:05:06,570] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 01:05:06,571] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:05:06,574] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run23
[2019-04-04 01:06:14,728] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.35256022], dtype=float32), 0.19758838]
[2019-04-04 01:06:14,729] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [16.73218611, 84.96871866666667, 0.0, 0.0, 26.0, 25.12629091598237, 0.4701971584329625, 0.0, 0.0, 0.0]
[2019-04-04 01:06:14,729] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 01:06:14,729] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.1941317e-21 2.8269246e-13 7.6883090e-20 6.1769412e-16 1.1355760e-15
 6.5114531e-21 1.0000000e+00], sampled 0.606874511220158
[2019-04-04 01:06:46,476] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.35256022], dtype=float32), 0.19758838]
[2019-04-04 01:06:46,476] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-8.483333333333334, 87.66666666666667, 0.0, 0.0, 26.0, 24.00926770653047, 0.05559269467764027, 0.0, 1.0, 43598.73583005442]
[2019-04-04 01:06:46,476] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 01:06:46,477] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [7.2005174e-20 1.3129093e-11 6.7849415e-18 1.7424542e-14 4.0606339e-14
 7.0599576e-19 1.0000000e+00], sampled 0.23719095292429815
[2019-04-04 01:06:55,509] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.35256022], dtype=float32), 0.19758838]
[2019-04-04 01:06:55,509] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [1.011628713, 22.3497551, 0.0, 0.0, 26.0, 24.80508228191353, 0.2239434741284441, 0.0, 1.0, 127044.1806054284]
[2019-04-04 01:06:55,509] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 01:06:55,510] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [4.8168401e-20 1.5585586e-12 1.1914662e-18 6.5425460e-15 1.8230904e-14
 6.2382117e-20 1.0000000e+00], sampled 0.9316588003371186
[2019-04-04 01:07:28,998] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 01:07:37,037] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.35256022], dtype=float32), 0.19758838]
[2019-04-04 01:07:37,037] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [1.268697767, 32.14264637, 0.0, 0.0, 26.0, 25.82993570219698, 0.5394550308345735, 0.0, 1.0, 0.0]
[2019-04-04 01:07:37,037] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 01:07:37,038] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.8704009e-21 3.7615966e-13 2.0544786e-19 1.4692811e-15 4.4293237e-15
 8.7771044e-21 1.0000000e+00], sampled 0.3432519228998977
[2019-04-04 01:07:50,129] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 01:07:54,392] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 01:07:55,457] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 2200000, evaluation results [2200000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 01:08:04,093] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.4563339e-23 2.1046240e-15 7.3972686e-21 3.2130252e-16 6.1128277e-17
 1.5809399e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:08:04,093] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8806
[2019-04-04 01:08:04,139] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.0, 100.0, 0.0, 0.0, 26.0, 24.67225078103023, 0.3103696567001642, 0.0, 1.0, 168195.256806504], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 937200.0000, 
sim time next is 937800.0000, 
raw observation next is [5.0, 100.0, 0.0, 0.0, 26.0, 24.70086450921662, 0.336654431689496, 0.0, 1.0, 103112.1873685872], 
processed observation next is [1.0, 0.8695652173913043, 0.6011080332409973, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5584053757680515, 0.6122181438964986, 0.0, 1.0, 0.4910104160408914], 
reward next is 0.5090, 
noisyNet noise sample is [array([-1.3051767], dtype=float32), 1.6564623]. 
=============================================
[2019-04-04 01:08:09,465] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.8843509e-23 1.5954685e-14 2.6209693e-20 4.0151977e-16 4.5519852e-16
 5.1827924e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:08:09,467] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2017
[2019-04-04 01:08:09,487] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.0, 96.0, 0.0, 0.0, 26.0, 25.18794576628664, 0.4001979571462781, 0.0, 1.0, 39583.33075645589], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 943800.0000, 
sim time next is 944400.0000, 
raw observation next is [5.0, 96.0, 0.0, 0.0, 26.0, 25.1984416441627, 0.4019166307177366, 0.0, 1.0, 39412.4544095686], 
processed observation next is [1.0, 0.9565217391304348, 0.6011080332409973, 0.96, 0.0, 0.0, 0.6666666666666666, 0.5998701370135583, 0.6339722102392455, 0.0, 1.0, 0.18767835433127905], 
reward next is 0.8123, 
noisyNet noise sample is [array([0.94341755], dtype=float32), 0.4975954]. 
=============================================
[2019-04-04 01:08:10,451] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.3202194e-22 2.8129038e-16 4.3952100e-21 1.4014785e-16 4.1307851e-17
 1.0607816e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:08:10,455] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7900
[2019-04-04 01:08:10,460] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.5, 44.0, 0.0, 26.0, 25.70523540464143, 0.5063306262400625, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1351800.0000, 
sim time next is 1352400.0000, 
raw observation next is [1.1, 92.66666666666667, 39.66666666666667, 0.0, 26.0, 25.69508530593252, 0.5111911069466692, 1.0, 1.0, 78322.4907570533], 
processed observation next is [1.0, 0.6521739130434783, 0.49307479224376743, 0.9266666666666667, 0.13222222222222224, 0.0, 0.6666666666666666, 0.6412571088277099, 0.6703970356488899, 1.0, 1.0, 0.3729642417002538], 
reward next is 0.6270, 
noisyNet noise sample is [array([0.7331664], dtype=float32), -1.9372804]. 
=============================================
[2019-04-04 01:08:19,651] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.8805315e-21 2.5345465e-13 7.9778203e-20 2.5811980e-15 5.4350420e-15
 1.0464144e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 01:08:19,656] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3819
[2019-04-04 01:08:19,670] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 26.0, 25.41131299355769, 0.4829914861237265, 0.0, 1.0, 59416.58246164225], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1386000.0000, 
sim time next is 1386600.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 26.0, 25.30779517332659, 0.4852116922155083, 0.0, 1.0, 72318.18988285243], 
processed observation next is [1.0, 0.043478260869565216, 0.46260387811634357, 0.95, 0.0, 0.0, 0.6666666666666666, 0.6089829311105491, 0.6617372307385028, 0.0, 1.0, 0.34437233277548773], 
reward next is 0.6556, 
noisyNet noise sample is [array([-0.15051755], dtype=float32), -0.90405166]. 
=============================================
[2019-04-04 01:08:20,101] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0870777e-21 1.2292818e-12 3.3492429e-19 8.1545731e-16 1.5185545e-15
 1.2841495e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 01:08:20,104] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1955
[2019-04-04 01:08:20,132] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.1, 92.0, 0.0, 0.0, 26.0, 25.4323113371299, 0.5768327516640165, 0.0, 1.0, 18763.94620164015], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1304400.0000, 
sim time next is 1305000.0000, 
raw observation next is [3.0, 92.0, 0.0, 0.0, 26.0, 25.47429166864557, 0.5771277229161814, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.5457063711911359, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6228576390537975, 0.6923759076387271, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7657557], dtype=float32), 0.85576123]. 
=============================================
[2019-04-04 01:08:20,179] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[86.75486]
 [86.76792]
 [86.67375]
 [86.50426]
 [86.33997]], R is [[86.8341217 ]
 [86.8764267 ]
 [86.79325104]
 [86.66134644]
 [86.40905762]].
[2019-04-04 01:08:21,612] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.4275626e-23 1.8408205e-13 3.7689157e-20 2.2986820e-16 3.5786231e-16
 6.5017694e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:08:21,612] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9245
[2019-04-04 01:08:21,657] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 100.0, 32.0, 0.0, 26.0, 25.90604498365613, 0.5150207673105102, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1414800.0000, 
sim time next is 1415400.0000, 
raw observation next is [-0.5, 99.16666666666667, 36.66666666666667, 0.0, 26.0, 25.96053950577738, 0.5070920453760568, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.44875346260387816, 0.9916666666666667, 0.12222222222222223, 0.0, 0.6666666666666666, 0.663378292148115, 0.669030681792019, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.63202316], dtype=float32), -0.11303418]. 
=============================================
[2019-04-04 01:08:23,095] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0629452e-21 6.6018920e-14 1.8887003e-19 9.2688677e-16 6.0333458e-16
 4.0269374e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:08:23,096] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0570
[2019-04-04 01:08:23,146] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.45273500032571, 0.4729432189747944, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1409400.0000, 
sim time next is 1410000.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.56989485602632, 0.4634661032222374, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.44598337950138506, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6308245713355266, 0.6544887010740791, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.10563476], dtype=float32), -0.87300634]. 
=============================================
[2019-04-04 01:08:23,175] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[86.12036 ]
 [87.78612 ]
 [84.904236]
 [84.87875 ]
 [84.85558 ]], R is [[88.39259338]
 [88.50866699]
 [88.62358093]
 [88.55336761]
 [88.48387146]].
[2019-04-04 01:08:24,490] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.9254638e-22 2.4535449e-14 9.7565574e-21 4.7044422e-16 1.9109447e-16
 3.8445372e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:08:24,495] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4390
[2019-04-04 01:08:24,529] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.783333333333333, 74.66666666666667, 89.0, 102.3333333333333, 26.0, 26.1596312801943, 0.6083159642414445, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1588200.0000, 
sim time next is 1588800.0000, 
raw observation next is [6.966666666666667, 73.33333333333334, 102.0, 119.1666666666667, 26.0, 26.21822615641518, 0.6312192144959778, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.6555863342566944, 0.7333333333333334, 0.34, 0.13167587476979745, 0.6666666666666666, 0.6848521797012651, 0.7104064048319927, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.24385308], dtype=float32), 0.98249626]. 
=============================================
[2019-04-04 01:08:27,209] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.7158469e-22 3.0232541e-13 8.7434910e-20 5.0677341e-16 2.9702494e-15
 6.5440205e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:08:27,218] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0985
[2019-04-04 01:08:27,241] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.433333333333333, 92.0, 0.0, 0.0, 26.0, 25.3106081505866, 0.4834933235260699, 0.0, 1.0, 60988.42207490419], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1464000.0000, 
sim time next is 1464600.0000, 
raw observation next is [1.516666666666667, 92.0, 0.0, 0.0, 26.0, 25.31672357982757, 0.4875627932713815, 0.0, 1.0, 46622.74766997824], 
processed observation next is [1.0, 0.9565217391304348, 0.5046168051708219, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6097269649856308, 0.6625209310904605, 0.0, 1.0, 0.22201308414275353], 
reward next is 0.7780, 
noisyNet noise sample is [array([-0.29836845], dtype=float32), -2.281158]. 
=============================================
[2019-04-04 01:08:31,881] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.0348750e-19 5.5562291e-12 4.2510776e-18 4.9040890e-14 3.6034350e-14
 2.9585619e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 01:08:31,882] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5223
[2019-04-04 01:08:31,906] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.5, 78.16666666666666, 0.0, 0.0, 26.0, 24.37724949317457, 0.1595942682828584, 0.0, 1.0, 45854.25793643053], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1817400.0000, 
sim time next is 1818000.0000, 
raw observation next is [-5.6, 78.0, 0.0, 0.0, 26.0, 24.36013967770334, 0.1528062961196614, 0.0, 1.0, 45927.68327435266], 
processed observation next is [0.0, 0.043478260869565216, 0.30747922437673136, 0.78, 0.0, 0.0, 0.6666666666666666, 0.5300116398086118, 0.5509354320398872, 0.0, 1.0, 0.21870325368739363], 
reward next is 0.7813, 
noisyNet noise sample is [array([0.16536258], dtype=float32), 1.4302697]. 
=============================================
[2019-04-04 01:08:31,926] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[79.72816]
 [79.77394]
 [79.77684]
 [79.77119]
 [79.71002]], R is [[79.66926575]
 [79.65422058]
 [79.63967896]
 [79.62561035]
 [79.61198425]].
[2019-04-04 01:08:39,235] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.5900569e-19 1.5511127e-11 3.5090011e-18 4.6866780e-14 4.3528278e-14
 2.8643659e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 01:08:39,235] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1610
[2019-04-04 01:08:39,260] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.666666666666667, 84.0, 0.0, 0.0, 26.0, 25.0880739883594, 0.3184138557922362, 0.0, 1.0, 46589.48388229196], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1801200.0000, 
sim time next is 1801800.0000, 
raw observation next is [-4.75, 84.5, 0.0, 0.0, 26.0, 25.0602835331803, 0.3128778378271226, 0.0, 1.0, 46167.18341318843], 
processed observation next is [0.0, 0.8695652173913043, 0.3310249307479225, 0.845, 0.0, 0.0, 0.6666666666666666, 0.5883569610983583, 0.6042926126090409, 0.0, 1.0, 0.21984373053899253], 
reward next is 0.7802, 
noisyNet noise sample is [array([1.4175811], dtype=float32), 1.5675358]. 
=============================================
[2019-04-04 01:08:40,464] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.1445451e-20 3.2282046e-12 4.5749945e-18 4.3862792e-15 4.0846290e-14
 4.6568511e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 01:08:40,465] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.9946
[2019-04-04 01:08:40,483] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.09999999999999999, 90.33333333333334, 0.0, 0.0, 26.0, 25.21365418309197, 0.4197741953802738, 0.0, 1.0, 43043.18319270705], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1739400.0000, 
sim time next is 1740000.0000, 
raw observation next is [-0.2, 89.66666666666667, 0.0, 0.0, 26.0, 25.19614758419054, 0.4151683833572427, 0.0, 1.0, 43074.70186700125], 
processed observation next is [0.0, 0.13043478260869565, 0.4570637119113574, 0.8966666666666667, 0.0, 0.0, 0.6666666666666666, 0.5996789653492117, 0.6383894611190809, 0.0, 1.0, 0.20511762793810118], 
reward next is 0.7949, 
noisyNet noise sample is [array([-0.7028734], dtype=float32), -0.18053854]. 
=============================================
[2019-04-04 01:08:40,487] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[79.58557 ]
 [79.637436]
 [79.74079 ]
 [79.84183 ]
 [79.944725]], R is [[79.50392914]
 [79.50392151]
 [79.50406647]
 [79.50436401]
 [79.504776  ]].
[2019-04-04 01:08:43,441] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.8713818e-19 2.0484287e-12 7.1154415e-18 5.4367867e-14 4.4512103e-14
 5.3350952e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 01:08:43,441] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6079
[2019-04-04 01:08:43,489] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 71.0, 153.3333333333333, 26.66666666666667, 26.0, 24.97739660052914, 0.2679249419022362, 0.0, 1.0, 37076.61414555725], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1861800.0000, 
sim time next is 1862400.0000, 
raw observation next is [-4.5, 71.0, 161.6666666666667, 33.33333333333334, 26.0, 24.98777258073243, 0.2702160698139288, 0.0, 1.0, 36466.86359836875], 
processed observation next is [0.0, 0.5652173913043478, 0.3379501385041552, 0.71, 0.5388888888888891, 0.03683241252302027, 0.6666666666666666, 0.5823143817277024, 0.5900720232713096, 0.0, 1.0, 0.17365173142080356], 
reward next is 0.8263, 
noisyNet noise sample is [array([-1.5455632], dtype=float32), 0.39412516]. 
=============================================
[2019-04-04 01:08:58,704] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.3757001e-22 4.4919265e-14 7.1291278e-20 4.7896770e-16 7.3534816e-16
 6.1719760e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:08:58,704] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7935
[2019-04-04 01:08:58,756] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 77.0, 156.0, 0.0, 26.0, 25.53486147436868, 0.3023591702177633, 1.0, 1.0, 52571.68456474561], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2032200.0000, 
sim time next is 2032800.0000, 
raw observation next is [-4.5, 77.66666666666667, 154.6666666666667, 0.0, 26.0, 25.35074112643948, 0.2998232188013544, 1.0, 1.0, 43344.24290241934], 
processed observation next is [1.0, 0.5217391304347826, 0.3379501385041552, 0.7766666666666667, 0.5155555555555558, 0.0, 0.6666666666666666, 0.6125617605366234, 0.5999410729337847, 1.0, 1.0, 0.20640115667818734], 
reward next is 0.7936, 
noisyNet noise sample is [array([0.19736025], dtype=float32), -1.4656192]. 
=============================================
[2019-04-04 01:09:08,723] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6954669e-22 4.5737909e-14 7.6523189e-20 4.8605959e-16 1.2350001e-15
 1.8796565e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:09:08,723] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9220
[2019-04-04 01:09:08,787] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 82.00000000000001, 0.0, 0.0, 26.0, 26.14878463318504, 0.5500761057371049, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2052600.0000, 
sim time next is 2053200.0000, 
raw observation next is [-3.9, 82.0, 0.0, 0.0, 26.0, 26.31075565217404, 0.5353325302774522, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.3545706371191136, 0.82, 0.0, 0.0, 0.6666666666666666, 0.6925629710145035, 0.6784441767591507, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.346096], dtype=float32), 0.07148687]. 
=============================================
[2019-04-04 01:09:09,550] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.6318975e-22 4.5397620e-15 2.8692750e-20 6.4941794e-16 1.1895736e-16
 5.2670811e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:09:09,550] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.6035
[2019-04-04 01:09:09,597] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.483333333333333, 66.0, 138.6666666666667, 0.0, 26.0, 25.32819736905734, 0.2938113233951226, 1.0, 1.0, 28353.55562079027], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2207400.0000, 
sim time next is 2208000.0000, 
raw observation next is [-3.566666666666667, 67.0, 141.3333333333333, 0.0, 26.0, 25.17812822154766, 0.3031984220391954, 1.0, 1.0, 61531.18889550156], 
processed observation next is [1.0, 0.5652173913043478, 0.3638042474607572, 0.67, 0.471111111111111, 0.0, 0.6666666666666666, 0.5981773517956382, 0.6010661406797319, 1.0, 1.0, 0.2930056614071503], 
reward next is 0.7070, 
noisyNet noise sample is [array([-0.12651528], dtype=float32), -0.57618797]. 
=============================================
[2019-04-04 01:09:09,728] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[85.89566 ]
 [85.95543 ]
 [85.947655]
 [85.908775]
 [85.99361 ]], R is [[85.75130463]
 [85.7587738 ]
 [85.78656006]
 [85.81248474]
 [85.83796692]].
[2019-04-04 01:09:11,932] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.8018057e-22 4.6357178e-14 5.2456518e-20 2.8258313e-16 5.8638476e-16
 8.4373139e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:09:11,932] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7482
[2019-04-04 01:09:12,032] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.2, 52.33333333333334, 0.0, 0.0, 26.0, 25.59389466084579, 0.418846898110051, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2311800.0000, 
sim time next is 2312400.0000, 
raw observation next is [-1.2, 52.66666666666667, 0.0, 0.0, 26.0, 25.61565647182639, 0.3922011159591281, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.42936288088642666, 0.5266666666666667, 0.0, 0.0, 0.6666666666666666, 0.6346380393188659, 0.6307337053197094, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8520871], dtype=float32), -0.627037]. 
=============================================
[2019-04-04 01:09:13,365] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.3545379e-22 4.1074884e-14 2.3932754e-20 1.2852162e-15 2.6650534e-16
 9.6618482e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:09:13,369] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8153
[2019-04-04 01:09:13,467] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.883333333333333, 75.66666666666666, 191.3333333333333, 160.6666666666667, 26.0, 25.7467814848031, 0.3170447190843111, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1939800.0000, 
sim time next is 1940400.0000, 
raw observation next is [-5.6, 75.0, 201.5, 123.0, 26.0, 25.71021300561715, 0.3177933869733382, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.30747922437673136, 0.75, 0.6716666666666666, 0.13591160220994475, 0.6666666666666666, 0.6425177504680958, 0.6059311289911128, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3603655], dtype=float32), -0.28705466]. 
=============================================
[2019-04-04 01:09:20,772] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.1865767e-22 6.3938933e-14 9.5633285e-20 1.0519643e-15 1.0850012e-15
 9.5456784e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:09:20,792] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2352
[2019-04-04 01:09:20,870] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.1, 68.0, 112.0, 0.0, 26.0, 26.25935664052702, 0.5102294774850981, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2127000.0000, 
sim time next is 2127600.0000, 
raw observation next is [-5.0, 68.0, 105.5, 0.0, 26.0, 26.29070465045418, 0.5085977847540678, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.32409972299168976, 0.68, 0.3516666666666667, 0.0, 0.6666666666666666, 0.690892054204515, 0.6695325949180226, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.01163973], dtype=float32), -0.7066]. 
=============================================
[2019-04-04 01:09:28,418] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.5404339e-21 1.6323892e-13 5.5592307e-19 1.1894885e-14 4.0165019e-15
 3.3164890e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:09:28,420] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5203
[2019-04-04 01:09:28,490] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 71.0, 19.0, 0.0, 26.0, 25.64419987231317, 0.400212119657408, 1.0, 1.0, 44421.56753954262], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2221200.0000, 
sim time next is 2221800.0000, 
raw observation next is [-4.5, 70.5, 13.66666666666666, 0.0, 26.0, 25.68560474353259, 0.3129328095002157, 1.0, 1.0, 36275.1717695824], 
processed observation next is [1.0, 0.7391304347826086, 0.3379501385041552, 0.705, 0.04555555555555554, 0.0, 0.6666666666666666, 0.6404670619610492, 0.604310936500072, 1.0, 1.0, 0.1727389131884876], 
reward next is 0.8273, 
noisyNet noise sample is [array([-0.88110125], dtype=float32), -0.7129161]. 
=============================================
[2019-04-04 01:09:32,483] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0939802e-19 5.7892016e-12 4.2516150e-17 4.3250044e-14 2.7148531e-14
 8.1219014e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 01:09:32,483] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1613
[2019-04-04 01:09:32,502] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.4, 87.0, 0.0, 0.0, 26.0, 24.05992721602802, 0.06347761018118757, 0.0, 1.0, 43610.77796549167], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2260800.0000, 
sim time next is 2261400.0000, 
raw observation next is [-8.483333333333334, 87.66666666666667, 0.0, 0.0, 26.0, 24.00926770653047, 0.05559269467764027, 0.0, 1.0, 43598.73583005442], 
processed observation next is [1.0, 0.17391304347826086, 0.2276084949215143, 0.8766666666666667, 0.0, 0.0, 0.6666666666666666, 0.5007723088775391, 0.51853089822588, 0.0, 1.0, 0.20761302776216392], 
reward next is 0.7924, 
noisyNet noise sample is [array([-0.7027793], dtype=float32), -0.4555121]. 
=============================================
[2019-04-04 01:09:34,748] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.4208700e-21 9.2190546e-13 4.6242788e-19 2.7623212e-15 4.5872691e-15
 9.6296934e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:09:34,748] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8245
[2019-04-04 01:09:34,766] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.283333333333333, 75.5, 0.0, 0.0, 26.0, 24.72898716201501, 0.2551157193383058, 0.0, 1.0, 44154.56163385813], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2243400.0000, 
sim time next is 2244000.0000, 
raw observation next is [-6.366666666666667, 76.0, 0.0, 0.0, 26.0, 24.67120873571065, 0.2435578951541035, 0.0, 1.0, 44124.53194704583], 
processed observation next is [1.0, 1.0, 0.28624192059095105, 0.76, 0.0, 0.0, 0.6666666666666666, 0.5559340613092209, 0.5811859650513679, 0.0, 1.0, 0.21011681879545632], 
reward next is 0.7899, 
noisyNet noise sample is [array([-0.95752907], dtype=float32), 0.09796007]. 
=============================================
[2019-04-04 01:09:34,770] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[78.78868]
 [78.89642]
 [78.74772]
 [78.6411 ]
 [78.57797]], R is [[78.91316986]
 [78.91378021]
 [78.91414642]
 [78.91430664]
 [78.91442871]].
[2019-04-04 01:09:58,138] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.4749878e-21 9.8858866e-13 6.3694841e-19 3.7384684e-15 8.7966735e-15
 1.2846346e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 01:09:58,138] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6878
[2019-04-04 01:09:58,207] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.80220018272265, 0.2804169753503105, 0.0, 1.0, 44414.6243259827], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2763600.0000, 
sim time next is 2764200.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.76854295212603, 0.2719883519393703, 0.0, 1.0, 44134.15580724889], 
processed observation next is [1.0, 1.0, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5640452460105024, 0.5906627839797901, 0.0, 1.0, 0.2101626467011852], 
reward next is 0.7898, 
noisyNet noise sample is [array([2.0791812], dtype=float32), -0.6694498]. 
=============================================
[2019-04-04 01:10:01,859] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.6827952e-19 5.7979283e-13 1.2742741e-18 9.0465634e-15 1.5560175e-14
 6.6635828e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 01:10:01,859] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.7404
[2019-04-04 01:10:01,878] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.3, 25.0, 27.0, 161.0, 26.0, 25.05024041960854, 0.2351474450329766, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2480400.0000, 
sim time next is 2481000.0000, 
raw observation next is [2.933333333333334, 25.5, 20.0, 121.6666666666667, 26.0, 24.98811310147967, 0.2170247160326375, 0.0, 1.0, 31150.43560682388], 
processed observation next is [0.0, 0.7391304347826086, 0.543859649122807, 0.255, 0.06666666666666667, 0.134438305709024, 0.6666666666666666, 0.5823427584566391, 0.5723415720108792, 0.0, 1.0, 0.1483354076515423], 
reward next is 0.8517, 
noisyNet noise sample is [array([0.0257069], dtype=float32), -0.47218764]. 
=============================================
[2019-04-04 01:10:01,883] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[79.88175 ]
 [80.228745]
 [80.50868 ]
 [80.7347  ]
 [80.99637 ]], R is [[79.55912781]
 [79.76353455]
 [79.96589661]
 [80.16623688]
 [80.27553558]].
[2019-04-04 01:10:04,227] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.9900858e-21 3.8254176e-14 5.8531080e-20 1.5686126e-15 5.9746221e-16
 1.6526761e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:10:04,227] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9238
[2019-04-04 01:10:04,256] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.666666666666666, 28.66666666666667, 16.0, 51.0, 26.0, 25.82055846524057, 0.3767408953169065, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2827200.0000, 
sim time next is 2827800.0000, 
raw observation next is [5.5, 29.0, 5.0, 46.0, 26.0, 25.73447930322649, 0.3608104894687671, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.6149584487534627, 0.29, 0.016666666666666666, 0.05082872928176796, 0.6666666666666666, 0.6445399419355408, 0.6202701631562557, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.737127], dtype=float32), -1.4882699]. 
=============================================
[2019-04-04 01:10:17,130] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.5260654e-20 2.1296025e-12 2.5875725e-18 3.1380721e-14 3.5187399e-14
 1.0768307e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 01:10:17,130] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3603
[2019-04-04 01:10:17,250] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 35.0, 0.0, 0.0, 26.0, 25.15160919658054, 0.2421111243966857, 0.0, 1.0, 40067.20841973571], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2502000.0000, 
sim time next is 2502600.0000, 
raw observation next is [-0.7833333333333333, 35.5, 0.0, 0.0, 26.0, 25.13965045471057, 0.2398398535135781, 0.0, 1.0, 39944.98434872424], 
processed observation next is [0.0, 1.0, 0.44090489381348114, 0.355, 0.0, 0.0, 0.6666666666666666, 0.5949708712258808, 0.5799466178378594, 0.0, 1.0, 0.19021421118440116], 
reward next is 0.8098, 
noisyNet noise sample is [array([-1.2782943], dtype=float32), 1.2221336]. 
=============================================
[2019-04-04 01:10:22,626] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.1118742e-18 2.2384943e-11 2.1273935e-16 9.7808887e-14 2.4751113e-13
 1.5435026e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 01:10:22,627] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.9117
[2019-04-04 01:10:22,649] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-16.0, 83.0, 0.0, 0.0, 26.0, 23.08813308392492, -0.1239352117427715, 0.0, 1.0, 43343.73676134676], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2700000.0000, 
sim time next is 2700600.0000, 
raw observation next is [-15.83333333333333, 83.0, 0.0, 0.0, 26.0, 23.00791831140102, -0.1373208116441677, 0.0, 1.0, 43285.87274400142], 
processed observation next is [1.0, 0.2608695652173913, 0.02400738688827338, 0.83, 0.0, 0.0, 0.6666666666666666, 0.41732652595008507, 0.45422639611861076, 0.0, 1.0, 0.2061232035428639], 
reward next is 0.7939, 
noisyNet noise sample is [array([-0.59289396], dtype=float32), 0.3963325]. 
=============================================
[2019-04-04 01:10:24,695] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.9236000e-21 5.3122417e-14 4.4523003e-20 1.4239620e-15 3.0060858e-16
 1.0732658e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:10:24,695] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6203
[2019-04-04 01:10:24,715] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.333333333333333, 50.83333333333334, 157.6666666666667, 593.0, 26.0, 26.0231780871031, 0.4632928843119255, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2803800.0000, 
sim time next is 2804400.0000, 
raw observation next is [-1.0, 50.0, 149.5, 635.5, 26.0, 26.04318363990414, 0.4737178041032116, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4349030470914128, 0.5, 0.49833333333333335, 0.7022099447513812, 0.6666666666666666, 0.670265303325345, 0.6579059347010706, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5443407], dtype=float32), 0.17296739]. 
=============================================
[2019-04-04 01:10:27,943] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.1224218e-19 1.1483332e-11 6.6584822e-18 3.2477605e-14 6.8403359e-14
 1.3455577e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 01:10:27,987] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5263
[2019-04-04 01:10:28,001] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.5, 65.0, 0.0, 0.0, 26.0, 25.30598284197608, 0.356281898500943, 0.0, 1.0, 39967.14912992465], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3013200.0000, 
sim time next is 3013800.0000, 
raw observation next is [-3.583333333333333, 65.0, 0.0, 0.0, 26.0, 25.29426889079347, 0.351567874765414, 0.0, 1.0, 39701.44651828431], 
processed observation next is [0.0, 0.9130434782608695, 0.36334256694367506, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6078557408994559, 0.6171892915884714, 0.0, 1.0, 0.18905450722992528], 
reward next is 0.8109, 
noisyNet noise sample is [array([0.5645803], dtype=float32), -2.244669]. 
=============================================
[2019-04-04 01:10:32,847] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0528408e-20 1.5820721e-13 7.4656245e-19 5.6630306e-15 5.5715977e-15
 6.8054088e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:10:32,847] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9034
[2019-04-04 01:10:32,892] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.2, 61.00000000000001, 0.0, 0.0, 26.0, 25.19958971886931, 0.3729138759017004, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2661600.0000, 
sim time next is 2662200.0000, 
raw observation next is [-1.2, 61.5, 0.0, 0.0, 26.0, 25.11127177081986, 0.3532388723852326, 0.0, 1.0, 25677.83914514001], 
processed observation next is [1.0, 0.8260869565217391, 0.42936288088642666, 0.615, 0.0, 0.0, 0.6666666666666666, 0.5926059809016552, 0.6177462907950776, 0.0, 1.0, 0.12227542450066671], 
reward next is 0.8777, 
noisyNet noise sample is [array([-0.07181323], dtype=float32), -0.2764439]. 
=============================================
[2019-04-04 01:10:35,968] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.9593859e-21 7.9307178e-15 1.1378806e-19 2.6046301e-15 3.7369141e-15
 1.7466272e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:10:35,969] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0135
[2019-04-04 01:10:36,014] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.666666666666666, 30.83333333333333, 205.3333333333333, 115.3333333333333, 26.0, 25.91481586539268, 0.4433567312374745, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2814600.0000, 
sim time next is 2815200.0000, 
raw observation next is [6.0, 30.0, 183.5, 86.5, 26.0, 25.62846943455152, 0.417903513866839, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.6288088642659281, 0.3, 0.6116666666666667, 0.09558011049723757, 0.6666666666666666, 0.6357057862126266, 0.6393011712889464, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.23361321], dtype=float32), 0.38707432]. 
=============================================
[2019-04-04 01:10:36,764] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.4666964e-20 1.8056989e-12 2.6513411e-18 8.9611427e-15 1.1677106e-14
 9.6557045e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 01:10:36,764] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5687
[2019-04-04 01:10:36,792] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.5, 100.0, 0.0, 0.0, 26.0, 25.36169949152286, 0.341554889954311, 0.0, 1.0, 48855.65587379775], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3105000.0000, 
sim time next is 3105600.0000, 
raw observation next is [-0.3333333333333334, 100.0, 0.0, 0.0, 26.0, 25.39693831265348, 0.3466969163485047, 0.0, 1.0, 30917.44936108638], 
processed observation next is [0.0, 0.9565217391304348, 0.4533702677747, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6164115260544566, 0.6155656387828349, 0.0, 1.0, 0.14722594933850658], 
reward next is 0.8528, 
noisyNet noise sample is [array([-1.7103894], dtype=float32), -0.5822793]. 
=============================================
[2019-04-04 01:10:42,459] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.9163877e-19 2.1349516e-11 1.4958316e-17 4.2739971e-14 2.1432522e-13
 3.2701998e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 01:10:42,460] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2158
[2019-04-04 01:10:42,477] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 77.0, 0.0, 0.0, 26.0, 23.88586607555141, 0.08692345320719057, 0.0, 1.0, 59879.26740965401], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2961600.0000, 
sim time next is 2962200.0000, 
raw observation next is [-4.0, 77.0, 0.0, 0.0, 26.0, 23.81566998378192, 0.07489316288812199, 0.0, 1.0, 60450.56888520911], 
processed observation next is [0.0, 0.2608695652173913, 0.3518005540166205, 0.77, 0.0, 0.0, 0.6666666666666666, 0.48463916531516, 0.524964387629374, 0.0, 1.0, 0.2878598518343291], 
reward next is 0.7121, 
noisyNet noise sample is [array([-0.86637306], dtype=float32), -0.44024622]. 
=============================================
[2019-04-04 01:10:55,712] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.0631254e-19 5.6969269e-12 3.4831437e-17 8.4508769e-14 1.1653883e-13
 3.8996545e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 01:10:55,712] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6789
[2019-04-04 01:10:55,837] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.08025170683843, 0.06840716411479587, 0.0, 1.0, 40045.30071062675], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3038400.0000, 
sim time next is 3039000.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.04877380767008, 0.06151533515351556, 0.0, 1.0, 40112.34901749661], 
processed observation next is [0.0, 0.17391304347826086, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5040644839725067, 0.5205051117178385, 0.0, 1.0, 0.1910111857976029], 
reward next is 0.8090, 
noisyNet noise sample is [array([-0.10427491], dtype=float32), 0.48611256]. 
=============================================
[2019-04-04 01:10:55,868] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[78.568085]
 [78.57556 ]
 [78.59094 ]
 [78.610275]
 [78.62811 ]], R is [[78.57793427]
 [78.60146332]
 [78.62516022]
 [78.64909363]
 [78.67333984]].
[2019-04-04 01:11:10,722] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0502901e-19 2.8556388e-11 1.9372863e-17 4.7495448e-14 1.3905688e-13
 2.7332194e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 01:11:10,723] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1363
[2019-04-04 01:11:10,748] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 23.90984109903956, 0.02958920391666374, 0.0, 1.0, 40198.08265195469], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3042000.0000, 
sim time next is 3042600.0000, 
raw observation next is [-6.0, 75.83333333333334, 0.0, 0.0, 26.0, 23.88464341453021, 0.02359580570904125, 0.0, 1.0, 40180.42283557937], 
processed observation next is [0.0, 0.21739130434782608, 0.296398891966759, 0.7583333333333334, 0.0, 0.0, 0.6666666666666666, 0.4903869512108508, 0.5078652685696804, 0.0, 1.0, 0.1913353468360922], 
reward next is 0.8087, 
noisyNet noise sample is [array([0.46735734], dtype=float32), 0.3383378]. 
=============================================
[2019-04-04 01:11:13,667] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.6142895e-23 7.9610594e-15 7.2382981e-21 2.9427296e-16 1.2286042e-16
 6.5852114e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 01:11:13,668] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4986
[2019-04-04 01:11:13,750] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.333333333333334, 97.66666666666667, 113.0, 795.1666666666666, 26.0, 27.11696250937133, 0.7602889307106514, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3151200.0000, 
sim time next is 3151800.0000, 
raw observation next is [7.5, 96.5, 114.0, 805.0, 26.0, 27.11265651525353, 0.7799959442472845, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.6703601108033241, 0.965, 0.38, 0.8895027624309392, 0.6666666666666666, 0.7593880429377942, 0.7599986480824281, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.88537204], dtype=float32), 0.49559242]. 
=============================================
[2019-04-04 01:11:18,799] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.8429041e-21 9.7761935e-13 4.1229567e-19 8.9537278e-15 2.3932005e-14
 2.4537715e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 01:11:18,800] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2794
[2019-04-04 01:11:18,835] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.3333333333333334, 61.33333333333334, 48.66666666666666, 419.6666666666667, 26.0, 25.07818130908914, 0.3277695817826191, 0.0, 1.0, 49360.22542125468], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3084000.0000, 
sim time next is 3084600.0000, 
raw observation next is [0.1666666666666666, 66.66666666666667, 40.33333333333334, 353.3333333333334, 26.0, 25.05455214936956, 0.3280904695440376, 0.0, 1.0, 50386.04530588872], 
processed observation next is [0.0, 0.6956521739130435, 0.4672206832871654, 0.6666666666666667, 0.13444444444444448, 0.39042357274401485, 0.6666666666666666, 0.5878793457807966, 0.6093634898480126, 0.0, 1.0, 0.23993354907566058], 
reward next is 0.7601, 
noisyNet noise sample is [array([-0.69409287], dtype=float32), 1.1204839]. 
=============================================
[2019-04-04 01:11:20,152] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.0446724e-21 5.2178200e-13 3.1776596e-19 4.5466077e-15 3.3254431e-15
 3.9339256e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 01:11:20,153] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2313
[2019-04-04 01:11:20,186] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.333333333333333, 100.0, 0.0, 0.0, 26.0, 25.3435338525623, 0.3325157327081766, 0.0, 1.0, 39111.68749458012], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3118800.0000, 
sim time next is 3119400.0000, 
raw observation next is [1.5, 100.0, 0.0, 0.0, 26.0, 25.41875754680582, 0.3466451376122295, 0.0, 1.0, 18765.85897311381], 
processed observation next is [1.0, 0.08695652173913043, 0.5041551246537397, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6182297955671517, 0.6155483792040765, 0.0, 1.0, 0.08936123320530386], 
reward next is 0.9106, 
noisyNet noise sample is [array([-0.26311022], dtype=float32), -1.1966673]. 
=============================================
[2019-04-04 01:11:23,568] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0988008e-22 1.2411262e-14 1.0498890e-20 5.5885815e-16 2.5645217e-16
 4.7952541e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 01:11:23,569] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7436
[2019-04-04 01:11:23,575] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.0, 100.0, 112.0, 806.0, 26.0, 27.39635942068344, 0.883952352391125, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3159000.0000, 
sim time next is 3159600.0000, 
raw observation next is [7.0, 100.0, 110.1666666666667, 798.8333333333334, 26.0, 27.46548719954303, 0.8999518871777435, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.6565096952908588, 1.0, 0.36722222222222234, 0.8826887661141806, 0.6666666666666666, 0.7887905999619192, 0.7999839623925812, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.66073954], dtype=float32), 1.1604719]. 
=============================================
[2019-04-04 01:11:25,513] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.4551087e-22 1.2431217e-13 3.3409489e-19 1.5502016e-15 1.7199274e-15
 7.2791684e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:11:25,513] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2880
[2019-04-04 01:11:25,544] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 84.83333333333334, 0.0, 0.0, 26.0, 25.3815881931991, 0.4622837915980054, 0.0, 1.0, 58552.1911969145], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3456600.0000, 
sim time next is 3457200.0000, 
raw observation next is [1.0, 83.66666666666667, 0.0, 0.0, 26.0, 25.41945720718427, 0.4686307181259802, 0.0, 1.0, 27458.72141847509], 
processed observation next is [1.0, 0.0, 0.4903047091412743, 0.8366666666666667, 0.0, 0.0, 0.6666666666666666, 0.6182881005986891, 0.6562102393753267, 0.0, 1.0, 0.1307558162784528], 
reward next is 0.8692, 
noisyNet noise sample is [array([2.465856], dtype=float32), -0.6259364]. 
=============================================
[2019-04-04 01:11:31,026] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.9252763e-22 9.6278491e-15 5.6704523e-20 7.2787706e-16 1.5575755e-15
 2.8853960e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:11:31,029] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5544
[2019-04-04 01:11:31,071] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 67.0, 10.0, 100.8333333333333, 26.0, 25.73569984122184, 0.52995645168803, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3433200.0000, 
sim time next is 3433800.0000, 
raw observation next is [2.0, 67.0, 0.0, 0.0, 26.0, 25.71794982184244, 0.3730156912699203, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.518005540166205, 0.67, 0.0, 0.0, 0.6666666666666666, 0.6431624851535366, 0.6243385637566401, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.31048757], dtype=float32), 0.0855948]. 
=============================================
[2019-04-04 01:11:48,864] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.6119067e-21 5.4338142e-13 5.0111674e-20 1.6869626e-16 2.8303962e-15
 2.2838087e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:11:48,864] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8883
[2019-04-04 01:11:48,972] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.5, 27.0, 93.0, 489.6666666666666, 26.0, 25.62421579810986, 0.4366900730960031, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3660600.0000, 
sim time next is 3661200.0000, 
raw observation next is [11.0, 26.0, 95.0, 533.0, 26.0, 25.63613680712733, 0.4506851522531968, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.7673130193905818, 0.26, 0.31666666666666665, 0.5889502762430939, 0.6666666666666666, 0.6363447339272774, 0.6502283840843989, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5889615], dtype=float32), -0.83997023]. 
=============================================
[2019-04-04 01:11:57,317] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.4861170e-20 1.0617951e-12 4.7492063e-19 1.8891005e-15 2.4191303e-14
 8.1263910e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 01:11:57,317] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7142
[2019-04-04 01:11:57,356] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.833333333333333, 48.66666666666667, 0.0, 0.0, 26.0, 25.3904142911624, 0.3934664265723167, 0.0, 1.0, 49351.44854668745], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3621000.0000, 
sim time next is 3621600.0000, 
raw observation next is [-2.0, 50.0, 0.0, 0.0, 26.0, 25.41382175424405, 0.3905463998134928, 0.0, 1.0, 29694.06079499436], 
processed observation next is [0.0, 0.9565217391304348, 0.40720221606648205, 0.5, 0.0, 0.0, 0.6666666666666666, 0.6178184795203375, 0.6301821332711642, 0.0, 1.0, 0.14140028949997313], 
reward next is 0.8586, 
noisyNet noise sample is [array([1.5600281], dtype=float32), 0.65725905]. 
=============================================
[2019-04-04 01:12:18,198] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.2195362e-20 1.6822203e-12 2.1200930e-17 2.2588807e-14 4.8034415e-14
 2.8900269e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 01:12:18,198] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7070
[2019-04-04 01:12:18,222] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.666666666666666, 62.33333333333333, 0.0, 0.0, 26.0, 24.88489138401096, 0.2680382634821208, 0.0, 1.0, 42333.27091130289], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3912000.0000, 
sim time next is 3912600.0000, 
raw observation next is [-6.833333333333334, 63.16666666666666, 0.0, 0.0, 26.0, 24.80060352091012, 0.2530881031385069, 0.0, 1.0, 42487.29870437142], 
processed observation next is [1.0, 0.2608695652173913, 0.27331486611265005, 0.6316666666666666, 0.0, 0.0, 0.6666666666666666, 0.5667169600758433, 0.5843627010461689, 0.0, 1.0, 0.20232047002081627], 
reward next is 0.7977, 
noisyNet noise sample is [array([-0.874002], dtype=float32), 0.3679129]. 
=============================================
[2019-04-04 01:12:33,360] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.0438065e-21 4.5532599e-13 1.3380947e-18 2.4109942e-15 2.0607868e-14
 1.0308286e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 01:12:33,360] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0957
[2019-04-04 01:12:33,371] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.4, 75.66666666666667, 0.0, 0.0, 26.0, 25.53547244106747, 0.4099602093216294, 0.0, 1.0, 18747.56861575616], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4321200.0000, 
sim time next is 4321800.0000, 
raw observation next is [4.35, 75.5, 0.0, 0.0, 26.0, 25.53340598579315, 0.403251406668364, 0.0, 1.0, 18745.13104393443], 
processed observation next is [1.0, 0.0, 0.5831024930747922, 0.755, 0.0, 0.0, 0.6666666666666666, 0.627783832149429, 0.6344171355561213, 0.0, 1.0, 0.08926252878064014], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.2774405], dtype=float32), 1.3118613]. 
=============================================
[2019-04-04 01:12:35,821] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.6881874e-20 5.3846733e-13 7.2973344e-18 1.6175917e-14 3.7807854e-14
 1.4154888e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 01:12:35,821] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7469
[2019-04-04 01:12:35,866] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.16666666666667, 58.83333333333334, 0.0, 0.0, 26.0, 24.72624676304253, 0.2764948966777076, 0.0, 1.0, 43842.50480785495], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3978600.0000, 
sim time next is 3979200.0000, 
raw observation next is [-11.33333333333333, 59.66666666666667, 0.0, 0.0, 26.0, 24.73370907817024, 0.2671236238937156, 0.0, 1.0, 43785.71735076367], 
processed observation next is [1.0, 0.043478260869565216, 0.14866112650046176, 0.5966666666666667, 0.0, 0.0, 0.6666666666666666, 0.5611424231808533, 0.5890412079645718, 0.0, 1.0, 0.20850341595601748], 
reward next is 0.7915, 
noisyNet noise sample is [array([1.1351465], dtype=float32), 1.5732452]. 
=============================================
[2019-04-04 01:12:46,796] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.8110497e-22 1.5938321e-14 5.0515278e-21 1.8269314e-16 1.4957239e-16
 2.6926246e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 01:12:46,796] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2816
[2019-04-04 01:12:46,820] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.0, 35.0, 98.0, 728.0, 26.0, 27.15546292257693, 0.7817550602941606, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4115400.0000, 
sim time next is 4116000.0000, 
raw observation next is [4.0, 35.0, 96.0, 711.5, 26.0, 27.22294263741497, 0.6742537293696985, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5734072022160666, 0.35, 0.32, 0.7861878453038674, 0.6666666666666666, 0.7685785531179142, 0.7247512431232329, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.96819663], dtype=float32), -1.12229]. 
=============================================
[2019-04-04 01:12:46,831] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[89.066376]
 [89.08032 ]
 [89.15531 ]
 [89.33738 ]
 [89.357376]], R is [[88.92221832]
 [89.03299713]
 [89.14266968]
 [89.25124359]
 [89.35873413]].
[2019-04-04 01:12:46,838] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.8724936e-20 2.5113581e-12 4.2932212e-18 5.7212447e-15 1.4192421e-14
 4.7311758e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 01:12:46,838] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4269
[2019-04-04 01:12:46,857] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.333333333333334, 36.33333333333333, 0.0, 0.0, 26.0, 24.84516892751611, 0.2286926485071905, 0.0, 1.0, 40387.37840132401], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4077600.0000, 
sim time next is 4078200.0000, 
raw observation next is [-4.166666666666667, 35.16666666666667, 0.0, 0.0, 26.0, 24.78929686555816, 0.2317953975124562, 0.0, 1.0, 40362.70866115568], 
processed observation next is [1.0, 0.17391304347826086, 0.3471837488457987, 0.35166666666666674, 0.0, 0.0, 0.6666666666666666, 0.5657747387965134, 0.5772651325041521, 0.0, 1.0, 0.19220337457693182], 
reward next is 0.8078, 
noisyNet noise sample is [array([-0.288058], dtype=float32), -0.8590031]. 
=============================================
[2019-04-04 01:13:07,612] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.9288547e-21 5.8121422e-14 6.6446469e-20 6.3241465e-16 6.9235483e-16
 1.1213242e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:13:07,612] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9819
[2019-04-04 01:13:07,644] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.666666666666667, 46.0, 171.5, 28.83333333333333, 26.0, 26.34599398021719, 0.5960873769284607, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4548000.0000, 
sim time next is 4548600.0000, 
raw observation next is [2.5, 46.5, 153.0, 12.0, 26.0, 26.39592594176162, 0.5969475658009313, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5318559556786704, 0.465, 0.51, 0.013259668508287293, 0.6666666666666666, 0.6996604951468018, 0.6989825219336439, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.15105], dtype=float32), -0.2910208]. 
=============================================
[2019-04-04 01:13:12,116] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.2692713e-21 2.8357487e-13 3.8583685e-19 1.5390535e-15 1.5755811e-15
 5.2109660e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:13:12,117] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3023
[2019-04-04 01:13:12,137] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.3, 72.0, 0.0, 0.0, 26.0, 25.32804796892131, 0.4843999350408912, 0.0, 1.0, 46722.85876522443], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4489200.0000, 
sim time next is 4489800.0000, 
raw observation next is [-0.3333333333333333, 72.16666666666667, 0.0, 0.0, 26.0, 25.33001908444198, 0.4856890290274005, 0.0, 1.0, 44604.57279549362], 
processed observation next is [1.0, 1.0, 0.4533702677747, 0.7216666666666667, 0.0, 0.0, 0.6666666666666666, 0.6108349237034982, 0.6618963430091335, 0.0, 1.0, 0.2124027275975887], 
reward next is 0.7876, 
noisyNet noise sample is [array([0.25834164], dtype=float32), 0.8934701]. 
=============================================
[2019-04-04 01:13:15,672] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1636383e-20 1.1822924e-12 5.5547577e-19 4.3973195e-15 7.2554877e-15
 2.1859290e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 01:13:15,672] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2398
[2019-04-04 01:13:15,695] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 47.0, 0.0, 0.0, 26.0, 25.41162345860911, 0.3640483394297764, 0.0, 1.0, 45524.57525347792], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4230000.0000, 
sim time next is 4230600.0000, 
raw observation next is [1.166666666666667, 47.16666666666667, 0.0, 0.0, 26.0, 25.40351821720841, 0.3634843212299074, 0.0, 1.0, 45492.53666369375], 
processed observation next is [0.0, 1.0, 0.49492151431209613, 0.47166666666666673, 0.0, 0.0, 0.6666666666666666, 0.6169598514340343, 0.6211614404099691, 0.0, 1.0, 0.21663112696997022], 
reward next is 0.7834, 
noisyNet noise sample is [array([1.0291388], dtype=float32), 1.3839422]. 
=============================================
[2019-04-04 01:13:27,109] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.2272607e-22 3.0908469e-14 2.9477344e-20 2.9730706e-16 1.5925360e-16
 4.1617911e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:13:27,109] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4088
[2019-04-04 01:13:27,176] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.199999999999999, 62.66666666666667, 94.5, 522.0, 26.0, 26.09040411112895, 0.5355582764112072, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4351200.0000, 
sim time next is 4351800.0000, 
raw observation next is [5.75, 59.83333333333333, 97.0, 553.0, 26.0, 26.27802616607842, 0.5608931222683621, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.6218836565096953, 0.5983333333333333, 0.3233333333333333, 0.611049723756906, 0.6666666666666666, 0.6898355138398683, 0.6869643740894541, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0024154], dtype=float32), -0.84540695]. 
=============================================
[2019-04-04 01:13:37,454] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.7393557e-19 6.6534916e-11 8.1244778e-18 3.1169312e-14 3.5107689e-14
 1.2512826e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 01:13:37,455] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8776
[2019-04-04 01:13:37,489] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.333333333333333, 61.66666666666667, 0.0, 0.0, 26.0, 24.60254703648043, 0.1814282145117248, 0.0, 1.0, 39491.55340868184], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4861200.0000, 
sim time next is 4861800.0000, 
raw observation next is [-3.5, 62.5, 0.0, 0.0, 26.0, 24.57684250125638, 0.1767932969518152, 0.0, 1.0, 39480.93899097785], 
processed observation next is [0.0, 0.2608695652173913, 0.36565096952908593, 0.625, 0.0, 0.0, 0.6666666666666666, 0.5480702084380317, 0.5589310989839383, 0.0, 1.0, 0.1880044713856088], 
reward next is 0.8120, 
noisyNet noise sample is [array([-0.417903], dtype=float32), 1.4643505]. 
=============================================
[2019-04-04 01:13:40,017] A3C_AGENT_WORKER-Thread-7 INFO:Evaluating...
[2019-04-04 01:13:40,022] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 01:13:40,022] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:13:40,030] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run24
[2019-04-04 01:13:40,058] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 01:13:40,058] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:13:40,061] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 01:13:40,062] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:13:40,066] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run24
[2019-04-04 01:13:40,102] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run24
[2019-04-04 01:15:14,199] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.3516434], dtype=float32), 0.19180621]
[2019-04-04 01:15:14,199] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-8.081808869, 67.55221144000001, 352.9557389, 360.2058406, 26.0, 25.79148359331497, 0.4587497897802411, 1.0, 1.0, 0.0]
[2019-04-04 01:15:14,199] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 01:15:14,200] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.0995184e-21 8.9194379e-14 3.0145773e-20 8.5474841e-16 5.4369679e-16
 1.8030681e-21 1.0000000e+00], sampled 0.038360211069104366
[2019-04-04 01:15:32,732] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.3516434], dtype=float32), 0.19180621]
[2019-04-04 01:15:32,733] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-4.842447122, 64.349227935, 0.0, 0.0, 26.0, 25.27708765618114, 0.3687058982614317, 0.0, 1.0, 44109.35041734033]
[2019-04-04 01:15:32,733] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 01:15:32,734] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [3.2814930e-20 1.1699411e-12 1.9975541e-18 8.9383711e-15 2.1505687e-14
 8.4657257e-20 1.0000000e+00], sampled 0.8594064584144493
[2019-04-04 01:16:01,949] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 01:16:22,960] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 01:16:27,059] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 01:16:28,089] A3C_AGENT_WORKER-Thread-7 INFO:Global step: 2300000, evaluation results [2300000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 01:16:32,743] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.0986540e-21 1.1286248e-12 1.8010224e-18 2.3657024e-14 1.9035475e-14
 4.6322392e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 01:16:32,744] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5443
[2019-04-04 01:16:32,780] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.2000000000000001, 63.66666666666667, 0.0, 0.0, 26.0, 25.42297883314252, 0.4494053147583457, 0.0, 1.0, 36296.55157329138], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4584000.0000, 
sim time next is 4584600.0000, 
raw observation next is [0.1, 64.0, 0.0, 0.0, 26.0, 25.43833897192, 0.4415889131197385, 0.0, 1.0, 26028.23901997651], 
processed observation next is [1.0, 0.043478260869565216, 0.4653739612188367, 0.64, 0.0, 0.0, 0.6666666666666666, 0.6198615809933333, 0.6471963043732462, 0.0, 1.0, 0.12394399533322148], 
reward next is 0.8761, 
noisyNet noise sample is [array([0.5123223], dtype=float32), 0.3572962]. 
=============================================
[2019-04-04 01:16:33,929] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:16:33,929] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:16:33,932] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run18
[2019-04-04 01:16:34,495] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.8601557e-20 1.3198284e-12 1.3474326e-18 7.2616626e-15 1.2561203e-14
 6.1518139e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 01:16:34,495] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8600
[2019-04-04 01:16:34,512] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.833333333333333, 82.83333333333333, 0.0, 0.0, 26.0, 25.06726886691895, 0.4034610076018452, 0.0, 1.0, 41616.17290268088], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4751400.0000, 
sim time next is 4752000.0000, 
raw observation next is [-4.0, 84.0, 0.0, 0.0, 26.0, 25.09266322408297, 0.3978784527014491, 0.0, 1.0, 41540.77341939047], 
processed observation next is [0.0, 0.0, 0.3518005540166205, 0.84, 0.0, 0.0, 0.6666666666666666, 0.5910552686735807, 0.632626150900483, 0.0, 1.0, 0.19781320675900224], 
reward next is 0.8022, 
noisyNet noise sample is [array([1.2321784], dtype=float32), -1.9446337]. 
=============================================
[2019-04-04 01:16:34,522] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[78.44281]
 [78.44758]
 [78.4125 ]
 [78.42137]
 [78.41536]], R is [[78.36413574]
 [78.38232422]
 [78.39998627]
 [78.41708374]
 [78.43341064]].
[2019-04-04 01:16:36,151] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:16:36,151] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:16:36,154] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run18
[2019-04-04 01:16:37,410] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.8505322e-20 5.6400900e-12 7.1140810e-19 5.4150922e-15 2.6994969e-14
 3.6644308e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 01:16:37,412] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.0367
[2019-04-04 01:16:37,430] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.833333333333333, 37.5, 196.0, 626.0, 26.0, 25.13613329246367, 0.433955080993593, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4801800.0000, 
sim time next is 4802400.0000, 
raw observation next is [3.0, 37.0, 184.0, 655.0, 26.0, 25.13209451473363, 0.4361052369952593, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.5457063711911359, 0.37, 0.6133333333333333, 0.7237569060773481, 0.6666666666666666, 0.5943412095611359, 0.6453684123317531, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.76410186], dtype=float32), -0.98094255]. 
=============================================
[2019-04-04 01:16:40,037] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:16:40,038] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:16:40,060] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run18
[2019-04-04 01:16:40,217] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.8668555e-21 5.9282914e-14 9.1926488e-20 4.1388334e-15 3.0886531e-15
 4.1772055e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:16:40,218] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7305
[2019-04-04 01:16:40,227] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 72.0, 139.1666666666667, 1.833333333333333, 26.0, 26.07344698383989, 0.5454158235581718, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4722000.0000, 
sim time next is 4722600.0000, 
raw observation next is [1.0, 72.0, 131.3333333333333, 3.666666666666666, 26.0, 26.1281742328816, 0.5476872767646727, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.4903047091412743, 0.72, 0.4377777777777776, 0.004051565377532228, 0.6666666666666666, 0.6773478527401334, 0.6825624255882242, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.10353733], dtype=float32), -0.9028638]. 
=============================================
[2019-04-04 01:16:46,815] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:16:46,815] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:16:46,828] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run18
[2019-04-04 01:16:47,117] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:16:47,118] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:16:47,122] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run18
[2019-04-04 01:16:48,313] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:16:48,313] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:16:48,335] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run18
[2019-04-04 01:16:51,039] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:16:51,039] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:16:51,057] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run18
[2019-04-04 01:16:51,618] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:16:51,619] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:16:51,636] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run18
[2019-04-04 01:16:53,926] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:16:53,927] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:16:53,938] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run18
[2019-04-04 01:16:53,954] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:16:53,958] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:16:53,986] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run18
[2019-04-04 01:16:54,417] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:16:54,417] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:16:54,420] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run18
[2019-04-04 01:16:54,549] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:16:54,549] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:16:54,552] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run18
[2019-04-04 01:16:55,021] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:16:55,021] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:16:55,025] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run18
[2019-04-04 01:17:04,505] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:17:04,505] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:17:04,508] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run18
[2019-04-04 01:17:05,101] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:17:05,101] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:17:05,109] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run18
[2019-04-04 01:17:07,001] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:17:07,001] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:17:07,005] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run18
[2019-04-04 01:17:08,126] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.2000164e-20 8.3462469e-13 7.5805882e-19 5.1401834e-15 1.8305888e-14
 1.6830005e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 01:17:08,126] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0280
[2019-04-04 01:17:08,209] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.3666666666666667, 95.33333333333333, 0.0, 0.0, 26.0, 24.46798957150236, 0.1834816950130772, 0.0, 1.0, 40127.40013445167], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 81600.0000, 
sim time next is 82200.0000, 
raw observation next is [0.3333333333333333, 95.16666666666667, 0.0, 0.0, 26.0, 24.44366966321932, 0.1789108912398988, 0.0, 1.0, 40102.08782873357], 
processed observation next is [0.0, 0.9565217391304348, 0.4718374884579871, 0.9516666666666667, 0.0, 0.0, 0.6666666666666666, 0.5369724719349435, 0.559636963746633, 0.0, 1.0, 0.1909623229939694], 
reward next is 0.8090, 
noisyNet noise sample is [array([1.6288985], dtype=float32), -0.39026386]. 
=============================================
[2019-04-04 01:17:12,151] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.6963748e-21 7.3935432e-14 2.3739727e-19 2.1128210e-15 3.9451013e-15
 8.0590260e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:17:12,151] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7629
[2019-04-04 01:17:12,274] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.3, 61.0, 0.0, 0.0, 26.0, 25.26696233427857, 0.3631959783602204, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 151200.0000, 
sim time next is 151800.0000, 
raw observation next is [-7.383333333333333, 61.5, 0.0, 0.0, 26.0, 25.39275058883189, 0.3596560141248426, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.25807940904893817, 0.615, 0.0, 0.0, 0.6666666666666666, 0.6160625490693242, 0.6198853380416142, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.26222458], dtype=float32), -0.38610137]. 
=============================================
[2019-04-04 01:17:22,730] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.7669544e-20 1.4295537e-12 5.7023668e-18 6.4259464e-15 2.3435703e-14
 2.3237969e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 01:17:22,731] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0858
[2019-04-04 01:17:22,830] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 77.33333333333333, 0.0, 0.0, 26.0, 23.44735283116438, -0.08220963518279119, 0.0, 1.0, 44239.1953753111], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 183000.0000, 
sim time next is 183600.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 26.0, 23.40492987506879, -0.0921209691309605, 0.0, 1.0, 44036.07026832593], 
processed observation next is [1.0, 0.13043478260869565, 0.21606648199445982, 0.78, 0.0, 0.0, 0.6666666666666666, 0.4504108229223993, 0.4692930102896798, 0.0, 1.0, 0.20969557270631395], 
reward next is 0.7903, 
noisyNet noise sample is [array([-1.2715231], dtype=float32), 0.31007323]. 
=============================================
[2019-04-04 01:17:51,456] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.0948135e-21 4.4615800e-14 6.8704970e-19 1.9818081e-15 1.6063012e-15
 2.1082103e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 01:17:51,457] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6270
[2019-04-04 01:17:51,488] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-13.9, 68.0, 0.0, 0.0, 26.0, 23.72635098491163, -0.009128637589630348, 0.0, 1.0, 47184.94092371457], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 343800.0000, 
sim time next is 344400.0000, 
raw observation next is [-13.9, 67.33333333333334, 0.0, 0.0, 26.0, 23.65919300768117, -0.01447291577866559, 0.0, 1.0, 47224.31904413381], 
processed observation next is [1.0, 1.0, 0.07756232686980608, 0.6733333333333335, 0.0, 0.0, 0.6666666666666666, 0.4715994173067643, 0.4951756947404448, 0.0, 1.0, 0.2248777097339705], 
reward next is 0.7751, 
noisyNet noise sample is [array([-0.6758591], dtype=float32), 1.6954004]. 
=============================================
[2019-04-04 01:18:12,649] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.1678464e-21 1.8677205e-13 5.0299773e-19 1.9396324e-15 7.5006944e-15
 7.3414987e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 01:18:12,649] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7902
[2019-04-04 01:18:12,726] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 85.66666666666667, 88.16666666666666, 134.6666666666667, 26.0, 24.79973332181223, 0.269471391471996, 0.0, 1.0, 44224.46355040994], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 555600.0000, 
sim time next is 556200.0000, 
raw observation next is [-0.6, 85.0, 77.0, 141.0, 26.0, 24.82087438287883, 0.2764568168466149, 0.0, 1.0, 30507.55761369906], 
processed observation next is [0.0, 0.43478260869565216, 0.44598337950138506, 0.85, 0.25666666666666665, 0.1558011049723757, 0.6666666666666666, 0.5684061985732359, 0.5921522722822049, 0.0, 1.0, 0.14527408387475743], 
reward next is 0.8547, 
noisyNet noise sample is [array([-1.4040306], dtype=float32), -1.3344827]. 
=============================================
[2019-04-04 01:18:13,196] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.0468962e-21 3.0088769e-14 1.5795619e-19 1.0376245e-15 5.6602015e-15
 7.1966967e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:18:13,196] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9973
[2019-04-04 01:18:13,284] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.15, 51.5, 0.0, 0.0, 26.0, 24.20549900048527, 0.08114389544476418, 0.0, 1.0, 44755.98902093977], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 426600.0000, 
sim time next is 427200.0000, 
raw observation next is [-11.33333333333333, 52.33333333333334, 0.0, 0.0, 26.0, 24.15025130919734, 0.06860687359578016, 0.0, 1.0, 44741.88744325441], 
processed observation next is [1.0, 0.9565217391304348, 0.14866112650046176, 0.5233333333333334, 0.0, 0.0, 0.6666666666666666, 0.5125209424331117, 0.52286895786526, 0.0, 1.0, 0.21305660687264005], 
reward next is 0.7869, 
noisyNet noise sample is [array([-0.83893603], dtype=float32), 0.46397218]. 
=============================================
[2019-04-04 01:18:18,485] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.3704108e-20 3.9920508e-13 2.5625228e-19 2.8234084e-15 1.5205957e-14
 1.1666612e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 01:18:18,485] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0350
[2019-04-04 01:18:18,582] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.633333333333334, 60.66666666666667, 104.3333333333333, 79.33333333333334, 26.0, 24.88542202668259, 0.2246630112471559, 0.0, 1.0, 31844.54262865201], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 648600.0000, 
sim time next is 649200.0000, 
raw observation next is [-2.566666666666667, 60.33333333333334, 108.1666666666667, 89.66666666666667, 26.0, 24.89113090865252, 0.2253088351468503, 0.0, 1.0, 34787.18876028684], 
processed observation next is [0.0, 0.5217391304347826, 0.39150507848568794, 0.6033333333333334, 0.3605555555555557, 0.0990791896869245, 0.6666666666666666, 0.5742609090543768, 0.5751029450489501, 0.0, 1.0, 0.1656532798108897], 
reward next is 0.8343, 
noisyNet noise sample is [array([0.17660742], dtype=float32), 1.6621169]. 
=============================================
[2019-04-04 01:18:19,550] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.4614252e-22 1.8365795e-14 4.3040045e-21 7.5640730e-17 8.3885862e-17
 1.8163149e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:18:19,551] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2489
[2019-04-04 01:18:19,642] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.1, 27.0, 118.0, 0.0, 26.0, 25.24215425828807, 0.1375528262628962, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 472800.0000, 
sim time next is 473400.0000, 
raw observation next is [-2.0, 26.5, 122.0, 0.0, 26.0, 25.12827208604327, 0.1385448183632547, 1.0, 1.0, 63810.24570589628], 
processed observation next is [1.0, 0.4782608695652174, 0.40720221606648205, 0.265, 0.4066666666666667, 0.0, 0.6666666666666666, 0.5940226738369393, 0.5461816061210849, 1.0, 1.0, 0.3038583128852204], 
reward next is 0.6961, 
noisyNet noise sample is [array([-0.95465916], dtype=float32), -0.1748009]. 
=============================================
[2019-04-04 01:18:33,082] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.2138277e-20 4.4267777e-13 1.1742926e-18 3.9018475e-15 1.0553653e-14
 4.7454916e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 01:18:33,083] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6055
[2019-04-04 01:18:33,158] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.533333333333333, 85.66666666666667, 34.16666666666667, 37.5, 26.0, 24.95605252552194, 0.3027443074381306, 0.0, 1.0, 50127.51130480439], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 578400.0000, 
sim time next is 579000.0000, 
raw observation next is [-1.616666666666667, 86.33333333333333, 27.33333333333334, 30.0, 26.0, 24.94916562821606, 0.3073156065278208, 0.0, 1.0, 48590.68432463502], 
processed observation next is [0.0, 0.6956521739130435, 0.4178208679593721, 0.8633333333333333, 0.09111111111111113, 0.03314917127071823, 0.6666666666666666, 0.5790971356846718, 0.6024385355092736, 0.0, 1.0, 0.23138421106969057], 
reward next is 0.7686, 
noisyNet noise sample is [array([0.31902415], dtype=float32), 1.1974277]. 
=============================================
[2019-04-04 01:18:33,163] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[78.80567]
 [78.64392]
 [78.62939]
 [78.66713]
 [78.62705]], R is [[79.07844543]
 [79.04895782]
 [79.0415802 ]
 [79.08322906]
 [79.13018036]].
[2019-04-04 01:18:51,846] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.8013536e-24 7.2811587e-16 6.2725433e-22 8.9106791e-18 8.9262276e-18
 5.5847652e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 01:18:51,867] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9454
[2019-04-04 01:18:51,909] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 76.0, 0.0, 0.0, 26.0, 25.81706221988414, 0.6229881632690072, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1027800.0000, 
sim time next is 1028400.0000, 
raw observation next is [14.4, 75.66666666666667, 0.0, 0.0, 26.0, 25.86425972913853, 0.6231090142585715, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.8614958448753465, 0.7566666666666667, 0.0, 0.0, 0.6666666666666666, 0.6553549774282109, 0.7077030047528572, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4191298], dtype=float32), -1.3882519]. 
=============================================
[2019-04-04 01:18:57,717] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.1690212e-24 2.0722150e-16 2.3936289e-22 1.3406348e-17 3.5626830e-18
 1.0636580e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 01:18:57,718] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.2993
[2019-04-04 01:18:57,815] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.616666666666666, 93.66666666666666, 101.3333333333333, 0.0, 26.0, 25.16360024683478, 0.2544374694382054, 1.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 910200.0000, 
sim time next is 910800.0000, 
raw observation next is [3.8, 93.0, 100.0, 0.0, 26.0, 25.16036124725183, 0.2522958483399024, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5678670360110805, 0.93, 0.3333333333333333, 0.0, 0.6666666666666666, 0.5966967706043192, 0.5840986161133008, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.66875726], dtype=float32), 1.3525072]. 
=============================================
[2019-04-04 01:18:58,760] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.8466146e-22 2.8432646e-14 1.6512410e-20 7.3748280e-17 8.2521404e-17
 9.4561224e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:18:58,760] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8459
[2019-04-04 01:18:58,817] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.533333333333333, 78.0, 0.0, 0.0, 26.0, 24.57566399822124, 0.1843559917088464, 0.0, 1.0, 39268.00146015653], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 876000.0000, 
sim time next is 876600.0000, 
raw observation next is [-1.45, 77.5, 0.0, 0.0, 26.0, 24.61344081981355, 0.1847178269616885, 0.0, 1.0, 39216.82556991155], 
processed observation next is [1.0, 0.13043478260869565, 0.422437673130194, 0.775, 0.0, 0.0, 0.6666666666666666, 0.5511200683177959, 0.5615726089872295, 0.0, 1.0, 0.18674678842815023], 
reward next is 0.8133, 
noisyNet noise sample is [array([-0.51230496], dtype=float32), 0.9348982]. 
=============================================
[2019-04-04 01:18:59,127] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.7152488e-23 2.8391647e-16 7.2723090e-22 7.9246881e-18 7.5926602e-18
 6.4937318e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 01:18:59,143] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2961
[2019-04-04 01:18:59,194] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.7131667e-24 3.0779835e-16 1.8155736e-22 9.4779182e-18 1.6193851e-18
 3.4975241e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 01:18:59,210] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4732
[2019-04-04 01:18:59,212] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.9, 92.16666666666667, 12.0, 0.0, 26.0, 24.51399361889501, 0.310557693130901, 1.0, 1.0, 196475.6107603647], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 924600.0000, 
sim time next is 925200.0000, 
raw observation next is [5.0, 92.0, 9.0, 0.0, 26.0, 25.00417326812636, 0.3543358689796134, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.6011080332409973, 0.92, 0.03, 0.0, 0.6666666666666666, 0.5836811056771968, 0.6181119563265378, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.48954648], dtype=float32), -0.21075304]. 
=============================================
[2019-04-04 01:18:59,270] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.36666666666667, 86.0, 126.6666666666667, 0.0, 26.0, 26.77678412591492, 0.5768948313239153, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 994800.0000, 
sim time next is 995400.0000, 
raw observation next is [12.45, 86.0, 128.0, 0.0, 26.0, 25.73372131643142, 0.5533700844303221, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.8074792243767314, 0.86, 0.4266666666666667, 0.0, 0.6666666666666666, 0.644476776369285, 0.6844566948101073, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0349686], dtype=float32), -3.20235]. 
=============================================
[2019-04-04 01:19:07,460] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.1128554e-23 7.6035451e-14 2.1351128e-20 1.5909513e-16 1.4145895e-16
 1.4972443e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:19:07,460] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9580
[2019-04-04 01:19:07,486] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 24.45692023726708, 0.1421628220362263, 0.0, 1.0, 38682.83253134735], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 886800.0000, 
sim time next is 887400.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 24.41645935052396, 0.1437951009507819, 0.0, 1.0, 38639.94900970437], 
processed observation next is [1.0, 0.2608695652173913, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5347049458769968, 0.5479317003169273, 0.0, 1.0, 0.1839997571890684], 
reward next is 0.8160, 
noisyNet noise sample is [array([1.1665365], dtype=float32), 2.1486864]. 
=============================================
[2019-04-04 01:19:09,867] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.4691648e-25 7.7169691e-17 8.5125393e-23 1.2974950e-18 1.6220931e-18
 1.4943434e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 01:19:09,867] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8711
[2019-04-04 01:19:09,917] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.3, 80.0, 107.0, 117.0, 26.0, 26.95312563801329, 0.8352418899979117, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1072800.0000, 
sim time next is 1073400.0000, 
raw observation next is [13.66666666666667, 78.33333333333334, 109.3333333333333, 77.99999999999999, 26.0, 27.04837380246572, 0.8494201794219874, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.8411819021237306, 0.7833333333333334, 0.36444444444444435, 0.08618784530386739, 0.6666666666666666, 0.7540311502054765, 0.7831400598073291, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0857552], dtype=float32), -0.8120367]. 
=============================================
[2019-04-04 01:19:11,619] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.2041283e-24 2.0413261e-16 5.7693525e-22 1.7407034e-18 3.7343863e-18
 3.4544830e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 01:19:11,620] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0551
[2019-04-04 01:19:11,655] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 84.0, 72.16666666666667, 0.0, 26.0, 25.51391244553232, 0.3029986006142934, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 901200.0000, 
sim time next is 901800.0000, 
raw observation next is [1.1, 84.0, 77.0, 0.0, 26.0, 25.53266740861726, 0.2942945479064944, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.49307479224376743, 0.84, 0.25666666666666665, 0.0, 0.6666666666666666, 0.6277222840514384, 0.5980981826354982, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0534779], dtype=float32), 0.08774938]. 
=============================================
[2019-04-04 01:19:14,945] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.3605359e-23 5.0114599e-14 4.4020896e-21 5.3343478e-17 1.8257540e-16
 2.5856727e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:19:14,945] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6008
[2019-04-04 01:19:14,973] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.1, 77.0, 0.0, 0.0, 26.0, 25.64854955293799, 0.6257830795204946, 0.0, 1.0, 21703.97238401216], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1135200.0000, 
sim time next is 1135800.0000, 
raw observation next is [11.1, 77.0, 0.0, 0.0, 26.0, 25.6534845489747, 0.6239289920519155, 0.0, 1.0, 20445.45831683237], 
processed observation next is [0.0, 0.13043478260869565, 0.7700831024930749, 0.77, 0.0, 0.0, 0.6666666666666666, 0.6377903790812249, 0.7079763306839718, 0.0, 1.0, 0.09735932531824938], 
reward next is 0.9026, 
noisyNet noise sample is [array([-1.1480026], dtype=float32), -0.56898]. 
=============================================
[2019-04-04 01:19:26,784] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.2625410e-23 6.9281451e-16 9.6546265e-22 9.3583178e-18 3.0874367e-17
 1.5426150e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 01:19:26,787] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6195
[2019-04-04 01:19:26,814] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.45, 73.5, 0.0, 0.0, 26.0, 25.2964552057658, 0.5534028149206998, 0.0, 1.0, 196664.0695617451], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1542600.0000, 
sim time next is 1543200.0000, 
raw observation next is [7.533333333333333, 73.66666666666666, 0.0, 0.0, 26.0, 25.26567745959006, 0.5811164690012379, 0.0, 1.0, 198415.2365098056], 
processed observation next is [1.0, 0.8695652173913043, 0.6712834718374886, 0.7366666666666666, 0.0, 0.0, 0.6666666666666666, 0.6054731216325049, 0.6937054896670793, 0.0, 1.0, 0.9448344595705028], 
reward next is 0.0552, 
noisyNet noise sample is [array([1.4371374], dtype=float32), -1.4188311]. 
=============================================
[2019-04-04 01:19:27,776] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.62369505e-23 1.91428726e-15 5.62194515e-21 3.58543984e-17
 1.06651804e-16 5.81209365e-23 1.00000000e+00], sum to 1.0000
[2019-04-04 01:19:27,786] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6909
[2019-04-04 01:19:27,811] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.08333333333333331, 95.16666666666667, 0.0, 0.0, 26.0, 25.29669641117133, 0.5089141162380603, 0.0, 1.0, 46166.71884541216], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1378200.0000, 
sim time next is 1378800.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 26.0, 25.28020962011097, 0.5115013519789595, 0.0, 1.0, 43181.07747705271], 
processed observation next is [1.0, 1.0, 0.46260387811634357, 0.95, 0.0, 0.0, 0.6666666666666666, 0.6066841350092474, 0.6705004506596532, 0.0, 1.0, 0.20562417846215578], 
reward next is 0.7944, 
noisyNet noise sample is [array([-0.42970175], dtype=float32), -1.4378684]. 
=============================================
[2019-04-04 01:19:32,750] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.1022236e-23 2.7017839e-14 2.4653294e-21 2.9520600e-17 1.5204658e-17
 6.3191324e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 01:19:32,755] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9158
[2019-04-04 01:19:32,778] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [18.63333333333333, 63.66666666666667, 71.5, 0.0, 26.0, 25.03153388163404, 0.4916902128388969, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1179600.0000, 
sim time next is 1180200.0000, 
raw observation next is [18.71666666666667, 63.33333333333333, 63.00000000000001, 0.0, 26.0, 25.05951985498358, 0.4901742175069504, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.981071098799631, 0.6333333333333333, 0.21000000000000002, 0.0, 0.6666666666666666, 0.5882933212486318, 0.6633914058356501, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.17525436], dtype=float32), -0.16569497]. 
=============================================
[2019-04-04 01:19:42,975] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.7259595e-23 7.1010812e-15 7.1806547e-21 1.2543856e-16 2.0464720e-16
 7.1299874e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 01:19:42,976] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9352
[2019-04-04 01:19:43,006] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 0.0, 0.0, 26.0, 25.53155896722158, 0.4953461041648859, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1450800.0000, 
sim time next is 1451400.0000, 
raw observation next is [1.1, 92.0, 0.0, 0.0, 26.0, 25.44074470147399, 0.4816385913224221, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.49307479224376743, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6200620584561657, 0.6605461971074741, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.7245116], dtype=float32), 0.7816704]. 
=============================================
[2019-04-04 01:19:52,862] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0571650e-24 1.7545718e-16 1.7798461e-22 5.9039061e-18 8.0415960e-18
 3.2686931e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 01:19:52,865] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0477
[2019-04-04 01:19:52,883] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.1, 50.33333333333334, 80.33333333333333, 328.0, 26.0, 26.90601399286931, 0.779515521095893, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1525800.0000, 
sim time next is 1526400.0000, 
raw observation next is [12.2, 50.0, 82.0, 253.0, 26.0, 26.98153580880275, 0.7942011728113694, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.8005540166204987, 0.5, 0.2733333333333333, 0.2795580110497238, 0.6666666666666666, 0.7484613174002291, 0.7647337242704565, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.82696617], dtype=float32), -0.042424772]. 
=============================================
[2019-04-04 01:19:56,883] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.3341064e-22 3.5991607e-15 2.1282821e-20 1.8779053e-16 3.0883570e-16
 1.4907245e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:19:56,884] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7411
[2019-04-04 01:19:56,966] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.0, 83.33333333333334, 0.0, 0.0, 26.0, 25.71671463364167, 0.5972397830102607, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1636800.0000, 
sim time next is 1637400.0000, 
raw observation next is [7.1, 82.66666666666667, 0.0, 0.0, 26.0, 25.67220544148928, 0.5864123950220548, 0.0, 1.0, 70871.9775822074], 
processed observation next is [1.0, 0.9565217391304348, 0.6592797783933518, 0.8266666666666667, 0.0, 0.0, 0.6666666666666666, 0.6393504534574399, 0.6954707983406849, 0.0, 1.0, 0.33748560753432094], 
reward next is 0.6625, 
noisyNet noise sample is [array([-0.32217923], dtype=float32), -0.82241577]. 
=============================================
[2019-04-04 01:20:10,509] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.7428525e-21 6.6938864e-13 2.3427120e-18 2.9029162e-15 6.8919043e-15
 1.8857846e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 01:20:10,509] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2632
[2019-04-04 01:20:10,597] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 87.0, 97.0, 0.0, 26.0, 24.96955792847374, 0.3400224375561655, 0.0, 1.0, 18738.88830610051], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1765800.0000, 
sim time next is 1766400.0000, 
raw observation next is [-2.3, 87.0, 100.6666666666667, 0.0, 26.0, 24.9547816724825, 0.333335327445405, 0.0, 1.0, 33970.08946800172], 
processed observation next is [0.0, 0.43478260869565216, 0.3988919667590028, 0.87, 0.33555555555555566, 0.0, 0.6666666666666666, 0.5795651393735417, 0.611111775815135, 0.0, 1.0, 0.1617623308000082], 
reward next is 0.8382, 
noisyNet noise sample is [array([-1.0508102], dtype=float32), 0.40776983]. 
=============================================
[2019-04-04 01:20:15,138] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.66510074e-19 3.34619658e-12 7.72032150e-18 1.79876713e-14
 1.09486395e-14 6.42108153e-19 1.00000000e+00], sum to 1.0000
[2019-04-04 01:20:15,138] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6811
[2019-04-04 01:20:15,176] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.733333333333333, 79.66666666666667, 0.0, 0.0, 26.0, 24.29310604270948, 0.1391736646776451, 0.0, 1.0, 46089.18082732759], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1819200.0000, 
sim time next is 1819800.0000, 
raw observation next is [-5.8, 80.5, 0.0, 0.0, 26.0, 24.27132401101603, 0.1343641621256087, 0.0, 1.0, 46161.09428629086], 
processed observation next is [0.0, 0.043478260869565216, 0.30193905817174516, 0.805, 0.0, 0.0, 0.6666666666666666, 0.5226103342513359, 0.5447880540418696, 0.0, 1.0, 0.21981473469662316], 
reward next is 0.7802, 
noisyNet noise sample is [array([-2.698169], dtype=float32), -0.83218694]. 
=============================================
[2019-04-04 01:20:26,837] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.2811068e-19 4.3735042e-12 7.8557020e-18 2.6880033e-14 4.0178218e-14
 1.6647332e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 01:20:26,838] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5939
[2019-04-04 01:20:26,851] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.2, 87.0, 0.0, 0.0, 26.0, 23.98693362464066, 0.06872632644386133, 0.0, 1.0, 46740.53216738699], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1825200.0000, 
sim time next is 1825800.0000, 
raw observation next is [-6.2, 86.33333333333333, 0.0, 0.0, 26.0, 23.95084767252502, 0.06139918729695543, 0.0, 1.0, 46791.32986269236], 
processed observation next is [0.0, 0.13043478260869565, 0.2908587257617729, 0.8633333333333333, 0.0, 0.0, 0.6666666666666666, 0.4959039727104182, 0.5204663957656518, 0.0, 1.0, 0.22281585648901123], 
reward next is 0.7772, 
noisyNet noise sample is [array([-0.6318629], dtype=float32), -0.089973226]. 
=============================================
[2019-04-04 01:20:32,385] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.1285034e-21 3.7039599e-15 2.2887813e-20 3.1773011e-16 2.5177030e-16
 6.4505325e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:20:32,385] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3598
[2019-04-04 01:20:32,506] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.8, 70.5, 0.0, 0.0, 26.0, 25.09527870508664, 0.3455202310221422, 1.0, 1.0, 110779.2661981412], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2230200.0000, 
sim time next is 2230800.0000, 
raw observation next is [-4.866666666666666, 70.66666666666666, 0.0, 0.0, 26.0, 24.99861267582316, 0.3613658022200326, 1.0, 1.0, 135714.5318282975], 
processed observation next is [1.0, 0.8260869565217391, 0.3277931671283472, 0.7066666666666666, 0.0, 0.0, 0.6666666666666666, 0.5832177229852634, 0.6204552674066776, 1.0, 1.0, 0.6462596753728452], 
reward next is 0.3537, 
noisyNet noise sample is [array([0.6743715], dtype=float32), 1.0672945]. 
=============================================
[2019-04-04 01:20:37,685] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.3362569e-22 1.7024085e-14 1.9438347e-20 1.8966162e-16 2.0958500e-16
 2.9735606e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:20:37,685] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5457
[2019-04-04 01:20:37,748] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.2, 77.0, 0.0, 0.0, 26.0, 25.49245182559808, 0.4043834773180189, 0.0, 1.0, 31743.99861523229], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2143200.0000, 
sim time next is 2143800.0000, 
raw observation next is [-5.3, 78.5, 0.0, 0.0, 26.0, 25.38064546644452, 0.3814161330258899, 0.0, 1.0, 34459.72555863416], 
processed observation next is [1.0, 0.8260869565217391, 0.31578947368421056, 0.785, 0.0, 0.0, 0.6666666666666666, 0.6150537888703766, 0.62713871100863, 0.0, 1.0, 0.16409393123159124], 
reward next is 0.8359, 
noisyNet noise sample is [array([0.40659872], dtype=float32), 0.22103967]. 
=============================================
[2019-04-04 01:20:38,846] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.7989222e-21 1.3861261e-12 1.8003079e-18 1.3762293e-15 5.7394974e-15
 3.1333495e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 01:20:38,847] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4795
[2019-04-04 01:20:38,871] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.600000000000001, 83.0, 0.0, 0.0, 26.0, 24.32468237883197, 0.1204292569497475, 0.0, 1.0, 41423.72929312006], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1998600.0000, 
sim time next is 1999200.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.34271959253359, 0.125302715003559, 0.0, 1.0, 41355.13504356789], 
processed observation next is [1.0, 0.13043478260869565, 0.30747922437673136, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5285599660444659, 0.541767571667853, 0.0, 1.0, 0.19692921449318043], 
reward next is 0.8031, 
noisyNet noise sample is [array([1.3653507], dtype=float32), -0.13059972]. 
=============================================
[2019-04-04 01:20:50,581] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.7199492e-20 2.8506239e-13 2.6566868e-18 2.2871550e-15 4.3594393e-15
 5.0502394e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 01:20:50,581] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7218
[2019-04-04 01:20:50,614] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.9, 86.16666666666667, 0.0, 0.0, 26.0, 24.1267127936026, 0.1004539897640537, 0.0, 1.0, 43737.09222615077], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2257800.0000, 
sim time next is 2258400.0000, 
raw observation next is [-8.0, 86.33333333333334, 0.0, 0.0, 26.0, 24.15635758961614, 0.09977588330870964, 0.0, 1.0, 43682.06076511289], 
processed observation next is [1.0, 0.13043478260869565, 0.24099722991689754, 0.8633333333333334, 0.0, 0.0, 0.6666666666666666, 0.5130297991346783, 0.5332586277695699, 0.0, 1.0, 0.20800981316720424], 
reward next is 0.7920, 
noisyNet noise sample is [array([-0.6454655], dtype=float32), -1.1958836]. 
=============================================
[2019-04-04 01:20:51,020] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.6700384e-21 3.2710439e-13 1.9955664e-18 5.2451425e-16 5.2300600e-15
 2.8478148e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 01:20:51,020] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1942
[2019-04-04 01:20:51,046] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.533333333333334, 77.0, 0.0, 0.0, 26.0, 24.14886341213963, 0.08516802762487825, 0.0, 1.0, 42074.23045600582], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2175600.0000, 
sim time next is 2176200.0000, 
raw observation next is [-6.45, 76.5, 0.0, 0.0, 26.0, 24.12438756386508, 0.07403210653297965, 0.0, 1.0, 42051.50385924979], 
processed observation next is [1.0, 0.17391304347826086, 0.28393351800554023, 0.765, 0.0, 0.0, 0.6666666666666666, 0.5103656303220901, 0.5246773688443266, 0.0, 1.0, 0.20024525647261804], 
reward next is 0.7998, 
noisyNet noise sample is [array([-1.5433335], dtype=float32), 0.2760456]. 
=============================================
[2019-04-04 01:21:01,191] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.2463679e-21 1.5158301e-13 3.6943142e-19 5.5512284e-16 3.3443601e-15
 2.9693470e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 01:21:01,192] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5136
[2019-04-04 01:21:01,238] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.100000000000001, 78.33333333333334, 0.0, 0.0, 26.0, 23.75849940046304, -0.0003213479278438523, 0.0, 1.0, 41891.66792788614], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2182200.0000, 
sim time next is 2182800.0000, 
raw observation next is [-6.0, 77.66666666666667, 0.0, 0.0, 26.0, 23.71107453459674, -0.007402014178009554, 0.0, 1.0, 41891.52558331854], 
processed observation next is [1.0, 0.2608695652173913, 0.296398891966759, 0.7766666666666667, 0.0, 0.0, 0.6666666666666666, 0.4759228778830617, 0.4975326619406635, 0.0, 1.0, 0.1994834551586597], 
reward next is 0.8005, 
noisyNet noise sample is [array([0.17405334], dtype=float32), 0.9936339]. 
=============================================
[2019-04-04 01:21:08,346] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.6242136e-22 3.9318822e-14 1.1643849e-19 5.2214854e-16 1.1173140e-15
 5.8260591e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:21:08,380] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9538
[2019-04-04 01:21:08,397] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 55.66666666666667, 0.0, 0.0, 26.0, 25.3719994907346, 0.423621357890099, 0.0, 1.0, 60810.418440787], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2325000.0000, 
sim time next is 2325600.0000, 
raw observation next is [-1.7, 56.0, 0.0, 0.0, 26.0, 25.35589019679706, 0.4260993855719952, 0.0, 1.0, 49044.21485467817], 
processed observation next is [1.0, 0.9565217391304348, 0.4155124653739613, 0.56, 0.0, 0.0, 0.6666666666666666, 0.6129908497330883, 0.6420331285239984, 0.0, 1.0, 0.23354388026037223], 
reward next is 0.7665, 
noisyNet noise sample is [array([0.6374382], dtype=float32), -0.18096861]. 
=============================================
[2019-04-04 01:21:17,662] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.0713300e-22 1.4492036e-14 1.7176741e-20 2.3607102e-16 1.6812834e-16
 3.6960143e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:21:17,662] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3297
[2019-04-04 01:21:17,751] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.283333333333333, 54.33333333333333, 0.0, 0.0, 26.0, 25.11628296940174, 0.3255899255339746, 1.0, 1.0, 63860.0655605661], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2315400.0000, 
sim time next is 2316000.0000, 
raw observation next is [-1.366666666666667, 54.66666666666667, 0.0, 0.0, 26.0, 25.03396668015496, 0.3194240322362359, 0.0, 1.0, 91760.57670374578], 
processed observation next is [1.0, 0.8260869565217391, 0.42474607571560485, 0.5466666666666667, 0.0, 0.0, 0.6666666666666666, 0.5861638900129135, 0.6064746774120786, 0.0, 1.0, 0.43695512716069423], 
reward next is 0.5630, 
noisyNet noise sample is [array([1.4723334], dtype=float32), -0.6681922]. 
=============================================
[2019-04-04 01:21:17,790] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[82.43293 ]
 [82.210686]
 [82.189926]
 [82.2907  ]
 [82.368416]], R is [[79.79834747]
 [79.6962738 ]
 [79.8102951 ]
 [80.01219177]
 [80.21207428]].
[2019-04-04 01:21:27,293] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0743601e-20 3.0151746e-13 2.4567125e-20 2.9771223e-16 1.0529118e-15
 2.7894418e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:21:27,293] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5998
[2019-04-04 01:21:27,338] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.3, 26.83333333333333, 67.33333333333333, 714.6666666666666, 26.0, 24.97475612311481, 0.2816270087194306, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2473800.0000, 
sim time next is 2474400.0000, 
raw observation next is [3.3, 26.66666666666667, 64.66666666666667, 697.3333333333334, 26.0, 24.97409056221018, 0.2804224218277619, 0.0, 1.0, 18697.68870803133], 
processed observation next is [0.0, 0.6521739130434783, 0.554016620498615, 0.2666666666666667, 0.21555555555555558, 0.7705340699815838, 0.6666666666666666, 0.581174213517515, 0.593474140609254, 0.0, 1.0, 0.0890366128953873], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.32225946], dtype=float32), 0.5768754]. 
=============================================
[2019-04-04 01:21:32,687] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [8.9154510e-23 2.2533027e-15 1.3907656e-20 1.7420270e-16 1.2210164e-16
 4.9500769e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 01:21:32,707] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1852
[2019-04-04 01:21:32,791] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.2, 60.0, 0.0, 0.0, 26.0, 25.06487058647085, 0.393459150681809, 1.0, 1.0, 20888.83668794326], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2660400.0000, 
sim time next is 2661000.0000, 
raw observation next is [-1.2, 60.5, 0.0, 0.0, 26.0, 25.19194566223795, 0.3902333788554735, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.42936288088642666, 0.605, 0.0, 0.0, 0.6666666666666666, 0.5993288051864957, 0.6300777929518245, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.0469587], dtype=float32), -0.39891648]. 
=============================================
[2019-04-04 01:21:32,800] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[83.53344 ]
 [83.38225 ]
 [82.99094 ]
 [82.884796]
 [82.944565]], R is [[83.63785553]
 [83.70200348]
 [83.4250946 ]
 [82.99835968]
 [82.94116974]].
[2019-04-04 01:21:46,939] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.6483248e-21 1.9875706e-14 8.3558099e-19 1.1240686e-15 1.0604051e-15
 1.8526915e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 01:21:46,940] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0797
[2019-04-04 01:21:47,003] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 69.0, 0.0, 0.0, 26.0, 24.77660116535049, 0.2452729850617457, 0.0, 1.0, 41773.92781756782], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2596200.0000, 
sim time next is 2596800.0000, 
raw observation next is [-5.0, 70.0, 0.0, 0.0, 26.0, 24.83795776562423, 0.2503804404752119, 0.0, 1.0, 41725.97291366019], 
processed observation next is [1.0, 0.043478260869565216, 0.32409972299168976, 0.7, 0.0, 0.0, 0.6666666666666666, 0.569829813802019, 0.5834601468250706, 0.0, 1.0, 0.19869510911266758], 
reward next is 0.8013, 
noisyNet noise sample is [array([1.3694705], dtype=float32), -0.40190375]. 
=============================================
[2019-04-04 01:21:49,779] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.5263061e-21 7.1199870e-15 2.4944210e-19 2.2804901e-15 4.2660926e-16
 1.8075928e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:21:49,779] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4386
[2019-04-04 01:21:49,836] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 78.0, 0.0, 0.0, 26.0, 24.91735521974622, 0.4025858968592665, 0.0, 1.0, 49588.62481319209], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2923200.0000, 
sim time next is 2923800.0000, 
raw observation next is [-1.0, 79.16666666666667, 0.0, 0.0, 26.0, 24.95814174938439, 0.4062593651049748, 1.0, 1.0, 18966.80501071285], 
processed observation next is [1.0, 0.8695652173913043, 0.4349030470914128, 0.7916666666666667, 0.0, 0.0, 0.6666666666666666, 0.5798451457820324, 0.6354197883683249, 1.0, 1.0, 0.09031811909863262], 
reward next is 0.9097, 
noisyNet noise sample is [array([-1.5344287], dtype=float32), -1.8158727]. 
=============================================
[2019-04-04 01:22:01,036] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.0249556e-22 1.6656578e-14 1.2418760e-19 5.1502499e-16 1.7207149e-15
 1.9731420e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:22:01,036] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4197
[2019-04-04 01:22:01,041] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.8494141e-21 1.8478386e-13 3.0980163e-19 9.9850192e-16 1.1878602e-15
 1.1807462e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 01:22:01,041] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5082
[2019-04-04 01:22:01,077] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 96.5, 0.0, 0.0, 26.0, 24.94836492288863, 0.260569450836967, 0.0, 1.0, 55783.0961790892], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2871000.0000, 
sim time next is 2871600.0000, 
raw observation next is [1.0, 97.66666666666666, 0.0, 0.0, 26.0, 24.99192311093055, 0.2441483376127192, 0.0, 1.0, 55925.1354633003], 
processed observation next is [1.0, 0.21739130434782608, 0.4903047091412743, 0.9766666666666666, 0.0, 0.0, 0.6666666666666666, 0.5826602592442125, 0.5813827792042398, 0.0, 1.0, 0.2663101688728586], 
reward next is 0.7337, 
noisyNet noise sample is [array([0.03934119], dtype=float32), 0.735847]. 
=============================================
[2019-04-04 01:22:01,101] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 44.0, 0.0, 0.0, 26.0, 25.4921789834992, 0.4279612051169308, 0.0, 1.0, 24450.23853140712], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2844000.0000, 
sim time next is 2844600.0000, 
raw observation next is [2.0, 47.0, 0.0, 0.0, 26.0, 25.47773758308423, 0.3771580446542933, 0.0, 1.0, 41417.34469563999], 
processed observation next is [1.0, 0.9565217391304348, 0.518005540166205, 0.47, 0.0, 0.0, 0.6666666666666666, 0.6231447985903525, 0.6257193482180977, 0.0, 1.0, 0.197225450931619], 
reward next is 0.8028, 
noisyNet noise sample is [array([-0.14055173], dtype=float32), -0.14193028]. 
=============================================
[2019-04-04 01:22:13,648] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.3822436e-21 1.1499144e-13 4.6819663e-19 1.5385956e-15 4.3977891e-15
 1.0058433e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 01:22:13,648] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8742
[2019-04-04 01:22:13,718] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 85.00000000000001, 0.0, 0.0, 26.0, 24.94570141990914, 0.3473947106934908, 0.0, 1.0, 43352.70557267527], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2938200.0000, 
sim time next is 2938800.0000, 
raw observation next is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.90423157677807, 0.3398234318940764, 0.0, 1.0, 43355.75156073116], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.85, 0.0, 0.0, 0.6666666666666666, 0.5753526313981725, 0.6132744772980255, 0.0, 1.0, 0.20645595981300552], 
reward next is 0.7935, 
noisyNet noise sample is [array([0.30816126], dtype=float32), -0.45734936]. 
=============================================
[2019-04-04 01:22:18,338] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.3667736e-21 1.5407586e-13 4.5779251e-19 5.2406445e-15 7.6615977e-15
 9.5324785e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 01:22:18,338] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5367
[2019-04-04 01:22:18,366] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.84008398536729, 0.246376479061709, 0.0, 1.0, 37931.16879212891], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3024000.0000, 
sim time next is 3024600.0000, 
raw observation next is [-4.166666666666667, 66.0, 0.0, 0.0, 26.0, 24.80523955170657, 0.2405401219831908, 0.0, 1.0, 37891.24811552476], 
processed observation next is [0.0, 0.0, 0.3471837488457987, 0.66, 0.0, 0.0, 0.6666666666666666, 0.5671032959755475, 0.5801800406610637, 0.0, 1.0, 0.18043451483583217], 
reward next is 0.8196, 
noisyNet noise sample is [array([0.17060696], dtype=float32), -0.6278292]. 
=============================================
[2019-04-04 01:22:44,010] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [3.1357855e-22 3.3323513e-15 1.6539400e-19 6.8298469e-16 4.5104150e-16
 1.5201438e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:22:44,011] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.5796
[2019-04-04 01:22:44,069] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.166666666666667, 100.0, 0.0, 0.0, 26.0, 25.93355090874136, 0.6830744519518244, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3181800.0000, 
sim time next is 3182400.0000, 
raw observation next is [3.0, 100.0, 0.0, 0.0, 26.0, 25.82135291521322, 0.6701807175269593, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.5457063711911359, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6517794096011018, 0.7233935725089865, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.45008847], dtype=float32), 0.5274127]. 
=============================================
[2019-04-04 01:22:57,207] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-04 01:22:57,214] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 01:22:57,232] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:22:57,232] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 01:22:57,231] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 01:22:57,232] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:22:57,234] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:22:57,236] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run25
[2019-04-04 01:22:57,282] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run25
[2019-04-04 01:22:57,317] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run25
[2019-04-04 01:23:12,642] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.36400276], dtype=float32), 0.1730678]
[2019-04-04 01:23:12,642] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-7.064585561499999, 61.89590507833334, 13.47183344666667, 104.4098494, 26.0, 25.38121789478602, 0.3340325991753561, 1.0, 1.0, 41125.25007762538]
[2019-04-04 01:23:12,642] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 01:23:12,643] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.1973734e-21 3.3645541e-14 6.5227676e-20 6.7428651e-16 6.8245344e-16
 2.3922792e-21 1.0000000e+00], sampled 0.33053382458897074
[2019-04-04 01:24:12,517] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.36400276], dtype=float32), 0.1730678]
[2019-04-04 01:24:12,517] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-6.165147209000001, 79.63059891333333, 142.0249721, 0.0, 26.0, 25.05385736702329, 0.2770876500962851, 0.0, 1.0, 35325.52787185458]
[2019-04-04 01:24:12,517] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 01:24:12,517] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.1138003e-20 4.6860280e-13 4.7323153e-19 2.8817871e-15 2.7206575e-15
 4.6751999e-20 1.0000000e+00], sampled 0.786253754715322
[2019-04-04 01:25:11,199] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 01:25:31,024] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 01:25:33,004] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.36400276], dtype=float32), 0.1730678]
[2019-04-04 01:25:33,004] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [5.453941924333334, 15.54479492333333, 0.0, 0.0, 26.0, 26.21634984757777, 0.5807054888343467, 0.0, 1.0, 0.0]
[2019-04-04 01:25:33,004] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 01:25:33,005] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.0691923e-21 7.5690776e-14 9.2562693e-20 4.2533045e-16 1.7690828e-15
 5.1822021e-21 1.0000000e+00], sampled 0.7641676351732584
[2019-04-04 01:25:33,053] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 01:25:34,076] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 2400000, evaluation results [2400000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 01:25:41,083] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.6324577e-22 7.4087045e-14 1.8028156e-19 4.0194736e-16 4.8314987e-16
 2.1657420e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:25:41,084] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2812
[2019-04-04 01:25:41,102] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.166666666666667, 72.0, 0.0, 0.0, 26.0, 24.75325045217686, 0.2400841941741658, 0.0, 1.0, 42152.43364872318], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3387000.0000, 
sim time next is 3387600.0000, 
raw observation next is [-5.0, 71.0, 0.0, 0.0, 26.0, 24.74658514334127, 0.2482205678714617, 0.0, 1.0, 42285.4112472412], 
processed observation next is [1.0, 0.21739130434782608, 0.32409972299168976, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5622154286117725, 0.5827401892904872, 0.0, 1.0, 0.20135910117733904], 
reward next is 0.7986, 
noisyNet noise sample is [array([0.34113616], dtype=float32), -0.60756814]. 
=============================================
[2019-04-04 01:25:43,678] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.1807715e-22 3.7764807e-15 6.1533837e-21 2.2168108e-16 2.0574228e-16
 9.3096915e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 01:25:43,678] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0396
[2019-04-04 01:25:43,721] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.0, 64.0, 113.5, 769.0, 26.0, 26.37756357866645, 0.595245898753449, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3322800.0000, 
sim time next is 3323400.0000, 
raw observation next is [-6.833333333333334, 62.33333333333334, 114.3333333333333, 778.6666666666667, 26.0, 26.3765979228646, 0.5984215294437801, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.27331486611265005, 0.6233333333333334, 0.381111111111111, 0.8604051565377533, 0.6666666666666666, 0.6980498269053834, 0.6994738431479267, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5764701], dtype=float32), -1.2075485]. 
=============================================
[2019-04-04 01:25:48,362] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.8019199e-21 7.2300727e-14 7.9383554e-20 1.1501283e-15 8.2408201e-16
 3.2215714e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:25:48,362] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0567
[2019-04-04 01:25:48,418] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 55.0, 0.0, 0.0, 26.0, 25.45556875562222, 0.4771929822832314, 0.0, 1.0, 25682.42665131432], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3353400.0000, 
sim time next is 3354000.0000, 
raw observation next is [-3.0, 55.0, 0.0, 0.0, 26.0, 25.3598477633458, 0.462259774709489, 1.0, 1.0, 25288.52036596868], 
processed observation next is [1.0, 0.8260869565217391, 0.3795013850415513, 0.55, 0.0, 0.0, 0.6666666666666666, 0.6133206469454834, 0.6540865915698296, 1.0, 1.0, 0.12042152555223182], 
reward next is 0.8796, 
noisyNet noise sample is [array([-0.6696825], dtype=float32), 0.22512339]. 
=============================================
[2019-04-04 01:25:48,434] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[80.69519 ]
 [81.60632 ]
 [83.752525]
 [83.61464 ]
 [83.450195]], R is [[82.80238342]
 [82.85205841]
 [82.89419556]
 [82.91456604]
 [82.88274384]].
[2019-04-04 01:25:49,475] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.3680334e-22 4.4171413e-15 3.6205230e-20 1.9613501e-16 5.0781647e-16
 7.0363136e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:25:49,476] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6616
[2019-04-04 01:25:49,513] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.666666666666666, 69.0, 0.0, 0.0, 26.0, 25.32545850877167, 0.4243059700441039, 0.0, 1.0, 66761.02246608761], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3364800.0000, 
sim time next is 3365400.0000, 
raw observation next is [-4.833333333333334, 70.0, 0.0, 0.0, 26.0, 25.24984955573755, 0.4146380922671717, 0.0, 1.0, 53192.88335064634], 
processed observation next is [1.0, 0.9565217391304348, 0.32871652816251157, 0.7, 0.0, 0.0, 0.6666666666666666, 0.604154129644796, 0.6382126974223906, 0.0, 1.0, 0.25329944452688735], 
reward next is 0.7467, 
noisyNet noise sample is [array([-0.695469], dtype=float32), 0.23330635]. 
=============================================
[2019-04-04 01:26:04,019] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.3053412e-22 8.9183150e-14 4.5015363e-20 3.7886717e-16 5.5998492e-16
 1.4076741e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:26:04,020] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1901
[2019-04-04 01:26:04,082] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.0, 43.66666666666667, 107.0, 790.0, 26.0, 25.37336021540277, 0.4694607535598136, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3679800.0000, 
sim time next is 3680400.0000, 
raw observation next is [6.0, 44.33333333333334, 105.5, 783.0, 26.0, 25.38963828951396, 0.4752304678586364, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.6288088642659281, 0.4433333333333334, 0.3516666666666667, 0.8651933701657458, 0.6666666666666666, 0.6158031907928301, 0.6584101559528788, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.67537284], dtype=float32), 0.52922845]. 
=============================================
[2019-04-04 01:26:19,140] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.0326614e-21 1.3964800e-13 8.4482082e-20 2.9969487e-16 1.5193079e-15
 7.1250462e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:26:19,141] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1003
[2019-04-04 01:26:19,169] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.6666666666666667, 68.66666666666667, 0.0, 0.0, 26.0, 25.45375880727784, 0.4048932638732558, 0.0, 1.0, 89476.66854211177], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3706800.0000, 
sim time next is 3707400.0000, 
raw observation next is [0.3333333333333333, 70.33333333333333, 0.0, 0.0, 26.0, 25.42037969986173, 0.4093641586891285, 0.0, 1.0, 76936.83655967977], 
processed observation next is [0.0, 0.9130434782608695, 0.4718374884579871, 0.7033333333333333, 0.0, 0.0, 0.6666666666666666, 0.6183649749884775, 0.6364547195630429, 0.0, 1.0, 0.36636588837942746], 
reward next is 0.6336, 
noisyNet noise sample is [array([0.77610993], dtype=float32), 1.8425964]. 
=============================================
[2019-04-04 01:26:19,190] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2920452e-21 2.1456193e-14 2.8234350e-19 4.5306945e-16 2.1178562e-15
 2.6881116e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:26:19,190] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5587
[2019-04-04 01:26:19,237] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 77.0, 0.0, 0.0, 26.0, 25.30862239525782, 0.4026604670861376, 0.0, 1.0, 44162.28474514392], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3807000.0000, 
sim time next is 3807600.0000, 
raw observation next is [-4.0, 77.0, 0.0, 0.0, 26.0, 25.3331287410855, 0.4078558128378999, 0.0, 1.0, 44115.14999956332], 
processed observation next is [1.0, 0.043478260869565216, 0.3518005540166205, 0.77, 0.0, 0.0, 0.6666666666666666, 0.6110940617571249, 0.6359519376126334, 0.0, 1.0, 0.21007214285506343], 
reward next is 0.7899, 
noisyNet noise sample is [array([0.83096343], dtype=float32), 1.7638035]. 
=============================================
[2019-04-04 01:26:19,920] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.5246223e-23 1.3174162e-16 6.4523046e-22 1.6548000e-17 1.3307246e-17
 4.4626469e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 01:26:19,921] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3368
[2019-04-04 01:26:19,954] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.0, 35.0, 98.0, 728.0, 26.0, 27.15546292257693, 0.7817550602941606, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4115400.0000, 
sim time next is 4116000.0000, 
raw observation next is [4.0, 35.0, 96.0, 711.5, 26.0, 27.22294263741497, 0.6742537293696985, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5734072022160666, 0.35, 0.32, 0.7861878453038674, 0.6666666666666666, 0.7685785531179142, 0.7247512431232329, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3292143], dtype=float32), 1.173286]. 
=============================================
[2019-04-04 01:26:19,961] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[89.78761 ]
 [89.814545]
 [89.90385 ]
 [90.09836 ]
 [90.12802 ]], R is [[89.62194824]
 [89.7257309 ]
 [89.82847595]
 [89.93019104]
 [90.03089142]].
[2019-04-04 01:26:37,081] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3069812e-21 1.2487419e-13 1.2481832e-19 4.4262644e-16 3.0312713e-15
 9.0506435e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:26:37,082] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5082
[2019-04-04 01:26:37,094] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.1, 73.0, 0.0, 0.0, 26.0, 25.78695217962955, 0.4490501155051628, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4309200.0000, 
sim time next is 4309800.0000, 
raw observation next is [5.05, 73.5, 0.0, 0.0, 26.0, 25.74505424664915, 0.4370453933015619, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.9130434782608695, 0.6024930747922439, 0.735, 0.0, 0.0, 0.6666666666666666, 0.6454211872207626, 0.6456817977671873, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8174258], dtype=float32), -0.0750439]. 
=============================================
[2019-04-04 01:26:38,461] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.1999025e-22 7.8352428e-15 2.2367597e-20 2.9214333e-16 3.8951088e-16
 5.4370136e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:26:38,462] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5318
[2019-04-04 01:26:38,499] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 37.33333333333334, 0.0, 0.0, 26.0, 25.92050040810451, 0.5908294607137937, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4137600.0000, 
sim time next is 4138200.0000, 
raw observation next is [1.0, 38.0, 0.0, 0.0, 26.0, 25.93830672474444, 0.5846819700331713, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.4903047091412743, 0.38, 0.0, 0.0, 0.6666666666666666, 0.6615255603953699, 0.6948939900110571, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8691026], dtype=float32), -0.8084341]. 
=============================================
[2019-04-04 01:26:42,543] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.1985953e-23 6.9734063e-16 7.0297670e-22 1.9521231e-17 6.4337856e-18
 1.3670507e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 01:26:42,543] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3425
[2019-04-04 01:26:42,553] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.5, 33.0, 106.0, 794.0, 26.0, 26.9785602324479, 0.7308986913131911, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4113000.0000, 
sim time next is 4113600.0000, 
raw observation next is [3.666666666666667, 33.66666666666666, 104.0, 777.5, 26.0, 26.97740413264479, 0.7390518460133914, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.564173591874423, 0.33666666666666656, 0.3466666666666667, 0.8591160220994475, 0.6666666666666666, 0.7481170110537324, 0.7463506153377971, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5125815], dtype=float32), 0.6605837]. 
=============================================
[2019-04-04 01:26:45,221] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.8468921e-21 2.0071540e-12 1.8535629e-18 2.0137389e-15 1.1352821e-14
 2.6011695e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 01:26:45,221] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1683
[2019-04-04 01:26:45,272] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 49.0, 0.0, 0.0, 26.0, 24.62697108627293, 0.2093710124692632, 0.0, 1.0, 40164.62108737577], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4172400.0000, 
sim time next is 4173000.0000, 
raw observation next is [-5.0, 49.83333333333334, 0.0, 0.0, 26.0, 24.58568081293782, 0.2222451645639958, 0.0, 1.0, 40565.53575282174], 
processed observation next is [0.0, 0.30434782608695654, 0.32409972299168976, 0.4983333333333334, 0.0, 0.0, 0.6666666666666666, 0.548806734411485, 0.574081721521332, 0.0, 1.0, 0.19316921787057972], 
reward next is 0.8068, 
noisyNet noise sample is [array([1.1132114], dtype=float32), 1.7410465]. 
=============================================
[2019-04-04 01:26:45,312] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[81.731285]
 [81.826195]
 [81.91972 ]
 [82.00734 ]
 [82.08474 ]], R is [[81.69213104]
 [81.68395233]
 [81.6763916 ]
 [81.66943359]
 [81.66303253]].
[2019-04-04 01:26:52,283] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.2463293e-22 1.4809243e-14 4.4269815e-20 1.3280838e-15 5.4154406e-16
 4.6024498e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:26:52,283] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5200
[2019-04-04 01:26:52,295] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 38.66666666666666, 0.0, 0.0, 26.0, 25.93247760267069, 0.5794369940232654, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4138800.0000, 
sim time next is 4139400.0000, 
raw observation next is [1.0, 39.33333333333334, 0.0, 0.0, 26.0, 25.93131338103092, 0.5701738103436694, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.4903047091412743, 0.3933333333333334, 0.0, 0.0, 0.6666666666666666, 0.6609427817525768, 0.6900579367812232, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.2600956], dtype=float32), 1.3729978]. 
=============================================
[2019-04-04 01:27:00,595] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.0389366e-22 2.6334936e-15 3.1088944e-20 1.3575462e-16 3.9932175e-16
 5.2039511e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:27:00,597] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4338
[2019-04-04 01:27:00,685] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.03110691416811, 0.452150011995568, 0.0, 1.0, 43980.62172635389], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4479000.0000, 
sim time next is 4479600.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.00132734988586, 0.4608037407030645, 0.0, 1.0, 198689.6266193342], 
processed observation next is [1.0, 0.8695652173913043, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5834439458238215, 0.6536012469010215, 0.0, 1.0, 0.9461410791396866], 
reward next is 0.0539, 
noisyNet noise sample is [array([0.16706817], dtype=float32), 2.2145224]. 
=============================================
[2019-04-04 01:27:05,565] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.4140702e-22 8.6691396e-14 4.2221929e-20 3.9016373e-16 1.0443792e-15
 1.5283655e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:27:05,565] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8026
[2019-04-04 01:27:05,615] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.666666666666667, 54.0, 134.8333333333333, 785.6666666666666, 26.0, 25.23198840453338, 0.3955656863944099, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4274400.0000, 
sim time next is 4275000.0000, 
raw observation next is [6.0, 53.5, 121.0, 822.0, 26.0, 25.21759589717154, 0.3952088647809995, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.6288088642659281, 0.535, 0.4033333333333333, 0.9082872928176795, 0.6666666666666666, 0.6014663247642952, 0.6317362882603331, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5586937], dtype=float32), 1.4773942]. 
=============================================
[2019-04-04 01:27:05,627] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[88.629395]
 [88.69032 ]
 [88.88315 ]
 [89.32332 ]
 [89.954216]], R is [[88.70102692]
 [88.81401825]
 [88.92588043]
 [89.03662109]
 [89.14625549]].
[2019-04-04 01:27:11,310] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.4614884e-23 4.0374706e-15 6.3668055e-21 6.8280980e-17 3.5301759e-17
 1.0225827e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:27:11,310] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1910
[2019-04-04 01:27:11,356] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 86.0, 214.6666666666667, 97.33333333333334, 26.0, 26.48342636703084, 0.6621030946930381, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4445400.0000, 
sim time next is 4446000.0000, 
raw observation next is [1.0, 86.0, 196.5, 73.0, 26.0, 26.50146284245206, 0.6608231045004946, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4903047091412743, 0.86, 0.655, 0.08066298342541436, 0.6666666666666666, 0.708455236871005, 0.7202743681668315, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.40413097], dtype=float32), 1.8322446]. 
=============================================
[2019-04-04 01:27:11,429] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[86.17348 ]
 [87.37157 ]
 [88.353676]
 [88.40691 ]
 [88.58844 ]], R is [[85.45726776]
 [85.60269928]
 [85.74667358]
 [85.88920593]
 [86.03031158]].
[2019-04-04 01:27:23,675] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.9607976e-22 3.6003690e-15 1.6225246e-20 2.3892849e-16 1.2388497e-16
 1.5779755e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:27:23,675] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9027
[2019-04-04 01:27:23,687] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.60474089722592, 0.5650338282331148, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4483800.0000, 
sim time next is 4484400.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.58643883072681, 0.5545549333474481, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6322032358939008, 0.6848516444491494, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6455017], dtype=float32), 0.11417504]. 
=============================================
[2019-04-04 01:27:39,941] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.41434241e-22 2.15744522e-15 5.67265641e-21 1.10978204e-16
 1.12768434e-16 4.88010466e-23 1.00000000e+00], sum to 1.0000
[2019-04-04 01:27:39,942] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7354
[2019-04-04 01:27:39,978] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.733333333333333, 44.0, 130.0, 144.0, 26.0, 26.269570764432, 0.7449678769019701, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4638000.0000, 
sim time next is 4638600.0000, 
raw observation next is [5.6, 44.5, 117.0, 147.0, 26.0, 26.74585545941523, 0.7954558550026151, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.6177285318559557, 0.445, 0.39, 0.16243093922651933, 0.6666666666666666, 0.7288212882846024, 0.7651519516675384, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.8230923], dtype=float32), 1.0043421]. 
=============================================
[2019-04-04 01:27:44,781] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:27:44,781] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:27:44,792] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run19
[2019-04-04 01:27:51,165] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:27:51,165] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:27:51,190] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run19
[2019-04-04 01:27:55,155] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.6917021e-20 8.5664745e-14 1.1885376e-19 1.4672032e-15 1.9371110e-15
 9.4785907e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:27:55,155] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4330
[2019-04-04 01:27:55,208] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.8333333333333334, 36.5, 0.0, 0.0, 26.0, 25.43935808997933, 0.3625531793944313, 0.0, 1.0, 37280.64286759164], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4918200.0000, 
sim time next is 4918800.0000, 
raw observation next is [0.6666666666666667, 37.0, 0.0, 0.0, 26.0, 25.42808167324989, 0.3662770259017269, 0.0, 1.0, 42986.36922649428], 
processed observation next is [0.0, 0.9565217391304348, 0.4810710987996307, 0.37, 0.0, 0.0, 0.6666666666666666, 0.6190068061041574, 0.6220923419672423, 0.0, 1.0, 0.20469699631663943], 
reward next is 0.7953, 
noisyNet noise sample is [array([-0.6111515], dtype=float32), -0.51550674]. 
=============================================
[2019-04-04 01:28:01,225] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:28:01,225] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:28:01,228] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run19
[2019-04-04 01:28:06,257] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.4412742e-24 1.8720331e-16 2.9005701e-22 1.1729545e-17 1.9451495e-18
 1.8011337e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 01:28:06,257] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7100
[2019-04-04 01:28:06,297] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.333333333333334, 25.66666666666667, 123.8333333333333, 861.6666666666667, 26.0, 27.45340061938646, 0.6675940579443592, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5055600.0000, 
sim time next is 5056200.0000, 
raw observation next is [8.5, 25.5, 124.0, 865.0, 26.0, 27.44755140689433, 0.8705603298880525, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.698060941828255, 0.255, 0.41333333333333333, 0.9558011049723757, 0.6666666666666666, 0.7872959505745275, 0.7901867766293509, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7640253], dtype=float32), 1.3135499]. 
=============================================
[2019-04-04 01:28:11,181] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:28:11,181] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:28:11,190] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run19
[2019-04-04 01:28:14,068] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:28:14,068] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:28:14,078] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run19
[2019-04-04 01:28:15,109] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:28:15,109] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:28:15,112] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run19
[2019-04-04 01:28:15,453] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:28:15,454] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:28:15,459] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run19
[2019-04-04 01:28:16,180] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:28:16,180] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:28:16,184] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run19
[2019-04-04 01:28:16,775] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.5675789e-21 1.0496835e-15 2.4843822e-20 8.2565807e-17 2.7603078e-16
 2.3119362e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:28:16,776] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.3632
[2019-04-04 01:28:16,789] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.0, 19.0, 0.0, 0.0, 26.0, 27.48734861998543, 0.9290641152306298, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5083200.0000, 
sim time next is 5083800.0000, 
raw observation next is [9.833333333333334, 19.0, 0.0, 0.0, 26.0, 27.40956879010525, 0.9148982539520637, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.7349953831948293, 0.19, 0.0, 0.0, 0.6666666666666666, 0.7841307325087709, 0.804966084650688, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.36510408], dtype=float32), -0.8557314]. 
=============================================
[2019-04-04 01:28:19,221] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:28:19,221] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:28:19,224] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run19
[2019-04-04 01:28:20,717] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:28:20,717] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:28:20,720] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run19
[2019-04-04 01:28:24,546] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:28:24,546] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:28:24,550] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run19
[2019-04-04 01:28:26,657] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:28:26,657] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:28:26,660] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run19
[2019-04-04 01:28:26,724] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:28:26,724] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:28:26,735] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run19
[2019-04-04 01:28:34,361] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:28:34,362] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:28:34,365] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run19
[2019-04-04 01:28:37,686] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:28:37,686] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:28:37,697] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run19
[2019-04-04 01:28:39,352] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.7228110e-22 1.7068757e-14 1.7421763e-20 1.9198662e-16 1.4877003e-16
 1.9890566e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:28:39,352] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3691
[2019-04-04 01:28:39,444] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.5, 43.0, 82.0, 623.0, 26.0, 26.34605469163317, 0.4093901572061058, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 311400.0000, 
sim time next is 312000.0000, 
raw observation next is [-9.5, 42.66666666666667, 80.0, 598.6666666666667, 26.0, 25.74361116269202, 0.4019455470529199, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.1994459833795014, 0.4266666666666667, 0.26666666666666666, 0.661510128913444, 0.6666666666666666, 0.645300930224335, 0.63398184901764, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.15414], dtype=float32), -0.023816492]. 
=============================================
[2019-04-04 01:28:39,550] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[79.05103 ]
 [79.12134 ]
 [79.154945]
 [79.23924 ]
 [79.44623 ]], R is [[78.88299561]
 [79.09416962]
 [79.30323029]
 [79.5102005 ]
 [79.71509552]].
[2019-04-04 01:28:40,069] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:28:40,069] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:28:40,077] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run19
[2019-04-04 01:28:44,773] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.6224186e-21 1.7250536e-14 1.1029643e-19 2.9183258e-16 6.3007409e-16
 1.7317660e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:28:44,774] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4494
[2019-04-04 01:28:44,788] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.4, 70.5, 0.0, 0.0, 26.0, 24.34317931302077, 0.1537389125694532, 0.0, 1.0, 45709.14642020054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 165000.0000, 
sim time next is 165600.0000, 
raw observation next is [-8.4, 71.0, 0.0, 0.0, 26.0, 24.29287266623632, 0.1440526799793897, 0.0, 1.0, 45292.55173545384], 
processed observation next is [1.0, 0.9565217391304348, 0.2299168975069252, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5244060555196933, 0.54801755999313, 0.0, 1.0, 0.21567881778787543], 
reward next is 0.7843, 
noisyNet noise sample is [array([-0.09072252], dtype=float32), -2.2423892]. 
=============================================
[2019-04-04 01:29:09,806] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.4695097e-23 2.2522199e-15 1.8736082e-20 6.3295252e-17 2.2500879e-17
 2.4995895e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:29:09,806] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4822
[2019-04-04 01:29:09,847] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 68.5, 153.0, 0.0, 26.0, 25.27870619666147, 0.2277688850628579, 1.0, 1.0, 25380.31838272041], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 214200.0000, 
sim time next is 214800.0000, 
raw observation next is [-5.4, 67.33333333333333, 149.0, 0.0, 26.0, 25.29167289181696, 0.2285559052733439, 1.0, 1.0, 24853.41698544394], 
processed observation next is [1.0, 0.4782608695652174, 0.31301939058171746, 0.6733333333333333, 0.49666666666666665, 0.0, 0.6666666666666666, 0.6076394076514134, 0.5761853017577813, 1.0, 1.0, 0.11834960469259019], 
reward next is 0.8817, 
noisyNet noise sample is [array([-0.87361014], dtype=float32), 0.6985842]. 
=============================================
[2019-04-04 01:29:19,093] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.7516910e-22 6.1530533e-14 3.3766046e-20 2.6300185e-16 3.3008845e-16
 2.8556351e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:29:19,094] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2631
[2019-04-04 01:29:19,241] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-16.45, 79.5, 0.0, 0.0, 26.0, 21.92792010392076, -0.3418148870755788, 1.0, 1.0, 203358.3625419095], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 372600.0000, 
sim time next is 373200.0000, 
raw observation next is [-16.53333333333333, 80.0, 0.0, 0.0, 26.0, 22.68914621317499, -0.2505289343859654, 1.0, 1.0, 169295.3726917159], 
processed observation next is [1.0, 0.30434782608695654, 0.0046168051708218244, 0.8, 0.0, 0.0, 0.6666666666666666, 0.39076218443124916, 0.4164903552046782, 1.0, 1.0, 0.8061684413891232], 
reward next is 0.1938, 
noisyNet noise sample is [array([0.00237251], dtype=float32), -1.4177667]. 
=============================================
[2019-04-04 01:29:34,677] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.5173687e-22 1.0978124e-14 1.7435592e-20 2.4026563e-16 1.6378756e-16
 5.7988061e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 01:29:34,678] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3111
[2019-04-04 01:29:34,753] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.7, 46.83333333333334, 52.66666666666667, 868.6666666666666, 26.0, 26.26040099820711, 0.5108370374702098, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 395400.0000, 
sim time next is 396000.0000, 
raw observation next is [-10.5, 46.0, 51.5, 859.5, 26.0, 26.35091430048618, 0.5154388282080333, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.17174515235457063, 0.46, 0.17166666666666666, 0.9497237569060774, 0.6666666666666666, 0.6959095250405151, 0.6718129427360111, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.3967943], dtype=float32), -2.541114]. 
=============================================
[2019-04-04 01:29:34,787] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[80.003586]
 [80.16855 ]
 [80.37836 ]
 [80.47788 ]
 [80.34812 ]], R is [[80.09374237]
 [80.29280853]
 [80.48988342]
 [80.6849823 ]
 [80.5568161 ]].
[2019-04-04 01:29:38,164] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.6189504e-20 3.2716787e-14 7.3611041e-19 2.1459211e-15 2.3562309e-15
 2.7699432e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 01:29:38,164] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8924
[2019-04-04 01:29:38,276] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-16.28333333333333, 78.5, 0.0, 0.0, 26.0, 21.58261355467166, -0.5246877354090199, 0.0, 1.0, 49481.77494666538], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 371400.0000, 
sim time next is 372000.0000, 
raw observation next is [-16.36666666666667, 79.0, 0.0, 0.0, 26.0, 21.49329037909963, -0.4405880206216344, 1.0, 1.0, 202242.6109366239], 
processed observation next is [1.0, 0.30434782608695654, 0.009233610341643453, 0.79, 0.0, 0.0, 0.6666666666666666, 0.2911075315916358, 0.3531373264594552, 1.0, 1.0, 0.9630600520791615], 
reward next is 0.0369, 
noisyNet noise sample is [array([0.10386831], dtype=float32), 0.68647796]. 
=============================================
[2019-04-04 01:29:38,285] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[76.917656]
 [76.95445 ]
 [76.95286 ]
 [76.979515]
 [77.00756 ]], R is [[79.2230072 ]
 [79.19515228]
 [79.16808319]
 [79.14172363]
 [79.11625671]].
[2019-04-04 01:29:55,717] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4603417e-21 4.7648063e-14 9.8707786e-20 9.1399725e-16 4.3714605e-16
 4.3220534e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:29:55,717] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9327
[2019-04-04 01:29:55,772] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 71.0, 0.0, 0.0, 26.0, 24.60917921981412, 0.1329700189053568, 0.0, 1.0, 41513.82801797466], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 688200.0000, 
sim time next is 688800.0000, 
raw observation next is [-3.9, 71.0, 0.0, 0.0, 26.0, 24.56899298698391, 0.126161346828298, 0.0, 1.0, 41448.69076253452], 
processed observation next is [0.0, 1.0, 0.3545706371191136, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5474160822486592, 0.5420537822760993, 0.0, 1.0, 0.19737471791683103], 
reward next is 0.8026, 
noisyNet noise sample is [array([0.06919315], dtype=float32), 0.31367704]. 
=============================================
[2019-04-04 01:30:05,391] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.85248142e-21 1.21966764e-14 1.56839390e-19 6.37807657e-16
 3.39145666e-16 5.72967977e-21 1.00000000e+00], sum to 1.0000
[2019-04-04 01:30:05,391] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0910
[2019-04-04 01:30:05,452] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.283333333333333, 83.66666666666667, 60.66666666666666, 54.33333333333333, 26.0, 24.98607294927492, 0.3125817838515922, 0.0, 1.0, 34065.14333365833], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 576600.0000, 
sim time next is 577200.0000, 
raw observation next is [-1.366666666666667, 84.33333333333334, 50.83333333333334, 49.66666666666666, 26.0, 24.98679180957946, 0.3082220422158882, 0.0, 1.0, 35267.1675885884], 
processed observation next is [0.0, 0.6956521739130435, 0.42474607571560485, 0.8433333333333334, 0.16944444444444448, 0.05488029465930017, 0.6666666666666666, 0.5822326507982885, 0.6027406807386294, 0.0, 1.0, 0.1679388932789924], 
reward next is 0.8321, 
noisyNet noise sample is [array([-0.2935181], dtype=float32), -1.1187189]. 
=============================================
[2019-04-04 01:30:10,170] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.6378229e-23 1.7596274e-14 2.2309469e-21 1.8595624e-17 7.5030576e-18
 2.3086755e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:30:10,172] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1846
[2019-04-04 01:30:10,184] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [18.11666666666667, 65.33333333333334, 140.0, 0.0, 26.0, 25.2710793318689, 0.5200015568447952, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1162200.0000, 
sim time next is 1162800.0000, 
raw observation next is [18.3, 65.0, 145.0, 0.0, 26.0, 25.23942585825207, 0.514923305453996, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.9695290858725764, 0.65, 0.48333333333333334, 0.0, 0.6666666666666666, 0.6032854881876725, 0.6716411018179986, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9538602], dtype=float32), -0.20900692]. 
=============================================
[2019-04-04 01:30:11,686] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [9.8322385e-24 3.2743099e-16 7.5075092e-22 6.9765654e-18 1.3775883e-17
 2.3892397e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 01:30:11,687] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0962
[2019-04-04 01:30:11,731] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.233333333333333, 84.33333333333334, 0.0, 0.0, 26.0, 25.36186286442709, 0.4492530964910428, 0.0, 1.0, 42729.50003464293], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 956400.0000, 
sim time next is 957000.0000, 
raw observation next is [6.416666666666666, 83.16666666666667, 0.0, 0.0, 26.0, 25.45991181375604, 0.4564619513085434, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.6403508771929826, 0.8316666666666667, 0.0, 0.0, 0.6666666666666666, 0.6216593178130033, 0.6521539837695145, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.55285174], dtype=float32), 0.72785413]. 
=============================================
[2019-04-04 01:30:11,735] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[86.82835 ]
 [86.64013 ]
 [86.53342 ]
 [86.44589 ]
 [86.361565]], R is [[86.96497345]
 [86.8918457 ]
 [86.88709259]
 [86.83740997]
 [86.78808594]].
[2019-04-04 01:30:13,028] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.2789076e-22 4.8981971e-15 5.9346966e-21 2.8129345e-17 5.9526657e-17
 6.4888076e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 01:30:13,029] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4359
[2019-04-04 01:30:13,057] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.9, 62.5, 0.0, 0.0, 26.0, 24.90347920361462, 0.2770424011899555, 0.0, 1.0, 44053.95495300784], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 768600.0000, 
sim time next is 769200.0000, 
raw observation next is [-6.0, 63.0, 0.0, 0.0, 26.0, 24.84645286189493, 0.2676150435246745, 0.0, 1.0, 43632.70865133088], 
processed observation next is [1.0, 0.9130434782608695, 0.296398891966759, 0.63, 0.0, 0.0, 0.6666666666666666, 0.5705377384912443, 0.5892050145082248, 0.0, 1.0, 0.20777480310157562], 
reward next is 0.7922, 
noisyNet noise sample is [array([-0.08929715], dtype=float32), 1.7439709]. 
=============================================
[2019-04-04 01:30:22,180] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.9436231e-24 3.6943199e-16 2.1182607e-22 1.2331386e-18 6.6792885e-19
 1.1807123e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 01:30:22,180] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6870
[2019-04-04 01:30:22,257] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.9, 92.16666666666667, 12.0, 0.0, 26.0, 24.51399361889501, 0.310557693130901, 1.0, 1.0, 196475.6107603647], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 924600.0000, 
sim time next is 925200.0000, 
raw observation next is [5.0, 92.0, 9.0, 0.0, 26.0, 25.00417326812636, 0.3543358689796134, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.6011080332409973, 0.92, 0.03, 0.0, 0.6666666666666666, 0.5836811056771968, 0.6181119563265378, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.09936848], dtype=float32), -1.2983679]. 
=============================================
[2019-04-04 01:30:22,693] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [5.62695162e-26 9.28977760e-18 1.11192224e-23 4.38354155e-20
 6.48572414e-20 1.21789869e-25 1.00000000e+00], sum to 1.0000
[2019-04-04 01:30:22,714] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7291
[2019-04-04 01:30:22,752] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [18.55, 49.5, 35.0, 0.0, 26.0, 27.85850073034734, 1.014135601800239, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1096200.0000, 
sim time next is 1096800.0000, 
raw observation next is [18.26666666666667, 49.66666666666666, 29.33333333333334, 0.4999999999999999, 26.0, 27.90586038241105, 1.02158701235903, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.968605724838412, 0.4966666666666666, 0.0977777777777778, 0.0005524861878453037, 0.6666666666666666, 0.8254883652009207, 0.8405290041196767, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5916187], dtype=float32), -0.25986695]. 
=============================================
[2019-04-04 01:30:23,494] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.6827973e-24 5.7268972e-16 1.1393129e-21 4.5960554e-18 1.2566504e-17
 4.1272600e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 01:30:23,496] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2098
[2019-04-04 01:30:23,511] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.8, 83.0, 0.0, 0.0, 26.0, 25.45994589277844, 0.467837792626999, 0.0, 1.0, 40248.97528848458], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 971400.0000, 
sim time next is 972000.0000, 
raw observation next is [8.8, 83.0, 0.0, 0.0, 26.0, 25.57205301863976, 0.4794603013826313, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.7063711911357342, 0.83, 0.0, 0.0, 0.6666666666666666, 0.63100441821998, 0.6598201004608771, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2964956], dtype=float32), 1.0378543]. 
=============================================
[2019-04-04 01:30:23,595] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[89.93935]
 [89.92324]
 [89.84555]
 [89.88608]
 [89.97765]], R is [[89.92259979]
 [89.83171844]
 [89.67157745]
 [89.68557739]
 [89.78871918]].
[2019-04-04 01:30:39,135] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.5406134e-26 1.5100326e-17 2.2821169e-23 5.2496382e-19 6.6176595e-19
 1.1828530e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 01:30:39,146] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0161
[2019-04-04 01:30:39,232] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.31666666666667, 76.0, 46.33333333333334, 0.0, 26.0, 26.89543637496941, 0.7494416080042688, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1007400.0000, 
sim time next is 1008000.0000, 
raw observation next is [15.5, 75.0, 41.0, 0.0, 26.0, 26.97290538526313, 0.5170703458668701, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.8919667590027703, 0.75, 0.13666666666666666, 0.0, 0.6666666666666666, 0.747742115438594, 0.6723567819556234, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7369622], dtype=float32), -1.3703222]. 
=============================================
[2019-04-04 01:30:39,306] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[93.90458 ]
 [93.849815]
 [93.79126 ]
 [93.71963 ]
 [93.68218 ]], R is [[93.76039886]
 [93.82279205]
 [93.88456726]
 [93.94572449]
 [94.00627136]].
[2019-04-04 01:30:42,387] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.9101271e-25 6.6910263e-17 9.5135376e-23 6.8859777e-19 9.2572274e-19
 2.8823457e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 01:30:42,389] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2173
[2019-04-04 01:30:42,465] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.5, 75.0, 57.0, 0.0, 26.0, 25.72568130341499, 0.6132239800302134, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1155600.0000, 
sim time next is 1156200.0000, 
raw observation next is [15.78333333333333, 73.66666666666667, 65.66666666666667, 0.0, 26.0, 25.70930326393778, 0.6150558387798492, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.8998153277931671, 0.7366666666666667, 0.2188888888888889, 0.0, 0.6666666666666666, 0.6424419386614817, 0.7050186129266164, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9325262], dtype=float32), -0.61361045]. 
=============================================
[2019-04-04 01:30:47,012] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.8947533e-23 9.0375218e-16 1.5239759e-21 9.6544617e-18 2.0261812e-17
 6.1694066e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 01:30:47,013] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7894
[2019-04-04 01:30:47,036] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.9, 97.33333333333333, 0.0, 0.0, 26.0, 25.31268299776696, 0.5923362786962912, 0.0, 1.0, 40687.42214815506], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1282800.0000, 
sim time next is 1283400.0000, 
raw observation next is [5.8, 98.0, 0.0, 0.0, 26.0, 25.40368155782577, 0.6048596937741478, 0.0, 1.0, 23113.67498513456], 
processed observation next is [0.0, 0.8695652173913043, 0.6232686980609419, 0.98, 0.0, 0.0, 0.6666666666666666, 0.6169734631521475, 0.7016198979247159, 0.0, 1.0, 0.11006511897683123], 
reward next is 0.8899, 
noisyNet noise sample is [array([0.29041663], dtype=float32), -1.8459073]. 
=============================================
[2019-04-04 01:30:53,608] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.7124518e-24 1.6431763e-16 1.6509001e-21 8.9732727e-18 9.3558189e-18
 1.0005383e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:30:53,608] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7494
[2019-04-04 01:30:53,692] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.8, 100.0, 97.0, 0.0, 26.0, 24.88348516386986, 0.4610979144068033, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1257000.0000, 
sim time next is 1257600.0000, 
raw observation next is [13.8, 100.0, 96.0, 0.0, 26.0, 24.84909131709885, 0.4571268188895259, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.844875346260388, 1.0, 0.32, 0.0, 0.6666666666666666, 0.5707576097582375, 0.6523756062965086, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.705861], dtype=float32), -0.746936]. 
=============================================
[2019-04-04 01:30:55,318] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.03290185e-24 6.28448684e-17 1.02880387e-22 1.20224105e-18
 5.01602789e-19 2.81363275e-24 1.00000000e+00], sum to 1.0000
[2019-04-04 01:30:55,323] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9324
[2019-04-04 01:30:55,342] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 114.5, 0.0, 26.0, 26.11485694853901, 0.5918578759193486, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1335600.0000, 
sim time next is 1336200.0000, 
raw observation next is [1.1, 92.0, 118.6666666666667, 0.0, 26.0, 26.11964533576917, 0.592669214764029, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.49307479224376743, 0.92, 0.39555555555555566, 0.0, 0.6666666666666666, 0.6766371113140975, 0.6975564049213431, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9864811], dtype=float32), 1.3340461]. 
=============================================
[2019-04-04 01:30:58,507] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.70787005e-24 4.71110461e-16 2.00787996e-21 1.31286525e-17
 4.54008419e-18 1.24638698e-23 1.00000000e+00], sum to 1.0000
[2019-04-04 01:30:58,509] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2007
[2019-04-04 01:30:58,543] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 26.0, 25.44440264307963, 0.5674828395390242, 0.0, 1.0, 59021.13556101803], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1375200.0000, 
sim time next is 1375800.0000, 
raw observation next is [0.4166666666666667, 95.83333333333333, 0.0, 0.0, 26.0, 25.41371035471071, 0.5227816211151622, 0.0, 1.0, 66232.08854554096], 
processed observation next is [1.0, 0.9565217391304348, 0.47414589104339805, 0.9583333333333333, 0.0, 0.0, 0.6666666666666666, 0.6178091962258924, 0.6742605403717207, 0.0, 1.0, 0.31539089783590935], 
reward next is 0.6846, 
noisyNet noise sample is [array([2.2533753], dtype=float32), -0.19462177]. 
=============================================
[2019-04-04 01:31:07,510] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.8522867e-23 7.7536252e-15 7.2490199e-21 2.4935007e-17 7.2427326e-17
 2.5987257e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:31:07,510] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4237
[2019-04-04 01:31:07,561] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 26.0, 25.35270305277118, 0.510984316300366, 0.0, 1.0, 37502.10951607786], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1470600.0000, 
sim time next is 1471200.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 26.0, 25.54763399325637, 0.5095550608457321, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.5069252077562327, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6289694994380307, 0.6698516869485774, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7237053], dtype=float32), -1.1713071]. 
=============================================
[2019-04-04 01:31:18,886] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.4703846e-24 2.1412079e-18 6.7047516e-23 5.2877040e-19 7.7013162e-19
 5.4686643e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 01:31:18,886] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1091
[2019-04-04 01:31:18,913] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.600000000000001, 60.66666666666667, 0.0, 0.0, 26.0, 26.1921495738383, 0.6670877595694823, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1536000.0000, 
sim time next is 1536600.0000, 
raw observation next is [9.5, 60.83333333333334, 0.0, 0.0, 26.0, 26.16447718324521, 0.6567068650949435, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.7257617728531857, 0.6083333333333334, 0.0, 0.0, 0.6666666666666666, 0.6803730986037676, 0.7189022883649812, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.71625894], dtype=float32), -0.68276936]. 
=============================================
[2019-04-04 01:31:21,618] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.27708850e-23 3.30120249e-14 1.16516184e-20 2.63698926e-17
 6.37700335e-17 1.67414232e-22 1.00000000e+00], sum to 1.0000
[2019-04-04 01:31:21,618] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8721
[2019-04-04 01:31:21,662] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.6, 97.0, 0.0, 0.0, 26.0, 25.6217692004795, 0.5734413959577318, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1654800.0000, 
sim time next is 1655400.0000, 
raw observation next is [6.6, 97.0, 0.0, 0.0, 26.0, 25.619459881312, 0.572166318524177, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.6454293628808865, 0.97, 0.0, 0.0, 0.6666666666666666, 0.6349549901093333, 0.6907221061747256, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2890276], dtype=float32), 0.18743579]. 
=============================================
[2019-04-04 01:31:36,230] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2670602e-20 4.4077175e-14 3.6954139e-19 1.3999403e-15 1.2796490e-15
 1.5468307e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 01:31:36,231] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4713
[2019-04-04 01:31:36,285] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.483333333333333, 86.16666666666667, 40.66666666666666, 0.0, 26.0, 24.97855466178236, 0.3304793867083899, 0.0, 1.0, 64707.09898018058], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1786200.0000, 
sim time next is 1786800.0000, 
raw observation next is [-3.566666666666667, 85.33333333333334, 34.33333333333334, 0.0, 26.0, 24.97640432872877, 0.3331087779962963, 0.0, 1.0, 56937.79337887621], 
processed observation next is [0.0, 0.6956521739130435, 0.3638042474607572, 0.8533333333333334, 0.11444444444444447, 0.0, 0.6666666666666666, 0.5813670273940641, 0.6110362593320987, 0.0, 1.0, 0.27113234942322006], 
reward next is 0.7289, 
noisyNet noise sample is [array([1.0655391], dtype=float32), 1.2698905]. 
=============================================
[2019-04-04 01:31:46,859] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.2770807e-21 5.0091089e-14 4.2434606e-19 4.2639125e-16 1.1559519e-15
 2.6335671e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 01:31:46,859] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6162
[2019-04-04 01:31:46,873] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.2, 86.5, 0.0, 0.0, 26.0, 23.19586501877458, -0.1795779032600242, 0.0, 1.0, 44627.96567899168], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1924200.0000, 
sim time next is 1924800.0000, 
raw observation next is [-9.3, 88.0, 0.0, 0.0, 26.0, 23.14543316773755, -0.1807285873991039, 0.0, 1.0, 44583.86250996306], 
processed observation next is [1.0, 0.2608695652173913, 0.20498614958448752, 0.88, 0.0, 0.0, 0.6666666666666666, 0.42878609731146256, 0.43975713753363205, 0.0, 1.0, 0.21230410719030027], 
reward next is 0.7877, 
noisyNet noise sample is [array([-1.6611878], dtype=float32), 0.78493375]. 
=============================================
[2019-04-04 01:31:57,745] A3C_AGENT_WORKER-Thread-8 INFO:Evaluating...
[2019-04-04 01:31:57,746] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 01:31:57,746] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:31:57,748] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 01:31:57,748] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:31:57,771] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run26
[2019-04-04 01:31:57,811] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 01:31:57,835] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:31:57,854] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run26
[2019-04-04 01:31:57,995] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run26
[2019-04-04 01:32:23,863] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.36377555], dtype=float32), 0.16525461]
[2019-04-04 01:32:23,864] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-12.8, 67.0, 0.0, 0.0, 26.0, 23.16472031000189, -0.210531048909277, 0.0, 1.0, 46518.69004587013]
[2019-04-04 01:32:23,864] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 01:32:23,864] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [5.9022561e-20 5.8236413e-13 1.5504477e-18 4.6552973e-15 8.0993324e-15
 2.7277496e-19 1.0000000e+00], sampled 0.2909179250703865
[2019-04-04 01:34:06,694] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 01:34:27,200] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.36377555], dtype=float32), 0.16525461]
[2019-04-04 01:34:27,200] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [2.0, 53.66666666666667, 0.0, 0.0, 26.0, 25.6363243252452, 0.5280929909648306, 0.0, 1.0, 0.0]
[2019-04-04 01:34:27,200] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 01:34:27,201] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [8.3644399e-22 1.2301014e-14 3.7576033e-20 1.7816856e-16 2.9143542e-16
 1.7284264e-21 1.0000000e+00], sampled 0.7267388842502067
[2019-04-04 01:34:41,051] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 01:34:47,863] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 01:34:48,898] A3C_AGENT_WORKER-Thread-8 INFO:Global step: 2500000, evaluation results [2500000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 01:35:25,016] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.7384725e-21 4.1413487e-13 1.2658440e-18 1.1639876e-15 1.2528942e-15
 4.1488924e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 01:35:25,018] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1222
[2019-04-04 01:35:25,055] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 91.0, 0.0, 0.0, 26.0, 23.81030430533653, 0.00882092972217951, 0.0, 1.0, 43421.91337681599], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2264400.0000, 
sim time next is 2265000.0000, 
raw observation next is [-8.9, 91.00000000000001, 0.0, 0.0, 26.0, 23.76246506681927, -0.003680383970683784, 0.0, 1.0, 43366.54977951405], 
processed observation next is [1.0, 0.21739130434782608, 0.21606648199445982, 0.9100000000000001, 0.0, 0.0, 0.6666666666666666, 0.4802054222349392, 0.4987732053431054, 0.0, 1.0, 0.20650737990244786], 
reward next is 0.7935, 
noisyNet noise sample is [array([1.1124178], dtype=float32), -0.1935343]. 
=============================================
[2019-04-04 01:35:25,097] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[77.21537 ]
 [77.28556 ]
 [77.356606]
 [77.41865 ]
 [77.474525]], R is [[77.18016052]
 [77.20159149]
 [77.22257996]
 [77.24320984]
 [77.26345062]].
[2019-04-04 01:35:30,799] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.2354682e-23 2.1225137e-16 3.1110164e-21 3.6605105e-17 2.8182080e-17
 3.5623497e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:35:30,803] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9253
[2019-04-04 01:35:30,824] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 55.33333333333333, 0.0, 0.0, 26.0, 25.40218378438302, 0.4230242712954465, 0.0, 1.0, 51810.36194285156], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2324400.0000, 
sim time next is 2325000.0000, 
raw observation next is [-1.7, 55.66666666666667, 0.0, 0.0, 26.0, 25.3719994907346, 0.423621357890099, 0.0, 1.0, 60810.418440787], 
processed observation next is [1.0, 0.9130434782608695, 0.4155124653739613, 0.5566666666666668, 0.0, 0.0, 0.6666666666666666, 0.6143332908945499, 0.6412071192966997, 0.0, 1.0, 0.2895734211466047], 
reward next is 0.7104, 
noisyNet noise sample is [array([-1.3945779], dtype=float32), 0.37310126]. 
=============================================
[2019-04-04 01:35:30,890] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[81.12187 ]
 [80.940285]
 [80.87086 ]
 [80.788574]
 [80.75058 ]], R is [[81.18122864]
 [81.12269592]
 [81.15748596]
 [81.25657654]
 [81.35466003]].
[2019-04-04 01:35:42,597] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.6254396e-22 1.7516577e-14 7.8466749e-21 1.4345549e-16 6.4295935e-17
 2.8624064e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:35:42,597] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9374
[2019-04-04 01:35:42,709] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 29.5, 90.0, 845.0, 26.0, 24.92205102465552, 0.2570342367615934, 0.0, 1.0, 32810.6120489648], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2464200.0000, 
sim time next is 2464800.0000, 
raw observation next is [0.8666666666666667, 29.0, 89.5, 842.8333333333334, 26.0, 24.9122262737366, 0.2626160083225564, 0.0, 1.0, 26143.37100430447], 
processed observation next is [0.0, 0.5217391304347826, 0.4866112650046169, 0.29, 0.29833333333333334, 0.9313075506445673, 0.6666666666666666, 0.5760188561447167, 0.5875386694408521, 0.0, 1.0, 0.12449224287764034], 
reward next is 0.8755, 
noisyNet noise sample is [array([0.10214574], dtype=float32), -0.016882852]. 
=============================================
[2019-04-04 01:35:44,946] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.5570926e-21 6.8963783e-14 1.5895171e-19 1.8820792e-16 3.1489572e-16
 6.5237102e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:35:44,946] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0479
[2019-04-04 01:35:45,006] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.55, 29.0, 0.0, 0.0, 26.0, 24.91554849102632, 0.2113485216110403, 0.0, 1.0, 41154.06603762173], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2485800.0000, 
sim time next is 2486400.0000, 
raw observation next is [0.3666666666666668, 29.33333333333334, 0.0, 0.0, 26.0, 24.90555538474279, 0.2109580907247238, 0.0, 1.0, 46503.78432833819], 
processed observation next is [0.0, 0.782608695652174, 0.4727608494921515, 0.2933333333333334, 0.0, 0.0, 0.6666666666666666, 0.5754629487285658, 0.5703193635749079, 0.0, 1.0, 0.22144659203970565], 
reward next is 0.7786, 
noisyNet noise sample is [array([-1.9806993], dtype=float32), -1.0440971]. 
=============================================
[2019-04-04 01:35:49,903] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.5275769e-23 2.0218970e-16 1.0276198e-21 1.1615113e-17 1.0951505e-17
 2.5502205e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 01:35:49,903] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7312
[2019-04-04 01:35:49,965] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 48.0, 165.1666666666667, 193.3333333333333, 26.0, 25.95793603825287, 0.4724719205521996, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2647200.0000, 
sim time next is 2647800.0000, 
raw observation next is [0.5, 48.5, 155.0, 206.0, 26.0, 25.98739160740521, 0.4781647970863867, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.4764542936288089, 0.485, 0.5166666666666667, 0.2276243093922652, 0.6666666666666666, 0.6656159672837676, 0.6593882656954623, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3567111], dtype=float32), -0.51157856]. 
=============================================
[2019-04-04 01:35:56,023] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.2909464e-21 5.5807047e-14 4.2420019e-20 3.4812991e-16 3.8291368e-16
 1.1090095e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 01:35:56,024] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5368
[2019-04-04 01:35:56,094] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.2, 35.66666666666667, 0.0, 0.0, 26.0, 25.26894970255266, 0.2731017079695999, 0.0, 1.0, 40058.51177069263], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2496000.0000, 
sim time next is 2496600.0000, 
raw observation next is [-1.2, 35.0, 0.0, 0.0, 26.0, 25.27499537877184, 0.2784485239245637, 0.0, 1.0, 40067.03978633757], 
processed observation next is [0.0, 0.9130434782608695, 0.42936288088642666, 0.35, 0.0, 0.0, 0.6666666666666666, 0.6062496148976534, 0.5928161746415213, 0.0, 1.0, 0.19079542755398843], 
reward next is 0.8092, 
noisyNet noise sample is [array([1.0670074], dtype=float32), 1.9070269]. 
=============================================
[2019-04-04 01:36:08,307] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.7052292e-21 5.7670442e-14 8.9646423e-20 4.2581098e-16 1.2674018e-15
 1.3603273e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 01:36:08,307] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5147
[2019-04-04 01:36:08,324] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 39.66666666666667, 0.0, 0.0, 26.0, 25.20723954378183, 0.2248363180303597, 0.0, 1.0, 39132.84865408989], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2508600.0000, 
sim time next is 2509200.0000, 
raw observation next is [-1.7, 40.0, 0.0, 0.0, 26.0, 25.14670300296087, 0.215813423597294, 0.0, 1.0, 39139.22048346814], 
processed observation next is [1.0, 0.043478260869565216, 0.4155124653739613, 0.4, 0.0, 0.0, 0.6666666666666666, 0.5955585835800724, 0.5719378078657646, 0.0, 1.0, 0.18637724039746734], 
reward next is 0.8136, 
noisyNet noise sample is [array([-0.80931795], dtype=float32), -0.89258534]. 
=============================================
[2019-04-04 01:36:49,369] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.8842623e-22 6.0913256e-15 4.3449012e-20 1.2254587e-16 1.1463038e-16
 1.4754268e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:36:49,370] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8670
[2019-04-04 01:36:49,446] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.666666666666667, 100.0, 0.0, 0.0, 26.0, 25.30027865718768, 0.506841977966908, 0.0, 1.0, 42609.47676954906], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3213600.0000, 
sim time next is 3214200.0000, 
raw observation next is [-1.833333333333333, 100.0, 0.0, 0.0, 26.0, 25.30983595712523, 0.5023871371912098, 0.0, 1.0, 41276.86057133339], 
processed observation next is [1.0, 0.17391304347826086, 0.41181902123730385, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6091529964271025, 0.6674623790637365, 0.0, 1.0, 0.1965564789111114], 
reward next is 0.8034, 
noisyNet noise sample is [array([1.9352243], dtype=float32), -2.224638]. 
=============================================
[2019-04-04 01:36:51,469] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.2683411e-21 5.3150088e-14 8.2928992e-20 1.1218582e-15 5.0095339e-16
 1.2634981e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 01:36:51,469] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1627
[2019-04-04 01:36:51,534] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 71.0, 0.0, 0.0, 26.0, 24.62572542205321, 0.1974654482698006, 0.0, 1.0, 37953.66652884999], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3027600.0000, 
sim time next is 3028200.0000, 
raw observation next is [-5.0, 71.0, 0.0, 0.0, 26.0, 24.58324336535351, 0.1942639775639764, 0.0, 1.0, 38056.83393186396], 
processed observation next is [0.0, 0.043478260869565216, 0.32409972299168976, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5486036137794592, 0.5647546591879921, 0.0, 1.0, 0.18122301872316174], 
reward next is 0.8188, 
noisyNet noise sample is [array([-0.05478587], dtype=float32), -0.64809346]. 
=============================================
[2019-04-04 01:36:58,310] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.5080056e-24 3.4537874e-15 2.4184481e-21 1.4331046e-17 1.4015632e-17
 6.5676501e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 01:36:58,310] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9512
[2019-04-04 01:36:58,363] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 71.0, 88.0, 399.0, 26.0, 25.30772780191081, 0.4172058817791314, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3486600.0000, 
sim time next is 3487200.0000, 
raw observation next is [-1.0, 71.0, 89.83333333333334, 444.1666666666666, 26.0, 25.46321279700039, 0.4338616962060126, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.4349030470914128, 0.71, 0.29944444444444446, 0.49079189686924485, 0.6666666666666666, 0.6219343997500326, 0.6446205654020042, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.01935177], dtype=float32), 1.1012932]. 
=============================================
[2019-04-04 01:37:01,127] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.9020081e-22 7.1968561e-15 2.4310567e-20 2.1573205e-16 1.6444806e-16
 1.1528042e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:37:01,128] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6362
[2019-04-04 01:37:01,143] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.3333333333333334, 100.0, 0.0, 0.0, 26.0, 25.6380130337655, 0.5509357122613241, 0.0, 1.0, 24020.10201370977], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3202800.0000, 
sim time next is 3203400.0000, 
raw observation next is [0.1666666666666666, 100.0, 0.0, 0.0, 26.0, 25.50466886158073, 0.5436955759457, 0.0, 1.0, 97630.23386971737], 
processed observation next is [1.0, 0.043478260869565216, 0.4672206832871654, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6253890717983941, 0.6812318586485667, 0.0, 1.0, 0.4649058755700827], 
reward next is 0.5351, 
noisyNet noise sample is [array([-0.9421881], dtype=float32), 0.3120282]. 
=============================================
[2019-04-04 01:37:23,985] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.9631676e-24 3.6225594e-17 2.7426243e-22 4.9961316e-18 3.5237026e-18
 3.4594679e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 01:37:23,985] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3644
[2019-04-04 01:37:24,015] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.3333333333333334, 60.0, 113.5, 811.0, 26.0, 26.37469784386521, 0.6264907489607064, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3764400.0000, 
sim time next is 3765000.0000, 
raw observation next is [-0.1666666666666666, 60.0, 112.0, 804.0, 26.0, 26.48310155917869, 0.6414986237729785, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.4579870729455217, 0.6, 0.37333333333333335, 0.8883977900552487, 0.6666666666666666, 0.7069251299315574, 0.7138328745909929, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.26635924], dtype=float32), 0.69644177]. 
=============================================
[2019-04-04 01:37:24,019] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[88.513306]
 [88.41399 ]
 [88.28375 ]
 [88.226   ]
 [88.23823 ]], R is [[88.71656799]
 [88.82940674]
 [88.94111633]
 [89.05170441]
 [89.16118622]].
[2019-04-04 01:37:28,875] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.9336480e-23 1.0186341e-16 4.2276199e-22 1.4525775e-17 9.3213709e-18
 1.3706491e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 01:37:28,891] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6202
[2019-04-04 01:37:28,948] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 49.0, 70.66666666666666, 579.0, 26.0, 26.81085125230287, 0.7531173434285549, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3514200.0000, 
sim time next is 3514800.0000, 
raw observation next is [3.0, 49.0, 66.33333333333334, 552.0, 26.0, 26.96337203305146, 0.517504942972744, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.5457063711911359, 0.49, 0.22111111111111115, 0.6099447513812155, 0.6666666666666666, 0.746947669420955, 0.6725016476575814, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.24896884], dtype=float32), 1.5443021]. 
=============================================
[2019-04-04 01:37:36,350] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.5129935e-22 4.6557416e-15 4.2619525e-20 5.6322602e-17 1.3995712e-16
 1.6591833e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:37:36,350] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1550
[2019-04-04 01:37:36,385] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.833333333333333, 59.66666666666667, 0.0, 0.0, 26.0, 24.95485126446658, 0.3164377864700692, 0.0, 1.0, 32582.52425026905], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3697800.0000, 
sim time next is 3698400.0000, 
raw observation next is [3.666666666666667, 60.33333333333334, 0.0, 0.0, 26.0, 24.94525728067377, 0.326921664883735, 0.0, 1.0, 197705.0304616939], 
processed observation next is [0.0, 0.8260869565217391, 0.564173591874423, 0.6033333333333334, 0.0, 0.0, 0.6666666666666666, 0.5787714400561473, 0.6089738882945783, 0.0, 1.0, 0.9414525260080663], 
reward next is 0.0585, 
noisyNet noise sample is [array([-0.12635212], dtype=float32), -0.7806764]. 
=============================================
[2019-04-04 01:37:41,519] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.5516588e-22 1.6264244e-14 3.2421493e-20 1.8992588e-16 8.2601085e-17
 8.9033949e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:37:41,519] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8476
[2019-04-04 01:37:41,556] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.666666666666666, 70.0, 17.16666666666666, 171.6666666666667, 26.0, 24.24930899617439, 0.1925595961069502, 0.0, 1.0, 41553.49759620601], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3570000.0000, 
sim time next is 3570600.0000, 
raw observation next is [-6.833333333333334, 70.0, 31.33333333333333, 222.3333333333333, 26.0, 24.22234566707758, 0.1954363359398497, 0.0, 1.0, 41508.08335725342], 
processed observation next is [0.0, 0.30434782608695654, 0.27331486611265005, 0.7, 0.10444444444444442, 0.24567219152854508, 0.6666666666666666, 0.5185288055897983, 0.5651454453132833, 0.0, 1.0, 0.19765753979644485], 
reward next is 0.8023, 
noisyNet noise sample is [array([1.3046553], dtype=float32), 1.000745]. 
=============================================
[2019-04-04 01:37:50,318] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0663225e-24 3.2802484e-16 2.6893924e-22 8.1111925e-18 2.7429646e-18
 5.2599880e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 01:37:50,318] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5892
[2019-04-04 01:37:50,403] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.66666666666667, 51.50000000000001, 98.33333333333334, 613.3333333333334, 26.0, 26.37689666615852, 0.5200384531896552, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4007400.0000, 
sim time next is 4008000.0000, 
raw observation next is [-10.33333333333333, 50.0, 99.66666666666667, 655.6666666666667, 26.0, 26.42914941968986, 0.533546210770827, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.17636195752539252, 0.5, 0.33222222222222225, 0.7244935543278086, 0.6666666666666666, 0.7024291183074883, 0.6778487369236089, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.10342702], dtype=float32), 0.8781841]. 
=============================================
[2019-04-04 01:37:50,413] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[92.06626 ]
 [91.83997 ]
 [91.725624]
 [91.8258  ]
 [92.09561 ]], R is [[92.37255859]
 [92.44883728]
 [92.52435303]
 [92.59911346]
 [92.67312622]].
[2019-04-04 01:37:50,432] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.87238360e-23 2.62289496e-16 1.61027185e-21 1.09631665e-17
 3.62267025e-17 7.99514314e-23 1.00000000e+00], sum to 1.0000
[2019-04-04 01:37:50,433] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2803
[2019-04-04 01:37:50,447] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.1666666666666667, 73.0, 0.0, 0.0, 26.0, 25.84431037127768, 0.6202761280288754, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3532200.0000, 
sim time next is 3532800.0000, 
raw observation next is [-0.3333333333333333, 74.0, 0.0, 0.0, 26.0, 25.93552012918922, 0.616028953478413, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.4533702677747, 0.74, 0.0, 0.0, 0.6666666666666666, 0.6612933440991018, 0.7053429844928044, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.65114844], dtype=float32), 0.09174611]. 
=============================================
[2019-04-04 01:38:00,455] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.1354110e-23 1.2302832e-16 3.8154920e-22 7.8720810e-18 1.5577285e-17
 5.7952870e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 01:38:00,455] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.2494
[2019-04-04 01:38:00,514] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 49.0, 118.8333333333333, 804.1666666666666, 26.0, 26.32507708406003, 0.6071846587730633, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3928800.0000, 
sim time next is 3929400.0000, 
raw observation next is [-6.0, 49.0, 120.0, 810.0, 26.0, 26.35142423346651, 0.609989746416046, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.296398891966759, 0.49, 0.4, 0.8950276243093923, 0.6666666666666666, 0.6959520194555425, 0.7033299154720153, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3235886], dtype=float32), -0.79631233]. 
=============================================
[2019-04-04 01:38:01,305] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.4216771e-24 2.4844252e-16 8.0576080e-22 2.7441277e-17 2.8103054e-18
 2.9842393e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 01:38:01,306] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3044
[2019-04-04 01:38:01,319] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 60.0, 109.0, 790.0, 26.0, 26.64284261875446, 0.6510273942967152, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3766200.0000, 
sim time next is 3766800.0000, 
raw observation next is [0.0, 60.00000000000001, 107.5, 783.0, 26.0, 26.61812505337262, 0.6505696654280465, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.46260387811634357, 0.6000000000000001, 0.35833333333333334, 0.8651933701657458, 0.6666666666666666, 0.7181770877810516, 0.7168565551426821, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.18828675], dtype=float32), 0.13685682]. 
=============================================
[2019-04-04 01:38:05,630] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.9888014e-24 1.1340603e-16 3.1825072e-22 2.9323387e-18 1.1302204e-17
 3.4969107e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 01:38:05,630] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0812
[2019-04-04 01:38:05,643] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.3333333333333334, 60.0, 113.5, 811.0, 26.0, 26.37469784386521, 0.6264907489607064, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3764400.0000, 
sim time next is 3765000.0000, 
raw observation next is [-0.1666666666666666, 60.0, 112.0, 804.0, 26.0, 26.48310155917869, 0.6414986237729785, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.4579870729455217, 0.6, 0.37333333333333335, 0.8883977900552487, 0.6666666666666666, 0.7069251299315574, 0.7138328745909929, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6516602], dtype=float32), 1.0594839]. 
=============================================
[2019-04-04 01:38:05,651] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[88.50598 ]
 [88.40914 ]
 [88.28023 ]
 [88.223305]
 [88.23815 ]], R is [[88.705513  ]
 [88.81845856]
 [88.93027496]
 [89.04096985]
 [89.15055847]].
[2019-04-04 01:38:10,472] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.4481492e-24 2.1858167e-16 1.5804212e-22 8.3375703e-18 2.4472304e-18
 1.1478658e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 01:38:10,474] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0324
[2019-04-04 01:38:10,550] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6666666666666667, 58.5, 117.0, 830.6666666666667, 26.0, 26.62593362447512, 0.6711748041064179, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3845400.0000, 
sim time next is 3846000.0000, 
raw observation next is [-0.3333333333333334, 57.00000000000001, 117.0, 832.8333333333334, 26.0, 26.67164117318834, 0.6743821986860853, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.4533702677747, 0.5700000000000001, 0.39, 0.9202578268876612, 0.6666666666666666, 0.7226367644323618, 0.7247940662286951, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2679549], dtype=float32), 0.1131804]. 
=============================================
[2019-04-04 01:38:10,601] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[89.4553  ]
 [89.5985  ]
 [89.808556]
 [90.03236 ]
 [90.29396 ]], R is [[89.46743011]
 [89.57275391]
 [89.67702484]
 [89.78025818]
 [89.88245392]].
[2019-04-04 01:38:20,720] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.02720784e-22 6.05448497e-17 2.21361085e-21 6.32438474e-17
 5.50880318e-17 2.56301618e-23 1.00000000e+00], sum to 1.0000
[2019-04-04 01:38:20,721] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.2243
[2019-04-04 01:38:20,749] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 36.0, 34.66666666666666, 120.3333333333333, 26.0, 26.74727186397172, 0.6518332303552766, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4123200.0000, 
sim time next is 4123800.0000, 
raw observation next is [3.0, 35.5, 23.0, 57.0, 26.0, 26.636558246562, 0.6274873714718255, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.5457063711911359, 0.355, 0.07666666666666666, 0.06298342541436464, 0.6666666666666666, 0.7197131872135, 0.7091624571572752, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4706562], dtype=float32), 0.39775905]. 
=============================================
[2019-04-04 01:38:21,211] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.5514687e-21 9.9122967e-14 2.5129111e-19 8.3796673e-16 9.8321658e-16
 4.6857342e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 01:38:21,212] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6172
[2019-04-04 01:38:21,271] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.66666666666667, 67.0, 0.0, 0.0, 26.0, 23.98358378727368, 0.06950128421111705, 0.0, 1.0, 43753.08724385611], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3991200.0000, 
sim time next is 3991800.0000, 
raw observation next is [-12.83333333333333, 68.0, 0.0, 0.0, 26.0, 23.9206066475089, 0.0562749376262967, 0.0, 1.0, 43786.69438677255], 
processed observation next is [1.0, 0.17391304347826086, 0.10710987996306563, 0.68, 0.0, 0.0, 0.6666666666666666, 0.4933838872924084, 0.5187583125420989, 0.0, 1.0, 0.20850806850844072], 
reward next is 0.7915, 
noisyNet noise sample is [array([-1.5070906], dtype=float32), 0.11374696]. 
=============================================
[2019-04-04 01:38:44,695] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.9898711e-22 1.7389115e-14 2.7481357e-20 1.6150375e-16 1.6831765e-16
 3.8654061e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:38:44,695] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0672
[2019-04-04 01:38:44,746] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 47.66666666666666, 0.0, 0.0, 26.0, 25.37691460794728, 0.3258325172086913, 0.0, 1.0, 43855.85786117217], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4254000.0000, 
sim time next is 4254600.0000, 
raw observation next is [3.0, 48.33333333333334, 0.0, 0.0, 26.0, 25.38308340702506, 0.3252484532164843, 0.0, 1.0, 38400.32162562429], 
processed observation next is [0.0, 0.21739130434782608, 0.5457063711911359, 0.48333333333333345, 0.0, 0.0, 0.6666666666666666, 0.6152569505854218, 0.6084161510721614, 0.0, 1.0, 0.1828586744077347], 
reward next is 0.8171, 
noisyNet noise sample is [array([-1.5826778], dtype=float32), 1.0810909]. 
=============================================
[2019-04-04 01:38:52,231] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.4064496e-22 9.3881119e-16 1.9545866e-20 5.5886027e-16 6.6843929e-17
 5.2392471e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:38:52,237] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7785
[2019-04-04 01:38:52,275] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 90.83333333333334, 122.0, 2.0, 26.0, 26.24447408160266, 0.6246954704201436, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4457400.0000, 
sim time next is 4458000.0000, 
raw observation next is [0.0, 89.66666666666667, 103.5, 0.9999999999999998, 26.0, 26.29376631326909, 0.6256801695036694, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.46260387811634357, 0.8966666666666667, 0.345, 0.0011049723756906074, 0.6666666666666666, 0.6911471927724241, 0.7085600565012231, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.46609476], dtype=float32), -0.6926613]. 
=============================================
[2019-04-04 01:38:52,350] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[80.7358  ]
 [80.8785  ]
 [81.50379 ]
 [82.35769 ]
 [83.305016]], R is [[80.9862442 ]
 [81.17638397]
 [81.36462402]
 [81.55097961]
 [81.73547363]].
[2019-04-04 01:39:01,057] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.5062627e-23 2.4284572e-16 5.9569893e-22 1.0230067e-17 5.9838080e-18
 9.8932261e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 01:39:01,057] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1736
[2019-04-04 01:39:01,072] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.833333333333333, 45.5, 190.0, 45.66666666666666, 26.0, 26.27172900311142, 0.5899486937475796, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4547400.0000, 
sim time next is 4548000.0000, 
raw observation next is [2.666666666666667, 46.0, 171.5, 28.83333333333333, 26.0, 26.34599398021719, 0.5960873769284607, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5364727608494922, 0.46, 0.5716666666666667, 0.031860036832412515, 0.6666666666666666, 0.6954994983514325, 0.6986957923094869, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4024681], dtype=float32), -2.3953454]. 
=============================================
[2019-04-04 01:39:01,114] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[86.71649 ]
 [87.210464]
 [88.00571 ]
 [88.94683 ]
 [89.79732 ]], R is [[86.45372009]
 [86.58917999]
 [86.72328949]
 [86.85605621]
 [86.98749542]].
[2019-04-04 01:39:06,996] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.4908718e-22 3.8275322e-15 1.6184386e-20 1.0258365e-16 7.1121889e-17
 2.5944576e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:39:06,997] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2197
[2019-04-04 01:39:07,080] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.333333333333333, 82.0, 132.3333333333333, 419.3333333333334, 26.0, 25.08999939929086, 0.4268419235486307, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4783200.0000, 
sim time next is 4783800.0000, 
raw observation next is [-5.166666666666667, 79.5, 140.6666666666667, 419.6666666666667, 26.0, 25.47092022751614, 0.4561575605218042, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.31948291782086796, 0.795, 0.468888888888889, 0.4637200736648251, 0.6666666666666666, 0.6225766856263449, 0.6520525201739348, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0426244], dtype=float32), -0.3859505]. 
=============================================
[2019-04-04 01:39:10,359] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.2911952e-25 1.1540787e-16 3.7321123e-23 1.0944566e-18 4.4350297e-19
 2.9645986e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 01:39:10,359] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4870
[2019-04-04 01:39:10,426] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.3, 57.0, 99.5, 584.0, 26.0, 26.37378845354358, 0.5779263155712183, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4352400.0000, 
sim time next is 4353000.0000, 
raw observation next is [6.916666666666666, 54.5, 102.0, 615.0, 26.0, 26.43580449781494, 0.5946304853599416, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.6542012927054479, 0.545, 0.34, 0.6795580110497238, 0.6666666666666666, 0.7029837081512449, 0.6982101617866472, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3290499], dtype=float32), -0.084801875]. 
=============================================
[2019-04-04 01:39:10,441] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[92.50516 ]
 [92.103424]
 [92.01697 ]
 [91.97713 ]
 [91.30795 ]], R is [[93.16842651]
 [93.23674011]
 [93.30437469]
 [93.37133026]
 [93.43761444]].
[2019-04-04 01:39:16,669] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [8.9398309e-24 4.3949697e-16 1.7581920e-21 2.6304186e-17 2.3027970e-17
 2.0240506e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 01:39:16,670] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5564
[2019-04-04 01:39:16,726] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 86.0, 160.5, 3.0, 26.0, 26.3042741523689, 0.5517976612613782, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4708800.0000, 
sim time next is 4709400.0000, 
raw observation next is [1.0, 86.0, 143.0, 2.0, 26.0, 26.27856235682959, 0.5452418471696461, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.4903047091412743, 0.86, 0.4766666666666667, 0.0022099447513812156, 0.6666666666666666, 0.6898801964024658, 0.6817472823898821, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.9649558], dtype=float32), -0.16424166]. 
=============================================
[2019-04-04 01:39:18,468] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.2551951e-22 6.4400322e-15 7.2686558e-20 8.7505445e-17 6.0876057e-16
 1.7929513e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:39:18,469] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4447
[2019-04-04 01:39:18,481] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.933333333333334, 74.66666666666667, 0.0, 0.0, 26.0, 25.55735172464115, 0.3799836747150292, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4346400.0000, 
sim time next is 4347000.0000, 
raw observation next is [2.95, 74.5, 0.0, 0.0, 26.0, 25.53694704076979, 0.3643233321159549, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.5443213296398892, 0.745, 0.0, 0.0, 0.6666666666666666, 0.6280789200641491, 0.6214411107053183, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9300669], dtype=float32), -1.6561414]. 
=============================================
[2019-04-04 01:39:18,523] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[82.00374 ]
 [82.08412 ]
 [82.2264  ]
 [82.20632 ]
 [82.080345]], R is [[84.38816071]
 [84.54428101]
 [84.69883728]
 [84.6204834 ]
 [84.45511627]].
[2019-04-04 01:39:21,341] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.5243954e-24 2.4271433e-15 1.7072864e-21 5.2016988e-18 1.5355510e-17
 7.6802942e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 01:39:21,361] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9799
[2019-04-04 01:39:21,412] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 71.0, 102.5, 142.5, 26.0, 25.44152528271828, 0.4376479903494938, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4609200.0000, 
sim time next is 4609800.0000, 
raw observation next is [-2.0, 71.0, 123.0, 171.0, 26.0, 25.63566599506286, 0.4738534092556028, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.40720221606648205, 0.71, 0.41, 0.18895027624309393, 0.6666666666666666, 0.6363054995885715, 0.6579511364185343, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0059795], dtype=float32), -0.39704022]. 
=============================================
[2019-04-04 01:39:35,028] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.4482246e-22 5.8514264e-15 1.9049873e-20 1.4053920e-16 2.0671858e-16
 1.0675289e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:39:35,028] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7815
[2019-04-04 01:39:35,042] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.92045630052909, 0.3589987058520723, 0.0, 1.0, 41050.6348964639], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4756200.0000, 
sim time next is 4756800.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.93694701388418, 0.3599985792749528, 0.0, 1.0, 40948.84559212266], 
processed observation next is [0.0, 0.043478260869565216, 0.3518005540166205, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5780789178236816, 0.6199995264249843, 0.0, 1.0, 0.19499450281963174], 
reward next is 0.8050, 
noisyNet noise sample is [array([0.06831247], dtype=float32), 0.53416115]. 
=============================================
[2019-04-04 01:39:35,810] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:39:35,811] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:39:35,819] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run20
[2019-04-04 01:39:36,696] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.3651416e-22 2.5941453e-14 1.2365721e-19 2.2318551e-16 3.7388799e-16
 1.8215337e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:39:36,699] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9143
[2019-04-04 01:39:36,720] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.8, 66.33333333333333, 0.0, 0.0, 26.0, 25.50706256863821, 0.4168635556688815, 0.0, 1.0, 18751.73822521675], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4588800.0000, 
sim time next is 4589400.0000, 
raw observation next is [-0.9500000000000001, 66.66666666666667, 0.0, 0.0, 26.0, 25.39555405103141, 0.4145481907208635, 0.0, 1.0, 75703.6830702487], 
processed observation next is [1.0, 0.08695652173913043, 0.43628808864265933, 0.6666666666666667, 0.0, 0.0, 0.6666666666666666, 0.6162961709192842, 0.6381827302402878, 0.0, 1.0, 0.3604937289059462], 
reward next is 0.6395, 
noisyNet noise sample is [array([0.49047518], dtype=float32), 1.132337]. 
=============================================
[2019-04-04 01:39:39,815] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:39:39,815] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:39:39,819] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run20
[2019-04-04 01:39:41,461] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.7343908e-21 4.8477312e-14 2.1559351e-19 2.3629265e-16 2.6508800e-15
 1.0941424e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 01:39:41,466] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6247
[2019-04-04 01:39:41,534] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 43.0, 0.0, 0.0, 26.0, 24.99440725981882, 0.3295306629265488, 0.0, 1.0, 40510.10504205435], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4820400.0000, 
sim time next is 4821000.0000, 
raw observation next is [1.0, 43.66666666666667, 0.0, 0.0, 26.0, 24.98031205761268, 0.3299543235867029, 0.0, 1.0, 40225.65613315458], 
processed observation next is [0.0, 0.8260869565217391, 0.4903047091412743, 0.4366666666666667, 0.0, 0.0, 0.6666666666666666, 0.5816926714677232, 0.609984774528901, 0.0, 1.0, 0.1915507434912123], 
reward next is 0.8084, 
noisyNet noise sample is [array([-1.2546817], dtype=float32), -0.343045]. 
=============================================
[2019-04-04 01:39:41,555] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[79.09307 ]
 [78.814156]
 [78.402   ]
 [77.88145 ]
 [77.39532 ]], R is [[79.23165131]
 [79.24643707]
 [79.30621338]
 [79.41956329]
 [79.53490448]].
[2019-04-04 01:39:47,045] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:39:47,045] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:39:47,053] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run20
[2019-04-04 01:39:47,216] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.4100374e-21 3.1230503e-13 1.3843404e-19 3.2694545e-16 6.9219109e-16
 9.5167412e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:39:47,216] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0948
[2019-04-04 01:39:47,231] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.7, 93.0, 0.0, 0.0, 26.0, 21.29541288100414, -0.5541443627601895, 0.0, 1.0, 40272.25919150066], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 24600.0000, 
sim time next is 25200.0000, 
raw observation next is [7.7, 93.0, 0.0, 0.0, 26.0, 21.31438586249307, -0.5485331347517629, 0.0, 1.0, 40271.59813724393], 
processed observation next is [0.0, 0.30434782608695654, 0.6759002770083103, 0.93, 0.0, 0.0, 0.6666666666666666, 0.2761988218744224, 0.31715562174941236, 0.0, 1.0, 0.19176951493925679], 
reward next is 0.8082, 
noisyNet noise sample is [array([0.45217574], dtype=float32), 1.3010136]. 
=============================================
[2019-04-04 01:39:53,160] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:39:53,161] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:39:53,163] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run20
[2019-04-04 01:39:55,467] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.2546968e-24 1.7585999e-15 1.3577033e-21 2.2921049e-17 6.3987847e-18
 3.6574564e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 01:39:55,467] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9757
[2019-04-04 01:39:55,522] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.8, 71.83333333333334, 93.33333333333333, 12.0, 26.0, 25.3412344751165, 0.2565425761663069, 1.0, 1.0, 39248.75917341828], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 121800.0000, 
sim time next is 122400.0000, 
raw observation next is [-7.8, 74.0, 117.5, 18.0, 26.0, 25.30862976089714, 0.2580145794055813, 1.0, 1.0, 41398.49980056042], 
processed observation next is [1.0, 0.43478260869565216, 0.24653739612188366, 0.74, 0.39166666666666666, 0.019889502762430938, 0.6666666666666666, 0.6090524800747618, 0.5860048598018605, 1.0, 1.0, 0.197135713336002], 
reward next is 0.8029, 
noisyNet noise sample is [array([-0.9411558], dtype=float32), -1.0552914]. 
=============================================
[2019-04-04 01:39:55,713] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:39:55,713] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:39:55,717] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run20
[2019-04-04 01:39:55,963] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:39:55,963] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:39:55,967] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run20
[2019-04-04 01:39:56,097] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:39:56,098] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:39:56,103] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run20
[2019-04-04 01:39:57,001] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:39:57,001] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:39:57,005] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run20
[2019-04-04 01:39:57,070] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:39:57,070] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:39:57,076] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run20
[2019-04-04 01:39:58,349] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:39:58,349] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:39:58,353] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run20
[2019-04-04 01:39:58,440] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:39:58,440] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:39:58,445] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run20
[2019-04-04 01:40:00,029] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:40:00,029] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:40:00,033] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run20
[2019-04-04 01:40:00,082] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.7876771e-23 1.1428198e-16 8.8136044e-22 1.5852013e-17 4.4400255e-18
 1.3540813e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 01:40:00,082] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1880
[2019-04-04 01:40:00,146] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.3, 67.33333333333334, 0.0, 0.0, 26.0, 24.98629882654301, 0.3043687129411883, 1.0, 1.0, 110398.8680526219], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 157800.0000, 
sim time next is 158400.0000, 
raw observation next is [-8.4, 68.0, 0.0, 0.0, 26.0, 24.98698591364652, 0.3143169160407694, 0.0, 1.0, 83795.70213747746], 
processed observation next is [1.0, 0.8695652173913043, 0.2299168975069252, 0.68, 0.0, 0.0, 0.6666666666666666, 0.5822488261372101, 0.6047723053469231, 0.0, 1.0, 0.399027153035607], 
reward next is 0.6010, 
noisyNet noise sample is [array([0.3930187], dtype=float32), -2.188535]. 
=============================================
[2019-04-04 01:40:01,472] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:40:01,472] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:40:01,477] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run20
[2019-04-04 01:40:03,091] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.6073319e-23 1.0232145e-15 3.4123805e-21 1.3250664e-17 5.1437287e-17
 9.9615859e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 01:40:03,092] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7103
[2019-04-04 01:40:03,101] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.7, 19.0, 0.0, 0.0, 26.0, 26.77370561138118, 0.7795936152454107, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5090400.0000, 
sim time next is 5091000.0000, 
raw observation next is [8.65, 19.16666666666667, 0.0, 0.0, 26.0, 26.72111417084481, 0.7263804065684587, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.7022160664819946, 0.1916666666666667, 0.0, 0.0, 0.6666666666666666, 0.7267595142370675, 0.7421268021894862, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.71227807], dtype=float32), -0.48260275]. 
=============================================
[2019-04-04 01:40:03,150] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[82.32119 ]
 [82.01862 ]
 [81.78337 ]
 [81.566734]
 [81.34291 ]], R is [[82.67697144]
 [82.85020447]
 [83.02170563]
 [83.19149017]
 [83.35957336]].
[2019-04-04 01:40:04,569] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:40:04,569] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:40:04,573] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run20
[2019-04-04 01:40:09,985] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:40:09,985] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:40:09,988] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run20
[2019-04-04 01:40:10,821] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:40:10,821] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:40:10,824] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run20
[2019-04-04 01:40:23,672] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.0897241e-23 2.0713299e-16 5.2024450e-21 1.5765053e-17 1.0141941e-17
 1.0371854e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:40:23,672] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0717
[2019-04-04 01:40:23,746] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.0, 65.33333333333334, 0.0, 0.0, 26.0, 25.16535250428964, 0.3231470301526972, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 156000.0000, 
sim time next is 156600.0000, 
raw observation next is [-8.1, 66.0, 0.0, 0.0, 26.0, 25.1238693376273, 0.3019215855228873, 1.0, 1.0, 34609.96962800329], 
processed observation next is [1.0, 0.8260869565217391, 0.23822714681440446, 0.66, 0.0, 0.0, 0.6666666666666666, 0.5936557781356084, 0.6006405285076291, 1.0, 1.0, 0.16480937918096802], 
reward next is 0.8352, 
noisyNet noise sample is [array([-0.49423826], dtype=float32), -0.27897978]. 
=============================================
[2019-04-04 01:40:26,897] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.6690232e-22 1.5036000e-14 3.8739996e-20 6.1126411e-17 2.1965158e-16
 8.4603621e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:40:26,898] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0900
[2019-04-04 01:40:26,963] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.98758475969688, -0.185682409930267, 0.0, 1.0, 44403.1259219667], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 189000.0000, 
sim time next is 189600.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.92349736331452, -0.1938747755881511, 0.0, 1.0, 44493.17285049838], 
processed observation next is [1.0, 0.17391304347826086, 0.21606648199445982, 0.78, 0.0, 0.0, 0.6666666666666666, 0.4102914469428767, 0.4353750748039496, 0.0, 1.0, 0.2118722516690399], 
reward next is 0.7881, 
noisyNet noise sample is [array([-0.14838296], dtype=float32), 0.36219126]. 
=============================================
[2019-04-04 01:40:33,604] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.5995945e-21 1.6724832e-14 2.6356489e-19 5.1102874e-16 2.1886201e-16
 1.9809942e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 01:40:33,625] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.9247
[2019-04-04 01:40:33,644] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.8, 69.5, 0.0, 0.0, 26.0, 22.73126496201315, -0.2386512096715706, 0.0, 1.0, 47817.79253018797], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 281400.0000, 
sim time next is 282000.0000, 
raw observation next is [-11.9, 69.0, 0.0, 0.0, 26.0, 22.65121348546379, -0.2519783060343611, 0.0, 1.0, 47827.8059221057], 
processed observation next is [1.0, 0.2608695652173913, 0.13296398891966757, 0.69, 0.0, 0.0, 0.6666666666666666, 0.387601123788649, 0.4160072313218796, 0.0, 1.0, 0.2277514567719319], 
reward next is 0.7722, 
noisyNet noise sample is [array([1.0438302], dtype=float32), -0.7111364]. 
=============================================
[2019-04-04 01:40:33,663] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[76.99626 ]
 [77.04008 ]
 [77.06346 ]
 [77.083824]
 [77.08235 ]], R is [[76.94341278]
 [76.9462738 ]
 [76.94905853]
 [76.95180511]
 [76.95462036]].
[2019-04-04 01:40:46,038] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.2300725e-22 2.9951631e-14 7.7905770e-20 7.6104395e-17 1.6512885e-16
 1.0746429e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 01:40:46,038] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8483
[2019-04-04 01:40:46,055] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 82.33333333333334, 0.0, 0.0, 26.0, 24.24286899239265, 0.1059327706270172, 0.0, 1.0, 42335.81455259971], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 613200.0000, 
sim time next is 613800.0000, 
raw observation next is [-3.9, 80.5, 0.0, 0.0, 26.0, 24.22128922904358, 0.1033258356827847, 0.0, 1.0, 42430.7967819286], 
processed observation next is [0.0, 0.08695652173913043, 0.3545706371191136, 0.805, 0.0, 0.0, 0.6666666666666666, 0.5184407690869651, 0.5344419452275949, 0.0, 1.0, 0.20205141324727904], 
reward next is 0.7979, 
noisyNet noise sample is [array([-0.5124484], dtype=float32), 0.026994238]. 
=============================================
[2019-04-04 01:41:04,985] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.8602998e-22 5.1479541e-15 1.3497565e-20 6.0525911e-17 1.3192468e-16
 9.9703355e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:41:04,986] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7565
[2019-04-04 01:41:04,999] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 86.0, 0.0, 0.0, 26.0, 24.46248971573678, 0.1525586964270502, 0.0, 1.0, 42162.44315111593], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 608400.0000, 
sim time next is 609000.0000, 
raw observation next is [-3.9, 86.0, 0.0, 0.0, 26.0, 24.42832901466321, 0.1458366793930325, 0.0, 1.0, 42138.93995359495], 
processed observation next is [0.0, 0.043478260869565216, 0.3545706371191136, 0.86, 0.0, 0.0, 0.6666666666666666, 0.5356940845552675, 0.5486122264643442, 0.0, 1.0, 0.2006616188266426], 
reward next is 0.7993, 
noisyNet noise sample is [array([-1.2281969], dtype=float32), -0.7173692]. 
=============================================
[2019-04-04 01:41:05,017] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[81.27161]
 [81.3717 ]
 [81.45688]
 [81.51872]
 [81.60915]], R is [[81.16938782]
 [81.15692139]
 [81.14445496]
 [81.13196564]
 [81.11942291]].
[2019-04-04 01:41:07,159] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-04-04 01:41:07,160] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 01:41:07,161] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:41:07,176] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run27
[2019-04-04 01:41:07,201] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 01:41:07,202] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 01:41:07,203] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:41:07,204] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:41:07,207] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run27
[2019-04-04 01:41:07,223] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run27
[2019-04-04 01:41:48,235] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.37414968], dtype=float32), 0.14790659]
[2019-04-04 01:41:48,236] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [17.67894075, 75.88207957, 163.9759246, 0.0, 26.0, 24.81006610211959, 0.4727442623757679, 0.0, 0.0, 0.0]
[2019-04-04 01:41:48,236] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 01:41:48,237] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [6.7213972e-23 1.0975480e-15 1.5672799e-21 6.6206676e-18 4.9445974e-18
 1.0808457e-22 1.0000000e+00], sampled 0.15120710707374463
[2019-04-04 01:41:51,551] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.37414968], dtype=float32), 0.14790659]
[2019-04-04 01:41:51,551] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [1.547002238666667, 89.60426809, 98.92353198333335, 0.0, 26.0, 24.98820566692453, 0.424577359559705, 0.0, 1.0, 0.0]
[2019-04-04 01:41:51,551] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 01:41:51,552] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [5.0899992e-22 5.5143594e-15 3.4031135e-20 9.7508474e-17 9.1765809e-17
 2.3555676e-21 1.0000000e+00], sampled 0.7471252268619711
[2019-04-04 01:42:04,787] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.37414968], dtype=float32), 0.14790659]
[2019-04-04 01:42:04,787] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-4.666666666666667, 84.0, 0.0, 0.0, 26.0, 25.03393117210714, 0.2448641321437929, 0.0, 1.0, 30835.66562559727]
[2019-04-04 01:42:04,787] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 01:42:04,787] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.2367405e-22 4.3904305e-15 1.1736599e-20 3.6562958e-17 5.1596860e-17
 8.8362266e-22 1.0000000e+00], sampled 0.21117759203926367
[2019-04-04 01:42:44,468] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.37414968], dtype=float32), 0.14790659]
[2019-04-04 01:42:44,468] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-17.82325484, 68.55828409333333, 0.0, 0.0, 26.0, 22.90114438868202, -0.1694680826134541, 0.0, 1.0, 44419.34457113005]
[2019-04-04 01:42:44,468] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 01:42:44,469] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.4329678e-20 3.1814173e-13 1.3000961e-18 1.3341213e-15 2.5100996e-15
 1.7777317e-19 1.0000000e+00], sampled 0.8464963419783381
[2019-04-04 01:43:20,565] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.37414968], dtype=float32), 0.14790659]
[2019-04-04 01:43:20,565] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [8.6, 26.0, 0.0, 0.0, 26.0, 25.50364497276636, 0.3627464312145487, 0.0, 1.0, 33564.61652506736]
[2019-04-04 01:43:20,565] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 01:43:20,566] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [5.1362033e-23 4.0794881e-15 1.8860560e-21 6.1602510e-18 1.7450325e-17
 1.9272054e-22 1.0000000e+00], sampled 0.8409553371973877
[2019-04-04 01:43:33,149] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 01:44:05,257] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 01:44:08,439] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 01:44:09,473] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 2600000, evaluation results [2600000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 01:44:22,078] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.2574957e-22 3.2235728e-15 2.1111478e-20 5.7752910e-17 6.1673078e-17
 8.9897690e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:44:22,078] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9643
[2019-04-04 01:44:22,191] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.899999999999999, 71.0, 0.0, 0.0, 26.0, 24.49943217045107, 0.1139857434458505, 0.0, 1.0, 41302.88315288259], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 690000.0000, 
sim time next is 690600.0000, 
raw observation next is [-3.9, 71.0, 0.0, 0.0, 26.0, 24.46910440084979, 0.1084123731199622, 0.0, 1.0, 41223.7609573981], 
processed observation next is [0.0, 1.0, 0.3545706371191136, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5390920334041492, 0.5361374577066541, 0.0, 1.0, 0.19630362360665762], 
reward next is 0.8037, 
noisyNet noise sample is [array([0.8775632], dtype=float32), -0.26468393]. 
=============================================
[2019-04-04 01:44:23,167] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.3465823e-24 3.1511697e-15 5.7790695e-21 2.0127912e-18 1.1069866e-17
 6.7141706e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 01:44:23,167] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1587
[2019-04-04 01:44:23,217] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 75.0, 0.0, 0.0, 26.0, 24.27173470705799, 0.05338054274654772, 0.0, 1.0, 41433.56783361636], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 702000.0000, 
sim time next is 702600.0000, 
raw observation next is [-3.3, 75.0, 0.0, 0.0, 26.0, 24.2711637327798, 0.05503330739065138, 0.0, 1.0, 41505.69802623042], 
processed observation next is [1.0, 0.13043478260869565, 0.37119113573407203, 0.75, 0.0, 0.0, 0.6666666666666666, 0.52259697773165, 0.5183444357968838, 0.0, 1.0, 0.1976461810772877], 
reward next is 0.8024, 
noisyNet noise sample is [array([0.30649325], dtype=float32), 0.41606024]. 
=============================================
[2019-04-04 01:44:26,421] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.9892832e-23 1.6479604e-15 7.5116094e-21 8.7025704e-18 2.6386193e-17
 1.3491992e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:44:26,421] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7366
[2019-04-04 01:44:26,471] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.9, 75.0, 0.0, 0.0, 26.0, 24.3152449616222, 0.04604373455308772, 0.0, 1.0, 41637.81335645763], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 705000.0000, 
sim time next is 705600.0000, 
raw observation next is [-2.8, 75.0, 0.0, 0.0, 26.0, 24.26489863165661, 0.03775188827597047, 0.0, 1.0, 41647.60692278988], 
processed observation next is [1.0, 0.17391304347826086, 0.38504155124653744, 0.75, 0.0, 0.0, 0.6666666666666666, 0.5220748859713842, 0.5125839627586568, 0.0, 1.0, 0.19832193772757084], 
reward next is 0.8017, 
noisyNet noise sample is [array([1.2721323], dtype=float32), -0.5184849]. 
=============================================
[2019-04-04 01:44:30,757] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3306386e-24 8.4106309e-17 6.8702205e-23 1.1004306e-18 5.0436190e-19
 2.9543703e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 01:44:30,757] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8617
[2019-04-04 01:44:30,907] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 75.0, 99.0, 0.0, 26.0, 25.51651485372726, 0.292360921801498, 1.0, 1.0, 27797.11344947165], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 822600.0000, 
sim time next is 823200.0000, 
raw observation next is [-4.5, 76.33333333333333, 97.66666666666666, 0.0, 26.0, 25.43856129370378, 0.2890968198612372, 1.0, 1.0, 27884.98156795038], 
processed observation next is [1.0, 0.5217391304347826, 0.3379501385041552, 0.7633333333333333, 0.32555555555555554, 0.0, 0.6666666666666666, 0.6198801078086484, 0.5963656066204124, 1.0, 1.0, 0.13278562651404943], 
reward next is 0.8672, 
noisyNet noise sample is [array([2.0316408], dtype=float32), -0.8406384]. 
=============================================
[2019-04-04 01:44:47,591] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.6305184e-25 1.1928356e-17 2.5343933e-23 4.2064404e-19 2.0636764e-19
 2.8850724e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 01:44:47,591] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1163
[2019-04-04 01:44:47,616] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.800000000000001, 93.33333333333334, 0.0, 0.0, 26.0, 25.6340139651921, 0.373607108816002, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 926400.0000, 
sim time next is 927000.0000, 
raw observation next is [4.7, 94.0, 0.0, 0.0, 26.0, 25.48897083306646, 0.3532077745564017, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.592797783933518, 0.94, 0.0, 0.0, 0.6666666666666666, 0.6240809027555384, 0.6177359248521339, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.10918181], dtype=float32), 2.1103964]. 
=============================================
[2019-04-04 01:44:47,638] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[87.22822 ]
 [87.308464]
 [87.43243 ]
 [87.50107 ]
 [86.71958 ]], R is [[87.40320587]
 [87.5291748 ]
 [87.65388489]
 [87.77734375]
 [86.963974  ]].
[2019-04-04 01:44:50,404] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.6683359e-26 2.4579442e-17 9.4073604e-24 1.8504322e-19 3.3412019e-20
 1.1862421e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 01:44:50,404] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7194
[2019-04-04 01:44:50,411] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.23333333333333, 88.33333333333333, 100.0, 0.0, 26.0, 26.62812961089266, 0.6615781517978238, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 988800.0000, 
sim time next is 989400.0000, 
raw observation next is [11.41666666666667, 87.16666666666667, 104.0, 0.0, 26.0, 26.65191340074291, 0.6680436545040082, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.7788550323176363, 0.8716666666666667, 0.3466666666666667, 0.0, 0.6666666666666666, 0.7209927833952424, 0.7226812181680028, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0491872], dtype=float32), -1.7378973]. 
=============================================
[2019-04-04 01:45:00,797] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [7.5727242e-27 3.8076026e-19 7.5435581e-24 2.2500587e-20 1.0030654e-20
 3.1610125e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 01:45:00,797] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.8782
[2019-04-04 01:45:00,808] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 76.66666666666667, 0.0, 0.0, 26.0, 25.50938651676638, 0.6034302538747894, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1026600.0000, 
sim time next is 1027200.0000, 
raw observation next is [14.4, 76.33333333333334, 0.0, 0.0, 26.0, 25.73072201518756, 0.619269484806908, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.8614958448753465, 0.7633333333333334, 0.0, 0.0, 0.6666666666666666, 0.6442268345989633, 0.7064231616023027, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.73497546], dtype=float32), 1.1459996]. 
=============================================
[2019-04-04 01:45:08,132] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.3219600e-26 9.0213922e-18 8.9215025e-24 1.1356691e-19 4.7557124e-20
 9.4291595e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 01:45:08,132] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8415
[2019-04-04 01:45:08,146] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.6, 58.00000000000001, 0.0, 0.0, 26.0, 26.15194938505705, 0.7121613283182157, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1106400.0000, 
sim time next is 1107000.0000, 
raw observation next is [14.4, 58.5, 0.0, 0.0, 26.0, 26.01673206486544, 0.6957325988332679, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.8614958448753465, 0.585, 0.0, 0.0, 0.6666666666666666, 0.6680610054054533, 0.7319108662777559, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.23230575], dtype=float32), 0.15888245]. 
=============================================
[2019-04-04 01:45:08,182] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[95.50528 ]
 [96.574234]
 [94.690445]
 [93.66837 ]
 [95.809784]], R is [[95.00719452]
 [95.05712128]
 [95.10655212]
 [95.15548706]
 [95.20393372]].
[2019-04-04 01:45:18,454] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.8328625e-24 2.0715669e-16 3.7479365e-22 3.2948049e-18 6.4973201e-18
 2.7840388e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 01:45:18,455] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4115
[2019-04-04 01:45:18,478] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.5, 100.0, 0.0, 0.0, 26.0, 25.43356038561261, 0.5900047325583417, 0.0, 1.0, 53318.42179421529], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1287600.0000, 
sim time next is 1288200.0000, 
raw observation next is [5.5, 100.0, 0.0, 0.0, 26.0, 25.43213843851156, 0.5930369849193565, 0.0, 1.0, 43798.48455027219], 
processed observation next is [0.0, 0.9130434782608695, 0.6149584487534627, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6193448698759635, 0.6976789949731188, 0.0, 1.0, 0.20856421214415327], 
reward next is 0.7914, 
noisyNet noise sample is [array([0.77244747], dtype=float32), -0.60065913]. 
=============================================
[2019-04-04 01:45:33,385] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.00232225e-22 2.42447808e-15 1.03477185e-20 5.05460004e-18
 1.44257156e-17 3.41817621e-22 1.00000000e+00], sum to 1.0000
[2019-04-04 01:45:33,385] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9844
[2019-04-04 01:45:33,419] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 86.33333333333333, 0.0, 0.0, 26.0, 25.07678731633122, 0.3891389999760537, 0.0, 1.0, 43279.59381557771], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1743000.0000, 
sim time next is 1743600.0000, 
raw observation next is [-0.6, 85.66666666666667, 0.0, 0.0, 26.0, 25.05453702888274, 0.3846517031207497, 0.0, 1.0, 43325.28732294629], 
processed observation next is [0.0, 0.17391304347826086, 0.44598337950138506, 0.8566666666666667, 0.0, 0.0, 0.6666666666666666, 0.5878780857402285, 0.6282172343735832, 0.0, 1.0, 0.20631089201402997], 
reward next is 0.7937, 
noisyNet noise sample is [array([-0.9155514], dtype=float32), 0.49016008]. 
=============================================
[2019-04-04 01:45:38,739] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.7182668e-24 1.9109667e-16 1.7587891e-21 5.0735123e-18 1.5515430e-17
 1.1309981e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:45:38,740] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1653
[2019-04-04 01:45:38,766] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 26.0, 25.54763399325637, 0.5095550608457321, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1471200.0000, 
sim time next is 1471800.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 26.0, 25.59205124781221, 0.5153228607210237, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.5069252077562327, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6326709373176843, 0.6717742869070079, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.22746597], dtype=float32), -1.5563117]. 
=============================================
[2019-04-04 01:45:43,885] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2707903e-23 4.1263951e-15 7.7069217e-21 3.1039694e-17 3.0024744e-17
 3.9062718e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:45:43,886] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4371
[2019-04-04 01:45:43,923] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 26.0, 25.21355004619539, 0.4706732029652509, 0.0, 1.0, 39635.23691453414], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1391400.0000, 
sim time next is 1392000.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 26.0, 25.26038350835589, 0.4748243062714487, 0.0, 1.0, 39573.48333031681], 
processed observation next is [1.0, 0.08695652173913043, 0.46260387811634357, 0.95, 0.0, 0.0, 0.6666666666666666, 0.6050319590296575, 0.6582747687571496, 0.0, 1.0, 0.18844515871579434], 
reward next is 0.8116, 
noisyNet noise sample is [array([0.13170409], dtype=float32), 0.36298984]. 
=============================================
[2019-04-04 01:45:43,933] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[82.99814 ]
 [83.04828 ]
 [83.09832 ]
 [83.10292 ]
 [83.102905]], R is [[82.99243164]
 [82.97377014]
 [82.95500183]
 [82.9360199 ]
 [82.91655731]].
[2019-04-04 01:45:53,661] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2651734e-25 9.6461378e-19 3.0854846e-23 5.8140126e-19 1.7055437e-19
 5.6748524e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 01:45:53,661] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4472
[2019-04-04 01:45:53,716] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.5, 61.0, 0.0, 0.0, 26.0, 26.5607703335678, 0.7472992262800741, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1620000.0000, 
sim time next is 1620600.0000, 
raw observation next is [10.31666666666667, 61.83333333333334, 0.0, 0.0, 26.0, 26.56742472329142, 0.7409429479163827, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.7483841181902126, 0.6183333333333334, 0.0, 0.0, 0.6666666666666666, 0.713952060274285, 0.7469809826387942, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.40786147], dtype=float32), -0.19565119]. 
=============================================
[2019-04-04 01:45:57,453] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.5393301e-25 1.1970932e-17 2.4736747e-22 8.6563128e-19 6.7743859e-19
 2.8808398e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 01:45:57,456] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2722
[2019-04-04 01:45:57,508] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 74.5, 0.0, 26.0, 25.89499361825269, 0.5372907609760166, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1681200.0000, 
sim time next is 1681800.0000, 
raw observation next is [1.1, 90.66666666666667, 77.33333333333334, 0.0, 26.0, 25.90716680905187, 0.5332114642863558, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.49307479224376743, 0.9066666666666667, 0.25777777777777783, 0.0, 0.6666666666666666, 0.6589305674209891, 0.6777371547621186, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.49606422], dtype=float32), 1.2783293]. 
=============================================
[2019-04-04 01:45:58,906] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.7656285e-25 2.8711624e-17 4.6789218e-23 3.6256526e-19 1.6754363e-18
 4.0174928e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 01:45:58,906] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1014
[2019-04-04 01:45:58,926] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.7, 60.5, 0.0, 0.0, 26.0, 26.3052705064862, 0.6687042857081772, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1535400.0000, 
sim time next is 1536000.0000, 
raw observation next is [9.600000000000001, 60.66666666666667, 0.0, 0.0, 26.0, 26.1921495738383, 0.6670877595694823, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.7285318559556788, 0.6066666666666667, 0.0, 0.0, 0.6666666666666666, 0.6826791311531917, 0.7223625865231608, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1559572], dtype=float32), 0.48497397]. 
=============================================
[2019-04-04 01:45:58,933] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[86.30959]
 [86.49184]
 [86.60143]
 [86.59147]
 [86.31602]], R is [[86.39089966]
 [86.5269928 ]
 [86.66172028]
 [86.79510498]
 [86.92715454]].
[2019-04-04 01:46:02,488] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.2959843e-21 2.4624246e-14 5.2310643e-20 9.6776227e-17 2.7292005e-16
 3.9883511e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:46:02,489] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7683
[2019-04-04 01:46:02,516] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 85.66666666666667, 0.0, 0.0, 26.0, 24.97014485752009, 0.3563164569113353, 0.0, 1.0, 43686.58879984014], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1748400.0000, 
sim time next is 1749000.0000, 
raw observation next is [-1.1, 86.33333333333333, 0.0, 0.0, 26.0, 24.94396700035031, 0.3499636171314837, 0.0, 1.0, 43751.74241670171], 
processed observation next is [0.0, 0.21739130434782608, 0.4321329639889197, 0.8633333333333333, 0.0, 0.0, 0.6666666666666666, 0.578663916695859, 0.6166545390438279, 0.0, 1.0, 0.2083416305557224], 
reward next is 0.7917, 
noisyNet noise sample is [array([1.2946035], dtype=float32), -0.7247453]. 
=============================================
[2019-04-04 01:46:02,541] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[80.444275]
 [80.42304 ]
 [80.39464 ]
 [80.37375 ]
 [80.37566 ]], R is [[80.44624329]
 [80.43375397]
 [80.4216156 ]
 [80.409729  ]
 [80.39817047]].
[2019-04-04 01:46:05,033] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.6253433e-23 2.9407544e-16 3.1686143e-21 1.1978332e-17 1.2125355e-17
 3.5141494e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 01:46:05,036] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9443
[2019-04-04 01:46:05,052] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.4867065371, 0.1906146497998635, 0.0, 1.0, 42754.11672610916], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1986600.0000, 
sim time next is 1987200.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.45577426068806, 0.1849205436057038, 0.0, 1.0, 42651.76270200649], 
processed observation next is [1.0, 0.0, 0.30747922437673136, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5379811883906717, 0.5616401812019013, 0.0, 1.0, 0.20310363191431663], 
reward next is 0.7969, 
noisyNet noise sample is [array([1.3251896], dtype=float32), -0.35061398]. 
=============================================
[2019-04-04 01:46:14,975] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.2216696e-21 9.6701415e-15 8.0022648e-20 1.8317465e-16 3.1435563e-16
 3.5973241e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:46:14,975] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4058
[2019-04-04 01:46:15,039] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.199999999999999, 72.33333333333333, 173.3333333333333, 67.5, 26.0, 24.9533912057362, 0.2616909529673233, 0.0, 1.0, 46737.89656451304], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1856400.0000, 
sim time next is 1857000.0000, 
raw observation next is [-5.1, 71.66666666666667, 162.6666666666667, 54.00000000000001, 26.0, 24.9633582471834, 0.2639561247342531, 0.0, 1.0, 40201.99411507563], 
processed observation next is [0.0, 0.4782608695652174, 0.3213296398891967, 0.7166666666666667, 0.5422222222222224, 0.059668508287292824, 0.6666666666666666, 0.58027985393195, 0.5879853749114177, 0.0, 1.0, 0.19143806721464585], 
reward next is 0.8086, 
noisyNet noise sample is [array([0.5512872], dtype=float32), -1.6724923]. 
=============================================
[2019-04-04 01:46:15,042] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[77.748825]
 [78.332634]
 [78.49065 ]
 [78.750046]
 [78.960754]], R is [[77.17537689]
 [77.18106079]
 [77.15675354]
 [77.14081573]
 [77.17649078]].
[2019-04-04 01:46:27,967] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.2223422e-23 3.0084948e-16 1.6535599e-21 3.4731130e-17 9.2214583e-18
 2.3559607e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 01:46:27,967] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2801
[2019-04-04 01:46:28,023] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.916666666666667, 77.66666666666667, 0.0, 0.0, 26.0, 25.36889626559109, 0.3383794335069452, 1.0, 1.0, 31225.55352242813], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1966200.0000, 
sim time next is 1966800.0000, 
raw observation next is [-4.833333333333334, 76.33333333333334, 0.0, 0.0, 26.0, 25.31884531788874, 0.3106445252762683, 1.0, 1.0, 29428.87244001735], 
processed observation next is [1.0, 0.782608695652174, 0.32871652816251157, 0.7633333333333334, 0.0, 0.0, 0.6666666666666666, 0.6099037764907284, 0.6035481750920894, 1.0, 1.0, 0.14013748780960641], 
reward next is 0.8599, 
noisyNet noise sample is [array([-1.532086], dtype=float32), -0.5198844]. 
=============================================
[2019-04-04 01:46:39,560] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.0075459e-24 6.9013684e-17 5.2213701e-22 2.4387685e-18 2.6620229e-18
 8.2079699e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 01:46:39,560] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5031
[2019-04-04 01:46:39,628] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.8999999999999999, 50.5, 0.0, 0.0, 26.0, 24.89548908209278, 0.3161692003843071, 1.0, 1.0, 135165.556747875], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2309400.0000, 
sim time next is 2310000.0000, 
raw observation next is [-1.0, 51.0, 0.0, 0.0, 26.0, 25.14712304855039, 0.3949133395880278, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.4349030470914128, 0.51, 0.0, 0.0, 0.6666666666666666, 0.5955935873791992, 0.6316377798626759, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.9247416], dtype=float32), -0.46183276]. 
=============================================
[2019-04-04 01:46:39,635] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[84.25427 ]
 [84.168846]
 [84.32653 ]
 [84.61112 ]
 [84.777855]], R is [[84.62287903]
 [84.13300323]
 [83.35372925]
 [83.15007019]
 [83.318573  ]].
[2019-04-04 01:46:39,987] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.3935226e-23 9.1998938e-16 2.4466294e-21 1.4710125e-17 4.9687813e-18
 2.0859214e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:46:39,987] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9405
[2019-04-04 01:46:40,039] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.466666666666667, 80.0, 60.16666666666666, 30.83333333333334, 26.0, 25.2832614266921, 0.3149290896026654, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2103600.0000, 
sim time next is 2104200.0000, 
raw observation next is [-7.55, 80.5, 72.0, 37.0, 26.0, 25.4089837599761, 0.3263693067428794, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.25346260387811637, 0.805, 0.24, 0.04088397790055249, 0.6666666666666666, 0.6174153133313416, 0.6087897689142932, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.77010924], dtype=float32), -1.0816741]. 
=============================================
[2019-04-04 01:46:43,092] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.6419363e-22 6.7455696e-15 9.0466094e-20 3.2752718e-16 8.0731968e-17
 3.0057227e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:46:43,092] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7530
[2019-04-04 01:46:43,123] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.533333333333333, 84.33333333333333, 0.0, 0.0, 26.0, 24.18206743030889, 0.08848944376720148, 0.0, 1.0, 43424.79155368249], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2094000.0000, 
sim time next is 2094600.0000, 
raw observation next is [-6.616666666666667, 83.66666666666667, 0.0, 0.0, 26.0, 24.13590745055667, 0.07610600722437852, 0.0, 1.0, 43500.84823573153], 
processed observation next is [1.0, 0.21739130434782608, 0.2793167128347184, 0.8366666666666667, 0.0, 0.0, 0.6666666666666666, 0.5113256208797224, 0.5253686690747928, 0.0, 1.0, 0.20714689636062633], 
reward next is 0.7929, 
noisyNet noise sample is [array([-0.26925093], dtype=float32), -0.106198944]. 
=============================================
[2019-04-04 01:46:50,330] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.9157598e-24 3.3408009e-16 5.1998232e-22 2.7015723e-18 1.3091288e-18
 2.1792399e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 01:46:50,330] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8874
[2019-04-04 01:46:50,374] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.8, 82.00000000000001, 140.0, 91.0, 26.0, 25.56879308393145, 0.3452785572969356, 1.0, 1.0, 18737.89968702337], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2106600.0000, 
sim time next is 2107200.0000, 
raw observation next is [-7.8, 82.0, 157.0, 104.5, 26.0, 25.57576201268981, 0.3607681434885109, 1.0, 1.0, 27255.43171760517], 
processed observation next is [1.0, 0.391304347826087, 0.24653739612188366, 0.82, 0.5233333333333333, 0.11546961325966851, 0.6666666666666666, 0.6313135010574843, 0.6202560478295036, 1.0, 1.0, 0.12978777008383413], 
reward next is 0.8702, 
noisyNet noise sample is [array([0.83474445], dtype=float32), -1.9037206]. 
=============================================
[2019-04-04 01:46:50,771] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.1289601e-25 1.9358458e-17 1.7088559e-22 2.8864257e-18 4.3326083e-18
 3.5806570e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 01:46:50,771] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7541
[2019-04-04 01:46:50,871] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 72.0, 0.0, 0.0, 26.0, 26.08795524657295, 0.4952905593743821, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2139600.0000, 
sim time next is 2140200.0000, 
raw observation next is [-5.0, 72.5, 0.0, 0.0, 26.0, 25.91512715548837, 0.4684587856039416, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.32409972299168976, 0.725, 0.0, 0.0, 0.6666666666666666, 0.6595939296240309, 0.6561529285346471, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.634293], dtype=float32), 0.83194005]. 
=============================================
[2019-04-04 01:46:56,567] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.23394956e-23 3.31274215e-16 9.28518820e-22 5.72455430e-18
 2.41895921e-18 4.97567263e-23 1.00000000e+00], sum to 1.0000
[2019-04-04 01:46:56,567] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1476
[2019-04-04 01:46:56,657] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 75.0, 8.499999999999998, 43.66666666666666, 26.0, 24.57436344321753, 0.2695301684499496, 1.0, 1.0, 128403.550681884], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2187600.0000, 
sim time next is 2188200.0000, 
raw observation next is [-5.6, 75.0, 15.0, 87.33333333333331, 26.0, 25.2202794939473, 0.3099709990197004, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.30747922437673136, 0.75, 0.05, 0.09650092081031306, 0.6666666666666666, 0.6016899578289415, 0.6033236663399001, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.38636732], dtype=float32), -0.38141957]. 
=============================================
[2019-04-04 01:47:07,340] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3084670e-22 5.6654722e-15 1.7316083e-20 3.1475744e-17 1.0249603e-16
 1.5054800e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:47:07,340] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6669
[2019-04-04 01:47:07,374] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 47.33333333333333, 0.0, 0.0, 26.0, 25.03423804158064, 0.1827555784438874, 0.0, 1.0, 38425.79519834371], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2518800.0000, 
sim time next is 2519400.0000, 
raw observation next is [-1.7, 48.16666666666667, 0.0, 0.0, 26.0, 24.99354677001551, 0.1776982852619892, 0.0, 1.0, 38414.98633304137], 
processed observation next is [1.0, 0.13043478260869565, 0.4155124653739613, 0.4816666666666667, 0.0, 0.0, 0.6666666666666666, 0.5827955641679591, 0.5592327617539964, 0.0, 1.0, 0.18292850634781604], 
reward next is 0.8171, 
noisyNet noise sample is [array([0.17345002], dtype=float32), -0.40569168]. 
=============================================
[2019-04-04 01:47:07,836] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.5414817e-24 1.9035019e-16 3.6604731e-22 9.1503944e-18 1.0043873e-18
 2.5438940e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 01:47:07,837] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7108
[2019-04-04 01:47:07,861] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.3666666666666668, 43.66666666666667, 121.8333333333333, 35.0, 26.0, 25.57042682772497, 0.3951465266145414, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2302800.0000, 
sim time next is 2303400.0000, 
raw observation next is [0.1833333333333333, 43.83333333333334, 105.6666666666667, 28.0, 26.0, 25.76705454118275, 0.41115064529862, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.46768236380424755, 0.4383333333333334, 0.3522222222222223, 0.030939226519337018, 0.6666666666666666, 0.6472545450985624, 0.63705021509954, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.29404354], dtype=float32), 0.92163295]. 
=============================================
[2019-04-04 01:47:08,594] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.7838649e-22 1.4568721e-14 6.0828807e-20 9.0853224e-17 1.7905568e-16
 3.8337459e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:47:08,595] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4244
[2019-04-04 01:47:08,689] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.3, 86.83333333333333, 0.0, 0.0, 26.0, 24.05853144884542, 0.07281443966173014, 0.0, 1.0, 43638.49646676458], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2260200.0000, 
sim time next is 2260800.0000, 
raw observation next is [-8.4, 87.0, 0.0, 0.0, 26.0, 24.05992721602802, 0.06347761018118757, 0.0, 1.0, 43610.77796549167], 
processed observation next is [1.0, 0.17391304347826086, 0.2299168975069252, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5049939346690016, 0.5211592033937292, 0.0, 1.0, 0.20767037126424603], 
reward next is 0.7923, 
noisyNet noise sample is [array([-0.42780998], dtype=float32), 0.6365571]. 
=============================================
[2019-04-04 01:47:09,700] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [4.0291019e-22 1.3591306e-14 1.1886147e-19 1.7520168e-16 7.7006187e-17
 4.8087596e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:47:09,700] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.9144
[2019-04-04 01:47:09,768] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.0, 91.00000000000001, 0.0, 0.0, 26.0, 23.57431751789938, -0.0357199833851567, 0.0, 1.0, 43182.43247501701], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2268600.0000, 
sim time next is 2269200.0000, 
raw observation next is [-9.100000000000001, 91.0, 0.0, 0.0, 26.0, 23.59970606435493, -0.04778790735273014, 0.0, 1.0, 43156.98722186944], 
processed observation next is [1.0, 0.2608695652173913, 0.21052631578947364, 0.91, 0.0, 0.0, 0.6666666666666666, 0.4666421720295775, 0.48407069754909, 0.0, 1.0, 0.20550946296128306], 
reward next is 0.7945, 
noisyNet noise sample is [array([0.44338438], dtype=float32), -0.76553947]. 
=============================================
[2019-04-04 01:47:10,080] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [4.6689266e-20 1.6656277e-12 1.5003512e-18 9.3413504e-16 4.9789588e-15
 3.4519232e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 01:47:10,080] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0218
[2019-04-04 01:47:10,165] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.3, 60.66666666666667, 0.0, 0.0, 26.0, 23.07434229941386, -0.1901171284559354, 0.0, 1.0, 44038.86235203341], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2443200.0000, 
sim time next is 2443800.0000, 
raw observation next is [-9.4, 60.83333333333334, 0.0, 0.0, 26.0, 23.03151772895281, -0.1996443887961127, 0.0, 1.0, 44056.21939400317], 
processed observation next is [0.0, 0.2608695652173913, 0.20221606648199447, 0.6083333333333334, 0.0, 0.0, 0.6666666666666666, 0.4192931440794008, 0.43345187040129574, 0.0, 1.0, 0.20979152092382464], 
reward next is 0.7902, 
noisyNet noise sample is [array([1.1837395], dtype=float32), -0.52523625]. 
=============================================
[2019-04-04 01:47:12,827] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0250192e-23 5.7192531e-17 3.2579284e-22 2.5091287e-18 2.9226119e-18
 1.4323024e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 01:47:12,827] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1057
[2019-04-04 01:47:12,890] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.1333333333333334, 35.33333333333334, 0.0, 0.0, 26.0, 25.22870236015867, 0.3476266848262554, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2571600.0000, 
sim time next is 2572200.0000, 
raw observation next is [-0.04999999999999999, 35.5, 0.0, 0.0, 26.0, 25.19698900871095, 0.3269688187044775, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.461218836565097, 0.355, 0.0, 0.0, 0.6666666666666666, 0.5997490840592459, 0.6089896062348258, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3658356], dtype=float32), 1.8411075]. 
=============================================
[2019-04-04 01:47:13,290] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.9414858e-23 1.2737020e-16 9.5512148e-22 5.1815975e-18 4.4169828e-18
 1.8956692e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 01:47:13,290] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4005
[2019-04-04 01:47:13,362] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.2, 52.0, 0.0, 0.0, 26.0, 25.50686074055875, 0.4156808413630275, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2311200.0000, 
sim time next is 2311800.0000, 
raw observation next is [-1.2, 52.33333333333334, 0.0, 0.0, 26.0, 25.59389466084579, 0.418846898110051, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.42936288088642666, 0.5233333333333334, 0.0, 0.0, 0.6666666666666666, 0.6328245550704826, 0.6396156327033503, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.00800862], dtype=float32), 0.32830748]. 
=============================================
[2019-04-04 01:47:33,221] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.41059358e-23 1.01454284e-16 3.13479460e-21 8.08194766e-17
 2.84392205e-17 2.57444173e-23 1.00000000e+00], sum to 1.0000
[2019-04-04 01:47:33,224] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0075
[2019-04-04 01:47:33,254] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.433333333333333, 52.0, 0.0, 0.0, 26.0, 25.11085527817831, 0.4046251918533503, 0.0, 1.0, 61779.19744075568], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2580000.0000, 
sim time next is 2580600.0000, 
raw observation next is [-2.616666666666667, 54.0, 0.0, 0.0, 26.0, 25.28989237578768, 0.4157496878829272, 0.0, 1.0, 48658.19523439195], 
processed observation next is [1.0, 0.8695652173913043, 0.3901200369344414, 0.54, 0.0, 0.0, 0.6666666666666666, 0.60749103131564, 0.6385832292943091, 0.0, 1.0, 0.2317056915923426], 
reward next is 0.7683, 
noisyNet noise sample is [array([-0.60958844], dtype=float32), 1.5342951]. 
=============================================
[2019-04-04 01:47:37,879] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.9344147e-23 4.4185692e-17 1.6377902e-21 4.8864322e-18 4.8659338e-18
 2.4436514e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 01:47:37,884] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5651
[2019-04-04 01:47:37,963] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 59.0, 0.0, 0.0, 26.0, 25.05547765824042, 0.4105037993891509, 1.0, 1.0, 87055.91005170991], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2749800.0000, 
sim time next is 2750400.0000, 
raw observation next is [-5.0, 59.0, 0.0, 0.0, 26.0, 25.11392675087005, 0.4170734548615414, 0.0, 1.0, 37701.46491945855], 
processed observation next is [1.0, 0.8695652173913043, 0.32409972299168976, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5928272292391709, 0.6390244849538471, 0.0, 1.0, 0.179530785330755], 
reward next is 0.8205, 
noisyNet noise sample is [array([0.7577974], dtype=float32), 0.2960953]. 
=============================================
[2019-04-04 01:47:41,523] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3165480e-22 1.2442454e-14 3.0331488e-20 6.3893972e-17 1.0566533e-16
 9.5091520e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:47:41,524] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0140
[2019-04-04 01:47:41,558] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.383333333333333, 56.5, 0.0, 0.0, 26.0, 24.73906990825132, 0.1256096568207206, 0.0, 1.0, 38627.51128689079], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2527800.0000, 
sim time next is 2528400.0000, 
raw observation next is [-2.466666666666667, 56.00000000000001, 0.0, 0.0, 26.0, 24.72175207042626, 0.1221395393964346, 0.0, 1.0, 38693.8301045395], 
processed observation next is [1.0, 0.2608695652173913, 0.39427516158818104, 0.56, 0.0, 0.0, 0.6666666666666666, 0.5601460058688549, 0.5407131797988115, 0.0, 1.0, 0.18425633383114046], 
reward next is 0.8157, 
noisyNet noise sample is [array([0.01430072], dtype=float32), -0.7829783]. 
=============================================
[2019-04-04 01:47:47,828] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.9469065e-23 1.1840599e-15 7.1418354e-21 6.2312020e-18 1.4217352e-17
 4.7111600e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 01:47:47,828] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.6491
[2019-04-04 01:47:47,858] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 96.5, 49.0, 0.0, 26.0, 25.38981470608445, 0.3097593843282179, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2885400.0000, 
sim time next is 2886000.0000, 
raw observation next is [0.3333333333333334, 97.66666666666666, 53.83333333333333, 0.0, 26.0, 25.43062423849855, 0.3127764253049045, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4718374884579871, 0.9766666666666666, 0.17944444444444443, 0.0, 0.6666666666666666, 0.6192186865415458, 0.6042588084349682, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3929052], dtype=float32), -0.40826792]. 
=============================================
[2019-04-04 01:47:47,865] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[81.5119 ]
 [81.9397 ]
 [82.33213]
 [83.00705]
 [83.98998]], R is [[81.85038757]
 [82.03188324]
 [82.21156311]
 [82.38945007]
 [82.56555939]].
[2019-04-04 01:47:54,003] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.3745193e-22 1.6261102e-15 4.0075233e-20 2.6305189e-17 1.0486903e-16
 4.5628194e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:47:54,004] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0620
[2019-04-04 01:47:54,041] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 25.04353643760982, 0.3293582275861105, 0.0, 1.0, 45928.1318316438], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2760600.0000, 
sim time next is 2761200.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.98980059220864, 0.3181576749949248, 0.0, 1.0, 45607.87215979794], 
processed observation next is [1.0, 1.0, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5824833826840532, 0.6060525583316416, 0.0, 1.0, 0.21718034361808541], 
reward next is 0.7828, 
noisyNet noise sample is [array([1.0216814], dtype=float32), -1.0601035]. 
=============================================
[2019-04-04 01:47:58,167] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.5325244e-22 5.0192343e-15 9.3523127e-20 1.7828007e-16 1.1723423e-16
 2.9413192e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:47:58,167] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9800
[2019-04-04 01:47:58,186] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.83170397725888, 0.3347308395078181, 0.0, 1.0, 43323.48322472895], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2940600.0000, 
sim time next is 2941200.0000, 
raw observation next is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.88031317309304, 0.3310345491023737, 0.0, 1.0, 43297.67905520149], 
processed observation next is [0.0, 0.043478260869565216, 0.40720221606648205, 0.85, 0.0, 0.0, 0.6666666666666666, 0.5733594310910867, 0.6103448497007912, 0.0, 1.0, 0.20617942407238807], 
reward next is 0.7938, 
noisyNet noise sample is [array([1.4282911], dtype=float32), 1.559239]. 
=============================================
[2019-04-04 01:47:59,253] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.7740537e-22 1.6272528e-14 9.1005707e-20 1.3343191e-16 3.1728801e-16
 1.7890228e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:47:59,253] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6537
[2019-04-04 01:47:59,292] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.5, 96.5, 0.0, 0.0, 26.0, 24.73168222363366, 0.2261876861687102, 0.0, 1.0, 55379.38884654026], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2874600.0000, 
sim time next is 2875200.0000, 
raw observation next is [1.666666666666667, 95.33333333333334, 0.0, 0.0, 26.0, 24.77915244644671, 0.238348122571261, 0.0, 1.0, 54858.57438206342], 
processed observation next is [1.0, 0.2608695652173913, 0.5087719298245615, 0.9533333333333335, 0.0, 0.0, 0.6666666666666666, 0.5649293705372257, 0.5794493741904203, 0.0, 1.0, 0.2612313065812544], 
reward next is 0.7388, 
noisyNet noise sample is [array([-0.22838616], dtype=float32), 2.6886475]. 
=============================================
[2019-04-04 01:48:03,379] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4879569e-22 1.2432390e-15 2.6455999e-20 4.0807917e-17 6.7079444e-17
 1.1261038e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:48:03,379] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7115
[2019-04-04 01:48:03,400] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6666666666666666, 100.0, 0.0, 0.0, 26.0, 25.60613754539386, 0.5832202354286468, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3206400.0000, 
sim time next is 3207000.0000, 
raw observation next is [-0.8333333333333334, 100.0, 0.0, 0.0, 26.0, 25.65841584455035, 0.5563053709359195, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.43951985226223456, 1.0, 0.0, 0.0, 0.6666666666666666, 0.638201320379196, 0.6854351236453065, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.27083454], dtype=float32), -0.35445687]. 
=============================================
[2019-04-04 01:48:03,412] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[78.19803 ]
 [78.61243 ]
 [79.02674 ]
 [79.212555]
 [79.15482 ]], R is [[78.05058289]
 [78.27008057]
 [78.48738098]
 [78.41254425]
 [78.15787506]].
[2019-04-04 01:48:04,359] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.6720162e-23 1.9262924e-16 3.0087053e-21 1.9408665e-17 1.2091913e-17
 4.4466754e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 01:48:04,360] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5417
[2019-04-04 01:48:04,387] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 26.0, 25.41351859206494, 0.4990034255490309, 0.0, 1.0, 67274.73179093328], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3276600.0000, 
sim time next is 3277200.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 26.0, 25.43988817483739, 0.4883295703801847, 0.0, 1.0, 36367.76728815391], 
processed observation next is [1.0, 0.9565217391304348, 0.296398891966759, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6199906812364491, 0.6627765234600616, 0.0, 1.0, 0.17317984422930432], 
reward next is 0.8268, 
noisyNet noise sample is [array([1.2994707], dtype=float32), 1.6318598]. 
=============================================
[2019-04-04 01:48:08,095] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.4404120e-21 1.3416403e-13 1.6116317e-19 1.3795716e-16 1.7473714e-16
 7.7109502e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:48:08,096] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9563
[2019-04-04 01:48:08,118] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 70.0, 0.0, 0.0, 26.0, 23.78757533784637, -0.002851895053407522, 0.0, 1.0, 40185.80136078024], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3045600.0000, 
sim time next is 3046200.0000, 
raw observation next is [-6.0, 71.16666666666667, 0.0, 0.0, 26.0, 23.76000552961344, -0.009181010160241948, 0.0, 1.0, 40215.71112227936], 
processed observation next is [0.0, 0.2608695652173913, 0.296398891966759, 0.7116666666666667, 0.0, 0.0, 0.6666666666666666, 0.4800004608011201, 0.4969396632799194, 0.0, 1.0, 0.19150338629656838], 
reward next is 0.8085, 
noisyNet noise sample is [array([0.9890619], dtype=float32), -1.0371346]. 
=============================================
[2019-04-04 01:48:10,403] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0408959e-21 4.6516519e-14 4.8990815e-20 5.2248301e-17 1.5075240e-16
 1.6113357e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 01:48:10,403] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8891
[2019-04-04 01:48:10,419] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.04877380767008, 0.06151533515351556, 0.0, 1.0, 40112.34901749661], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3039000.0000, 
sim time next is 3039600.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.0189863889765, 0.05484934610696175, 0.0, 1.0, 40162.00141522152], 
processed observation next is [0.0, 0.17391304347826086, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.501582199081375, 0.5182831153689872, 0.0, 1.0, 0.19124762578676915], 
reward next is 0.8088, 
noisyNet noise sample is [array([-0.4301803], dtype=float32), -2.4001107]. 
=============================================
[2019-04-04 01:48:11,622] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.0062964e-22 7.0766621e-15 1.5541352e-20 7.6823982e-17 7.2225910e-17
 4.9688307e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:48:11,622] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0374
[2019-04-04 01:48:11,669] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 64.0, 42.0, 214.5, 26.0, 23.5599079640424, -0.02591098440141355, 0.0, 1.0, 40645.80537820391], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3052800.0000, 
sim time next is 3053400.0000, 
raw observation next is [-6.0, 63.16666666666667, 55.66666666666668, 262.6666666666667, 26.0, 23.59627182223154, -0.01032683484091365, 0.0, 1.0, 40458.15007326171], 
processed observation next is [0.0, 0.34782608695652173, 0.296398891966759, 0.6316666666666667, 0.18555555555555558, 0.29023941068139963, 0.6666666666666666, 0.4663559851859616, 0.4965577217196955, 0.0, 1.0, 0.19265785749172246], 
reward next is 0.8073, 
noisyNet noise sample is [array([0.29641595], dtype=float32), 0.35014364]. 
=============================================
[2019-04-04 01:48:12,151] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.9297360e-22 4.3240765e-16 9.6714773e-21 7.4290092e-18 3.3492464e-17
 1.7718524e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:48:12,152] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0326
[2019-04-04 01:48:12,208] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 92.0, 0.0, 0.0, 26.0, 24.99748039798364, 0.27530477630989, 0.0, 1.0, 37543.39411578916], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3092400.0000, 
sim time next is 3093000.0000, 
raw observation next is [-1.0, 92.0, 0.0, 0.0, 26.0, 24.99307021090812, 0.2783594962014384, 0.0, 1.0, 37392.48279228785], 
processed observation next is [0.0, 0.8260869565217391, 0.4349030470914128, 0.92, 0.0, 0.0, 0.6666666666666666, 0.5827558509090099, 0.5927864987338128, 0.0, 1.0, 0.17805944186803738], 
reward next is 0.8219, 
noisyNet noise sample is [array([0.18905644], dtype=float32), 0.7666633]. 
=============================================
[2019-04-04 01:48:12,215] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[82.87814 ]
 [82.76749 ]
 [82.82224 ]
 [83.057526]
 [83.29554 ]], R is [[83.07191467]
 [83.06241608]
 [83.06656647]
 [83.08579254]
 [83.1085434 ]].
[2019-04-04 01:48:14,098] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [7.85747587e-24 8.70157939e-17 9.85590648e-22 3.88781629e-18
 9.59677659e-18 1.23035385e-23 1.00000000e+00], sum to 1.0000
[2019-04-04 01:48:14,100] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9325
[2019-04-04 01:48:14,149] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.72215675263039, 0.4708367806089418, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3262200.0000, 
sim time next is 3262800.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.15009625866716, 0.4828489440101838, 1.0, 1.0, 196217.9094192962], 
processed observation next is [1.0, 0.782608695652174, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5958413548889299, 0.6609496480033946, 1.0, 1.0, 0.9343709972347438], 
reward next is 0.0656, 
noisyNet noise sample is [array([0.45391667], dtype=float32), -0.18580484]. 
=============================================
[2019-04-04 01:48:20,614] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.1331080e-23 5.2512083e-16 2.7747021e-20 3.8043833e-17 6.1789882e-17
 6.2785518e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:48:20,616] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9647
[2019-04-04 01:48:20,632] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.24873969859949, 0.388650353272429, 0.0, 1.0, 41890.30296305628], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3477000.0000, 
sim time next is 3477600.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.28543980315915, 0.3838747799258298, 0.0, 1.0, 41849.95172471827], 
processed observation next is [1.0, 0.2608695652173913, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6071199835965958, 0.6279582599752765, 0.0, 1.0, 0.1992854844034203], 
reward next is 0.8007, 
noisyNet noise sample is [array([-0.9905859], dtype=float32), -0.70418364]. 
=============================================
[2019-04-04 01:48:24,535] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.2770489e-24 4.5362411e-18 1.0112207e-22 8.0822479e-19 1.0458568e-18
 6.9011825e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 01:48:24,535] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5361
[2019-04-04 01:48:24,572] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.0, 100.0, 95.33333333333334, 735.0, 26.0, 27.6909242561071, 0.9627956356580049, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3163800.0000, 
sim time next is 3164400.0000, 
raw observation next is [7.0, 100.0, 92.5, 721.0, 26.0, 27.71824770992445, 0.8400871192407618, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.6565096952908588, 1.0, 0.30833333333333335, 0.7966850828729282, 0.6666666666666666, 0.8098539758270377, 0.7800290397469206, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.99976975], dtype=float32), 2.075041]. 
=============================================
[2019-04-04 01:48:25,628] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.8600027e-23 4.9114816e-15 6.9514506e-21 8.6038126e-17 8.7157645e-17
 3.9940583e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:48:25,631] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4305
[2019-04-04 01:48:25,649] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.94264865084124, 0.3213587915673476, 0.0, 1.0, 41197.8117180035], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3375600.0000, 
sim time next is 3376200.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.93775212899168, 0.3120916143822904, 0.0, 1.0, 41177.1884560047], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5781460107493066, 0.6040305381274301, 0.0, 1.0, 0.19608184979049856], 
reward next is 0.8039, 
noisyNet noise sample is [array([0.9177115], dtype=float32), 1.2004967]. 
=============================================
[2019-04-04 01:48:27,322] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.0536336e-24 5.7117065e-18 5.6906179e-23 2.6657827e-18 6.5721495e-19
 8.8727268e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 01:48:27,322] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8970
[2019-04-04 01:48:27,335] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.166666666666667, 46.66666666666667, 78.0, 616.3333333333334, 26.0, 25.50190748045217, 0.6000308716960121, 1.0, 1.0, 62378.03064504134], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3340200.0000, 
sim time next is 3340800.0000, 
raw observation next is [-2.0, 46.0, 73.5, 587.5, 26.0, 26.04204494118304, 0.6483160012345127, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.40720221606648205, 0.46, 0.245, 0.649171270718232, 0.6666666666666666, 0.6701704117652533, 0.7161053337448376, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.18614635], dtype=float32), 0.52387625]. 
=============================================
[2019-04-04 01:48:44,272] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.0487183e-24 1.7358777e-16 1.5907764e-22 1.4131318e-18 2.0599494e-18
 7.7126741e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 01:48:44,272] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0811
[2019-04-04 01:48:44,283] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.16666666666667, 27.33333333333334, 107.6666666666667, 729.6666666666667, 26.0, 25.65056888328702, 0.4707984550366519, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3665400.0000, 
sim time next is 3666000.0000, 
raw observation next is [11.33333333333333, 26.66666666666667, 109.3333333333333, 746.3333333333334, 26.0, 25.63765463590512, 0.4746945818000386, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.7765466297322253, 0.2666666666666667, 0.36444444444444435, 0.8246777163904236, 0.6666666666666666, 0.6364712196587599, 0.6582315272666796, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2765617], dtype=float32), -0.5035395]. 
=============================================
[2019-04-04 01:48:44,297] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[92.09522 ]
 [92.17823 ]
 [92.307655]
 [92.49995 ]
 [92.56818 ]], R is [[92.12480927]
 [92.20355988]
 [92.28152466]
 [92.35871124]
 [92.43512726]].
[2019-04-04 01:48:45,636] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.5872205e-23 4.1051034e-16 1.2275751e-21 1.0742552e-17 3.0493375e-17
 6.8647459e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 01:48:45,637] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.5959
[2019-04-04 01:48:45,691] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 54.0, 112.5, 787.0, 26.0, 25.36320656070966, 0.4529625221767962, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3582000.0000, 
sim time next is 3582600.0000, 
raw observation next is [-3.833333333333333, 54.16666666666666, 113.0, 796.6666666666667, 26.0, 25.29792198713378, 0.4452134249670008, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.3564173591874424, 0.5416666666666665, 0.37666666666666665, 0.8802946593001842, 0.6666666666666666, 0.6081601655944816, 0.6484044749890002, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.46116242], dtype=float32), 0.27848303]. 
=============================================
[2019-04-04 01:48:45,728] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.3616633e-24 1.9195220e-16 4.1121125e-22 1.4134122e-18 3.9450051e-18
 2.1540483e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 01:48:45,728] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4275
[2019-04-04 01:48:45,774] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.0, 53.83333333333334, 94.0, 541.3333333333333, 26.0, 25.84275455845108, 0.4847927350080858, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3919800.0000, 
sim time next is 3920400.0000, 
raw observation next is [-8.0, 53.0, 95.5, 579.0, 26.0, 25.90889042083212, 0.4881493706242583, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.24099722991689754, 0.53, 0.31833333333333336, 0.6397790055248619, 0.6666666666666666, 0.65907420173601, 0.6627164568747528, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5243511], dtype=float32), -0.11016013]. 
=============================================
[2019-04-04 01:48:57,410] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.9443392e-25 1.5074945e-17 5.5093326e-23 1.3529333e-18 3.7151173e-19
 5.3752203e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 01:48:57,411] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7578
[2019-04-04 01:48:57,433] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.166666666666667, 39.16666666666666, 112.3333333333333, 812.0, 26.0, 26.73493712544568, 0.7392913093130123, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3937800.0000, 
sim time next is 3938400.0000, 
raw observation next is [-5.0, 38.0, 110.5, 806.0, 26.0, 26.80836566796286, 0.7536454868407466, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.32409972299168976, 0.38, 0.36833333333333335, 0.8906077348066298, 0.6666666666666666, 0.7340304723302383, 0.7512151622802489, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.1279743], dtype=float32), 0.14728595]. 
=============================================
[2019-04-04 01:48:57,957] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.1784045e-23 6.3149133e-16 3.1531442e-21 1.8942653e-17 3.0061414e-17
 5.2876128e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 01:48:57,957] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4083
[2019-04-04 01:48:57,991] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.333333333333333, 61.66666666666667, 0.0, 0.0, 26.0, 25.40708248875125, 0.468043613284061, 0.0, 1.0, 22260.62122996803], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3885600.0000, 
sim time next is 3886200.0000, 
raw observation next is [-1.5, 62.5, 0.0, 0.0, 26.0, 25.42718297692799, 0.4622742672519587, 0.0, 1.0, 18762.46555748103], 
processed observation next is [1.0, 1.0, 0.4210526315789474, 0.625, 0.0, 0.0, 0.6666666666666666, 0.6189319147439992, 0.6540914224173195, 0.0, 1.0, 0.089345074083243], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.05996095], dtype=float32), -1.287021]. 
=============================================
[2019-04-04 01:48:58,270] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5907687e-24 1.0987827e-17 1.9261020e-23 4.5838488e-19 3.0043960e-19
 1.9744708e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 01:48:58,271] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6067
[2019-04-04 01:48:58,308] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.0, 35.0, 93.83333333333334, 652.0, 26.0, 25.90378369617365, 0.6708065689847188, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4117200.0000, 
sim time next is 4117800.0000, 
raw observation next is [4.0, 35.0, 93.66666666666666, 609.0, 26.0, 26.49511937282342, 0.7181115541718291, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5734072022160666, 0.35, 0.3122222222222222, 0.6729281767955801, 0.6666666666666666, 0.7079266144019517, 0.7393705180572764, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1733878], dtype=float32), -2.4355326]. 
=============================================
[2019-04-04 01:48:58,756] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.79995655e-23 6.02302100e-16 1.06144244e-20 2.46965492e-17
 4.03649647e-17 1.49149161e-22 1.00000000e+00], sum to 1.0000
[2019-04-04 01:48:58,756] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7308
[2019-04-04 01:48:58,771] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.666666666666668, 51.66666666666666, 0.0, 0.0, 26.0, 25.25231365871654, 0.4080077943504195, 0.0, 1.0, 52283.157404385], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3969600.0000, 
sim time next is 3970200.0000, 
raw observation next is [-8.833333333333332, 52.33333333333334, 0.0, 0.0, 26.0, 25.19483158524974, 0.396749364768706, 0.0, 1.0, 46823.93937000672], 
processed observation next is [1.0, 0.9565217391304348, 0.2179132040627886, 0.5233333333333334, 0.0, 0.0, 0.6666666666666666, 0.5995692987708118, 0.6322497882562353, 0.0, 1.0, 0.22297113985717487], 
reward next is 0.7770, 
noisyNet noise sample is [array([0.0715335], dtype=float32), -1.4081422]. 
=============================================
[2019-04-04 01:49:03,020] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.1303167e-23 5.7393417e-16 4.7063664e-22 4.6700533e-18 1.8700399e-18
 2.0459350e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:49:03,020] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5626
[2019-04-04 01:49:03,036] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.333333333333334, 54.5, 148.6666666666667, 749.3333333333334, 26.0, 25.25425645486551, 0.3972033807040876, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4273800.0000, 
sim time next is 4274400.0000, 
raw observation next is [5.666666666666667, 54.0, 134.8333333333333, 785.6666666666666, 26.0, 25.23198840453338, 0.3955656863944099, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.6195752539242845, 0.54, 0.4494444444444443, 0.8681399631675875, 0.6666666666666666, 0.6026657003777816, 0.6318552287981366, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.43971342], dtype=float32), 0.9979029]. 
=============================================
[2019-04-04 01:49:08,469] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.63602122e-22 1.63629020e-15 4.34794968e-21 1.16303215e-17
 8.70230930e-17 6.56226566e-23 1.00000000e+00], sum to 1.0000
[2019-04-04 01:49:08,472] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0830
[2019-04-04 01:49:08,479] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 26.0, 0.0, 0.0, 26.0, 25.83972887366738, 0.5357399260910971, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4039200.0000, 
sim time next is 4039800.0000, 
raw observation next is [-3.166666666666667, 26.83333333333334, 0.0, 0.0, 26.0, 25.88718856424417, 0.5429870207235695, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.3748845798707295, 0.26833333333333337, 0.0, 0.0, 0.6666666666666666, 0.6572657136870141, 0.6809956735745232, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0638057], dtype=float32), -0.72538006]. 
=============================================
[2019-04-04 01:49:26,885] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-04 01:49:26,886] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 01:49:26,887] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:49:26,888] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 01:49:26,888] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:49:26,888] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 01:49:26,890] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:49:26,895] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run28
[2019-04-04 01:49:26,920] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run28
[2019-04-04 01:49:26,921] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run28
[2019-04-04 01:49:53,513] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.37342975], dtype=float32), 0.14219177]
[2019-04-04 01:49:53,513] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-7.435891757166667, 76.46654588333332, 0.6030958216666665, 0.0, 26.0, 23.58521054350227, -0.07171908020517147, 0.0, 1.0, 43646.45450914509]
[2019-04-04 01:49:53,513] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 01:49:53,513] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.2741702e-21 3.6213904e-14 6.0604376e-20 1.1091387e-16 2.0248378e-16
 7.8186485e-21 1.0000000e+00], sampled 0.1981580089764895
[2019-04-04 01:51:00,936] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.37342975], dtype=float32), 0.14219177]
[2019-04-04 01:51:00,936] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-6.516666666666667, 83.0, 0.0, 0.0, 26.0, 25.22880137705562, 0.402811064269501, 0.0, 1.0, 42264.40818521218]
[2019-04-04 01:51:00,937] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 01:51:00,938] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.1813120e-22 2.2199347e-15 1.0805376e-20 4.5242326e-17 5.4585364e-17
 7.6816756e-22 1.0000000e+00], sampled 0.11696493413963072
[2019-04-04 01:52:15,414] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 01:52:46,888] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 01:52:51,297] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 01:52:52,335] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 2700000, evaluation results [2700000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 01:52:52,481] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.4450696e-23 8.6118992e-15 2.9127921e-20 5.8433905e-17 7.0610956e-17
 4.7724999e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:52:52,481] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3786
[2019-04-04 01:52:52,506] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.4, 70.33333333333333, 0.0, 0.0, 26.0, 25.48835197151182, 0.373227685589935, 0.0, 1.0, 18753.66731281949], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4340400.0000, 
sim time next is 4341000.0000, 
raw observation next is [3.35, 70.66666666666667, 0.0, 0.0, 26.0, 25.44948725428051, 0.3794028449939719, 0.0, 1.0, 42448.54698686583], 
processed observation next is [1.0, 0.21739130434782608, 0.5554016620498616, 0.7066666666666667, 0.0, 0.0, 0.6666666666666666, 0.6207906045233758, 0.6264676149979906, 0.0, 1.0, 0.20213593803269445], 
reward next is 0.7979, 
noisyNet noise sample is [array([-1.8776189], dtype=float32), -0.8740373]. 
=============================================
[2019-04-04 01:52:52,585] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[81.99927]
 [82.07205]
 [82.05662]
 [81.9915 ]
 [81.91403]], R is [[82.01248932]
 [82.10306549]
 [82.19271851]
 [82.18028259]
 [82.04361725]].
[2019-04-04 01:52:55,970] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.9776674e-22 9.6078492e-16 6.5915260e-21 2.1950320e-17 3.1529220e-17
 4.8497275e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:52:55,970] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4401
[2019-04-04 01:52:56,020] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.65, 41.16666666666667, 0.0, 0.0, 26.0, 25.03901896384231, 0.3141431729447501, 0.0, 1.0, 27022.54719219249], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4212600.0000, 
sim time next is 4213200.0000, 
raw observation next is [1.6, 41.33333333333334, 0.0, 0.0, 26.0, 25.0258223982799, 0.3120890383238644, 0.0, 1.0, 30407.70915767929], 
processed observation next is [0.0, 0.782608695652174, 0.5069252077562327, 0.41333333333333344, 0.0, 0.0, 0.6666666666666666, 0.5854851998566583, 0.6040296794412882, 0.0, 1.0, 0.14479861503656805], 
reward next is 0.8552, 
noisyNet noise sample is [array([-0.364507], dtype=float32), 0.97927076]. 
=============================================
[2019-04-04 01:53:02,378] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.5767840e-22 2.2843386e-15 1.7090466e-20 3.1329598e-17 4.4324969e-17
 1.0203543e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:53:02,380] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7814
[2019-04-04 01:53:02,432] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.9666666666666668, 73.0, 0.0, 0.0, 26.0, 25.41846087357875, 0.436144992950477, 0.0, 1.0, 31992.81928284683], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4504800.0000, 
sim time next is 4505400.0000, 
raw observation next is [-0.95, 73.0, 0.0, 0.0, 26.0, 25.38408349345884, 0.4362897002931628, 0.0, 1.0, 55214.72246744762], 
processed observation next is [1.0, 0.13043478260869565, 0.43628808864265933, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6153402911215698, 0.6454299000977209, 0.0, 1.0, 0.26292724984498866], 
reward next is 0.7371, 
noisyNet noise sample is [array([0.59491795], dtype=float32), 2.683379]. 
=============================================
[2019-04-04 01:53:08,872] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5271409e-22 1.0157903e-15 8.1435172e-21 3.6471852e-17 5.9596186e-17
 2.8286485e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:53:08,872] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2618
[2019-04-04 01:53:08,896] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.5, 44.5, 33.0, 187.0, 26.0, 25.02982166320326, 0.3204341676432377, 0.0, 1.0, 48949.14586975839], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4901400.0000, 
sim time next is 4902000.0000, 
raw observation next is [2.333333333333333, 44.33333333333334, 27.5, 155.8333333333333, 26.0, 25.0026188342774, 0.3207876775341879, 0.0, 1.0, 49352.41992647255], 
processed observation next is [0.0, 0.7391304347826086, 0.5272391505078486, 0.4433333333333334, 0.09166666666666666, 0.17219152854511965, 0.6666666666666666, 0.5835515695231166, 0.6069292258447293, 0.0, 1.0, 0.23501152345939308], 
reward next is 0.7650, 
noisyNet noise sample is [array([-0.419491], dtype=float32), 1.4126685]. 
=============================================
[2019-04-04 01:53:08,960] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[82.558464]
 [82.73319 ]
 [82.91154 ]
 [83.14324 ]
 [83.4181  ]], R is [[82.38573456]
 [82.32878113]
 [82.30001831]
 [82.33702087]
 [82.42469788]].
[2019-04-04 01:53:10,310] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.8007773e-23 7.3381780e-17 7.9971911e-22 8.8619672e-18 3.4518590e-18
 6.2715799e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 01:53:10,310] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8396
[2019-04-04 01:53:10,320] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 52.0, 23.33333333333334, 23.33333333333334, 26.0, 25.74786600621344, 0.4956911991582501, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4556400.0000, 
sim time next is 4557000.0000, 
raw observation next is [2.0, 52.0, 18.66666666666667, 18.66666666666667, 26.0, 25.56865384433846, 0.345527051538691, 1.0, 1.0, 28310.95432949887], 
processed observation next is [1.0, 0.7391304347826086, 0.518005540166205, 0.52, 0.06222222222222224, 0.02062615101289135, 0.6666666666666666, 0.6307211536948717, 0.6151756838462303, 1.0, 1.0, 0.1348140682357089], 
reward next is 0.8652, 
noisyNet noise sample is [array([1.9787283], dtype=float32), 0.2917114]. 
=============================================
[2019-04-04 01:53:10,347] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[83.249886]
 [83.28478 ]
 [83.38743 ]
 [83.455986]
 [83.38934 ]], R is [[83.34951782]
 [83.51602173]
 [83.68086243]
 [83.84405518]
 [84.00561523]].
[2019-04-04 01:53:19,523] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.07486319e-25 3.25481716e-18 2.93463640e-23 1.08130097e-19
 2.96021006e-19 1.43991097e-24 1.00000000e+00], sum to 1.0000
[2019-04-04 01:53:19,524] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7045
[2019-04-04 01:53:19,568] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 65.5, 164.0, 509.0, 26.0, 26.10797153804938, 0.5696292845503337, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4613400.0000, 
sim time next is 4614000.0000, 
raw observation next is [-0.6666666666666667, 63.66666666666667, 158.1666666666667, 552.0, 26.0, 26.24375816658005, 0.5837525168222993, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.44413665743305636, 0.6366666666666667, 0.5272222222222224, 0.6099447513812155, 0.6666666666666666, 0.6869798472150043, 0.6945841722740997, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5772596], dtype=float32), 0.06457115]. 
=============================================
[2019-04-04 01:53:19,578] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[91.685425]
 [91.44078 ]
 [91.12009 ]
 [90.7303  ]
 [90.36531 ]], R is [[91.53022003]
 [91.61492157]
 [91.69877625]
 [91.78179169]
 [91.86397552]].
[2019-04-04 01:53:26,933] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:53:26,934] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:53:26,938] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run21
[2019-04-04 01:53:29,541] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [5.9597596e-23 3.4191143e-15 1.0737168e-20 1.9077898e-17 1.7394695e-17
 4.6756233e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:53:29,541] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1980
[2019-04-04 01:53:29,634] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 48.66666666666667, 0.0, 0.0, 26.0, 25.14852297642786, 0.272285558324682, 0.0, 1.0, 38383.20992265364], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4940400.0000, 
sim time next is 4941000.0000, 
raw observation next is [-2.0, 48.0, 0.0, 0.0, 26.0, 25.18929333323468, 0.2753284304380846, 0.0, 1.0, 38397.31055922278], 
processed observation next is [1.0, 0.17391304347826086, 0.40720221606648205, 0.48, 0.0, 0.0, 0.6666666666666666, 0.5991077777695567, 0.5917761434793615, 0.0, 1.0, 0.18284433599629896], 
reward next is 0.8172, 
noisyNet noise sample is [array([-1.0056281], dtype=float32), 0.29569894]. 
=============================================
[2019-04-04 01:53:29,676] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[80.14207 ]
 [80.16797 ]
 [80.27085 ]
 [80.433945]
 [80.5597  ]], R is [[80.15583038]
 [80.17149353]
 [80.18715668]
 [80.20252991]
 [80.217659  ]].
[2019-04-04 01:53:32,348] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.4110788e-21 3.1036293e-15 4.1283686e-20 1.2162653e-16 2.5984559e-16
 2.0001130e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:53:32,349] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9726
[2019-04-04 01:53:32,429] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.5, 44.5, 33.0, 187.0, 26.0, 25.02982166320326, 0.3204341676432377, 0.0, 1.0, 48949.14586975839], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4901400.0000, 
sim time next is 4902000.0000, 
raw observation next is [2.333333333333333, 44.33333333333334, 27.5, 155.8333333333333, 26.0, 25.0026188342774, 0.3207876775341879, 0.0, 1.0, 49352.41992647255], 
processed observation next is [0.0, 0.7391304347826086, 0.5272391505078486, 0.4433333333333334, 0.09166666666666666, 0.17219152854511965, 0.6666666666666666, 0.5835515695231166, 0.6069292258447293, 0.0, 1.0, 0.23501152345939308], 
reward next is 0.7650, 
noisyNet noise sample is [array([-0.18465468], dtype=float32), -0.55474085]. 
=============================================
[2019-04-04 01:53:32,513] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[79.12337]
 [79.19073]
 [79.26763]
 [79.40855]
 [79.61187]], R is [[79.09073639]
 [79.06673431]
 [79.07059479]
 [79.13989258]
 [79.25954437]].
[2019-04-04 01:53:34,087] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:53:34,087] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:53:34,090] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run21
[2019-04-04 01:53:39,240] A3C_AGENT_WORKER-Thread-19 INFO:Local step 170000, global step 2713891: loss 0.0826
[2019-04-04 01:53:39,240] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 170000, global step 2713891: learning rate 0.0001
[2019-04-04 01:53:44,421] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.1547005e-22 1.7508219e-15 2.1723698e-21 2.3216374e-17 1.5425727e-17
 1.5771330e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:53:44,429] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.0877
[2019-04-04 01:53:44,480] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.3, 49.5, 283.0, 308.0, 26.0, 25.02244627031235, 0.3505963730420363, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4879800.0000, 
sim time next is 4880400.0000, 
raw observation next is [0.5333333333333332, 48.66666666666667, 282.6666666666667, 321.6666666666667, 26.0, 25.046324862305, 0.3522659476211006, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.4773776546629733, 0.4866666666666667, 0.9422222222222223, 0.3554327808471455, 0.6666666666666666, 0.5871937385254166, 0.6174219825403668, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.6021998], dtype=float32), -0.90908915]. 
=============================================
[2019-04-04 01:53:45,714] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:53:45,714] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:53:45,740] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run21
[2019-04-04 01:53:46,201] A3C_AGENT_WORKER-Thread-17 INFO:Local step 170000, global step 2715678: loss 0.0670
[2019-04-04 01:53:46,207] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 170000, global step 2715681: learning rate 0.0001
[2019-04-04 01:53:48,508] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [6.8410611e-23 9.0074055e-16 1.2688201e-21 6.9249146e-18 3.8371479e-17
 1.2804013e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:53:48,508] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.2122
[2019-04-04 01:53:48,559] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.1666666666666667, 39.16666666666666, 0.0, 0.0, 26.0, 25.38287685521041, 0.3487681049673995, 0.0, 1.0, 52741.75284248315], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4921800.0000, 
sim time next is 4922400.0000, 
raw observation next is [0.3333333333333333, 39.33333333333334, 0.0, 0.0, 26.0, 25.36956837549946, 0.3502359075353431, 0.0, 1.0, 50633.92420296117], 
processed observation next is [0.0, 1.0, 0.4718374884579871, 0.3933333333333334, 0.0, 0.0, 0.6666666666666666, 0.6141306979582882, 0.6167453025117811, 0.0, 1.0, 0.24111392477600557], 
reward next is 0.7589, 
noisyNet noise sample is [array([-1.2564166], dtype=float32), -0.68180746]. 
=============================================
[2019-04-04 01:53:52,681] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:53:52,681] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:53:52,689] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run21
[2019-04-04 01:53:53,100] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:53:53,100] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:53:53,112] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run21
[2019-04-04 01:53:53,498] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:53:53,499] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:53:53,508] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run21
[2019-04-04 01:53:54,033] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:53:54,033] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:53:54,052] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run21
[2019-04-04 01:53:58,568] A3C_AGENT_WORKER-Thread-7 INFO:Local step 170000, global step 2718344: loss 0.0448
[2019-04-04 01:53:58,568] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 170000, global step 2718344: learning rate 0.0001
[2019-04-04 01:54:01,513] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:54:01,514] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:54:01,564] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run21
[2019-04-04 01:54:01,722] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:54:01,722] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:54:01,726] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run21
[2019-04-04 01:54:02,087] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:54:02,087] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:54:02,090] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run21
[2019-04-04 01:54:02,623] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [6.0028538e-25 1.1858939e-17 2.6325387e-23 1.3688761e-18 1.5267821e-19
 6.8493316e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 01:54:02,623] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.3117
[2019-04-04 01:54:02,698] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.5, 27.66666666666666, 123.3333333333333, 851.6666666666666, 26.0, 27.25875147373031, 0.6377190990119795, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5053800.0000, 
sim time next is 5054400.0000, 
raw observation next is [8.0, 26.0, 123.5, 855.0, 26.0, 27.36467483681292, 0.8534562602448988, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.6842105263157896, 0.26, 0.4116666666666667, 0.9447513812154696, 0.6666666666666666, 0.78038956973441, 0.784485420081633, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6511064], dtype=float32), 0.44463652]. 
=============================================
[2019-04-04 01:54:02,893] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:54:02,893] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:54:02,899] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run21
[2019-04-04 01:54:06,050] A3C_AGENT_WORKER-Thread-5 INFO:Local step 170000, global step 2719501: loss 0.0488
[2019-04-04 01:54:06,051] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 170000, global step 2719501: learning rate 0.0001
[2019-04-04 01:54:06,249] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:54:06,249] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:54:06,253] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run21
[2019-04-04 01:54:07,213] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:54:07,213] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:54:07,216] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run21
[2019-04-04 01:54:08,004] A3C_AGENT_WORKER-Thread-2 INFO:Local step 170000, global step 2719724: loss 0.0370
[2019-04-04 01:54:08,004] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 170000, global step 2719724: learning rate 0.0001
[2019-04-04 01:54:08,573] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:54:08,573] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:54:08,576] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run21
[2019-04-04 01:54:09,015] A3C_AGENT_WORKER-Thread-14 INFO:Local step 170000, global step 2719824: loss 0.0262
[2019-04-04 01:54:09,031] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 170000, global step 2719824: learning rate 0.0001
[2019-04-04 01:54:09,444] A3C_AGENT_WORKER-Thread-3 INFO:Local step 170000, global step 2719871: loss 0.0258
[2019-04-04 01:54:09,445] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 170000, global step 2719871: learning rate 0.0001
[2019-04-04 01:54:12,568] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.6292707e-23 2.7170766e-17 1.3995836e-21 9.8876720e-18 7.0604191e-18
 3.0562923e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 01:54:12,574] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7210
[2019-04-04 01:54:12,630] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.6, 50.83333333333334, 98.33333333333333, 691.3333333333334, 26.0, 25.71794882284061, 0.4203006007855087, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 301800.0000, 
sim time next is 302400.0000, 
raw observation next is [-10.6, 49.0, 94.5, 708.0, 26.0, 25.91247653886598, 0.4385979486473259, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.1689750692520776, 0.49, 0.315, 0.7823204419889502, 0.6666666666666666, 0.6593730449054984, 0.6461993162157753, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.0597463], dtype=float32), 1.8062552]. 
=============================================
[2019-04-04 01:54:15,405] A3C_AGENT_WORKER-Thread-18 INFO:Local step 170000, global step 2720712: loss 0.0213
[2019-04-04 01:54:15,453] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 170000, global step 2720712: learning rate 0.0001
[2019-04-04 01:54:16,169] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:54:16,169] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:54:16,173] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run21
[2019-04-04 01:54:16,680] A3C_AGENT_WORKER-Thread-6 INFO:Local step 170000, global step 2720981: loss 0.0154
[2019-04-04 01:54:16,681] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 170000, global step 2720981: learning rate 0.0001
[2019-04-04 01:54:17,142] A3C_AGENT_WORKER-Thread-15 INFO:Local step 170000, global step 2721081: loss 0.0095
[2019-04-04 01:54:17,143] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 170000, global step 2721081: learning rate 0.0001
[2019-04-04 01:54:17,498] A3C_AGENT_WORKER-Thread-9 INFO:Local step 170000, global step 2721159: loss 0.0171
[2019-04-04 01:54:17,499] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 170000, global step 2721159: learning rate 0.0001
[2019-04-04 01:54:17,918] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.4918614e-22 4.0331914e-15 1.9372463e-20 2.6160887e-17 2.6631617e-17
 1.9905150e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:54:17,929] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3048
[2019-04-04 01:54:18,018] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.233333333333333, 83.33333333333334, 23.33333333333333, 0.0, 26.0, 24.5220054204006, 0.1773502594089305, 0.0, 1.0, 50689.92145719513], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 58800.0000, 
sim time next is 59400.0000, 
raw observation next is [6.05, 84.0, 18.0, 0.0, 26.0, 24.50422910401001, 0.1860511567913048, 0.0, 1.0, 58935.20283097521], 
processed observation next is [0.0, 0.6956521739130435, 0.6301939058171746, 0.84, 0.06, 0.0, 0.6666666666666666, 0.5420190920008343, 0.5620170522637683, 0.0, 1.0, 0.28064382300464386], 
reward next is 0.7194, 
noisyNet noise sample is [array([0.7811967], dtype=float32), -0.31905892]. 
=============================================
[2019-04-04 01:54:18,879] A3C_AGENT_WORKER-Thread-19 INFO:Local step 170500, global step 2721464: loss 0.0040
[2019-04-04 01:54:18,882] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 170500, global step 2721464: learning rate 0.0001
[2019-04-04 01:54:19,790] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.3645756e-25 3.1841562e-16 1.3073096e-22 4.5228995e-19 4.9893296e-19
 9.1595862e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 01:54:19,790] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6354
[2019-04-04 01:54:19,850] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.46666666666667, 68.0, 40.83333333333333, 385.5, 26.0, 25.63172786091829, 0.3217546741508705, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 290400.0000, 
sim time next is 291000.0000, 
raw observation next is [-12.38333333333333, 67.5, 51.66666666666666, 385.0, 26.0, 25.79070427502599, 0.3113413593197087, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.1195752539242845, 0.675, 0.1722222222222222, 0.425414364640884, 0.6666666666666666, 0.6492253562521659, 0.6037804531065696, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2174273], dtype=float32), -0.22169316]. 
=============================================
[2019-04-04 01:54:19,895] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[86.76716 ]
 [87.48908 ]
 [88.053825]
 [88.18902 ]
 [87.89849 ]], R is [[86.41358185]
 [86.54944611]
 [86.68395233]
 [86.49728394]
 [86.21980286]].
[2019-04-04 01:54:20,864] A3C_AGENT_WORKER-Thread-16 INFO:Local step 170000, global step 2721839: loss 0.0160
[2019-04-04 01:54:20,869] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 170000, global step 2721839: learning rate 0.0001
[2019-04-04 01:54:20,926] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 01:54:20,926] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:54:20,969] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run21
[2019-04-04 01:54:21,062] A3C_AGENT_WORKER-Thread-8 INFO:Local step 170000, global step 2721861: loss 0.0140
[2019-04-04 01:54:21,063] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 170000, global step 2721861: learning rate 0.0001
[2019-04-04 01:54:21,503] A3C_AGENT_WORKER-Thread-4 INFO:Local step 170000, global step 2721934: loss 0.0063
[2019-04-04 01:54:21,504] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 170000, global step 2721934: learning rate 0.0001
[2019-04-04 01:54:28,823] A3C_AGENT_WORKER-Thread-10 INFO:Local step 170000, global step 2723235: loss 0.0139
[2019-04-04 01:54:28,823] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 170000, global step 2723235: learning rate 0.0001
[2019-04-04 01:54:29,473] A3C_AGENT_WORKER-Thread-17 INFO:Local step 170500, global step 2723352: loss 0.0234
[2019-04-04 01:54:29,493] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 170500, global step 2723352: learning rate 0.0001
[2019-04-04 01:54:33,664] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.3472987e-24 1.9966267e-16 1.8311400e-22 4.8022932e-19 3.5863390e-19
 3.8232538e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 01:54:33,677] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3368
[2019-04-04 01:54:33,766] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.0, 41.0, 0.0, 0.0, 26.0, 23.46338043244446, -0.05871066988511914, 1.0, 1.0, 159859.1910821923], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 459600.0000, 
sim time next is 460200.0000, 
raw observation next is [-7.899999999999999, 40.5, 7.666666666666665, 0.0, 26.0, 23.99652384943818, 0.0145429933000659, 1.0, 1.0, 110191.647413441], 
processed observation next is [1.0, 0.30434782608695654, 0.24376731301939064, 0.405, 0.02555555555555555, 0.0, 0.6666666666666666, 0.499710320786515, 0.5048476644333553, 1.0, 1.0, 0.5247221305401952], 
reward next is 0.4753, 
noisyNet noise sample is [array([-1.7845654], dtype=float32), -1.8635466]. 
=============================================
[2019-04-04 01:54:35,249] A3C_AGENT_WORKER-Thread-20 INFO:Local step 170000, global step 2724510: loss 0.0104
[2019-04-04 01:54:35,258] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 170000, global step 2724510: learning rate 0.0001
[2019-04-04 01:54:39,231] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.1713549e-24 8.9992569e-18 1.1578107e-21 2.4517530e-18 2.9718542e-18
 1.0775969e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 01:54:39,231] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8068
[2019-04-04 01:54:39,262] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.816666666666666, 73.33333333333333, 0.0, 0.0, 26.0, 24.54567363774285, 0.1784882008017659, 0.0, 1.0, 44155.79998082417], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 251400.0000, 
sim time next is 252000.0000, 
raw observation next is [-3.9, 75.0, 0.0, 0.0, 26.0, 24.51369520840438, 0.1730210927603322, 0.0, 1.0, 44197.73220390179], 
processed observation next is [1.0, 0.9565217391304348, 0.3545706371191136, 0.75, 0.0, 0.0, 0.6666666666666666, 0.5428079340336982, 0.5576736975867774, 0.0, 1.0, 0.21046539144715137], 
reward next is 0.7895, 
noisyNet noise sample is [array([-0.42805716], dtype=float32), 1.0732017]. 
=============================================
[2019-04-04 01:54:39,303] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[78.427734]
 [78.2546  ]
 [78.02013 ]
 [77.75025 ]
 [77.579994]], R is [[78.7241745 ]
 [78.72666931]
 [78.72835541]
 [78.72825623]
 [78.72805023]].
[2019-04-04 01:54:39,583] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.16906465e-23 1.06106811e-16 8.05358271e-22 2.27244526e-18
 8.37840799e-19 2.26516944e-23 1.00000000e+00], sum to 1.0000
[2019-04-04 01:54:39,583] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9076
[2019-04-04 01:54:39,629] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 77.33333333333334, 0.0, 0.0, 26.0, 24.46117997356117, 0.1593161444588466, 0.0, 1.0, 44179.42109850749], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 253200.0000, 
sim time next is 253800.0000, 
raw observation next is [-3.9, 78.5, 0.0, 0.0, 26.0, 24.42796967631594, 0.1522721240304382, 0.0, 1.0, 44179.02728437835], 
processed observation next is [1.0, 0.9565217391304348, 0.3545706371191136, 0.785, 0.0, 0.0, 0.6666666666666666, 0.535664139692995, 0.5507573746768127, 0.0, 1.0, 0.21037632040180168], 
reward next is 0.7896, 
noisyNet noise sample is [array([-1.1488408], dtype=float32), -0.34091774]. 
=============================================
[2019-04-04 01:54:41,404] A3C_AGENT_WORKER-Thread-7 INFO:Local step 170500, global step 2725594: loss 0.0093
[2019-04-04 01:54:41,404] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 170500, global step 2725594: learning rate 0.0001
[2019-04-04 01:54:48,624] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.0704203e-24 2.3722731e-17 1.5887630e-22 3.6916397e-18 2.5038783e-19
 3.2955867e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 01:54:48,625] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7045
[2019-04-04 01:54:48,732] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.700000000000001, 61.5, 90.0, 65.33333333333333, 26.0, 25.574636971764, 0.3110214327191965, 1.0, 1.0, 100843.9331224681], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 141000.0000, 
sim time next is 141600.0000, 
raw observation next is [-6.700000000000001, 62.0, 75.5, 55.16666666666666, 26.0, 25.22990909434404, 0.3565030613007941, 1.0, 1.0, 65152.7291039367], 
processed observation next is [1.0, 0.6521739130434783, 0.2770083102493075, 0.62, 0.25166666666666665, 0.06095764272559852, 0.6666666666666666, 0.6024924245286701, 0.6188343537669314, 1.0, 1.0, 0.31025109097112713], 
reward next is 0.6897, 
noisyNet noise sample is [array([-0.6396698], dtype=float32), -0.55940855]. 
=============================================
[2019-04-04 01:54:49,656] A3C_AGENT_WORKER-Thread-2 INFO:Local step 170500, global step 2727226: loss 0.0073
[2019-04-04 01:54:49,657] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 170500, global step 2727226: learning rate 0.0001
[2019-04-04 01:54:50,379] A3C_AGENT_WORKER-Thread-5 INFO:Local step 170500, global step 2727393: loss 0.0061
[2019-04-04 01:54:50,379] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 170500, global step 2727393: learning rate 0.0001
[2019-04-04 01:54:50,741] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [4.6665002e-22 3.7515259e-15 3.4915727e-20 1.0231706e-17 6.0041150e-17
 9.1684752e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:54:50,741] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.6070
[2019-04-04 01:54:50,787] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 67.0, 0.0, 0.0, 26.0, 23.75846298507189, -0.02503167225461504, 0.0, 1.0, 45780.8829456885], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 270000.0000, 
sim time next is 270600.0000, 
raw observation next is [-9.0, 67.5, 0.0, 0.0, 26.0, 23.66661994082127, -0.04518219247734239, 0.0, 1.0, 45864.26586220293], 
processed observation next is [1.0, 0.13043478260869565, 0.21329639889196678, 0.675, 0.0, 0.0, 0.6666666666666666, 0.4722183284017725, 0.4849392691742192, 0.0, 1.0, 0.21840126601049012], 
reward next is 0.7816, 
noisyNet noise sample is [array([0.9032821], dtype=float32), 1.526323]. 
=============================================
[2019-04-04 01:54:51,587] A3C_AGENT_WORKER-Thread-3 INFO:Local step 170500, global step 2727728: loss 0.0083
[2019-04-04 01:54:51,591] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 170500, global step 2727728: learning rate 0.0001
[2019-04-04 01:54:51,936] A3C_AGENT_WORKER-Thread-14 INFO:Local step 170500, global step 2727834: loss 0.0078
[2019-04-04 01:54:51,940] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 170500, global step 2727834: learning rate 0.0001
[2019-04-04 01:54:53,786] A3C_AGENT_WORKER-Thread-18 INFO:Local step 170500, global step 2728389: loss 0.0171
[2019-04-04 01:54:53,786] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 170500, global step 2728389: learning rate 0.0001
[2019-04-04 01:54:54,902] A3C_AGENT_WORKER-Thread-15 INFO:Local step 170500, global step 2728642: loss 0.0302
[2019-04-04 01:54:54,903] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 170500, global step 2728642: learning rate 0.0001
[2019-04-04 01:54:55,756] A3C_AGENT_WORKER-Thread-9 INFO:Local step 170500, global step 2728859: loss 0.0172
[2019-04-04 01:54:55,766] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 170500, global step 2728859: learning rate 0.0001
[2019-04-04 01:54:55,946] A3C_AGENT_WORKER-Thread-6 INFO:Local step 170500, global step 2728911: loss 0.0184
[2019-04-04 01:54:55,949] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 170500, global step 2728911: learning rate 0.0001
[2019-04-04 01:54:56,036] A3C_AGENT_WORKER-Thread-19 INFO:Local step 171000, global step 2728931: loss 0.8739
[2019-04-04 01:54:56,036] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 171000, global step 2728931: learning rate 0.0001
[2019-04-04 01:54:58,377] A3C_AGENT_WORKER-Thread-8 INFO:Local step 170500, global step 2729473: loss 0.0166
[2019-04-04 01:54:58,408] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 170500, global step 2729477: learning rate 0.0001
[2019-04-04 01:54:59,355] A3C_AGENT_WORKER-Thread-4 INFO:Local step 170500, global step 2729709: loss 0.0148
[2019-04-04 01:54:59,375] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 170500, global step 2729713: learning rate 0.0001
[2019-04-04 01:55:00,613] A3C_AGENT_WORKER-Thread-16 INFO:Local step 170500, global step 2730098: loss 0.0250
[2019-04-04 01:55:00,615] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 170500, global step 2730098: learning rate 0.0001
[2019-04-04 01:55:02,865] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [9.5924704e-23 1.4277096e-15 4.7657167e-21 6.9310725e-18 4.7083154e-18
 4.1155964e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:55:02,865] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0874
[2019-04-04 01:55:02,958] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.08333333333333331, 91.16666666666667, 70.66666666666666, 103.6666666666667, 26.0, 25.1118829885312, 0.2989672247063788, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 550200.0000, 
sim time next is 550800.0000, 
raw observation next is [0.0, 91.0, 89.0, 103.5, 26.0, 25.12143452922684, 0.2950828730101057, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.46260387811634357, 0.91, 0.2966666666666667, 0.1143646408839779, 0.6666666666666666, 0.59345287743557, 0.5983609576700353, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.33037433], dtype=float32), -0.24142598]. 
=============================================
[2019-04-04 01:55:04,602] A3C_AGENT_WORKER-Thread-17 INFO:Local step 171000, global step 2731156: loss 0.7712
[2019-04-04 01:55:04,602] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 171000, global step 2731156: learning rate 0.0001
[2019-04-04 01:55:04,777] A3C_AGENT_WORKER-Thread-10 INFO:Local step 170500, global step 2731207: loss 0.0046
[2019-04-04 01:55:04,778] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 170500, global step 2731207: learning rate 0.0001
[2019-04-04 01:55:07,199] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.4314332e-23 1.5912546e-16 3.0644206e-21 3.8874900e-18 3.6555824e-18
 4.3169081e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 01:55:07,199] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.3951
[2019-04-04 01:55:07,223] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.7, 54.00000000000001, 0.0, 0.0, 26.0, 23.91423110972305, 0.01653685657150449, 0.0, 1.0, 44862.73991901593], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 429600.0000, 
sim time next is 430200.0000, 
raw observation next is [-11.7, 54.0, 0.0, 0.0, 26.0, 23.85722597738979, 0.004009742644270009, 0.0, 1.0, 44919.92057320673], 
processed observation next is [1.0, 1.0, 0.13850415512465375, 0.54, 0.0, 0.0, 0.6666666666666666, 0.48810216478248236, 0.5013365808814233, 0.0, 1.0, 0.2139043836819368], 
reward next is 0.7861, 
noisyNet noise sample is [array([0.32566828], dtype=float32), -1.276605]. 
=============================================
[2019-04-04 01:55:10,438] A3C_AGENT_WORKER-Thread-20 INFO:Local step 170500, global step 2732826: loss 0.0067
[2019-04-04 01:55:10,438] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 170500, global step 2732826: learning rate 0.0001
[2019-04-04 01:55:12,208] A3C_AGENT_WORKER-Thread-7 INFO:Local step 171000, global step 2733253: loss 0.4362
[2019-04-04 01:55:12,210] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 171000, global step 2733253: learning rate 0.0001
[2019-04-04 01:55:13,807] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.2077738e-22 1.1169858e-15 5.5479755e-21 3.3753459e-18 2.3675089e-17
 4.9519871e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:55:13,807] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9884
[2019-04-04 01:55:13,885] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.8, 87.0, 0.0, 0.0, 26.0, 24.98344802229088, 0.2928499985149806, 0.0, 1.0, 32535.5878971461], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 580200.0000, 
sim time next is 580800.0000, 
raw observation next is [-1.9, 87.0, 0.0, 0.0, 26.0, 24.96359027555758, 0.2877356933041705, 0.0, 1.0, 47974.6594760397], 
processed observation next is [0.0, 0.7391304347826086, 0.4099722991689751, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5802991896297982, 0.5959118977680569, 0.0, 1.0, 0.22845075940971285], 
reward next is 0.7715, 
noisyNet noise sample is [array([0.2330852], dtype=float32), -0.7933878]. 
=============================================
[2019-04-04 01:55:17,153] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [4.0849979e-24 3.2626769e-16 1.4314517e-21 4.6222155e-19 3.1982500e-18
 3.4785386e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 01:55:17,153] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3881
[2019-04-04 01:55:17,181] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 75.0, 0.0, 0.0, 26.0, 24.32545368468866, 0.05325961073746097, 0.0, 1.0, 41610.5220766743], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 704400.0000, 
sim time next is 705000.0000, 
raw observation next is [-2.9, 75.0, 0.0, 0.0, 26.0, 24.3152449616222, 0.04604373455308772, 0.0, 1.0, 41637.81335645763], 
processed observation next is [1.0, 0.13043478260869565, 0.38227146814404434, 0.75, 0.0, 0.0, 0.6666666666666666, 0.5262704134685166, 0.5153479115176959, 0.0, 1.0, 0.19827530169741728], 
reward next is 0.8017, 
noisyNet noise sample is [array([0.13784613], dtype=float32), 0.32496065]. 
=============================================
[2019-04-04 01:55:17,201] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[82.74629 ]
 [82.72299 ]
 [82.695274]
 [82.68877 ]
 [82.66137 ]], R is [[82.75097656]
 [82.72532654]
 [82.70010376]
 [82.67523193]
 [82.65083313]].
[2019-04-04 01:55:18,707] A3C_AGENT_WORKER-Thread-2 INFO:Local step 171000, global step 2735186: loss 0.3617
[2019-04-04 01:55:18,721] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 171000, global step 2735186: learning rate 0.0001
[2019-04-04 01:55:19,483] A3C_AGENT_WORKER-Thread-5 INFO:Local step 171000, global step 2735397: loss 0.3514
[2019-04-04 01:55:19,484] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 171000, global step 2735397: learning rate 0.0001
[2019-04-04 01:55:19,837] A3C_AGENT_WORKER-Thread-3 INFO:Local step 171000, global step 2735493: loss 0.3282
[2019-04-04 01:55:19,843] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 171000, global step 2735494: learning rate 0.0001
[2019-04-04 01:55:21,253] A3C_AGENT_WORKER-Thread-14 INFO:Local step 171000, global step 2735948: loss 0.3715
[2019-04-04 01:55:21,254] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 171000, global step 2735948: learning rate 0.0001
[2019-04-04 01:55:23,111] A3C_AGENT_WORKER-Thread-18 INFO:Local step 171000, global step 2736466: loss 0.3499
[2019-04-04 01:55:23,111] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 171000, global step 2736466: learning rate 0.0001
[2019-04-04 01:55:23,198] A3C_AGENT_WORKER-Thread-19 INFO:Local step 171500, global step 2736496: loss 0.0340
[2019-04-04 01:55:23,199] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 171500, global step 2736496: learning rate 0.0001
[2019-04-04 01:55:23,343] A3C_AGENT_WORKER-Thread-15 INFO:Local step 171000, global step 2736536: loss 0.3627
[2019-04-04 01:55:23,344] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 171000, global step 2736536: learning rate 0.0001
[2019-04-04 01:55:24,023] A3C_AGENT_WORKER-Thread-6 INFO:Local step 171000, global step 2736764: loss 0.3546
[2019-04-04 01:55:24,024] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 171000, global step 2736764: learning rate 0.0001
[2019-04-04 01:55:24,185] A3C_AGENT_WORKER-Thread-9 INFO:Local step 171000, global step 2736820: loss 0.3437
[2019-04-04 01:55:24,187] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 171000, global step 2736821: learning rate 0.0001
[2019-04-04 01:55:26,099] A3C_AGENT_WORKER-Thread-8 INFO:Local step 171000, global step 2737428: loss 0.3415
[2019-04-04 01:55:26,100] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 171000, global step 2737428: learning rate 0.0001
[2019-04-04 01:55:27,659] A3C_AGENT_WORKER-Thread-4 INFO:Local step 171000, global step 2737969: loss 0.3315
[2019-04-04 01:55:27,659] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 171000, global step 2737969: learning rate 0.0001
[2019-04-04 01:55:28,713] A3C_AGENT_WORKER-Thread-16 INFO:Local step 171000, global step 2738364: loss 0.2988
[2019-04-04 01:55:28,723] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 171000, global step 2738366: learning rate 0.0001
[2019-04-04 01:55:28,884] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.2854946e-23 5.6208228e-16 2.1256765e-21 7.0975457e-19 3.4759345e-18
 3.0391918e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:55:28,885] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1549
[2019-04-04 01:55:28,936] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.566666666666667, 60.33333333333334, 108.1666666666667, 89.66666666666667, 26.0, 24.89113090865252, 0.2253088351468503, 0.0, 1.0, 34787.18876028684], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 649200.0000, 
sim time next is 649800.0000, 
raw observation next is [-2.5, 60.0, 112.0, 100.0, 26.0, 24.88906791404057, 0.226723332761997, 0.0, 1.0, 40008.832728658], 
processed observation next is [0.0, 0.5217391304347826, 0.39335180055401664, 0.6, 0.37333333333333335, 0.11049723756906077, 0.6666666666666666, 0.5740889928367142, 0.5755744442539991, 0.0, 1.0, 0.19051825108884762], 
reward next is 0.8095, 
noisyNet noise sample is [array([0.03505618], dtype=float32), 0.5136485]. 
=============================================
[2019-04-04 01:55:30,533] A3C_AGENT_WORKER-Thread-17 INFO:Local step 171500, global step 2738993: loss 0.0012
[2019-04-04 01:55:30,548] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 171500, global step 2738993: learning rate 0.0001
[2019-04-04 01:55:31,908] A3C_AGENT_WORKER-Thread-10 INFO:Local step 171000, global step 2739556: loss 0.2653
[2019-04-04 01:55:31,909] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 171000, global step 2739556: learning rate 0.0001
[2019-04-04 01:55:34,784] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.9913258e-25 5.8176092e-18 5.8574323e-23 3.2642031e-19 1.3836065e-19
 1.8626868e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 01:55:34,784] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9849
[2019-04-04 01:55:34,830] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 82.66666666666667, 0.0, 0.0, 26.0, 25.16534337330609, 0.2882112853256677, 1.0, 1.0, 55203.53091954767], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 841800.0000, 
sim time next is 842400.0000, 
raw observation next is [-3.9, 82.0, 0.0, 0.0, 26.0, 25.13553171864319, 0.2844315993486783, 1.0, 1.0, 60477.78975647275], 
processed observation next is [1.0, 0.782608695652174, 0.3545706371191136, 0.82, 0.0, 0.0, 0.6666666666666666, 0.5946276432202658, 0.5948105331162261, 1.0, 1.0, 0.2879894750308226], 
reward next is 0.7120, 
noisyNet noise sample is [array([-0.7940027], dtype=float32), 0.6583012]. 
=============================================
[2019-04-04 01:55:36,413] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2327728e-24 2.5898043e-18 2.1408431e-23 5.8428465e-20 6.5399846e-20
 1.6039113e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 01:55:36,413] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0776
[2019-04-04 01:55:36,449] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.5, 45.33333333333333, 79.33333333333333, 21.66666666666667, 26.0, 25.59122307874308, 0.3852151382067355, 1.0, 1.0, 24271.85937881611], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 748200.0000, 
sim time next is 748800.0000, 
raw observation next is [-0.6, 45.0, 76.5, 17.0, 26.0, 25.67063217322353, 0.2942881602218132, 1.0, 1.0, 23438.78694443085], 
processed observation next is [1.0, 0.6956521739130435, 0.44598337950138506, 0.45, 0.255, 0.01878453038674033, 0.6666666666666666, 0.6392193477686275, 0.5980960534072711, 1.0, 1.0, 0.11161327116395642], 
reward next is 0.8884, 
noisyNet noise sample is [array([0.11058671], dtype=float32), 0.15960366]. 
=============================================
[2019-04-04 01:55:36,959] A3C_AGENT_WORKER-Thread-7 INFO:Local step 171500, global step 2741404: loss 0.0010
[2019-04-04 01:55:36,963] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 171500, global step 2741405: learning rate 0.0001
[2019-04-04 01:55:37,003] A3C_AGENT_WORKER-Thread-19 INFO:Local step 172000, global step 2741420: loss 1.5382
[2019-04-04 01:55:37,015] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 172000, global step 2741420: learning rate 0.0001
[2019-04-04 01:55:37,734] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.5180140e-25 1.5229968e-17 2.7969948e-22 2.1353826e-19 7.8974929e-19
 9.0601984e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 01:55:37,734] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8279
[2019-04-04 01:55:37,749] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 79.5, 0.0, 0.0, 26.0, 24.75598200917459, 0.2241161367041825, 0.0, 1.0, 39419.24702649705], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 869400.0000, 
sim time next is 870000.0000, 
raw observation next is [-1.9, 79.33333333333334, 0.0, 0.0, 26.0, 24.80114489882464, 0.2180463480007825, 0.0, 1.0, 39391.48383632438], 
processed observation next is [1.0, 0.043478260869565216, 0.4099722991689751, 0.7933333333333334, 0.0, 0.0, 0.6666666666666666, 0.5667620749020532, 0.5726821160002609, 0.0, 1.0, 0.18757849445868754], 
reward next is 0.8124, 
noisyNet noise sample is [array([0.3751915], dtype=float32), -0.5318392]. 
=============================================
[2019-04-04 01:55:37,759] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[85.884926]
 [85.84282 ]
 [85.84677 ]
 [85.82713 ]
 [85.82165 ]], R is [[85.88272095]
 [85.83618164]
 [85.7900238 ]
 [85.74419403]
 [85.69863892]].
[2019-04-04 01:55:38,165] A3C_AGENT_WORKER-Thread-20 INFO:Local step 171000, global step 2741837: loss 0.2078
[2019-04-04 01:55:38,166] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 171000, global step 2741837: learning rate 0.0001
[2019-04-04 01:55:42,480] A3C_AGENT_WORKER-Thread-2 INFO:Local step 171500, global step 2743512: loss 0.0354
[2019-04-04 01:55:42,480] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 171500, global step 2743512: learning rate 0.0001
[2019-04-04 01:55:43,353] A3C_AGENT_WORKER-Thread-17 INFO:Local step 172000, global step 2743885: loss 1.0472
[2019-04-04 01:55:43,365] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 172000, global step 2743885: learning rate 0.0001
[2019-04-04 01:55:43,526] A3C_AGENT_WORKER-Thread-5 INFO:Local step 171500, global step 2743946: loss 0.0114
[2019-04-04 01:55:43,533] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 171500, global step 2743946: learning rate 0.0001
[2019-04-04 01:55:43,692] A3C_AGENT_WORKER-Thread-3 INFO:Local step 171500, global step 2744016: loss 0.0246
[2019-04-04 01:55:43,694] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 171500, global step 2744016: learning rate 0.0001
[2019-04-04 01:55:45,142] A3C_AGENT_WORKER-Thread-14 INFO:Local step 171500, global step 2744632: loss 0.0330
[2019-04-04 01:55:45,143] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 171500, global step 2744632: learning rate 0.0001
[2019-04-04 01:55:45,806] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.0269589e-24 3.9186993e-17 4.0793179e-22 3.6964290e-19 9.2656709e-19
 1.6404773e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 01:55:45,809] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6385
[2019-04-04 01:55:45,822] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.5, 100.0, 0.0, 0.0, 26.0, 25.50432957085948, 0.5964899583831936, 0.0, 1.0, 18752.32145978201], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1290600.0000, 
sim time next is 1291200.0000, 
raw observation next is [5.5, 100.0, 0.0, 0.0, 26.0, 25.50209512323323, 0.5909746035305775, 0.0, 1.0, 18749.96661130094], 
processed observation next is [0.0, 0.9565217391304348, 0.6149584487534627, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6251745936027691, 0.6969915345101926, 0.0, 1.0, 0.08928555529190924], 
reward next is 0.9107, 
noisyNet noise sample is [array([0.8792382], dtype=float32), 0.45451075]. 
=============================================
[2019-04-04 01:55:46,567] A3C_AGENT_WORKER-Thread-15 INFO:Local step 171500, global step 2745218: loss 0.0263
[2019-04-04 01:55:46,568] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 171500, global step 2745218: learning rate 0.0001
[2019-04-04 01:55:46,654] A3C_AGENT_WORKER-Thread-18 INFO:Local step 171500, global step 2745253: loss 0.0455
[2019-04-04 01:55:46,655] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 171500, global step 2745253: learning rate 0.0001
[2019-04-04 01:55:47,359] A3C_AGENT_WORKER-Thread-6 INFO:Local step 171500, global step 2745587: loss 0.0534
[2019-04-04 01:55:47,360] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 171500, global step 2745589: learning rate 0.0001
[2019-04-04 01:55:47,504] A3C_AGENT_WORKER-Thread-9 INFO:Local step 171500, global step 2745662: loss 0.0497
[2019-04-04 01:55:47,504] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 171500, global step 2745662: learning rate 0.0001
[2019-04-04 01:55:49,103] A3C_AGENT_WORKER-Thread-8 INFO:Local step 171500, global step 2746459: loss 0.0838
[2019-04-04 01:55:49,103] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 171500, global step 2746459: learning rate 0.0001
[2019-04-04 01:55:50,353] A3C_AGENT_WORKER-Thread-7 INFO:Local step 172000, global step 2747081: loss 0.7905
[2019-04-04 01:55:50,354] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 172000, global step 2747081: learning rate 0.0001
[2019-04-04 01:55:50,537] A3C_AGENT_WORKER-Thread-4 INFO:Local step 171500, global step 2747179: loss 0.0718
[2019-04-04 01:55:50,539] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 171500, global step 2747179: learning rate 0.0001
[2019-04-04 01:55:52,146] A3C_AGENT_WORKER-Thread-16 INFO:Local step 171500, global step 2748071: loss 0.0305
[2019-04-04 01:55:52,154] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 171500, global step 2748071: learning rate 0.0001
[2019-04-04 01:55:52,915] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.4039693e-28 1.6295463e-20 8.9644095e-26 4.8753088e-21 5.3575212e-22
 1.3051985e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 01:55:52,918] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0674
[2019-04-04 01:55:52,945] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.333333333333334, 86.33333333333334, 98.0, 701.3333333333334, 26.0, 26.31505669262645, 0.5428608130758613, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1513200.0000, 
sim time next is 1513800.0000, 
raw observation next is [5.800000000000001, 83.0, 100.0, 700.0, 26.0, 25.86404210075208, 0.5672829693584897, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.6232686980609419, 0.83, 0.3333333333333333, 0.7734806629834254, 0.6666666666666666, 0.65533684172934, 0.6890943231194965, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6560694], dtype=float32), -0.70971227]. 
=============================================
[2019-04-04 01:55:53,750] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.1508560e-24 1.0366506e-16 3.2915695e-22 3.9187102e-19 4.5458181e-19
 3.9674193e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 01:55:53,751] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9184
[2019-04-04 01:55:53,770] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 26.0, 25.44406321659527, 0.5634002874657053, 0.0, 1.0, 39164.60673135251], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1314000.0000, 
sim time next is 1314600.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 26.0, 25.56069704653373, 0.5620170903168232, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.5069252077562327, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6300580872111441, 0.6873390301056078, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.49377105], dtype=float32), -0.9446715]. 
=============================================
[2019-04-04 01:55:54,397] A3C_AGENT_WORKER-Thread-19 INFO:Local step 172500, global step 2749389: loss 0.0220
[2019-04-04 01:55:54,398] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 172500, global step 2749391: learning rate 0.0001
[2019-04-04 01:55:54,433] A3C_AGENT_WORKER-Thread-10 INFO:Local step 171500, global step 2749415: loss 0.0124
[2019-04-04 01:55:54,434] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 171500, global step 2749416: learning rate 0.0001
[2019-04-04 01:55:54,839] A3C_AGENT_WORKER-Thread-2 INFO:Local step 172000, global step 2749672: loss 0.5018
[2019-04-04 01:55:54,841] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 172000, global step 2749673: learning rate 0.0001
[2019-04-04 01:55:56,170] A3C_AGENT_WORKER-Thread-5 INFO:Local step 172000, global step 2750511: loss 0.4742
[2019-04-04 01:55:56,171] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 172000, global step 2750511: learning rate 0.0001
[2019-04-04 01:55:56,549] A3C_AGENT_WORKER-Thread-3 INFO:Local step 172000, global step 2750722: loss 0.4294
[2019-04-04 01:55:56,554] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 172000, global step 2750724: learning rate 0.0001
[2019-04-04 01:55:58,055] A3C_AGENT_WORKER-Thread-14 INFO:Local step 172000, global step 2751644: loss 0.3303
[2019-04-04 01:55:58,056] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 172000, global step 2751644: learning rate 0.0001
[2019-04-04 01:55:58,850] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.2233913e-26 8.6891692e-18 3.4944570e-24 2.3058262e-20 6.3857293e-21
 3.4117226e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 01:55:58,855] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8314
[2019-04-04 01:55:58,861] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.0, 100.0, 76.0, 0.0, 26.0, 23.30472534653298, 0.1230740888890166, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1245600.0000, 
sim time next is 1246200.0000, 
raw observation next is [14.9, 100.0, 76.66666666666667, 0.0, 26.0, 23.30085875325023, 0.1224842962833061, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.8753462603878118, 1.0, 0.2555555555555556, 0.0, 0.6666666666666666, 0.441738229437519, 0.540828098761102, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.153569], dtype=float32), 0.2481824]. 
=============================================
[2019-04-04 01:55:59,072] A3C_AGENT_WORKER-Thread-15 INFO:Local step 172000, global step 2752220: loss 0.2813
[2019-04-04 01:55:59,072] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 172000, global step 2752220: learning rate 0.0001
[2019-04-04 01:55:59,426] A3C_AGENT_WORKER-Thread-18 INFO:Local step 172000, global step 2752401: loss 0.2890
[2019-04-04 01:55:59,428] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 172000, global step 2752401: learning rate 0.0001
[2019-04-04 01:55:59,847] A3C_AGENT_WORKER-Thread-9 INFO:Local step 172000, global step 2752603: loss 0.2446
[2019-04-04 01:55:59,849] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 172000, global step 2752603: learning rate 0.0001
[2019-04-04 01:56:00,021] A3C_AGENT_WORKER-Thread-6 INFO:Local step 172000, global step 2752688: loss 0.2210
[2019-04-04 01:56:00,026] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 172000, global step 2752689: learning rate 0.0001
[2019-04-04 01:56:00,203] A3C_AGENT_WORKER-Thread-20 INFO:Local step 171500, global step 2752796: loss 0.0135
[2019-04-04 01:56:00,206] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 171500, global step 2752798: learning rate 0.0001
[2019-04-04 01:56:00,798] A3C_AGENT_WORKER-Thread-17 INFO:Local step 172500, global step 2753098: loss 0.1274
[2019-04-04 01:56:00,819] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 172500, global step 2753101: learning rate 0.0001
[2019-04-04 01:56:01,689] A3C_AGENT_WORKER-Thread-8 INFO:Local step 172000, global step 2753532: loss 0.1867
[2019-04-04 01:56:01,690] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 172000, global step 2753532: learning rate 0.0001
[2019-04-04 01:56:02,946] A3C_AGENT_WORKER-Thread-4 INFO:Local step 172000, global step 2754198: loss 0.1624
[2019-04-04 01:56:02,950] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 172000, global step 2754198: learning rate 0.0001
[2019-04-04 01:56:04,866] A3C_AGENT_WORKER-Thread-16 INFO:Local step 172000, global step 2755137: loss 0.1404
[2019-04-04 01:56:04,867] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 172000, global step 2755138: learning rate 0.0001
[2019-04-04 01:56:07,259] A3C_AGENT_WORKER-Thread-10 INFO:Local step 172000, global step 2756281: loss 0.1308
[2019-04-04 01:56:07,259] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 172000, global step 2756281: learning rate 0.0001
[2019-04-04 01:56:07,576] A3C_AGENT_WORKER-Thread-7 INFO:Local step 172500, global step 2756420: loss 0.0172
[2019-04-04 01:56:07,577] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 172500, global step 2756420: learning rate 0.0001
[2019-04-04 01:56:10,250] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.1318559e-24 8.3144198e-17 2.6960075e-22 1.8087258e-19 6.1893381e-19
 1.4955815e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 01:56:10,251] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.8149
[2019-04-04 01:56:10,265] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.27507672706881, 0.4608899176000655, 0.0, 1.0, 41068.09431772098], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1397400.0000, 
sim time next is 1398000.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.28072445750458, 0.460273857396243, 0.0, 1.0, 39514.83920118988], 
processed observation next is [1.0, 0.17391304347826086, 0.44598337950138506, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6067270381253816, 0.653424619132081, 0.0, 1.0, 0.18816590095804706], 
reward next is 0.8118, 
noisyNet noise sample is [array([-0.44236165], dtype=float32), -1.0861402]. 
=============================================
[2019-04-04 01:56:10,279] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[86.232765]
 [86.27145 ]
 [86.29481 ]
 [86.3197  ]
 [86.35281 ]], R is [[86.16191101]
 [86.1047287 ]
 [86.02639771]
 [85.89385223]
 [85.68722534]].
[2019-04-04 01:56:12,363] A3C_AGENT_WORKER-Thread-2 INFO:Local step 172500, global step 2758517: loss 0.0648
[2019-04-04 01:56:12,366] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 172500, global step 2758518: learning rate 0.0001
[2019-04-04 01:56:13,341] A3C_AGENT_WORKER-Thread-20 INFO:Local step 172000, global step 2758990: loss 0.1223
[2019-04-04 01:56:13,342] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 172000, global step 2758992: learning rate 0.0001
[2019-04-04 01:56:13,397] A3C_AGENT_WORKER-Thread-19 INFO:Local step 173000, global step 2759017: loss 2.9560
[2019-04-04 01:56:13,398] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 173000, global step 2759017: learning rate 0.0001
[2019-04-04 01:56:13,888] A3C_AGENT_WORKER-Thread-5 INFO:Local step 172500, global step 2759227: loss 0.1064
[2019-04-04 01:56:13,909] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 172500, global step 2759227: learning rate 0.0001
[2019-04-04 01:56:14,735] A3C_AGENT_WORKER-Thread-3 INFO:Local step 172500, global step 2759572: loss 0.0817
[2019-04-04 01:56:14,736] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 172500, global step 2759572: learning rate 0.0001
[2019-04-04 01:56:16,239] A3C_AGENT_WORKER-Thread-14 INFO:Local step 172500, global step 2760242: loss 0.0794
[2019-04-04 01:56:16,243] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 172500, global step 2760242: learning rate 0.0001
[2019-04-04 01:56:16,756] A3C_AGENT_WORKER-Thread-15 INFO:Local step 172500, global step 2760490: loss 0.0670
[2019-04-04 01:56:16,757] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 172500, global step 2760490: learning rate 0.0001
[2019-04-04 01:56:16,914] A3C_AGENT_WORKER-Thread-18 INFO:Local step 172500, global step 2760569: loss 0.0917
[2019-04-04 01:56:16,915] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 172500, global step 2760569: learning rate 0.0001
[2019-04-04 01:56:17,579] A3C_AGENT_WORKER-Thread-6 INFO:Local step 172500, global step 2760903: loss 0.0627
[2019-04-04 01:56:17,581] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 172500, global step 2760904: learning rate 0.0001
[2019-04-04 01:56:17,672] A3C_AGENT_WORKER-Thread-9 INFO:Local step 172500, global step 2760956: loss 0.0765
[2019-04-04 01:56:17,678] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 172500, global step 2760956: learning rate 0.0001
[2019-04-04 01:56:19,394] A3C_AGENT_WORKER-Thread-8 INFO:Local step 172500, global step 2761816: loss 0.0421
[2019-04-04 01:56:19,395] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 172500, global step 2761816: learning rate 0.0001
[2019-04-04 01:56:20,094] A3C_AGENT_WORKER-Thread-17 INFO:Local step 173000, global step 2762114: loss 3.1091
[2019-04-04 01:56:20,095] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 173000, global step 2762114: learning rate 0.0001
[2019-04-04 01:56:20,912] A3C_AGENT_WORKER-Thread-4 INFO:Local step 172500, global step 2762503: loss 0.0102
[2019-04-04 01:56:20,913] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 172500, global step 2762503: learning rate 0.0001
[2019-04-04 01:56:20,932] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.3641358e-26 1.3139217e-18 4.8460448e-23 5.9559305e-19 7.4606954e-20
 1.0497547e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 01:56:20,933] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0209
[2019-04-04 01:56:20,944] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.9, 92.0, 0.0, 0.0, 26.0, 25.46828280023246, 0.465669243548722, 0.0, 1.0, 56136.43757299529], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1474200.0000, 
sim time next is 1474800.0000, 
raw observation next is [2.0, 92.0, 0.0, 0.0, 26.0, 25.42452838978479, 0.4653074187391583, 0.0, 1.0, 67652.67395380404], 
processed observation next is [1.0, 0.043478260869565216, 0.518005540166205, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6187106991487324, 0.6551024729130528, 0.0, 1.0, 0.3221555902562097], 
reward next is 0.6778, 
noisyNet noise sample is [array([0.21574318], dtype=float32), -1.1344888]. 
=============================================
[2019-04-04 01:56:21,413] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.3636243e-25 1.4099441e-18 2.2536202e-23 1.0773319e-19 1.1279923e-19
 1.3423494e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 01:56:21,413] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6271
[2019-04-04 01:56:21,431] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.5, 92.0, 59.5, 0.0, 26.0, 26.00486428497665, 0.5608523460020316, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1677600.0000, 
sim time next is 1678200.0000, 
raw observation next is [1.433333333333333, 92.0, 61.66666666666667, 0.0, 26.0, 26.02457719978128, 0.554317945592865, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.502308402585411, 0.92, 0.20555555555555557, 0.0, 0.6666666666666666, 0.66871476664844, 0.684772648530955, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.19845615], dtype=float32), -1.2851107]. 
=============================================
[2019-04-04 01:56:22,683] A3C_AGENT_WORKER-Thread-16 INFO:Local step 172500, global step 2763323: loss 0.0289
[2019-04-04 01:56:22,694] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 172500, global step 2763323: learning rate 0.0001
[2019-04-04 01:56:24,744] A3C_AGENT_WORKER-Thread-10 INFO:Local step 172500, global step 2764222: loss 0.0344
[2019-04-04 01:56:24,744] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 172500, global step 2764222: learning rate 0.0001
[2019-04-04 01:56:26,003] A3C_AGENT_WORKER-Thread-7 INFO:Local step 173000, global step 2764753: loss 3.3971
[2019-04-04 01:56:26,003] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 173000, global step 2764753: learning rate 0.0001
[2019-04-04 01:56:31,300] A3C_AGENT_WORKER-Thread-2 INFO:Local step 173000, global step 2766570: loss 3.6373
[2019-04-04 01:56:31,308] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 173000, global step 2766570: learning rate 0.0001
[2019-04-04 01:56:31,584] A3C_AGENT_WORKER-Thread-20 INFO:Local step 172500, global step 2766667: loss 0.0390
[2019-04-04 01:56:31,585] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 172500, global step 2766667: learning rate 0.0001
[2019-04-04 01:56:33,718] A3C_AGENT_WORKER-Thread-5 INFO:Local step 173000, global step 2767403: loss 3.3805
[2019-04-04 01:56:33,719] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 173000, global step 2767403: learning rate 0.0001
[2019-04-04 01:56:34,922] A3C_AGENT_WORKER-Thread-3 INFO:Local step 173000, global step 2767824: loss 3.4500
[2019-04-04 01:56:34,923] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 173000, global step 2767824: learning rate 0.0001
[2019-04-04 01:56:35,355] A3C_AGENT_WORKER-Thread-14 INFO:Local step 173000, global step 2767976: loss 3.3376
[2019-04-04 01:56:35,357] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 173000, global step 2767976: learning rate 0.0001
[2019-04-04 01:56:36,472] A3C_AGENT_WORKER-Thread-15 INFO:Local step 173000, global step 2768325: loss 3.3909
[2019-04-04 01:56:36,472] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 173000, global step 2768325: learning rate 0.0001
[2019-04-04 01:56:36,564] A3C_AGENT_WORKER-Thread-18 INFO:Local step 173000, global step 2768349: loss 3.4132
[2019-04-04 01:56:36,568] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 173000, global step 2768349: learning rate 0.0001
[2019-04-04 01:56:36,666] A3C_AGENT_WORKER-Thread-9 INFO:Local step 173000, global step 2768375: loss 3.4263
[2019-04-04 01:56:36,667] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 173000, global step 2768375: learning rate 0.0001
[2019-04-04 01:56:37,112] A3C_AGENT_WORKER-Thread-6 INFO:Local step 173000, global step 2768495: loss 3.4398
[2019-04-04 01:56:37,113] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 173000, global step 2768495: learning rate 0.0001
[2019-04-04 01:56:39,607] A3C_AGENT_WORKER-Thread-19 INFO:Local step 173500, global step 2769223: loss 5.2549
[2019-04-04 01:56:39,609] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 173500, global step 2769223: learning rate 0.0001
[2019-04-04 01:56:39,832] A3C_AGENT_WORKER-Thread-8 INFO:Local step 173000, global step 2769291: loss 3.1698
[2019-04-04 01:56:39,833] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 173000, global step 2769291: learning rate 0.0001
[2019-04-04 01:56:40,636] A3C_AGENT_WORKER-Thread-4 INFO:Local step 173000, global step 2769557: loss 3.1708
[2019-04-04 01:56:40,636] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 173000, global step 2769557: learning rate 0.0001
[2019-04-04 01:56:42,942] A3C_AGENT_WORKER-Thread-16 INFO:Local step 173000, global step 2770310: loss 3.1459
[2019-04-04 01:56:42,943] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 173000, global step 2770310: learning rate 0.0001
[2019-04-04 01:56:43,671] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.0695251e-23 4.5127335e-18 2.1386973e-22 9.8354153e-19 4.4905273e-19
 4.6881562e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 01:56:43,671] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5349
[2019-04-04 01:56:43,786] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 82.0, 0.0, 0.0, 26.0, 24.81972358013049, 0.2448726394124953, 1.0, 1.0, 198466.2585087998], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2050200.0000, 
sim time next is 2050800.0000, 
raw observation next is [-3.899999999999999, 82.0, 0.0, 0.0, 26.0, 23.92623374726284, 0.2921219985774974, 1.0, 1.0, 199967.3931973158], 
processed observation next is [1.0, 0.7391304347826086, 0.35457063711911363, 0.82, 0.0, 0.0, 0.6666666666666666, 0.4938528122719035, 0.5973739995258325, 1.0, 1.0, 0.95222568189198], 
reward next is 0.0478, 
noisyNet noise sample is [array([0.25219858], dtype=float32), -0.111241214]. 
=============================================
[2019-04-04 01:56:44,025] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.8520466e-22 1.3775530e-15 9.3804277e-21 1.6503368e-17 1.8459430e-17
 5.0807262e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:56:44,025] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3457
[2019-04-04 01:56:44,078] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.75, 84.5, 0.0, 0.0, 26.0, 25.03683651255906, 0.2530667625370775, 0.0, 1.0, 32805.10001825666], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1877400.0000, 
sim time next is 1878000.0000, 
raw observation next is [-4.833333333333333, 85.0, 0.0, 0.0, 26.0, 25.03765559609259, 0.2482291340162096, 0.0, 1.0, 38938.4311840258], 
processed observation next is [0.0, 0.7391304347826086, 0.3287165281625116, 0.85, 0.0, 0.0, 0.6666666666666666, 0.5864712996743826, 0.5827430446720698, 0.0, 1.0, 0.18542110087631333], 
reward next is 0.8146, 
noisyNet noise sample is [array([0.05768206], dtype=float32), 0.55697984]. 
=============================================
[2019-04-04 01:56:44,082] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[76.631065]
 [76.604195]
 [76.69234 ]
 [76.53674 ]
 [76.31731 ]], R is [[76.73674011]
 [76.81316376]
 [76.80025482]
 [76.77360535]
 [76.74607849]].
[2019-04-04 01:56:45,209] A3C_AGENT_WORKER-Thread-10 INFO:Local step 173000, global step 2770949: loss 3.1426
[2019-04-04 01:56:45,213] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 173000, global step 2770949: learning rate 0.0001
[2019-04-04 01:56:45,734] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [6.9979212e-24 7.0580258e-17 2.3197824e-21 5.8404014e-18 1.9325762e-18
 1.7334695e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 01:56:45,734] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.2177
[2019-04-04 01:56:45,798] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 86.83333333333334, 0.0, 0.0, 26.0, 24.92562742319028, 0.295955352989227, 0.0, 1.0, 42398.85750771189], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2070600.0000, 
sim time next is 2071200.0000, 
raw observation next is [-4.5, 87.66666666666667, 0.0, 0.0, 26.0, 24.87484169124182, 0.2871141941427749, 0.0, 1.0, 42494.93064017963], 
processed observation next is [1.0, 1.0, 0.3379501385041552, 0.8766666666666667, 0.0, 0.0, 0.6666666666666666, 0.5729034742701516, 0.595704731380925, 0.0, 1.0, 0.20235681257228394], 
reward next is 0.7976, 
noisyNet noise sample is [array([0.36412624], dtype=float32), 0.2540661]. 
=============================================
[2019-04-04 01:56:46,141] A3C_AGENT_WORKER-Thread-17 INFO:Local step 173500, global step 2771209: loss 5.1344
[2019-04-04 01:56:46,142] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 173500, global step 2771209: learning rate 0.0001
[2019-04-04 01:56:46,175] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.6050268e-23 7.6387688e-17 2.3495730e-21 1.4203088e-18 6.0780160e-18
 5.5640354e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 01:56:46,175] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5200
[2019-04-04 01:56:46,211] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 89.33333333333334, 0.0, 0.0, 26.0, 24.67047890838175, 0.2253900623257923, 0.0, 1.0, 42696.83207828485], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2078400.0000, 
sim time next is 2079000.0000, 
raw observation next is [-4.5, 88.5, 0.0, 0.0, 26.0, 24.62489523821775, 0.2239866023834517, 0.0, 1.0, 42708.70646087905], 
processed observation next is [1.0, 0.043478260869565216, 0.3379501385041552, 0.885, 0.0, 0.0, 0.6666666666666666, 0.5520746031848125, 0.5746622007944838, 0.0, 1.0, 0.20337479267085262], 
reward next is 0.7966, 
noisyNet noise sample is [array([1.2701714], dtype=float32), 0.6668649]. 
=============================================
[2019-04-04 01:56:46,284] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[79.74422 ]
 [79.77554 ]
 [79.73959 ]
 [79.71007 ]
 [79.670586]], R is [[79.71622467]
 [79.71574402]
 [79.715271  ]
 [79.71485138]
 [79.71459198]].
[2019-04-04 01:56:47,353] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.1895404e-22 1.6990793e-15 3.7689444e-20 4.0885202e-17 1.7132007e-17
 1.4482590e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 01:56:47,353] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1431
[2019-04-04 01:56:47,455] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.800000000000001, 78.16666666666667, 0.0, 0.0, 26.0, 23.77552735957714, 0.01076023100295946, 0.0, 1.0, 43331.41048358567], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2099400.0000, 
sim time next is 2100000.0000, 
raw observation next is [-6.9, 78.33333333333334, 0.0, 0.0, 26.0, 23.73381054300751, 0.09466526859594608, 1.0, 1.0, 202393.2886562294], 
processed observation next is [1.0, 0.30434782608695654, 0.27146814404432135, 0.7833333333333334, 0.0, 0.0, 0.6666666666666666, 0.4778175452506259, 0.531555089531982, 1.0, 1.0, 0.9637775650296637], 
reward next is 0.0362, 
noisyNet noise sample is [array([-0.6238295], dtype=float32), 0.33127594]. 
=============================================
[2019-04-04 01:56:47,464] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[77.13327 ]
 [77.0917  ]
 [77.051605]
 [77.026665]
 [77.017166]], R is [[78.55605316]
 [78.56415558]
 [78.57165527]
 [78.57865906]
 [78.58527374]].
[2019-04-04 01:56:47,708] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [4.1138027e-24 3.8896130e-17 1.3983028e-21 1.0329232e-18 2.2178321e-18
 4.6695425e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 01:56:47,708] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9221
[2019-04-04 01:56:47,800] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.1, 78.66666666666667, 0.0, 0.0, 26.0, 24.78087171698596, 0.2550456503826236, 1.0, 1.0, 51852.88639827752], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2101200.0000, 
sim time next is 2101800.0000, 
raw observation next is [-7.199999999999999, 78.83333333333334, 24.66666666666666, 12.33333333333333, 26.0, 25.06778984453942, 0.2892643623506735, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.26315789473684215, 0.7883333333333334, 0.0822222222222222, 0.013627992633517492, 0.6666666666666666, 0.5889824870449516, 0.5964214541168912, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6256615], dtype=float32), -0.9576775]. 
=============================================
[2019-04-04 01:56:49,082] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.9219395e-24 7.2690515e-18 1.0010880e-22 1.1744538e-18 4.4878560e-19
 2.3792520e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 01:56:49,082] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8256
[2019-04-04 01:56:49,117] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.383333333333333, 76.16666666666666, 236.3333333333333, 73.66666666666666, 26.0, 25.77627183665338, 0.388040841973438, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2112600.0000, 
sim time next is 2113200.0000, 
raw observation next is [-7.3, 75.0, 250.5, 80.5, 26.0, 25.75074458529394, 0.3830382097966343, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.26038781163434904, 0.75, 0.835, 0.08895027624309393, 0.6666666666666666, 0.6458953821078284, 0.6276794032655447, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.73199874], dtype=float32), 0.6872608]. 
=============================================
[2019-04-04 01:56:52,679] A3C_AGENT_WORKER-Thread-20 INFO:Local step 173000, global step 2773212: loss 3.0045
[2019-04-04 01:56:52,681] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 173000, global step 2773212: learning rate 0.0001
[2019-04-04 01:56:53,073] A3C_AGENT_WORKER-Thread-7 INFO:Local step 173500, global step 2773322: loss 4.6035
[2019-04-04 01:56:53,077] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 173500, global step 2773322: learning rate 0.0001
[2019-04-04 01:56:58,447] A3C_AGENT_WORKER-Thread-2 INFO:Local step 173500, global step 2775018: loss 3.9359
[2019-04-04 01:56:58,450] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 173500, global step 2775019: learning rate 0.0001
[2019-04-04 01:57:01,873] A3C_AGENT_WORKER-Thread-3 INFO:Local step 173500, global step 2775991: loss 4.3796
[2019-04-04 01:57:01,876] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 173500, global step 2775991: learning rate 0.0001
[2019-04-04 01:57:02,032] A3C_AGENT_WORKER-Thread-5 INFO:Local step 173500, global step 2776042: loss 4.3051
[2019-04-04 01:57:02,033] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 173500, global step 2776042: learning rate 0.0001
[2019-04-04 01:57:03,272] A3C_AGENT_WORKER-Thread-14 INFO:Local step 173500, global step 2776447: loss 4.5725
[2019-04-04 01:57:03,273] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 173500, global step 2776447: learning rate 0.0001
[2019-04-04 01:57:03,531] A3C_AGENT_WORKER-Thread-19 INFO:Local step 174000, global step 2776521: loss 3.4469
[2019-04-04 01:57:03,532] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 174000, global step 2776521: learning rate 0.0001
[2019-04-04 01:57:03,670] A3C_AGENT_WORKER-Thread-15 INFO:Local step 173500, global step 2776563: loss 4.6813
[2019-04-04 01:57:03,671] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 173500, global step 2776563: learning rate 0.0001
[2019-04-04 01:57:03,838] A3C_AGENT_WORKER-Thread-18 INFO:Local step 173500, global step 2776613: loss 4.5904
[2019-04-04 01:57:03,841] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 173500, global step 2776614: learning rate 0.0001
[2019-04-04 01:57:04,004] A3C_AGENT_WORKER-Thread-9 INFO:Local step 173500, global step 2776671: loss 4.5042
[2019-04-04 01:57:04,005] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 173500, global step 2776671: learning rate 0.0001
[2019-04-04 01:57:04,075] A3C_AGENT_WORKER-Thread-6 INFO:Local step 173500, global step 2776692: loss 4.5787
[2019-04-04 01:57:04,079] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 173500, global step 2776692: learning rate 0.0001
[2019-04-04 01:57:05,366] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.00869632e-25 1.68912531e-17 1.03644976e-22 2.92836904e-19
 4.98512545e-19 5.99940039e-24 1.00000000e+00], sum to 1.0000
[2019-04-04 01:57:05,367] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3903
[2019-04-04 01:57:05,439] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.1, 78.66666666666667, 0.0, 0.0, 26.0, 24.78087171698596, 0.2550456503826236, 1.0, 1.0, 51852.88639827752], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2101200.0000, 
sim time next is 2101800.0000, 
raw observation next is [-7.199999999999999, 78.83333333333334, 24.66666666666666, 12.33333333333333, 26.0, 25.06778984453942, 0.2892643623506735, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.26315789473684215, 0.7883333333333334, 0.0822222222222222, 0.013627992633517492, 0.6666666666666666, 0.5889824870449516, 0.5964214541168912, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7445139], dtype=float32), -0.66036975]. 
=============================================
[2019-04-04 01:57:06,303] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.3250552e-25 2.5065472e-17 6.0405787e-23 5.3538083e-20 2.4926899e-19
 5.8162825e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 01:57:06,303] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2083
[2019-04-04 01:57:06,365] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 75.0, 15.0, 87.33333333333331, 26.0, 25.2202794939473, 0.3099709990197004, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2188200.0000, 
sim time next is 2188800.0000, 
raw observation next is [-5.6, 75.0, 21.5, 131.0, 26.0, 25.49604865484374, 0.3471602717850472, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.30747922437673136, 0.75, 0.07166666666666667, 0.14475138121546963, 0.6666666666666666, 0.6246707212369783, 0.6157200905950158, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1309892], dtype=float32), -0.9042731]. 
=============================================
[2019-04-04 01:57:07,528] A3C_AGENT_WORKER-Thread-8 INFO:Local step 173500, global step 2777790: loss 4.5261
[2019-04-04 01:57:07,528] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 173500, global step 2777790: learning rate 0.0001
[2019-04-04 01:57:07,924] A3C_AGENT_WORKER-Thread-4 INFO:Local step 173500, global step 2777913: loss 4.4170
[2019-04-04 01:57:07,926] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 173500, global step 2777914: learning rate 0.0001
[2019-04-04 01:57:09,657] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.8842155e-25 2.3020494e-18 2.0842977e-23 1.0776196e-19 2.6940158e-20
 4.1263268e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 01:57:09,657] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9469
[2019-04-04 01:57:09,700] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.85, 73.0, 178.0, 50.0, 26.0, 25.7407690345856, 0.3466965341245802, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2284200.0000, 
sim time next is 2284800.0000, 
raw observation next is [-5.566666666666666, 71.33333333333333, 175.1666666666667, 60.0, 26.0, 25.73251123426163, 0.3479477692854727, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.3084025854108957, 0.7133333333333333, 0.583888888888889, 0.06629834254143646, 0.6666666666666666, 0.6443759361884691, 0.6159825897618242, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.02323116], dtype=float32), -0.45746312]. 
=============================================
[2019-04-04 01:57:10,204] A3C_AGENT_WORKER-Thread-17 INFO:Local step 174000, global step 2778643: loss 3.4857
[2019-04-04 01:57:10,205] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 174000, global step 2778643: learning rate 0.0001
[2019-04-04 01:57:10,287] A3C_AGENT_WORKER-Thread-16 INFO:Local step 173500, global step 2778667: loss 3.6312
[2019-04-04 01:57:10,288] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 173500, global step 2778667: learning rate 0.0001
[2019-04-04 01:57:11,921] A3C_AGENT_WORKER-Thread-10 INFO:Local step 173500, global step 2779288: loss 3.8685
[2019-04-04 01:57:11,923] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 173500, global step 2779288: learning rate 0.0001
[2019-04-04 01:57:16,024] A3C_AGENT_WORKER-Thread-7 INFO:Local step 174000, global step 2780730: loss 3.5847
[2019-04-04 01:57:16,026] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 174000, global step 2780730: learning rate 0.0001
[2019-04-04 01:57:18,479] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.14370095e-23 7.30837034e-17 7.34566525e-22 9.43261039e-19
 2.19911260e-18 7.87881613e-24 1.00000000e+00], sum to 1.0000
[2019-04-04 01:57:18,480] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2861
[2019-04-04 01:57:18,528] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.2, 62.5, 0.0, 0.0, 26.0, 24.9714935240008, 0.3584046472876619, 0.0, 1.0, 81202.30901040214], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2663400.0000, 
sim time next is 2664000.0000, 
raw observation next is [-1.2, 63.0, 0.0, 0.0, 26.0, 24.95541146598098, 0.372744739425137, 0.0, 1.0, 64368.91402166285], 
processed observation next is [1.0, 0.8695652173913043, 0.42936288088642666, 0.63, 0.0, 0.0, 0.6666666666666666, 0.5796176221650816, 0.6242482464750457, 0.0, 1.0, 0.30651863819839453], 
reward next is 0.6935, 
noisyNet noise sample is [array([0.5816907], dtype=float32), -1.0493973]. 
=============================================
[2019-04-04 01:57:18,537] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[79.48734]
 [80.33684]
 [78.67965]
 [79.7169 ]
 [81.59773]], R is [[79.12588501]
 [78.94794464]
 [78.74355316]
 [78.83383942]
 [79.04550171]].
[2019-04-04 01:57:18,544] A3C_AGENT_WORKER-Thread-20 INFO:Local step 173500, global step 2781724: loss 3.8617
[2019-04-04 01:57:18,545] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 173500, global step 2781724: learning rate 0.0001
[2019-04-04 01:57:21,791] A3C_AGENT_WORKER-Thread-2 INFO:Local step 174000, global step 2782700: loss 3.4458
[2019-04-04 01:57:21,812] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 174000, global step 2782700: learning rate 0.0001
[2019-04-04 01:57:25,408] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.30541450e-22 1.31623647e-15 1.19185595e-20 1.38352352e-17
 5.92710643e-17 2.95472409e-21 1.00000000e+00], sum to 1.0000
[2019-04-04 01:57:25,416] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7836
[2019-04-04 01:57:25,430] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.2, 48.0, 0.0, 0.0, 26.0, 24.4006672738174, 0.1007959137186688, 0.0, 1.0, 43320.60136058913], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2422800.0000, 
sim time next is 2423400.0000, 
raw observation next is [-6.383333333333334, 48.83333333333333, 0.0, 0.0, 26.0, 24.34109444545035, 0.09033266471949557, 0.0, 1.0, 43385.70521556772], 
processed observation next is [0.0, 0.043478260869565216, 0.28578024007386893, 0.4883333333333333, 0.0, 0.0, 0.6666666666666666, 0.5284245371208623, 0.5301108882398319, 0.0, 1.0, 0.2065985962646082], 
reward next is 0.7934, 
noisyNet noise sample is [array([0.14281873], dtype=float32), -0.41086414]. 
=============================================
[2019-04-04 01:57:26,223] A3C_AGENT_WORKER-Thread-19 INFO:Local step 174500, global step 2783758: loss 3.7237
[2019-04-04 01:57:26,223] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 174500, global step 2783758: learning rate 0.0001
[2019-04-04 01:57:26,993] A3C_AGENT_WORKER-Thread-3 INFO:Local step 174000, global step 2783934: loss 3.5812
[2019-04-04 01:57:27,001] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 174000, global step 2783934: learning rate 0.0001
[2019-04-04 01:57:27,796] A3C_AGENT_WORKER-Thread-5 INFO:Local step 174000, global step 2784140: loss 3.5482
[2019-04-04 01:57:27,797] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 174000, global step 2784140: learning rate 0.0001
[2019-04-04 01:57:28,341] A3C_AGENT_WORKER-Thread-14 INFO:Local step 174000, global step 2784274: loss 3.4989
[2019-04-04 01:57:28,342] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 174000, global step 2784274: learning rate 0.0001
[2019-04-04 01:57:28,938] A3C_AGENT_WORKER-Thread-15 INFO:Local step 174000, global step 2784428: loss 3.4557
[2019-04-04 01:57:28,939] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 174000, global step 2784428: learning rate 0.0001
[2019-04-04 01:57:29,001] A3C_AGENT_WORKER-Thread-18 INFO:Local step 174000, global step 2784442: loss 3.4734
[2019-04-04 01:57:29,063] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 174000, global step 2784442: learning rate 0.0001
[2019-04-04 01:57:29,416] A3C_AGENT_WORKER-Thread-9 INFO:Local step 174000, global step 2784516: loss 3.4291
[2019-04-04 01:57:29,418] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 174000, global step 2784516: learning rate 0.0001
[2019-04-04 01:57:29,605] A3C_AGENT_WORKER-Thread-6 INFO:Local step 174000, global step 2784554: loss 3.4817
[2019-04-04 01:57:29,650] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 174000, global step 2784554: learning rate 0.0001
[2019-04-04 01:57:33,619] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.8495726e-22 6.8174569e-16 5.7561221e-21 1.6523525e-17 1.3732597e-17
 3.8580829e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:57:33,619] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3902
[2019-04-04 01:57:33,681] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.2333333333333333, 29.66666666666667, 0.0, 0.0, 26.0, 24.922689121866, 0.224052761651003, 0.0, 1.0, 173585.7806244541], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2488800.0000, 
sim time next is 2489400.0000, 
raw observation next is [-0.35, 29.5, 0.0, 0.0, 26.0, 24.94139546947256, 0.2493861225353271, 0.0, 1.0, 106876.7085997909], 
processed observation next is [0.0, 0.8260869565217391, 0.45290858725761773, 0.295, 0.0, 0.0, 0.6666666666666666, 0.5784496224560467, 0.5831287075117757, 0.0, 1.0, 0.5089367076180519], 
reward next is 0.4911, 
noisyNet noise sample is [array([0.06546079], dtype=float32), 1.484281]. 
=============================================
[2019-04-04 01:57:35,713] A3C_AGENT_WORKER-Thread-4 INFO:Local step 174000, global step 2785930: loss 3.2261
[2019-04-04 01:57:35,761] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 174000, global step 2785930: learning rate 0.0001
[2019-04-04 01:57:35,785] A3C_AGENT_WORKER-Thread-8 INFO:Local step 174000, global step 2785946: loss 3.2580
[2019-04-04 01:57:35,785] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 174000, global step 2785946: learning rate 0.0001
[2019-04-04 01:57:37,037] A3C_AGENT_WORKER-Thread-17 INFO:Local step 174500, global step 2786221: loss 3.1102
[2019-04-04 01:57:37,037] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 174500, global step 2786221: learning rate 0.0001
[2019-04-04 01:57:39,286] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4908651e-23 1.4659885e-16 1.9849570e-21 1.4780598e-18 2.7050785e-18
 7.0364979e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 01:57:39,321] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4912
[2019-04-04 01:57:39,362] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.7855760160398, 0.2505037022713388, 0.0, 1.0, 42218.74772118029], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2769600.0000, 
sim time next is 2770200.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.78303818629344, 0.2471352488818172, 0.0, 1.0, 42073.26572856863], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5652531821911199, 0.582378416293939, 0.0, 1.0, 0.20034888442175539], 
reward next is 0.7997, 
noisyNet noise sample is [array([1.3623338], dtype=float32), -0.9854608]. 
=============================================
[2019-04-04 01:57:39,797] A3C_AGENT_WORKER-Thread-16 INFO:Local step 174000, global step 2786899: loss 3.4231
[2019-04-04 01:57:39,800] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 174000, global step 2786899: learning rate 0.0001
[2019-04-04 01:57:41,816] A3C_AGENT_WORKER-Thread-10 INFO:Local step 174000, global step 2787446: loss 3.2848
[2019-04-04 01:57:41,853] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 174000, global step 2787446: learning rate 0.0001
[2019-04-04 01:57:44,205] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.7048612e-24 1.5537590e-16 4.7821586e-22 3.7327606e-19 3.4232575e-19
 3.8583310e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 01:57:44,205] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5855
[2019-04-04 01:57:44,249] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 73.0, 0.0, 0.0, 26.0, 24.90176689921308, 0.2551997886396772, 0.0, 1.0, 41645.68953058771], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2598600.0000, 
sim time next is 2599200.0000, 
raw observation next is [-5.0, 74.0, 0.0, 0.0, 26.0, 24.97989800314202, 0.252424487895651, 0.0, 1.0, 41599.9774153428], 
processed observation next is [1.0, 0.08695652173913043, 0.32409972299168976, 0.74, 0.0, 0.0, 0.6666666666666666, 0.5816581669285016, 0.584141495965217, 0.0, 1.0, 0.19809513054925143], 
reward next is 0.8019, 
noisyNet noise sample is [array([-0.3617301], dtype=float32), -0.13250035]. 
=============================================
[2019-04-04 01:57:47,912] A3C_AGENT_WORKER-Thread-7 INFO:Local step 174500, global step 2788933: loss 2.7433
[2019-04-04 01:57:47,913] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 174500, global step 2788933: learning rate 0.0001
[2019-04-04 01:57:51,657] A3C_AGENT_WORKER-Thread-20 INFO:Local step 174000, global step 2789766: loss 3.3364
[2019-04-04 01:57:51,663] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 174000, global step 2789770: learning rate 0.0001
[2019-04-04 01:57:52,569] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.0503969e-23 1.7142271e-17 1.3087074e-21 6.7324743e-18 1.1096369e-18
 3.5750356e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 01:57:52,569] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7812
[2019-04-04 01:57:52,688] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 50.0, 41.0, 103.0, 26.0, 25.79891836892181, 0.4668598677144953, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2653200.0000, 
sim time next is 2653800.0000, 
raw observation next is [0.3166666666666667, 50.66666666666667, 29.66666666666666, 96.0, 26.0, 26.04550509568164, 0.4821419723976383, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.47137580794090495, 0.5066666666666667, 0.09888888888888887, 0.10607734806629834, 0.6666666666666666, 0.6704587579734701, 0.6607139907992128, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.49260476], dtype=float32), -0.81688964]. 
=============================================
[2019-04-04 01:57:55,641] A3C_AGENT_WORKER-Thread-2 INFO:Local step 174500, global step 2790781: loss 2.4764
[2019-04-04 01:57:55,643] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 174500, global step 2790781: learning rate 0.0001
[2019-04-04 01:57:59,052] A3C_AGENT_WORKER-Thread-19 INFO:Local step 175000, global step 2791517: loss 0.0134
[2019-04-04 01:57:59,053] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 175000, global step 2791517: learning rate 0.0001
[2019-04-04 01:58:00,829] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.8849393e-23 1.7349764e-17 2.9361943e-22 4.9526393e-18 3.8308257e-19
 3.9520271e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 01:58:00,829] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9715
[2019-04-04 01:58:00,934] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.666666666666667, 60.66666666666666, 112.3333333333333, 797.1666666666667, 26.0, 25.29426394596282, 0.4015509190729253, 1.0, 1.0, 166630.5289785643], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2724000.0000, 
sim time next is 2724600.0000, 
raw observation next is [-6.333333333333333, 59.83333333333334, 111.6666666666667, 795.3333333333334, 26.0, 24.58196828002751, 0.4310887986134332, 1.0, 1.0, 198773.4843706625], 
processed observation next is [1.0, 0.5217391304347826, 0.28716528162511545, 0.5983333333333334, 0.37222222222222234, 0.8788213627992634, 0.6666666666666666, 0.5484973566689592, 0.6436962662044777, 1.0, 1.0, 0.9465404017650596], 
reward next is 0.0535, 
noisyNet noise sample is [array([-1.3146737], dtype=float32), -0.44642892]. 
=============================================
[2019-04-04 01:58:01,855] A3C_AGENT_WORKER-Thread-3 INFO:Local step 174500, global step 2792100: loss 2.3893
[2019-04-04 01:58:01,856] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 174500, global step 2792100: learning rate 0.0001
[2019-04-04 01:58:02,929] A3C_AGENT_WORKER-Thread-5 INFO:Local step 174500, global step 2792350: loss 2.0845
[2019-04-04 01:58:02,937] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 174500, global step 2792350: learning rate 0.0001
[2019-04-04 01:58:03,249] A3C_AGENT_WORKER-Thread-14 INFO:Local step 174500, global step 2792423: loss 2.1566
[2019-04-04 01:58:03,249] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 174500, global step 2792423: learning rate 0.0001
[2019-04-04 01:58:03,273] A3C_AGENT_WORKER-Thread-9 INFO:Local step 174500, global step 2792432: loss 2.0893
[2019-04-04 01:58:03,286] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 174500, global step 2792433: learning rate 0.0001
[2019-04-04 01:58:03,507] A3C_AGENT_WORKER-Thread-18 INFO:Local step 174500, global step 2792488: loss 2.1595
[2019-04-04 01:58:03,508] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 174500, global step 2792488: learning rate 0.0001
[2019-04-04 01:58:03,648] A3C_AGENT_WORKER-Thread-6 INFO:Local step 174500, global step 2792530: loss 2.2331
[2019-04-04 01:58:03,648] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 174500, global step 2792530: learning rate 0.0001
[2019-04-04 01:58:03,749] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.5905630e-24 3.7696546e-17 1.6063139e-22 5.2864136e-19 4.5966313e-19
 7.8904566e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 01:58:03,749] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3009
[2019-04-04 01:58:03,787] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.7, 47.33333333333334, 166.0, 53.66666666666666, 26.0, 25.76500204087464, 0.2991750007864951, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2544600.0000, 
sim time next is 2545200.0000, 
raw observation next is [-0.6, 47.0, 182.5, 58.0, 26.0, 25.71937677261337, 0.3013500748799102, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.44598337950138506, 0.47, 0.6083333333333333, 0.06408839779005525, 0.6666666666666666, 0.6432813977177808, 0.6004500249599701, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.98855346], dtype=float32), 0.11037732]. 
=============================================
[2019-04-04 01:58:03,800] A3C_AGENT_WORKER-Thread-15 INFO:Local step 174500, global step 2792574: loss 2.2936
[2019-04-04 01:58:03,801] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 174500, global step 2792574: learning rate 0.0001
[2019-04-04 01:58:10,240] A3C_AGENT_WORKER-Thread-8 INFO:Local step 174500, global step 2794085: loss 2.3166
[2019-04-04 01:58:10,241] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 174500, global step 2794085: learning rate 0.0001
[2019-04-04 01:58:10,500] A3C_AGENT_WORKER-Thread-17 INFO:Local step 175000, global step 2794140: loss 0.0269
[2019-04-04 01:58:10,514] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 175000, global step 2794140: learning rate 0.0001
[2019-04-04 01:58:10,726] A3C_AGENT_WORKER-Thread-4 INFO:Local step 174500, global step 2794197: loss 2.1634
[2019-04-04 01:58:10,748] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 174500, global step 2794197: learning rate 0.0001
[2019-04-04 01:58:12,906] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0753308e-22 8.1338202e-16 1.5130856e-21 4.0355400e-18 3.5254235e-18
 7.6762521e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 01:58:12,911] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1060
[2019-04-04 01:58:12,974] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 54.0, 107.6666666666667, 774.3333333333333, 26.0, 25.02169680424517, 0.3163763415866995, 0.0, 1.0, 62565.95261111204], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3063000.0000, 
sim time next is 3063600.0000, 
raw observation next is [-4.0, 54.0, 108.5, 782.0, 26.0, 25.0453530105336, 0.3334154381869984, 0.0, 1.0, 34446.42400518098], 
processed observation next is [0.0, 0.4782608695652174, 0.3518005540166205, 0.54, 0.3616666666666667, 0.8640883977900552, 0.6666666666666666, 0.5871127508777999, 0.6111384793956661, 0.0, 1.0, 0.16403059050086183], 
reward next is 0.8360, 
noisyNet noise sample is [array([-0.37068546], dtype=float32), 0.41304624]. 
=============================================
[2019-04-04 01:58:13,757] A3C_AGENT_WORKER-Thread-16 INFO:Local step 174500, global step 2794858: loss 1.9572
[2019-04-04 01:58:13,758] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 174500, global step 2794858: learning rate 0.0001
[2019-04-04 01:58:14,852] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.5142644e-22 8.5597857e-16 1.0223146e-20 1.4990933e-17 2.8942444e-17
 7.9586927e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:58:14,852] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2393
[2019-04-04 01:58:14,882] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.3333333333333333, 39.33333333333334, 86.5, 690.0, 26.0, 25.12339509626046, 0.3633780159185124, 0.0, 1.0, 18697.71884366193], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3079200.0000, 
sim time next is 3079800.0000, 
raw observation next is [0.5, 39.5, 84.0, 673.0, 26.0, 25.12393866727817, 0.3661078196000065, 0.0, 1.0, 18696.50628669648], 
processed observation next is [0.0, 0.6521739130434783, 0.4764542936288089, 0.395, 0.28, 0.7436464088397791, 0.6666666666666666, 0.5936615556065142, 0.6220359398666688, 0.0, 1.0, 0.08903098231760229], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.9438358], dtype=float32), -1.2578673]. 
=============================================
[2019-04-04 01:58:15,745] A3C_AGENT_WORKER-Thread-10 INFO:Local step 174500, global step 2795435: loss 1.8914
[2019-04-04 01:58:15,747] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 174500, global step 2795435: learning rate 0.0001
[2019-04-04 01:58:22,228] A3C_AGENT_WORKER-Thread-7 INFO:Local step 175000, global step 2797025: loss 0.0352
[2019-04-04 01:58:22,229] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 175000, global step 2797025: learning rate 0.0001
[2019-04-04 01:58:27,319] A3C_AGENT_WORKER-Thread-20 INFO:Local step 174500, global step 2798246: loss 2.2646
[2019-04-04 01:58:27,320] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 174500, global step 2798246: learning rate 0.0001
[2019-04-04 01:58:29,455] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.1316621e-22 6.1185730e-15 8.8958921e-21 3.7312268e-17 2.4192465e-17
 8.8956843e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 01:58:29,455] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1829
[2019-04-04 01:58:29,522] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.5, 62.5, 110.0, 800.0, 26.0, 25.12358306527952, 0.409327719338324, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2986200.0000, 
sim time next is 2986800.0000, 
raw observation next is [-2.333333333333333, 61.66666666666667, 108.5, 791.8333333333334, 26.0, 25.12884688685212, 0.4063783560909659, 0.0, 1.0, 18716.68195802214], 
processed observation next is [0.0, 0.5652173913043478, 0.3979686057248385, 0.6166666666666667, 0.3616666666666667, 0.8749539594843463, 0.6666666666666666, 0.5940705739043434, 0.635459452030322, 0.0, 1.0, 0.08912705694296258], 
reward next is 0.9109, 
noisyNet noise sample is [array([-0.41962755], dtype=float32), 0.4863869]. 
=============================================
[2019-04-04 01:58:29,631] A3C_AGENT_WORKER-Thread-19 INFO:Local step 175500, global step 2798712: loss 0.0779
[2019-04-04 01:58:29,635] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 175500, global step 2798712: learning rate 0.0001
[2019-04-04 01:58:30,188] A3C_AGENT_WORKER-Thread-2 INFO:Local step 175000, global step 2798834: loss 0.0286
[2019-04-04 01:58:30,209] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 175000, global step 2798834: learning rate 0.0001
[2019-04-04 01:58:34,470] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-04 01:58:34,491] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 01:58:34,491] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:58:34,493] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run29
[2019-04-04 01:58:34,526] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 01:58:34,526] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:58:34,527] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 01:58:34,527] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 01:58:34,530] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run29
[2019-04-04 01:58:34,546] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run29
[2019-04-04 02:01:42,245] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 02:01:43,185] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.37131095], dtype=float32), 0.14237113]
[2019-04-04 02:01:43,185] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-4.0, 34.0, 46.0, 234.5, 26.0, 25.38615515206241, 0.3344951207962724, 1.0, 1.0, 0.0]
[2019-04-04 02:01:43,185] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 02:01:43,186] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [5.9666260e-24 3.3569742e-16 2.1186406e-22 6.0128188e-19 1.2004995e-18
 3.4272750e-23 1.0000000e+00], sampled 0.39239912674484034
[2019-04-04 02:02:08,814] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 02:02:11,612] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 02:02:12,642] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 2800000, evaluation results [2800000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 02:02:12,718] A3C_AGENT_WORKER-Thread-3 INFO:Local step 175000, global step 2800044: loss 0.0237
[2019-04-04 02:02:12,729] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 175000, global step 2800045: learning rate 0.0001
[2019-04-04 02:02:12,780] A3C_AGENT_WORKER-Thread-9 INFO:Local step 175000, global step 2800067: loss 0.0216
[2019-04-04 02:02:12,781] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 175000, global step 2800067: learning rate 0.0001
[2019-04-04 02:02:13,068] A3C_AGENT_WORKER-Thread-5 INFO:Local step 175000, global step 2800187: loss 0.0185
[2019-04-04 02:02:13,073] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 175000, global step 2800192: learning rate 0.0001
[2019-04-04 02:02:13,381] A3C_AGENT_WORKER-Thread-18 INFO:Local step 175000, global step 2800298: loss 0.0105
[2019-04-04 02:02:13,382] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 175000, global step 2800298: learning rate 0.0001
[2019-04-04 02:02:13,621] A3C_AGENT_WORKER-Thread-14 INFO:Local step 175000, global step 2800385: loss 0.0069
[2019-04-04 02:02:13,624] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 175000, global step 2800385: learning rate 0.0001
[2019-04-04 02:02:13,818] A3C_AGENT_WORKER-Thread-6 INFO:Local step 175000, global step 2800446: loss 0.0079
[2019-04-04 02:02:13,847] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 175000, global step 2800446: learning rate 0.0001
[2019-04-04 02:02:13,998] A3C_AGENT_WORKER-Thread-15 INFO:Local step 175000, global step 2800512: loss 0.0051
[2019-04-04 02:02:13,998] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 175000, global step 2800512: learning rate 0.0001
[2019-04-04 02:02:15,766] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [7.6395612e-23 6.0822434e-16 3.7068746e-21 1.0781059e-17 9.1856471e-18
 2.3978787e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 02:02:15,766] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.1146
[2019-04-04 02:02:15,827] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.5, 57.5, 6.0, 99.0, 26.0, 25.05503100006522, 0.3403752114116368, 0.0, 1.0, 48348.00502101535], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3000600.0000, 
sim time next is 3001200.0000, 
raw observation next is [-1.666666666666667, 58.33333333333334, 0.0, 0.0, 26.0, 25.03835415092971, 0.3428552639723497, 0.0, 1.0, 51008.15246615215], 
processed observation next is [0.0, 0.7391304347826086, 0.4164358264081256, 0.5833333333333335, 0.0, 0.0, 0.6666666666666666, 0.5865295125774758, 0.6142850879907832, 0.0, 1.0, 0.24289596412453404], 
reward next is 0.7571, 
noisyNet noise sample is [array([1.1350669], dtype=float32), 1.4575698]. 
=============================================
[2019-04-04 02:02:17,807] A3C_AGENT_WORKER-Thread-17 INFO:Local step 175500, global step 2801858: loss 0.7510
[2019-04-04 02:02:17,811] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 175500, global step 2801861: learning rate 0.0001
[2019-04-04 02:02:17,872] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.7640915e-24 1.1983412e-16 4.3246238e-22 1.0507675e-18 7.1897071e-19
 2.1545743e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 02:02:17,877] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2307
[2019-04-04 02:02:17,894] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.3333333333333333, 100.0, 0.0, 0.0, 26.0, 25.3031440480109, 0.3219674233989786, 0.0, 1.0, 39579.48690476564], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3111600.0000, 
sim time next is 3112200.0000, 
raw observation next is [0.5, 100.0, 0.0, 0.0, 26.0, 25.30590056233434, 0.320850936082458, 0.0, 1.0, 39469.85022859983], 
processed observation next is [1.0, 0.0, 0.4764542936288089, 1.0, 0.0, 0.0, 0.6666666666666666, 0.608825046861195, 0.6069503120274861, 0.0, 1.0, 0.1879516677552373], 
reward next is 0.8120, 
noisyNet noise sample is [array([0.03876894], dtype=float32), -0.730888]. 
=============================================
[2019-04-04 02:02:18,357] A3C_AGENT_WORKER-Thread-4 INFO:Local step 175000, global step 2802112: loss 0.0078
[2019-04-04 02:02:18,358] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 175000, global step 2802112: learning rate 0.0001
[2019-04-04 02:02:18,722] A3C_AGENT_WORKER-Thread-8 INFO:Local step 175000, global step 2802274: loss 0.0127
[2019-04-04 02:02:18,724] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 175000, global step 2802276: learning rate 0.0001
[2019-04-04 02:02:20,521] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [9.32958136e-27 1.56705510e-18 2.48596025e-24 1.29728084e-20
 5.75271966e-21 1.10254023e-25 1.00000000e+00], sum to 1.0000
[2019-04-04 02:02:20,522] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1278
[2019-04-04 02:02:20,565] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.5, 100.0, 83.0, 392.0, 26.0, 25.77333774796746, 0.4478601126609875, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3141000.0000, 
sim time next is 3141600.0000, 
raw observation next is [6.666666666666666, 100.0, 85.66666666666667, 434.5, 26.0, 25.98033147509004, 0.4692270018139912, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.6472760849492153, 1.0, 0.28555555555555556, 0.48011049723756904, 0.6666666666666666, 0.66502762292417, 0.6564090006046638, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.04456192], dtype=float32), 0.6768355]. 
=============================================
[2019-04-04 02:02:20,716] A3C_AGENT_WORKER-Thread-16 INFO:Local step 175000, global step 2803141: loss 0.0042
[2019-04-04 02:02:20,719] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 175000, global step 2803143: learning rate 0.0001
[2019-04-04 02:02:22,167] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.7542835e-24 6.7438356e-18 2.2102431e-22 6.9152496e-19 2.7588404e-19
 2.1043334e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:02:22,171] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3242
[2019-04-04 02:02:22,194] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 100.0, 0.0, 0.0, 26.0, 25.82135291521322, 0.6701807175269593, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3182400.0000, 
sim time next is 3183000.0000, 
raw observation next is [3.0, 100.0, 0.0, 0.0, 26.0, 25.75898767244954, 0.6537104408804584, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.5457063711911359, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6465823060374616, 0.7179034802934862, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2799798], dtype=float32), 0.20927337]. 
=============================================
[2019-04-04 02:02:22,217] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[81.20215 ]
 [79.483635]
 [80.064896]
 [81.2299  ]
 [82.98883 ]], R is [[80.43326569]
 [80.62893677]
 [80.82264709]
 [81.01441956]
 [81.20427704]].
[2019-04-04 02:02:22,346] A3C_AGENT_WORKER-Thread-10 INFO:Local step 175000, global step 2803926: loss 0.0033
[2019-04-04 02:02:22,361] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 175000, global step 2803928: learning rate 0.0001
[2019-04-04 02:02:24,061] A3C_AGENT_WORKER-Thread-7 INFO:Local step 175500, global step 2804664: loss 1.0708
[2019-04-04 02:02:24,063] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 175500, global step 2804664: learning rate 0.0001
[2019-04-04 02:02:27,749] A3C_AGENT_WORKER-Thread-19 INFO:Local step 176000, global step 2806265: loss 0.2959
[2019-04-04 02:02:27,750] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 176000, global step 2806265: learning rate 0.0001
[2019-04-04 02:02:28,720] A3C_AGENT_WORKER-Thread-2 INFO:Local step 175500, global step 2806753: loss 0.9073
[2019-04-04 02:02:28,721] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 175500, global step 2806754: learning rate 0.0001
[2019-04-04 02:02:29,013] A3C_AGENT_WORKER-Thread-20 INFO:Local step 175000, global step 2806890: loss 0.0109
[2019-04-04 02:02:29,019] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 175000, global step 2806892: learning rate 0.0001
[2019-04-04 02:02:31,580] A3C_AGENT_WORKER-Thread-9 INFO:Local step 175500, global step 2807937: loss 1.1439
[2019-04-04 02:02:31,582] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 175500, global step 2807937: learning rate 0.0001
[2019-04-04 02:02:31,746] A3C_AGENT_WORKER-Thread-5 INFO:Local step 175500, global step 2808003: loss 1.0973
[2019-04-04 02:02:31,753] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 175500, global step 2808007: learning rate 0.0001
[2019-04-04 02:02:31,958] A3C_AGENT_WORKER-Thread-3 INFO:Local step 175500, global step 2808089: loss 1.1723
[2019-04-04 02:02:31,959] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 175500, global step 2808091: learning rate 0.0001
[2019-04-04 02:02:32,568] A3C_AGENT_WORKER-Thread-15 INFO:Local step 175500, global step 2808332: loss 1.2125
[2019-04-04 02:02:32,568] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 175500, global step 2808332: learning rate 0.0001
[2019-04-04 02:02:32,600] A3C_AGENT_WORKER-Thread-18 INFO:Local step 175500, global step 2808345: loss 1.1899
[2019-04-04 02:02:32,635] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 175500, global step 2808346: learning rate 0.0001
[2019-04-04 02:02:32,767] A3C_AGENT_WORKER-Thread-6 INFO:Local step 175500, global step 2808404: loss 1.1246
[2019-04-04 02:02:32,769] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 175500, global step 2808404: learning rate 0.0001
[2019-04-04 02:02:33,156] A3C_AGENT_WORKER-Thread-14 INFO:Local step 175500, global step 2808579: loss 1.2760
[2019-04-04 02:02:33,164] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 175500, global step 2808579: learning rate 0.0001
[2019-04-04 02:02:35,674] A3C_AGENT_WORKER-Thread-17 INFO:Local step 176000, global step 2809699: loss 0.1070
[2019-04-04 02:02:35,677] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 176000, global step 2809700: learning rate 0.0001
[2019-04-04 02:02:36,831] A3C_AGENT_WORKER-Thread-4 INFO:Local step 175500, global step 2810231: loss 1.1740
[2019-04-04 02:02:36,833] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 175500, global step 2810231: learning rate 0.0001
[2019-04-04 02:02:37,040] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.0654569e-26 1.6878299e-18 1.2789941e-24 2.1671399e-20 1.2295816e-20
 1.2013749e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:02:37,041] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0558
[2019-04-04 02:02:37,072] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.5, 50.0, 102.3333333333333, 693.3333333333334, 26.0, 26.48737067929524, 0.5839664800785161, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3405000.0000, 
sim time next is 3405600.0000, 
raw observation next is [2.0, 48.0, 104.0, 711.0, 26.0, 26.53790666784383, 0.6002541237615617, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.518005540166205, 0.48, 0.3466666666666667, 0.7856353591160221, 0.6666666666666666, 0.7114922223203193, 0.7000847079205205, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.30998206], dtype=float32), 0.014393403]. 
=============================================
[2019-04-04 02:02:37,402] A3C_AGENT_WORKER-Thread-8 INFO:Local step 175500, global step 2810500: loss 0.9114
[2019-04-04 02:02:37,403] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 175500, global step 2810500: learning rate 0.0001
[2019-04-04 02:02:39,749] A3C_AGENT_WORKER-Thread-16 INFO:Local step 175500, global step 2811592: loss 0.8677
[2019-04-04 02:02:39,750] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 175500, global step 2811593: learning rate 0.0001
[2019-04-04 02:02:41,121] A3C_AGENT_WORKER-Thread-10 INFO:Local step 175500, global step 2812180: loss 1.1449
[2019-04-04 02:02:41,121] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 175500, global step 2812180: learning rate 0.0001
[2019-04-04 02:02:41,848] A3C_AGENT_WORKER-Thread-7 INFO:Local step 176000, global step 2812573: loss 0.1619
[2019-04-04 02:02:41,848] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 176000, global step 2812573: learning rate 0.0001
[2019-04-04 02:02:43,042] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.8210356e-25 7.1762294e-18 3.6286617e-23 1.6106237e-19 1.3659126e-19
 6.2818724e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:02:43,043] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2972
[2019-04-04 02:02:43,060] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.66868748311508, 0.610649281789963, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3531600.0000, 
sim time next is 3532200.0000, 
raw observation next is [-0.1666666666666667, 73.0, 0.0, 0.0, 26.0, 25.84431037127768, 0.6202761280288754, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.4579870729455217, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6536925309398066, 0.7067587093429585, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.30223206], dtype=float32), 0.96748376]. 
=============================================
[2019-04-04 02:02:44,166] A3C_AGENT_WORKER-Thread-19 INFO:Local step 176500, global step 2813724: loss 0.7514
[2019-04-04 02:02:44,166] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 176500, global step 2813724: learning rate 0.0001
[2019-04-04 02:02:45,943] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.5439987e-26 6.9429508e-19 1.5153707e-24 3.0057126e-20 3.0060438e-21
 1.2973066e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:02:45,944] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9038
[2019-04-04 02:02:45,982] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.333333333333333, 50.00000000000001, 113.6666666666667, 825.8333333333334, 26.0, 26.66097397205181, 0.5838884549287223, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3849600.0000, 
sim time next is 3850200.0000, 
raw observation next is [1.5, 49.5, 113.0, 824.0, 26.0, 26.10716292885248, 0.6300162273755254, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5041551246537397, 0.495, 0.37666666666666665, 0.9104972375690608, 0.6666666666666666, 0.6755969107377066, 0.7100054091251752, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.585025], dtype=float32), 1.0201023]. 
=============================================
[2019-04-04 02:02:46,973] A3C_AGENT_WORKER-Thread-2 INFO:Local step 176000, global step 2814871: loss 0.1775
[2019-04-04 02:02:46,975] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 176000, global step 2814871: learning rate 0.0001
[2019-04-04 02:02:47,667] A3C_AGENT_WORKER-Thread-20 INFO:Local step 175500, global step 2815218: loss 0.7815
[2019-04-04 02:02:47,670] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 175500, global step 2815221: learning rate 0.0001
[2019-04-04 02:02:48,240] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.9696580e-24 6.8199242e-17 6.9062755e-22 7.2783984e-19 1.1610987e-18
 1.1528514e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 02:02:48,242] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4446
[2019-04-04 02:02:48,273] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.31862039561346, 0.4047185117648353, 0.0, 1.0, 61982.47077916178], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3894600.0000, 
sim time next is 3895200.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.26267830552031, 0.4025018251259354, 0.0, 1.0, 48909.76253121537], 
processed observation next is [1.0, 0.08695652173913043, 0.40720221606648205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6052231921266925, 0.6341672750419785, 0.0, 1.0, 0.23290363110102558], 
reward next is 0.7671, 
noisyNet noise sample is [array([-1.3963693], dtype=float32), 2.6358347]. 
=============================================
[2019-04-04 02:02:48,621] A3C_AGENT_WORKER-Thread-9 INFO:Local step 176000, global step 2815658: loss 0.1614
[2019-04-04 02:02:48,624] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 176000, global step 2815658: learning rate 0.0001
[2019-04-04 02:02:49,323] A3C_AGENT_WORKER-Thread-5 INFO:Local step 176000, global step 2816027: loss 0.1868
[2019-04-04 02:02:49,326] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 176000, global step 2816028: learning rate 0.0001
[2019-04-04 02:02:49,741] A3C_AGENT_WORKER-Thread-6 INFO:Local step 176000, global step 2816260: loss 0.1842
[2019-04-04 02:02:49,743] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 176000, global step 2816261: learning rate 0.0001
[2019-04-04 02:02:49,888] A3C_AGENT_WORKER-Thread-3 INFO:Local step 176000, global step 2816330: loss 0.2021
[2019-04-04 02:02:49,889] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 176000, global step 2816331: learning rate 0.0001
[2019-04-04 02:02:49,912] A3C_AGENT_WORKER-Thread-15 INFO:Local step 176000, global step 2816342: loss 0.1959
[2019-04-04 02:02:49,912] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 176000, global step 2816342: learning rate 0.0001
[2019-04-04 02:02:50,103] A3C_AGENT_WORKER-Thread-18 INFO:Local step 176000, global step 2816449: loss 0.1870
[2019-04-04 02:02:50,105] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 176000, global step 2816449: learning rate 0.0001
[2019-04-04 02:02:50,847] A3C_AGENT_WORKER-Thread-14 INFO:Local step 176000, global step 2816818: loss 0.1547
[2019-04-04 02:02:50,855] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 176000, global step 2816818: learning rate 0.0001
[2019-04-04 02:02:51,356] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.8308746e-24 2.8055187e-17 5.2855962e-23 1.1892405e-19 1.3625358e-18
 2.3573889e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:02:51,357] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3888
[2019-04-04 02:02:51,385] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.0, 45.66666666666667, 101.1666666666667, 764.1666666666667, 26.0, 25.50049207200338, 0.4858114260102029, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3681600.0000, 
sim time next is 3682200.0000, 
raw observation next is [6.0, 46.33333333333334, 98.33333333333334, 752.3333333333333, 26.0, 25.49571053286211, 0.4852664115450744, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.6288088642659281, 0.46333333333333343, 0.32777777777777783, 0.8313075506445672, 0.6666666666666666, 0.6246425444051757, 0.6617554705150248, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8644201], dtype=float32), -1.3609284]. 
=============================================
[2019-04-04 02:02:51,551] A3C_AGENT_WORKER-Thread-17 INFO:Local step 176500, global step 2817184: loss 0.6814
[2019-04-04 02:02:51,552] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 176500, global step 2817184: learning rate 0.0001
[2019-04-04 02:02:52,145] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.9488067e-25 1.2076425e-17 1.3573601e-23 4.1738784e-20 5.7895095e-20
 2.3183357e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:02:52,148] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1816
[2019-04-04 02:02:52,181] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.666666666666667, 77.0, 98.16666666666667, 634.0, 26.0, 25.96601288647788, 0.4707802293512444, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3748800.0000, 
sim time next is 3749400.0000, 
raw observation next is [-3.5, 77.0, 100.0, 675.0, 26.0, 26.12975506150596, 0.4860761436307552, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.36565096952908593, 0.77, 0.3333333333333333, 0.7458563535911602, 0.6666666666666666, 0.67747958845883, 0.6620253812102518, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3527277], dtype=float32), 0.37307495]. 
=============================================
[2019-04-04 02:02:54,239] A3C_AGENT_WORKER-Thread-4 INFO:Local step 176000, global step 2818536: loss 0.2802
[2019-04-04 02:02:54,241] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 176000, global step 2818536: learning rate 0.0001
[2019-04-04 02:02:54,405] A3C_AGENT_WORKER-Thread-8 INFO:Local step 176000, global step 2818612: loss 0.2646
[2019-04-04 02:02:54,422] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 176000, global step 2818614: learning rate 0.0001
[2019-04-04 02:02:55,303] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.8188517e-23 1.7352563e-15 2.8884367e-21 2.6445330e-18 7.6334924e-18
 1.0692034e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 02:02:55,309] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0968
[2019-04-04 02:02:55,322] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.24873969859949, 0.388650353272429, 0.0, 1.0, 41890.30296305628], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3477000.0000, 
sim time next is 3477600.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.28543980315915, 0.3838747799258298, 0.0, 1.0, 41849.95172471827], 
processed observation next is [1.0, 0.2608695652173913, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6071199835965958, 0.6279582599752765, 0.0, 1.0, 0.1992854844034203], 
reward next is 0.8007, 
noisyNet noise sample is [array([-1.7792412], dtype=float32), -1.8934326]. 
=============================================
[2019-04-04 02:02:57,091] A3C_AGENT_WORKER-Thread-16 INFO:Local step 176000, global step 2819917: loss 0.2061
[2019-04-04 02:02:57,092] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 176000, global step 2819917: learning rate 0.0001
[2019-04-04 02:02:57,644] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.3274457e-24 4.3686743e-17 8.6852409e-22 2.7018093e-18 1.0282450e-18
 2.8109307e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 02:02:57,645] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6929
[2019-04-04 02:02:57,660] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 77.0, 0.0, 0.0, 26.0, 25.30862239525782, 0.4026604670861376, 0.0, 1.0, 44162.28474514392], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3807000.0000, 
sim time next is 3807600.0000, 
raw observation next is [-4.0, 77.0, 0.0, 0.0, 26.0, 25.3331287410855, 0.4078558128378999, 0.0, 1.0, 44115.14999956332], 
processed observation next is [1.0, 0.043478260869565216, 0.3518005540166205, 0.77, 0.0, 0.0, 0.6666666666666666, 0.6110940617571249, 0.6359519376126334, 0.0, 1.0, 0.21007214285506343], 
reward next is 0.7899, 
noisyNet noise sample is [array([0.33029762], dtype=float32), -0.06494685]. 
=============================================
[2019-04-04 02:02:57,881] A3C_AGENT_WORKER-Thread-7 INFO:Local step 176500, global step 2820316: loss 0.7411
[2019-04-04 02:02:57,917] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 176500, global step 2820332: learning rate 0.0001
[2019-04-04 02:02:58,116] A3C_AGENT_WORKER-Thread-10 INFO:Local step 176000, global step 2820431: loss 0.2379
[2019-04-04 02:02:58,121] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 176000, global step 2820432: learning rate 0.0001
[2019-04-04 02:02:58,145] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.0059105e-23 9.3094405e-16 3.7515000e-21 6.8207274e-18 6.2472200e-18
 1.3691292e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 02:02:58,148] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0332
[2019-04-04 02:02:58,176] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.35739332184442, 0.3153872233087934, 0.0, 1.0, 49157.25925377775], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3732000.0000, 
sim time next is 3732600.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.28247454317658, 0.3097593287215458, 0.0, 1.0, 46245.72903792594], 
processed observation next is [1.0, 0.17391304347826086, 0.3795013850415513, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6068728785980483, 0.6032531095738486, 0.0, 1.0, 0.22021775732345686], 
reward next is 0.7798, 
noisyNet noise sample is [array([0.54308194], dtype=float32), -1.31415]. 
=============================================
[2019-04-04 02:02:59,394] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.7661493e-23 1.2348157e-16 3.5635727e-22 8.1166674e-19 5.2912783e-18
 3.5522381e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 02:02:59,394] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5235
[2019-04-04 02:02:59,401] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.666666666666666, 43.0, 116.3333333333333, 827.1666666666667, 26.0, 25.33496169158196, 0.4551014136993774, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3674400.0000, 
sim time next is 3675000.0000, 
raw observation next is [4.833333333333334, 42.5, 115.6666666666667, 825.3333333333334, 26.0, 25.30719035199024, 0.453078880736373, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.5964912280701755, 0.425, 0.38555555555555565, 0.9119705340699816, 0.6666666666666666, 0.60893252933252, 0.651026293578791, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.31474838], dtype=float32), 0.23905754]. 
=============================================
[2019-04-04 02:02:59,415] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[83.86142 ]
 [84.174995]
 [84.620415]
 [85.2494  ]
 [86.15097 ]], R is [[83.86473083]
 [84.0260849 ]
 [84.18582153]
 [84.34396362]
 [84.50052643]].
[2019-04-04 02:03:02,104] A3C_AGENT_WORKER-Thread-19 INFO:Local step 177000, global step 2822296: loss 0.0779
[2019-04-04 02:03:02,106] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 177000, global step 2822297: learning rate 0.0001
[2019-04-04 02:03:03,192] A3C_AGENT_WORKER-Thread-2 INFO:Local step 176500, global step 2822821: loss 1.0231
[2019-04-04 02:03:03,193] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 176500, global step 2822821: learning rate 0.0001
[2019-04-04 02:03:04,736] A3C_AGENT_WORKER-Thread-9 INFO:Local step 176500, global step 2823459: loss 1.1056
[2019-04-04 02:03:04,746] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 176500, global step 2823464: learning rate 0.0001
[2019-04-04 02:03:05,043] A3C_AGENT_WORKER-Thread-20 INFO:Local step 176000, global step 2823617: loss 0.2366
[2019-04-04 02:03:05,045] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 176000, global step 2823619: learning rate 0.0001
[2019-04-04 02:03:06,069] A3C_AGENT_WORKER-Thread-5 INFO:Local step 176500, global step 2824069: loss 1.4249
[2019-04-04 02:03:06,071] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 176500, global step 2824071: learning rate 0.0001
[2019-04-04 02:03:06,162] A3C_AGENT_WORKER-Thread-15 INFO:Local step 176500, global step 2824114: loss 1.2859
[2019-04-04 02:03:06,164] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 176500, global step 2824116: learning rate 0.0001
[2019-04-04 02:03:06,238] A3C_AGENT_WORKER-Thread-6 INFO:Local step 176500, global step 2824145: loss 1.2786
[2019-04-04 02:03:06,242] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 176500, global step 2824147: learning rate 0.0001
[2019-04-04 02:03:06,622] A3C_AGENT_WORKER-Thread-3 INFO:Local step 176500, global step 2824323: loss 1.1447
[2019-04-04 02:03:06,622] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 176500, global step 2824323: learning rate 0.0001
[2019-04-04 02:03:06,968] A3C_AGENT_WORKER-Thread-18 INFO:Local step 176500, global step 2824460: loss 1.2485
[2019-04-04 02:03:06,971] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 176500, global step 2824462: learning rate 0.0001
[2019-04-04 02:03:07,119] A3C_AGENT_WORKER-Thread-14 INFO:Local step 176500, global step 2824522: loss 1.1882
[2019-04-04 02:03:07,121] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 176500, global step 2824522: learning rate 0.0001
[2019-04-04 02:03:09,517] A3C_AGENT_WORKER-Thread-17 INFO:Local step 177000, global step 2825542: loss 0.0716
[2019-04-04 02:03:09,519] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 177000, global step 2825543: learning rate 0.0001
[2019-04-04 02:03:10,891] A3C_AGENT_WORKER-Thread-4 INFO:Local step 176500, global step 2826064: loss 1.2895
[2019-04-04 02:03:10,893] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 176500, global step 2826064: learning rate 0.0001
[2019-04-04 02:03:11,261] A3C_AGENT_WORKER-Thread-8 INFO:Local step 176500, global step 2826230: loss 1.1336
[2019-04-04 02:03:11,262] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 176500, global step 2826230: learning rate 0.0001
[2019-04-04 02:03:14,059] A3C_AGENT_WORKER-Thread-16 INFO:Local step 176500, global step 2827420: loss 1.1028
[2019-04-04 02:03:14,061] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 176500, global step 2827421: learning rate 0.0001
[2019-04-04 02:03:15,516] A3C_AGENT_WORKER-Thread-10 INFO:Local step 176500, global step 2828024: loss 1.0722
[2019-04-04 02:03:15,516] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 176500, global step 2828024: learning rate 0.0001
[2019-04-04 02:03:16,350] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.3102489e-25 5.6396744e-18 4.7427312e-24 1.0527289e-19 4.9842860e-20
 2.8126870e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:03:16,351] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3538
[2019-04-04 02:03:16,379] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 35.0, 109.0, 724.0, 26.0, 26.44083596781659, 0.5639495359215754, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4096800.0000, 
sim time next is 4097400.0000, 
raw observation next is [-1.833333333333333, 34.5, 110.6666666666667, 739.0, 26.0, 26.53381008076571, 0.5822239171785163, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.41181902123730385, 0.345, 0.368888888888889, 0.8165745856353591, 0.6666666666666666, 0.7111508400638092, 0.6940746390595054, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3886324], dtype=float32), 0.248964]. 
=============================================
[2019-04-04 02:03:16,695] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.2627850e-23 9.7167279e-16 9.2843075e-21 4.5074689e-18 6.3581507e-18
 2.9033376e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 02:03:16,696] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8992
[2019-04-04 02:03:16,743] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.0, 63.0, 0.0, 0.0, 26.0, 24.18022256450548, 0.1137711891745468, 0.0, 1.0, 43643.89971858619], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3988800.0000, 
sim time next is 3989400.0000, 
raw observation next is [-12.16666666666667, 64.0, 0.0, 0.0, 26.0, 24.11401463913138, 0.1050442692036519, 0.0, 1.0, 43686.3789886024], 
processed observation next is [1.0, 0.17391304347826086, 0.12557710064635264, 0.64, 0.0, 0.0, 0.6666666666666666, 0.509501219927615, 0.5350147564012173, 0.0, 1.0, 0.20803037613620193], 
reward next is 0.7920, 
noisyNet noise sample is [array([-2.0814657], dtype=float32), 1.0769194]. 
=============================================
[2019-04-04 02:03:17,069] A3C_AGENT_WORKER-Thread-7 INFO:Local step 177000, global step 2828677: loss 0.0908
[2019-04-04 02:03:17,070] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 177000, global step 2828677: learning rate 0.0001
[2019-04-04 02:03:18,537] A3C_AGENT_WORKER-Thread-19 INFO:Local step 177500, global step 2829411: loss 3.1983
[2019-04-04 02:03:18,539] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 177500, global step 2829412: learning rate 0.0001
[2019-04-04 02:03:19,710] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.5027415e-23 6.2681576e-17 2.8289072e-22 1.5252577e-18 9.0631243e-19
 2.0368540e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 02:03:19,710] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5232
[2019-04-04 02:03:19,720] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.666666666666667, 38.0, 83.33333333333333, 548.0, 26.0, 25.45929964331321, 0.4480269658523726, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4206000.0000, 
sim time next is 4206600.0000, 
raw observation next is [2.5, 38.5, 68.0, 550.0, 26.0, 25.47009645663128, 0.4506504901370003, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.5318559556786704, 0.385, 0.22666666666666666, 0.6077348066298343, 0.6666666666666666, 0.6225080380526066, 0.6502168300456668, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9258791], dtype=float32), -0.9440319]. 
=============================================
[2019-04-04 02:03:21,854] A3C_AGENT_WORKER-Thread-2 INFO:Local step 177000, global step 2830873: loss 0.0700
[2019-04-04 02:03:21,854] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 177000, global step 2830873: learning rate 0.0001
[2019-04-04 02:03:22,015] A3C_AGENT_WORKER-Thread-20 INFO:Local step 176500, global step 2830960: loss 1.1716
[2019-04-04 02:03:22,015] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 176500, global step 2830960: learning rate 0.0001
[2019-04-04 02:03:23,034] A3C_AGENT_WORKER-Thread-9 INFO:Local step 177000, global step 2831444: loss 0.0742
[2019-04-04 02:03:23,040] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 177000, global step 2831447: learning rate 0.0001
[2019-04-04 02:03:24,471] A3C_AGENT_WORKER-Thread-5 INFO:Local step 177000, global step 2832162: loss 0.0802
[2019-04-04 02:03:24,474] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 177000, global step 2832165: learning rate 0.0001
[2019-04-04 02:03:24,496] A3C_AGENT_WORKER-Thread-6 INFO:Local step 177000, global step 2832179: loss 0.0723
[2019-04-04 02:03:24,497] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 177000, global step 2832180: learning rate 0.0001
[2019-04-04 02:03:24,568] A3C_AGENT_WORKER-Thread-15 INFO:Local step 177000, global step 2832220: loss 0.0744
[2019-04-04 02:03:24,577] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 177000, global step 2832221: learning rate 0.0001
[2019-04-04 02:03:24,800] A3C_AGENT_WORKER-Thread-3 INFO:Local step 177000, global step 2832345: loss 0.0406
[2019-04-04 02:03:24,805] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 177000, global step 2832345: learning rate 0.0001
[2019-04-04 02:03:24,929] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.3448712e-24 8.4815733e-18 1.9707178e-23 1.4300778e-19 2.2400312e-19
 1.9284904e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:03:24,931] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3014
[2019-04-04 02:03:24,949] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.0, 52.0, 120.0, 847.0, 26.0, 25.26391760101993, 0.4155743766300264, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4278600.0000, 
sim time next is 4279200.0000, 
raw observation next is [7.0, 52.0, 131.3333333333333, 825.3333333333334, 26.0, 25.27423932470244, 0.4259804472766375, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.6565096952908588, 0.52, 0.4377777777777776, 0.9119705340699816, 0.6666666666666666, 0.6061866103918699, 0.6419934824255459, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.20447774], dtype=float32), 3.5299869]. 
=============================================
[2019-04-04 02:03:25,238] A3C_AGENT_WORKER-Thread-17 INFO:Local step 177500, global step 2832564: loss 2.7296
[2019-04-04 02:03:25,241] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 177500, global step 2832566: learning rate 0.0001
[2019-04-04 02:03:25,511] A3C_AGENT_WORKER-Thread-18 INFO:Local step 177000, global step 2832688: loss 0.0475
[2019-04-04 02:03:25,511] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 177000, global step 2832688: learning rate 0.0001
[2019-04-04 02:03:25,742] A3C_AGENT_WORKER-Thread-14 INFO:Local step 177000, global step 2832785: loss 0.0422
[2019-04-04 02:03:25,742] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 177000, global step 2832785: learning rate 0.0001
[2019-04-04 02:03:26,606] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.2332094e-24 1.2135258e-17 6.4917787e-23 1.6949392e-19 2.7796298e-19
 6.6043522e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:03:26,608] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2000
[2019-04-04 02:03:26,616] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.966666666666667, 57.5, 215.3333333333333, 495.6666666666666, 26.0, 25.41006333857722, 0.4457083011145771, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4284600.0000, 
sim time next is 4285200.0000, 
raw observation next is [6.933333333333334, 58.00000000000001, 222.1666666666667, 440.3333333333334, 26.0, 25.42454425480155, 0.4459594345116258, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.6546629732225301, 0.5800000000000001, 0.7405555555555557, 0.4865561694290977, 0.6666666666666666, 0.6187120212334625, 0.6486531448372086, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1170111], dtype=float32), -1.8301712]. 
=============================================
[2019-04-04 02:03:27,208] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.8655251e-23 6.2706246e-16 2.1008224e-22 2.3141988e-19 9.6422376e-19
 1.5154928e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 02:03:27,208] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1367
[2019-04-04 02:03:27,237] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.0, 54.0, 193.0, 367.5, 26.0, 25.62762144313424, 0.4094514685030994, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4269600.0000, 
sim time next is 4270200.0000, 
raw observation next is [4.166666666666667, 54.16666666666666, 196.6666666666667, 446.3333333333334, 26.0, 25.57067082129296, 0.4069441444758612, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.5780240073868884, 0.5416666666666665, 0.6555555555555557, 0.49318600368324134, 0.6666666666666666, 0.6308892351077467, 0.6356480481586204, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.75446135], dtype=float32), -0.5950638]. 
=============================================
[2019-04-04 02:03:27,582] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.89727173e-24 1.45026318e-17 3.82718304e-22 1.17999093e-18
 3.18449155e-19 1.10828465e-23 1.00000000e+00], sum to 1.0000
[2019-04-04 02:03:27,582] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4332
[2019-04-04 02:03:27,600] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 86.0, 112.5, 0.0, 26.0, 26.07259901300294, 0.4826875044145688, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4711200.0000, 
sim time next is 4711800.0000, 
raw observation next is [1.0, 86.0, 117.0, 0.0, 26.0, 25.93239575589477, 0.4584221030436155, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.4903047091412743, 0.86, 0.39, 0.0, 0.6666666666666666, 0.6610329796578975, 0.6528073676812052, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.17730327], dtype=float32), -0.5411614]. 
=============================================
[2019-04-04 02:03:28,860] A3C_AGENT_WORKER-Thread-4 INFO:Local step 177000, global step 2834374: loss 0.0384
[2019-04-04 02:03:28,860] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 177000, global step 2834374: learning rate 0.0001
[2019-04-04 02:03:29,519] A3C_AGENT_WORKER-Thread-8 INFO:Local step 177000, global step 2834696: loss 0.0387
[2019-04-04 02:03:29,521] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 177000, global step 2834696: learning rate 0.0001
[2019-04-04 02:03:32,336] A3C_AGENT_WORKER-Thread-16 INFO:Local step 177000, global step 2836170: loss 0.0626
[2019-04-04 02:03:32,337] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 177000, global step 2836171: learning rate 0.0001
[2019-04-04 02:03:33,129] A3C_AGENT_WORKER-Thread-7 INFO:Local step 177500, global step 2836605: loss 3.1322
[2019-04-04 02:03:33,130] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 177500, global step 2836605: learning rate 0.0001
[2019-04-04 02:03:33,338] A3C_AGENT_WORKER-Thread-10 INFO:Local step 177000, global step 2836717: loss 0.0509
[2019-04-04 02:03:33,348] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 177000, global step 2836717: learning rate 0.0001
[2019-04-04 02:03:34,269] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.0223959e-25 2.1008107e-18 2.4214089e-23 3.3630956e-19 1.6257286e-19
 3.4078204e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:03:34,271] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3558
[2019-04-04 02:03:34,317] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 24.98648938545873, 0.4709814744012324, 1.0, 1.0, 46649.74135286372], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4476000.0000, 
sim time next is 4476600.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.04793858260687, 0.4736139093264727, 1.0, 1.0, 18711.31836746487], 
processed observation next is [1.0, 0.8260869565217391, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5873282152172393, 0.6578713031088242, 1.0, 1.0, 0.089101516035547], 
reward next is 0.9109, 
noisyNet noise sample is [array([0.64774936], dtype=float32), 0.34711635]. 
=============================================
[2019-04-04 02:03:35,890] A3C_AGENT_WORKER-Thread-19 INFO:Local step 178000, global step 2838020: loss 0.1417
[2019-04-04 02:03:35,891] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 178000, global step 2838020: learning rate 0.0001
[2019-04-04 02:03:37,884] A3C_AGENT_WORKER-Thread-2 INFO:Local step 177500, global step 2838974: loss 2.8070
[2019-04-04 02:03:37,885] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 177500, global step 2838974: learning rate 0.0001
[2019-04-04 02:03:37,964] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.9213229e-24 7.1417140e-17 1.6013407e-21 1.7168963e-18 4.6675957e-18
 7.2779670e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 02:03:37,964] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3129
[2019-04-04 02:03:37,981] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.7, 69.33333333333334, 0.0, 0.0, 26.0, 25.55575717839561, 0.3640530929360776, 0.0, 1.0, 39620.94132015212], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4336800.0000, 
sim time next is 4337400.0000, 
raw observation next is [3.65, 69.16666666666666, 0.0, 0.0, 26.0, 25.45521983551161, 0.3651030756638449, 0.0, 1.0, 89231.09638617112], 
processed observation next is [1.0, 0.17391304347826086, 0.5637119113573408, 0.6916666666666665, 0.0, 0.0, 0.6666666666666666, 0.6212683196259675, 0.6217010252212817, 0.0, 1.0, 0.42490998279129105], 
reward next is 0.5751, 
noisyNet noise sample is [array([0.87018454], dtype=float32), 0.0975166]. 
=============================================
[2019-04-04 02:03:38,413] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.7777678e-24 4.6645363e-17 3.9595612e-22 1.0597767e-18 6.5945258e-19
 3.7804436e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 02:03:38,413] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4260
[2019-04-04 02:03:38,416] A3C_AGENT_WORKER-Thread-9 INFO:Local step 177500, global step 2839221: loss 2.6774
[2019-04-04 02:03:38,425] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 177500, global step 2839223: learning rate 0.0001
[2019-04-04 02:03:38,486] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.9, 72.0, 0.0, 0.0, 26.0, 25.52360645142301, 0.4339486057293669, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4519800.0000, 
sim time next is 4520400.0000, 
raw observation next is [-0.8666666666666667, 72.33333333333333, 18.5, 11.0, 26.0, 25.59855651400763, 0.4296890230639438, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.4385964912280702, 0.7233333333333333, 0.06166666666666667, 0.012154696132596685, 0.6666666666666666, 0.6332130428339692, 0.643229674354648, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3952509], dtype=float32), 0.9141048]. 
=============================================
[2019-04-04 02:03:38,911] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.9765868e-24 3.1843870e-16 2.6013043e-22 1.9712868e-18 8.5577485e-19
 1.1914677e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 02:03:38,914] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5995
[2019-04-04 02:03:38,925] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.666666666666667, 44.66666666666667, 223.8333333333333, 382.0, 26.0, 25.06499623130399, 0.3676451856906753, 0.0, 1.0, 18689.92216452699], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4891200.0000, 
sim time next is 4891800.0000, 
raw observation next is [2.833333333333333, 44.83333333333333, 211.6666666666667, 390.0, 26.0, 25.06576350213476, 0.3699495572481308, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.541089566020314, 0.4483333333333333, 0.7055555555555557, 0.430939226519337, 0.6666666666666666, 0.5888136251778967, 0.6233165190827102, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.08831082], dtype=float32), 0.278726]. 
=============================================
[2019-04-04 02:03:39,993] A3C_AGENT_WORKER-Thread-6 INFO:Local step 177500, global step 2839952: loss 3.0998
[2019-04-04 02:03:39,998] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 177500, global step 2839953: learning rate 0.0001
[2019-04-04 02:03:40,088] A3C_AGENT_WORKER-Thread-15 INFO:Local step 177500, global step 2839999: loss 3.0976
[2019-04-04 02:03:40,089] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 177500, global step 2839999: learning rate 0.0001
[2019-04-04 02:03:40,162] A3C_AGENT_WORKER-Thread-3 INFO:Local step 177500, global step 2840045: loss 3.1745
[2019-04-04 02:03:40,163] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 177500, global step 2840045: learning rate 0.0001
[2019-04-04 02:03:40,242] A3C_AGENT_WORKER-Thread-5 INFO:Local step 177500, global step 2840088: loss 3.1838
[2019-04-04 02:03:40,245] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 177500, global step 2840088: learning rate 0.0001
[2019-04-04 02:03:40,317] A3C_AGENT_WORKER-Thread-20 INFO:Local step 177000, global step 2840134: loss 0.0644
[2019-04-04 02:03:40,320] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 177000, global step 2840134: learning rate 0.0001
[2019-04-04 02:03:40,501] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.6267347e-25 1.0041159e-17 1.2248841e-23 5.7080852e-20 1.1573482e-19
 1.5754890e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:03:40,503] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3011
[2019-04-04 02:03:40,519] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 25.53289610481542, 0.451113378670124, 1.0, 1.0, 19875.61658156659], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4557600.0000, 
sim time next is 4558200.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 25.14977893791432, 0.4331085106042901, 1.0, 1.0, 60575.36979527318], 
processed observation next is [1.0, 0.782608695652174, 0.518005540166205, 0.52, 0.0, 0.0, 0.6666666666666666, 0.59581491149286, 0.6443695035347634, 1.0, 1.0, 0.28845414188225327], 
reward next is 0.7115, 
noisyNet noise sample is [array([-0.7915447], dtype=float32), -0.9552813]. 
=============================================
[2019-04-04 02:03:41,116] A3C_AGENT_WORKER-Thread-18 INFO:Local step 177500, global step 2840526: loss 2.9085
[2019-04-04 02:03:41,116] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 177500, global step 2840526: learning rate 0.0001
[2019-04-04 02:03:41,287] A3C_AGENT_WORKER-Thread-14 INFO:Local step 177500, global step 2840606: loss 2.9056
[2019-04-04 02:03:41,288] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 177500, global step 2840606: learning rate 0.0001
[2019-04-04 02:03:42,061] A3C_AGENT_WORKER-Thread-17 INFO:Local step 178000, global step 2840998: loss 0.1037
[2019-04-04 02:03:42,072] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 178000, global step 2841000: learning rate 0.0001
[2019-04-04 02:03:44,203] A3C_AGENT_WORKER-Thread-4 INFO:Local step 177500, global step 2841997: loss 2.9335
[2019-04-04 02:03:44,204] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 177500, global step 2841997: learning rate 0.0001
[2019-04-04 02:03:44,905] A3C_AGENT_WORKER-Thread-8 INFO:Local step 177500, global step 2842353: loss 2.9547
[2019-04-04 02:03:44,907] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 177500, global step 2842353: learning rate 0.0001
[2019-04-04 02:03:45,138] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.7722420e-24 2.0213089e-18 2.6489283e-23 5.3396153e-19 1.7411342e-19
 3.3128660e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:03:45,140] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9058
[2019-04-04 02:03:45,161] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.0, 49.0, 0.0, 0.0, 26.0, 26.26706104113046, 0.6962123187442159, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4644000.0000, 
sim time next is 4644600.0000, 
raw observation next is [3.833333333333333, 49.66666666666667, 0.0, 0.0, 26.0, 26.31825924123196, 0.7001147397621118, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.5687903970452447, 0.4966666666666667, 0.0, 0.0, 0.6666666666666666, 0.6931882701026634, 0.7333715799207039, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.5395284], dtype=float32), -0.614954]. 
=============================================
[2019-04-04 02:03:47,942] A3C_AGENT_WORKER-Thread-16 INFO:Local step 177500, global step 2843858: loss 3.1955
[2019-04-04 02:03:47,943] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 177500, global step 2843859: learning rate 0.0001
[2019-04-04 02:03:49,232] A3C_AGENT_WORKER-Thread-10 INFO:Local step 177500, global step 2844469: loss 3.4158
[2019-04-04 02:03:49,234] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 177500, global step 2844469: learning rate 0.0001
[2019-04-04 02:03:50,230] A3C_AGENT_WORKER-Thread-7 INFO:Local step 178000, global step 2844960: loss 0.0260
[2019-04-04 02:03:50,246] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 178000, global step 2844964: learning rate 0.0001
[2019-04-04 02:03:50,687] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:03:50,688] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:03:50,694] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run22
[2019-04-04 02:03:52,668] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.5543851e-25 3.8233218e-19 6.7554017e-24 1.7635205e-19 4.2604244e-20
 4.5811754e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:03:52,668] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2632
[2019-04-04 02:03:52,681] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.85, 49.5, 203.0, 599.0, 26.0, 26.62712656256016, 0.7941416542527157, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4631400.0000, 
sim time next is 4632000.0000, 
raw observation next is [4.9, 49.66666666666666, 201.6666666666667, 520.6666666666666, 26.0, 26.97730287042286, 0.8264752619748582, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5983379501385043, 0.4966666666666666, 0.6722222222222224, 0.5753222836095764, 0.6666666666666666, 0.7481085725352384, 0.7754917539916194, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.53820354], dtype=float32), -1.06157]. 
=============================================
[2019-04-04 02:03:52,691] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[87.99541]
 [87.92412]
 [88.15735]
 [88.15253]
 [88.24707]], R is [[87.48942566]
 [87.61453247]
 [87.73838806]
 [87.86100769]
 [87.98239899]].
[2019-04-04 02:03:53,337] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.4028208e-24 2.4625428e-17 2.7508284e-22 1.7514538e-18 7.3061048e-19
 1.4687701e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:03:53,337] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5290
[2019-04-04 02:03:53,349] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.833333333333333, 49.66666666666667, 0.0, 0.0, 26.0, 26.31825924123196, 0.7001147397621118, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4644600.0000, 
sim time next is 4645200.0000, 
raw observation next is [3.666666666666667, 50.33333333333334, 0.0, 0.0, 26.0, 26.38614780080945, 0.7075970380197042, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.564173591874423, 0.5033333333333334, 0.0, 0.0, 0.6666666666666666, 0.6988456500674541, 0.7358656793399013, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.26557446], dtype=float32), -0.8744529]. 
=============================================
[2019-04-04 02:03:55,263] A3C_AGENT_WORKER-Thread-9 INFO:Local step 178000, global step 2847026: loss 0.0109
[2019-04-04 02:03:55,264] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 178000, global step 2847027: learning rate 0.0001
[2019-04-04 02:03:55,290] A3C_AGENT_WORKER-Thread-2 INFO:Local step 178000, global step 2847043: loss 0.0111
[2019-04-04 02:03:55,292] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 178000, global step 2847044: learning rate 0.0001
[2019-04-04 02:03:56,159] A3C_AGENT_WORKER-Thread-20 INFO:Local step 177500, global step 2847418: loss 3.2778
[2019-04-04 02:03:56,160] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 177500, global step 2847418: learning rate 0.0001
[2019-04-04 02:03:56,230] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.3297861e-24 5.4189095e-17 5.6895789e-22 7.4591911e-19 1.0937262e-18
 3.2984292e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:03:56,231] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6616
[2019-04-04 02:03:56,253] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 57.0, 0.0, 0.0, 26.0, 25.69145984037336, 0.4959008350120493, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4671000.0000, 
sim time next is 4671600.0000, 
raw observation next is [2.0, 58.66666666666666, 0.0, 0.0, 26.0, 25.62326620599688, 0.4820320350479985, 0.0, 1.0, 21528.19719528709], 
processed observation next is [1.0, 0.043478260869565216, 0.518005540166205, 0.5866666666666666, 0.0, 0.0, 0.6666666666666666, 0.6352721838330734, 0.6606773450159995, 0.0, 1.0, 0.10251522473946234], 
reward next is 0.8975, 
noisyNet noise sample is [array([2.1875894], dtype=float32), 0.44152597]. 
=============================================
[2019-04-04 02:03:56,492] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:03:56,492] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:03:56,495] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run22
[2019-04-04 02:03:56,961] A3C_AGENT_WORKER-Thread-6 INFO:Local step 178000, global step 2847740: loss 0.0088
[2019-04-04 02:03:56,964] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 178000, global step 2847741: learning rate 0.0001
[2019-04-04 02:03:57,220] A3C_AGENT_WORKER-Thread-3 INFO:Local step 178000, global step 2847843: loss 0.0007
[2019-04-04 02:03:57,222] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 178000, global step 2847843: learning rate 0.0001
[2019-04-04 02:03:57,267] A3C_AGENT_WORKER-Thread-15 INFO:Local step 178000, global step 2847866: loss 0.0022
[2019-04-04 02:03:57,267] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 178000, global step 2847866: learning rate 0.0001
[2019-04-04 02:03:57,413] A3C_AGENT_WORKER-Thread-5 INFO:Local step 178000, global step 2847930: loss 0.0003
[2019-04-04 02:03:57,413] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 178000, global step 2847930: learning rate 0.0001
[2019-04-04 02:03:58,556] A3C_AGENT_WORKER-Thread-14 INFO:Local step 178000, global step 2848397: loss 0.0001
[2019-04-04 02:03:58,558] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 178000, global step 2848397: learning rate 0.0001
[2019-04-04 02:03:58,703] A3C_AGENT_WORKER-Thread-18 INFO:Local step 178000, global step 2848443: loss 0.0000
[2019-04-04 02:03:58,704] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 178000, global step 2848443: learning rate 0.0001
[2019-04-04 02:03:59,597] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0134832e-23 5.4877031e-17 3.2548974e-22 9.5250323e-19 1.7496107e-18
 1.7952354e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 02:03:59,599] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9735
[2019-04-04 02:03:59,647] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.6666666666666666, 39.66666666666666, 0.0, 0.0, 26.0, 25.39232887083631, 0.3498560467282801, 0.0, 1.0, 29936.96237721269], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4923600.0000, 
sim time next is 4924200.0000, 
raw observation next is [0.8333333333333334, 39.83333333333334, 0.0, 0.0, 26.0, 25.38381406927507, 0.3499340325211842, 0.0, 1.0, 37921.74697241068], 
processed observation next is [0.0, 1.0, 0.4856879039704525, 0.39833333333333343, 0.0, 0.0, 0.6666666666666666, 0.6153178391062557, 0.6166446775070614, 0.0, 1.0, 0.1805797474876699], 
reward next is 0.8194, 
noisyNet noise sample is [array([-1.7279078], dtype=float32), -0.4951064]. 
=============================================
[2019-04-04 02:04:01,597] A3C_AGENT_WORKER-Thread-4 INFO:Local step 178000, global step 2849560: loss 0.0007
[2019-04-04 02:04:01,606] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 178000, global step 2849561: learning rate 0.0001
[2019-04-04 02:04:02,196] A3C_AGENT_WORKER-Thread-8 INFO:Local step 178000, global step 2849814: loss 0.0036
[2019-04-04 02:04:02,197] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 178000, global step 2849815: learning rate 0.0001
[2019-04-04 02:04:05,181] A3C_AGENT_WORKER-Thread-16 INFO:Local step 178000, global step 2851237: loss 0.0037
[2019-04-04 02:04:05,182] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 178000, global step 2851237: learning rate 0.0001
[2019-04-04 02:04:05,312] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:04:05,313] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:04:05,337] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run22
[2019-04-04 02:04:06,483] A3C_AGENT_WORKER-Thread-10 INFO:Local step 178000, global step 2851790: loss 0.0085
[2019-04-04 02:04:06,487] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 178000, global step 2851793: learning rate 0.0001
[2019-04-04 02:04:07,071] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.2851283e-22 2.9125774e-15 1.2393151e-20 1.7694725e-17 2.2400676e-17
 3.4146201e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 02:04:07,071] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6157
[2019-04-04 02:04:07,083] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.6965601e-24 3.5011677e-17 1.5620029e-22 9.3556361e-19 4.2647520e-19
 5.3208546e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:04:07,084] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9061
[2019-04-04 02:04:07,098] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.5, 65.5, 0.0, 0.0, 26.0, 24.74776645695325, 0.209420981009707, 0.0, 1.0, 39537.33482467769], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4858200.0000, 
sim time next is 4858800.0000, 
raw observation next is [-3.333333333333333, 63.66666666666667, 0.0, 0.0, 26.0, 24.70952359415261, 0.2034980601257555, 0.0, 1.0, 39562.05470774141], 
processed observation next is [0.0, 0.21739130434782608, 0.37026777469990774, 0.6366666666666667, 0.0, 0.0, 0.6666666666666666, 0.5591269661793842, 0.5678326867085851, 0.0, 1.0, 0.18839073670353051], 
reward next is 0.8116, 
noisyNet noise sample is [array([-0.4658122], dtype=float32), -0.51037633]. 
=============================================
[2019-04-04 02:04:07,130] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.1, 61.0, 130.0, 603.0, 26.0, 25.64806811570702, 0.3941926181029398, 1.0, 1.0, 77435.13454932184], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 131400.0000, 
sim time next is 132000.0000, 
raw observation next is [-8.0, 61.0, 131.5, 583.1666666666666, 26.0, 25.53883914126589, 0.4026493660184749, 1.0, 1.0, 57167.31579053279], 
processed observation next is [1.0, 0.5217391304347826, 0.24099722991689754, 0.61, 0.43833333333333335, 0.6443830570902394, 0.6666666666666666, 0.6282365951054908, 0.6342164553394917, 1.0, 1.0, 0.2722253132882514], 
reward next is 0.7278, 
noisyNet noise sample is [array([0.3477646], dtype=float32), -0.060679123]. 
=============================================
[2019-04-04 02:04:07,138] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[83.056305]
 [83.086426]
 [83.58614 ]
 [84.078804]
 [84.27046 ]], R is [[82.51089478]
 [82.31704712]
 [82.49388123]
 [82.66894531]
 [82.61551666]].
[2019-04-04 02:04:09,965] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:04:09,965] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:04:09,979] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run22
[2019-04-04 02:04:10,155] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:04:10,156] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:04:10,169] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run22
[2019-04-04 02:04:11,720] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:04:11,720] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:04:11,744] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run22
[2019-04-04 02:04:11,884] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:04:11,885] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:04:11,889] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run22
[2019-04-04 02:04:11,968] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:04:11,968] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:04:11,981] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run22
[2019-04-04 02:04:12,218] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:04:12,219] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:04:12,223] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run22
[2019-04-04 02:04:12,505] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:04:12,505] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:04:12,508] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run22
[2019-04-04 02:04:12,607] A3C_AGENT_WORKER-Thread-20 INFO:Local step 178000, global step 2854254: loss 0.0107
[2019-04-04 02:04:12,608] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 178000, global step 2854254: learning rate 0.0001
[2019-04-04 02:04:12,705] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:04:12,705] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:04:12,711] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run22
[2019-04-04 02:04:15,694] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:04:15,694] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:04:15,708] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run22
[2019-04-04 02:04:16,444] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:04:16,444] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:04:16,448] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run22
[2019-04-04 02:04:21,410] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:04:21,410] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:04:21,413] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run22
[2019-04-04 02:04:21,761] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:04:21,761] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:04:21,764] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run22
[2019-04-04 02:04:28,458] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:04:28,458] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:04:28,464] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run22
[2019-04-04 02:04:29,308] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [6.8013012e-23 1.8291978e-16 2.9586831e-21 3.7852728e-18 2.5729362e-18
 1.9962095e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 02:04:29,309] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.1793
[2019-04-04 02:04:29,375] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.683333333333334, 85.33333333333334, 12.0, 0.0, 26.0, 24.54824014586847, 0.1890691172121965, 0.0, 1.0, 34829.88553126116], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 60600.0000, 
sim time next is 61200.0000, 
raw observation next is [5.5, 86.0, 0.0, 0.0, 26.0, 24.54659692194853, 0.1947913318725229, 0.0, 1.0, 42206.84572453996], 
processed observation next is [0.0, 0.7391304347826086, 0.6149584487534627, 0.86, 0.0, 0.0, 0.6666666666666666, 0.545549743495711, 0.5649304439575077, 0.0, 1.0, 0.2009849796406665], 
reward next is 0.7990, 
noisyNet noise sample is [array([1.0284764], dtype=float32), 0.56299055]. 
=============================================
[2019-04-04 02:04:52,179] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0267097e-23 2.4666125e-18 1.9143535e-22 7.0767018e-19 4.2244830e-19
 2.5807137e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:04:52,179] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9793
[2019-04-04 02:04:52,278] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.33333333333333, 54.33333333333334, 0.0, 0.0, 26.0, 25.66050216195514, 0.4144404830469139, 1.0, 1.0, 78153.48783110396], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 322800.0000, 
sim time next is 323400.0000, 
raw observation next is [-11.51666666666667, 55.66666666666666, 0.0, 0.0, 26.0, 25.67947911762393, 0.4185488529477538, 1.0, 1.0, 66655.30825625523], 
processed observation next is [1.0, 0.7391304347826086, 0.14358264081255764, 0.5566666666666665, 0.0, 0.0, 0.6666666666666666, 0.6399565931353276, 0.6395162843159179, 1.0, 1.0, 0.3174062297916916], 
reward next is 0.6826, 
noisyNet noise sample is [array([1.4446801], dtype=float32), 1.3056059]. 
=============================================
[2019-04-04 02:04:57,749] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.3575524e-22 1.0648316e-15 7.1674909e-21 4.6100322e-18 4.9803759e-18
 3.7771301e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 02:04:57,750] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0490
[2019-04-04 02:04:57,864] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-16.28333333333333, 78.5, 0.0, 0.0, 26.0, 21.58261355467166, -0.5246877354090199, 0.0, 1.0, 49481.77494666538], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 371400.0000, 
sim time next is 372000.0000, 
raw observation next is [-16.36666666666667, 79.0, 0.0, 0.0, 26.0, 21.49329037909963, -0.4405880206216344, 1.0, 1.0, 202242.6109366239], 
processed observation next is [1.0, 0.30434782608695654, 0.009233610341643453, 0.79, 0.0, 0.0, 0.6666666666666666, 0.2911075315916358, 0.3531373264594552, 1.0, 1.0, 0.9630600520791615], 
reward next is 0.0369, 
noisyNet noise sample is [array([0.75095946], dtype=float32), -1.5880252]. 
=============================================
[2019-04-04 02:04:57,870] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[78.16667]
 [78.19694]
 [78.18202]
 [78.19716]
 [78.21598]], R is [[79.93251038]
 [79.89756012]
 [79.86347198]
 [79.83015442]
 [79.79780579]].
[2019-04-04 02:05:06,899] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.5099466e-24 1.2419856e-18 2.4813001e-23 2.5319631e-19 1.1529285e-19
 3.6277721e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:05:06,899] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.7608
[2019-04-04 02:05:07,051] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 37.66666666666667, 26.33333333333333, 504.3333333333333, 26.0, 25.48937205171736, 0.390125224903618, 1.0, 1.0, 188965.6461299974], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 403800.0000, 
sim time next is 404400.0000, 
raw observation next is [-8.900000000000002, 37.33333333333334, 23.66666666666666, 453.6666666666667, 26.0, 25.60099869448844, 0.448602468048809, 1.0, 1.0, 116910.4217151053], 
processed observation next is [1.0, 0.6956521739130435, 0.2160664819944598, 0.3733333333333334, 0.07888888888888887, 0.5012891344383057, 0.6666666666666666, 0.6334165578740366, 0.6495341560162696, 1.0, 1.0, 0.5567162938814538], 
reward next is 0.4433, 
noisyNet noise sample is [array([-1.1100864], dtype=float32), -0.51531243]. 
=============================================
[2019-04-04 02:05:11,758] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.5067482e-23 8.5093836e-17 1.3522966e-21 6.1428355e-19 1.1260669e-18
 1.1247424e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 02:05:11,758] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3025
[2019-04-04 02:05:11,844] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 87.0, 0.0, 0.0, 26.0, 24.96836698320608, 0.2918630260362654, 0.0, 1.0, 125930.8805881882], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 588000.0000, 
sim time next is 588600.0000, 
raw observation next is [-2.8, 87.0, 0.0, 0.0, 26.0, 24.97592635739803, 0.3012205881477752, 0.0, 1.0, 79433.76675624374], 
processed observation next is [0.0, 0.8260869565217391, 0.38504155124653744, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5813271964498359, 0.600406862715925, 0.0, 1.0, 0.37825603217258924], 
reward next is 0.6217, 
noisyNet noise sample is [array([-0.17705928], dtype=float32), -1.2523661]. 
=============================================
[2019-04-04 02:05:12,024] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3535486e-24 1.9471763e-18 2.1885873e-23 8.2315347e-20 5.1859623e-20
 9.5674955e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:05:12,024] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8426
[2019-04-04 02:05:12,067] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 34.0, 38.0, 0.0, 26.0, 25.64885082261351, 0.2701266552510019, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 489600.0000, 
sim time next is 490200.0000, 
raw observation next is [1.1, 35.5, 32.0, 0.0, 26.0, 25.63991937534581, 0.1645693290090091, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.49307479224376743, 0.355, 0.10666666666666667, 0.0, 0.6666666666666666, 0.6366599479454841, 0.5548564430030031, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6782298], dtype=float32), 0.73788255]. 
=============================================
[2019-04-04 02:05:13,869] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5470230e-23 5.3043364e-17 1.1062583e-21 1.2377856e-18 1.9290849e-18
 4.2690240e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 02:05:13,936] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0775
[2019-04-04 02:05:13,977] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.8, 71.5, 0.0, 0.0, 26.0, 24.04356254960178, 0.07067700496739582, 0.0, 1.0, 41523.44196774241], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 785400.0000, 
sim time next is 786000.0000, 
raw observation next is [-7.8, 72.0, 0.0, 0.0, 26.0, 23.99171389074302, 0.06186535718369105, 0.0, 1.0, 41501.46920167027], 
processed observation next is [1.0, 0.08695652173913043, 0.24653739612188366, 0.72, 0.0, 0.0, 0.6666666666666666, 0.4993094908952518, 0.5206217857278971, 0.0, 1.0, 0.19762604381747748], 
reward next is 0.8024, 
noisyNet noise sample is [array([-1.6175466], dtype=float32), 0.18451306]. 
=============================================
[2019-04-04 02:05:14,033] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[79.96488 ]
 [79.98754 ]
 [80.072815]
 [80.15149 ]
 [80.196465]], R is [[79.94570923]
 [79.94852448]
 [79.95122528]
 [79.95389557]
 [79.95652771]].
[2019-04-04 02:05:27,713] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.0128239e-24 2.9823382e-17 1.1081161e-22 6.1578718e-20 5.8880887e-20
 2.2949851e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 02:05:27,713] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4326
[2019-04-04 02:05:27,806] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 72.33333333333334, 57.66666666666666, 17.0, 26.0, 24.85403966442264, 0.2351790200958371, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 636600.0000, 
sim time next is 637200.0000, 
raw observation next is [-3.9, 71.0, 77.0, 25.5, 26.0, 25.13008609069737, 0.2543150253935838, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.3545706371191136, 0.71, 0.25666666666666665, 0.0281767955801105, 0.6666666666666666, 0.5941738408914476, 0.5847716751311945, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6978242], dtype=float32), 0.31092617]. 
=============================================
[2019-04-04 02:05:31,142] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.8452545e-23 6.6010257e-17 1.0919195e-21 1.2753294e-18 1.9416652e-18
 3.0470491e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 02:05:31,142] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9488
[2019-04-04 02:05:31,189] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.2, 83.0, 104.5, 138.6666666666667, 26.0, 24.97345992447614, 0.3237237395282644, 0.0, 1.0, 47249.20296949052], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 573600.0000, 
sim time next is 574200.0000, 
raw observation next is [-1.2, 83.0, 100.0, 73.0, 26.0, 24.96331982358114, 0.3250365299198316, 0.0, 1.0, 45053.95935703665], 
processed observation next is [0.0, 0.6521739130434783, 0.42936288088642666, 0.83, 0.3333333333333333, 0.08066298342541436, 0.6666666666666666, 0.5802766519650951, 0.6083455099732772, 0.0, 1.0, 0.21454266360493643], 
reward next is 0.7855, 
noisyNet noise sample is [array([0.8461644], dtype=float32), 0.4809784]. 
=============================================
[2019-04-04 02:05:33,775] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.4315578e-26 5.7215607e-19 4.5052722e-24 3.8908815e-21 1.4495253e-20
 3.4187691e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:05:33,775] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5057
[2019-04-04 02:05:33,864] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.9000000000000001, 92.0, 0.0, 0.0, 26.0, 24.96901472659698, 0.2432995229236881, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 499200.0000, 
sim time next is 499800.0000, 
raw observation next is [1.0, 94.0, 0.0, 0.0, 26.0, 25.07025465221489, 0.2356194627595504, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.4903047091412743, 0.94, 0.0, 0.0, 0.6666666666666666, 0.5891878876845741, 0.5785398209198501, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.426343], dtype=float32), -1.9208347]. 
=============================================
[2019-04-04 02:05:35,404] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.9433911e-24 2.2962806e-16 6.9837672e-22 1.1441839e-18 2.0002601e-19
 3.9121603e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 02:05:35,404] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5768
[2019-04-04 02:05:35,424] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 75.0, 0.0, 0.0, 26.0, 24.32545368468866, 0.05325961073746097, 0.0, 1.0, 41610.5220766743], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 704400.0000, 
sim time next is 705000.0000, 
raw observation next is [-2.9, 75.0, 0.0, 0.0, 26.0, 24.3152449616222, 0.04604373455308772, 0.0, 1.0, 41637.81335645763], 
processed observation next is [1.0, 0.13043478260869565, 0.38227146814404434, 0.75, 0.0, 0.0, 0.6666666666666666, 0.5262704134685166, 0.5153479115176959, 0.0, 1.0, 0.19827530169741728], 
reward next is 0.8017, 
noisyNet noise sample is [array([-1.0278716], dtype=float32), -0.10274149]. 
=============================================
[2019-04-04 02:05:35,509] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[82.82049]
 [82.80394]
 [82.78323]
 [82.78261]
 [82.72309]], R is [[82.81835175]
 [82.79202271]
 [82.76613617]
 [82.74060822]
 [82.71555328]].
[2019-04-04 02:05:48,143] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.2459391e-26 1.2398791e-18 8.7137055e-24 2.8105597e-20 1.9307100e-20
 2.7570435e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:05:48,146] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2199
[2019-04-04 02:05:48,198] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 71.0, 108.1666666666667, 0.0, 26.0, 25.64265652043524, 0.3114539408888212, 1.0, 1.0, 27001.89851153543], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 819600.0000, 
sim time next is 820200.0000, 
raw observation next is [-4.5, 71.0, 106.3333333333333, 0.0, 26.0, 25.62307371182087, 0.3146522682969999, 1.0, 1.0, 27352.33112828393], 
processed observation next is [1.0, 0.4782608695652174, 0.3379501385041552, 0.71, 0.35444444444444434, 0.0, 0.6666666666666666, 0.635256142651739, 0.6048840894323333, 1.0, 1.0, 0.1302491958489711], 
reward next is 0.8698, 
noisyNet noise sample is [array([-1.1158323], dtype=float32), 0.83440894]. 
=============================================
[2019-04-04 02:05:51,416] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [7.7494170e-25 1.5808111e-17 1.5954794e-23 3.1092265e-20 4.2931514e-20
 9.6949506e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:05:51,416] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8688
[2019-04-04 02:05:51,495] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 9.666666666666664, 0.0, 26.0, 25.06802907566821, 0.239358943512335, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 892200.0000, 
sim time next is 892800.0000, 
raw observation next is [0.0, 72.0, 14.5, 0.0, 26.0, 24.98244285485567, 0.2257670300949691, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.46260387811634357, 0.72, 0.04833333333333333, 0.0, 0.6666666666666666, 0.5818702379046391, 0.575255676698323, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.14566585], dtype=float32), 0.9367233]. 
=============================================
[2019-04-04 02:06:47,442] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.0694452e-25 2.8176261e-18 4.4626830e-23 8.7810276e-20 2.9500631e-20
 1.7997394e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:06:47,449] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7670
[2019-04-04 02:06:47,504] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 100.0, 0.0, 0.0, 26.0, 25.59916014823041, 0.4606866331560492, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1496400.0000, 
sim time next is 1497000.0000, 
raw observation next is [1.1, 100.0, 5.999999999999998, 0.0, 26.0, 25.52699937654192, 0.4523950947032175, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.49307479224376743, 1.0, 0.019999999999999993, 0.0, 0.6666666666666666, 0.62724994804516, 0.6507983649010725, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4255458], dtype=float32), -0.96903837]. 
=============================================
[2019-04-04 02:06:47,517] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[85.65623 ]
 [84.53707 ]
 [85.375404]
 [83.71366 ]
 [83.76663 ]], R is [[86.40129852]
 [86.53728485]
 [86.67191315]
 [86.80519104]
 [86.54536438]].
[2019-04-04 02:06:53,093] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.8364882e-25 5.2410164e-18 1.3796825e-23 1.9635770e-20 4.9790602e-20
 7.4083515e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:06:53,093] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0241
[2019-04-04 02:06:53,123] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.53333333333333, 100.0, 29.66666666666666, 0.0, 26.0, 24.61589366270319, 0.4214415712908554, 0.0, 1.0, 19685.54749608948], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1267800.0000, 
sim time next is 1268400.0000, 
raw observation next is [13.26666666666667, 100.0, 24.33333333333333, 0.0, 26.0, 24.607532485985, 0.4210896789086691, 0.0, 1.0, 24169.16340516739], 
processed observation next is [0.0, 0.6956521739130435, 0.8301015697137583, 1.0, 0.08111111111111109, 0.0, 0.6666666666666666, 0.5506277071654168, 0.6403632263028897, 0.0, 1.0, 0.1150912543103209], 
reward next is 0.8849, 
noisyNet noise sample is [array([2.3037882], dtype=float32), 0.67692083]. 
=============================================
[2019-04-04 02:06:56,201] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.5626225e-25 9.0134116e-18 3.4460483e-23 6.1083896e-20 5.8266874e-20
 1.4974876e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:06:56,205] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.2185
[2019-04-04 02:06:56,235] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.3, 92.0, 15.5, 0.0, 26.0, 25.49241399389967, 0.4995711225743403, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1670400.0000, 
sim time next is 1671000.0000, 
raw observation next is [3.116666666666667, 92.0, 20.33333333333334, 0.0, 26.0, 25.46480188223064, 0.4853191412880473, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.5489381348107111, 0.92, 0.0677777777777778, 0.0, 0.6666666666666666, 0.6220668235192198, 0.6617730470960158, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2875878], dtype=float32), -0.72183716]. 
=============================================
[2019-04-04 02:06:56,375] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[86.346466]
 [86.48117 ]
 [86.38812 ]
 [86.185616]
 [85.42532 ]], R is [[86.29837036]
 [86.43538666]
 [86.57103729]
 [86.7053299 ]
 [86.83827972]].
[2019-04-04 02:07:08,295] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.5442501e-25 1.5093530e-17 2.4057398e-22 1.0926540e-19 2.9716148e-19
 7.7739955e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:07:08,295] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4682
[2019-04-04 02:07:08,320] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.2, 96.0, 0.0, 0.0, 26.0, 25.37514655959959, 0.4504109963282244, 0.0, 1.0, 35118.37753114508], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1490400.0000, 
sim time next is 1491000.0000, 
raw observation next is [2.016666666666667, 96.66666666666666, 0.0, 0.0, 26.0, 25.37757542512059, 0.4581521658343639, 0.0, 1.0, 34730.30055740014], 
processed observation next is [1.0, 0.2608695652173913, 0.5184672206832872, 0.9666666666666666, 0.0, 0.0, 0.6666666666666666, 0.6147979520933825, 0.6527173886114547, 0.0, 1.0, 0.16538238360666735], 
reward next is 0.8346, 
noisyNet noise sample is [array([0.84217507], dtype=float32), -2.868555]. 
=============================================
[2019-04-04 02:07:08,349] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[83.565125]
 [83.58084 ]
 [83.58034 ]
 [83.59685 ]
 [83.6246  ]], R is [[83.53144836]
 [83.52890778]
 [83.51741028]
 [83.50640869]
 [83.49563599]].
[2019-04-04 02:07:10,970] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.2804488e-26 2.1148014e-18 1.0780204e-23 2.9136145e-20 2.3641660e-20
 3.4853359e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:07:10,970] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3995
[2019-04-04 02:07:11,014] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.8, 90.66666666666667, 0.0, 0.0, 26.0, 25.67029033007471, 0.5898770897643041, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1644000.0000, 
sim time next is 1644600.0000, 
raw observation next is [6.699999999999999, 91.83333333333333, 0.0, 0.0, 26.0, 25.64448107875081, 0.5820271375966611, 0.0, 1.0, 20144.90892849684], 
processed observation next is [1.0, 0.0, 0.6481994459833795, 0.9183333333333333, 0.0, 0.0, 0.6666666666666666, 0.6370400898959009, 0.6940090458655538, 0.0, 1.0, 0.09592813775474686], 
reward next is 0.9041, 
noisyNet noise sample is [array([0.23342693], dtype=float32), -1.08827]. 
=============================================
[2019-04-04 02:07:14,650] A3C_AGENT_WORKER-Thread-4 INFO:Evaluating...
[2019-04-04 02:07:14,654] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 02:07:14,654] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:07:14,656] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run30
[2019-04-04 02:07:14,687] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 02:07:14,708] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:07:14,710] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run30
[2019-04-04 02:07:14,739] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 02:07:14,741] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:07:14,743] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run30
[2019-04-04 02:08:20,095] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.3723178], dtype=float32), 0.15600024]
[2019-04-04 02:08:20,095] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [15.0, 96.0, 0.0, 0.0, 26.0, 23.58347076702589, 0.1641174065662103, 0.0, 0.0, 0.0]
[2019-04-04 02:08:20,096] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 02:08:20,096] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [4.7815497e-25 1.2973719e-17 1.5888288e-23 1.3101966e-20 2.1456946e-20
 1.9806124e-24 1.0000000e+00], sampled 0.4610982476526022
[2019-04-04 02:10:12,918] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 02:10:33,958] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 02:10:36,144] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 02:10:37,168] A3C_AGENT_WORKER-Thread-4 INFO:Global step: 2900000, evaluation results [2900000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 02:10:40,420] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.07450844e-26 4.43937721e-20 4.90041479e-25 7.88123309e-21
 2.48310189e-21 1.77285611e-26 1.00000000e+00], sum to 1.0000
[2019-04-04 02:10:40,420] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0982
[2019-04-04 02:10:40,439] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.06666666666667, 51.66666666666667, 153.5, 103.3333333333333, 26.0, 26.71843497170012, 0.7474405578072387, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1600800.0000, 
sim time next is 1601400.0000, 
raw observation next is [13.43333333333334, 50.33333333333334, 158.0, 82.66666666666667, 26.0, 25.89628161470464, 0.6939506492156906, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.8347183748845802, 0.5033333333333334, 0.5266666666666666, 0.09134438305709025, 0.6666666666666666, 0.6580234678920535, 0.7313168830718969, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.1162096], dtype=float32), -0.4842132]. 
=============================================
[2019-04-04 02:10:42,905] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.5113300e-23 2.0226453e-16 2.7940319e-21 5.2895429e-18 3.6382344e-18
 3.1531306e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 02:10:42,906] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5038
[2019-04-04 02:10:42,956] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.466666666666667, 83.0, 124.8333333333333, 0.0, 26.0, 24.94110343126335, 0.3471836890907887, 0.0, 1.0, 46867.40142435212], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1772400.0000, 
sim time next is 1773000.0000, 
raw observation next is [-2.55, 83.0, 126.0, 0.0, 26.0, 24.94465363246887, 0.3516515446275985, 0.0, 1.0, 45027.94160100788], 
processed observation next is [0.0, 0.5217391304347826, 0.3919667590027701, 0.83, 0.42, 0.0, 0.6666666666666666, 0.5787211360390726, 0.6172171815425328, 0.0, 1.0, 0.21441876952860894], 
reward next is 0.7856, 
noisyNet noise sample is [array([-0.03274344], dtype=float32), -1.7692478]. 
=============================================
[2019-04-04 02:10:42,960] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[77.86976]
 [77.94545]
 [78.05954]
 [78.19547]
 [78.361  ]], R is [[77.80123138]
 [77.8000412 ]
 [77.8054657 ]
 [77.8377533 ]
 [77.89694977]].
[2019-04-04 02:10:54,332] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.4179456e-23 1.4351022e-16 3.2271309e-21 3.3002265e-18 7.5232920e-18
 5.0751665e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 02:10:54,332] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9368
[2019-04-04 02:10:54,350] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.566666666666666, 76.33333333333334, 0.0, 0.0, 26.0, 24.57210973373219, 0.1406645388333801, 0.0, 1.0, 44864.91415006802], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1894800.0000, 
sim time next is 1895400.0000, 
raw observation next is [-6.75, 77.0, 0.0, 0.0, 26.0, 24.53235242062739, 0.1326262750747133, 0.0, 1.0, 44883.7625001777], 
processed observation next is [0.0, 0.9565217391304348, 0.275623268698061, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5443627017189492, 0.5442087583582378, 0.0, 1.0, 0.21373220238179855], 
reward next is 0.7863, 
noisyNet noise sample is [array([0.19549417], dtype=float32), -0.4750057]. 
=============================================
[2019-04-04 02:10:58,396] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [7.589708e-25 1.964597e-19 7.854543e-23 9.873791e-20 6.580299e-20
 4.970299e-25 1.000000e+00], sum to 1.0000
[2019-04-04 02:10:58,396] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8517
[2019-04-04 02:10:58,443] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 86.0, 0.0, 0.0, 26.0, 25.37509897557654, 0.4346209721470897, 0.0, 1.0, 42354.99503465436], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2062800.0000, 
sim time next is 2063400.0000, 
raw observation next is [-3.9, 85.33333333333334, 0.0, 0.0, 26.0, 25.39646902239796, 0.4328974461515244, 0.0, 1.0, 30827.21402637961], 
processed observation next is [1.0, 0.9130434782608695, 0.3545706371191136, 0.8533333333333334, 0.0, 0.0, 0.6666666666666666, 0.6163724185331633, 0.6442991487171749, 0.0, 1.0, 0.14679625726847434], 
reward next is 0.8532, 
noisyNet noise sample is [array([-0.7802722], dtype=float32), -0.68782336]. 
=============================================
[2019-04-04 02:11:03,516] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.3798126e-25 1.6867228e-18 1.6959642e-23 4.7432113e-20 6.9775166e-20
 3.0778052e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:11:03,516] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7517
[2019-04-04 02:11:03,578] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 80.16666666666667, 118.6666666666667, 0.0, 26.0, 26.35890798986649, 0.4908797682661111, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2038200.0000, 
sim time next is 2038800.0000, 
raw observation next is [-4.1, 81.33333333333334, 111.3333333333333, 0.0, 26.0, 26.36680308968076, 0.4885136725098649, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3490304709141275, 0.8133333333333335, 0.371111111111111, 0.0, 0.6666666666666666, 0.6972335908067301, 0.6628378908366216, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0825703], dtype=float32), -1.8984066]. 
=============================================
[2019-04-04 02:11:07,853] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.7329088e-24 2.0734870e-17 2.2670724e-22 1.7404701e-19 1.9438134e-19
 4.7773405e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:11:07,856] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8161
[2019-04-04 02:11:07,897] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.7, 78.0, 0.0, 0.0, 26.0, 24.30028461088968, 0.1569432662635857, 0.0, 1.0, 42608.78583177715], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2167200.0000, 
sim time next is 2167800.0000, 
raw observation next is [-6.700000000000001, 78.0, 0.0, 0.0, 26.0, 24.37081712083968, 0.1707564950661233, 0.0, 1.0, 42569.45129801419], 
processed observation next is [1.0, 0.08695652173913043, 0.2770083102493075, 0.78, 0.0, 0.0, 0.6666666666666666, 0.5309014267366399, 0.5569188316887078, 0.0, 1.0, 0.2027116728476866], 
reward next is 0.7973, 
noisyNet noise sample is [array([-1.0439614], dtype=float32), -1.2398839]. 
=============================================
[2019-04-04 02:11:30,420] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0210113e-23 1.5057817e-17 3.9350023e-22 9.4634260e-19 5.8372130e-19
 5.4736786e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 02:11:30,420] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5478
[2019-04-04 02:11:30,455] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.3, 80.0, 0.0, 0.0, 26.0, 24.47467086700884, 0.1965832520855111, 0.0, 1.0, 42438.43440666024], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2162400.0000, 
sim time next is 2163000.0000, 
raw observation next is [-7.299999999999999, 79.5, 0.0, 0.0, 26.0, 24.46262983067114, 0.1903912408624214, 0.0, 1.0, 42453.99146300081], 
processed observation next is [1.0, 0.0, 0.2603878116343491, 0.795, 0.0, 0.0, 0.6666666666666666, 0.5385524858892617, 0.5634637469541405, 0.0, 1.0, 0.2021618641095277], 
reward next is 0.7978, 
noisyNet noise sample is [array([-1.308515], dtype=float32), 0.026682105]. 
=============================================
[2019-04-04 02:11:30,459] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[79.0937  ]
 [79.04766 ]
 [78.980156]
 [78.88238 ]
 [78.804344]], R is [[79.12083435]
 [79.12754059]
 [79.13404846]
 [79.14040375]
 [79.14679718]].
[2019-04-04 02:11:33,143] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.9208517e-25 9.0605663e-19 1.0483186e-23 5.7691389e-20 2.9770253e-20
 2.7022016e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:11:33,148] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5192
[2019-04-04 02:11:33,227] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.616666666666667, 55.66666666666667, 0.0, 0.0, 26.0, 24.94506129678298, 0.3537184093086396, 1.0, 1.0, 43631.20550018981], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2317800.0000, 
sim time next is 2318400.0000, 
raw observation next is [-1.7, 56.0, 0.0, 0.0, 26.0, 25.00152839940588, 0.3570679732289589, 0.0, 1.0, 18730.56746857888], 
processed observation next is [1.0, 0.8695652173913043, 0.4155124653739613, 0.56, 0.0, 0.0, 0.6666666666666666, 0.5834606999504901, 0.6190226577429864, 0.0, 1.0, 0.08919317842180419], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.97743106], dtype=float32), 1.1681987]. 
=============================================
[2019-04-04 02:11:48,272] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.30178642e-25 4.63250663e-19 4.96814067e-24 5.13090408e-20
 1.08130455e-20 5.64772516e-26 1.00000000e+00], sum to 1.0000
[2019-04-04 02:11:48,273] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6205
[2019-04-04 02:11:48,280] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.466666666666667, 28.0, 151.5, 287.6666666666667, 26.0, 25.84612039378364, 0.3817021212109628, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2558400.0000, 
sim time next is 2559000.0000, 
raw observation next is [3.383333333333333, 28.5, 144.0, 300.3333333333333, 26.0, 25.82298679259759, 0.378137008079257, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5563250230840259, 0.285, 0.48, 0.3318600368324125, 0.6666666666666666, 0.6519155660497992, 0.6260456693597524, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.42745864], dtype=float32), 0.24341215]. 
=============================================
[2019-04-04 02:11:48,289] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[86.79448 ]
 [86.93107 ]
 [87.11219 ]
 [87.129845]
 [87.09309 ]], R is [[86.76197815]
 [86.89435577]
 [87.02541351]
 [87.155159  ]
 [87.28360748]].
[2019-04-04 02:11:52,604] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.5330485e-24 1.5672353e-17 3.1675492e-22 1.8980029e-19 3.2821834e-19
 7.3503945e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:11:52,604] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1524
[2019-04-04 02:11:52,625] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 44.83333333333334, 0.0, 0.0, 26.0, 24.92735402918071, 0.1840188682211628, 0.0, 1.0, 38577.21825497227], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2517000.0000, 
sim time next is 2517600.0000, 
raw observation next is [-1.7, 45.66666666666667, 0.0, 0.0, 26.0, 24.98410758034917, 0.1888183636333582, 0.0, 1.0, 38511.08999087518], 
processed observation next is [1.0, 0.13043478260869565, 0.4155124653739613, 0.4566666666666667, 0.0, 0.0, 0.6666666666666666, 0.5820089650290976, 0.5629394545444527, 0.0, 1.0, 0.18338614281369134], 
reward next is 0.8166, 
noisyNet noise sample is [array([-0.8959797], dtype=float32), 1.974131]. 
=============================================
[2019-04-04 02:11:59,038] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.8007811e-25 6.5846753e-18 8.8730696e-24 1.8433226e-20 2.4897710e-20
 8.5728502e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:11:59,040] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5923
[2019-04-04 02:11:59,083] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.533333333333334, 54.83333333333334, 107.6666666666667, 28.0, 26.0, 25.64517903468457, 0.274340040757456, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2538600.0000, 
sim time next is 2539200.0000, 
raw observation next is [-2.266666666666667, 53.66666666666667, 121.8333333333333, 30.5, 26.0, 25.65561870697538, 0.2824691061694298, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.39981532779316714, 0.5366666666666667, 0.406111111111111, 0.03370165745856354, 0.6666666666666666, 0.6379682255812815, 0.5941563687231433, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.04218931], dtype=float32), 0.27236724]. 
=============================================
[2019-04-04 02:12:00,401] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0803634e-22 3.7177173e-16 1.9301798e-20 4.0392363e-18 1.0432474e-17
 6.4928963e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 02:12:00,401] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3266
[2019-04-04 02:12:00,417] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-15.0, 89.66666666666667, 0.0, 0.0, 26.0, 23.43901901650056, -0.01876596742700646, 0.0, 1.0, 44479.33604797773], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2693400.0000, 
sim time next is 2694000.0000, 
raw observation next is [-15.0, 88.33333333333334, 0.0, 0.0, 26.0, 23.42010681963442, -0.02414641470089889, 0.0, 1.0, 44413.45477428773], 
processed observation next is [1.0, 0.17391304347826086, 0.04709141274238226, 0.8833333333333334, 0.0, 0.0, 0.6666666666666666, 0.4516755683028683, 0.49195119509970037, 0.0, 1.0, 0.21149264178232255], 
reward next is 0.7885, 
noisyNet noise sample is [array([1.1917965], dtype=float32), -0.33302155]. 
=============================================
[2019-04-04 02:12:00,436] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[76.37175 ]
 [76.427605]
 [76.5279  ]
 [76.60651 ]
 [76.6856  ]], R is [[76.38529205]
 [76.40962982]
 [76.43362427]
 [76.45729828]
 [76.48067474]].
[2019-04-04 02:12:10,728] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.8309706e-24 4.6382556e-18 8.4793523e-23 8.8742721e-20 1.2066281e-19
 2.3113344e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:12:10,728] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5827
[2019-04-04 02:12:10,776] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 59.0, 0.0, 0.0, 26.0, 25.19695540410071, 0.4041100600304703, 1.0, 1.0, 62296.81730698907], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2746800.0000, 
sim time next is 2747400.0000, 
raw observation next is [-5.0, 59.0, 0.0, 0.0, 26.0, 25.18840978686011, 0.3965364083754925, 1.0, 1.0, 58619.31484583399], 
processed observation next is [1.0, 0.8260869565217391, 0.32409972299168976, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5990341489050092, 0.6321788027918308, 1.0, 1.0, 0.2791395945039714], 
reward next is 0.7209, 
noisyNet noise sample is [array([-0.11009818], dtype=float32), 1.4287064]. 
=============================================
[2019-04-04 02:12:14,077] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.5387156e-24 4.7535406e-18 9.1395612e-22 1.1531930e-18 4.6773610e-19
 4.0622654e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 02:12:14,077] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5002
[2019-04-04 02:12:14,092] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 72.0, 0.0, 0.0, 26.0, 25.00591539544889, 0.3098522154547808, 0.0, 1.0, 54839.3142382212], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2853000.0000, 
sim time next is 2853600.0000, 
raw observation next is [1.0, 72.0, 0.0, 0.0, 26.0, 25.04029982507649, 0.3138533869972739, 0.0, 1.0, 53832.64774929287], 
processed observation next is [1.0, 0.0, 0.4903047091412743, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5866916520897076, 0.604617795665758, 0.0, 1.0, 0.2563459416632994], 
reward next is 0.7437, 
noisyNet noise sample is [array([-1.3250035], dtype=float32), -1.6076571]. 
=============================================
[2019-04-04 02:12:14,977] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2324200e-24 6.8832987e-19 9.9830417e-23 2.3862026e-19 1.5198613e-19
 1.7487112e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:12:14,980] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3217
[2019-04-04 02:12:15,031] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 97.66666666666667, 73.33333333333334, 45.0, 26.0, 24.73440501623491, 0.3610379447086046, 1.0, 1.0, 196994.294538754], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2910000.0000, 
sim time next is 2910600.0000, 
raw observation next is [2.0, 96.5, 71.0, 54.0, 26.0, 25.15613702378743, 0.4209336089274429, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.518005540166205, 0.965, 0.23666666666666666, 0.05966850828729282, 0.6666666666666666, 0.5963447519822859, 0.6403112029758143, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.54413646], dtype=float32), 2.2047832]. 
=============================================
[2019-04-04 02:12:16,215] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.6964355e-24 1.1659677e-18 7.2755240e-23 4.6457084e-19 2.8621597e-19
 1.8922219e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:12:16,215] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8833
[2019-04-04 02:12:16,282] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.333333333333334, 32.33333333333334, 0.0, 0.0, 26.0, 24.62605158435519, 0.1976765124467284, 1.0, 1.0, 196217.9094192962], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2830800.0000, 
sim time next is 2831400.0000, 
raw observation next is [4.0, 33.5, 0.0, 0.0, 26.0, 23.53622986955531, 0.1331194530708159, 1.0, 1.0, 196605.2886036068], 
processed observation next is [1.0, 0.782608695652174, 0.5734072022160666, 0.335, 0.0, 0.0, 0.6666666666666666, 0.4613524891296092, 0.5443731510236053, 1.0, 1.0, 0.9362156600171753], 
reward next is 0.0638, 
noisyNet noise sample is [array([-0.09566215], dtype=float32), -0.25414822]. 
=============================================
[2019-04-04 02:12:21,556] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.8564480e-24 2.5819635e-17 3.6500849e-22 3.3206700e-19 6.2965743e-19
 5.8672248e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:12:21,559] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9223
[2019-04-04 02:12:21,578] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 95.33333333333334, 0.0, 0.0, 26.0, 24.81873649561512, 0.2576424800058389, 0.0, 1.0, 56167.09933830073], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2870400.0000, 
sim time next is 2871000.0000, 
raw observation next is [1.0, 96.5, 0.0, 0.0, 26.0, 24.94836492288863, 0.260569450836967, 0.0, 1.0, 55783.0961790892], 
processed observation next is [1.0, 0.21739130434782608, 0.4903047091412743, 0.965, 0.0, 0.0, 0.6666666666666666, 0.5790304102407191, 0.5868564836123223, 0.0, 1.0, 0.2656337913289962], 
reward next is 0.7344, 
noisyNet noise sample is [array([-0.51505816], dtype=float32), 1.7534375]. 
=============================================
[2019-04-04 02:12:21,593] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[80.43172]
 [80.43954]
 [80.49467]
 [80.56039]
 [80.63056]], R is [[80.39027405]
 [80.31890869]
 [80.25017548]
 [80.18206787]
 [80.11466217]].
[2019-04-04 02:12:22,383] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.8238565e-23 5.8063493e-17 5.7561628e-22 9.0681391e-19 1.4215013e-18
 4.2434519e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 02:12:22,384] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9201
[2019-04-04 02:12:22,410] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.5, 65.0, 0.0, 0.0, 26.0, 25.30598284197608, 0.356281898500943, 0.0, 1.0, 39967.14912992465], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3013200.0000, 
sim time next is 3013800.0000, 
raw observation next is [-3.583333333333333, 65.0, 0.0, 0.0, 26.0, 25.29426889079347, 0.351567874765414, 0.0, 1.0, 39701.44651828431], 
processed observation next is [0.0, 0.9130434782608695, 0.36334256694367506, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6078557408994559, 0.6171892915884714, 0.0, 1.0, 0.18905450722992528], 
reward next is 0.8109, 
noisyNet noise sample is [array([-0.00680967], dtype=float32), 0.3137441]. 
=============================================
[2019-04-04 02:12:24,366] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.0090921e-23 8.5694522e-17 8.8085627e-22 1.7306063e-18 5.2880093e-18
 4.3705945e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 02:12:24,366] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9992
[2019-04-04 02:12:24,408] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.666666666666667, 58.33333333333334, 0.0, 0.0, 26.0, 25.03835415092971, 0.3428552639723497, 0.0, 1.0, 51008.15246615215], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3001200.0000, 
sim time next is 3001800.0000, 
raw observation next is [-1.833333333333333, 59.16666666666666, 0.0, 0.0, 26.0, 25.09708940797158, 0.3389554105814523, 0.0, 1.0, 18715.62290091659], 
processed observation next is [0.0, 0.7391304347826086, 0.41181902123730385, 0.5916666666666666, 0.0, 0.0, 0.6666666666666666, 0.591424117330965, 0.6129851368604841, 0.0, 1.0, 0.08912201381388853], 
reward next is 0.9109, 
noisyNet noise sample is [array([0.39230096], dtype=float32), 1.4402736]. 
=============================================
[2019-04-04 02:12:29,410] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.97353040e-24 4.89213929e-18 1.05139886e-22 3.79304756e-19
 2.90230121e-19 5.92290415e-24 1.00000000e+00], sum to 1.0000
[2019-04-04 02:12:29,411] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8338
[2019-04-04 02:12:29,440] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 100.0, 0.0, 0.0, 26.0, 25.32413580111331, 0.325580103835285, 0.0, 1.0, 51078.49049892206], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3108000.0000, 
sim time next is 3108600.0000, 
raw observation next is [0.0, 100.0, 0.0, 0.0, 26.0, 25.31657022926753, 0.3254416127422708, 0.0, 1.0, 43996.18602847654], 
processed observation next is [0.0, 1.0, 0.46260387811634357, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6097141857722942, 0.6084805375807569, 0.0, 1.0, 0.2095056477546502], 
reward next is 0.7905, 
noisyNet noise sample is [array([0.55048233], dtype=float32), -1.4390719]. 
=============================================
[2019-04-04 02:12:34,452] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.1179132e-27 2.8707215e-19 5.4385846e-24 5.2738394e-21 1.1570651e-20
 1.8416944e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:12:34,474] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9630
[2019-04-04 02:12:34,582] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.0, 100.0, 28.33333333333333, 185.3333333333333, 26.0, 25.46780815855699, 0.3296990852095083, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3138600.0000, 
sim time next is 3139200.0000, 
raw observation next is [6.0, 100.0, 42.0, 237.0, 26.0, 25.37600581538433, 0.3723072969966096, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.6288088642659281, 1.0, 0.14, 0.261878453038674, 0.6666666666666666, 0.6146671512820276, 0.6241024323322032, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.30367523], dtype=float32), -0.386754]. 
=============================================
[2019-04-04 02:12:46,344] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1318046e-23 8.5719370e-17 1.8543027e-21 3.5072998e-19 1.4162132e-18
 8.0296511e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 02:12:46,345] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3586
[2019-04-04 02:12:46,394] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 98.66666666666667, 0.0, 0.0, 26.0, 25.34078315742936, 0.4829566565846545, 0.0, 1.0, 40700.53279956403], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3219000.0000, 
sim time next is 3219600.0000, 
raw observation next is [-3.0, 97.33333333333334, 0.0, 0.0, 26.0, 25.30894061514403, 0.4720538581478273, 0.0, 1.0, 40776.46168907909], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.9733333333333334, 0.0, 0.0, 0.6666666666666666, 0.6090783845953359, 0.6573512860492757, 0.0, 1.0, 0.1941736270908528], 
reward next is 0.8058, 
noisyNet noise sample is [array([-0.54252416], dtype=float32), -1.0547538]. 
=============================================
[2019-04-04 02:12:50,536] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.2188976e-25 6.5742309e-19 1.4333189e-23 5.3744755e-20 5.9133895e-20
 4.5695187e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:12:50,536] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4377
[2019-04-04 02:12:50,602] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 55.0, 0.0, 0.0, 26.0, 25.65648115630587, 0.5083615695119712, 1.0, 1.0, 56645.68653299917], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3351000.0000, 
sim time next is 3351600.0000, 
raw observation next is [-3.0, 55.0, 0.0, 0.0, 26.0, 25.54719496885439, 0.512345460020572, 1.0, 1.0, 42561.53674438046], 
processed observation next is [1.0, 0.8260869565217391, 0.3795013850415513, 0.55, 0.0, 0.0, 0.6666666666666666, 0.6289329140711993, 0.6707818200068574, 1.0, 1.0, 0.2026739844970498], 
reward next is 0.7973, 
noisyNet noise sample is [array([0.40274888], dtype=float32), 1.3005397]. 
=============================================
[2019-04-04 02:12:55,602] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.8490355e-25 6.0166078e-18 1.8040640e-23 2.0076972e-20 2.3625973e-20
 2.4657488e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:12:55,602] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7150
[2019-04-04 02:12:55,662] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 59.5, 111.0, 758.0, 26.0, 25.57117083339504, 0.4816104547708062, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3580200.0000, 
sim time next is 3580800.0000, 
raw observation next is [-4.333333333333334, 57.66666666666667, 111.5, 767.6666666666667, 26.0, 25.50010391073022, 0.471965352702187, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.3425669436749769, 0.5766666666666667, 0.37166666666666665, 0.8482504604051566, 0.6666666666666666, 0.6250086592275185, 0.6573217842340623, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6998712], dtype=float32), -1.315492]. 
=============================================
[2019-04-04 02:12:59,467] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.2312446e-27 7.8270977e-20 4.0219398e-25 6.9835541e-22 2.4603970e-21
 4.4990821e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 02:12:59,468] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1738
[2019-04-04 02:12:59,481] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 50.0, 96.5, 713.0, 26.0, 26.61746764298194, 0.7016439909026381, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3337200.0000, 
sim time next is 3337800.0000, 
raw observation next is [-2.833333333333333, 49.33333333333334, 93.33333333333334, 700.0, 26.0, 26.6585886997368, 0.7013495324184927, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.3841181902123731, 0.4933333333333334, 0.3111111111111111, 0.7734806629834254, 0.6666666666666666, 0.7215490583114, 0.733783177472831, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.45240706], dtype=float32), -0.2503581]. 
=============================================
[2019-04-04 02:12:59,726] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [6.34560107e-25 6.83805190e-18 1.43475229e-23 2.78585965e-20
 8.41186978e-20 1.56466705e-24 1.00000000e+00], sum to 1.0000
[2019-04-04 02:12:59,727] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3486
[2019-04-04 02:12:59,766] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 60.0, 0.0, 0.0, 26.0, 25.33859928050897, 0.3671694500338121, 0.0, 1.0, 38968.13331712262], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3625200.0000, 
sim time next is 3625800.0000, 
raw observation next is [-1.0, 54.16666666666666, 0.0, 0.0, 26.0, 25.31789215335619, 0.367592918719642, 0.0, 1.0, 38574.90154910761], 
processed observation next is [0.0, 1.0, 0.4349030470914128, 0.5416666666666665, 0.0, 0.0, 0.6666666666666666, 0.6098243461130158, 0.6225309729065474, 0.0, 1.0, 0.1836900073767029], 
reward next is 0.8163, 
noisyNet noise sample is [array([0.31094155], dtype=float32), -0.9587779]. 
=============================================
[2019-04-04 02:13:02,606] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.2332722e-25 2.1900555e-17 1.0171089e-22 6.9651774e-20 1.1468882e-19
 1.8017041e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:13:02,607] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1000
[2019-04-04 02:13:02,647] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.24012855604513, 0.3792948372184029, 0.0, 1.0, 41600.11595313538], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3480000.0000, 
sim time next is 3480600.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.25588706468866, 0.3642360880912466, 0.0, 1.0, 45837.00538318273], 
processed observation next is [1.0, 0.2608695652173913, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6046572553907218, 0.6214120293637488, 0.0, 1.0, 0.21827145420563204], 
reward next is 0.7817, 
noisyNet noise sample is [array([1.3779556], dtype=float32), -0.6332413]. 
=============================================
[2019-04-04 02:13:10,437] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.2728463e-25 2.9577849e-18 1.4668054e-23 4.1592561e-20 4.3361919e-20
 3.4776595e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:13:10,441] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3212
[2019-04-04 02:13:10,467] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.833333333333333, 64.16666666666667, 0.0, 0.0, 26.0, 25.38803020516873, 0.4472232307313223, 0.0, 1.0, 55049.94742317206], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3887400.0000, 
sim time next is 3888000.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.35363681414955, 0.443614038523537, 0.0, 1.0, 56405.5721779751], 
processed observation next is [1.0, 0.0, 0.40720221606648205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6128030678457957, 0.6478713461745124, 0.0, 1.0, 0.2685979627522624], 
reward next is 0.7314, 
noisyNet noise sample is [array([-0.736991], dtype=float32), -1.1970347]. 
=============================================
[2019-04-04 02:13:10,527] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[85.3594  ]
 [85.27307 ]
 [85.31517 ]
 [85.390366]
 [85.46468 ]], R is [[85.1241684 ]
 [85.01078033]
 [85.02457428]
 [85.08498383]
 [85.12813568]].
[2019-04-04 02:13:14,176] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1521029e-25 5.1527572e-19 8.7947207e-24 4.7806315e-20 1.6070387e-20
 9.7806796e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:13:14,176] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8566
[2019-04-04 02:13:14,214] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 26.0, 25.29351395095959, 0.5363946758886117, 0.0, 1.0, 62801.50195586943], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3444600.0000, 
sim time next is 3445200.0000, 
raw observation next is [1.0, 79.0, 0.0, 0.0, 26.0, 25.43106304990352, 0.5524651329416211, 0.0, 1.0, 18762.25115525546], 
processed observation next is [1.0, 0.9130434782608695, 0.4903047091412743, 0.79, 0.0, 0.0, 0.6666666666666666, 0.6192552541586268, 0.6841550443138736, 0.0, 1.0, 0.0893440531202641], 
reward next is 0.9107, 
noisyNet noise sample is [array([-1.0354104], dtype=float32), -1.1228505]. 
=============================================
[2019-04-04 02:13:15,150] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.0027792e-26 3.6428434e-19 5.5511397e-25 6.2013343e-22 2.9895992e-21
 4.8679374e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:13:15,150] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8065
[2019-04-04 02:13:15,167] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.0, 25.0, 0.0, 0.0, 26.0, 25.62658698147229, 0.3824618351634165, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3631200.0000, 
sim time next is 3631800.0000, 
raw observation next is [9.0, 25.0, 0.0, 0.0, 26.0, 25.60078620067299, 0.3734109537857981, 0.0, 1.0, 18735.48893980787], 
processed observation next is [0.0, 0.0, 0.7119113573407203, 0.25, 0.0, 0.0, 0.6666666666666666, 0.6333988500560824, 0.6244703179285994, 0.0, 1.0, 0.08921661399908509], 
reward next is 0.9108, 
noisyNet noise sample is [array([2.035684], dtype=float32), 0.40709597]. 
=============================================
[2019-04-04 02:13:18,010] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.1693074e-27 4.4477026e-19 1.3236819e-25 4.4549462e-22 2.3904912e-21
 3.4174912e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:13:18,012] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9055
[2019-04-04 02:13:18,106] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.0, 25.0, 0.0, 0.0, 26.0, 25.62658698147229, 0.3824618351634165, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3631200.0000, 
sim time next is 3631800.0000, 
raw observation next is [9.0, 25.0, 0.0, 0.0, 26.0, 25.60078620067299, 0.3734109537857981, 0.0, 1.0, 18735.48893980787], 
processed observation next is [0.0, 0.0, 0.7119113573407203, 0.25, 0.0, 0.0, 0.6666666666666666, 0.6333988500560824, 0.6244703179285994, 0.0, 1.0, 0.08921661399908509], 
reward next is 0.9108, 
noisyNet noise sample is [array([-0.783487], dtype=float32), 0.1945194]. 
=============================================
[2019-04-04 02:13:19,798] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0180978e-24 2.4590884e-17 6.8694607e-23 2.3234324e-20 1.6515568e-19
 3.9959441e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:13:19,798] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3954
[2019-04-04 02:13:19,819] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 38.0, 0.0, 0.0, 26.0, 24.71024340140844, 0.1949178310500141, 0.0, 1.0, 40164.89355677031], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4082400.0000, 
sim time next is 4083000.0000, 
raw observation next is [-4.166666666666667, 38.5, 0.0, 0.0, 26.0, 24.6851568521959, 0.1920669973210452, 0.0, 1.0, 40143.74606340181], 
processed observation next is [1.0, 0.2608695652173913, 0.3471837488457987, 0.385, 0.0, 0.0, 0.6666666666666666, 0.5570964043496582, 0.5640223324403484, 0.0, 1.0, 0.19116069554000864], 
reward next is 0.8088, 
noisyNet noise sample is [array([0.2655013], dtype=float32), -1.7597628]. 
=============================================
[2019-04-04 02:13:19,856] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[87.333664]
 [87.35409 ]
 [87.371346]
 [87.36993 ]
 [87.35895 ]], R is [[87.21408844]
 [87.15068817]
 [87.08791351]
 [87.02571869]
 [86.96408081]].
[2019-04-04 02:13:22,478] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.8309706e-24 3.2536521e-17 2.5600372e-22 9.6806731e-20 3.3480934e-19
 1.0139705e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 02:13:22,478] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8439
[2019-04-04 02:13:22,546] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.40751156960319, 0.3433931914747304, 0.0, 1.0, 21731.00274658688], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3728400.0000, 
sim time next is 3729000.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.39774539797779, 0.3339450714021434, 0.0, 1.0, 35423.14202899583], 
processed observation next is [1.0, 0.13043478260869565, 0.3795013850415513, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6164787831648159, 0.6113150238007145, 0.0, 1.0, 0.16868162870950396], 
reward next is 0.8313, 
noisyNet noise sample is [array([0.12258662], dtype=float32), -0.29083562]. 
=============================================
[2019-04-04 02:13:22,567] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[85.08528 ]
 [85.124374]
 [85.10741 ]
 [85.09013 ]
 [85.07071 ]], R is [[85.05927277]
 [85.10520172]
 [85.05865479]
 [85.0117569 ]
 [84.96431732]].
[2019-04-04 02:13:28,564] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.3728015e-25 8.7603886e-19 8.3063746e-23 8.8541193e-20 1.0491289e-19
 2.8336120e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:13:28,564] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3799
[2019-04-04 02:13:28,609] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.833333333333333, 70.0, 0.0, 0.0, 26.0, 25.45948417819925, 0.5495744759941811, 0.0, 1.0, 18758.371222703], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3790200.0000, 
sim time next is 3790800.0000, 
raw observation next is [-3.0, 71.0, 0.0, 0.0, 26.0, 25.58969351040121, 0.5529030067622883, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.3795013850415513, 0.71, 0.0, 0.0, 0.6666666666666666, 0.6324744592001009, 0.6843010022540961, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.07295922], dtype=float32), 0.48555684]. 
=============================================
[2019-04-04 02:13:44,243] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.3886188e-24 1.0868731e-16 2.1904092e-22 1.6284534e-19 4.4792016e-19
 4.0938760e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 02:13:44,261] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9541
[2019-04-04 02:13:44,275] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 49.0, 0.0, 0.0, 26.0, 25.36272475081522, 0.3224369182665339, 0.0, 1.0, 41048.78596003421], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4256400.0000, 
sim time next is 4257000.0000, 
raw observation next is [3.0, 49.0, 0.0, 0.0, 26.0, 25.35806608941573, 0.3216976863271655, 0.0, 1.0, 39893.92161470089], 
processed observation next is [0.0, 0.2608695652173913, 0.5457063711911359, 0.49, 0.0, 0.0, 0.6666666666666666, 0.6131721741179774, 0.6072325621090552, 0.0, 1.0, 0.1899710553080995], 
reward next is 0.8100, 
noisyNet noise sample is [array([-1.1549314], dtype=float32), 0.8431684]. 
=============================================
[2019-04-04 02:13:44,307] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[85.12448 ]
 [85.12812 ]
 [85.13325 ]
 [85.12907 ]
 [85.111946]], R is [[85.07151794]
 [85.02532959]
 [84.96954346]
 [84.9209137 ]
 [84.88884735]].
[2019-04-04 02:13:47,899] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [4.5252521e-24 3.9452160e-18 7.2072390e-23 8.5434245e-20 1.2505853e-19
 1.1474017e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:13:47,899] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.3758
[2019-04-04 02:13:47,972] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 41.0, 0.0, 0.0, 26.0, 25.83976219356566, 0.5937583480925896, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3955800.0000, 
sim time next is 3956400.0000, 
raw observation next is [-6.0, 41.0, 0.0, 0.0, 26.0, 25.91584373057166, 0.5852338718155966, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.296398891966759, 0.41, 0.0, 0.0, 0.6666666666666666, 0.659653644214305, 0.6950779572718656, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.34463328], dtype=float32), 1.7953365]. 
=============================================
[2019-04-04 02:13:55,813] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.5646125e-25 3.2405806e-19 2.5197296e-24 1.8487168e-20 1.7639906e-20
 4.1837491e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:13:55,813] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6425
[2019-04-04 02:13:55,854] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.333333333333333, 37.5, 118.6666666666667, 824.3333333333334, 26.0, 26.42399732640986, 0.588165831917534, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4017000.0000, 
sim time next is 4017600.0000, 
raw observation next is [-6.0, 37.0, 118.5, 828.5, 26.0, 26.44405242298257, 0.5947328745432109, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.296398891966759, 0.37, 0.395, 0.9154696132596685, 0.6666666666666666, 0.7036710352485475, 0.6982442915144036, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.77878135], dtype=float32), -0.6766901]. 
=============================================
[2019-04-04 02:14:00,147] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.6088933e-25 8.2099992e-18 2.1358997e-23 5.3239919e-20 3.2754901e-20
 5.3800205e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:14:00,147] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8513
[2019-04-04 02:14:00,168] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 40.66666666666667, 0.0, 0.0, 26.0, 25.35926998264121, 0.4717763838483245, 0.0, 1.0, 196646.6135831886], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4134000.0000, 
sim time next is 4134600.0000, 
raw observation next is [1.0, 39.5, 0.0, 0.0, 26.0, 25.32801196048116, 0.4949055331062746, 0.0, 1.0, 198133.9827187007], 
processed observation next is [1.0, 0.8695652173913043, 0.4903047091412743, 0.395, 0.0, 0.0, 0.6666666666666666, 0.6106676633734299, 0.6649685110354249, 0.0, 1.0, 0.9434951558033366], 
reward next is 0.0565, 
noisyNet noise sample is [array([-0.7584855], dtype=float32), -0.22651048]. 
=============================================
[2019-04-04 02:14:04,718] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.0623008e-25 1.2124280e-19 2.5591265e-24 2.0987740e-20 2.2138570e-20
 1.3984593e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:14:04,718] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3903
[2019-04-04 02:14:04,768] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.166666666666667, 22.33333333333334, 74.33333333333333, 602.6666666666666, 26.0, 27.28886139706719, 0.6874948926808083, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4032600.0000, 
sim time next is 4033200.0000, 
raw observation next is [-1.333333333333333, 22.66666666666667, 70.66666666666667, 575.3333333333334, 26.0, 26.60790269343866, 0.6887721865593317, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.42566943674976926, 0.2266666666666667, 0.23555555555555557, 0.6357274401473297, 0.6666666666666666, 0.7173252244532217, 0.7295907288531106, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.1014032], dtype=float32), 0.1527571]. 
=============================================
[2019-04-04 02:14:04,998] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2521717e-24 7.6604460e-18 1.3820950e-23 1.9749729e-20 1.1126246e-19
 2.4264933e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:14:04,999] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5707
[2019-04-04 02:14:05,025] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.6666666666666667, 30.83333333333333, 118.6666666666667, 830.3333333333334, 26.0, 25.0468526958025, 0.3918159788861394, 0.0, 1.0, 22314.90300088832], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4189800.0000, 
sim time next is 4190400.0000, 
raw observation next is [1.0, 30.0, 118.5, 834.5, 26.0, 25.11174259382972, 0.3942277463157997, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.4903047091412743, 0.3, 0.395, 0.9220994475138121, 0.6666666666666666, 0.5926452161524768, 0.6314092487719333, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3121563], dtype=float32), -1.442291]. 
=============================================
[2019-04-04 02:14:05,792] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.4471058e-27 2.0717060e-19 2.8356123e-25 3.6707666e-21 6.8011755e-22
 5.4529725e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 02:14:05,795] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5319
[2019-04-04 02:14:05,842] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.666666666666667, 37.0, 102.0, 644.0, 26.0, 26.10470191000693, 0.4925011417782312, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4094400.0000, 
sim time next is 4095000.0000, 
raw observation next is [-2.5, 36.5, 104.0, 679.0, 26.0, 26.20421056989942, 0.5158958057679439, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.39335180055401664, 0.365, 0.3466666666666667, 0.7502762430939226, 0.6666666666666666, 0.683684214158285, 0.6719652685893146, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.24237803], dtype=float32), -0.7525539]. 
=============================================
[2019-04-04 02:14:05,847] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[92.974495]
 [92.61877 ]
 [92.36992 ]
 [92.15667 ]
 [92.1616  ]], R is [[93.1829834 ]
 [93.25115204]
 [93.31864166]
 [93.3854599 ]
 [93.45160675]].
[2019-04-04 02:14:07,687] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.1038541e-25 3.0789199e-18 1.4171500e-23 1.2034656e-19 5.2248023e-20
 2.2583780e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:14:07,690] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2922
[2019-04-04 02:14:07,720] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 47.0, 0.0, 0.0, 26.0, 25.42326278541265, 0.3658581338694184, 0.0, 1.0, 42149.17099244615], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4229400.0000, 
sim time next is 4230000.0000, 
raw observation next is [1.0, 47.0, 0.0, 0.0, 26.0, 25.41162345860911, 0.3640483394297764, 0.0, 1.0, 45524.57525347792], 
processed observation next is [0.0, 1.0, 0.4903047091412743, 0.47, 0.0, 0.0, 0.6666666666666666, 0.6176352882174259, 0.6213494464765922, 0.0, 1.0, 0.21678369168322817], 
reward next is 0.7832, 
noisyNet noise sample is [array([-2.9744296], dtype=float32), -1.0174949]. 
=============================================
[2019-04-04 02:14:07,741] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[86.01565 ]
 [85.94345 ]
 [85.8211  ]
 [85.750465]
 [85.70997 ]], R is [[85.98847961]
 [85.92787933]
 [85.87910461]
 [85.77350616]
 [85.70698547]].
[2019-04-04 02:14:09,851] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.9628749e-25 1.1359773e-18 5.4249529e-23 1.2694120e-19 4.2242550e-20
 1.0322583e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:14:09,852] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5817
[2019-04-04 02:14:09,936] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 86.0, 112.5, 0.0, 26.0, 26.07259901300294, 0.4826875044145688, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4711200.0000, 
sim time next is 4711800.0000, 
raw observation next is [1.0, 86.0, 117.0, 0.0, 26.0, 25.93239575589477, 0.4584221030436155, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.4903047091412743, 0.86, 0.39, 0.0, 0.6666666666666666, 0.6610329796578975, 0.6528073676812052, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2807817], dtype=float32), -0.37757966]. 
=============================================
[2019-04-04 02:14:14,149] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.3281831e-24 5.6539588e-17 2.0891227e-22 1.2336508e-19 1.3517262e-18
 2.6970656e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 02:14:14,149] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0857
[2019-04-04 02:14:14,221] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.8, 76.0, 0.0, 0.0, 26.0, 25.50574536854075, 0.4144086941815637, 0.0, 1.0, 62524.51731677207], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4312800.0000, 
sim time next is 4313400.0000, 
raw observation next is [4.733333333333333, 75.83333333333334, 0.0, 0.0, 26.0, 25.49981648676922, 0.4175732998085318, 0.0, 1.0, 48108.14706495027], 
processed observation next is [0.0, 0.9565217391304348, 0.5937211449676825, 0.7583333333333334, 0.0, 0.0, 0.6666666666666666, 0.6249847072307683, 0.6391910999361773, 0.0, 1.0, 0.22908641459500131], 
reward next is 0.7709, 
noisyNet noise sample is [array([0.35540703], dtype=float32), 0.16853121]. 
=============================================
[2019-04-04 02:14:21,708] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.1272399e-25 2.9815516e-18 7.5300775e-23 4.0160168e-20 2.0147643e-19
 2.8837534e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:14:21,708] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1288
[2019-04-04 02:14:21,808] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.333333333333334, 65.66666666666667, 0.0, 0.0, 26.0, 25.78419513179138, 0.5807682704455714, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4412400.0000, 
sim time next is 4413000.0000, 
raw observation next is [6.216666666666666, 65.83333333333333, 0.0, 0.0, 26.0, 25.76098289987601, 0.5736131349239758, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.6348107109879964, 0.6583333333333333, 0.0, 0.0, 0.6666666666666666, 0.6467485749896674, 0.691204378307992, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0147607], dtype=float32), -0.9003896]. 
=============================================
[2019-04-04 02:14:21,841] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[83.27538 ]
 [83.412445]
 [83.5102  ]
 [83.66177 ]
 [83.79271 ]], R is [[83.33866119]
 [83.50527191]
 [83.67021942]
 [83.83351898]
 [83.99518585]].
[2019-04-04 02:14:24,602] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.7078854e-25 1.6032623e-18 2.2622161e-23 4.0673905e-19 3.5806914e-20
 4.6476973e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:14:24,602] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5114
[2019-04-04 02:14:24,642] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 81.5, 71.0, 0.0, 26.0, 26.11227259186053, 0.6046041640552892, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4462200.0000, 
sim time next is 4462800.0000, 
raw observation next is [0.0, 80.33333333333334, 67.33333333333334, 0.0, 26.0, 26.21016329910996, 0.6161901060557948, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.46260387811634357, 0.8033333333333335, 0.22444444444444447, 0.0, 0.6666666666666666, 0.6841802749258301, 0.7053967020185983, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.29472515], dtype=float32), -0.7997402]. 
=============================================
[2019-04-04 02:14:31,347] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5226435e-23 1.1722528e-16 1.0458911e-21 8.9566817e-19 1.4952203e-18
 1.6566590e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 02:14:31,347] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5428
[2019-04-04 02:14:31,361] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.166666666666667, 61.83333333333333, 0.0, 0.0, 26.0, 24.67987780416417, 0.1980413185090211, 0.0, 1.0, 39561.34959837276], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4859400.0000, 
sim time next is 4860000.0000, 
raw observation next is [-3.0, 60.0, 0.0, 0.0, 26.0, 24.651321954251, 0.1928671791374717, 0.0, 1.0, 39539.55233482625], 
processed observation next is [0.0, 0.2608695652173913, 0.3795013850415513, 0.6, 0.0, 0.0, 0.6666666666666666, 0.5542768295209166, 0.5642890597124905, 0.0, 1.0, 0.18828358254679167], 
reward next is 0.8117, 
noisyNet noise sample is [array([0.44485712], dtype=float32), -0.78130406]. 
=============================================
[2019-04-04 02:14:31,395] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[82.59795 ]
 [82.4994  ]
 [82.40801 ]
 [82.329445]
 [82.258995]], R is [[82.69124603]
 [82.6759491 ]
 [82.66079712]
 [82.6459198 ]
 [82.6313858 ]].
[2019-04-04 02:14:33,621] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.8207579e-24 2.8523203e-17 3.8825320e-22 2.6631488e-19 3.1193617e-19
 7.6072776e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:14:33,621] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4813
[2019-04-04 02:14:33,657] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.8666666666666667, 72.33333333333334, 0.0, 0.0, 26.0, 25.26675100463027, 0.4081145380886584, 0.0, 1.0, 42838.2286768827], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4508400.0000, 
sim time next is 4509000.0000, 
raw observation next is [-0.8500000000000001, 72.0, 0.0, 0.0, 26.0, 25.21679536259799, 0.412574941342855, 0.0, 1.0, 42104.48246218143], 
processed observation next is [1.0, 0.17391304347826086, 0.43905817174515244, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6013996135498324, 0.6375249804476183, 0.0, 1.0, 0.2004975355341973], 
reward next is 0.7995, 
noisyNet noise sample is [array([-0.33653337], dtype=float32), -0.66941726]. 
=============================================
[2019-04-04 02:14:33,731] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[83.10581]
 [83.10215]
 [83.08401]
 [83.06138]
 [83.05015]], R is [[83.11019897]
 [83.07510376]
 [83.03318787]
 [82.98706818]
 [82.91729736]].
[2019-04-04 02:14:34,418] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.1269771e-25 1.7973917e-18 2.4342174e-23 1.3994569e-19 2.4577811e-20
 2.7422538e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:14:34,471] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8110
[2019-04-04 02:14:34,493] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 52.0, 68.5, 48.0, 26.0, 25.91366781911533, 0.5252840828921194, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4554000.0000, 
sim time next is 4554600.0000, 
raw observation next is [2.0, 52.0, 55.0, 41.33333333333333, 26.0, 26.02367656776397, 0.530022349507092, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.518005540166205, 0.52, 0.18333333333333332, 0.04567219152854511, 0.6666666666666666, 0.668639713980331, 0.676674116502364, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.56114864], dtype=float32), 0.95353657]. 
=============================================
[2019-04-04 02:14:35,040] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.6024428e-25 3.3008181e-18 1.0681353e-22 6.8606980e-20 5.9727900e-19
 2.5899150e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:14:35,040] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4328
[2019-04-04 02:14:35,052] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.8, 65.0, 0.0, 0.0, 26.0, 25.91182146870536, 0.6263789036828692, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4410000.0000, 
sim time next is 4410600.0000, 
raw observation next is [6.683333333333334, 65.16666666666667, 0.0, 0.0, 26.0, 25.91900605327369, 0.6125410570021578, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.6477377654662975, 0.6516666666666667, 0.0, 0.0, 0.6666666666666666, 0.6599171711061409, 0.7041803523340526, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1154317], dtype=float32), 1.5648377]. 
=============================================
[2019-04-04 02:14:35,324] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.5556911e-25 7.1751069e-18 1.4071519e-23 7.7372334e-21 4.7959017e-20
 3.5892158e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:14:35,324] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7825
[2019-04-04 02:14:35,360] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 117.0, 33.0, 26.0, 26.03589725025416, 0.5213216215684114, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4525200.0000, 
sim time next is 4525800.0000, 
raw observation next is [0.1666666666666667, 70.16666666666667, 119.0, 22.0, 26.0, 26.14634576382698, 0.5265995625128417, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4672206832871654, 0.7016666666666667, 0.39666666666666667, 0.02430939226519337, 0.6666666666666666, 0.6788621469855816, 0.6755331875042806, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6603731], dtype=float32), 1.0040668]. 
=============================================
[2019-04-04 02:14:36,484] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.2010177e-25 1.9369232e-18 4.5440752e-23 4.9929068e-20 4.7211518e-20
 2.1950327e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:14:36,484] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6019
[2019-04-04 02:14:36,500] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.5, 37.0, 0.0, 0.0, 26.0, 25.57452880917353, 0.4820590698874079, 0.0, 1.0, 18740.87276500594], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5009400.0000, 
sim time next is 5010000.0000, 
raw observation next is [2.333333333333333, 38.0, 0.0, 0.0, 26.0, 25.56361670766733, 0.474106263523172, 0.0, 1.0, 24070.94910037158], 
processed observation next is [1.0, 1.0, 0.5272391505078486, 0.38, 0.0, 0.0, 0.6666666666666666, 0.6303013923056108, 0.6580354211743907, 0.0, 1.0, 0.11462356714462657], 
reward next is 0.8854, 
noisyNet noise sample is [array([0.20859234], dtype=float32), -0.0040814215]. 
=============================================
[2019-04-04 02:14:36,548] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[82.176605]
 [82.19506 ]
 [82.3191  ]
 [82.304214]
 [82.07842 ]], R is [[82.23590088]
 [82.32430267]
 [82.50106049]
 [82.5867691 ]
 [82.51934814]].
[2019-04-04 02:14:44,251] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:14:44,261] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:14:44,264] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run23
[2019-04-04 02:14:44,427] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.14058519e-24 1.20134766e-17 1.09918238e-22 1.69971114e-19
 4.09015157e-20 3.56961019e-24 1.00000000e+00], sum to 1.0000
[2019-04-04 02:14:44,427] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3635
[2019-04-04 02:14:44,476] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.2000000000000001, 63.66666666666667, 0.0, 0.0, 26.0, 25.42297883314252, 0.4494053147583457, 0.0, 1.0, 36296.55157329138], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4584000.0000, 
sim time next is 4584600.0000, 
raw observation next is [0.1, 64.0, 0.0, 0.0, 26.0, 25.43833897192, 0.4415889131197385, 0.0, 1.0, 26028.23901997651], 
processed observation next is [1.0, 0.043478260869565216, 0.4653739612188367, 0.64, 0.0, 0.0, 0.6666666666666666, 0.6198615809933333, 0.6471963043732462, 0.0, 1.0, 0.12394399533322148], 
reward next is 0.8761, 
noisyNet noise sample is [array([0.6673983], dtype=float32), 1.5956217]. 
=============================================
[2019-04-04 02:14:51,427] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:14:51,427] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:14:51,432] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run23
[2019-04-04 02:14:58,341] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.58676815e-24 3.84381194e-18 1.81794384e-22 3.51584615e-19
 7.67471226e-20 1.10718965e-24 1.00000000e+00], sum to 1.0000
[2019-04-04 02:14:58,341] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9216
[2019-04-04 02:14:58,349] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.6, 44.5, 117.0, 147.0, 26.0, 26.74585545941523, 0.7954558550026151, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4638600.0000, 
sim time next is 4639200.0000, 
raw observation next is [5.466666666666667, 45.0, 104.1666666666667, 146.6666666666667, 26.0, 27.08878488462091, 0.8241220900696012, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.6140350877192983, 0.45, 0.3472222222222223, 0.1620626151012892, 0.6666666666666666, 0.7573987403850758, 0.7747073633565337, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.1206365], dtype=float32), -0.60832936]. 
=============================================
[2019-04-04 02:15:01,705] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:15:01,705] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:15:01,709] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run23
[2019-04-04 02:15:12,788] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.20347981e-25 5.43992642e-19 2.11401359e-23 1.04092874e-19
 6.83958425e-20 4.23959686e-25 1.00000000e+00], sum to 1.0000
[2019-04-04 02:15:12,788] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8565
[2019-04-04 02:15:12,886] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.666666666666667, 31.66666666666667, 0.0, 0.0, 26.0, 25.63286602953966, 0.5740545590701878, 0.0, 1.0, 49659.37578964593], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5001600.0000, 
sim time next is 5002200.0000, 
raw observation next is [3.5, 33.0, 0.0, 0.0, 26.0, 25.69370977139023, 0.5792816883454509, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.5595567867036012, 0.33, 0.0, 0.0, 0.6666666666666666, 0.6411424809491857, 0.6930938961151503, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2294787], dtype=float32), -0.61918247]. 
=============================================
[2019-04-04 02:15:17,544] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:15:17,544] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:15:17,551] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run23
[2019-04-04 02:15:17,741] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:15:17,741] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:15:17,754] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run23
[2019-04-04 02:15:18,961] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:15:18,961] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:15:18,965] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run23
[2019-04-04 02:15:19,011] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:15:19,013] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:15:19,059] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run23
[2019-04-04 02:15:19,141] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:15:19,141] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:15:19,152] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run23
[2019-04-04 02:15:21,241] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:15:21,241] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:15:21,245] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run23
[2019-04-04 02:15:22,153] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:15:22,153] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:15:22,156] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run23
[2019-04-04 02:15:22,706] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.97201374e-25 1.83746140e-19 1.15281182e-23 1.04536905e-20
 1.17767384e-20 1.30916903e-25 1.00000000e+00], sum to 1.0000
[2019-04-04 02:15:22,706] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.2444
[2019-04-04 02:15:22,752] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.333333333333334, 19.0, 0.0, 0.0, 26.0, 27.20477274652416, 0.8799240333733672, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5085600.0000, 
sim time next is 5086200.0000, 
raw observation next is [9.166666666666666, 19.0, 0.0, 0.0, 26.0, 27.1795677232633, 0.8681626140043132, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.7165281625115422, 0.19, 0.0, 0.0, 0.6666666666666666, 0.7649639769386084, 0.7893875380014377, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.90476394], dtype=float32), -0.2429473]. 
=============================================
[2019-04-04 02:15:24,525] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:15:24,526] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:15:24,546] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run23
[2019-04-04 02:15:25,213] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:15:25,213] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:15:25,217] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run23
[2019-04-04 02:15:26,880] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:15:26,880] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:15:26,884] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run23
[2019-04-04 02:15:32,638] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:15:32,638] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:15:32,655] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run23
[2019-04-04 02:15:36,113] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:15:36,113] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:15:36,127] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run23
[2019-04-04 02:15:42,041] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:15:42,041] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:15:42,044] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run23
[2019-04-04 02:15:43,501] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.6402270e-23 5.5690848e-17 1.1488356e-21 1.3455160e-19 1.2547426e-19
 5.3437816e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 02:15:43,501] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.3672
[2019-04-04 02:15:43,558] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.516666666666667, 91.0, 0.0, 0.0, 26.0, 24.25385380400035, 0.1254378336943152, 0.0, 1.0, 41816.43060319208], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 93000.0000, 
sim time next is 93600.0000, 
raw observation next is [-1.7, 91.0, 0.0, 0.0, 26.0, 24.24658010880745, 0.1239493097257282, 0.0, 1.0, 41971.55930925649], 
processed observation next is [1.0, 0.08695652173913043, 0.4155124653739613, 0.91, 0.0, 0.0, 0.6666666666666666, 0.5205483424006209, 0.5413164365752428, 0.0, 1.0, 0.1998645681393166], 
reward next is 0.8001, 
noisyNet noise sample is [array([0.57065594], dtype=float32), 0.8586989]. 
=============================================
[2019-04-04 02:15:49,733] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [8.5885006e-24 1.0785867e-18 1.9401434e-22 1.8074154e-19 9.9649078e-20
 1.4945656e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:15:49,735] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.8254
[2019-04-04 02:15:49,795] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.700000000000001, 61.0, 145.0, 231.9999999999999, 26.0, 25.92424416315448, 0.4446574751965781, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 137400.0000, 
sim time next is 138000.0000, 
raw observation next is [-6.700000000000001, 61.0, 146.5, 169.0, 26.0, 25.90108012763567, 0.4337984057604154, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.2770083102493075, 0.61, 0.48833333333333334, 0.1867403314917127, 0.6666666666666666, 0.6584233439696391, 0.6445994685868052, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.29107335], dtype=float32), 0.12648249]. 
=============================================
[2019-04-04 02:15:49,799] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[75.95289 ]
 [76.0325  ]
 [76.403404]
 [76.9462  ]
 [77.58315 ]], R is [[76.32052612]
 [76.55731964]
 [76.79174805]
 [77.02383423]
 [77.25359344]].
[2019-04-04 02:15:57,288] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2866379e-24 3.2991548e-19 5.0046835e-23 3.5855436e-20 4.3744542e-20
 1.4061960e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:15:57,288] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3528
[2019-04-04 02:15:57,390] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 26.0, 24.93462412523466, 0.2546397250824329, 0.0, 1.0, 50372.2664557042], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 246600.0000, 
sim time next is 247200.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 26.0, 24.89186908730751, 0.2443691973464033, 0.0, 1.0, 46747.75556281266], 
processed observation next is [1.0, 0.8695652173913043, 0.368421052631579, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5743224239422924, 0.5814563991154678, 0.0, 1.0, 0.2226083598229174], 
reward next is 0.7774, 
noisyNet noise sample is [array([-0.32177255], dtype=float32), 1.3387445]. 
=============================================
[2019-04-04 02:16:10,801] A3C_AGENT_WORKER-Thread-8 INFO:Evaluating...
[2019-04-04 02:16:10,804] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 02:16:10,804] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:16:10,806] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run31
[2019-04-04 02:16:10,853] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 02:16:10,854] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:16:10,858] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 02:16:10,870] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:16:10,885] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run31
[2019-04-04 02:16:10,995] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run31
[2019-04-04 02:18:17,753] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.38329312], dtype=float32), 0.16036928]
[2019-04-04 02:18:17,753] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-15.0, 83.0, 0.0, 0.0, 26.0, 22.78028221356607, -0.1963134636358817, 0.0, 1.0, 43241.98676735981]
[2019-04-04 02:18:17,753] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 02:18:17,754] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [7.9945941e-23 2.1875518e-16 3.2058059e-21 1.0537821e-18 1.5619227e-18
 4.7045352e-22 1.0000000e+00], sampled 0.4884598415232493
[2019-04-04 02:18:18,853] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.38329312], dtype=float32), 0.16036928]
[2019-04-04 02:18:18,854] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-8.5, 64.0, 112.0, 781.0, 26.0, 26.03370081062282, 0.4770638060295586, 1.0, 1.0, 0.0]
[2019-04-04 02:18:18,854] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 02:18:18,854] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.5918067e-24 8.9698463e-19 1.7315000e-23 4.9025215e-20 2.6818753e-20
 1.1633830e-24 1.0000000e+00], sampled 0.13618526472713266
[2019-04-04 02:18:29,155] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.38329312], dtype=float32), 0.16036928]
[2019-04-04 02:18:29,156] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-1.0, 59.0, 114.5, 256.0, 26.0, 25.20037266706066, 0.3705312621193666, 1.0, 1.0, 0.0]
[2019-04-04 02:18:29,156] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 02:18:29,156] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [7.1766419e-25 2.4591153e-18 2.0420510e-23 1.9723379e-20 2.7189905e-20
 2.4241619e-24 1.0000000e+00], sampled 0.20178080876372306
[2019-04-04 02:18:47,479] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 02:19:11,309] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 02:19:13,747] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 02:19:14,771] A3C_AGENT_WORKER-Thread-8 INFO:Global step: 3000000, evaluation results [3000000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 02:19:18,713] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.2361825e-25 4.8856865e-18 1.8275731e-23 1.2598115e-20 1.1796162e-20
 7.3453457e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:19:18,713] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2864
[2019-04-04 02:19:18,782] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-15.6, 90.0, 32.0, 607.5, 26.0, 25.27369705323454, 0.2104426870125358, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 378000.0000, 
sim time next is 378600.0000, 
raw observation next is [-15.41666666666667, 86.0, 34.33333333333334, 651.0, 26.0, 25.38356487944959, 0.2122684626246538, 1.0, 1.0, 18801.31329496738], 
processed observation next is [1.0, 0.391304347826087, 0.0355493998153277, 0.86, 0.11444444444444447, 0.7193370165745856, 0.6666666666666666, 0.6152970732874659, 0.5707561542082179, 1.0, 1.0, 0.08953006330936848], 
reward next is 0.9105, 
noisyNet noise sample is [array([-0.05222956], dtype=float32), -0.04280347]. 
=============================================
[2019-04-04 02:19:33,175] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.7981759e-25 1.2289123e-18 3.8402115e-23 1.1405179e-20 6.9983086e-20
 2.6569017e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:19:33,175] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2062
[2019-04-04 02:19:33,201] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.883333333333334, 88.83333333333334, 0.0, 0.0, 26.0, 24.97051669150389, 0.2589283383981768, 0.0, 1.0, 39658.81573584395], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 522600.0000, 
sim time next is 523200.0000, 
raw observation next is [4.766666666666667, 88.66666666666667, 0.0, 0.0, 26.0, 25.01243049554322, 0.2614050786553819, 0.0, 1.0, 39463.55542556795], 
processed observation next is [0.0, 0.043478260869565216, 0.5946445060018468, 0.8866666666666667, 0.0, 0.0, 0.6666666666666666, 0.584369207961935, 0.5871350262184606, 0.0, 1.0, 0.18792169250270455], 
reward next is 0.8121, 
noisyNet noise sample is [array([-0.15992549], dtype=float32), -0.6182893]. 
=============================================
[2019-04-04 02:19:50,335] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.0843512e-25 1.4380757e-19 1.3475526e-23 3.8932735e-20 1.7282758e-20
 1.0322145e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:19:50,335] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8905
[2019-04-04 02:19:50,421] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.333333333333333, 48.0, 70.83333333333334, 7.666666666666665, 26.0, 24.69713283323, 0.3357535940435903, 1.0, 1.0, 197075.4339219436], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 750000.0000, 
sim time next is 750600.0000, 
raw observation next is [-1.7, 49.5, 68.0, 3.0, 26.0, 25.22742281717996, 0.3958715926577653, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.4155124653739613, 0.495, 0.22666666666666666, 0.0033149171270718232, 0.6666666666666666, 0.6022852347649966, 0.6319571975525884, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.62319577], dtype=float32), 0.1431435]. 
=============================================
[2019-04-04 02:19:52,875] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.8560552e-25 2.7083294e-19 2.0367218e-23 1.7724356e-20 2.4679089e-20
 2.0099834e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:19:52,880] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1907
[2019-04-04 02:19:52,945] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.9, 62.5, 0.0, 0.0, 26.0, 24.90347920361462, 0.2770424011899555, 0.0, 1.0, 44053.95495300784], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 768600.0000, 
sim time next is 769200.0000, 
raw observation next is [-6.0, 63.0, 0.0, 0.0, 26.0, 24.84645286189493, 0.2676150435246745, 0.0, 1.0, 43632.70865133088], 
processed observation next is [1.0, 0.9130434782608695, 0.296398891966759, 0.63, 0.0, 0.0, 0.6666666666666666, 0.5705377384912443, 0.5892050145082248, 0.0, 1.0, 0.20777480310157562], 
reward next is 0.7922, 
noisyNet noise sample is [array([-0.18277621], dtype=float32), 0.6453664]. 
=============================================
[2019-04-04 02:20:02,910] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.8050065e-25 8.9861820e-19 2.4374231e-23 6.1754203e-20 2.7359183e-20
 8.7136018e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:20:02,910] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7385
[2019-04-04 02:20:02,970] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.283333333333333, 75.0, 41.66666666666666, 0.0, 26.0, 25.871694127301, 0.3152053168449394, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 809400.0000, 
sim time next is 810000.0000, 
raw observation next is [-6.2, 75.0, 46.5, 0.0, 26.0, 25.79265591348689, 0.3061719363941869, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.2908587257617729, 0.75, 0.155, 0.0, 0.6666666666666666, 0.6493879927905741, 0.6020573121313956, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.203017], dtype=float32), 1.4854485]. 
=============================================
[2019-04-04 02:20:02,999] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[85.987946]
 [86.23656 ]
 [86.55174 ]
 [87.06654 ]
 [87.51763 ]], R is [[86.04266357]
 [86.18223572]
 [86.32041168]
 [86.45720673]
 [86.59263611]].
[2019-04-04 02:20:07,251] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.5512008e-26 5.1604877e-19 5.4020899e-24 8.8499949e-21 6.4996153e-21
 3.8469226e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:20:07,252] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6000
[2019-04-04 02:20:07,279] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.2, 79.83333333333334, 0.0, 0.0, 26.0, 24.79317261947479, 0.2258518015242442, 0.0, 1.0, 39465.43468282282], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 868200.0000, 
sim time next is 868800.0000, 
raw observation next is [-2.1, 79.66666666666667, 0.0, 0.0, 26.0, 24.78651452917165, 0.2196883615292922, 0.0, 1.0, 39436.89078638557], 
processed observation next is [1.0, 0.043478260869565216, 0.404432132963989, 0.7966666666666667, 0.0, 0.0, 0.6666666666666666, 0.5655428774309709, 0.5732294538430974, 0.0, 1.0, 0.18779471803040748], 
reward next is 0.8122, 
noisyNet noise sample is [array([1.5895406], dtype=float32), -0.8486358]. 
=============================================
[2019-04-04 02:20:10,879] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.1075420e-27 3.1694382e-20 8.6831074e-25 5.9244918e-22 4.9174257e-22
 8.1741798e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 02:20:10,880] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2486
[2019-04-04 02:20:10,932] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.7, 92.5, 18.0, 0.0, 26.0, 25.72357548480072, 0.2908680021155182, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 923400.0000, 
sim time next is 924000.0000, 
raw observation next is [4.8, 92.33333333333333, 15.0, 0.0, 26.0, 25.22273504082424, 0.3138752501077078, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.5955678670360112, 0.9233333333333333, 0.05, 0.0, 0.6666666666666666, 0.6018945867353533, 0.6046250833692359, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.9691981], dtype=float32), 1.6027434]. 
=============================================
[2019-04-04 02:20:10,954] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[84.445435]
 [84.45031 ]
 [84.41764 ]
 [84.4018  ]
 [84.45955 ]], R is [[84.56871033]
 [84.72302246]
 [84.87579346]
 [85.02703857]
 [85.17676544]].
[2019-04-04 02:20:12,149] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [8.5689831e-27 1.2977213e-20 1.4481020e-24 2.2314918e-21 5.8215077e-21
 3.9149611e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:20:12,165] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7210
[2019-04-04 02:20:12,204] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.0, 96.0, 0.0, 0.0, 26.0, 25.17542196720087, 0.398189473813938, 0.0, 1.0, 39766.3954230889], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 943200.0000, 
sim time next is 943800.0000, 
raw observation next is [5.0, 96.0, 0.0, 0.0, 26.0, 25.18794576628664, 0.4001979571462781, 0.0, 1.0, 39583.33075645589], 
processed observation next is [1.0, 0.9565217391304348, 0.6011080332409973, 0.96, 0.0, 0.0, 0.6666666666666666, 0.5989954805238865, 0.6333993190487593, 0.0, 1.0, 0.18849205122121854], 
reward next is 0.8115, 
noisyNet noise sample is [array([-0.31848362], dtype=float32), 0.96259683]. 
=============================================
[2019-04-04 02:20:19,157] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.48229875e-27 1.15070904e-20 4.62073796e-25 1.18956145e-21
 1.04589112e-21 6.60577870e-27 1.00000000e+00], sum to 1.0000
[2019-04-04 02:20:19,157] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.4111
[2019-04-04 02:20:19,194] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.0, 100.0, 0.0, 0.0, 26.0, 24.64845458987838, 0.2953339551923961, 0.0, 1.0, 47947.87873243982], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 936600.0000, 
sim time next is 937200.0000, 
raw observation next is [5.0, 100.0, 0.0, 0.0, 26.0, 24.67225078103023, 0.3103696567001642, 0.0, 1.0, 168195.256806504], 
processed observation next is [1.0, 0.8695652173913043, 0.6011080332409973, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5560208984191858, 0.603456552233388, 0.0, 1.0, 0.8009297943166858], 
reward next is 0.1991, 
noisyNet noise sample is [array([-0.9979713], dtype=float32), -0.79297]. 
=============================================
[2019-04-04 02:20:21,429] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.3482189e-26 1.3350095e-19 3.5189097e-24 6.7539135e-22 2.0835207e-21
 1.7207719e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:20:21,429] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7737
[2019-04-04 02:20:21,499] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.2, 96.0, 0.0, 0.0, 26.0, 25.32788201017861, 0.4468540018151462, 0.0, 1.0, 36923.79409380144], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1489200.0000, 
sim time next is 1489800.0000, 
raw observation next is [2.2, 96.0, 0.0, 0.0, 26.0, 25.33085354398818, 0.450198498111988, 0.0, 1.0, 37003.31834619569], 
processed observation next is [1.0, 0.21739130434782608, 0.5235457063711911, 0.96, 0.0, 0.0, 0.6666666666666666, 0.610904461999015, 0.6500661660373294, 0.0, 1.0, 0.1762062778390271], 
reward next is 0.8238, 
noisyNet noise sample is [array([1.3080239], dtype=float32), 1.7469498]. 
=============================================
[2019-04-04 02:20:27,733] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.4154062e-28 1.1724958e-21 1.9457652e-26 6.9756963e-23 1.5073174e-23
 3.1610936e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 02:20:27,736] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5787
[2019-04-04 02:20:27,743] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.83333333333333, 82.66666666666667, 114.8333333333333, 0.0, 26.0, 26.42200516403481, 0.6687112288559719, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 999600.0000, 
sim time next is 1000200.0000, 
raw observation next is [14.11666666666667, 81.83333333333333, 110.6666666666667, 0.0, 26.0, 26.54773120096637, 0.6813825032078479, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.8536472760849494, 0.8183333333333332, 0.368888888888889, 0.0, 0.6666666666666666, 0.7123109334138643, 0.7271275010692827, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.06313851], dtype=float32), -1.2427857]. 
=============================================
[2019-04-04 02:20:44,510] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.0838175e-26 4.0597639e-19 5.1298564e-24 6.6614701e-21 2.8123709e-21
 6.5369994e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:20:44,539] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9027
[2019-04-04 02:20:44,580] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 26.0, 25.3409226810372, 0.4702695879158071, 0.0, 1.0, 43036.52726729905], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1730400.0000, 
sim time next is 1731000.0000, 
raw observation next is [0.5, 92.0, 0.0, 0.0, 26.0, 25.37778900648344, 0.4685460934494231, 0.0, 1.0, 39655.30938154563], 
processed observation next is [0.0, 0.0, 0.4764542936288089, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6148157505402866, 0.6561820311498078, 0.0, 1.0, 0.1888348065787887], 
reward next is 0.8112, 
noisyNet noise sample is [array([1.7676556], dtype=float32), 1.8348933]. 
=============================================
[2019-04-04 02:20:44,639] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[85.77936 ]
 [85.66609 ]
 [85.56662 ]
 [85.42639 ]
 [85.295876]], R is [[85.86889648]
 [85.80527496]
 [85.74206543]
 [85.67925262]
 [85.61676788]].
[2019-04-04 02:20:48,561] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.4961848e-26 3.0257065e-20 3.2979762e-24 9.0827396e-21 4.4218143e-21
 1.6263121e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:20:48,562] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6587
[2019-04-04 02:20:48,639] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 26.0, 24.96706534765517, 0.4576913461369636, 1.0, 1.0, 67410.58191113203], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1367400.0000, 
sim time next is 1368000.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 26.0, 24.90458596037231, 0.4627027194062487, 1.0, 1.0, 84083.71554617138], 
processed observation next is [1.0, 0.8695652173913043, 0.4764542936288089, 0.96, 0.0, 0.0, 0.6666666666666666, 0.5753821633643591, 0.654234239802083, 1.0, 1.0, 0.400398645457959], 
reward next is 0.5996, 
noisyNet noise sample is [array([0.06570783], dtype=float32), -0.5887866]. 
=============================================
[2019-04-04 02:20:48,646] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[85.35877 ]
 [83.845726]
 [84.14449 ]
 [84.84917 ]
 [86.25767 ]], R is [[86.06772614]
 [85.88604736]
 [85.93812561]
 [86.07874298]
 [86.21795654]].
[2019-04-04 02:20:53,737] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.1710714e-26 4.2264654e-19 1.0369594e-23 8.3020742e-21 7.3292104e-21
 1.7942135e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:20:53,737] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0139
[2019-04-04 02:20:53,771] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.04770115056276, 0.4073238246931652, 0.0, 1.0, 38615.83262069663], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1407000.0000, 
sim time next is 1407600.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.05670201850242, 0.4145965825798845, 0.0, 1.0, 38631.91622386652], 
processed observation next is [1.0, 0.30434782608695654, 0.44598337950138506, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5880585015418683, 0.6381988608599615, 0.0, 1.0, 0.18396150582793583], 
reward next is 0.8160, 
noisyNet noise sample is [array([-0.62836194], dtype=float32), 0.5742782]. 
=============================================
[2019-04-04 02:21:04,679] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.5095302e-26 2.0877635e-19 8.0862053e-24 4.2541051e-21 2.4174335e-21
 1.3365289e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:21:04,679] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9940
[2019-04-04 02:21:04,779] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.7000000000000001, 92.0, 22.5, 0.0, 26.0, 25.95731973512394, 0.5913353779505082, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1327200.0000, 
sim time next is 1327800.0000, 
raw observation next is [0.6, 92.0, 27.0, 0.0, 26.0, 26.06283110054968, 0.5932425739570528, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.479224376731302, 0.92, 0.09, 0.0, 0.6666666666666666, 0.6719025917124734, 0.6977475246523509, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.273986], dtype=float32), -0.12636103]. 
=============================================
[2019-04-04 02:21:09,632] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.8533115e-24 5.8012296e-19 9.3464492e-23 3.0010513e-19 9.9564725e-20
 1.0383800e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:21:09,645] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8146
[2019-04-04 02:21:09,702] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 26.0, 25.48591048160988, 0.5700123329346336, 0.0, 1.0, 36108.98609967782], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1374600.0000, 
sim time next is 1375200.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 26.0, 25.44440264307963, 0.5674828395390242, 0.0, 1.0, 59021.13556101803], 
processed observation next is [1.0, 0.9565217391304348, 0.4764542936288089, 0.96, 0.0, 0.0, 0.6666666666666666, 0.6203668869233026, 0.6891609465130081, 0.0, 1.0, 0.28105302648103825], 
reward next is 0.7189, 
noisyNet noise sample is [array([-2.2161298], dtype=float32), 2.4230943]. 
=============================================
[2019-04-04 02:21:21,123] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [6.2122907e-24 1.4061581e-17 3.0186019e-22 1.6091191e-19 1.3644024e-19
 1.7298099e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 02:21:21,123] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8054
[2019-04-04 02:21:21,183] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.199999999999999, 72.33333333333333, 173.3333333333333, 67.5, 26.0, 24.9533912057362, 0.2616909529673233, 0.0, 1.0, 46737.89656451304], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1856400.0000, 
sim time next is 1857000.0000, 
raw observation next is [-5.1, 71.66666666666667, 162.6666666666667, 54.00000000000001, 26.0, 24.9633582471834, 0.2639561247342531, 0.0, 1.0, 40201.99411507563], 
processed observation next is [0.0, 0.4782608695652174, 0.3213296398891967, 0.7166666666666667, 0.5422222222222224, 0.059668508287292824, 0.6666666666666666, 0.58027985393195, 0.5879853749114177, 0.0, 1.0, 0.19143806721464585], 
reward next is 0.8086, 
noisyNet noise sample is [array([-0.9810713], dtype=float32), -0.49191323]. 
=============================================
[2019-04-04 02:21:21,297] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[82.16452 ]
 [82.75741 ]
 [82.81156 ]
 [83.000565]
 [83.20367 ]], R is [[81.5348587 ]
 [81.49694824]
 [81.42948151]
 [81.37081909]
 [81.36419678]].
[2019-04-04 02:21:23,367] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.9540517e-26 1.8848424e-19 3.5416151e-24 6.0556738e-21 4.5134327e-21
 9.9921319e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:21:23,368] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2205
[2019-04-04 02:21:23,430] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.266666666666667, 100.0, 15.0, 0.0, 26.0, 25.39757871601828, 0.4608725881633002, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1498800.0000, 
sim time next is 1499400.0000, 
raw observation next is [1.35, 100.0, 18.0, 0.0, 26.0, 25.56787340515372, 0.4768331339276884, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.5000000000000001, 1.0, 0.06, 0.0, 0.6666666666666666, 0.6306561170961432, 0.6589443779758961, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8683648], dtype=float32), -0.90398943]. 
=============================================
[2019-04-04 02:21:23,547] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.9751091e-27 8.5679966e-21 3.7482023e-25 2.8419518e-22 2.8116722e-22
 7.6217899e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 02:21:23,547] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8872
[2019-04-04 02:21:23,563] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.2, 82.0, 0.0, 0.0, 26.0, 25.5898401825248, 0.583613518286071, 0.0, 1.0, 93418.84051648348], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1638000.0000, 
sim time next is 1638600.0000, 
raw observation next is [7.2, 82.66666666666667, 0.0, 0.0, 26.0, 25.53142472217041, 0.6088248626438326, 0.0, 1.0, 91388.5079616094], 
processed observation next is [1.0, 1.0, 0.662049861495845, 0.8266666666666667, 0.0, 0.0, 0.6666666666666666, 0.6276187268475342, 0.7029416208812775, 0.0, 1.0, 0.43518337124575907], 
reward next is 0.5648, 
noisyNet noise sample is [array([-0.25543877], dtype=float32), 0.74333173]. 
=============================================
[2019-04-04 02:21:33,372] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.6914518e-26 1.8604546e-19 5.5010766e-24 3.7638418e-21 5.4771098e-21
 3.8309893e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:21:33,372] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9772
[2019-04-04 02:21:33,402] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.85, 92.0, 53.0, 0.0, 26.0, 25.88313748051812, 0.5534475131535236, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1675800.0000, 
sim time next is 1676400.0000, 
raw observation next is [1.733333333333333, 92.0, 55.16666666666667, 0.0, 26.0, 25.92848307028386, 0.5577596359608527, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.5106186518928902, 0.92, 0.1838888888888889, 0.0, 0.6666666666666666, 0.660706922523655, 0.6859198786536176, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.01152235], dtype=float32), 1.892659]. 
=============================================
[2019-04-04 02:21:44,487] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.3197622e-24 9.1192063e-18 2.4833374e-22 2.3217236e-19 3.4364986e-19
 5.1942897e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 02:21:44,488] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6925
[2019-04-04 02:21:44,571] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 87.0, 100.6666666666667, 0.0, 26.0, 24.9547816724825, 0.333335327445405, 0.0, 1.0, 33970.08946800172], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1766400.0000, 
sim time next is 1767000.0000, 
raw observation next is [-2.3, 87.0, 104.3333333333333, 0.0, 26.0, 24.92229441355542, 0.3316072296496629, 0.0, 1.0, 56758.80928022896], 
processed observation next is [0.0, 0.43478260869565216, 0.3988919667590028, 0.87, 0.3477777777777777, 0.0, 0.6666666666666666, 0.576857867796285, 0.6105357432165542, 0.0, 1.0, 0.2702800441915665], 
reward next is 0.7297, 
noisyNet noise sample is [array([0.00239238], dtype=float32), 1.7397674]. 
=============================================
[2019-04-04 02:21:44,578] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[79.90491]
 [80.06735]
 [80.16908]
 [80.28608]
 [80.30818]], R is [[79.69922638]
 [79.74047089]
 [79.85383606]
 [79.96604919]
 [79.95641327]].
[2019-04-04 02:21:50,212] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.1801380e-23 3.8873584e-17 1.0113640e-21 1.1313759e-18 5.9400026e-19
 8.8468466e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 02:21:50,212] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8482
[2019-04-04 02:21:50,309] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 83.0, 15.0, 0.0, 26.0, 25.01208707985355, 0.2497854512403863, 0.0, 1.0, 54554.82386625389], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1875600.0000, 
sim time next is 1876200.0000, 
raw observation next is [-4.583333333333333, 83.5, 10.33333333333333, 0.0, 26.0, 25.00623055310931, 0.2501017487024184, 0.0, 1.0, 54314.63767591492], 
processed observation next is [0.0, 0.7391304347826086, 0.3356417359187443, 0.835, 0.03444444444444444, 0.0, 0.6666666666666666, 0.5838525460924426, 0.5833672495674728, 0.0, 1.0, 0.25864113179007103], 
reward next is 0.7414, 
noisyNet noise sample is [array([0.72680074], dtype=float32), -0.7568227]. 
=============================================
[2019-04-04 02:21:58,759] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.4568042e-26 7.7385357e-20 1.8191351e-24 5.3097713e-21 6.2839942e-21
 7.1029375e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:21:58,760] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2262
[2019-04-04 02:21:58,796] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.633333333333334, 65.0, 227.8333333333333, 5.0, 26.0, 25.80596186636981, 0.3295325742032177, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1945200.0000, 
sim time next is 1945800.0000, 
raw observation next is [-4.45, 65.0, 227.0, 4.0, 26.0, 25.67439929789363, 0.3258000371518159, 1.0, 1.0, 42840.61957944637], 
processed observation next is [1.0, 0.5217391304347826, 0.3393351800554017, 0.65, 0.7566666666666667, 0.004419889502762431, 0.6666666666666666, 0.6395332748244691, 0.6086000123839387, 1.0, 1.0, 0.20400295037831606], 
reward next is 0.7960, 
noisyNet noise sample is [array([-0.4218851], dtype=float32), -0.1806462]. 
=============================================
[2019-04-04 02:21:58,853] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.4368722e-25 1.0567121e-19 9.1991547e-24 1.7023642e-20 2.6434699e-21
 1.1314927e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:21:58,853] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7628
[2019-04-04 02:21:58,942] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.3, 62.0, 120.3333333333333, 0.0, 26.0, 25.74158055138917, 0.3446914087737483, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1951800.0000, 
sim time next is 1952400.0000, 
raw observation next is [-3.2, 62.0, 116.1666666666667, 0.0, 26.0, 25.76057828336438, 0.3445323225852374, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.37396121883656513, 0.62, 0.38722222222222236, 0.0, 0.6666666666666666, 0.6467148569470318, 0.6148441075284125, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.038998], dtype=float32), 1.7796013]. 
=============================================
[2019-04-04 02:22:04,451] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.4509449e-23 2.6504829e-17 7.7113287e-22 1.2177869e-18 4.8703324e-19
 1.2591760e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 02:22:04,452] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2929
[2019-04-04 02:22:04,553] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.75, 84.5, 0.0, 0.0, 26.0, 25.03683651255906, 0.2530667625370775, 0.0, 1.0, 32805.10001825666], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1877400.0000, 
sim time next is 1878000.0000, 
raw observation next is [-4.833333333333333, 85.0, 0.0, 0.0, 26.0, 25.03765559609259, 0.2482291340162096, 0.0, 1.0, 38938.4311840258], 
processed observation next is [0.0, 0.7391304347826086, 0.3287165281625116, 0.85, 0.0, 0.0, 0.6666666666666666, 0.5864712996743826, 0.5827430446720698, 0.0, 1.0, 0.18542110087631333], 
reward next is 0.8146, 
noisyNet noise sample is [array([-0.61799425], dtype=float32), -1.2692717]. 
=============================================
[2019-04-04 02:22:04,647] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[77.59209]
 [77.58455]
 [77.7048 ]
 [77.57771]
 [77.36966]], R is [[77.67323303]
 [77.74028778]
 [77.7181015 ]
 [77.68227386]
 [77.6456604 ]].
[2019-04-04 02:22:09,096] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.8333080e-26 8.3496561e-20 1.7639938e-24 1.1608446e-21 1.4581040e-21
 3.1263196e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:22:09,096] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1352
[2019-04-04 02:22:09,146] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 82.0, 0.0, 0.0, 26.0, 26.25482588447422, 0.5320916283008637, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2053800.0000, 
sim time next is 2054400.0000, 
raw observation next is [-3.899999999999999, 82.0, 0.0, 0.0, 26.0, 26.17223772011582, 0.5120061241279094, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.35457063711911363, 0.82, 0.0, 0.0, 0.6666666666666666, 0.6810198100096517, 0.6706687080426365, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6792676], dtype=float32), 0.74319625]. 
=============================================
[2019-04-04 02:22:10,949] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.9699738e-25 2.8131032e-19 1.2487999e-23 2.9727811e-20 1.1439951e-20
 2.7012431e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:22:10,951] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0481
[2019-04-04 02:22:10,994] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 54.66666666666667, 0.0, 0.0, 26.0, 25.45055753928966, 0.4287674455916695, 0.0, 1.0, 18759.99925482402], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2323200.0000, 
sim time next is 2323800.0000, 
raw observation next is [-1.7, 55.0, 0.0, 0.0, 26.0, 25.43301926057366, 0.4261490673601802, 0.0, 1.0, 32336.74252528141], 
processed observation next is [1.0, 0.9130434782608695, 0.4155124653739613, 0.55, 0.0, 0.0, 0.6666666666666666, 0.6194182717144715, 0.6420496891200601, 0.0, 1.0, 0.15398448821562577], 
reward next is 0.8460, 
noisyNet noise sample is [array([0.21684138], dtype=float32), 0.96816516]. 
=============================================
[2019-04-04 02:22:18,549] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [5.1639747e-26 7.4402608e-20 2.8292607e-24 1.3144768e-20 2.6210979e-21
 9.6685387e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:22:18,549] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0135
[2019-04-04 02:22:18,592] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.7833333333333332, 46.0, 187.6666666666667, 66.0, 26.0, 25.27813311453504, 0.4151256246957483, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2296200.0000, 
sim time next is 2296800.0000, 
raw observation next is [-0.6, 45.0, 171.0, 64.5, 26.0, 25.7804699135174, 0.4593240235849318, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.44598337950138506, 0.45, 0.57, 0.0712707182320442, 0.6666666666666666, 0.6483724927931167, 0.6531080078616439, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.206897], dtype=float32), 0.38798437]. 
=============================================
[2019-04-04 02:22:20,583] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.2756990e-25 1.0506588e-19 2.7867148e-24 1.3932074e-21 2.5986971e-21
 3.1872997e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:22:20,583] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0264
[2019-04-04 02:22:20,654] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.533333333333333, 55.33333333333333, 0.0, 0.0, 26.0, 24.9205715311107, 0.3444587517944526, 0.0, 1.0, 83069.90383516529], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2317200.0000, 
sim time next is 2317800.0000, 
raw observation next is [-1.616666666666667, 55.66666666666667, 0.0, 0.0, 26.0, 24.94506129678298, 0.3537184093086396, 1.0, 1.0, 43631.20550018981], 
processed observation next is [1.0, 0.8260869565217391, 0.4178208679593721, 0.5566666666666668, 0.0, 0.0, 0.6666666666666666, 0.5787551080652484, 0.6179061364362132, 1.0, 1.0, 0.2077676452389991], 
reward next is 0.7922, 
noisyNet noise sample is [array([0.8758448], dtype=float32), -2.2992415]. 
=============================================
[2019-04-04 02:22:20,827] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.655446e-25 4.487907e-19 7.444536e-23 3.732474e-20 8.387134e-20
 3.198899e-24 1.000000e+00], sum to 1.0000
[2019-04-04 02:22:20,827] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2111
[2019-04-04 02:22:20,846] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.0, 78.5, 0.0, 0.0, 26.0, 24.31499365597586, 0.1562797242551757, 0.0, 1.0, 42585.33867906682], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2165400.0000, 
sim time next is 2166000.0000, 
raw observation next is [-6.9, 78.33333333333334, 0.0, 0.0, 26.0, 24.30471993289186, 0.1492211344562627, 0.0, 1.0, 42594.85071370676], 
processed observation next is [1.0, 0.043478260869565216, 0.27146814404432135, 0.7833333333333334, 0.0, 0.0, 0.6666666666666666, 0.5253933277409883, 0.5497403781520875, 0.0, 1.0, 0.20283262244622266], 
reward next is 0.7972, 
noisyNet noise sample is [array([-0.17984316], dtype=float32), -0.14476337]. 
=============================================
[2019-04-04 02:22:20,889] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[80.63689 ]
 [80.58573 ]
 [80.570206]
 [80.5244  ]
 [80.498535]], R is [[80.66818237]
 [80.65871429]
 [80.64942932]
 [80.64043427]
 [80.6317215 ]].
[2019-04-04 02:22:30,724] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.24964813e-23 2.03470011e-18 1.69959186e-22 5.79162978e-20
 2.91988035e-19 1.30489245e-23 1.00000000e+00], sum to 1.0000
[2019-04-04 02:22:30,725] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8765
[2019-04-04 02:22:30,770] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.816666666666667, 43.83333333333334, 0.0, 0.0, 26.0, 24.94910534341871, 0.2661221769122331, 0.0, 1.0, 42233.8667567277], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2398200.0000, 
sim time next is 2398800.0000, 
raw observation next is [-1.933333333333334, 43.66666666666667, 0.0, 0.0, 26.0, 24.96709907776706, 0.2645328193126386, 0.0, 1.0, 33224.26308223402], 
processed observation next is [0.0, 0.782608695652174, 0.40904893813481075, 0.4366666666666667, 0.0, 0.0, 0.6666666666666666, 0.5805915898139217, 0.5881776064375462, 0.0, 1.0, 0.15821077658206678], 
reward next is 0.8418, 
noisyNet noise sample is [array([-2.2568574], dtype=float32), 0.012801803]. 
=============================================
[2019-04-04 02:22:45,316] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.7119020e-23 8.2723752e-17 7.3024993e-22 1.9478366e-19 3.2180862e-19
 1.7171024e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 02:22:45,316] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2360
[2019-04-04 02:22:45,357] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.60546972680335, 0.2032493892234127, 0.0, 1.0, 39533.57893524713], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2343600.0000, 
sim time next is 2344200.0000, 
raw observation next is [-2.383333333333333, 62.5, 0.0, 0.0, 26.0, 24.56845591603492, 0.1962069693745433, 0.0, 1.0, 39645.55916907276], 
processed observation next is [0.0, 0.13043478260869565, 0.3965835641735919, 0.625, 0.0, 0.0, 0.6666666666666666, 0.5473713263362434, 0.5654023231248478, 0.0, 1.0, 0.18878837699558457], 
reward next is 0.8112, 
noisyNet noise sample is [array([-0.27384394], dtype=float32), -0.027417358]. 
=============================================
[2019-04-04 02:22:46,899] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.0940754e-24 3.9940358e-18 1.6746086e-22 1.7101106e-19 3.8222134e-19
 1.3346859e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 02:22:46,908] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8016
[2019-04-04 02:22:46,948] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.78279101768214, 0.2492490475083805, 0.0, 1.0, 38610.86455626436], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2338200.0000, 
sim time next is 2338800.0000, 
raw observation next is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.72399014774924, 0.2395209467274793, 0.0, 1.0, 38721.5800340112], 
processed observation next is [0.0, 0.043478260869565216, 0.3988919667590028, 0.62, 0.0, 0.0, 0.6666666666666666, 0.5603325123124367, 0.5798403155758264, 0.0, 1.0, 0.18438847635243427], 
reward next is 0.8156, 
noisyNet noise sample is [array([-0.53311336], dtype=float32), 2.2788796]. 
=============================================
[2019-04-04 02:22:54,714] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.7511776e-25 6.8075191e-19 1.7033296e-23 9.5177582e-21 8.7937272e-21
 4.3926699e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:22:54,714] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.6137
[2019-04-04 02:22:54,795] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.9, 76.33333333333334, 79.66666666666667, 15.16666666666666, 26.0, 25.70691299273188, 0.3579626825851049, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2623200.0000, 
sim time next is 2623800.0000, 
raw observation next is [-6.800000000000001, 75.66666666666666, 82.33333333333333, 30.33333333333333, 26.0, 25.86143795045314, 0.3669347187715784, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.2742382271468144, 0.7566666666666666, 0.27444444444444444, 0.03351749539594843, 0.6666666666666666, 0.6551198292044283, 0.6223115729238594, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.14130628], dtype=float32), 0.7463218]. 
=============================================
[2019-04-04 02:23:17,970] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.9191251e-25 2.0935706e-18 1.1883719e-23 6.9103113e-21 1.0232042e-20
 1.7509465e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:23:17,970] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1152
[2019-04-04 02:23:18,015] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.85, 70.0, 93.0, 91.0, 26.0, 25.85318982308765, 0.3635993866020142, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2626200.0000, 
sim time next is 2626800.0000, 
raw observation next is [-5.566666666666666, 68.33333333333333, 102.8333333333333, 121.6666666666667, 26.0, 25.87453131449431, 0.3648260994312356, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.3084025854108957, 0.6833333333333332, 0.3427777777777777, 0.134438305709024, 0.6666666666666666, 0.6562109428745257, 0.6216086998104119, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1153349], dtype=float32), -1.3874905]. 
=============================================
[2019-04-04 02:23:18,124] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.1974024e-25 8.1119270e-20 3.6944881e-24 2.8316095e-20 2.1014487e-21
 7.1210061e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:23:18,125] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8913
[2019-04-04 02:23:18,186] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.833333333333333, 33.0, 210.1666666666667, 98.66666666666666, 26.0, 25.66116714128086, 0.3193469954104961, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2551200.0000, 
sim time next is 2551800.0000, 
raw observation next is [2.016666666666667, 31.5, 202.3333333333333, 175.3333333333333, 26.0, 25.66112833861658, 0.3144622210515167, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.5184672206832872, 0.315, 0.6744444444444443, 0.1937384898710865, 0.6666666666666666, 0.6384273615513818, 0.6048207403505056, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.36152038], dtype=float32), 0.6855147]. 
=============================================
[2019-04-04 02:23:20,140] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.9330840e-25 2.7725347e-19 2.1027215e-23 4.2113351e-20 1.7939032e-20
 7.6301171e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:23:20,141] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7842
[2019-04-04 02:23:20,180] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.15, 67.0, 0.0, 0.0, 26.0, 25.46109691301452, 0.4427315418962319, 0.0, 1.0, 31339.54885192225], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2669400.0000, 
sim time next is 2670000.0000, 
raw observation next is [-2.466666666666666, 67.66666666666667, 0.0, 0.0, 26.0, 25.42180531203391, 0.4396589798987451, 0.0, 1.0, 57519.66473177991], 
processed observation next is [1.0, 0.9130434782608695, 0.39427516158818104, 0.6766666666666667, 0.0, 0.0, 0.6666666666666666, 0.6184837760028259, 0.6465529932995817, 0.0, 1.0, 0.2739031653894281], 
reward next is 0.7261, 
noisyNet noise sample is [array([0.07411733], dtype=float32), -0.19487034]. 
=============================================
[2019-04-04 02:23:20,190] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[81.493515]
 [81.75124 ]
 [81.813736]
 [81.89649 ]
 [81.7065  ]], R is [[81.40390778]
 [81.44062805]
 [81.53690338]
 [81.72153473]
 [81.68061066]].
[2019-04-04 02:23:20,668] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.6049839e-24 7.1426875e-19 3.3019685e-23 6.4427928e-20 2.8665618e-20
 5.5769493e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:23:20,669] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1201
[2019-04-04 02:23:20,769] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 50.0, 41.0, 103.0, 26.0, 25.79891836892181, 0.4668598677144953, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2653200.0000, 
sim time next is 2653800.0000, 
raw observation next is [0.3166666666666667, 50.66666666666667, 29.66666666666666, 96.0, 26.0, 26.04550509568164, 0.4821419723976383, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.47137580794090495, 0.5066666666666667, 0.09888888888888887, 0.10607734806629834, 0.6666666666666666, 0.6704587579734701, 0.6607139907992128, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.55565745], dtype=float32), 0.8033518]. 
=============================================
[2019-04-04 02:23:32,086] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1194036e-24 4.9373387e-19 9.7867037e-23 6.9495715e-20 1.2073557e-19
 9.7724424e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:23:32,115] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1249
[2019-04-04 02:23:32,137] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 25.11264152969877, 0.3409766793940275, 0.0, 1.0, 46240.5534046117], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2760000.0000, 
sim time next is 2760600.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 25.04353643760982, 0.3293582275861105, 0.0, 1.0, 45928.1318316438], 
processed observation next is [1.0, 0.9565217391304348, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5869613698008184, 0.6097860758620368, 0.0, 1.0, 0.21870538967449427], 
reward next is 0.7813, 
noisyNet noise sample is [array([-0.09041098], dtype=float32), -0.83915925]. 
=============================================
[2019-04-04 02:23:32,290] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.7049083e-24 3.8201853e-20 4.0581763e-23 3.5540190e-20 4.7789174e-20
 4.0881602e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:23:32,291] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.4885
[2019-04-04 02:23:32,367] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 59.0, 0.0, 0.0, 26.0, 25.05547765824042, 0.4105037993891509, 1.0, 1.0, 87055.91005170991], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2749800.0000, 
sim time next is 2750400.0000, 
raw observation next is [-5.0, 59.0, 0.0, 0.0, 26.0, 25.11392675087005, 0.4170734548615414, 0.0, 1.0, 37701.46491945855], 
processed observation next is [1.0, 0.8695652173913043, 0.32409972299168976, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5928272292391709, 0.6390244849538471, 0.0, 1.0, 0.179530785330755], 
reward next is 0.8205, 
noisyNet noise sample is [array([0.6444603], dtype=float32), 1.0843364]. 
=============================================
[2019-04-04 02:23:34,083] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.3946988e-25 7.0214393e-20 1.8169021e-24 1.9691577e-20 3.2663797e-21
 1.0432391e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:23:34,086] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4527
[2019-04-04 02:23:34,150] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.1, 58.0, 224.0, 171.0, 26.0, 25.73347640159336, 0.3870077172292539, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2633400.0000, 
sim time next is 2634000.0000, 
raw observation next is [-2.833333333333333, 56.66666666666667, 227.5, 167.0, 26.0, 25.71635767182471, 0.3867700690522657, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.3841181902123731, 0.5666666666666668, 0.7583333333333333, 0.18453038674033148, 0.6666666666666666, 0.6430298059853925, 0.6289233563507552, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.04212023], dtype=float32), -0.1667486]. 
=============================================
[2019-04-04 02:23:34,152] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[85.22971 ]
 [85.356514]
 [85.158745]
 [84.95835 ]
 [85.18818 ]], R is [[84.94374847]
 [85.09431458]
 [85.00609589]
 [84.76953888]
 [84.92184448]].
[2019-04-04 02:23:41,175] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.2790197e-25 8.0642891e-20 3.5153160e-23 4.1291719e-20 2.7224569e-20
 2.6481577e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:23:41,175] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1530
[2019-04-04 02:23:41,180] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.5, 100.0, 0.0, 0.0, 26.0, 26.19018312908312, 0.7254706489325341, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3180600.0000, 
sim time next is 3181200.0000, 
raw observation next is [3.333333333333333, 100.0, 0.0, 0.0, 26.0, 26.05818267898679, 0.7036096481318066, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.5549399815327793, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6715152232488991, 0.7345365493772689, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8611073], dtype=float32), -0.33358073]. 
=============================================
[2019-04-04 02:23:48,155] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.8298803e-25 6.6721041e-20 1.2190341e-23 2.8165164e-20 5.0073241e-21
 2.3032446e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:23:48,155] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2095
[2019-04-04 02:23:48,271] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 100.0, 78.0, 27.0, 26.0, 25.92084114481721, 0.3538867630415987, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2908800.0000, 
sim time next is 2909400.0000, 
raw observation next is [2.0, 98.83333333333334, 75.66666666666666, 36.00000000000001, 26.0, 25.44042317157564, 0.3764351671068822, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.518005540166205, 0.9883333333333334, 0.2522222222222222, 0.03977900552486189, 0.6666666666666666, 0.6200352642979702, 0.6254783890356274, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.1015108], dtype=float32), 0.42632696]. 
=============================================
[2019-04-04 02:23:49,471] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.4215554e-24 2.7686903e-17 5.1116364e-22 5.4715185e-19 4.8439847e-19
 1.4643855e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 02:23:49,471] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7887
[2019-04-04 02:23:49,497] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 84.0, 0.0, 0.0, 26.0, 24.28780752491831, 0.1935089505011843, 0.0, 1.0, 42892.14186863689], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2955600.0000, 
sim time next is 2956200.0000, 
raw observation next is [-3.166666666666667, 82.83333333333333, 0.0, 0.0, 26.0, 24.2849136261236, 0.1887786813028078, 0.0, 1.0, 42819.63314037053], 
processed observation next is [0.0, 0.21739130434782608, 0.3748845798707295, 0.8283333333333333, 0.0, 0.0, 0.6666666666666666, 0.5237428021769667, 0.562926227100936, 0.0, 1.0, 0.2039030149541454], 
reward next is 0.7961, 
noisyNet noise sample is [array([-0.91614777], dtype=float32), -0.047390655]. 
=============================================
[2019-04-04 02:23:53,315] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.7350120e-25 2.6542456e-18 2.0665960e-22 1.2273329e-19 1.5433598e-19
 4.7808769e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:23:53,316] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9829
[2019-04-04 02:23:53,330] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.666666666666667, 69.16666666666667, 0.0, 0.0, 26.0, 24.80934858496982, 0.2319241356713299, 0.0, 1.0, 42730.73512258197], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3388200.0000, 
sim time next is 3388800.0000, 
raw observation next is [-4.333333333333334, 67.33333333333334, 0.0, 0.0, 26.0, 24.72145400173088, 0.2231336036753243, 0.0, 1.0, 42664.5514999299], 
processed observation next is [1.0, 0.21739130434782608, 0.3425669436749769, 0.6733333333333335, 0.0, 0.0, 0.6666666666666666, 0.5601211668109066, 0.5743778678917747, 0.0, 1.0, 0.20316453095204712], 
reward next is 0.7968, 
noisyNet noise sample is [array([1.0121552], dtype=float32), -0.44408393]. 
=============================================
[2019-04-04 02:23:59,429] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.9877373e-25 4.9947952e-19 3.4898361e-23 8.8628331e-21 2.5419979e-20
 5.2352434e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:23:59,429] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2283
[2019-04-04 02:23:59,478] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.333333333333333, 68.66666666666667, 0.0, 0.0, 26.0, 25.21645676288252, 0.3464976608157298, 0.0, 1.0, 42421.50907439947], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2850000.0000, 
sim time next is 2850600.0000, 
raw observation next is [1.166666666666667, 70.33333333333333, 0.0, 0.0, 26.0, 25.20506202046645, 0.3257530326318893, 0.0, 1.0, 46611.55610209408], 
processed observation next is [1.0, 1.0, 0.49492151431209613, 0.7033333333333333, 0.0, 0.0, 0.6666666666666666, 0.6004218350388708, 0.6085843442106298, 0.0, 1.0, 0.22195979096235277], 
reward next is 0.7780, 
noisyNet noise sample is [array([0.1336115], dtype=float32), -0.06987796]. 
=============================================
[2019-04-04 02:24:02,525] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.4191696e-27 7.6352866e-21 1.9588961e-25 6.1092453e-22 2.8978605e-22
 4.0972738e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 02:24:02,525] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9030
[2019-04-04 02:24:02,543] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 49.0, 113.0, 806.0, 26.0, 26.07264829613202, 0.5982876086554513, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3418200.0000, 
sim time next is 3418800.0000, 
raw observation next is [3.0, 49.0, 111.3333333333333, 800.8333333333334, 26.0, 26.29847878225935, 0.6139510776248985, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5457063711911359, 0.49, 0.371111111111111, 0.8848987108655617, 0.6666666666666666, 0.6915398985216127, 0.7046503592082995, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8260321], dtype=float32), -0.6127416]. 
=============================================
[2019-04-04 02:24:06,839] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.6115765e-27 7.9993354e-20 7.7049366e-25 3.2079452e-22 4.0101036e-22
 1.3223740e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:24:06,867] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6906
[2019-04-04 02:24:06,928] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.0, 100.0, 42.0, 237.0, 26.0, 25.37600581538433, 0.3723072969966096, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3139200.0000, 
sim time next is 3139800.0000, 
raw observation next is [6.166666666666666, 100.0, 55.66666666666668, 288.6666666666667, 26.0, 25.60880831148994, 0.3830750740090569, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.6334256694367498, 1.0, 0.18555555555555558, 0.31896869244935544, 0.6666666666666666, 0.6340673592908285, 0.6276916913363523, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5861884], dtype=float32), 1.869341]. 
=============================================
[2019-04-04 02:24:22,026] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.6160108e-24 5.7343241e-18 4.1843622e-23 2.1717826e-20 5.3178217e-20
 1.7546124e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 02:24:22,026] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3065
[2019-04-04 02:24:22,047] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.666666666666668, 28.66666666666667, 0.0, 0.0, 26.0, 25.47199203057536, 0.3508202266334964, 0.0, 1.0, 42492.89646972158], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3655200.0000, 
sim time next is 3655800.0000, 
raw observation next is [8.5, 29.5, 4.0, 121.0, 26.0, 25.45632849017798, 0.3605036363095248, 0.0, 1.0, 44497.7843921971], 
processed observation next is [0.0, 0.30434782608695654, 0.698060941828255, 0.295, 0.013333333333333334, 0.13370165745856355, 0.6666666666666666, 0.6213607075148317, 0.6201678787698416, 0.0, 1.0, 0.21189421139141476], 
reward next is 0.7881, 
noisyNet noise sample is [array([0.61767673], dtype=float32), 2.056567]. 
=============================================
[2019-04-04 02:24:25,547] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.3752378e-26 1.3648709e-19 2.7453966e-23 1.5660555e-20 5.9236363e-21
 3.6787495e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:24:25,548] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5205
[2019-04-04 02:24:25,568] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.5, 96.5, 0.0, 0.0, 26.0, 25.45468231894926, 0.5992649334300142, 0.0, 1.0, 56185.37329824246], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3198600.0000, 
sim time next is 3199200.0000, 
raw observation next is [1.333333333333333, 97.66666666666666, 0.0, 0.0, 26.0, 25.52024396892082, 0.6047160110945988, 0.0, 1.0, 18749.87550455641], 
processed observation next is [1.0, 0.0, 0.4995383194829178, 0.9766666666666666, 0.0, 0.0, 0.6666666666666666, 0.6266869974100683, 0.7015720036981996, 0.0, 1.0, 0.08928512145026862], 
reward next is 0.9107, 
noisyNet noise sample is [array([-1.1168905], dtype=float32), 0.28984153]. 
=============================================
[2019-04-04 02:24:34,522] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.03100244e-25 1.40892123e-20 1.18416739e-24 3.20684555e-21
 1.87597454e-21 2.02093823e-26 1.00000000e+00], sum to 1.0000
[2019-04-04 02:24:34,522] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8310
[2019-04-04 02:24:34,542] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 52.0, 104.6666666666667, 780.1666666666667, 26.0, 25.43787558424417, 0.519381494974024, 1.0, 1.0, 63235.66746092769], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3421200.0000, 
sim time next is 3421800.0000, 
raw observation next is [3.0, 53.5, 103.0, 775.0, 26.0, 25.74933877338393, 0.5707398082156591, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.535, 0.3433333333333333, 0.856353591160221, 0.6666666666666666, 0.6457782311153274, 0.690246602738553, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.00816197], dtype=float32), -1.5110886]. 
=============================================
[2019-04-04 02:24:34,918] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.6587659e-25 1.8248252e-19 1.3523263e-23 9.1801336e-21 1.5789526e-20
 3.1184956e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:24:34,919] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1393
[2019-04-04 02:24:34,942] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.833333333333333, 64.16666666666667, 0.0, 0.0, 26.0, 25.38803020516873, 0.4472232307313223, 0.0, 1.0, 55049.94742317206], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3887400.0000, 
sim time next is 3888000.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.35363681414955, 0.443614038523537, 0.0, 1.0, 56405.5721779751], 
processed observation next is [1.0, 0.0, 0.40720221606648205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6128030678457957, 0.6478713461745124, 0.0, 1.0, 0.2685979627522624], 
reward next is 0.7314, 
noisyNet noise sample is [array([-0.56779045], dtype=float32), -0.045549892]. 
=============================================
[2019-04-04 02:24:34,949] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[85.51099 ]
 [85.45501 ]
 [85.54714 ]
 [85.667854]
 [85.73564 ]], R is [[85.2824173 ]
 [85.16744995]
 [85.17967224]
 [85.23852539]
 [85.28013611]].
[2019-04-04 02:24:41,052] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.4253525e-26 2.7612381e-21 9.3018324e-26 1.1834963e-21 3.9175234e-22
 1.3899437e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 02:24:41,053] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6443
[2019-04-04 02:24:41,071] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 52.0, 115.5, 814.5, 26.0, 26.19217129971147, 0.6377806402552485, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3502800.0000, 
sim time next is 3503400.0000, 
raw observation next is [2.166666666666667, 51.5, 115.3333333333333, 811.6666666666666, 26.0, 26.24545065052456, 0.6549440281957789, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5226223453370269, 0.515, 0.3844444444444443, 0.8968692449355432, 0.6666666666666666, 0.6871208875437134, 0.7183146760652597, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.28512537], dtype=float32), 0.1429136]. 
=============================================
[2019-04-04 02:24:46,772] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.2202171e-26 9.6748770e-20 6.6245383e-24 4.4499720e-21 1.0527404e-20
 1.0004108e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:24:46,773] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9292
[2019-04-04 02:24:46,797] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 67.0, 0.0, 0.0, 26.0, 25.73247613922467, 0.4904372516254664, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3434400.0000, 
sim time next is 3435000.0000, 
raw observation next is [1.833333333333333, 69.0, 0.0, 0.0, 26.0, 25.36164299137058, 0.4461176489254406, 1.0, 1.0, 97268.85813151131], 
processed observation next is [1.0, 0.782608695652174, 0.5133887349953832, 0.69, 0.0, 0.0, 0.6666666666666666, 0.6134702492808817, 0.6487058829751469, 1.0, 1.0, 0.4631850387214824], 
reward next is 0.5368, 
noisyNet noise sample is [array([0.03352491], dtype=float32), 0.75180346]. 
=============================================
[2019-04-04 02:24:46,807] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[82.181915]
 [81.77494 ]
 [82.51805 ]
 [82.22529 ]
 [82.42013 ]], R is [[82.53499603]
 [82.70964813]
 [82.8825531 ]
 [83.0537262 ]
 [83.22319031]].
[2019-04-04 02:24:48,546] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.8205076e-26 6.5592507e-19 1.4221703e-23 4.5347289e-21 1.0772369e-20
 9.1150008e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:24:48,546] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9622
[2019-04-04 02:24:48,584] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.333333333333333, 61.83333333333333, 0.0, 0.0, 26.0, 24.82780483731202, 0.232546028724875, 0.0, 1.0, 42627.99051662445], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3390600.0000, 
sim time next is 3391200.0000, 
raw observation next is [-3.0, 60.0, 0.0, 0.0, 26.0, 24.76576953807101, 0.224006998512049, 0.0, 1.0, 42771.63906723134], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.6, 0.0, 0.0, 0.6666666666666666, 0.563814128172584, 0.5746689995040163, 0.0, 1.0, 0.2036744717487207], 
reward next is 0.7963, 
noisyNet noise sample is [array([-0.13511659], dtype=float32), -0.435999]. 
=============================================
[2019-04-04 02:24:54,892] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.7531435e-26 2.2852480e-20 1.0074157e-24 3.6799642e-21 1.0073404e-21
 2.0054101e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:24:54,893] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5515
[2019-04-04 02:24:54,918] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.333333333333333, 32.33333333333334, 107.6666666666667, 800.0, 26.0, 26.86202403625707, 0.7233813160644081, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4112400.0000, 
sim time next is 4113000.0000, 
raw observation next is [3.5, 33.0, 106.0, 794.0, 26.0, 26.9785602324479, 0.7308986913131911, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5595567867036012, 0.33, 0.35333333333333333, 0.8773480662983425, 0.6666666666666666, 0.7482133527039917, 0.743632897104397, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2726148], dtype=float32), -0.5921321]. 
=============================================
[2019-04-04 02:24:54,978] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[85.50197 ]
 [85.48885 ]
 [85.469185]
 [85.60466 ]
 [85.911545]], R is [[85.60546112]
 [85.74940491]
 [85.89191437]
 [86.03299713]
 [86.17266846]].
[2019-04-04 02:24:55,256] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.9120372e-25 2.4858434e-19 8.0436309e-23 4.2003931e-20 4.6119623e-20
 1.1686226e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:24:55,310] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3062
[2019-04-04 02:24:55,327] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.37519670135785, 0.4701609823222057, 0.0, 1.0, 45654.86564195518], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3542400.0000, 
sim time next is 3543000.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.38696820569692, 0.4662736114178569, 0.0, 1.0, 38089.36704709761], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.6, 0.0, 0.0, 0.6666666666666666, 0.6155806838080767, 0.6554245371392856, 0.0, 1.0, 0.1813779383195124], 
reward next is 0.8186, 
noisyNet noise sample is [array([-0.13067165], dtype=float32), 0.2720008]. 
=============================================
[2019-04-04 02:24:55,366] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[81.287445]
 [81.48118 ]
 [81.57723 ]
 [81.661896]
 [81.71359 ]], R is [[81.1625824 ]
 [81.13355255]
 [81.13393402]
 [81.14692688]
 [81.1280899 ]].
[2019-04-04 02:24:58,356] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.3418784e-24 6.0366747e-19 3.0175526e-23 2.8444819e-20 3.4556763e-20
 6.9143052e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:24:58,356] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.6722
[2019-04-04 02:24:58,390] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.0, 45.0, 0.0, 0.0, 26.0, 25.52367922381159, 0.5166667179165784, 0.0, 1.0, 41837.16568550421], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3963600.0000, 
sim time next is 3964200.0000, 
raw observation next is [-7.166666666666667, 45.66666666666666, 0.0, 0.0, 26.0, 25.50887045707706, 0.5117555441151449, 0.0, 1.0, 46558.24047541543], 
processed observation next is [1.0, 0.9130434782608695, 0.26408125577100644, 0.45666666666666655, 0.0, 0.0, 0.6666666666666666, 0.6257392047564215, 0.670585181371715, 0.0, 1.0, 0.22170590702578774], 
reward next is 0.7783, 
noisyNet noise sample is [array([-1.0433778], dtype=float32), -0.2669042]. 
=============================================
[2019-04-04 02:25:03,435] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.0852509e-24 1.8152528e-17 2.5298065e-22 5.2679107e-20 1.3794905e-19
 9.9817797e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:25:03,447] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7113
[2019-04-04 02:25:03,477] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 77.0, 0.0, 0.0, 26.0, 24.74315527452878, 0.2320711191616457, 0.0, 1.0, 43000.82566608274], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3825600.0000, 
sim time next is 3826200.0000, 
raw observation next is [-5.0, 77.0, 0.0, 0.0, 26.0, 24.66972490126844, 0.2347309981558527, 0.0, 1.0, 42926.2577418834], 
processed observation next is [1.0, 0.2608695652173913, 0.32409972299168976, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5558104084390368, 0.5782436660519509, 0.0, 1.0, 0.2044107511518257], 
reward next is 0.7956, 
noisyNet noise sample is [array([0.41139793], dtype=float32), -0.054018486]. 
=============================================
[2019-04-04 02:25:06,900] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.5792248e-26 2.4606798e-20 3.9498248e-25 1.7218588e-21 6.1354966e-22
 2.5142337e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 02:25:06,904] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0028
[2019-04-04 02:25:06,915] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 45.0, 75.5, 634.0, 26.0, 26.88206741560917, 0.7516072563440529, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3859200.0000, 
sim time next is 3859800.0000, 
raw observation next is [3.0, 44.33333333333334, 71.66666666666666, 606.3333333333333, 26.0, 26.94171408729989, 0.7489511450347824, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.5457063711911359, 0.4433333333333334, 0.23888888888888885, 0.6699815837937384, 0.6666666666666666, 0.7451428406083241, 0.7496503816782608, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8613901], dtype=float32), 1.3018893]. 
=============================================
[2019-04-04 02:25:07,396] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.7587705e-25 3.9735507e-18 1.3947640e-23 5.7604931e-21 1.7003134e-20
 2.9757476e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:25:07,397] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8657
[2019-04-04 02:25:07,442] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.166666666666666, 31.16666666666667, 32.33333333333333, 215.0, 26.0, 25.46169849063133, 0.3835480248315149, 0.0, 1.0, 29651.87678953702], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3657000.0000, 
sim time next is 3657600.0000, 
raw observation next is [8.0, 32.0, 46.5, 262.0, 26.0, 25.48169022511965, 0.3969666850992357, 0.0, 1.0, 18753.17327691974], 
processed observation next is [0.0, 0.34782608695652173, 0.6842105263157896, 0.32, 0.155, 0.28950276243093925, 0.6666666666666666, 0.6234741854266375, 0.6323222283664119, 0.0, 1.0, 0.08930082512818924], 
reward next is 0.9107, 
noisyNet noise sample is [array([0.9356629], dtype=float32), 0.38959044]. 
=============================================
[2019-04-04 02:25:12,813] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.5079960e-23 1.2754566e-17 1.2176875e-21 1.8017293e-19 7.4164616e-19
 5.5285966e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 02:25:12,839] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3534
[2019-04-04 02:25:12,909] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.166666666666666, 59.83333333333334, 0.0, 0.0, 26.0, 24.90839451530307, 0.295918252242036, 0.0, 1.0, 41901.63964919668], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3910200.0000, 
sim time next is 3910800.0000, 
raw observation next is [-6.333333333333333, 60.66666666666667, 0.0, 0.0, 26.0, 24.92799566016194, 0.2920715495732042, 0.0, 1.0, 42025.06065439757], 
processed observation next is [1.0, 0.2608695652173913, 0.28716528162511545, 0.6066666666666667, 0.0, 0.0, 0.6666666666666666, 0.5773329716801617, 0.5973571831910681, 0.0, 1.0, 0.20011933644951221], 
reward next is 0.7999, 
noisyNet noise sample is [array([-0.14251304], dtype=float32), -1.8264678]. 
=============================================
[2019-04-04 02:25:15,424] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.5840460e-24 6.7539533e-18 6.9921873e-23 3.8002148e-20 5.5369628e-20
 8.5401490e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:25:15,425] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4189
[2019-04-04 02:25:15,448] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.166666666666667, 46.66666666666667, 0.0, 0.0, 26.0, 25.33295408909064, 0.3944931054136555, 0.0, 1.0, 39282.31894555877], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4155000.0000, 
sim time next is 4155600.0000, 
raw observation next is [-2.333333333333333, 47.33333333333334, 0.0, 0.0, 26.0, 25.31743381507285, 0.3889214039918867, 0.0, 1.0, 39291.80162893431], 
processed observation next is [0.0, 0.08695652173913043, 0.3979686057248385, 0.47333333333333344, 0.0, 0.0, 0.6666666666666666, 0.6097861512560708, 0.6296404679972956, 0.0, 1.0, 0.18710381728063954], 
reward next is 0.8129, 
noisyNet noise sample is [array([-0.43661752], dtype=float32), 1.3617833]. 
=============================================
[2019-04-04 02:25:17,023] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.5949338e-25 6.0992448e-18 8.7066326e-23 4.3404616e-20 1.2409857e-19
 6.5860873e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:25:17,024] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5742
[2019-04-04 02:25:17,074] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.36296147503767, 0.3459447152278023, 0.0, 1.0, 41053.61386748541], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3727800.0000, 
sim time next is 3728400.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.40751156960319, 0.3433931914747304, 0.0, 1.0, 21731.00274658688], 
processed observation next is [1.0, 0.13043478260869565, 0.3795013850415513, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6172926308002659, 0.6144643971582434, 0.0, 1.0, 0.10348096545993753], 
reward next is 0.8965, 
noisyNet noise sample is [array([1.5637238], dtype=float32), 0.40409818]. 
=============================================
[2019-04-04 02:25:23,282] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.1684699e-26 4.1004603e-21 2.0791575e-25 4.0131490e-22 7.9940801e-22
 6.2849103e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 02:25:23,286] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5207
[2019-04-04 02:25:23,335] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.5, 21.0, 89.0, 712.0, 26.0, 27.08656336474501, 0.7770805591007397, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4030200.0000, 
sim time next is 4030800.0000, 
raw observation next is [-1.333333333333333, 21.33333333333334, 85.33333333333334, 684.6666666666667, 26.0, 27.15256256693188, 0.783401847490123, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.42566943674976926, 0.2133333333333334, 0.2844444444444445, 0.7565377532228362, 0.6666666666666666, 0.7627135472443234, 0.7611339491633743, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.20666526], dtype=float32), 0.6660783]. 
=============================================
[2019-04-04 02:25:25,440] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.8573724e-24 1.9127369e-18 1.9834875e-22 6.8217094e-20 1.2031625e-19
 1.6071528e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 02:25:25,440] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2264
[2019-04-04 02:25:25,468] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 77.0, 0.0, 0.0, 26.0, 24.78271792494645, 0.2434241498254154, 0.0, 1.0, 43077.44810549774], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3825000.0000, 
sim time next is 3825600.0000, 
raw observation next is [-5.0, 77.0, 0.0, 0.0, 26.0, 24.74315527452878, 0.2320711191616457, 0.0, 1.0, 43000.82566608274], 
processed observation next is [1.0, 0.2608695652173913, 0.32409972299168976, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5619296062107318, 0.5773570397205486, 0.0, 1.0, 0.2047658365051559], 
reward next is 0.7952, 
noisyNet noise sample is [array([-0.7956688], dtype=float32), -0.48650464]. 
=============================================
[2019-04-04 02:25:28,204] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.23973275e-25 3.99189932e-20 1.23793299e-24 7.63187933e-21
 3.66360434e-21 1.17497334e-26 1.00000000e+00], sum to 1.0000
[2019-04-04 02:25:28,205] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9585
[2019-04-04 02:25:28,220] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.333333333333333, 36.33333333333333, 81.33333333333333, 373.6666666666667, 26.0, 27.33040898927599, 0.7739767051327252, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4120800.0000, 
sim time next is 4121400.0000, 
raw observation next is [3.166666666666667, 36.66666666666667, 69.66666666666667, 310.3333333333334, 26.0, 26.83194261901023, 0.7386249297211905, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.5503231763619576, 0.3666666666666667, 0.23222222222222225, 0.3429097605893187, 0.6666666666666666, 0.7359952182508526, 0.7462083099070634, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5682482], dtype=float32), 0.17402533]. 
=============================================
[2019-04-04 02:25:29,155] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.0876421e-25 1.6092305e-18 6.3946201e-23 4.0315045e-20 9.7614763e-20
 4.1890553e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:25:29,155] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5340
[2019-04-04 02:25:29,188] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.8, 68.0, 0.0, 0.0, 26.0, 25.55630572915067, 0.53268996589476, 0.0, 1.0, 66074.86926284489], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4424400.0000, 
sim time next is 4425000.0000, 
raw observation next is [3.666666666666667, 68.0, 0.0, 0.0, 26.0, 25.53889477077428, 0.5599692892413267, 0.0, 1.0, 56638.58018907436], 
processed observation next is [1.0, 0.21739130434782608, 0.564173591874423, 0.68, 0.0, 0.0, 0.6666666666666666, 0.6282412308978568, 0.6866564297471088, 0.0, 1.0, 0.2697075247098779], 
reward next is 0.7303, 
noisyNet noise sample is [array([-1.0169111], dtype=float32), 0.5557034]. 
=============================================
[2019-04-04 02:25:29,209] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[82.419334]
 [82.30621 ]
 [82.1423  ]
 [82.02492 ]
 [82.025276]], R is [[82.42649841]
 [82.28759003]
 [82.15153503]
 [81.99916077]
 [81.92076111]].
[2019-04-04 02:25:29,345] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-04 02:25:29,346] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 02:25:29,347] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:25:29,348] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 02:25:29,348] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:25:29,350] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 02:25:29,351] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run32
[2019-04-04 02:25:29,381] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:25:29,383] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run32
[2019-04-04 02:25:29,413] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run32
[2019-04-04 02:27:34,061] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 02:27:43,794] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.37676838], dtype=float32), 0.18461837]
[2019-04-04 02:27:43,795] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [5.0, 67.0, 0.0, 0.0, 26.0, 25.76483862792483, 0.5697619405525236, 0.0, 1.0, 0.0]
[2019-04-04 02:27:43,795] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 02:27:43,796] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.64062373e-24 1.32306124e-17 1.83354461e-22 5.16562596e-20
 1.58405113e-19 1.68861767e-23 1.00000000e+00], sampled 0.8095880124901528
[2019-04-04 02:27:50,226] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.37676838], dtype=float32), 0.18461837]
[2019-04-04 02:27:50,227] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-6.0, 92.0, 0.0, 0.0, 26.0, 24.35731626691095, 0.2236887321930442, 0.0, 1.0, 41127.7293927049]
[2019-04-04 02:27:50,227] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 02:27:50,229] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [9.8535282e-24 4.9372118e-17 4.8674460e-22 1.7888541e-19 3.2788920e-19
 7.2336260e-23 1.0000000e+00], sampled 0.12019331817505119
[2019-04-04 02:27:57,756] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 02:28:00,703] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 02:28:01,742] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 3100000, evaluation results [3100000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 02:28:13,865] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.78001311e-25 8.37715727e-20 8.96212726e-24 1.00969535e-20
 3.96502177e-20 2.42826079e-25 1.00000000e+00], sum to 1.0000
[2019-04-04 02:28:13,865] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9235
[2019-04-04 02:28:13,895] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 37.16666666666666, 0.0, 0.0, 26.0, 25.47298311021645, 0.5703616190214101, 0.0, 1.0, 44474.78444768094], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4135800.0000, 
sim time next is 4136400.0000, 
raw observation next is [1.0, 36.0, 0.0, 0.0, 26.0, 25.71966180432339, 0.5897693970202159, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.4903047091412743, 0.36, 0.0, 0.0, 0.6666666666666666, 0.6433051503602826, 0.6965897990067386, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.15802282], dtype=float32), 0.08771311]. 
=============================================
[2019-04-04 02:28:16,069] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.5235237e-25 1.9745908e-18 2.0657989e-23 3.0935980e-20 1.9753270e-20
 2.4927596e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:28:16,081] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4730
[2019-04-04 02:28:16,107] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.133333333333333, 42.66666666666666, 0.0, 0.0, 26.0, 25.09763990507345, 0.366232494091296, 0.0, 1.0, 100587.2262973448], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4218000.0000, 
sim time next is 4218600.0000, 
raw observation next is [1.066666666666667, 42.83333333333334, 0.0, 0.0, 26.0, 25.21595587443852, 0.3867083310794987, 0.0, 1.0, 60183.5639228952], 
processed observation next is [0.0, 0.8260869565217391, 0.49215143120960303, 0.42833333333333345, 0.0, 0.0, 0.6666666666666666, 0.6013296562032101, 0.6289027770264995, 0.0, 1.0, 0.2865883996328343], 
reward next is 0.7134, 
noisyNet noise sample is [array([-0.39164603], dtype=float32), 1.5622125]. 
=============================================
[2019-04-04 02:28:17,781] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.7213611e-25 1.5296795e-19 4.0359583e-24 5.3733451e-21 1.0885970e-20
 1.0979450e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:28:17,781] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2264
[2019-04-04 02:28:17,816] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 48.0, 0.0, 0.0, 26.0, 25.39813240381957, 0.3530482738635543, 0.0, 1.0, 48542.88099196129], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4234800.0000, 
sim time next is 4235400.0000, 
raw observation next is [2.0, 48.0, 0.0, 0.0, 26.0, 25.41090941306631, 0.3549968623584523, 0.0, 1.0, 36665.82060281384], 
processed observation next is [0.0, 0.0, 0.518005540166205, 0.48, 0.0, 0.0, 0.6666666666666666, 0.6175757844221925, 0.6183322874528174, 0.0, 1.0, 0.17459914572768495], 
reward next is 0.8254, 
noisyNet noise sample is [array([0.18169059], dtype=float32), -1.1180146]. 
=============================================
[2019-04-04 02:28:18,027] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.5608636e-24 9.6454057e-18 6.4664695e-23 3.3414313e-20 8.1537481e-20
 6.1902195e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:28:18,030] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6125
[2019-04-04 02:28:18,042] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 26.0, 25.40748120061664, 0.3397867600097772, 0.0, 1.0, 33362.18384059054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4243800.0000, 
sim time next is 4244400.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 26.0, 25.41050594214111, 0.3392473581081427, 0.0, 1.0, 34296.1469278966], 
processed observation next is [0.0, 0.13043478260869565, 0.5457063711911359, 0.45, 0.0, 0.0, 0.6666666666666666, 0.6175421618450926, 0.6130824527027142, 0.0, 1.0, 0.1633149853709362], 
reward next is 0.8367, 
noisyNet noise sample is [array([-0.39631957], dtype=float32), 0.36703286]. 
=============================================
[2019-04-04 02:28:20,400] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.2157543e-27 1.4337751e-20 9.4959748e-26 2.7232955e-22 2.4450326e-22
 1.6230079e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 02:28:20,408] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1920
[2019-04-04 02:28:20,511] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 29.0, 116.0, 835.5, 26.0, 25.68298275934324, 0.5173542102540248, 1.0, 1.0, 106330.9592665093], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4021200.0000, 
sim time next is 4021800.0000, 
raw observation next is [-3.833333333333333, 28.5, 115.3333333333333, 833.6666666666666, 26.0, 25.0300527553031, 0.5245545425227883, 1.0, 1.0, 173792.8795687351], 
processed observation next is [1.0, 0.5652173913043478, 0.3564173591874424, 0.285, 0.3844444444444443, 0.9211786372007366, 0.6666666666666666, 0.5858377296085916, 0.6748515141742627, 1.0, 1.0, 0.8275851408035005], 
reward next is 0.1724, 
noisyNet noise sample is [array([-0.43029144], dtype=float32), -0.56909835]. 
=============================================
[2019-04-04 02:28:46,846] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [9.3364930e-26 2.9496692e-20 2.4908204e-24 2.1549885e-20 1.7890707e-21
 1.7627610e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:28:46,846] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9696
[2019-04-04 02:28:46,884] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.5, 46.5, 153.0, 12.0, 26.0, 26.39592594176162, 0.5969475658009313, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4548600.0000, 
sim time next is 4549200.0000, 
raw observation next is [2.333333333333333, 47.0, 145.6666666666667, 21.33333333333333, 26.0, 26.43488984218634, 0.5987688413011814, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5272391505078486, 0.47, 0.48555555555555574, 0.02357274401473296, 0.6666666666666666, 0.7029074868488617, 0.6995896137670604, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6564477], dtype=float32), 0.17191629]. 
=============================================
[2019-04-04 02:29:03,248] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:29:03,249] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:29:03,256] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run24
[2019-04-04 02:29:09,508] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.9534847e-24 6.9428214e-18 5.7513968e-23 2.2227178e-19 1.7319858e-19
 8.0704273e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:29:09,521] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1775
[2019-04-04 02:29:09,607] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.833333333333333, 37.5, 196.0, 626.0, 26.0, 25.13613329246367, 0.433955080993593, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4801800.0000, 
sim time next is 4802400.0000, 
raw observation next is [3.0, 37.0, 184.0, 655.0, 26.0, 25.13209451473363, 0.4361052369952593, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.5457063711911359, 0.37, 0.6133333333333333, 0.7237569060773481, 0.6666666666666666, 0.5943412095611359, 0.6453684123317531, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.30294383], dtype=float32), -1.8630933]. 
=============================================
[2019-04-04 02:29:10,025] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.1910356e-25 1.0588957e-18 5.6363910e-23 2.2010743e-20 3.4543453e-20
 9.0332748e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:29:10,027] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2402
[2019-04-04 02:29:10,087] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 77.0, 0.0, 0.0, 26.0, 25.20485588250902, 0.4320462410759723, 0.0, 1.0, 42786.29812309823], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4748400.0000, 
sim time next is 4749000.0000, 
raw observation next is [-3.166666666666667, 78.16666666666667, 0.0, 0.0, 26.0, 25.19181364323104, 0.4256181048448116, 0.0, 1.0, 42147.43165358886], 
processed observation next is [1.0, 1.0, 0.3748845798707295, 0.7816666666666667, 0.0, 0.0, 0.6666666666666666, 0.5993178036025867, 0.6418727016149371, 0.0, 1.0, 0.20070205549328027], 
reward next is 0.7993, 
noisyNet noise sample is [array([-0.6640233], dtype=float32), 1.0181808]. 
=============================================
[2019-04-04 02:29:10,152] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[80.64448 ]
 [80.558044]
 [80.49978 ]
 [80.42895 ]
 [80.36345 ]], R is [[80.62463379]
 [80.61464691]
 [80.59649658]
 [80.55815125]
 [80.49168396]].
[2019-04-04 02:29:11,552] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:29:11,552] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:29:11,555] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run24
[2019-04-04 02:29:20,107] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:29:20,107] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:29:20,140] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run24
[2019-04-04 02:29:33,153] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.0591819e-25 6.5962587e-20 2.5070176e-23 6.4231576e-21 3.4209974e-20
 6.2550191e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:29:33,154] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0328
[2019-04-04 02:29:33,182] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 36.5, 0.0, 0.0, 26.0, 25.79380222322046, 0.5085262268064571, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5004600.0000, 
sim time next is 5005200.0000, 
raw observation next is [3.0, 36.0, 0.0, 0.0, 26.0, 25.75404304750592, 0.4816756286067547, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.5457063711911359, 0.36, 0.0, 0.0, 0.6666666666666666, 0.6461702539588267, 0.6605585428689182, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.13752787], dtype=float32), -0.74117416]. 
=============================================
[2019-04-04 02:29:37,634] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:29:37,634] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:29:37,642] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run24
[2019-04-04 02:29:38,256] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:29:38,257] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:29:38,299] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run24
[2019-04-04 02:29:39,970] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:29:39,970] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:29:39,987] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run24
[2019-04-04 02:29:40,441] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:29:40,441] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:29:40,444] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run24
[2019-04-04 02:29:40,725] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:29:40,725] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:29:40,728] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run24
[2019-04-04 02:29:41,045] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:29:41,045] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:29:41,049] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run24
[2019-04-04 02:29:42,443] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.7899623e-24 1.5339467e-17 9.0639708e-23 3.3843161e-20 4.5348868e-20
 5.9190869e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:29:42,444] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2245
[2019-04-04 02:29:42,477] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.1, 69.0, 0.0, 0.0, 26.0, 23.86113063699619, 0.01091827434881717, 0.0, 1.0, 45634.75237601891], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 268200.0000, 
sim time next is 268800.0000, 
raw observation next is [-8.366666666666667, 68.33333333333333, 0.0, 0.0, 26.0, 23.85557446550296, -0.004262564674513358, 0.0, 1.0, 45668.27721517906], 
processed observation next is [1.0, 0.08695652173913043, 0.23084025854108958, 0.6833333333333332, 0.0, 0.0, 0.6666666666666666, 0.4879645387919134, 0.49857914510849555, 0.0, 1.0, 0.21746798673894793], 
reward next is 0.7825, 
noisyNet noise sample is [array([0.5472159], dtype=float32), -0.07849756]. 
=============================================
[2019-04-04 02:29:42,681] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:29:42,681] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:29:42,720] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run24
[2019-04-04 02:29:42,904] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:29:42,921] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:29:42,924] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run24
[2019-04-04 02:29:46,797] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:29:46,798] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:29:46,801] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run24
[2019-04-04 02:29:51,250] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:29:51,250] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:29:51,254] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run24
[2019-04-04 02:29:56,589] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:29:56,590] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:29:56,593] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run24
[2019-04-04 02:29:58,081] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:29:58,086] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:29:58,101] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run24
[2019-04-04 02:29:58,303] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.5798440e-25 3.6594615e-18 8.4617096e-23 7.6921813e-20 1.5852359e-19
 1.2989427e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 02:29:58,303] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8215
[2019-04-04 02:29:58,384] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.7, 93.0, 72.5, 0.0, 26.0, 24.23880036722083, 0.08161802986169825, 0.0, 1.0, 18780.08396786054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 40800.0000, 
sim time next is 41400.0000, 
raw observation next is [7.7, 93.0, 75.0, 0.0, 26.0, 24.27896311487394, 0.08565884749127528, 0.0, 1.0, 18774.31476486237], 
processed observation next is [0.0, 0.4782608695652174, 0.6759002770083103, 0.93, 0.25, 0.0, 0.6666666666666666, 0.5232469262394949, 0.5285529491637585, 0.0, 1.0, 0.089401498880297], 
reward next is 0.9106, 
noisyNet noise sample is [array([0.84914464], dtype=float32), -1.3535537]. 
=============================================
[2019-04-04 02:29:58,436] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.7704870e-24 9.6157906e-19 6.5267903e-23 9.0596291e-20 5.9294271e-20
 5.5967086e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:29:58,437] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9125
[2019-04-04 02:29:58,483] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.25, 95.0, 0.0, 0.0, 26.0, 24.39685148527897, 0.1700303505498292, 0.0, 1.0, 40074.08927067763], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 83400.0000, 
sim time next is 84000.0000, 
raw observation next is [0.2, 95.0, 0.0, 0.0, 26.0, 24.3743861556693, 0.1667507946082195, 0.0, 1.0, 40068.55234990948], 
processed observation next is [0.0, 1.0, 0.46814404432132967, 0.95, 0.0, 0.0, 0.6666666666666666, 0.531198846305775, 0.5555835982027398, 0.0, 1.0, 0.19080263023766417], 
reward next is 0.8092, 
noisyNet noise sample is [array([0.95006883], dtype=float32), 0.15912813]. 
=============================================
[2019-04-04 02:29:58,502] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[78.53709 ]
 [78.46377 ]
 [78.36957 ]
 [78.273056]
 [78.148384]], R is [[78.63217163]
 [78.65502167]
 [78.67758942]
 [78.69985199]
 [78.72177124]].
[2019-04-04 02:30:02,589] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:30:02,589] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:30:02,593] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run24
[2019-04-04 02:30:12,379] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.5188827e-25 8.6316433e-20 1.1600996e-23 9.8304535e-21 3.1411988e-21
 2.1709736e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:30:12,379] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9042
[2019-04-04 02:30:12,502] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 60.0, 106.8333333333333, 0.0, 26.0, 26.18822110590567, 0.415510791796617, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 225600.0000, 
sim time next is 226200.0000, 
raw observation next is [-2.9, 59.5, 96.66666666666669, 0.0, 26.0, 26.16274402171172, 0.307903444323148, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.38227146814404434, 0.595, 0.3222222222222223, 0.0, 0.6666666666666666, 0.6802286684759767, 0.6026344814410494, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.42421117], dtype=float32), 0.15514654]. 
=============================================
[2019-04-04 02:30:15,821] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.8200418e-26 2.0491395e-20 1.5178352e-24 1.8040520e-21 1.9270583e-22
 6.6822213e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:30:15,822] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9163
[2019-04-04 02:30:15,885] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.95, 63.5, 149.0, 0.0, 26.0, 24.3818742310923, 0.2867877150808245, 1.0, 1.0, 200978.6543490316], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 221400.0000, 
sim time next is 222000.0000, 
raw observation next is [-3.766666666666667, 63.0, 143.6666666666667, 0.0, 26.0, 25.3019359012739, 0.3541668102570254, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.358264081255771, 0.63, 0.47888888888888903, 0.0, 0.6666666666666666, 0.6084946584394917, 0.6180556034190084, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2746623], dtype=float32), 1.3791335]. 
=============================================
[2019-04-04 02:30:15,909] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[81.9037 ]
 [81.31143]
 [80.782  ]
 [81.00806]
 [81.04282]], R is [[81.87503052]
 [81.09924316]
 [80.33807373]
 [80.44554138]
 [80.46942139]].
[2019-04-04 02:30:21,538] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.6555427e-24 1.7983998e-18 1.8588698e-22 4.3321413e-20 4.1329536e-20
 8.8757438e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:30:21,538] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3425
[2019-04-04 02:30:21,575] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.1, 69.66666666666666, 0.0, 0.0, 26.0, 24.00770358081122, 0.04666034498434433, 0.0, 1.0, 45414.20152680463], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 265200.0000, 
sim time next is 265800.0000, 
raw observation next is [-7.199999999999999, 70.33333333333334, 0.0, 0.0, 26.0, 24.00102754010927, 0.04466062116917624, 0.0, 1.0, 45465.48674201586], 
processed observation next is [1.0, 0.043478260869565216, 0.26315789473684215, 0.7033333333333335, 0.0, 0.0, 0.6666666666666666, 0.5000856283424392, 0.5148868737230587, 0.0, 1.0, 0.21650231781912316], 
reward next is 0.7835, 
noisyNet noise sample is [array([1.8472561], dtype=float32), 0.4831276]. 
=============================================
[2019-04-04 02:30:49,195] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.2636815e-25 1.1309346e-20 1.6181451e-24 8.2271311e-22 5.8574803e-22
 3.1444516e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:30:49,195] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5106
[2019-04-04 02:30:49,261] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.7, 33.83333333333334, 110.6666666666667, 0.0, 26.0, 25.51262260078344, 0.2560962266191513, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 481800.0000, 
sim time next is 482400.0000, 
raw observation next is [-0.6, 35.0, 106.5, 0.0, 26.0, 25.5754128531393, 0.2513228844463852, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.44598337950138506, 0.35, 0.355, 0.0, 0.6666666666666666, 0.6312844044282752, 0.5837742948154617, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.8819233], dtype=float32), 0.6876026]. 
=============================================
[2019-04-04 02:31:14,357] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.2723173e-26 1.1453050e-20 9.6096748e-25 1.8210195e-21 5.8759212e-22
 1.6135575e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:31:14,357] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0789
[2019-04-04 02:31:14,435] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.1333333333333333, 52.33333333333333, 124.0, 503.0, 26.0, 25.6592661048353, 0.3576568126056918, 1.0, 1.0, 35331.13486205231], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 736800.0000, 
sim time next is 737400.0000, 
raw observation next is [0.3166666666666667, 51.16666666666667, 117.0, 557.0, 26.0, 25.59796916801515, 0.3654084919066862, 1.0, 1.0, 20450.5396233918], 
processed observation next is [1.0, 0.5217391304347826, 0.47137580794090495, 0.5116666666666667, 0.39, 0.6154696132596685, 0.6666666666666666, 0.6331640973345959, 0.6218028306355621, 1.0, 1.0, 0.09738352201615143], 
reward next is 0.9026, 
noisyNet noise sample is [array([-1.697616], dtype=float32), -0.6148924]. 
=============================================
[2019-04-04 02:31:15,460] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.8797870e-25 8.6879379e-19 3.1904552e-23 1.8798080e-20 7.7500531e-21
 1.0039979e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 02:31:15,460] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5414
[2019-04-04 02:31:15,578] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.9333333333333333, 80.66666666666667, 123.1666666666667, 352.5, 26.0, 24.97026228076015, 0.3169965935311123, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 562800.0000, 
sim time next is 563400.0000, 
raw observation next is [-1.0, 80.5, 130.0, 396.0, 26.0, 24.96635215800529, 0.3143935647424858, 0.0, 1.0, 18730.72778200659], 
processed observation next is [0.0, 0.5217391304347826, 0.4349030470914128, 0.805, 0.43333333333333335, 0.4375690607734807, 0.6666666666666666, 0.5805293465004407, 0.6047978549141619, 0.0, 1.0, 0.089193941819079], 
reward next is 0.9108, 
noisyNet noise sample is [array([-0.25533608], dtype=float32), 1.1313555]. 
=============================================
[2019-04-04 02:31:17,495] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.6349203e-26 1.8672589e-19 9.6547645e-24 2.6748549e-21 4.5861179e-21
 2.2789654e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:31:17,495] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3715
[2019-04-04 02:31:17,540] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 76.0, 0.0, 0.0, 26.0, 24.2438440065476, 0.01718394564063278, 0.0, 1.0, 41981.82350274587], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 715800.0000, 
sim time next is 716400.0000, 
raw observation next is [-2.3, 76.0, 0.0, 0.0, 26.0, 24.17087384925158, 0.005562917802995925, 0.0, 1.0, 42035.92877027448], 
processed observation next is [1.0, 0.30434782608695654, 0.3988919667590028, 0.76, 0.0, 0.0, 0.6666666666666666, 0.5142394874376318, 0.501854305934332, 0.0, 1.0, 0.20017108938225942], 
reward next is 0.7998, 
noisyNet noise sample is [array([0.55798507], dtype=float32), -0.5651653]. 
=============================================
[2019-04-04 02:31:23,389] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.8105374e-25 1.2715354e-18 3.6260878e-23 6.4234517e-21 8.4086892e-21
 6.2008550e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:31:23,389] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8538
[2019-04-04 02:31:23,415] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 68.0, 0.0, 0.0, 26.0, 23.60818339258071, -0.04856946056187761, 0.0, 1.0, 43801.75308704003], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 630000.0000, 
sim time next is 630600.0000, 
raw observation next is [-4.5, 69.83333333333334, 0.0, 0.0, 26.0, 23.57621379442949, -0.05482083146737604, 0.0, 1.0, 43850.81213819271], 
processed observation next is [0.0, 0.30434782608695654, 0.3379501385041552, 0.6983333333333335, 0.0, 0.0, 0.6666666666666666, 0.46468448286912406, 0.4817263895108746, 0.0, 1.0, 0.20881339113425101], 
reward next is 0.7912, 
noisyNet noise sample is [array([0.37890905], dtype=float32), 0.80352235]. 
=============================================
[2019-04-04 02:31:38,103] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.5004366e-26 3.1605156e-20 1.0119760e-24 2.9792035e-22 3.8424551e-22
 8.0819416e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:31:38,103] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1784
[2019-04-04 02:31:38,141] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 73.0, 0.0, 0.0, 26.0, 24.43411922000636, 0.08813153681420287, 0.0, 1.0, 41006.26112588321], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 696000.0000, 
sim time next is 696600.0000, 
raw observation next is [-3.4, 73.5, 0.0, 0.0, 26.0, 24.4003973064064, 0.08525708954478706, 0.0, 1.0, 41048.74570111687], 
processed observation next is [1.0, 0.043478260869565216, 0.368421052631579, 0.735, 0.0, 0.0, 0.6666666666666666, 0.5333664422005334, 0.5284190298482624, 0.0, 1.0, 0.19547021762436606], 
reward next is 0.8045, 
noisyNet noise sample is [array([-0.02365666], dtype=float32), 0.05266498]. 
=============================================
[2019-04-04 02:31:45,796] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.7913462e-27 3.0102076e-22 8.2571408e-26 4.5140823e-23 2.0060635e-23
 3.4740608e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 02:31:45,796] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6221
[2019-04-04 02:31:45,866] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.600000000000001, 94.66666666666667, 0.0, 0.0, 26.0, 25.34320688888258, 0.327370925972723, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 927600.0000, 
sim time next is 928200.0000, 
raw observation next is [4.5, 95.33333333333333, 0.0, 0.0, 26.0, 25.19585159205904, 0.3158075948779777, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.5872576177285319, 0.9533333333333333, 0.0, 0.0, 0.6666666666666666, 0.5996542993382533, 0.6052691982926592, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5975847], dtype=float32), 0.80091715]. 
=============================================
[2019-04-04 02:31:52,661] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.1088819e-28 1.4103455e-21 3.4672859e-26 6.2668684e-24 4.6393765e-23
 8.9269308e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 02:31:52,662] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0823
[2019-04-04 02:31:52,711] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.0, 92.0, 43.5, 0.0, 26.0, 26.14637123743889, 0.5740108565304533, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 982800.0000, 
sim time next is 983400.0000, 
raw observation next is [10.08333333333333, 92.16666666666667, 49.00000000000001, 0.0, 26.0, 26.24191036606288, 0.5790427284364678, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.7419205909510619, 0.9216666666666667, 0.16333333333333336, 0.0, 0.6666666666666666, 0.6868258638385732, 0.693014242812156, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3303478], dtype=float32), 0.8017092]. 
=============================================
[2019-04-04 02:31:58,181] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.15517594e-29 1.20893464e-22 9.22908326e-28 5.06879616e-24
 6.03679456e-25 1.20208294e-29 1.00000000e+00], sum to 1.0000
[2019-04-04 02:31:58,200] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0792
[2019-04-04 02:31:58,218] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [19.4, 49.0, 92.0, 0.0, 26.0, 27.3245833194365, 0.939730984244206, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1092600.0000, 
sim time next is 1093200.0000, 
raw observation next is [19.4, 49.0, 82.5, 0.0, 26.0, 26.72357767076501, 0.9032164576194494, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 1.0, 0.49, 0.275, 0.0, 0.6666666666666666, 0.7269648058970842, 0.8010721525398165, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9420024], dtype=float32), -0.9018913]. 
=============================================
[2019-04-04 02:32:04,192] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.8771038e-28 1.5628023e-21 8.7611928e-26 6.5147756e-23 5.5439081e-23
 1.0736761e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 02:32:04,197] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5097
[2019-04-04 02:32:04,212] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.8, 93.0, 100.0, 0.0, 26.0, 25.16036124725183, 0.2522958483399024, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 910800.0000, 
sim time next is 911400.0000, 
raw observation next is [3.8, 93.0, 98.66666666666666, 0.0, 26.0, 25.12937993465778, 0.2450991145452246, 1.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.5652173913043478, 0.5678670360110805, 0.93, 0.32888888888888884, 0.0, 0.6666666666666666, 0.5941149945548151, 0.5816997048484082, 1.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-1.0308686], dtype=float32), 0.5819302]. 
=============================================
[2019-04-04 02:32:13,041] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.7347269e-29 2.4423946e-22 1.4216141e-27 5.0760108e-25 9.9935473e-25
 1.1203069e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 02:32:13,041] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4914
[2019-04-04 02:32:13,045] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [17.51666666666667, 68.33333333333334, 0.0, 0.0, 26.0, 24.55067922493936, 0.3654513520551457, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1199400.0000, 
sim time next is 1200000.0000, 
raw observation next is [17.33333333333334, 69.66666666666667, 0.0, 0.0, 26.0, 24.52554860983278, 0.3596607959925437, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.9130434782608695, 0.9427516158818101, 0.6966666666666668, 0.0, 0.0, 0.6666666666666666, 0.5437957174860649, 0.6198869319975145, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3135158], dtype=float32), 0.7832153]. 
=============================================
[2019-04-04 02:32:13,051] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[101.187256]
 [101.20646 ]
 [101.17518 ]
 [101.14304 ]
 [101.1174  ]], R is [[101.13383484]
 [101.12249756]
 [101.11127472]
 [101.10016632]
 [101.08916473]].
[2019-04-04 02:32:44,204] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.9082145e-27 4.2173618e-21 4.5218330e-25 3.5365558e-22 4.9971125e-22
 1.2947703e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:32:44,204] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3339
[2019-04-04 02:32:44,217] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.3, 67.0, 0.0, 0.0, 26.0, 25.7690074348451, 0.5963331840428968, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1539000.0000, 
sim time next is 1539600.0000, 
raw observation next is [7.933333333333334, 69.0, 0.0, 0.0, 26.0, 25.70716641642262, 0.5793050635582521, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.6823638042474609, 0.69, 0.0, 0.0, 0.6666666666666666, 0.6422638680352183, 0.6931016878527507, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0928148], dtype=float32), 0.4360351]. 
=============================================
[2019-04-04 02:32:45,196] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.1124293e-27 1.0140992e-21 1.0394375e-25 1.9850316e-22 8.9636024e-23
 2.1955066e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 02:32:45,196] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6813
[2019-04-04 02:32:45,249] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.333333333333334, 86.33333333333334, 98.0, 701.3333333333334, 26.0, 26.31505669262645, 0.5428608130758613, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1513200.0000, 
sim time next is 1513800.0000, 
raw observation next is [5.800000000000001, 83.0, 100.0, 700.0, 26.0, 25.86404210075208, 0.5672829693584897, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.6232686980609419, 0.83, 0.3333333333333333, 0.7734806629834254, 0.6666666666666666, 0.65533684172934, 0.6890943231194965, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0281514], dtype=float32), -0.76452804]. 
=============================================
[2019-04-04 02:32:50,659] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4978917e-26 2.2709339e-21 5.6584026e-25 2.7397591e-22 1.1349180e-22
 8.4509821e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 02:32:50,659] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2461
[2019-04-04 02:32:50,677] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.766666666666667, 64.33333333333334, 0.0, 0.0, 26.0, 26.3659032163016, 0.6991324721943787, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1622400.0000, 
sim time next is 1623000.0000, 
raw observation next is [9.583333333333334, 65.16666666666666, 0.0, 0.0, 26.0, 26.24670260780302, 0.6895663290040058, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.7280701754385965, 0.6516666666666666, 0.0, 0.0, 0.6666666666666666, 0.6872252173169183, 0.7298554430013353, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.339247], dtype=float32), -1.017959]. 
=============================================
[2019-04-04 02:32:50,699] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[84.2275 ]
 [85.59698]
 [85.64475]
 [85.78567]
 [85.94936]], R is [[85.07314301]
 [85.22241211]
 [85.37018585]
 [85.51648712]
 [85.66132355]].
[2019-04-04 02:33:02,240] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.9614674e-26 2.4475162e-21 7.9072055e-25 3.1301567e-22 5.1507447e-22
 1.7612554e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:33:02,241] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4385
[2019-04-04 02:33:02,325] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 88.0, 0.0, 0.0, 26.0, 25.57758321644909, 0.565647857631177, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1711800.0000, 
sim time next is 1712400.0000, 
raw observation next is [1.1, 88.0, 0.0, 0.0, 26.0, 25.7084951012651, 0.572078462928711, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.49307479224376743, 0.88, 0.0, 0.0, 0.6666666666666666, 0.6423745917720917, 0.690692820976237, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7538001], dtype=float32), -0.5754442]. 
=============================================
[2019-04-04 02:33:07,956] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.1516376e-27 2.1299299e-21 1.6499524e-25 1.1027522e-22 2.4815759e-22
 1.0635422e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 02:33:07,956] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0806
[2019-04-04 02:33:08,020] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.56666666666667, 53.0, 41.83333333333334, 30.83333333333334, 26.0, 27.51160435628253, 0.7542803724095822, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1615200.0000, 
sim time next is 1615800.0000, 
raw observation next is [12.38333333333333, 53.5, 33.66666666666667, 24.66666666666667, 26.0, 27.00002913858884, 0.764495754323134, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.8056325023084026, 0.535, 0.11222222222222224, 0.027255985267034995, 0.6666666666666666, 0.7500024282157366, 0.7548319181077113, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.18445666], dtype=float32), 0.44302326]. 
=============================================
[2019-04-04 02:33:14,326] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.6401316e-23 5.9460588e-18 1.4191748e-21 1.2948490e-19 2.7375798e-19
 1.5075999e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 02:33:14,353] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2365
[2019-04-04 02:33:14,411] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.1, 86.33333333333333, 0.0, 0.0, 26.0, 24.94396700035031, 0.3499636171314837, 0.0, 1.0, 43751.74241670171], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1749000.0000, 
sim time next is 1749600.0000, 
raw observation next is [-1.2, 87.0, 0.0, 0.0, 26.0, 24.91500909104109, 0.3440583696849334, 0.0, 1.0, 43814.16867802139], 
processed observation next is [0.0, 0.2608695652173913, 0.42936288088642666, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5762507575867574, 0.6146861232283111, 0.0, 1.0, 0.20863889846676853], 
reward next is 0.7914, 
noisyNet noise sample is [array([0.74527305], dtype=float32), -0.09176114]. 
=============================================
[2019-04-04 02:33:25,386] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.4964597e-24 2.1554594e-18 5.5831733e-22 8.1515709e-20 9.6645114e-20
 1.0683915e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 02:33:25,386] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2398
[2019-04-04 02:33:25,496] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 82.0, 0.0, 0.0, 26.0, 23.51514084538955, -0.1256645797769173, 0.0, 1.0, 44997.64793343841], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1921200.0000, 
sim time next is 1921800.0000, 
raw observation next is [-8.9, 82.0, 0.0, 0.0, 26.0, 23.43377019672638, -0.1391846253832466, 0.0, 1.0, 44929.38587854471], 
processed observation next is [1.0, 0.21739130434782608, 0.21606648199445982, 0.82, 0.0, 0.0, 0.6666666666666666, 0.4528141830605315, 0.4536051248722511, 0.0, 1.0, 0.2139494565644986], 
reward next is 0.7861, 
noisyNet noise sample is [array([-0.2888858], dtype=float32), 0.76342124]. 
=============================================
[2019-04-04 02:33:29,693] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.6207991e-24 7.3020130e-18 6.7241191e-22 5.4928890e-20 1.5164675e-19
 7.4508720e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 02:33:29,694] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9131
[2019-04-04 02:33:29,835] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 85.0, 65.0, 0.0, 26.0, 24.97069762139525, 0.3100581199270108, 0.0, 1.0, 50268.82598940083], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1762200.0000, 
sim time next is 1762800.0000, 
raw observation next is [-2.1, 85.66666666666667, 70.33333333333334, 0.0, 26.0, 24.87397993153539, 0.3190103414928868, 0.0, 1.0, 102833.9276508845], 
processed observation next is [0.0, 0.391304347826087, 0.404432132963989, 0.8566666666666667, 0.23444444444444448, 0.0, 0.6666666666666666, 0.5728316609612826, 0.606336780497629, 0.0, 1.0, 0.4896853697661167], 
reward next is 0.5103, 
noisyNet noise sample is [array([1.2568885], dtype=float32), -0.17879751]. 
=============================================
[2019-04-04 02:33:31,785] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.1261046e-24 2.2901546e-18 2.5318532e-22 3.7534916e-20 5.7158200e-20
 1.9604604e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 02:33:31,785] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3749
[2019-04-04 02:33:31,876] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 83.0, 120.1666666666667, 0.0, 26.0, 24.98029271408879, 0.344894005000265, 0.0, 1.0, 50534.91763641626], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1776000.0000, 
sim time next is 1776600.0000, 
raw observation next is [-2.8, 83.0, 119.0, 0.0, 26.0, 24.96439456273545, 0.3463579267881324, 0.0, 1.0, 54828.90168155248], 
processed observation next is [0.0, 0.5652173913043478, 0.38504155124653744, 0.83, 0.39666666666666667, 0.0, 0.6666666666666666, 0.5803662135612875, 0.6154526422627108, 0.0, 1.0, 0.2610900080073928], 
reward next is 0.7389, 
noisyNet noise sample is [array([0.55557007], dtype=float32), 1.1406955]. 
=============================================
[2019-04-04 02:33:49,296] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.1073926e-25 1.8132083e-20 7.7343292e-24 1.5517241e-20 5.4282500e-21
 1.5787498e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:33:49,320] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0068
[2019-04-04 02:33:49,369] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.533333333333334, 64.0, 174.6666666666667, 128.5, 26.0, 25.87215863575882, 0.4060181565658607, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2118000.0000, 
sim time next is 2118600.0000, 
raw observation next is [-6.45, 64.0, 151.0, 134.0, 26.0, 25.74971406205852, 0.3664353086785084, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.28393351800554023, 0.64, 0.5033333333333333, 0.14806629834254142, 0.6666666666666666, 0.6458095051715432, 0.6221451028928361, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.73007756], dtype=float32), 0.34429166]. 
=============================================
[2019-04-04 02:33:54,184] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.3052795e-25 8.8438571e-20 9.0938544e-24 3.2859635e-21 3.1864205e-21
 2.3840661e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:33:54,184] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.2970
[2019-04-04 02:33:54,229] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 61.0, 0.0, 0.0, 26.0, 25.00356004439524, 0.3120401214226344, 0.0, 1.0, 38568.458560946], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2330400.0000, 
sim time next is 2331000.0000, 
raw observation next is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.95428246004399, 0.3052051172519222, 0.0, 1.0, 38543.2572810547], 
processed observation next is [1.0, 1.0, 0.3988919667590028, 0.62, 0.0, 0.0, 0.6666666666666666, 0.5795235383369993, 0.6017350390839741, 0.0, 1.0, 0.18353932038597476], 
reward next is 0.8165, 
noisyNet noise sample is [array([-0.7510816], dtype=float32), -0.85402304]. 
=============================================
[2019-04-04 02:33:54,237] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[81.846054]
 [81.90656 ]
 [81.81679 ]
 [81.973465]
 [82.10015 ]], R is [[82.01622772]
 [82.0124054 ]
 [82.00849152]
 [82.00435638]
 [81.9996109 ]].
[2019-04-04 02:34:14,390] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.0291094e-26 6.3377305e-20 2.1578055e-24 2.8638604e-21 1.2148202e-21
 5.3688647e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:34:14,390] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7066
[2019-04-04 02:34:14,407] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.2, 65.0, 0.0, 0.0, 26.0, 25.36209804119012, 0.4587974201818167, 0.0, 1.0, 46979.20167103971], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2667600.0000, 
sim time next is 2668200.0000, 
raw observation next is [-1.516666666666667, 65.66666666666667, 0.0, 0.0, 26.0, 25.4635123139604, 0.4606349123137094, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.4205909510618652, 0.6566666666666667, 0.0, 0.0, 0.6666666666666666, 0.6219593594967, 0.6535449707712365, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.26128167], dtype=float32), -0.03549923]. 
=============================================
[2019-04-04 02:34:32,873] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.6214200e-24 1.4048974e-18 1.5850030e-23 4.9373336e-21 1.6616735e-20
 1.2040686e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:34:32,873] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2149
[2019-04-04 02:34:32,901] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.3, 26.83333333333333, 67.33333333333333, 714.6666666666666, 26.0, 24.97475612311481, 0.2816270087194306, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2473800.0000, 
sim time next is 2474400.0000, 
raw observation next is [3.3, 26.66666666666667, 64.66666666666667, 697.3333333333334, 26.0, 24.97409056221018, 0.2804224218277619, 0.0, 1.0, 18697.68870803133], 
processed observation next is [0.0, 0.6521739130434783, 0.554016620498615, 0.2666666666666667, 0.21555555555555558, 0.7705340699815838, 0.6666666666666666, 0.581174213517515, 0.593474140609254, 0.0, 1.0, 0.0890366128953873], 
reward next is 0.9110, 
noisyNet noise sample is [array([1.1775562], dtype=float32), 0.5605438]. 
=============================================
[2019-04-04 02:34:35,672] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.7536940e-26 3.9013932e-19 3.0849314e-23 3.9913949e-21 2.8092936e-21
 4.0606189e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:34:35,672] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2151
[2019-04-04 02:34:35,706] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 44.0, 0.0, 0.0, 26.0, 24.92560882941622, 0.1766751921566819, 0.0, 1.0, 38626.28616648732], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2516400.0000, 
sim time next is 2517000.0000, 
raw observation next is [-1.7, 44.83333333333334, 0.0, 0.0, 26.0, 24.92735402918071, 0.1840188682211628, 0.0, 1.0, 38577.21825497227], 
processed observation next is [1.0, 0.13043478260869565, 0.4155124653739613, 0.4483333333333334, 0.0, 0.0, 0.6666666666666666, 0.5772795024317258, 0.5613396227403876, 0.0, 1.0, 0.18370103930939177], 
reward next is 0.8163, 
noisyNet noise sample is [array([-1.0410347], dtype=float32), 1.2702358]. 
=============================================
[2019-04-04 02:34:35,731] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[85.08026]
 [85.01758]
 [84.96022]
 [84.89899]
 [84.87318]], R is [[85.15063477]
 [85.1151886 ]
 [85.08003998]
 [85.04514313]
 [85.01038361]].
[2019-04-04 02:34:43,075] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.8122268e-24 5.1361183e-18 2.4857257e-22 5.3358659e-20 7.7744567e-20
 1.9447886e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 02:34:43,087] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8550
[2019-04-04 02:34:43,175] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.9666666666666667, 44.66666666666667, 18.16666666666666, 23.0, 26.0, 24.96083074995219, 0.2726831361285817, 0.0, 1.0, 40076.67721661839], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2395200.0000, 
sim time next is 2395800.0000, 
raw observation next is [-1.15, 44.5, 0.0, 0.0, 26.0, 24.95557783111534, 0.2654938209473628, 0.0, 1.0, 44726.35813851634], 
processed observation next is [0.0, 0.7391304347826086, 0.4307479224376732, 0.445, 0.0, 0.0, 0.6666666666666666, 0.5796314859262782, 0.5884979403157876, 0.0, 1.0, 0.21298265780245876], 
reward next is 0.7870, 
noisyNet noise sample is [array([0.6328534], dtype=float32), -0.17243248]. 
=============================================
[2019-04-04 02:34:54,796] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.2445150e-26 2.3350989e-20 6.5465105e-24 3.2757758e-21 2.0490681e-21
 4.7042746e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:34:54,798] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5740
[2019-04-04 02:34:54,813] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 85.0, 0.0, 0.0, 26.0, 25.08401743300012, 0.383035797943971, 0.0, 1.0, 43663.304490666], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2934000.0000, 
sim time next is 2934600.0000, 
raw observation next is [-2.0, 85.00000000000001, 0.0, 0.0, 26.0, 25.03733944521629, 0.3788874816967112, 0.0, 1.0, 43585.23188445978], 
processed observation next is [1.0, 1.0, 0.40720221606648205, 0.8500000000000001, 0.0, 0.0, 0.6666666666666666, 0.5864449537680242, 0.6262958272322371, 0.0, 1.0, 0.2075487232593323], 
reward next is 0.7925, 
noisyNet noise sample is [array([-0.33410773], dtype=float32), 0.79012054]. 
=============================================
[2019-04-04 02:34:56,426] A3C_AGENT_WORKER-Thread-7 INFO:Evaluating...
[2019-04-04 02:34:56,428] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 02:34:56,429] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 02:34:56,430] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:34:56,442] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run33
[2019-04-04 02:34:56,430] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 02:34:56,468] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:34:56,473] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:34:56,479] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run33
[2019-04-04 02:34:56,499] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run33
[2019-04-04 02:37:05,188] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 02:37:14,895] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.3713392], dtype=float32), 0.18737526]
[2019-04-04 02:37:14,895] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-0.212662926, 55.29264590166667, 0.0, 299.1506964, 26.0, 25.14714292221994, 0.3599546319986479, 0.0, 1.0, 60121.82689455563]
[2019-04-04 02:37:14,895] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 02:37:14,896] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.82065642e-23 2.66133244e-18 2.30927456e-22 1.08160625e-19
 1.58341084e-19 1.57762557e-23 1.00000000e+00], sampled 0.49602363470323885
[2019-04-04 02:37:33,778] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 02:37:41,154] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 02:37:42,201] A3C_AGENT_WORKER-Thread-7 INFO:Global step: 3200000, evaluation results [3200000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 02:37:54,568] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.2917983e-26 4.5710123e-20 3.5350147e-24 5.9393846e-21 1.3099660e-21
 6.5816600e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:37:54,629] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3137
[2019-04-04 02:37:54,638] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.2, 27.0, 60.00000000000001, 71.0, 26.0, 25.09708197598049, 0.3468932078170839, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2824800.0000, 
sim time next is 2825400.0000, 
raw observation next is [6.1, 27.5, 49.0, 66.0, 26.0, 25.39858942528093, 0.3696981277740017, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.6315789473684211, 0.275, 0.16333333333333333, 0.07292817679558011, 0.6666666666666666, 0.616549118773411, 0.6232327092580006, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2304093], dtype=float32), -0.78376913]. 
=============================================
[2019-04-04 02:38:16,678] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.0227804e-25 6.7947511e-20 1.1834409e-23 6.0979561e-21 6.4568628e-21
 1.2431710e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:38:16,678] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5183
[2019-04-04 02:38:16,747] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 44.0, 0.0, 0.0, 26.0, 25.1243201200029, 0.334301217318226, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2836800.0000, 
sim time next is 2837400.0000, 
raw observation next is [2.0, 44.00000000000001, 0.0, 0.0, 26.0, 25.02018448515511, 0.3170445803483048, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.518005540166205, 0.44000000000000006, 0.0, 0.0, 0.6666666666666666, 0.5850153737629258, 0.6056815267827683, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2537309], dtype=float32), 1.3557129]. 
=============================================
[2019-04-04 02:38:27,204] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.1754907e-26 1.5049352e-20 1.2857939e-24 3.2986589e-22 3.8426164e-22
 8.6032147e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:38:27,204] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7963
[2019-04-04 02:38:27,227] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 100.0, 0.0, 0.0, 26.0, 25.29962031211526, 0.3234255689822552, 0.0, 1.0, 39888.30023972627], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3110400.0000, 
sim time next is 3111000.0000, 
raw observation next is [0.1666666666666667, 100.0, 0.0, 0.0, 26.0, 25.30764709337674, 0.3225186723125005, 0.0, 1.0, 39707.41281503543], 
processed observation next is [1.0, 0.0, 0.4672206832871654, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6089705911147284, 0.6075062241041668, 0.0, 1.0, 0.18908291816683537], 
reward next is 0.8109, 
noisyNet noise sample is [array([0.512769], dtype=float32), 0.9791888]. 
=============================================
[2019-04-04 02:38:27,233] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[85.79782 ]
 [85.9021  ]
 [85.90226 ]
 [85.91514 ]
 [85.916275]], R is [[85.6971283 ]
 [85.65021515]
 [85.60206604]
 [85.54979706]
 [85.48479462]].
[2019-04-04 02:38:37,008] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.3538792e-24 1.9366277e-18 1.7816045e-23 8.5334860e-21 1.4707018e-20
 1.1981077e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:38:37,039] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6285
[2019-04-04 02:38:37,065] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.12207493336014, 0.3212465910923743, 0.0, 1.0, 38766.80752917886], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3017400.0000, 
sim time next is 3018000.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.14347998162659, 0.3159453418563955, 0.0, 1.0, 38639.12592372071], 
processed observation next is [0.0, 0.9565217391304348, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5952899984688825, 0.6053151139521319, 0.0, 1.0, 0.1839958377320034], 
reward next is 0.8160, 
noisyNet noise sample is [array([-0.0885066], dtype=float32), -0.5237424]. 
=============================================
[2019-04-04 02:38:37,094] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[82.18959 ]
 [82.184044]
 [82.2066  ]
 [82.272575]
 [82.28867 ]], R is [[82.26776123]
 [82.26048279]
 [82.25265503]
 [82.24425507]
 [82.23524475]].
[2019-04-04 02:38:55,166] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.2430028e-26 2.0710334e-20 3.0753303e-24 1.5214839e-21 3.5824792e-21
 3.8204654e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:38:55,166] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3356
[2019-04-04 02:38:55,179] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.5, 74.0, 0.0, 0.0, 26.0, 25.10521224618351, 0.3854976130159011, 0.0, 1.0, 41756.19654774977], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3367800.0000, 
sim time next is 3368400.0000, 
raw observation next is [-5.666666666666667, 75.0, 0.0, 0.0, 26.0, 25.08904066718254, 0.3781800287084036, 0.0, 1.0, 41630.78543731099], 
processed observation next is [1.0, 1.0, 0.30563250230840255, 0.75, 0.0, 0.0, 0.6666666666666666, 0.5907533889318785, 0.6260600095694678, 0.0, 1.0, 0.19824183541576662], 
reward next is 0.8018, 
noisyNet noise sample is [array([1.1921962], dtype=float32), -0.9189834]. 
=============================================
[2019-04-04 02:38:59,759] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.0387031e-26 1.5074583e-19 4.1780117e-24 2.2312619e-21 2.6769883e-21
 2.8366294e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:38:59,759] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7503
[2019-04-04 02:38:59,780] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.0, 59.0, 0.0, 0.0, 26.0, 25.00275232802323, 0.320051535442822, 0.0, 1.0, 58838.26892040134], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3695400.0000, 
sim time next is 3696000.0000, 
raw observation next is [4.0, 59.0, 0.0, 0.0, 26.0, 24.95074321591507, 0.3244083154454887, 0.0, 1.0, 63040.06553439812], 
processed observation next is [0.0, 0.782608695652174, 0.5734072022160666, 0.59, 0.0, 0.0, 0.6666666666666666, 0.579228601326256, 0.6081361051484963, 0.0, 1.0, 0.30019078825903867], 
reward next is 0.6998, 
noisyNet noise sample is [array([1.2865485], dtype=float32), 0.24464855]. 
=============================================
[2019-04-04 02:38:59,795] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[86.117195]
 [85.76031 ]
 [85.180016]
 [84.528404]
 [84.22019 ]], R is [[86.16698456]
 [86.02513123]
 [85.92536926]
 [85.89004517]
 [85.89209747]].
[2019-04-04 02:39:04,935] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.7147999e-26 2.3684989e-20 2.0168509e-24 1.6876986e-21 1.1545822e-21
 1.7554332e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:39:04,935] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5045
[2019-04-04 02:39:04,966] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 26.0, 25.09750664945245, 0.4839227792317282, 0.0, 1.0, 183667.2044460197], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3443400.0000, 
sim time next is 3444000.0000, 
raw observation next is [1.0, 79.0, 0.0, 0.0, 26.0, 25.15815893315217, 0.5120168556426314, 0.0, 1.0, 101774.7258670429], 
processed observation next is [1.0, 0.8695652173913043, 0.4903047091412743, 0.79, 0.0, 0.0, 0.6666666666666666, 0.5965132444293474, 0.6706722852142105, 0.0, 1.0, 0.4846415517478233], 
reward next is 0.5154, 
noisyNet noise sample is [array([-0.15527895], dtype=float32), -1.0897664]. 
=============================================
[2019-04-04 02:39:04,979] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[82.88259]
 [82.66985]
 [82.873  ]
 [82.03429]
 [82.63679]], R is [[82.76287079]
 [82.06063843]
 [81.29436493]
 [81.39235687]
 [81.44372559]].
[2019-04-04 02:39:16,651] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.9365709e-27 5.8354260e-21 1.1322704e-24 8.8450286e-22 4.8575778e-22
 4.3579662e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 02:39:16,651] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9554
[2019-04-04 02:39:16,685] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 86.0, 0.0, 0.0, 26.0, 25.52261783542158, 0.4797229743279218, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3454200.0000, 
sim time next is 3454800.0000, 
raw observation next is [1.0, 86.0, 0.0, 0.0, 26.0, 25.49432476357946, 0.4652035281472713, 0.0, 1.0, 18753.37839158103], 
processed observation next is [1.0, 1.0, 0.4903047091412743, 0.86, 0.0, 0.0, 0.6666666666666666, 0.6245270636316217, 0.6550678427157571, 0.0, 1.0, 0.08930180186467157], 
reward next is 0.9107, 
noisyNet noise sample is [array([1.1193619], dtype=float32), -1.0555001]. 
=============================================
[2019-04-04 02:39:17,247] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.5230305e-26 1.2031258e-19 2.4168675e-24 5.6581517e-22 1.3622375e-21
 4.9958635e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:39:17,247] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5371
[2019-04-04 02:39:17,296] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 62.5, 0.0, 0.0, 26.0, 24.68831369794104, 0.2071734665632334, 0.0, 1.0, 42846.83216407803], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3393000.0000, 
sim time next is 3393600.0000, 
raw observation next is [-3.0, 63.33333333333333, 0.0, 0.0, 26.0, 24.62811026913117, 0.1969820042321454, 0.0, 1.0, 42831.4019685987], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.6333333333333333, 0.0, 0.0, 0.6666666666666666, 0.5523425224275975, 0.5656606680773818, 0.0, 1.0, 0.20395905699332714], 
reward next is 0.7960, 
noisyNet noise sample is [array([1.2039768], dtype=float32), 1.3600549]. 
=============================================
[2019-04-04 02:39:34,609] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.8808065e-26 4.8026391e-20 9.7980863e-25 1.8163633e-22 6.2444432e-22
 1.2699459e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:39:34,609] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2808
[2019-04-04 02:39:34,674] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 73.0, 19.0, 184.8333333333333, 26.0, 25.20968755229383, 0.3100796989575785, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3742800.0000, 
sim time next is 3743400.0000, 
raw observation next is [-4.0, 72.0, 32.99999999999999, 233.6666666666666, 26.0, 25.25838864699619, 0.3102797900131691, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.3518005540166205, 0.72, 0.10999999999999997, 0.25819521178637195, 0.6666666666666666, 0.6048657205830157, 0.6034265966710564, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.7939234], dtype=float32), -1.1431161]. 
=============================================
[2019-04-04 02:39:40,986] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.1454830e-27 1.5364517e-19 1.3273535e-24 5.5705364e-22 7.1808976e-22
 3.3274803e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:39:40,987] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3862
[2019-04-04 02:39:41,056] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.833333333333334, 76.0, 62.33333333333334, 347.6666666666667, 26.0, 25.35380518662232, 0.3805504265076136, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3831000.0000, 
sim time next is 3831600.0000, 
raw observation next is [-4.666666666666667, 75.0, 76.66666666666667, 397.3333333333333, 26.0, 25.36013457968831, 0.4162422112728754, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.3333333333333333, 0.75, 0.2555555555555556, 0.43904235727440144, 0.6666666666666666, 0.6133445483073592, 0.6387474037576252, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6623022], dtype=float32), -1.4234773]. 
=============================================
[2019-04-04 02:39:46,264] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.01060169e-26 3.38799568e-21 9.65954324e-26 1.61810972e-22
 4.98410745e-23 1.05620456e-26 1.00000000e+00], sum to 1.0000
[2019-04-04 02:39:46,265] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9202
[2019-04-04 02:39:46,311] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 35.0, 109.0, 724.0, 26.0, 26.44083596781659, 0.5639495359215754, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4096800.0000, 
sim time next is 4097400.0000, 
raw observation next is [-1.833333333333333, 34.5, 110.6666666666667, 739.0, 26.0, 26.53381008076571, 0.5822239171785163, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.41181902123730385, 0.345, 0.368888888888889, 0.8165745856353591, 0.6666666666666666, 0.7111508400638092, 0.6940746390595054, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.770107], dtype=float32), -0.6802947]. 
=============================================
[2019-04-04 02:39:49,444] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.6409334e-26 6.3829041e-21 8.2003605e-25 1.0318386e-21 3.9797316e-22
 8.5110935e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 02:39:49,445] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0442
[2019-04-04 02:39:49,503] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.5, 41.5, 116.0, 824.0, 26.0, 26.30913480052656, 0.6939995124403638, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3936600.0000, 
sim time next is 3937200.0000, 
raw observation next is [-5.333333333333333, 40.33333333333334, 114.1666666666667, 818.0, 26.0, 26.5681600492984, 0.7215807174674339, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.3148661126500462, 0.40333333333333343, 0.38055555555555565, 0.9038674033149171, 0.6666666666666666, 0.7140133374415333, 0.7405269058224779, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1343129], dtype=float32), -0.37754288]. 
=============================================
[2019-04-04 02:40:13,467] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.9855686e-26 1.7960395e-20 3.1182814e-25 3.3705782e-22 4.7726090e-22
 2.5078937e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:40:13,467] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2244
[2019-04-04 02:40:13,487] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 37.0, 214.0, 669.0, 26.0, 25.11015128226857, 0.4021523512516984, 0.0, 1.0, 18688.23204216188], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4195800.0000, 
sim time next is 4196400.0000, 
raw observation next is [2.0, 38.0, 209.5, 572.3333333333334, 26.0, 25.11339744448698, 0.403284642309629, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.518005540166205, 0.38, 0.6983333333333334, 0.6324125230202579, 0.6666666666666666, 0.5927831203739151, 0.6344282141032097, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.09839364], dtype=float32), -1.813085]. 
=============================================
[2019-04-04 02:40:16,915] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.1918192e-26 7.8137329e-20 2.1636743e-24 7.9841448e-22 1.3677570e-21
 3.9972531e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:40:16,915] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3801
[2019-04-04 02:40:17,005] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.333333333333334, 48.0, 94.66666666666667, 516.6666666666666, 26.0, 25.39920327571621, 0.4137441524632471, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4178400.0000, 
sim time next is 4179000.0000, 
raw observation next is [-4.166666666666667, 46.5, 97.33333333333334, 545.3333333333333, 26.0, 25.53127723325728, 0.4232072172641966, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.3471837488457987, 0.465, 0.3244444444444445, 0.6025782688766114, 0.6666666666666666, 0.6276064361047734, 0.6410690724213989, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1568643], dtype=float32), 0.45994812]. 
=============================================
[2019-04-04 02:40:17,009] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[89.785324]
 [89.94023 ]
 [89.345   ]
 [88.38716 ]
 [87.79248 ]], R is [[89.80198669]
 [89.90396881]
 [90.00492859]
 [89.30267334]
 [89.22054291]].
[2019-04-04 02:40:23,745] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.8860822e-25 7.1298620e-20 1.3358219e-23 4.6035404e-21 2.0833063e-21
 3.1374440e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:40:23,779] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2343
[2019-04-04 02:40:23,793] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.2000000000000001, 63.66666666666667, 0.0, 0.0, 26.0, 25.42297883314252, 0.4494053147583457, 0.0, 1.0, 36296.55157329138], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4584000.0000, 
sim time next is 4584600.0000, 
raw observation next is [0.1, 64.0, 0.0, 0.0, 26.0, 25.43833897192, 0.4415889131197385, 0.0, 1.0, 26028.23901997651], 
processed observation next is [1.0, 0.043478260869565216, 0.4653739612188367, 0.64, 0.0, 0.0, 0.6666666666666666, 0.6198615809933333, 0.6471963043732462, 0.0, 1.0, 0.12394399533322148], 
reward next is 0.8761, 
noisyNet noise sample is [array([-0.9835133], dtype=float32), -2.095731]. 
=============================================
[2019-04-04 02:40:29,728] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.3565594e-26 1.9519926e-21 7.0283622e-25 4.8475636e-22 1.0445115e-21
 2.8956210e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 02:40:29,740] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1500
[2019-04-04 02:40:29,796] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.0, 37.0, 34.0, 0.0, 26.0, 28.48160046991902, 1.101731800763726, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4380000.0000, 
sim time next is 4380600.0000, 
raw observation next is [13.0, 37.5, 29.0, 0.0, 26.0, 28.60413684596418, 1.103704571478539, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.8227146814404434, 0.375, 0.09666666666666666, 0.0, 0.6666666666666666, 0.8836780704970151, 0.8679015238261796, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9114066], dtype=float32), 0.38179266]. 
=============================================
[2019-04-04 02:40:45,343] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.8976624e-25 2.2679062e-19 6.4526209e-23 2.1272433e-20 2.7504851e-20
 4.1456723e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:40:45,362] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6899
[2019-04-04 02:40:45,371] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.266666666666667, 67.33333333333334, 0.0, 0.0, 26.0, 25.70604074441254, 0.5464218602730851, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4422000.0000, 
sim time next is 4422600.0000, 
raw observation next is [4.15, 67.5, 0.0, 0.0, 26.0, 25.68619383743743, 0.536978543328997, 0.0, 1.0, 54266.03330125226], 
processed observation next is [1.0, 0.17391304347826086, 0.5775623268698062, 0.675, 0.0, 0.0, 0.6666666666666666, 0.6405161531197857, 0.6789928477763323, 0.0, 1.0, 0.25840968238691553], 
reward next is 0.7416, 
noisyNet noise sample is [array([0.92154825], dtype=float32), 0.24027108]. 
=============================================
[2019-04-04 02:41:02,785] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:41:02,785] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:41:02,789] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run25
[2019-04-04 02:41:12,123] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.7099406e-27 4.7297995e-22 6.5727280e-26 5.8041173e-23 5.8594663e-23
 5.8199104e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 02:41:12,123] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0336
[2019-04-04 02:41:12,152] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.0, 19.0, 103.5, 782.0, 26.0, 28.55247690706437, 1.108481634662004, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5065200.0000, 
sim time next is 5065800.0000, 
raw observation next is [12.0, 19.0, 101.0, 769.6666666666667, 26.0, 28.64044736531397, 1.126138800875722, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.7950138504155125, 0.19, 0.33666666666666667, 0.8504604051565379, 0.6666666666666666, 0.8867039471094976, 0.8753796002919074, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5193259], dtype=float32), 1.760656]. 
=============================================
[2019-04-04 02:41:13,234] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.0643766e-25 3.1745460e-19 8.1826467e-24 4.5271066e-21 1.4872263e-20
 8.4461888e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:41:13,235] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3583
[2019-04-04 02:41:13,258] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 55.0, 0.0, 0.0, 26.0, 25.4021136260202, 0.392895754940746, 0.0, 1.0, 40883.64912220329], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4832400.0000, 
sim time next is 4833000.0000, 
raw observation next is [-1.0, 55.0, 0.0, 0.0, 26.0, 25.37970209879177, 0.3897163277249688, 0.0, 1.0, 52233.35679059825], 
processed observation next is [0.0, 0.9565217391304348, 0.4349030470914128, 0.55, 0.0, 0.0, 0.6666666666666666, 0.6149751748993143, 0.6299054425749896, 0.0, 1.0, 0.24873027043142024], 
reward next is 0.7513, 
noisyNet noise sample is [array([-1.0906391], dtype=float32), 0.22734879]. 
=============================================
[2019-04-04 02:41:13,263] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[82.47238 ]
 [82.50236 ]
 [82.610344]
 [82.71236 ]
 [82.89632 ]], R is [[82.40651703]
 [82.38776398]
 [82.43872833]
 [82.39532471]
 [82.36665344]].
[2019-04-04 02:41:14,857] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.3634456e-24 1.2230203e-17 2.1721614e-22 5.9522708e-20 2.8032052e-19
 3.8353215e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 02:41:14,858] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3428
[2019-04-04 02:41:14,875] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 60.0, 0.0, 0.0, 26.0, 24.98567167088429, 0.2753736897993791, 0.0, 1.0, 39195.49813963148], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4849800.0000, 
sim time next is 4850400.0000, 
raw observation next is [-3.0, 60.00000000000001, 0.0, 0.0, 26.0, 24.95553235539923, 0.2694432766970734, 0.0, 1.0, 39210.04827775796], 
processed observation next is [0.0, 0.13043478260869565, 0.3795013850415513, 0.6000000000000001, 0.0, 0.0, 0.6666666666666666, 0.5796276962832693, 0.5898144255656911, 0.0, 1.0, 0.18671451560837124], 
reward next is 0.8133, 
noisyNet noise sample is [array([-0.09222473], dtype=float32), 0.7576327]. 
=============================================
[2019-04-04 02:41:15,608] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:41:15,609] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:41:15,618] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run25
[2019-04-04 02:41:18,421] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:41:18,421] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:41:18,424] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run25
[2019-04-04 02:41:22,266] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.4114300e-25 4.1161836e-19 4.6948852e-24 3.8067563e-21 2.6318787e-21
 1.1311479e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:41:22,274] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4534
[2019-04-04 02:41:22,341] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.466666666666667, 57.33333333333334, 284.5, 166.5, 26.0, 25.20180872532139, 0.3339613477849896, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4875600.0000, 
sim time next is 4876200.0000, 
raw observation next is [-1.2, 56.0, 300.0, 164.0, 26.0, 25.13760809353214, 0.3281663283309165, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.42936288088642666, 0.56, 1.0, 0.18121546961325966, 0.6666666666666666, 0.5948006744610117, 0.6093887761103055, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.80937535], dtype=float32), -0.28228405]. 
=============================================
[2019-04-04 02:41:34,837] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [8.2698488e-25 2.2398774e-19 2.0525160e-23 9.2510054e-21 1.2110591e-20
 6.6448618e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:41:34,837] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1089
[2019-04-04 02:41:34,910] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.95, 87.5, 0.0, 0.0, 26.0, 24.56896975712736, 0.189475279799225, 0.0, 1.0, 53340.48088945011], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 63000.0000, 
sim time next is 63600.0000, 
raw observation next is [4.766666666666667, 88.0, 0.0, 0.0, 26.0, 24.55331205150782, 0.195689115209195, 0.0, 1.0, 59863.37408746044], 
processed observation next is [0.0, 0.7391304347826086, 0.5946445060018468, 0.88, 0.0, 0.0, 0.6666666666666666, 0.5461093376256517, 0.5652297050697317, 0.0, 1.0, 0.285063686130764], 
reward next is 0.7149, 
noisyNet noise sample is [array([1.5109694], dtype=float32), 0.77511626]. 
=============================================
[2019-04-04 02:41:35,131] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.5493720e-25 8.0058722e-19 3.8788142e-23 1.0443646e-20 1.8328471e-20
 5.8928627e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:41:35,132] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1365
[2019-04-04 02:41:35,184] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 55.0, 0.0, 0.0, 26.0, 25.4820275813458, 0.4066708508060211, 0.0, 1.0, 27562.47700215958], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5023800.0000, 
sim time next is 5024400.0000, 
raw observation next is [-1.0, 55.0, 0.0, 0.0, 26.0, 25.45343541489133, 0.4037385770425423, 0.0, 1.0, 44397.96654327291], 
processed observation next is [1.0, 0.13043478260869565, 0.4349030470914128, 0.55, 0.0, 0.0, 0.6666666666666666, 0.6211196179076109, 0.6345795256808474, 0.0, 1.0, 0.21141888830129957], 
reward next is 0.7886, 
noisyNet noise sample is [array([0.6279998], dtype=float32), -0.030796183]. 
=============================================
[2019-04-04 02:41:39,396] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.5336366e-24 3.0503249e-18 1.4710032e-22 3.9866797e-20 5.5076393e-20
 3.8302627e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 02:41:39,414] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1954
[2019-04-04 02:41:39,483] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.833333333333333, 69.16666666666667, 0.0, 0.0, 26.0, 24.78060795889647, 0.2227844876729617, 0.0, 1.0, 39435.71227414166], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4857000.0000, 
sim time next is 4857600.0000, 
raw observation next is [-3.666666666666667, 67.33333333333334, 0.0, 0.0, 26.0, 24.7790003560034, 0.217262363353719, 0.0, 1.0, 39495.63758729546], 
processed observation next is [0.0, 0.21739130434782608, 0.3610341643582641, 0.6733333333333335, 0.0, 0.0, 0.6666666666666666, 0.5649166963336167, 0.572420787784573, 0.0, 1.0, 0.18807446470140696], 
reward next is 0.8119, 
noisyNet noise sample is [array([-0.24862094], dtype=float32), -0.9251958]. 
=============================================
[2019-04-04 02:41:43,789] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.5062781e-26 6.2817481e-19 1.5630989e-23 2.7325063e-21 3.5003056e-21
 6.7109373e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:41:43,789] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6233
[2019-04-04 02:41:43,800] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.5, 50.0, 0.0, 0.0, 26.0, 25.23796087682423, 0.2845205912529093, 0.0, 1.0, 38825.56857207812], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4937400.0000, 
sim time next is 4938000.0000, 
raw observation next is [-1.666666666666667, 50.0, 0.0, 0.0, 26.0, 25.21315946380812, 0.2880748799539728, 0.0, 1.0, 38510.72981795999], 
processed observation next is [1.0, 0.13043478260869565, 0.4164358264081256, 0.5, 0.0, 0.0, 0.6666666666666666, 0.6010966219840098, 0.5960249599846575, 0.0, 1.0, 0.1833844277045714], 
reward next is 0.8166, 
noisyNet noise sample is [array([0.3440758], dtype=float32), -0.26687825]. 
=============================================
[2019-04-04 02:41:43,804] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[83.87042]
 [83.93841]
 [83.9231 ]
 [83.7348 ]
 [83.46721]], R is [[83.76937866]
 [83.74680328]
 [83.71887207]
 [83.67276001]
 [83.57563019]].
[2019-04-04 02:41:43,916] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:41:43,916] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:41:43,927] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run25
[2019-04-04 02:41:44,204] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:41:44,204] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:41:44,208] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run25
[2019-04-04 02:41:44,254] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:41:44,254] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:41:44,265] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run25
[2019-04-04 02:41:44,413] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:41:44,413] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:41:44,417] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run25
[2019-04-04 02:41:46,268] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:41:46,268] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:41:46,280] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run25
[2019-04-04 02:41:47,089] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:41:47,089] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:41:47,092] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run25
[2019-04-04 02:41:49,381] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:41:49,381] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:41:49,384] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run25
[2019-04-04 02:41:50,953] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:41:50,953] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:41:50,957] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run25
[2019-04-04 02:41:52,277] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.3691989e-27 2.4239898e-21 2.5161074e-25 5.7902738e-23 6.3702762e-22
 2.8794839e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 02:41:52,277] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.0932
[2019-04-04 02:41:52,321] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.7, 19.0, 0.0, 0.0, 26.0, 26.77370561138118, 0.7795936152454107, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5090400.0000, 
sim time next is 5091000.0000, 
raw observation next is [8.65, 19.16666666666667, 0.0, 0.0, 26.0, 26.72111417084481, 0.7263804065684587, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.7022160664819946, 0.1916666666666667, 0.0, 0.0, 0.6666666666666666, 0.7267595142370675, 0.7421268021894862, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8374814], dtype=float32), 0.40469563]. 
=============================================
[2019-04-04 02:41:52,433] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[84.25223]
 [83.91758]
 [83.65609]
 [83.36737]
 [83.06534]], R is [[84.5749054 ]
 [84.72915649]
 [84.88186646]
 [85.03305054]
 [85.182724  ]].
[2019-04-04 02:41:54,101] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:41:54,103] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:41:54,149] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run25
[2019-04-04 02:41:58,860] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:41:58,860] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:41:58,864] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run25
[2019-04-04 02:42:01,713] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:42:01,713] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:42:01,716] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run25
[2019-04-04 02:42:02,969] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [9.7991335e-25 2.5823746e-19 3.3979325e-23 6.3303337e-21 1.0494365e-20
 7.6209580e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:42:02,970] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2070
[2019-04-04 02:42:03,034] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.05, 90.5, 0.0, 0.0, 26.0, 24.60460091242745, 0.2095965362051254, 0.0, 1.0, 40533.52185906207], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 77400.0000, 
sim time next is 78000.0000, 
raw observation next is [0.8666666666666667, 92.33333333333334, 0.0, 0.0, 26.0, 24.58182071120883, 0.2054789175634937, 0.0, 1.0, 40472.2897358344], 
processed observation next is [0.0, 0.9130434782608695, 0.4866112650046169, 0.9233333333333335, 0.0, 0.0, 0.6666666666666666, 0.5484850592674025, 0.5684929725211646, 0.0, 1.0, 0.19272518921825904], 
reward next is 0.8073, 
noisyNet noise sample is [array([-0.22290042], dtype=float32), -1.3091221]. 
=============================================
[2019-04-04 02:42:03,098] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[77.10228]
 [77.13245]
 [77.2078 ]
 [77.32941]
 [77.45355]], R is [[77.10372925]
 [77.13967896]
 [77.17500305]
 [77.20973206]
 [77.24397278]].
[2019-04-04 02:42:08,194] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:42:08,194] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:42:08,198] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run25
[2019-04-04 02:42:08,889] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.6436545e-26 3.0257527e-20 2.6405936e-24 1.6945948e-21 9.8491038e-22
 6.5887943e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:42:08,889] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5887
[2019-04-04 02:42:08,978] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.3, 68.0, 0.0, 0.0, 26.0, 23.38884594130199, 0.05037957051855502, 1.0, 1.0, 203504.0791787211], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 113400.0000, 
sim time next is 114000.0000, 
raw observation next is [-7.3, 68.0, 0.0, 0.0, 26.0, 24.1150240080375, 0.1411152908471363, 0.0, 1.0, 158974.5997729312], 
processed observation next is [1.0, 0.30434782608695654, 0.26038781163434904, 0.68, 0.0, 0.0, 0.6666666666666666, 0.5095853340031251, 0.5470384302823788, 0.0, 1.0, 0.7570219036806247], 
reward next is 0.2430, 
noisyNet noise sample is [array([0.58826256], dtype=float32), -1.0653207]. 
=============================================
[2019-04-04 02:42:08,998] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[82.589424]
 [80.525406]
 [78.18427 ]
 [78.04508 ]
 [77.91976 ]], R is [[81.91957092]
 [81.13130188]
 [80.35649109]
 [80.33447266]
 [80.31292725]].
[2019-04-04 02:42:09,371] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:42:09,372] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:42:09,381] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run25
[2019-04-04 02:42:15,427] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.0838166e-25 8.9528517e-20 4.6436083e-23 4.3516192e-20 2.0669450e-20
 6.2827353e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:42:15,427] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5050
[2019-04-04 02:42:15,441] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.4, 70.5, 0.0, 0.0, 26.0, 24.34317931302077, 0.1537389125694532, 0.0, 1.0, 45709.14642020054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 165000.0000, 
sim time next is 165600.0000, 
raw observation next is [-8.4, 71.0, 0.0, 0.0, 26.0, 24.29287266623632, 0.1440526799793897, 0.0, 1.0, 45292.55173545384], 
processed observation next is [1.0, 0.9565217391304348, 0.2299168975069252, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5244060555196933, 0.54801755999313, 0.0, 1.0, 0.21567881778787543], 
reward next is 0.7843, 
noisyNet noise sample is [array([-0.32957655], dtype=float32), 0.70389116]. 
=============================================
[2019-04-04 02:42:25,122] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.2234139e-25 2.2044449e-19 9.4486534e-23 1.6063156e-20 2.0539681e-20
 4.2310685e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:42:25,127] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7210
[2019-04-04 02:42:25,167] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 67.0, 0.0, 0.0, 26.0, 23.75846298507189, -0.02503167225461504, 0.0, 1.0, 45780.8829456885], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 270000.0000, 
sim time next is 270600.0000, 
raw observation next is [-9.0, 67.5, 0.0, 0.0, 26.0, 23.66661994082127, -0.04518219247734239, 0.0, 1.0, 45864.26586220293], 
processed observation next is [1.0, 0.13043478260869565, 0.21329639889196678, 0.675, 0.0, 0.0, 0.6666666666666666, 0.4722183284017725, 0.4849392691742192, 0.0, 1.0, 0.21840126601049012], 
reward next is 0.7816, 
noisyNet noise sample is [array([0.87503105], dtype=float32), -1.0484666]. 
=============================================
[2019-04-04 02:42:51,966] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.6019152e-26 2.3658068e-21 7.7451611e-25 4.8850988e-22 2.1373925e-22
 1.5748332e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:42:51,966] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7624
[2019-04-04 02:42:51,997] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 58.5, 99.66666666666666, 669.0, 26.0, 25.86301252543569, 0.3895616733027364, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 733800.0000, 
sim time next is 734400.0000, 
raw observation next is [-0.6, 57.0, 107.5, 614.0, 26.0, 25.84235977200898, 0.3889771677855018, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.44598337950138506, 0.57, 0.35833333333333334, 0.6784530386740332, 0.6666666666666666, 0.6535299810007483, 0.6296590559285006, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0723581], dtype=float32), -0.6562549]. 
=============================================
[2019-04-04 02:43:09,425] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [9.4293345e-27 1.2413402e-20 1.3661331e-24 8.1674680e-22 1.5181727e-22
 1.7245898e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:43:09,425] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1597
[2019-04-04 02:43:09,488] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 76.0, 14.5, 0.0, 26.0, 25.6168682738347, 0.2850177096609972, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 720000.0000, 
sim time next is 720600.0000, 
raw observation next is [-2.3, 76.0, 19.33333333333334, 0.0, 26.0, 25.63106150430561, 0.2780822330586022, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.3988919667590028, 0.76, 0.06444444444444447, 0.0, 0.6666666666666666, 0.6359217920254675, 0.5926940776862007, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.2007089], dtype=float32), 0.95053893]. 
=============================================
[2019-04-04 02:43:12,089] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.4525867e-28 2.7499066e-21 1.5311227e-25 3.8283637e-23 4.2468529e-23
 1.3307541e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:43:12,089] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5161
[2019-04-04 02:43:12,151] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 76.0, 14.5, 0.0, 26.0, 25.6168682738347, 0.2850177096609972, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 720000.0000, 
sim time next is 720600.0000, 
raw observation next is [-2.3, 76.0, 19.33333333333334, 0.0, 26.0, 25.63106150430561, 0.2780822330586022, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.3988919667590028, 0.76, 0.06444444444444447, 0.0, 0.6666666666666666, 0.6359217920254675, 0.5926940776862007, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.13952813], dtype=float32), 0.077698685]. 
=============================================
[2019-04-04 02:43:17,552] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2916470e-27 4.1684804e-22 1.3224144e-26 4.2226194e-24 2.6052342e-24
 3.1508501e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 02:43:17,554] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2657
[2019-04-04 02:43:17,569] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [18.71666666666667, 63.33333333333333, 167.3333333333333, 0.0, 26.0, 25.10276888810612, 0.503040886186883, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1167000.0000, 
sim time next is 1167600.0000, 
raw observation next is [18.63333333333333, 63.66666666666667, 169.1666666666667, 0.0, 26.0, 25.09145144767247, 0.5026303510881603, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.9787626962142197, 0.6366666666666667, 0.563888888888889, 0.0, 0.6666666666666666, 0.590954287306039, 0.6675434503627201, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.81900364], dtype=float32), -0.6907166]. 
=============================================
[2019-04-04 02:43:17,787] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.88841709e-27 2.11256456e-21 1.46185491e-25 1.04355916e-22
 5.23538836e-23 2.56075343e-27 1.00000000e+00], sum to 1.0000
[2019-04-04 02:43:17,794] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.0105
[2019-04-04 02:43:17,832] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.366666666666667, 65.0, 0.0, 0.0, 26.0, 24.68861710742677, 0.2354357478652617, 0.0, 1.0, 43195.55861671759], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 771600.0000, 
sim time next is 772200.0000, 
raw observation next is [-6.45, 65.5, 0.0, 0.0, 26.0, 24.65519749063233, 0.2280741530499846, 0.0, 1.0, 43069.01960734087], 
processed observation next is [1.0, 0.9565217391304348, 0.28393351800554023, 0.655, 0.0, 0.0, 0.6666666666666666, 0.5545997908860274, 0.5760247176833282, 0.0, 1.0, 0.20509056955876606], 
reward next is 0.7949, 
noisyNet noise sample is [array([1.185661], dtype=float32), 0.6409319]. 
=============================================
[2019-04-04 02:43:18,464] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.2316013e-27 9.6078563e-22 1.7142267e-25 1.1019238e-22 4.5491049e-23
 4.2149524e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 02:43:18,464] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3318
[2019-04-04 02:43:18,529] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.899999999999999, 83.66666666666666, 57.33333333333334, 0.0, 26.0, 26.28186620210179, 0.4503129875735648, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 830400.0000, 
sim time next is 831000.0000, 
raw observation next is [-3.9, 84.83333333333334, 55.66666666666666, 0.0, 26.0, 26.26077279109061, 0.4445927523343778, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3545706371191136, 0.8483333333333334, 0.18555555555555553, 0.0, 0.6666666666666666, 0.6883977325908841, 0.6481975841114592, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7502086], dtype=float32), -1.117117]. 
=============================================
[2019-04-04 02:43:18,542] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[84.48985]
 [84.42725]
 [84.58366]
 [84.6656 ]
 [84.70296]], R is [[84.77430725]
 [84.92656708]
 [85.07730103]
 [85.22653198]
 [85.37426758]].
[2019-04-04 02:43:19,155] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.20430466e-27 1.75450703e-21 1.01074145e-24 1.25520505e-22
 6.62214083e-23 1.17834426e-26 1.00000000e+00], sum to 1.0000
[2019-04-04 02:43:19,155] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0921
[2019-04-04 02:43:19,204] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 26.0, 25.44406321659527, 0.5634002874657053, 0.0, 1.0, 39164.60673135251], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1314000.0000, 
sim time next is 1314600.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 26.0, 25.56069704653373, 0.5620170903168232, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.5069252077562327, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6300580872111441, 0.6873390301056078, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.00645245], dtype=float32), -0.37238422]. 
=============================================
[2019-04-04 02:43:21,763] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.6725740e-27 1.1569984e-21 2.3157017e-25 9.3326972e-23 8.4660357e-23
 3.7648356e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 02:43:21,769] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3924
[2019-04-04 02:43:21,791] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.45, 65.5, 0.0, 0.0, 26.0, 24.65519749063233, 0.2280741530499846, 0.0, 1.0, 43069.01960734087], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 772200.0000, 
sim time next is 772800.0000, 
raw observation next is [-6.533333333333333, 66.0, 0.0, 0.0, 26.0, 24.62260968390205, 0.2208489905353493, 0.0, 1.0, 42931.40748044642], 
processed observation next is [1.0, 0.9565217391304348, 0.2816251154201293, 0.66, 0.0, 0.0, 0.6666666666666666, 0.5518841403251707, 0.5736163301784497, 0.0, 1.0, 0.20443527371641151], 
reward next is 0.7956, 
noisyNet noise sample is [array([-0.11468173], dtype=float32), -0.16267519]. 
=============================================
[2019-04-04 02:43:24,563] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.1650364e-27 1.3900017e-20 1.2298371e-24 1.1982186e-22 1.7744362e-22
 6.6594713e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:43:24,564] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1764
[2019-04-04 02:43:24,586] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.533333333333333, 78.0, 0.0, 0.0, 26.0, 24.57566399822124, 0.1843559917088464, 0.0, 1.0, 39268.00146015653], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 876000.0000, 
sim time next is 876600.0000, 
raw observation next is [-1.45, 77.5, 0.0, 0.0, 26.0, 24.61344081981355, 0.1847178269616885, 0.0, 1.0, 39216.82556991155], 
processed observation next is [1.0, 0.13043478260869565, 0.422437673130194, 0.775, 0.0, 0.0, 0.6666666666666666, 0.5511200683177959, 0.5615726089872295, 0.0, 1.0, 0.18674678842815023], 
reward next is 0.8133, 
noisyNet noise sample is [array([2.060619], dtype=float32), 0.7861222]. 
=============================================
[2019-04-04 02:43:27,334] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.2385764e-28 2.5048106e-22 4.4953960e-26 9.8682736e-24 9.6829434e-24
 1.4253338e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 02:43:27,334] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1632
[2019-04-04 02:43:27,371] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.0, 96.0, 0.0, 0.0, 26.0, 25.23814493990613, 0.4082755560931193, 0.0, 1.0, 38745.95211914683], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 947400.0000, 
sim time next is 948000.0000, 
raw observation next is [5.0, 96.0, 0.0, 0.0, 26.0, 25.23897602048101, 0.408950040818914, 0.0, 1.0, 38645.2547826621], 
processed observation next is [1.0, 1.0, 0.6011080332409973, 0.96, 0.0, 0.0, 0.6666666666666666, 0.603248001706751, 0.6363166802729713, 0.0, 1.0, 0.18402502277458144], 
reward next is 0.8160, 
noisyNet noise sample is [array([0.6786573], dtype=float32), 0.14080119]. 
=============================================
[2019-04-04 02:43:27,410] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[86.44812 ]
 [86.42257 ]
 [86.40834 ]
 [86.421005]
 [86.440506]], R is [[86.39489746]
 [86.34645081]
 [86.297966  ]
 [86.24939728]
 [86.20067596]].
[2019-04-04 02:43:35,993] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.5293098e-27 1.5045950e-22 1.1467754e-25 7.6437302e-23 5.4276647e-23
 1.4403538e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 02:43:35,993] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.6134
[2019-04-04 02:43:36,032] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 27.33333333333333, 0.0, 26.0, 25.16116252289169, 0.4639600056447074, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1440600.0000, 
sim time next is 1441200.0000, 
raw observation next is [1.1, 92.0, 22.66666666666666, 0.0, 26.0, 25.59986680016079, 0.4871786981100868, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.49307479224376743, 0.92, 0.07555555555555554, 0.0, 0.6666666666666666, 0.6333222333467324, 0.6623928993700289, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.0281121], dtype=float32), 0.7621842]. 
=============================================
[2019-04-04 02:43:37,267] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0644630e-28 1.6390832e-22 1.6049326e-26 6.5585836e-24 2.2681958e-23
 8.5203796e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 02:43:37,273] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6739
[2019-04-04 02:43:37,283] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.9, 63.33333333333333, 0.0, 0.0, 26.0, 25.86888738047314, 0.69235391897169, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1114800.0000, 
sim time next is 1115400.0000, 
raw observation next is [12.8, 63.66666666666666, 0.0, 0.0, 26.0, 25.85807146590832, 0.684409044622568, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.8171745152354571, 0.6366666666666666, 0.0, 0.0, 0.6666666666666666, 0.6548392888256934, 0.7281363482075226, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.35037732], dtype=float32), 0.2100895]. 
=============================================
[2019-04-04 02:43:42,944] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.48358175e-29 4.09457701e-24 1.34118545e-27 2.32624786e-24
 2.31873362e-25 5.08030106e-29 1.00000000e+00], sum to 1.0000
[2019-04-04 02:43:42,945] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9863
[2019-04-04 02:43:42,977] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.28333333333333, 86.0, 125.3333333333333, 0.0, 26.0, 26.73518660766892, 0.7087886452958619, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 994200.0000, 
sim time next is 994800.0000, 
raw observation next is [12.36666666666667, 86.0, 126.6666666666667, 0.0, 26.0, 26.77678412591492, 0.5768948313239153, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.8051708217913206, 0.86, 0.42222222222222233, 0.0, 0.6666666666666666, 0.7313986771595765, 0.6922982771079718, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2631447], dtype=float32), 0.7347418]. 
=============================================
[2019-04-04 02:43:44,986] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [6.6864947e-29 3.5384821e-24 1.9642592e-27 1.5433265e-24 6.9674981e-25
 2.0275373e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 02:43:44,992] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8365
[2019-04-04 02:43:45,000] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.8, 49.0, 160.5, 0.0, 26.0, 27.3023873323706, 0.8474252623866438, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1605600.0000, 
sim time next is 1606200.0000, 
raw observation next is [13.8, 49.0, 155.3333333333333, 0.0, 26.0, 27.31025335739378, 0.8572699626386875, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.844875346260388, 0.49, 0.5177777777777777, 0.0, 0.6666666666666666, 0.7758544464494816, 0.7857566542128959, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.45598486], dtype=float32), -1.1557332]. 
=============================================
[2019-04-04 02:43:48,049] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.5507981e-28 1.1619167e-21 7.7516012e-26 6.3865510e-23 5.5490704e-23
 7.7577791e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 02:43:48,052] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9325
[2019-04-04 02:43:48,062] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.8, 100.0, 72.66666666666667, 0.0, 26.0, 24.75232287207088, 0.4452029026257652, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1262400.0000, 
sim time next is 1263000.0000, 
raw observation next is [13.8, 100.0, 68.33333333333333, 0.0, 26.0, 24.74238628687793, 0.4413254361501447, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.844875346260388, 1.0, 0.22777777777777777, 0.0, 0.6666666666666666, 0.5618655239064942, 0.6471084787167148, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.85037833], dtype=float32), -0.1607158]. 
=============================================
[2019-04-04 02:43:48,070] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[89.032585]
 [89.0433  ]
 [89.03025 ]
 [89.04662 ]
 [89.08918 ]], R is [[89.12905121]
 [89.23776245]
 [89.34538269]
 [89.45192719]
 [89.55741119]].
[2019-04-04 02:43:49,540] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-04 02:43:49,542] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 02:43:49,543] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 02:43:49,543] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:43:49,545] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:43:49,545] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 02:43:49,552] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run34
[2019-04-04 02:43:49,548] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run34
[2019-04-04 02:43:49,554] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:43:49,607] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run34
[2019-04-04 02:44:07,101] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.37205482], dtype=float32), 0.20443279]
[2019-04-04 02:44:07,102] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-13.52358921333333, 65.60930286333334, 0.0, 0.0, 26.0, 22.38837754193496, -0.3267248188118726, 0.0, 1.0, 49101.76855865443]
[2019-04-04 02:44:07,102] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 02:44:07,103] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [6.1198488e-25 7.9221750e-19 1.9722069e-23 2.3963162e-21 4.5863627e-21
 3.5956312e-24 1.0000000e+00], sampled 0.6226131072133843
[2019-04-04 02:46:05,864] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 02:46:36,059] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 02:46:42,701] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 02:46:43,734] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 3300000, evaluation results [3300000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 02:46:52,695] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0337237e-25 1.7681406e-20 7.4222447e-24 3.1135333e-21 1.8843804e-21
 5.0510547e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:46:52,695] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9370
[2019-04-04 02:46:52,748] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.2, 96.0, 0.0, 0.0, 26.0, 24.69507344116894, 0.4478227964968755, 0.0, 1.0, 30260.43785789407], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1278000.0000, 
sim time next is 1278600.0000, 
raw observation next is [7.016666666666667, 96.0, 0.0, 0.0, 26.0, 24.70398737735966, 0.4487052616607377, 0.0, 1.0, 27244.39166156606], 
processed observation next is [0.0, 0.8260869565217391, 0.656971375807941, 0.96, 0.0, 0.0, 0.6666666666666666, 0.5586656147799717, 0.6495684205535792, 0.0, 1.0, 0.1297351983884098], 
reward next is 0.8703, 
noisyNet noise sample is [array([-0.0484365], dtype=float32), -0.5150218]. 
=============================================
[2019-04-04 02:46:53,227] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [7.9715213e-28 1.9941697e-22 1.7846092e-25 6.8464910e-23 4.4458951e-23
 1.9736179e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 02:46:53,233] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.9275
[2019-04-04 02:46:53,280] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 12.0, 0.0, 26.0, 25.85200607246324, 0.5032279217539041, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1443000.0000, 
sim time next is 1443600.0000, 
raw observation next is [1.1, 92.0, 9.0, 0.0, 26.0, 25.89448892359356, 0.5067921796671271, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.49307479224376743, 0.92, 0.03, 0.0, 0.6666666666666666, 0.65787407696613, 0.6689307265557091, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4614149], dtype=float32), -1.0963821]. 
=============================================
[2019-04-04 02:46:53,918] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.3792259e-29 4.0366227e-23 7.2029641e-27 3.6151196e-24 3.5072105e-24
 1.4186555e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 02:46:53,930] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7431
[2019-04-04 02:46:53,954] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.3, 96.0, 80.5, 354.0, 26.0, 26.06658065194552, 0.5663758513927363, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1508400.0000, 
sim time next is 1509000.0000, 
raw observation next is [3.483333333333333, 95.5, 83.0, 472.0000000000001, 26.0, 26.13726231911924, 0.583333948388435, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.559095106186519, 0.955, 0.27666666666666667, 0.521546961325967, 0.6666666666666666, 0.6781051932599368, 0.6944446494628117, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.7922328], dtype=float32), -0.7502746]. 
=============================================
[2019-04-04 02:46:54,076] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[89.82863]
 [89.38902]
 [88.7373 ]
 [87.84895]
 [87.92937]], R is [[90.08050537]
 [90.17970276]
 [90.27790833]
 [90.3751297 ]
 [90.47138214]].
[2019-04-04 02:46:54,344] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.52234621e-27 1.14995424e-20 1.46565810e-24 1.02644006e-22
 2.86104199e-22 4.50605792e-26 1.00000000e+00], sum to 1.0000
[2019-04-04 02:46:54,350] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5623
[2019-04-04 02:46:54,392] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.2, 96.0, 0.0, 0.0, 26.0, 25.31714148992549, 0.4543666911725995, 0.0, 1.0, 47884.22954857634], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1483800.0000, 
sim time next is 1484400.0000, 
raw observation next is [2.2, 96.0, 0.0, 0.0, 26.0, 25.34876411467571, 0.4542443835074009, 0.0, 1.0, 40823.0403931985], 
processed observation next is [1.0, 0.17391304347826086, 0.5235457063711911, 0.96, 0.0, 0.0, 0.6666666666666666, 0.6123970095563092, 0.651414794502467, 0.0, 1.0, 0.19439543044380236], 
reward next is 0.8056, 
noisyNet noise sample is [array([-0.10105547], dtype=float32), 0.31666848]. 
=============================================
[2019-04-04 02:46:57,745] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.1642947e-28 4.0664978e-23 1.6148753e-26 1.2118937e-23 8.7249815e-24
 1.9249218e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 02:46:57,745] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1263
[2019-04-04 02:46:57,756] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.7, 51.66666666666667, 76.33333333333333, 539.6666666666666, 26.0, 25.99701545753517, 0.6898716494369279, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1523400.0000, 
sim time next is 1524000.0000, 
raw observation next is [11.8, 51.33333333333334, 76.66666666666667, 508.8333333333334, 26.0, 26.4355068260707, 0.7178440016166582, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.7894736842105264, 0.5133333333333334, 0.2555555555555556, 0.5622467771639044, 0.6666666666666666, 0.7029589021725583, 0.7392813338722194, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.25799483], dtype=float32), -1.4068246]. 
=============================================
[2019-04-04 02:46:57,808] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[87.78247 ]
 [88.0035  ]
 [88.06653 ]
 [88.191246]
 [88.16044 ]], R is [[87.90354919]
 [88.02451324]
 [88.14427185]
 [88.26283264]
 [88.38020325]].
[2019-04-04 02:47:01,893] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4993259e-27 6.2376135e-21 2.8984421e-25 6.3330561e-23 5.8907753e-23
 8.3981169e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 02:47:01,894] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3953
[2019-04-04 02:47:01,906] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.416666666666667, 79.50000000000001, 0.0, 0.0, 26.0, 25.45817698002547, 0.4907562761383306, 0.0, 1.0, 39311.98276358261], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1581000.0000, 
sim time next is 1581600.0000, 
raw observation next is [5.333333333333334, 80.0, 0.0, 0.0, 26.0, 25.44032852103544, 0.4990318731000995, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.6103416435826409, 0.8, 0.0, 0.0, 0.6666666666666666, 0.6200273767529533, 0.6663439577000332, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2619328], dtype=float32), -0.11129072]. 
=============================================
[2019-04-04 02:47:05,059] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.8214994e-28 1.0306540e-22 4.8131149e-26 1.5288946e-23 3.7612257e-23
 1.1254519e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 02:47:05,059] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2727
[2019-04-04 02:47:05,068] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.116666666666667, 67.33333333333333, 0.0, 0.0, 26.0, 26.07684633698063, 0.6564377842249214, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1624200.0000, 
sim time next is 1624800.0000, 
raw observation next is [8.833333333333334, 68.66666666666667, 0.0, 0.0, 26.0, 25.96661127666945, 0.6369090086672163, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.7072945521698984, 0.6866666666666668, 0.0, 0.0, 0.6666666666666666, 0.6638842730557876, 0.7123030028890721, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.75070584], dtype=float32), -1.3050233]. 
=============================================
[2019-04-04 02:47:16,882] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.9739332e-25 6.9969205e-20 1.9995084e-23 5.5343626e-21 3.9526365e-21
 3.6922339e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:47:16,882] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2899
[2019-04-04 02:47:16,961] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 87.0, 71.5, 0.0, 26.0, 24.99933895277263, 0.3434906658038827, 0.0, 1.0, 38419.50409158017], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1783200.0000, 
sim time next is 1783800.0000, 
raw observation next is [-3.1, 87.0, 66.0, 0.0, 26.0, 25.0162134254107, 0.3453880413953148, 0.0, 1.0, 34211.48044602873], 
processed observation next is [0.0, 0.6521739130434783, 0.37673130193905824, 0.87, 0.22, 0.0, 0.6666666666666666, 0.5846844521175584, 0.6151293471317716, 0.0, 1.0, 0.16291181164775584], 
reward next is 0.8371, 
noisyNet noise sample is [array([0.18833835], dtype=float32), -0.6975711]. 
=============================================
[2019-04-04 02:47:30,885] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.1437502e-25 2.4805766e-19 3.5195291e-23 3.5737983e-21 5.0909530e-21
 1.2043396e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:47:30,886] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9687
[2019-04-04 02:47:30,921] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.4, 78.0, 0.0, 0.0, 26.0, 23.91090293483037, -0.03593548115421654, 0.0, 1.0, 44743.60724809644], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1913400.0000, 
sim time next is 1914000.0000, 
raw observation next is [-8.4, 78.0, 0.0, 0.0, 26.0, 23.84220468040011, -0.04368505932205657, 0.0, 1.0, 44849.5148479788], 
processed observation next is [1.0, 0.13043478260869565, 0.2299168975069252, 0.78, 0.0, 0.0, 0.6666666666666666, 0.48685039003334235, 0.4854383135593145, 0.0, 1.0, 0.21356911832370856], 
reward next is 0.7864, 
noisyNet noise sample is [array([-0.21320663], dtype=float32), 1.5117347]. 
=============================================
[2019-04-04 02:47:30,948] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[80.2018  ]
 [80.19255 ]
 [80.20601 ]
 [80.18318 ]
 [80.225845]], R is [[80.19734192]
 [80.18230438]
 [80.16786957]
 [80.15274048]
 [80.13700104]].
[2019-04-04 02:47:32,563] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.1918914e-24 2.0591402e-18 4.2939004e-22 6.4658136e-20 7.8515893e-20
 7.8419091e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 02:47:32,563] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9693
[2019-04-04 02:47:32,576] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.2, 79.00000000000001, 0.0, 0.0, 26.0, 23.64554927976098, -0.01044088688623023, 0.0, 1.0, 47093.56772531285], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1833000.0000, 
sim time next is 1833600.0000, 
raw observation next is [-6.2, 79.0, 0.0, 0.0, 26.0, 23.61491124713546, -0.005473165893936001, 0.0, 1.0, 47108.94294084184], 
processed observation next is [0.0, 0.21739130434782608, 0.2908587257617729, 0.79, 0.0, 0.0, 0.6666666666666666, 0.4679092705946217, 0.498175611368688, 0.0, 1.0, 0.22432829971829446], 
reward next is 0.7757, 
noisyNet noise sample is [array([0.83387935], dtype=float32), -0.39476264]. 
=============================================
[2019-04-04 02:47:36,052] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [4.15421893e-25 1.04519471e-18 5.36461451e-23 1.15321385e-20
 6.61013437e-21 2.75869648e-24 1.00000000e+00], sum to 1.0000
[2019-04-04 02:47:36,052] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0150
[2019-04-04 02:47:36,104] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.700000000000001, 78.0, 0.0, 0.0, 26.0, 24.51320408492168, 0.1553648203399516, 0.0, 1.0, 42295.06904517108], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2171400.0000, 
sim time next is 2172000.0000, 
raw observation next is [-6.700000000000001, 78.0, 0.0, 0.0, 26.0, 24.49793332647902, 0.1320579434254893, 0.0, 1.0, 42595.02810495335], 
processed observation next is [1.0, 0.13043478260869565, 0.2770083102493075, 0.78, 0.0, 0.0, 0.6666666666666666, 0.5414944438732517, 0.5440193144751632, 0.0, 1.0, 0.20283346716644454], 
reward next is 0.7972, 
noisyNet noise sample is [array([0.46663514], dtype=float32), 0.053324785]. 
=============================================
[2019-04-04 02:47:36,192] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[78.47898 ]
 [78.41924 ]
 [78.36569 ]
 [78.3217  ]
 [78.274055]], R is [[78.46779633]
 [78.48171234]
 [78.49517822]
 [78.50814056]
 [78.52068329]].
[2019-04-04 02:47:37,874] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.23582256e-26 3.64081123e-21 1.78894504e-24 8.40339368e-22
 4.75202640e-22 4.76197888e-26 1.00000000e+00], sum to 1.0000
[2019-04-04 02:47:37,874] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0129
[2019-04-04 02:47:37,938] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.8, 72.33333333333334, 0.0, 0.0, 26.0, 25.16496973485071, 0.3266315870859962, 0.0, 1.0, 44971.0784355768], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2240400.0000, 
sim time next is 2241000.0000, 
raw observation next is [-5.9, 73.0, 0.0, 0.0, 26.0, 25.03924028569166, 0.3090821555550189, 0.0, 1.0, 44492.89274786998], 
processed observation next is [1.0, 0.9565217391304348, 0.2991689750692521, 0.73, 0.0, 0.0, 0.6666666666666666, 0.5866033571409716, 0.6030273851850063, 0.0, 1.0, 0.21187091784699988], 
reward next is 0.7881, 
noisyNet noise sample is [array([2.1618037], dtype=float32), 1.476342]. 
=============================================
[2019-04-04 02:47:37,949] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[78.565765]
 [78.82349 ]
 [78.76819 ]
 [78.75055 ]
 [78.79147 ]], R is [[78.62641144]
 [78.62599945]
 [78.62615967]
 [78.62774658]
 [78.6287384 ]].
[2019-04-04 02:47:44,854] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.6207064e-27 7.2313675e-22 8.9289835e-26 1.7004470e-23 2.8036486e-23
 5.7099378e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 02:47:44,855] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6002
[2019-04-04 02:47:44,924] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.3, 79.0, 128.0, 392.5, 26.0, 25.64573485180795, 0.3195491509297663, 1.0, 1.0, 22671.47621044272], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1936800.0000, 
sim time next is 1937400.0000, 
raw observation next is [-7.016666666666667, 78.33333333333334, 142.3333333333333, 340.3333333333333, 26.0, 25.71693068917333, 0.3307760351060114, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.2682363804247461, 0.7833333333333334, 0.4744444444444443, 0.3760589318600368, 0.6666666666666666, 0.643077557431111, 0.6102586783686704, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3490996], dtype=float32), 0.23031878]. 
=============================================
[2019-04-04 02:47:45,448] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.9410717e-26 9.8373986e-20 3.2784475e-23 2.1456526e-21 4.7205190e-21
 5.3628904e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:47:45,449] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8427
[2019-04-04 02:47:45,477] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.2, 87.0, 0.0, 0.0, 26.0, 24.2293864504177, 0.07941157896194796, 0.0, 1.0, 41074.15279122541], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2006400.0000, 
sim time next is 2007000.0000, 
raw observation next is [-6.2, 87.0, 0.0, 0.0, 26.0, 24.24099296882595, 0.07411455812404721, 0.0, 1.0, 41065.97827217368], 
processed observation next is [1.0, 0.21739130434782608, 0.2908587257617729, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5200827474021624, 0.5247048527080157, 0.0, 1.0, 0.19555227748654133], 
reward next is 0.8044, 
noisyNet noise sample is [array([0.08447897], dtype=float32), 1.221549]. 
=============================================
[2019-04-04 02:47:45,492] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[79.776245]
 [79.81623 ]
 [79.86663 ]
 [79.94121 ]
 [80.01339 ]], R is [[79.7621994 ]
 [79.76898193]
 [79.77560425]
 [79.78201294]
 [79.78833008]].
[2019-04-04 02:48:11,812] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.5416830e-27 2.0926102e-20 1.4140551e-24 3.9858393e-22 2.3143717e-22
 3.2000676e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:48:11,812] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8663
[2019-04-04 02:48:11,848] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.366666666666667, 76.0, 0.0, 0.0, 26.0, 24.67120873571065, 0.2435578951541035, 0.0, 1.0, 44124.53194704583], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2244000.0000, 
sim time next is 2244600.0000, 
raw observation next is [-6.45, 76.5, 0.0, 0.0, 26.0, 24.61607529525443, 0.2322844172711413, 0.0, 1.0, 44119.31403432272], 
processed observation next is [1.0, 1.0, 0.28393351800554023, 0.765, 0.0, 0.0, 0.6666666666666666, 0.5513396079378691, 0.5774281390903805, 0.0, 1.0, 0.21009197159201295], 
reward next is 0.7899, 
noisyNet noise sample is [array([0.31270856], dtype=float32), 0.023847492]. 
=============================================
[2019-04-04 02:48:27,896] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.1332393e-25 5.8797192e-19 3.7913040e-23 4.3662664e-21 4.8329053e-21
 3.0655385e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:48:27,896] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6842
[2019-04-04 02:48:27,980] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 47.0, 84.83333333333334, 293.8333333333334, 26.0, 25.0147912495596, 0.3145594170279688, 0.0, 1.0, 18716.26167040183], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2389200.0000, 
sim time next is 2389800.0000, 
raw observation next is [0.0, 47.0, 83.66666666666667, 246.6666666666667, 26.0, 25.01874624765584, 0.3068420104076771, 0.0, 1.0, 18714.58419284765], 
processed observation next is [0.0, 0.6521739130434783, 0.46260387811634357, 0.47, 0.2788888888888889, 0.27255985267034993, 0.6666666666666666, 0.5848955206379868, 0.6022806701358924, 0.0, 1.0, 0.08911706758498882], 
reward next is 0.9109, 
noisyNet noise sample is [array([0.1703076], dtype=float32), 2.0458045]. 
=============================================
[2019-04-04 02:48:37,455] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.7939182e-26 1.3764509e-21 2.6092210e-25 2.9762955e-22 1.7029861e-22
 2.2081222e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 02:48:37,455] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5965
[2019-04-04 02:48:37,541] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.666666666666666, 30.83333333333333, 205.3333333333333, 115.3333333333333, 26.0, 25.91481586539268, 0.4433567312374745, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2814600.0000, 
sim time next is 2815200.0000, 
raw observation next is [6.0, 30.0, 183.5, 86.5, 26.0, 25.62846943455152, 0.417903513866839, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.6288088642659281, 0.3, 0.6116666666666667, 0.09558011049723757, 0.6666666666666666, 0.6357057862126266, 0.6393011712889464, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.04263492], dtype=float32), 1.0491959]. 
=============================================
[2019-04-04 02:48:50,640] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.4048475e-27 9.5892609e-21 4.7838483e-25 1.2516860e-22 1.4418252e-22
 2.0625280e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:48:50,648] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8988
[2019-04-04 02:48:50,697] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 93.0, 77.0, 78.0, 26.0, 25.34201775090492, 0.3161023597650016, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2883600.0000, 
sim time next is 2884200.0000, 
raw observation next is [0.8333333333333334, 94.16666666666666, 67.66666666666666, 51.99999999999999, 26.0, 25.3906590342895, 0.3090531820027545, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4856879039704525, 0.9416666666666665, 0.22555555555555554, 0.0574585635359116, 0.6666666666666666, 0.6158882528574582, 0.6030177273342515, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3399065], dtype=float32), 0.17843911]. 
=============================================
[2019-04-04 02:48:51,757] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.9879310e-25 2.4048796e-20 6.2011389e-24 1.4295200e-21 2.5145772e-21
 4.2526037e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:48:51,757] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8517
[2019-04-04 02:48:51,804] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.3, 25.0, 27.0, 161.0, 26.0, 25.05024041960854, 0.2351474450329766, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2480400.0000, 
sim time next is 2481000.0000, 
raw observation next is [2.933333333333334, 25.5, 20.0, 121.6666666666667, 26.0, 24.98811310147967, 0.2170247160326375, 0.0, 1.0, 31150.43560682388], 
processed observation next is [0.0, 0.7391304347826086, 0.543859649122807, 0.255, 0.06666666666666667, 0.134438305709024, 0.6666666666666666, 0.5823427584566391, 0.5723415720108792, 0.0, 1.0, 0.1483354076515423], 
reward next is 0.8517, 
noisyNet noise sample is [array([0.15324843], dtype=float32), 0.42931223]. 
=============================================
[2019-04-04 02:48:51,814] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[81.47516]
 [81.72813]
 [81.95451]
 [82.08893]
 [82.22928]], R is [[81.25970459]
 [81.44710541]
 [81.63263702]
 [81.8163147 ]
 [81.90911102]].
[2019-04-04 02:48:56,774] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.5818325e-27 1.6536608e-21 1.5356853e-25 3.2838689e-22 6.2227772e-23
 2.4916530e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 02:48:56,775] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5470
[2019-04-04 02:48:56,817] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.4166666666666667, 46.33333333333334, 191.0, 189.6666666666667, 26.0, 25.46031412401701, 0.4455401773113885, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2639400.0000, 
sim time next is 2640000.0000, 
raw observation next is [-0.2333333333333334, 45.66666666666667, 177.5, 200.3333333333333, 26.0, 25.84649216947739, 0.4813745730640873, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.456140350877193, 0.4566666666666667, 0.5916666666666667, 0.2213627992633517, 0.6666666666666666, 0.6538743474564491, 0.6604581910213624, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2567307], dtype=float32), -0.8735063]. 
=============================================
[2019-04-04 02:48:56,841] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[83.97145 ]
 [84.26727 ]
 [84.24032 ]
 [84.35373 ]
 [84.558334]], R is [[83.95609283]
 [84.11653137]
 [83.96305084]
 [83.18169403]
 [82.71803284]].
[2019-04-04 02:49:06,762] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [4.3629562e-27 2.2387391e-21 3.3649848e-25 1.0674184e-22 7.8475050e-23
 2.4355686e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 02:49:06,762] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.5908
[2019-04-04 02:49:06,793] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.016666666666667, 52.83333333333334, 238.0, 155.0, 26.0, 25.75410879292788, 0.3978622046072235, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2635800.0000, 
sim time next is 2636400.0000, 
raw observation next is [-1.733333333333333, 51.66666666666667, 241.5, 151.0, 26.0, 25.77486686494223, 0.3909109622776804, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.41458910433979695, 0.5166666666666667, 0.805, 0.16685082872928178, 0.6666666666666666, 0.6479055720785191, 0.6303036540925602, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.04970422], dtype=float32), 0.33578578]. 
=============================================
[2019-04-04 02:49:11,089] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [4.11955037e-24 1.38130270e-18 1.26487611e-22 4.12753190e-21
 2.26457673e-20 1.00732394e-23 1.00000000e+00], sum to 1.0000
[2019-04-04 02:49:11,090] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.6013
[2019-04-04 02:49:11,208] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.166666666666667, 82.83333333333333, 0.0, 0.0, 26.0, 24.2849136261236, 0.1887786813028078, 0.0, 1.0, 42819.63314037053], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2956200.0000, 
sim time next is 2956800.0000, 
raw observation next is [-3.333333333333333, 81.66666666666667, 0.0, 0.0, 26.0, 24.26592666942121, 0.181884651841648, 0.0, 1.0, 42757.28256246836], 
processed observation next is [0.0, 0.21739130434782608, 0.37026777469990774, 0.8166666666666668, 0.0, 0.0, 0.6666666666666666, 0.5221605557851007, 0.5606282172805493, 0.0, 1.0, 0.20360610744032553], 
reward next is 0.7964, 
noisyNet noise sample is [array([0.04113435], dtype=float32), -0.7862097]. 
=============================================
[2019-04-04 02:49:26,589] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.5663942e-26 1.6295765e-21 5.1557104e-25 3.6279299e-22 5.1692084e-22
 5.2701028e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 02:49:26,590] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7136
[2019-04-04 02:49:26,653] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.666666666666666, 26.0, 114.1666666666667, 0.0, 26.0, 25.85769130177954, 0.3072778393010481, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2817600.0000, 
sim time next is 2818200.0000, 
raw observation next is [6.833333333333334, 25.0, 110.3333333333333, 0.0, 26.0, 25.3868266219746, 0.3275546232182864, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.651892890120037, 0.25, 0.36777777777777765, 0.0, 0.6666666666666666, 0.6155688851645499, 0.6091848744060955, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5751159], dtype=float32), -0.63450605]. 
=============================================
[2019-04-04 02:49:27,562] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.5195890e-26 3.4067878e-21 3.5088967e-24 4.6631217e-21 1.1869596e-21
 6.0298408e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:49:27,563] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3823
[2019-04-04 02:49:27,601] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.1333333333333334, 51.33333333333334, 18.33333333333333, 89.0, 26.0, 26.12073813495166, 0.4262957961360306, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2654400.0000, 
sim time next is 2655000.0000, 
raw observation next is [-0.04999999999999999, 52.0, 7.0, 82.0, 26.0, 25.83168798086335, 0.4159473848624478, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.461218836565097, 0.52, 0.023333333333333334, 0.09060773480662983, 0.6666666666666666, 0.6526406650719458, 0.6386491282874825, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.52774996], dtype=float32), 1.2519236]. 
=============================================
[2019-04-04 02:49:27,681] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[77.91022 ]
 [78.115204]
 [78.30594 ]
 [78.60292 ]
 [78.46524 ]], R is [[77.8338623 ]
 [78.05552673]
 [78.27497101]
 [78.49222565]
 [78.70730591]].
[2019-04-04 02:49:29,048] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.9080649e-26 2.4920895e-20 3.1055929e-24 7.5137550e-22 1.7460676e-21
 6.9087914e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:49:29,048] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9558
[2019-04-04 02:49:29,071] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 44.0, 0.0, 0.0, 26.0, 25.09097864215428, 0.3975565018577034, 0.0, 1.0, 71025.65680228345], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2839800.0000, 
sim time next is 2840400.0000, 
raw observation next is [2.0, 44.0, 0.0, 0.0, 26.0, 25.22788796881627, 0.4133522363374943, 0.0, 1.0, 56398.76055023851], 
processed observation next is [1.0, 0.9130434782608695, 0.518005540166205, 0.44, 0.0, 0.0, 0.6666666666666666, 0.6023239974013558, 0.6377840787791648, 0.0, 1.0, 0.2685655264297072], 
reward next is 0.7314, 
noisyNet noise sample is [array([0.10698379], dtype=float32), -0.23318858]. 
=============================================
[2019-04-04 02:49:34,166] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.21471294e-25 1.17515175e-20 1.16640973e-23 8.95578433e-21
 2.96404905e-21 5.58384641e-25 1.00000000e+00], sum to 1.0000
[2019-04-04 02:49:34,166] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7724
[2019-04-04 02:49:34,205] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.833333333333333, 90.16666666666667, 0.0, 0.0, 26.0, 25.47357703164096, 0.5392773512662085, 0.0, 1.0, 85564.41175837441], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3275400.0000, 
sim time next is 3276000.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 26.0, 25.42866328382127, 0.5417853259457495, 0.0, 1.0, 83301.59557899495], 
processed observation next is [1.0, 0.9565217391304348, 0.296398891966759, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6190552736517724, 0.6805951086485832, 0.0, 1.0, 0.3966742646618807], 
reward next is 0.6033, 
noisyNet noise sample is [array([0.04690825], dtype=float32), -0.4828453]. 
=============================================
[2019-04-04 02:49:34,246] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[77.464005]
 [77.157295]
 [76.96543 ]
 [77.10031 ]
 [77.175995]], R is [[77.63970184]
 [77.45585632]
 [77.38801575]
 [77.52493286]
 [77.74968719]].
[2019-04-04 02:49:38,942] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.7588528e-25 1.9365159e-19 3.0756134e-23 1.8948614e-21 6.0013096e-21
 2.1300160e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:49:38,942] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1809
[2019-04-04 02:49:38,955] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.0, 64.0, 0.0, 0.0, 26.0, 24.318504843732, 0.1113574496505897, 0.0, 1.0, 41158.01254702439], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2784000.0000, 
sim time next is 2784600.0000, 
raw observation next is [-7.0, 64.0, 0.0, 0.0, 26.0, 24.30686974707331, 0.1000246302561149, 0.0, 1.0, 41201.67779432892], 
processed observation next is [1.0, 0.21739130434782608, 0.2686980609418283, 0.64, 0.0, 0.0, 0.6666666666666666, 0.5255724789227759, 0.533341543418705, 0.0, 1.0, 0.19619846568728058], 
reward next is 0.8038, 
noisyNet noise sample is [array([-0.2818556], dtype=float32), 0.18832722]. 
=============================================
[2019-04-04 02:49:45,290] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.5391218e-26 1.0750283e-20 2.2409401e-24 9.0260551e-22 5.6691705e-22
 7.0036821e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:49:45,290] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0640
[2019-04-04 02:49:45,344] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.3333333333333334, 97.66666666666666, 53.83333333333333, 0.0, 26.0, 25.43062423849855, 0.3127764253049045, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2886000.0000, 
sim time next is 2886600.0000, 
raw observation next is [0.1666666666666666, 98.83333333333334, 58.66666666666666, 0.0, 26.0, 25.46394615576764, 0.3076971888866248, 1.0, 1.0, 18682.08336876821], 
processed observation next is [1.0, 0.391304347826087, 0.4672206832871654, 0.9883333333333334, 0.1955555555555555, 0.0, 0.6666666666666666, 0.6219955129806367, 0.6025657296288749, 1.0, 1.0, 0.0889623017560391], 
reward next is 0.9110, 
noisyNet noise sample is [array([1.1100374], dtype=float32), 0.74650925]. 
=============================================
[2019-04-04 02:49:50,441] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3114826e-26 1.9807438e-21 5.2514046e-25 8.8544096e-23 6.1580532e-22
 5.1249585e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 02:49:50,443] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3740
[2019-04-04 02:49:50,512] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.833333333333333, 28.33333333333334, 26.99999999999999, 56.0, 26.0, 25.75972350954894, 0.3971785715792159, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2826600.0000, 
sim time next is 2827200.0000, 
raw observation next is [5.666666666666666, 28.66666666666667, 16.0, 51.0, 26.0, 25.82055846524057, 0.3767408953169065, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.6195752539242845, 0.28666666666666674, 0.05333333333333334, 0.056353591160221, 0.6666666666666666, 0.6517132054367142, 0.6255802984389688, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.00774612], dtype=float32), 2.1774602]. 
=============================================
[2019-04-04 02:49:53,486] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [3.2122140e-25 4.5972605e-20 4.8104399e-24 1.6717692e-21 2.5304929e-21
 4.4169500e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:49:53,486] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.3058
[2019-04-04 02:49:53,553] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.04670453195752, 0.3205895278681672, 0.0, 1.0, 47609.16342866742], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3003000.0000, 
sim time next is 3003600.0000, 
raw observation next is [-2.0, 60.00000000000001, 0.0, 0.0, 26.0, 25.01585084798057, 0.3201143538857718, 0.0, 1.0, 55982.0596810974], 
processed observation next is [0.0, 0.782608695652174, 0.40720221606648205, 0.6000000000000001, 0.0, 0.0, 0.6666666666666666, 0.5846542373317142, 0.6067047846285906, 0.0, 1.0, 0.2665812365766543], 
reward next is 0.7334, 
noisyNet noise sample is [array([-0.58622664], dtype=float32), -0.2657834]. 
=============================================
[2019-04-04 02:50:11,582] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.2893397e-27 1.2746021e-22 3.9566089e-26 3.0202474e-23 7.9691398e-23
 7.3833437e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 02:50:11,582] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8329
[2019-04-04 02:50:11,633] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.333333333333333, 90.33333333333334, 102.6666666666667, 776.1666666666667, 26.0, 26.82653675238075, 0.8238859196633371, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3248400.0000, 
sim time next is 3249000.0000, 
raw observation next is [-3.0, 85.5, 101.0, 769.0, 26.0, 26.89292948876345, 0.8343330492233875, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3795013850415513, 0.855, 0.33666666666666667, 0.8497237569060774, 0.6666666666666666, 0.7410774573969542, 0.7781110164077959, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.42625207], dtype=float32), -0.4942833]. 
=============================================
[2019-04-04 02:50:11,684] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[85.161476]
 [85.389   ]
 [85.621155]
 [85.92963 ]
 [86.13025 ]], R is [[85.41882324]
 [85.56463623]
 [85.708992  ]
 [85.85190582]
 [85.99338531]].
[2019-04-04 02:50:17,858] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [4.1567824e-27 3.6179256e-21 9.2987857e-25 1.8950484e-22 3.0289371e-22
 1.0669847e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:50:17,859] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8417
[2019-04-04 02:50:17,955] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 76.0, 0.0, 0.0, 26.0, 25.70721875866396, 0.5073779360134476, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3535800.0000, 
sim time next is 3536400.0000, 
raw observation next is [-1.0, 74.0, 0.0, 0.0, 26.0, 25.68242989942167, 0.4713688236872011, 0.0, 1.0, 21402.0364450249], 
processed observation next is [1.0, 0.9565217391304348, 0.4349030470914128, 0.74, 0.0, 0.0, 0.6666666666666666, 0.6402024916184725, 0.657122941229067, 0.0, 1.0, 0.10191445926202333], 
reward next is 0.8981, 
noisyNet noise sample is [array([-0.40428963], dtype=float32), 1.5964347]. 
=============================================
[2019-04-04 02:50:22,184] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.6041727e-27 8.9866873e-21 3.0580385e-25 1.0058577e-22 6.6379736e-23
 5.7124025e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:50:22,184] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4973
[2019-04-04 02:50:22,256] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.0, 82.66666666666666, 29.99999999999999, 194.6666666666666, 26.0, 25.31658754867071, 0.3994344285826794, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3311400.0000, 
sim time next is 3312000.0000, 
raw observation next is [-11.0, 84.0, 44.0, 245.0, 26.0, 25.51245199902309, 0.4123911872431674, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.15789473684210528, 0.84, 0.14666666666666667, 0.27071823204419887, 0.6666666666666666, 0.6260376665852574, 0.6374637290810558, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6462974], dtype=float32), 0.7920189]. 
=============================================
[2019-04-04 02:50:22,259] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[86.04606 ]
 [85.18019 ]
 [83.35176 ]
 [80.895355]
 [79.08202 ]], R is [[86.50576019]
 [86.64070129]
 [86.37052917]
 [85.53951263]
 [84.7204361 ]].
[2019-04-04 02:50:30,609] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.7813465e-26 2.3204297e-20 4.5839248e-24 3.6202432e-22 5.8315733e-22
 1.7434285e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:50:30,609] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0891
[2019-04-04 02:50:30,639] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.815426107001, 0.2758249883687985, 0.0, 1.0, 41115.58644341333], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3381600.0000, 
sim time next is 3382200.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.82780537830124, 0.2769812020430907, 0.0, 1.0, 41144.1849516783], 
processed observation next is [1.0, 0.13043478260869565, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5689837815251032, 0.5923270673476969, 0.0, 1.0, 0.19592469024608714], 
reward next is 0.8041, 
noisyNet noise sample is [array([2.2542415], dtype=float32), -0.6044139]. 
=============================================
[2019-04-04 02:50:37,388] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.9241777e-27 9.1687549e-22 2.8619435e-25 1.6714622e-22 1.9731411e-22
 6.4750959e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 02:50:37,388] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7594
[2019-04-04 02:50:37,448] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 26.0, 25.15794340859629, 0.4202834500044692, 0.0, 1.0, 39121.35633113418], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3439200.0000, 
sim time next is 3439800.0000, 
raw observation next is [1.0, 79.0, 0.0, 0.0, 26.0, 25.05325187706555, 0.4154175641174782, 1.0, 1.0, 86742.98464506885], 
processed observation next is [1.0, 0.8260869565217391, 0.4903047091412743, 0.79, 0.0, 0.0, 0.6666666666666666, 0.5877709897554624, 0.6384725213724928, 1.0, 1.0, 0.413061831643185], 
reward next is 0.5869, 
noisyNet noise sample is [array([-1.3539954], dtype=float32), 0.24885197]. 
=============================================
[2019-04-04 02:50:44,525] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.5103344e-26 5.3515158e-22 1.1458271e-24 6.0331436e-22 3.8864885e-22
 8.2598143e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 02:50:44,528] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2381
[2019-04-04 02:50:44,565] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.333333333333333, 75.0, 0.0, 0.0, 26.0, 25.09432860842811, 0.4514649525827606, 1.0, 1.0, 92167.60119952037], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3436800.0000, 
sim time next is 3437400.0000, 
raw observation next is [1.166666666666667, 77.0, 0.0, 0.0, 26.0, 25.17686605181428, 0.4678758360979512, 1.0, 1.0, 18682.27693586107], 
processed observation next is [1.0, 0.782608695652174, 0.49492151431209613, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5980721709845233, 0.6559586120326504, 1.0, 1.0, 0.08896322350410034], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.535061], dtype=float32), 1.9052659]. 
=============================================
[2019-04-04 02:50:45,091] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [3.6168090e-28 1.0061613e-21 1.4000506e-25 2.0289439e-23 7.4744432e-23
 8.1036407e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 02:50:45,091] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.9285
[2019-04-04 02:50:45,129] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 71.0, 91.66666666666666, 489.3333333333334, 26.0, 25.56464750909159, 0.4532993567749493, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3487800.0000, 
sim time next is 3488400.0000, 
raw observation next is [-1.0, 71.0, 93.5, 534.5, 26.0, 25.66047569335633, 0.4571809373515999, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4349030470914128, 0.71, 0.31166666666666665, 0.5906077348066299, 0.6666666666666666, 0.6383729744463608, 0.6523936457838667, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.124447], dtype=float32), 0.2295986]. 
=============================================
[2019-04-04 02:51:10,943] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [7.02795256e-27 6.58521444e-22 2.09195041e-25 4.06564490e-23
 2.12943320e-22 1.17274275e-27 1.00000000e+00], sum to 1.0000
[2019-04-04 02:51:10,944] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1981
[2019-04-04 02:51:10,993] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.666666666666666, 43.66666666666667, 0.0, 0.0, 26.0, 25.5749373153522, 0.5331132582481731, 1.0, 1.0, 79609.53915348771], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3958800.0000, 
sim time next is 3959400.0000, 
raw observation next is [-6.833333333333334, 44.33333333333333, 0.0, 0.0, 26.0, 25.56894520502246, 0.5356341692579053, 0.0, 1.0, 57114.12798528703], 
processed observation next is [1.0, 0.8260869565217391, 0.27331486611265005, 0.4433333333333333, 0.0, 0.0, 0.6666666666666666, 0.6307454337518715, 0.6785447230859685, 0.0, 1.0, 0.27197203802517633], 
reward next is 0.7280, 
noisyNet noise sample is [array([0.5231521], dtype=float32), -0.29348606]. 
=============================================
[2019-04-04 02:51:13,054] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.9389999e-28 9.2892581e-23 1.7166011e-26 1.4248038e-23 1.5325851e-23
 5.3857003e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 02:51:13,055] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6656
[2019-04-04 02:51:13,064] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 71.0, 113.0, 795.5, 26.0, 26.42042521017812, 0.5675738566548758, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3754800.0000, 
sim time next is 3755400.0000, 
raw observation next is [-2.833333333333333, 70.0, 113.6666666666667, 804.3333333333334, 26.0, 26.40492278255343, 0.5734627790939955, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.3841181902123731, 0.7, 0.378888888888889, 0.8887661141804789, 0.6666666666666666, 0.7004102318794526, 0.6911542596979986, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5398219], dtype=float32), -0.33075398]. 
=============================================
[2019-04-04 02:51:22,515] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4394475e-27 6.7644078e-22 8.9655718e-26 7.0022771e-23 2.5491507e-23
 5.0522832e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 02:51:22,516] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0030
[2019-04-04 02:51:22,541] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.333333333333333, 33.0, 115.1666666666667, 776.8333333333334, 26.0, 26.61289020800783, 0.6103998119332888, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4099200.0000, 
sim time next is 4099800.0000, 
raw observation next is [-1.166666666666667, 32.5, 116.3333333333333, 784.6666666666667, 26.0, 26.64697701860292, 0.6214571435686085, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.43028624192059095, 0.325, 0.38777777777777767, 0.8670349907918969, 0.6666666666666666, 0.7205814182169101, 0.7071523811895362, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.1921982], dtype=float32), 0.7481463]. 
=============================================
[2019-04-04 02:51:23,571] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.03820066e-26 1.68575531e-21 1.10046925e-24 6.76306635e-22
 3.80637706e-22 9.39128354e-27 1.00000000e+00], sum to 1.0000
[2019-04-04 02:51:23,572] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8857
[2019-04-04 02:51:23,604] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.4, 61.0, 0.0, 0.0, 26.0, 26.52149042070161, 0.7772192749213985, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4399200.0000, 
sim time next is 4399800.0000, 
raw observation next is [9.25, 61.16666666666667, 0.0, 0.0, 26.0, 26.46538364840368, 0.713899414376986, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.718836565096953, 0.6116666666666667, 0.0, 0.0, 0.6666666666666666, 0.7054486373669734, 0.7379664714589954, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8031654], dtype=float32), 1.4416449]. 
=============================================
[2019-04-04 02:51:27,007] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2750913e-25 1.5619098e-19 1.2990715e-23 2.5281196e-21 1.9370902e-21
 9.5477029e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:51:27,007] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2262
[2019-04-04 02:51:27,137] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-13.66666666666667, 67.0, 0.0, 0.0, 26.0, 23.32929303492273, -0.01849727919923463, 1.0, 1.0, 202394.243502916], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4000800.0000, 
sim time next is 4001400.0000, 
raw observation next is [-13.5, 66.0, 0.0, 0.0, 26.0, 23.56233043235398, 0.07339330325314407, 1.0, 1.0, 202955.1771653006], 
processed observation next is [1.0, 0.30434782608695654, 0.0886426592797784, 0.66, 0.0, 0.0, 0.6666666666666666, 0.4635275360294984, 0.5244644344177146, 1.0, 1.0, 0.9664532245966696], 
reward next is 0.0335, 
noisyNet noise sample is [array([1.4704896], dtype=float32), 0.709011]. 
=============================================
[2019-04-04 02:51:36,349] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.6669205e-27 2.6623640e-21 2.4015653e-24 7.2105153e-22 6.8205575e-22
 7.7147848e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:51:36,350] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9884
[2019-04-04 02:51:36,365] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.366666666666667, 66.66666666666667, 0.0, 0.0, 26.0, 25.71532737685096, 0.5925270272074693, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4416000.0000, 
sim time next is 4416600.0000, 
raw observation next is [5.183333333333334, 66.83333333333333, 0.0, 0.0, 26.0, 25.76316908856136, 0.5860727285739925, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.6061865189289013, 0.6683333333333333, 0.0, 0.0, 0.6666666666666666, 0.6469307573801132, 0.6953575761913308, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2594101], dtype=float32), 0.038913302]. 
=============================================
[2019-04-04 02:51:37,805] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.1667333e-26 2.5780804e-21 4.7747325e-25 2.0748902e-22 1.2855744e-22
 4.1332250e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:51:37,806] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4995
[2019-04-04 02:51:37,814] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.0, 52.0, 120.1666666666667, 842.8333333333334, 26.0, 25.24076848422609, 0.4117986141407171, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4278000.0000, 
sim time next is 4278600.0000, 
raw observation next is [7.0, 52.0, 120.0, 847.0, 26.0, 25.26391760101993, 0.4155743766300264, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.6565096952908588, 0.52, 0.4, 0.9359116022099447, 0.6666666666666666, 0.6053264667516608, 0.6385247922100088, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.39344335], dtype=float32), 0.54453164]. 
=============================================
[2019-04-04 02:51:39,586] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.1182150e-27 1.6559712e-21 5.1889372e-25 6.0334853e-23 2.0236810e-22
 3.1093119e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:51:39,586] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9831
[2019-04-04 02:51:39,606] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.916666666666666, 66.16666666666667, 0.0, 0.0, 26.0, 25.66689542217449, 0.5615948010454662, 0.0, 1.0, 148432.7184830936], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4414200.0000, 
sim time next is 4414800.0000, 
raw observation next is [5.733333333333334, 66.33333333333334, 0.0, 0.0, 26.0, 25.64846365085189, 0.5732211494081069, 0.0, 1.0, 96988.92992008332], 
processed observation next is [1.0, 0.08695652173913043, 0.6214219759926132, 0.6633333333333334, 0.0, 0.0, 0.6666666666666666, 0.6373719709043243, 0.691073716469369, 0.0, 1.0, 0.461852047238492], 
reward next is 0.5381, 
noisyNet noise sample is [array([-1.9594617], dtype=float32), 0.11853013]. 
=============================================
[2019-04-04 02:51:46,518] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.6604336e-25 8.4733781e-21 2.1407847e-24 2.7134561e-21 1.5129182e-21
 7.8888235e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:51:46,520] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.6005
[2019-04-04 02:51:46,529] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 37.0, 89.5, 638.0, 26.0, 25.19429263062671, 0.4364635043255369, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4809600.0000, 
sim time next is 4810200.0000, 
raw observation next is [3.0, 36.5, 87.0, 608.3333333333334, 26.0, 25.19631805640931, 0.432807375894622, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.5457063711911359, 0.365, 0.29, 0.6721915285451198, 0.6666666666666666, 0.5996931713674426, 0.6442691252982073, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5178811], dtype=float32), 0.17075588]. 
=============================================
[2019-04-04 02:51:52,257] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.4518410e-28 1.9100644e-22 8.7025972e-27 1.3099197e-23 1.3974589e-23
 3.1804949e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 02:51:52,259] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4656
[2019-04-04 02:51:52,288] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 92.0, 161.5, 3.0, 26.0, 26.44168955088109, 0.5779535498566761, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4701600.0000, 
sim time next is 4702200.0000, 
raw observation next is [0.0, 92.0, 177.0, 4.0, 26.0, 26.42766164001271, 0.5802062916100533, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.46260387811634357, 0.92, 0.59, 0.004419889502762431, 0.6666666666666666, 0.7023051366677256, 0.6934020972033511, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.09635978], dtype=float32), -0.5792645]. 
=============================================
[2019-04-04 02:51:54,747] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:51:54,747] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:51:54,782] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run26
[2019-04-04 02:51:55,413] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:51:55,413] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:51:55,420] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run26
[2019-04-04 02:51:58,817] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.6696626e-25 3.7434107e-20 4.8554795e-24 1.1772825e-21 8.7008929e-22
 5.4297715e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:51:58,820] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6098
[2019-04-04 02:51:58,830] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 75.33333333333333, 0.0, 0.0, 26.0, 25.01493483113925, 0.372071905886254, 0.0, 1.0, 41298.06115706193], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4754400.0000, 
sim time next is 4755000.0000, 
raw observation next is [-4.0, 73.16666666666667, 0.0, 0.0, 26.0, 24.97488305925896, 0.3664445530003837, 0.0, 1.0, 41227.65683507043], 
processed observation next is [0.0, 0.0, 0.3518005540166205, 0.7316666666666667, 0.0, 0.0, 0.6666666666666666, 0.5812402549382467, 0.6221481843334612, 0.0, 1.0, 0.1963221754050973], 
reward next is 0.8037, 
noisyNet noise sample is [array([1.6468914], dtype=float32), 0.3749339]. 
=============================================
[2019-04-04 02:51:58,836] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[82.60806 ]
 [82.58744 ]
 [82.59809 ]
 [82.56029 ]
 [82.566345]], R is [[82.59785461]
 [82.5752182 ]
 [82.55253601]
 [82.52980042]
 [82.5069809 ]].
[2019-04-04 02:52:03,753] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:52:03,753] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:52:03,794] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run26
[2019-04-04 02:52:06,169] A3C_AGENT_WORKER-Thread-19 INFO:Local step 212500, global step 3393007: loss 0.0188
[2019-04-04 02:52:06,171] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 212500, global step 3393007: learning rate 0.0001
[2019-04-04 02:52:06,566] A3C_AGENT_WORKER-Thread-17 INFO:Local step 212500, global step 3393145: loss 0.0225
[2019-04-04 02:52:06,566] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 212500, global step 3393145: learning rate 0.0001
[2019-04-04 02:52:10,908] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.4564492e-26 1.1729566e-21 2.4389098e-24 3.8364063e-22 7.1761871e-22
 1.4392835e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:52:10,910] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7220
[2019-04-04 02:52:10,949] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.333333333333333, 25.16666666666667, 22.66666666666667, 202.6666666666667, 26.0, 27.23548611409671, 0.8395088197655118, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4989000.0000, 
sim time next is 4989600.0000, 
raw observation next is [6.0, 25.0, 17.0, 152.0, 26.0, 27.21147079733535, 0.6694008929908072, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.6288088642659281, 0.25, 0.056666666666666664, 0.16795580110497238, 0.6666666666666666, 0.7676225664446124, 0.7231336309969357, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6835574], dtype=float32), 0.8212971]. 
=============================================
[2019-04-04 02:52:12,518] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.8840222e-25 9.4242975e-20 9.3269265e-24 2.7485956e-21 1.5538320e-21
 1.0987746e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:52:12,522] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1422
[2019-04-04 02:52:12,540] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.666666666666667, 67.33333333333334, 0.0, 0.0, 26.0, 24.7790003560034, 0.217262363353719, 0.0, 1.0, 39495.63758729546], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4857600.0000, 
sim time next is 4858200.0000, 
raw observation next is [-3.5, 65.5, 0.0, 0.0, 26.0, 24.74776645695325, 0.209420981009707, 0.0, 1.0, 39537.33482467769], 
processed observation next is [0.0, 0.21739130434782608, 0.36565096952908593, 0.655, 0.0, 0.0, 0.6666666666666666, 0.5623138714127709, 0.5698069936699023, 0.0, 1.0, 0.18827302297465565], 
reward next is 0.8117, 
noisyNet noise sample is [array([1.927177], dtype=float32), -1.6637058]. 
=============================================
[2019-04-04 02:52:15,263] A3C_AGENT_WORKER-Thread-7 INFO:Local step 212500, global step 3396806: loss 0.0228
[2019-04-04 02:52:15,265] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 212500, global step 3396807: learning rate 0.0001
[2019-04-04 02:52:17,137] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:52:17,138] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:52:17,147] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run26
[2019-04-04 02:52:17,193] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:52:17,193] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:52:17,199] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run26
[2019-04-04 02:52:17,645] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:52:17,645] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:52:17,659] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run26
[2019-04-04 02:52:18,489] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:52:18,489] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:52:18,495] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run26
[2019-04-04 02:52:18,617] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:52:18,617] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:52:18,624] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run26
[2019-04-04 02:52:19,073] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:52:19,073] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:52:19,076] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run26
[2019-04-04 02:52:19,593] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:52:19,593] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:52:19,597] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run26
[2019-04-04 02:52:19,969] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:52:19,969] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:52:19,973] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run26
[2019-04-04 02:52:20,331] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:52:20,331] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:52:20,336] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run26
[2019-04-04 02:52:24,033] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:52:24,033] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:52:24,036] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run26
[2019-04-04 02:52:28,713] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:52:28,713] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:52:28,733] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run26
[2019-04-04 02:52:28,824] A3C_AGENT_WORKER-Thread-6 INFO:Evaluating...
[2019-04-04 02:52:28,826] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 02:52:28,826] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:52:28,828] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run35
[2019-04-04 02:52:28,855] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 02:52:28,857] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:52:28,859] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 02:52:28,859] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:52:28,862] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run35
[2019-04-04 02:52:28,882] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run35
[2019-04-04 02:53:38,639] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.36844492], dtype=float32), 0.19597048]
[2019-04-04 02:53:38,639] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [3.816666666666667, 73.0, 137.0, 233.3333333333333, 26.0, 25.82201165249739, 0.540583594783064, 1.0, 1.0, 0.0]
[2019-04-04 02:53:38,639] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 02:53:38,640] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [6.8291573e-27 1.6206990e-21 2.2380940e-25 5.1272570e-23 7.5148688e-23
 1.9734607e-26 1.0000000e+00], sampled 0.24719935494718404
[2019-04-04 02:55:18,893] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 02:55:47,041] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 02:55:50,915] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 02:55:51,952] A3C_AGENT_WORKER-Thread-6 INFO:Global step: 3400000, evaluation results [3400000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 02:55:53,811] A3C_AGENT_WORKER-Thread-15 INFO:Local step 212500, global step 3400371: loss 0.0066
[2019-04-04 02:55:53,816] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 212500, global step 3400371: learning rate 0.0001
[2019-04-04 02:55:55,069] A3C_AGENT_WORKER-Thread-14 INFO:Local step 212500, global step 3400607: loss 0.0181
[2019-04-04 02:55:55,078] A3C_AGENT_WORKER-Thread-9 INFO:Local step 212500, global step 3400611: loss 0.0204
[2019-04-04 02:55:55,079] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 212500, global step 3400611: learning rate 0.0001
[2019-04-04 02:55:55,081] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 212500, global step 3400612: learning rate 0.0001
[2019-04-04 02:55:56,145] A3C_AGENT_WORKER-Thread-3 INFO:Local step 212500, global step 3400801: loss 0.0156
[2019-04-04 02:55:56,146] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 212500, global step 3400801: learning rate 0.0001
[2019-04-04 02:55:56,473] A3C_AGENT_WORKER-Thread-2 INFO:Local step 212500, global step 3400870: loss 0.0256
[2019-04-04 02:55:56,473] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 212500, global step 3400870: learning rate 0.0001
[2019-04-04 02:55:56,758] A3C_AGENT_WORKER-Thread-5 INFO:Local step 212500, global step 3400929: loss 0.0303
[2019-04-04 02:55:56,763] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 212500, global step 3400929: learning rate 0.0001
[2019-04-04 02:55:57,590] A3C_AGENT_WORKER-Thread-6 INFO:Local step 212500, global step 3401109: loss 0.0118
[2019-04-04 02:55:57,590] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 212500, global step 3401109: learning rate 0.0001
[2019-04-04 02:55:58,122] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:55:58,122] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:55:58,150] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run26
[2019-04-04 02:55:58,282] A3C_AGENT_WORKER-Thread-18 INFO:Local step 212500, global step 3401276: loss 0.0103
[2019-04-04 02:55:58,282] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 212500, global step 3401276: learning rate 0.0001
[2019-04-04 02:55:58,438] A3C_AGENT_WORKER-Thread-8 INFO:Local step 212500, global step 3401327: loss 0.0091
[2019-04-04 02:55:58,439] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 212500, global step 3401327: learning rate 0.0001
[2019-04-04 02:55:58,614] A3C_AGENT_WORKER-Thread-19 INFO:Local step 213000, global step 3401362: loss 0.0298
[2019-04-04 02:55:58,622] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 213000, global step 3401363: learning rate 0.0001
[2019-04-04 02:55:59,220] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 02:55:59,220] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 02:55:59,227] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run26
[2019-04-04 02:55:59,345] A3C_AGENT_WORKER-Thread-10 INFO:Local step 212500, global step 3401543: loss 0.0099
[2019-04-04 02:55:59,346] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 212500, global step 3401543: learning rate 0.0001
[2019-04-04 02:55:59,354] A3C_AGENT_WORKER-Thread-17 INFO:Local step 213000, global step 3401550: loss 0.0409
[2019-04-04 02:55:59,355] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 213000, global step 3401550: learning rate 0.0001
[2019-04-04 02:55:59,399] A3C_AGENT_WORKER-Thread-4 INFO:Local step 212500, global step 3401560: loss 0.0100
[2019-04-04 02:55:59,402] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 212500, global step 3401560: learning rate 0.0001
[2019-04-04 02:56:08,438] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.51573697e-26 1.21850134e-20 6.49640800e-24 4.66070029e-22
 6.60481252e-22 1.66190375e-25 1.00000000e+00], sum to 1.0000
[2019-04-04 02:56:08,439] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0811
[2019-04-04 02:56:08,468] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.7, 54.0, 0.0, 0.0, 26.0, 23.80061001204898, -0.001904162500256388, 0.0, 1.0, 44979.02789675379], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 430800.0000, 
sim time next is 431400.0000, 
raw observation next is [-11.7, 54.0, 0.0, 0.0, 26.0, 23.78903805115505, -0.01261881303881647, 0.0, 1.0, 45032.03029718211], 
processed observation next is [1.0, 1.0, 0.13850415512465375, 0.54, 0.0, 0.0, 0.6666666666666666, 0.4824198375962541, 0.49579372898706114, 0.0, 1.0, 0.21443823951039098], 
reward next is 0.7856, 
noisyNet noise sample is [array([-1.3567351], dtype=float32), -0.22398588]. 
=============================================
[2019-04-04 02:56:11,396] A3C_AGENT_WORKER-Thread-7 INFO:Local step 213000, global step 3404034: loss 0.0423
[2019-04-04 02:56:11,398] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 213000, global step 3404034: learning rate 0.0001
[2019-04-04 02:56:12,701] A3C_AGENT_WORKER-Thread-20 INFO:Local step 212500, global step 3404328: loss 0.0392
[2019-04-04 02:56:12,735] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 212500, global step 3404328: learning rate 0.0001
[2019-04-04 02:56:13,877] A3C_AGENT_WORKER-Thread-16 INFO:Local step 212500, global step 3404603: loss 0.0389
[2019-04-04 02:56:13,925] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 212500, global step 3404603: learning rate 0.0001
[2019-04-04 02:56:29,376] A3C_AGENT_WORKER-Thread-15 INFO:Local step 213000, global step 3408034: loss 0.0545
[2019-04-04 02:56:29,382] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 213000, global step 3408034: learning rate 0.0001
[2019-04-04 02:56:30,328] A3C_AGENT_WORKER-Thread-14 INFO:Local step 213000, global step 3408295: loss 0.0588
[2019-04-04 02:56:30,330] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 213000, global step 3408295: learning rate 0.0001
[2019-04-04 02:56:31,235] A3C_AGENT_WORKER-Thread-5 INFO:Local step 213000, global step 3408546: loss 0.0521
[2019-04-04 02:56:31,236] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 213000, global step 3408546: learning rate 0.0001
[2019-04-04 02:56:31,300] A3C_AGENT_WORKER-Thread-9 INFO:Local step 213000, global step 3408563: loss 0.0490
[2019-04-04 02:56:31,302] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 213000, global step 3408563: learning rate 0.0001
[2019-04-04 02:56:31,512] A3C_AGENT_WORKER-Thread-2 INFO:Local step 213000, global step 3408612: loss 0.0598
[2019-04-04 02:56:31,513] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 213000, global step 3408612: learning rate 0.0001
[2019-04-04 02:56:31,653] A3C_AGENT_WORKER-Thread-3 INFO:Local step 213000, global step 3408648: loss 0.0593
[2019-04-04 02:56:31,655] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 213000, global step 3408648: learning rate 0.0001
[2019-04-04 02:56:32,574] A3C_AGENT_WORKER-Thread-6 INFO:Local step 213000, global step 3408870: loss 0.0421
[2019-04-04 02:56:32,610] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 213000, global step 3408870: learning rate 0.0001
[2019-04-04 02:56:33,045] A3C_AGENT_WORKER-Thread-19 INFO:Local step 213500, global step 3408966: loss 0.0751
[2019-04-04 02:56:33,045] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 213500, global step 3408966: learning rate 0.0001
[2019-04-04 02:56:33,080] A3C_AGENT_WORKER-Thread-18 INFO:Local step 213000, global step 3408977: loss 0.0515
[2019-04-04 02:56:33,080] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 213000, global step 3408977: learning rate 0.0001
[2019-04-04 02:56:33,669] A3C_AGENT_WORKER-Thread-8 INFO:Local step 213000, global step 3409103: loss 0.0378
[2019-04-04 02:56:33,670] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 213000, global step 3409103: learning rate 0.0001
[2019-04-04 02:56:34,598] A3C_AGENT_WORKER-Thread-10 INFO:Local step 213000, global step 3409303: loss 0.0317
[2019-04-04 02:56:34,618] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 213000, global step 3409303: learning rate 0.0001
[2019-04-04 02:56:34,654] A3C_AGENT_WORKER-Thread-17 INFO:Local step 213500, global step 3409316: loss 0.0836
[2019-04-04 02:56:34,655] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 213500, global step 3409316: learning rate 0.0001
[2019-04-04 02:56:34,932] A3C_AGENT_WORKER-Thread-4 INFO:Local step 213000, global step 3409364: loss 0.0373
[2019-04-04 02:56:34,933] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 213000, global step 3409364: learning rate 0.0001
[2019-04-04 02:56:44,844] A3C_AGENT_WORKER-Thread-7 INFO:Local step 213500, global step 3411618: loss 0.0607
[2019-04-04 02:56:44,845] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 213500, global step 3411618: learning rate 0.0001
[2019-04-04 02:56:46,632] A3C_AGENT_WORKER-Thread-20 INFO:Local step 213000, global step 3412008: loss 0.0650
[2019-04-04 02:56:46,634] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 213000, global step 3412008: learning rate 0.0001
[2019-04-04 02:56:48,051] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.7701057e-26 1.3236788e-22 1.5982617e-25 4.9559719e-23 5.0302549e-23
 2.8754554e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 02:56:48,053] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.2112
[2019-04-04 02:56:48,132] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.2, 28.0, 124.0, 0.0, 26.0, 24.98716506295168, 0.156985370298349, 1.0, 1.0, 73019.97114465002], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 478800.0000, 
sim time next is 479400.0000, 
raw observation next is [-1.1, 29.16666666666667, 122.3333333333333, 0.0, 26.0, 24.97023090307869, 0.0414422084054778, 1.0, 1.0, 55979.99850899134], 
processed observation next is [1.0, 0.5652173913043478, 0.4321329639889197, 0.29166666666666674, 0.4077777777777777, 0.0, 0.6666666666666666, 0.5808525752565575, 0.5138140694684926, 1.0, 1.0, 0.26657142147138735], 
reward next is 0.7334, 
noisyNet noise sample is [array([-0.21017195], dtype=float32), -0.35603914]. 
=============================================
[2019-04-04 02:56:51,385] A3C_AGENT_WORKER-Thread-16 INFO:Local step 213000, global step 3413064: loss 0.0483
[2019-04-04 02:56:51,414] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 213000, global step 3413064: learning rate 0.0001
[2019-04-04 02:57:05,026] A3C_AGENT_WORKER-Thread-19 INFO:Local step 214000, global step 3416293: loss 1.0625
[2019-04-04 02:57:05,027] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 214000, global step 3416293: learning rate 0.0001
[2019-04-04 02:57:05,357] A3C_AGENT_WORKER-Thread-5 INFO:Local step 213500, global step 3416374: loss 0.0590
[2019-04-04 02:57:05,358] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 213500, global step 3416374: learning rate 0.0001
[2019-04-04 02:57:05,395] A3C_AGENT_WORKER-Thread-15 INFO:Local step 213500, global step 3416387: loss 0.0562
[2019-04-04 02:57:05,407] A3C_AGENT_WORKER-Thread-14 INFO:Local step 213500, global step 3416389: loss 0.0560
[2019-04-04 02:57:05,407] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 213500, global step 3416389: learning rate 0.0001
[2019-04-04 02:57:05,429] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 213500, global step 3416387: learning rate 0.0001
[2019-04-04 02:57:05,534] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.2109036e-27 5.1586719e-21 2.3183535e-25 9.2632449e-24 5.6975473e-23
 8.9501901e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 02:57:05,535] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3156
[2019-04-04 02:57:05,602] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.433333333333334, 83.0, 0.0, 0.0, 26.0, 25.37725356284333, 0.4497890031410088, 0.0, 1.0, 52714.6218045596], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 967200.0000, 
sim time next is 967800.0000, 
raw observation next is [8.616666666666667, 83.0, 0.0, 0.0, 26.0, 25.42711417159363, 0.4534099837451946, 0.0, 1.0, 18763.63342267885], 
processed observation next is [1.0, 0.17391304347826086, 0.7012927054478302, 0.83, 0.0, 0.0, 0.6666666666666666, 0.6189261809661358, 0.6511366612483982, 0.0, 1.0, 0.08935063534608977], 
reward next is 0.9106, 
noisyNet noise sample is [array([-0.7597396], dtype=float32), 1.3854814]. 
=============================================
[2019-04-04 02:57:06,005] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [9.1729695e-26 1.6202176e-20 1.3384429e-24 2.9702728e-22 1.0626730e-22
 6.6617072e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:57:06,005] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8715
[2019-04-04 02:57:06,053] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.566666666666667, 60.33333333333334, 108.1666666666667, 89.66666666666667, 26.0, 24.89113090865252, 0.2253088351468503, 0.0, 1.0, 34787.18876028684], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 649200.0000, 
sim time next is 649800.0000, 
raw observation next is [-2.5, 60.0, 112.0, 100.0, 26.0, 24.88906791404057, 0.226723332761997, 0.0, 1.0, 40008.832728658], 
processed observation next is [0.0, 0.5217391304347826, 0.39335180055401664, 0.6, 0.37333333333333335, 0.11049723756906077, 0.6666666666666666, 0.5740889928367142, 0.5755744442539991, 0.0, 1.0, 0.19051825108884762], 
reward next is 0.8095, 
noisyNet noise sample is [array([-0.11848567], dtype=float32), 1.036045]. 
=============================================
[2019-04-04 02:57:06,527] A3C_AGENT_WORKER-Thread-3 INFO:Local step 213500, global step 3416659: loss 0.0560
[2019-04-04 02:57:06,528] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 213500, global step 3416659: learning rate 0.0001
[2019-04-04 02:57:06,726] A3C_AGENT_WORKER-Thread-2 INFO:Local step 213500, global step 3416701: loss 0.0584
[2019-04-04 02:57:06,728] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 213500, global step 3416701: learning rate 0.0001
[2019-04-04 02:57:06,822] A3C_AGENT_WORKER-Thread-17 INFO:Local step 214000, global step 3416722: loss 0.9253
[2019-04-04 02:57:06,824] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 214000, global step 3416722: learning rate 0.0001
[2019-04-04 02:57:07,436] A3C_AGENT_WORKER-Thread-9 INFO:Local step 213500, global step 3416854: loss 0.0554
[2019-04-04 02:57:07,437] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 213500, global step 3416854: learning rate 0.0001
[2019-04-04 02:57:08,019] A3C_AGENT_WORKER-Thread-6 INFO:Local step 213500, global step 3416988: loss 0.0570
[2019-04-04 02:57:08,019] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 213500, global step 3416988: learning rate 0.0001
[2019-04-04 02:57:09,121] A3C_AGENT_WORKER-Thread-8 INFO:Local step 213500, global step 3417220: loss 0.0579
[2019-04-04 02:57:09,121] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 213500, global step 3417220: learning rate 0.0001
[2019-04-04 02:57:09,984] A3C_AGENT_WORKER-Thread-18 INFO:Local step 213500, global step 3417429: loss 0.0606
[2019-04-04 02:57:09,985] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 213500, global step 3417429: learning rate 0.0001
[2019-04-04 02:57:10,151] A3C_AGENT_WORKER-Thread-4 INFO:Local step 213500, global step 3417470: loss 0.0660
[2019-04-04 02:57:10,153] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 213500, global step 3417470: learning rate 0.0001
[2019-04-04 02:57:10,534] A3C_AGENT_WORKER-Thread-10 INFO:Local step 213500, global step 3417580: loss 0.0629
[2019-04-04 02:57:10,534] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 213500, global step 3417580: learning rate 0.0001
[2019-04-04 02:57:16,603] A3C_AGENT_WORKER-Thread-7 INFO:Local step 214000, global step 3419308: loss 0.8765
[2019-04-04 02:57:16,605] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 214000, global step 3419308: learning rate 0.0001
[2019-04-04 02:57:21,154] A3C_AGENT_WORKER-Thread-20 INFO:Local step 213500, global step 3420633: loss 0.0494
[2019-04-04 02:57:21,160] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 213500, global step 3420633: learning rate 0.0001
[2019-04-04 02:57:23,287] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.8183346e-27 1.8861127e-22 3.2322725e-26 2.6060506e-23 6.5514571e-24
 8.4946434e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 02:57:23,287] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6655
[2019-04-04 02:57:23,333] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 71.0, 106.3333333333333, 0.0, 26.0, 25.62307371182087, 0.3146522682969999, 1.0, 1.0, 27352.33112828393], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 820200.0000, 
sim time next is 820800.0000, 
raw observation next is [-4.5, 71.0, 104.5, 0.0, 26.0, 25.63482878984945, 0.3142589978181249, 1.0, 1.0, 27507.23306636863], 
processed observation next is [1.0, 0.5217391304347826, 0.3379501385041552, 0.71, 0.34833333333333333, 0.0, 0.6666666666666666, 0.6362357324874542, 0.6047529992727083, 1.0, 1.0, 0.1309868241255649], 
reward next is 0.8690, 
noisyNet noise sample is [array([0.21638988], dtype=float32), -0.8297562]. 
=============================================
[2019-04-04 02:57:25,924] A3C_AGENT_WORKER-Thread-16 INFO:Local step 213500, global step 3421829: loss 0.0490
[2019-04-04 02:57:25,924] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 213500, global step 3421829: learning rate 0.0001
[2019-04-04 02:57:26,088] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1474887e-25 5.0467727e-20 2.6190050e-24 1.3410412e-21 3.8767601e-22
 2.4240774e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:57:26,089] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1226
[2019-04-04 02:57:26,161] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 54.0, 73.66666666666667, 34.16666666666666, 26.0, 24.8821419930352, 0.220668537675879, 0.0, 1.0, 46427.79326338567], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 661200.0000, 
sim time next is 661800.0000, 
raw observation next is [-0.6, 54.0, 64.33333333333334, 30.33333333333334, 26.0, 24.87638361704145, 0.2204618919391464, 0.0, 1.0, 45858.6972090968], 
processed observation next is [0.0, 0.6521739130434783, 0.44598337950138506, 0.54, 0.21444444444444447, 0.03351749539594844, 0.6666666666666666, 0.5730319680867876, 0.5734872973130488, 0.0, 1.0, 0.21837474861474665], 
reward next is 0.7816, 
noisyNet noise sample is [array([-1.0324817], dtype=float32), -0.36531577]. 
=============================================
[2019-04-04 02:57:27,393] A3C_AGENT_WORKER-Thread-19 INFO:Local step 214500, global step 3422205: loss 0.1407
[2019-04-04 02:57:27,399] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 214500, global step 3422205: learning rate 0.0001
[2019-04-04 02:57:28,260] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.5307748e-27 2.5390782e-22 2.1506634e-25 4.9359538e-23 2.0779070e-23
 4.8809139e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 02:57:28,260] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4760
[2019-04-04 02:57:28,299] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 84.0, 49.0, 0.0, 26.0, 25.42519541811563, 0.3791701366839011, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 833400.0000, 
sim time next is 834000.0000, 
raw observation next is [-3.899999999999999, 83.33333333333334, 45.66666666666667, 0.0, 26.0, 25.76830698133907, 0.4094998772813001, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.35457063711911363, 0.8333333333333335, 0.15222222222222223, 0.0, 0.6666666666666666, 0.6473589151115892, 0.6364999590937667, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9770049], dtype=float32), -0.5119026]. 
=============================================
[2019-04-04 02:57:28,312] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[83.062965]
 [83.22341 ]
 [82.8498  ]
 [82.87583 ]
 [82.84309 ]], R is [[83.03488159]
 [83.20453644]
 [82.45259094]
 [82.52327728]
 [82.69804382]].
[2019-04-04 02:57:28,955] A3C_AGENT_WORKER-Thread-17 INFO:Local step 214500, global step 3422700: loss 0.1647
[2019-04-04 02:57:28,956] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 214500, global step 3422700: learning rate 0.0001
[2019-04-04 02:57:31,270] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.49820921e-28 1.02640093e-22 6.15095062e-26 1.90628553e-23
 1.68108499e-23 5.68565912e-27 1.00000000e+00], sum to 1.0000
[2019-04-04 02:57:31,292] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3551
[2019-04-04 02:57:31,381] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 76.0, 9.666666666666664, 0.0, 26.0, 25.49463964297966, 0.2790805702847839, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 719400.0000, 
sim time next is 720000.0000, 
raw observation next is [-2.3, 76.0, 14.5, 0.0, 26.0, 25.6168682738347, 0.2850177096609972, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.3988919667590028, 0.76, 0.04833333333333333, 0.0, 0.6666666666666666, 0.6347390228195584, 0.5950059032203324, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1449995], dtype=float32), -1.5147585]. 
=============================================
[2019-04-04 02:57:31,437] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[88.7731 ]
 [88.84524]
 [88.78407]
 [87.76262]
 [86.30851]], R is [[88.48967743]
 [88.6047821 ]
 [88.71873474]
 [87.94720459]
 [87.10384369]].
[2019-04-04 02:57:35,378] A3C_AGENT_WORKER-Thread-5 INFO:Local step 214000, global step 3424623: loss 0.3864
[2019-04-04 02:57:35,379] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 214000, global step 3424623: learning rate 0.0001
[2019-04-04 02:57:35,393] A3C_AGENT_WORKER-Thread-14 INFO:Local step 214000, global step 3424630: loss 0.3963
[2019-04-04 02:57:35,395] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 214000, global step 3424630: learning rate 0.0001
[2019-04-04 02:57:36,551] A3C_AGENT_WORKER-Thread-3 INFO:Local step 214000, global step 3424947: loss 0.4660
[2019-04-04 02:57:36,552] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 214000, global step 3424947: learning rate 0.0001
[2019-04-04 02:57:36,700] A3C_AGENT_WORKER-Thread-15 INFO:Local step 214000, global step 3424979: loss 0.4765
[2019-04-04 02:57:36,701] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 214000, global step 3424979: learning rate 0.0001
[2019-04-04 02:57:36,772] A3C_AGENT_WORKER-Thread-7 INFO:Local step 214500, global step 3425000: loss 0.0975
[2019-04-04 02:57:36,773] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 214500, global step 3425000: learning rate 0.0001
[2019-04-04 02:57:36,981] A3C_AGENT_WORKER-Thread-2 INFO:Local step 214000, global step 3425063: loss 0.4221
[2019-04-04 02:57:36,982] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 214000, global step 3425063: learning rate 0.0001
[2019-04-04 02:57:38,326] A3C_AGENT_WORKER-Thread-6 INFO:Local step 214000, global step 3425484: loss 0.4381
[2019-04-04 02:57:38,329] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 214000, global step 3425486: learning rate 0.0001
[2019-04-04 02:57:38,539] A3C_AGENT_WORKER-Thread-9 INFO:Local step 214000, global step 3425544: loss 0.4905
[2019-04-04 02:57:38,540] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 214000, global step 3425544: learning rate 0.0001
[2019-04-04 02:57:39,755] A3C_AGENT_WORKER-Thread-8 INFO:Local step 214000, global step 3425887: loss 0.4896
[2019-04-04 02:57:39,771] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 214000, global step 3425892: learning rate 0.0001
[2019-04-04 02:57:40,760] A3C_AGENT_WORKER-Thread-18 INFO:Local step 214000, global step 3426180: loss 0.4701
[2019-04-04 02:57:40,764] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 214000, global step 3426180: learning rate 0.0001
[2019-04-04 02:57:40,781] A3C_AGENT_WORKER-Thread-10 INFO:Local step 214000, global step 3426189: loss 0.4618
[2019-04-04 02:57:40,783] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 214000, global step 3426189: learning rate 0.0001
[2019-04-04 02:57:41,242] A3C_AGENT_WORKER-Thread-4 INFO:Local step 214000, global step 3426369: loss 0.3474
[2019-04-04 02:57:41,243] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 214000, global step 3426369: learning rate 0.0001
[2019-04-04 02:57:49,320] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.1290679e-28 3.1395440e-22 2.1958929e-26 3.1466508e-24 6.6717343e-24
 2.2025530e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 02:57:49,320] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4387
[2019-04-04 02:57:49,330] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.03333333333333, 76.5, 48.33333333333333, 0.0, 26.0, 25.7391352170314, 0.6153846718970863, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1155000.0000, 
sim time next is 1155600.0000, 
raw observation next is [15.5, 75.0, 57.0, 0.0, 26.0, 25.72568130341499, 0.6132239800302134, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.8919667590027703, 0.75, 0.19, 0.0, 0.6666666666666666, 0.6438067752845825, 0.7044079933434045, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.14355707], dtype=float32), 1.3698323]. 
=============================================
[2019-04-04 02:57:50,351] A3C_AGENT_WORKER-Thread-20 INFO:Local step 214000, global step 3429513: loss 0.5128
[2019-04-04 02:57:50,352] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 214000, global step 3429513: learning rate 0.0001
[2019-04-04 02:57:52,435] A3C_AGENT_WORKER-Thread-19 INFO:Local step 215000, global step 3430291: loss 0.4296
[2019-04-04 02:57:52,461] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 215000, global step 3430292: learning rate 0.0001
[2019-04-04 02:57:55,099] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.3600734e-29 5.8430131e-24 1.0166020e-27 2.9889530e-26 1.2669072e-25
 5.9767938e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 02:57:55,105] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6675
[2019-04-04 02:57:55,124] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.0, 96.0, 0.0, 0.0, 26.0, 23.60667048239833, 0.1686142619788823, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1230600.0000, 
sim time next is 1231200.0000, 
raw observation next is [15.0, 96.0, 0.0, 0.0, 26.0, 23.58347076702589, 0.1641174065662103, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.2608695652173913, 0.8781163434903049, 0.96, 0.0, 0.0, 0.6666666666666666, 0.4652892305854908, 0.5547058021887368, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.761897], dtype=float32), 0.17266712]. 
=============================================
[2019-04-04 02:57:55,258] A3C_AGENT_WORKER-Thread-17 INFO:Local step 215000, global step 3431334: loss 0.3352
[2019-04-04 02:57:55,260] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 215000, global step 3431334: learning rate 0.0001
[2019-04-04 02:57:56,232] A3C_AGENT_WORKER-Thread-5 INFO:Local step 214500, global step 3431793: loss 0.0473
[2019-04-04 02:57:56,248] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 214500, global step 3431794: learning rate 0.0001
[2019-04-04 02:57:56,381] A3C_AGENT_WORKER-Thread-16 INFO:Local step 214000, global step 3431859: loss 0.3719
[2019-04-04 02:57:56,384] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 214000, global step 3431859: learning rate 0.0001
[2019-04-04 02:57:57,350] A3C_AGENT_WORKER-Thread-14 INFO:Local step 214500, global step 3432171: loss 0.0415
[2019-04-04 02:57:57,357] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 214500, global step 3432171: learning rate 0.0001
[2019-04-04 02:57:57,741] A3C_AGENT_WORKER-Thread-15 INFO:Local step 214500, global step 3432311: loss 0.0386
[2019-04-04 02:57:57,751] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 214500, global step 3432311: learning rate 0.0001
[2019-04-04 02:57:57,788] A3C_AGENT_WORKER-Thread-3 INFO:Local step 214500, global step 3432321: loss 0.0438
[2019-04-04 02:57:57,804] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 214500, global step 3432321: learning rate 0.0001
[2019-04-04 02:57:58,669] A3C_AGENT_WORKER-Thread-2 INFO:Local step 214500, global step 3432625: loss 0.0326
[2019-04-04 02:57:58,676] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 214500, global step 3432626: learning rate 0.0001
[2019-04-04 02:57:59,102] A3C_AGENT_WORKER-Thread-6 INFO:Local step 214500, global step 3432780: loss 0.0324
[2019-04-04 02:57:59,103] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 214500, global step 3432780: learning rate 0.0001
[2019-04-04 02:57:59,894] A3C_AGENT_WORKER-Thread-9 INFO:Local step 214500, global step 3433099: loss 0.0438
[2019-04-04 02:57:59,896] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 214500, global step 3433099: learning rate 0.0001
[2019-04-04 02:58:01,155] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.5015716e-27 6.5087426e-22 7.3388161e-26 2.1821016e-23 8.8852291e-24
 7.7856173e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 02:58:01,197] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5489
[2019-04-04 02:58:01,219] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.8, 100.0, 89.0, 0.0, 26.0, 24.79224152592196, 0.4509893656227959, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1259400.0000, 
sim time next is 1260000.0000, 
raw observation next is [13.8, 100.0, 86.0, 0.0, 26.0, 24.77633314570834, 0.4485801134079856, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.844875346260388, 1.0, 0.2866666666666667, 0.0, 0.6666666666666666, 0.5646944288090282, 0.6495267044693286, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6609433], dtype=float32), -0.10970363]. 
=============================================
[2019-04-04 02:58:01,295] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[88.578476]
 [88.62903 ]
 [88.66618 ]
 [88.70556 ]
 [88.66638 ]], R is [[88.65255737]
 [88.76602936]
 [88.87837219]
 [88.98958588]
 [89.0996933 ]].
[2019-04-04 02:58:01,295] A3C_AGENT_WORKER-Thread-18 INFO:Local step 214500, global step 3433578: loss 0.0844
[2019-04-04 02:58:01,302] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 214500, global step 3433578: learning rate 0.0001
[2019-04-04 02:58:01,551] A3C_AGENT_WORKER-Thread-8 INFO:Local step 214500, global step 3433665: loss 0.0817
[2019-04-04 02:58:01,552] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 214500, global step 3433665: learning rate 0.0001
[2019-04-04 02:58:01,564] A3C_AGENT_WORKER-Thread-10 INFO:Local step 214500, global step 3433671: loss 0.0531
[2019-04-04 02:58:01,565] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 214500, global step 3433671: learning rate 0.0001
[2019-04-04 02:58:01,994] A3C_AGENT_WORKER-Thread-7 INFO:Local step 215000, global step 3433805: loss 0.6120
[2019-04-04 02:58:01,996] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 215000, global step 3433806: learning rate 0.0001
[2019-04-04 02:58:03,078] A3C_AGENT_WORKER-Thread-4 INFO:Local step 214500, global step 3434142: loss 0.0780
[2019-04-04 02:58:03,079] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 214500, global step 3434142: learning rate 0.0001
[2019-04-04 02:58:13,683] A3C_AGENT_WORKER-Thread-20 INFO:Local step 214500, global step 3437056: loss 0.1547
[2019-04-04 02:58:13,684] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 214500, global step 3437056: learning rate 0.0001
[2019-04-04 02:58:14,087] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.8128544e-28 1.6039312e-23 2.2461070e-26 1.8750567e-23 3.8972482e-24
 3.1273065e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 02:58:14,087] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2048
[2019-04-04 02:58:14,174] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 32.0, 0.0, 26.0, 24.68832477527604, 0.4062036407325927, 1.0, 1.0, 196601.4990281954], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1440000.0000, 
sim time next is 1440600.0000, 
raw observation next is [1.1, 92.0, 27.33333333333333, 0.0, 26.0, 25.16116252289169, 0.4639600056447074, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.49307479224376743, 0.92, 0.0911111111111111, 0.0, 0.6666666666666666, 0.5967635435743075, 0.6546533352149025, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.44400173], dtype=float32), -1.3160557]. 
=============================================
[2019-04-04 02:58:18,758] A3C_AGENT_WORKER-Thread-16 INFO:Local step 214500, global step 3438565: loss 0.1752
[2019-04-04 02:58:18,773] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 214500, global step 3438570: learning rate 0.0001
[2019-04-04 02:58:20,225] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.6583622e-28 1.9567779e-22 3.7857484e-26 1.3486582e-23 8.9380236e-24
 9.8395732e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 02:58:20,233] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.6847409e-27 2.8405739e-23 4.6561060e-26 4.5591462e-23 8.5063999e-24
 8.2103954e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 02:58:20,249] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6967
[2019-04-04 02:58:20,249] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4509
[2019-04-04 02:58:20,263] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 93.5, 26.66666666666666, 0.0, 26.0, 25.71549967433145, 0.5142478123898219, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1354200.0000, 
sim time next is 1354800.0000, 
raw observation next is [0.9000000000000001, 94.0, 22.33333333333333, 0.0, 26.0, 25.70546432762558, 0.5093458578256049, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.48753462603878117, 0.94, 0.07444444444444442, 0.0, 0.6666666666666666, 0.6421220273021317, 0.669781952608535, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.44731], dtype=float32), 0.96905965]. 
=============================================
[2019-04-04 02:58:20,295] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 32.0, 0.0, 26.0, 24.68832477527604, 0.4062036407325927, 1.0, 1.0, 196601.4990281954], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1440000.0000, 
sim time next is 1440600.0000, 
raw observation next is [1.1, 92.0, 27.33333333333333, 0.0, 26.0, 25.16116252289169, 0.4639600056447074, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.49307479224376743, 0.92, 0.0911111111111111, 0.0, 0.6666666666666666, 0.5967635435743075, 0.6546533352149025, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.23212175], dtype=float32), 1.3495597]. 
=============================================
[2019-04-04 02:58:20,388] A3C_AGENT_WORKER-Thread-19 INFO:Local step 215500, global step 3439238: loss 0.1525
[2019-04-04 02:58:20,389] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 215500, global step 3439238: learning rate 0.0001
[2019-04-04 02:58:21,460] A3C_AGENT_WORKER-Thread-5 INFO:Local step 215000, global step 3439683: loss 0.7049
[2019-04-04 02:58:21,462] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 215000, global step 3439685: learning rate 0.0001
[2019-04-04 02:58:21,727] A3C_AGENT_WORKER-Thread-14 INFO:Local step 215000, global step 3439788: loss 0.6845
[2019-04-04 02:58:21,730] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 215000, global step 3439788: learning rate 0.0001
[2019-04-04 02:58:22,734] A3C_AGENT_WORKER-Thread-15 INFO:Local step 215000, global step 3440179: loss 0.5597
[2019-04-04 02:58:22,735] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 215000, global step 3440179: learning rate 0.0001
[2019-04-04 02:58:23,080] A3C_AGENT_WORKER-Thread-17 INFO:Local step 215500, global step 3440321: loss 0.1550
[2019-04-04 02:58:23,081] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 215500, global step 3440322: learning rate 0.0001
[2019-04-04 02:58:23,132] A3C_AGENT_WORKER-Thread-3 INFO:Local step 215000, global step 3440343: loss 0.5496
[2019-04-04 02:58:23,162] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 215000, global step 3440343: learning rate 0.0001
[2019-04-04 02:58:23,619] A3C_AGENT_WORKER-Thread-2 INFO:Local step 215000, global step 3440574: loss 0.5124
[2019-04-04 02:58:23,620] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 215000, global step 3440574: learning rate 0.0001
[2019-04-04 02:58:24,512] A3C_AGENT_WORKER-Thread-6 INFO:Local step 215000, global step 3441023: loss 0.4895
[2019-04-04 02:58:24,513] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 215000, global step 3441023: learning rate 0.0001
[2019-04-04 02:58:24,884] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.1325236e-27 1.5576600e-21 1.2620113e-25 5.1922067e-24 2.1233403e-23
 4.2139397e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 02:58:24,885] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4711
[2019-04-04 02:58:24,903] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.433333333333334, 85.83333333333334, 0.0, 0.0, 26.0, 25.58568150761173, 0.5271026122787691, 0.0, 1.0, 24102.24390413803], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1566600.0000, 
sim time next is 1567200.0000, 
raw observation next is [4.466666666666667, 85.66666666666667, 0.0, 0.0, 26.0, 25.59291219749442, 0.5175064503269392, 0.0, 1.0, 22442.60916703166], 
processed observation next is [1.0, 0.13043478260869565, 0.5863342566943676, 0.8566666666666667, 0.0, 0.0, 0.6666666666666666, 0.6327426831245351, 0.6725021501089797, 0.0, 1.0, 0.10686956746205552], 
reward next is 0.8931, 
noisyNet noise sample is [array([-0.283596], dtype=float32), 0.70457476]. 
=============================================
[2019-04-04 02:58:25,053] A3C_AGENT_WORKER-Thread-18 INFO:Local step 215000, global step 3441314: loss 0.4794
[2019-04-04 02:58:25,055] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 215000, global step 3441315: learning rate 0.0001
[2019-04-04 02:58:25,089] A3C_AGENT_WORKER-Thread-9 INFO:Local step 215000, global step 3441332: loss 0.4850
[2019-04-04 02:58:25,091] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 215000, global step 3441332: learning rate 0.0001
[2019-04-04 02:58:25,510] A3C_AGENT_WORKER-Thread-10 INFO:Local step 215000, global step 3441528: loss 0.4643
[2019-04-04 02:58:25,526] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 215000, global step 3441534: learning rate 0.0001
[2019-04-04 02:58:25,853] A3C_AGENT_WORKER-Thread-8 INFO:Local step 215000, global step 3441712: loss 0.3932
[2019-04-04 02:58:25,855] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 215000, global step 3441712: learning rate 0.0001
[2019-04-04 02:58:26,412] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.4394523e-26 6.9161395e-21 4.7292898e-24 1.0331817e-21 2.3498866e-21
 4.9860609e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:58:26,418] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5862
[2019-04-04 02:58:26,453] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.300000000000001, 80.0, 0.0, 0.0, 26.0, 24.33114289453402, 0.09093148515902176, 0.0, 1.0, 44918.02198627093], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1898400.0000, 
sim time next is 1899000.0000, 
raw observation next is [-7.3, 80.5, 0.0, 0.0, 26.0, 24.29085584415692, 0.08285143275370438, 0.0, 1.0, 44932.73832919998], 
processed observation next is [0.0, 1.0, 0.26038781163434904, 0.805, 0.0, 0.0, 0.6666666666666666, 0.5242379870130766, 0.5276171442512348, 0.0, 1.0, 0.213965420615238], 
reward next is 0.7860, 
noisyNet noise sample is [array([0.10696321], dtype=float32), -0.99144953]. 
=============================================
[2019-04-04 02:58:26,479] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[80.66862 ]
 [80.65385 ]
 [80.68539 ]
 [80.762184]
 [80.83398 ]], R is [[80.69029236]
 [80.66949463]
 [80.64894867]
 [80.62862396]
 [80.60850525]].
[2019-04-04 02:58:27,179] A3C_AGENT_WORKER-Thread-4 INFO:Local step 215000, global step 3442372: loss 0.3124
[2019-04-04 02:58:27,181] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 215000, global step 3442372: learning rate 0.0001
[2019-04-04 02:58:28,487] A3C_AGENT_WORKER-Thread-7 INFO:Local step 215500, global step 3443031: loss 0.1649
[2019-04-04 02:58:28,487] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 215500, global step 3443031: learning rate 0.0001
[2019-04-04 02:58:29,943] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.6981084e-28 2.7889814e-23 4.5719401e-26 1.1328370e-23 7.6358572e-24
 1.7980349e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 02:58:29,943] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8685
[2019-04-04 02:58:29,976] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.566666666666667, 92.0, 33.83333333333333, 0.0, 26.0, 25.72070642803859, 0.5391370572208104, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1672800.0000, 
sim time next is 1673400.0000, 
raw observation next is [2.383333333333333, 92.0, 37.66666666666667, 0.0, 26.0, 25.76732370719096, 0.5333769447236373, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.5286241920590952, 0.92, 0.12555555555555556, 0.0, 0.6666666666666666, 0.6472769755992468, 0.6777923149078791, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.265351], dtype=float32), -0.92515373]. 
=============================================
[2019-04-04 02:58:31,415] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1888644e-26 2.5533004e-20 4.2874799e-24 1.6442309e-22 1.3586929e-21
 2.5408571e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:58:31,417] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4838
[2019-04-04 02:58:31,442] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.25, 91.16666666666667, 0.0, 0.0, 26.0, 25.32768726345083, 0.4483729229428112, 0.0, 1.0, 42904.48344992023], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1734600.0000, 
sim time next is 1735200.0000, 
raw observation next is [0.2, 91.0, 0.0, 0.0, 26.0, 25.30825494827818, 0.4436592557008449, 0.0, 1.0, 42906.89894739845], 
processed observation next is [0.0, 0.08695652173913043, 0.46814404432132967, 0.91, 0.0, 0.0, 0.6666666666666666, 0.6090212456898483, 0.6478864185669483, 0.0, 1.0, 0.2043185664161831], 
reward next is 0.7957, 
noisyNet noise sample is [array([-0.25365618], dtype=float32), -0.7326545]. 
=============================================
[2019-04-04 02:58:33,016] A3C_AGENT_WORKER-Thread-20 INFO:Local step 215000, global step 3444872: loss 0.8154
[2019-04-04 02:58:33,023] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 215000, global step 3444872: learning rate 0.0001
[2019-04-04 02:58:33,789] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1477381e-25 6.1697696e-20 1.8437207e-23 3.1295015e-21 4.0539100e-21
 8.7276395e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:58:33,789] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0403
[2019-04-04 02:58:33,811] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.25, 91.16666666666667, 0.0, 0.0, 26.0, 25.32768726345083, 0.4483729229428112, 0.0, 1.0, 42904.48344992023], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1734600.0000, 
sim time next is 1735200.0000, 
raw observation next is [0.2, 91.0, 0.0, 0.0, 26.0, 25.30825494827818, 0.4436592557008449, 0.0, 1.0, 42906.89894739845], 
processed observation next is [0.0, 0.08695652173913043, 0.46814404432132967, 0.91, 0.0, 0.0, 0.6666666666666666, 0.6090212456898483, 0.6478864185669483, 0.0, 1.0, 0.2043185664161831], 
reward next is 0.7957, 
noisyNet noise sample is [array([0.36373013], dtype=float32), 0.85485756]. 
=============================================
[2019-04-04 02:58:37,576] A3C_AGENT_WORKER-Thread-16 INFO:Local step 215000, global step 3446320: loss 0.9387
[2019-04-04 02:58:37,577] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 215000, global step 3446320: learning rate 0.0001
[2019-04-04 02:58:42,444] A3C_AGENT_WORKER-Thread-5 INFO:Local step 215500, global step 3447905: loss 0.0713
[2019-04-04 02:58:42,445] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 215500, global step 3447905: learning rate 0.0001
[2019-04-04 02:58:42,637] A3C_AGENT_WORKER-Thread-14 INFO:Local step 215500, global step 3447974: loss 0.0690
[2019-04-04 02:58:42,638] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 215500, global step 3447974: learning rate 0.0001
[2019-04-04 02:58:43,567] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.0857302e-24 9.7624819e-20 1.2520871e-22 3.9201115e-21 4.0366443e-20
 3.1423580e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 02:58:43,567] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7029
[2019-04-04 02:58:43,586] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.7, 78.0, 0.0, 0.0, 26.0, 23.29107496167131, -0.09363154536304692, 0.0, 1.0, 47195.21689731849], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1841400.0000, 
sim time next is 1842000.0000, 
raw observation next is [-6.700000000000001, 78.0, 0.0, 0.0, 26.0, 23.26335249717017, -0.09961464366134927, 0.0, 1.0, 47181.92428217013], 
processed observation next is [0.0, 0.30434782608695654, 0.2770083102493075, 0.78, 0.0, 0.0, 0.6666666666666666, 0.43861270809751424, 0.46679511877955027, 0.0, 1.0, 0.22467582991509585], 
reward next is 0.7753, 
noisyNet noise sample is [array([-0.8091525], dtype=float32), -0.12926094]. 
=============================================
[2019-04-04 02:58:43,602] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[77.650665]
 [77.65443 ]
 [77.67405 ]
 [77.71388 ]
 [77.76745 ]], R is [[77.65717316]
 [77.6558609 ]
 [77.65457916]
 [77.65339661]
 [77.65237427]].
[2019-04-04 02:58:44,248] A3C_AGENT_WORKER-Thread-15 INFO:Local step 215500, global step 3448432: loss 0.0659
[2019-04-04 02:58:44,248] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 215500, global step 3448432: learning rate 0.0001
[2019-04-04 02:58:44,547] A3C_AGENT_WORKER-Thread-3 INFO:Local step 215500, global step 3448498: loss 0.0660
[2019-04-04 02:58:44,548] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 215500, global step 3448498: learning rate 0.0001
[2019-04-04 02:58:45,422] A3C_AGENT_WORKER-Thread-2 INFO:Local step 215500, global step 3448711: loss 0.0595
[2019-04-04 02:58:45,441] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 215500, global step 3448711: learning rate 0.0001
[2019-04-04 02:58:45,778] A3C_AGENT_WORKER-Thread-6 INFO:Local step 215500, global step 3448814: loss 0.0531
[2019-04-04 02:58:45,779] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 215500, global step 3448815: learning rate 0.0001
[2019-04-04 02:58:46,217] A3C_AGENT_WORKER-Thread-19 INFO:Local step 216000, global step 3448945: loss 0.1877
[2019-04-04 02:58:46,219] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 216000, global step 3448945: learning rate 0.0001
[2019-04-04 02:58:46,260] A3C_AGENT_WORKER-Thread-9 INFO:Local step 215500, global step 3448954: loss 0.0500
[2019-04-04 02:58:46,261] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 215500, global step 3448954: learning rate 0.0001
[2019-04-04 02:58:46,563] A3C_AGENT_WORKER-Thread-18 INFO:Local step 215500, global step 3449035: loss 0.0504
[2019-04-04 02:58:46,577] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 215500, global step 3449035: learning rate 0.0001
[2019-04-04 02:58:46,854] A3C_AGENT_WORKER-Thread-10 INFO:Local step 215500, global step 3449119: loss 0.0526
[2019-04-04 02:58:46,855] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 215500, global step 3449119: learning rate 0.0001
[2019-04-04 02:58:46,962] A3C_AGENT_WORKER-Thread-8 INFO:Local step 215500, global step 3449147: loss 0.0495
[2019-04-04 02:58:46,964] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 215500, global step 3449147: learning rate 0.0001
[2019-04-04 02:58:49,516] A3C_AGENT_WORKER-Thread-17 INFO:Local step 216000, global step 3449871: loss 0.1353
[2019-04-04 02:58:49,517] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 216000, global step 3449871: learning rate 0.0001
[2019-04-04 02:58:49,795] A3C_AGENT_WORKER-Thread-4 INFO:Local step 215500, global step 3449976: loss 0.0393
[2019-04-04 02:58:49,796] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 215500, global step 3449976: learning rate 0.0001
[2019-04-04 02:58:53,406] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.5328393e-27 5.7646642e-23 4.6164895e-26 1.2641139e-22 8.5138010e-24
 1.1027452e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 02:58:53,406] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1983
[2019-04-04 02:58:53,424] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.9, 62.0, 99.33333333333333, 0.0, 26.0, 25.73519274182837, 0.3298256318336322, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1954200.0000, 
sim time next is 1954800.0000, 
raw observation next is [-2.8, 62.0, 93.0, 0.0, 26.0, 25.7090927140359, 0.3162621520966807, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.38504155124653744, 0.62, 0.31, 0.0, 0.6666666666666666, 0.642424392836325, 0.6054207173655602, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.10843851], dtype=float32), -1.1046116]. 
=============================================
[2019-04-04 02:58:55,126] A3C_AGENT_WORKER-Thread-7 INFO:Local step 216000, global step 3451504: loss 0.1323
[2019-04-04 02:58:55,127] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 216000, global step 3451504: learning rate 0.0001
[2019-04-04 02:58:56,617] A3C_AGENT_WORKER-Thread-20 INFO:Local step 215500, global step 3451970: loss 0.0445
[2019-04-04 02:58:56,618] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 215500, global step 3451971: learning rate 0.0001
[2019-04-04 02:58:59,137] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.8346322e-26 8.3901414e-20 9.3565041e-24 6.2191971e-22 5.9546947e-22
 2.5817561e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:58:59,137] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1088
[2019-04-04 02:58:59,166] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.30740450772384, 0.1005486121797591, 0.0, 1.0, 41257.72252390526], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2001000.0000, 
sim time next is 2001600.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.25808535745524, 0.08749486572508425, 0.0, 1.0, 41262.88432756641], 
processed observation next is [1.0, 0.17391304347826086, 0.30747922437673136, 0.83, 0.0, 0.0, 0.6666666666666666, 0.52150711312127, 0.5291649552416947, 0.0, 1.0, 0.19648992536936385], 
reward next is 0.8035, 
noisyNet noise sample is [array([-0.7647993], dtype=float32), 2.6468902]. 
=============================================
[2019-04-04 02:59:00,143] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.6431543e-27 4.8907862e-22 6.3698053e-25 6.1656692e-22 1.0422940e-22
 1.7546565e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:59:00,143] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8345
[2019-04-04 02:59:00,208] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 77.0, 156.0, 0.0, 26.0, 25.53486147436868, 0.3023591702177633, 1.0, 1.0, 52571.68456474561], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2032200.0000, 
sim time next is 2032800.0000, 
raw observation next is [-4.5, 77.66666666666667, 154.6666666666667, 0.0, 26.0, 25.35074112643948, 0.2998232188013544, 1.0, 1.0, 43344.24290241934], 
processed observation next is [1.0, 0.5217391304347826, 0.3379501385041552, 0.7766666666666667, 0.5155555555555558, 0.0, 0.6666666666666666, 0.6125617605366234, 0.5999410729337847, 1.0, 1.0, 0.20640115667818734], 
reward next is 0.7936, 
noisyNet noise sample is [array([-1.8806673], dtype=float32), -0.11732851]. 
=============================================
[2019-04-04 02:59:00,271] A3C_AGENT_WORKER-Thread-16 INFO:Local step 215500, global step 3453072: loss 0.0493
[2019-04-04 02:59:00,280] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 215500, global step 3453072: learning rate 0.0001
[2019-04-04 02:59:04,835] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.8204094e-27 6.4737324e-21 1.8320535e-24 1.8839627e-22 2.1337102e-22
 7.0833207e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 02:59:04,835] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7164
[2019-04-04 02:59:04,913] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.0, 78.5, 0.0, 0.0, 26.0, 24.14464258692141, 0.1866143662050263, 1.0, 1.0, 202918.6153901444], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2100600.0000, 
sim time next is 2101200.0000, 
raw observation next is [-7.1, 78.66666666666667, 0.0, 0.0, 26.0, 24.78087171698596, 0.2550456503826236, 1.0, 1.0, 51852.88639827752], 
processed observation next is [1.0, 0.30434782608695654, 0.2659279778393352, 0.7866666666666667, 0.0, 0.0, 0.6666666666666666, 0.5650726430821633, 0.5850152167942079, 1.0, 1.0, 0.2469185066584644], 
reward next is 0.7531, 
noisyNet noise sample is [array([-0.13110647], dtype=float32), -0.60868746]. 
=============================================
[2019-04-04 02:59:06,337] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.2079238e-24 4.4743666e-20 1.0672229e-22 7.9513528e-21 1.4360193e-20
 3.7325660e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:59:06,338] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2412
[2019-04-04 02:59:06,355] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.2, 87.0, 0.0, 0.0, 26.0, 24.19878067515547, 0.09549961067089048, 0.0, 1.0, 43475.66201581914], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2091600.0000, 
sim time next is 2092200.0000, 
raw observation next is [-6.283333333333333, 86.33333333333333, 0.0, 0.0, 26.0, 24.11543424188593, 0.104119958614293, 0.0, 1.0, 43822.83469406805], 
processed observation next is [1.0, 0.21739130434782608, 0.288550323176362, 0.8633333333333333, 0.0, 0.0, 0.6666666666666666, 0.5096195201571607, 0.5347066528714309, 0.0, 1.0, 0.20868016520984786], 
reward next is 0.7913, 
noisyNet noise sample is [array([-0.76275724], dtype=float32), -0.5000593]. 
=============================================
[2019-04-04 02:59:07,998] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.1921562e-25 9.7496793e-20 3.6377535e-23 2.1252711e-21 2.8972651e-21
 1.5952352e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:59:07,999] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8962
[2019-04-04 02:59:08,032] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.7, 80.5, 0.0, 0.0, 26.0, 23.91855928716343, 0.03999057634350158, 0.0, 1.0, 43631.33120207255], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2097000.0000, 
sim time next is 2097600.0000, 
raw observation next is [-6.700000000000001, 79.66666666666667, 0.0, 0.0, 26.0, 23.87897140928377, 0.03193400653815419, 0.0, 1.0, 43594.9564209496], 
processed observation next is [1.0, 0.2608695652173913, 0.2770083102493075, 0.7966666666666667, 0.0, 0.0, 0.6666666666666666, 0.48991428410698096, 0.5106446688460514, 0.0, 1.0, 0.2075950305759505], 
reward next is 0.7924, 
noisyNet noise sample is [array([0.5636741], dtype=float32), -1.1277956]. 
=============================================
[2019-04-04 02:59:09,910] A3C_AGENT_WORKER-Thread-14 INFO:Local step 216000, global step 3455858: loss 0.0510
[2019-04-04 02:59:09,912] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 216000, global step 3455858: learning rate 0.0001
[2019-04-04 02:59:11,083] A3C_AGENT_WORKER-Thread-5 INFO:Local step 216000, global step 3456198: loss 0.0861
[2019-04-04 02:59:11,086] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 216000, global step 3456199: learning rate 0.0001
[2019-04-04 02:59:11,836] A3C_AGENT_WORKER-Thread-15 INFO:Local step 216000, global step 3456434: loss 0.0448
[2019-04-04 02:59:11,853] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 216000, global step 3456437: learning rate 0.0001
[2019-04-04 02:59:12,272] A3C_AGENT_WORKER-Thread-3 INFO:Local step 216000, global step 3456561: loss 0.0697
[2019-04-04 02:59:12,281] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 216000, global step 3456561: learning rate 0.0001
[2019-04-04 02:59:12,580] A3C_AGENT_WORKER-Thread-2 INFO:Local step 216000, global step 3456660: loss 0.0585
[2019-04-04 02:59:12,582] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 216000, global step 3456660: learning rate 0.0001
[2019-04-04 02:59:13,265] A3C_AGENT_WORKER-Thread-6 INFO:Local step 216000, global step 3456888: loss 0.0614
[2019-04-04 02:59:13,267] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 216000, global step 3456889: learning rate 0.0001
[2019-04-04 02:59:13,308] A3C_AGENT_WORKER-Thread-19 INFO:Local step 216500, global step 3456904: loss 0.3220
[2019-04-04 02:59:13,308] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 216500, global step 3456904: learning rate 0.0001
[2019-04-04 02:59:14,012] A3C_AGENT_WORKER-Thread-18 INFO:Local step 216000, global step 3457122: loss 0.0660
[2019-04-04 02:59:14,015] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 216000, global step 3457122: learning rate 0.0001
[2019-04-04 02:59:14,062] A3C_AGENT_WORKER-Thread-9 INFO:Local step 216000, global step 3457137: loss 0.0467
[2019-04-04 02:59:14,062] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 216000, global step 3457137: learning rate 0.0001
[2019-04-04 02:59:14,560] A3C_AGENT_WORKER-Thread-10 INFO:Local step 216000, global step 3457286: loss 0.0403
[2019-04-04 02:59:14,562] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 216000, global step 3457286: learning rate 0.0001
[2019-04-04 02:59:15,440] A3C_AGENT_WORKER-Thread-8 INFO:Local step 216000, global step 3457550: loss 0.0637
[2019-04-04 02:59:15,441] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 216000, global step 3457550: learning rate 0.0001
[2019-04-04 02:59:15,714] A3C_AGENT_WORKER-Thread-17 INFO:Local step 216500, global step 3457631: loss 0.2958
[2019-04-04 02:59:15,715] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 216500, global step 3457631: learning rate 0.0001
[2019-04-04 02:59:15,940] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.7753626e-28 5.4894451e-23 2.3454476e-26 4.1270848e-24 8.5529940e-24
 6.6514162e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 02:59:15,941] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8315
[2019-04-04 02:59:15,984] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.1, 69.0, 140.5, 0.0, 26.0, 25.6034007370554, 0.3633589214897736, 1.0, 1.0, 27877.58213572267], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2202000.0000, 
sim time next is 2202600.0000, 
raw observation next is [-4.0, 68.5, 138.0, 0.0, 26.0, 25.66215745641906, 0.3579155079665746, 1.0, 1.0, 25368.25672590219], 
processed observation next is [1.0, 0.4782608695652174, 0.3518005540166205, 0.685, 0.46, 0.0, 0.6666666666666666, 0.638513121368255, 0.6193051693221915, 1.0, 1.0, 0.12080122250429613], 
reward next is 0.8792, 
noisyNet noise sample is [array([0.41006228], dtype=float32), 2.4073493]. 
=============================================
[2019-04-04 02:59:17,500] A3C_AGENT_WORKER-Thread-4 INFO:Local step 216000, global step 3458153: loss 0.0655
[2019-04-04 02:59:17,503] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 216000, global step 3458155: learning rate 0.0001
[2019-04-04 02:59:20,555] A3C_AGENT_WORKER-Thread-7 INFO:Local step 216500, global step 3459057: loss 0.2548
[2019-04-04 02:59:20,556] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 216500, global step 3459057: learning rate 0.0001
[2019-04-04 02:59:23,904] A3C_AGENT_WORKER-Thread-20 INFO:Local step 216000, global step 3460144: loss 0.0868
[2019-04-04 02:59:23,910] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 216000, global step 3460144: learning rate 0.0001
[2019-04-04 02:59:27,434] A3C_AGENT_WORKER-Thread-16 INFO:Local step 216000, global step 3461275: loss 0.0407
[2019-04-04 02:59:27,437] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 216000, global step 3461275: learning rate 0.0001
[2019-04-04 02:59:35,441] A3C_AGENT_WORKER-Thread-14 INFO:Local step 216500, global step 3463770: loss 0.2235
[2019-04-04 02:59:35,442] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 216500, global step 3463771: learning rate 0.0001
[2019-04-04 02:59:36,583] A3C_AGENT_WORKER-Thread-5 INFO:Local step 216500, global step 3464189: loss 0.2815
[2019-04-04 02:59:36,587] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 216500, global step 3464189: learning rate 0.0001
[2019-04-04 02:59:37,258] A3C_AGENT_WORKER-Thread-15 INFO:Local step 216500, global step 3464416: loss 0.3080
[2019-04-04 02:59:37,258] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 216500, global step 3464416: learning rate 0.0001
[2019-04-04 02:59:38,044] A3C_AGENT_WORKER-Thread-19 INFO:Local step 217000, global step 3464688: loss 0.1881
[2019-04-04 02:59:38,045] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 217000, global step 3464688: learning rate 0.0001
[2019-04-04 02:59:38,631] A3C_AGENT_WORKER-Thread-3 INFO:Local step 216500, global step 3464880: loss 0.1896
[2019-04-04 02:59:38,633] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 216500, global step 3464880: learning rate 0.0001
[2019-04-04 02:59:38,684] A3C_AGENT_WORKER-Thread-2 INFO:Local step 216500, global step 3464895: loss 0.1835
[2019-04-04 02:59:38,685] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 216500, global step 3464895: learning rate 0.0001
[2019-04-04 02:59:38,935] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [6.3167189e-24 7.3949626e-19 1.0495115e-22 6.0021792e-21 1.9262886e-20
 2.9680513e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 02:59:38,935] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.6602
[2019-04-04 02:59:38,974] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.4, 61.0, 0.0, 0.0, 26.0, 23.42834672545534, -0.1018728424024672, 0.0, 1.0, 44419.61312006966], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2436000.0000, 
sim time next is 2436600.0000, 
raw observation next is [-8.4, 61.0, 0.0, 0.0, 26.0, 23.39240033448608, -0.1097425287544131, 0.0, 1.0, 44407.42444230639], 
processed observation next is [0.0, 0.17391304347826086, 0.2299168975069252, 0.61, 0.0, 0.0, 0.6666666666666666, 0.44936669454050665, 0.46341915708186227, 0.0, 1.0, 0.2114639259157447], 
reward next is 0.7885, 
noisyNet noise sample is [array([-0.5453292], dtype=float32), -0.3416941]. 
=============================================
[2019-04-04 02:59:38,987] A3C_AGENT_WORKER-Thread-6 INFO:Local step 216500, global step 3464987: loss 0.1577
[2019-04-04 02:59:38,987] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 216500, global step 3464987: learning rate 0.0001
[2019-04-04 02:59:39,239] A3C_AGENT_WORKER-Thread-17 INFO:Local step 217000, global step 3465071: loss 0.1253
[2019-04-04 02:59:39,240] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 217000, global step 3465071: learning rate 0.0001
[2019-04-04 02:59:39,612] A3C_AGENT_WORKER-Thread-10 INFO:Local step 216500, global step 3465198: loss 0.1590
[2019-04-04 02:59:39,612] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 216500, global step 3465198: learning rate 0.0001
[2019-04-04 02:59:39,669] A3C_AGENT_WORKER-Thread-18 INFO:Local step 216500, global step 3465223: loss 0.1489
[2019-04-04 02:59:39,671] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 216500, global step 3465223: learning rate 0.0001
[2019-04-04 02:59:40,145] A3C_AGENT_WORKER-Thread-9 INFO:Local step 216500, global step 3465410: loss 0.1402
[2019-04-04 02:59:40,146] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 216500, global step 3465410: learning rate 0.0001
[2019-04-04 02:59:41,132] A3C_AGENT_WORKER-Thread-8 INFO:Local step 216500, global step 3465731: loss 0.1020
[2019-04-04 02:59:41,133] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 216500, global step 3465731: learning rate 0.0001
[2019-04-04 02:59:42,464] A3C_AGENT_WORKER-Thread-4 INFO:Local step 216500, global step 3466211: loss 0.0413
[2019-04-04 02:59:42,466] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 216500, global step 3466212: learning rate 0.0001
[2019-04-04 02:59:43,394] A3C_AGENT_WORKER-Thread-7 INFO:Local step 217000, global step 3466596: loss 0.0866
[2019-04-04 02:59:43,395] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 217000, global step 3466596: learning rate 0.0001
[2019-04-04 02:59:48,516] A3C_AGENT_WORKER-Thread-20 INFO:Local step 216500, global step 3468606: loss 0.0070
[2019-04-04 02:59:48,516] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 216500, global step 3468606: learning rate 0.0001
[2019-04-04 02:59:51,526] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.8615299e-25 1.1397661e-19 7.5562052e-23 4.7720110e-21 1.0943637e-20
 2.3254138e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 02:59:51,526] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2736
[2019-04-04 02:59:51,544] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-15.0, 91.0, 0.0, 0.0, 26.0, 23.51180627460512, -0.01140833228865865, 0.0, 1.0, 44500.55273092505], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2692800.0000, 
sim time next is 2693400.0000, 
raw observation next is [-15.0, 89.66666666666667, 0.0, 0.0, 26.0, 23.43901901650056, -0.01876596742700646, 0.0, 1.0, 44479.33604797773], 
processed observation next is [1.0, 0.17391304347826086, 0.04709141274238226, 0.8966666666666667, 0.0, 0.0, 0.6666666666666666, 0.45325158470837995, 0.4937446775243312, 0.0, 1.0, 0.2118063621332273], 
reward next is 0.7882, 
noisyNet noise sample is [array([0.49893376], dtype=float32), 1.5652691]. 
=============================================
[2019-04-04 02:59:52,130] A3C_AGENT_WORKER-Thread-16 INFO:Local step 216500, global step 3469896: loss 0.0049
[2019-04-04 02:59:52,133] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 216500, global step 3469896: learning rate 0.0001
[2019-04-04 02:59:52,254] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.9159553e-25 3.4784247e-20 1.3644353e-24 3.2846959e-22 4.7729190e-22
 2.0133446e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:59:52,254] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8562
[2019-04-04 02:59:52,297] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.166666666666667, 32.66666666666666, 86.66666666666667, 831.6666666666667, 26.0, 24.96063454854883, 0.2652229029367758, 0.0, 1.0, 18730.29374652391], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2461200.0000, 
sim time next is 2461800.0000, 
raw observation next is [-0.8833333333333332, 31.83333333333334, 87.33333333333334, 834.3333333333334, 26.0, 25.0309166224724, 0.2659186045880641, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.43813481071098803, 0.3183333333333334, 0.29111111111111115, 0.921915285451197, 0.6666666666666666, 0.5859097185393667, 0.588639534862688, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4117224], dtype=float32), -1.0721093]. 
=============================================
[2019-04-04 02:59:55,902] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.7961023e-25 5.4722676e-20 1.0753507e-23 1.5621049e-21 5.0424595e-21
 7.4865917e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:59:55,905] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7039
[2019-04-04 02:59:55,972] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 65.0, 169.1666666666667, 709.1666666666667, 26.0, 25.05054027155332, 0.3961856520499302, 0.0, 1.0, 30595.95339748564], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2983200.0000, 
sim time next is 2983800.0000, 
raw observation next is [-3.0, 65.0, 157.3333333333333, 727.3333333333334, 26.0, 25.0600081472442, 0.3996871222216677, 0.0, 1.0, 27291.65754245599], 
processed observation next is [0.0, 0.5217391304347826, 0.3795013850415513, 0.65, 0.5244444444444443, 0.8036832412523021, 0.6666666666666666, 0.5883340122703501, 0.6332290407405559, 0.0, 1.0, 0.1299602740116952], 
reward next is 0.8700, 
noisyNet noise sample is [array([-0.8202381], dtype=float32), -2.0716877]. 
=============================================
[2019-04-04 02:59:56,394] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.1464490e-26 1.8419028e-20 3.2665117e-24 8.3236655e-22 6.5970572e-22
 3.1081157e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 02:59:56,394] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8170
[2019-04-04 02:59:56,451] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 60.0, 104.0, 767.3333333333334, 26.0, 25.11105607057395, 0.4129695866293375, 0.0, 1.0, 18712.95613905228], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2988600.0000, 
sim time next is 2989200.0000, 
raw observation next is [-2.0, 60.00000000000001, 102.5, 759.1666666666667, 26.0, 25.1486207520684, 0.4187886686817798, 0.0, 1.0, 18711.19869482028], 
processed observation next is [0.0, 0.6086956521739131, 0.40720221606648205, 0.6000000000000001, 0.3416666666666667, 0.8388581952117865, 0.6666666666666666, 0.5957183960057, 0.6395962228939266, 0.0, 1.0, 0.08910094616581085], 
reward next is 0.9109, 
noisyNet noise sample is [array([-0.46464577], dtype=float32), 1.7050159]. 
=============================================
[2019-04-04 02:59:57,723] A3C_AGENT_WORKER-Thread-14 INFO:Local step 217000, global step 3471785: loss 0.0814
[2019-04-04 02:59:57,724] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 217000, global step 3471785: learning rate 0.0001
[2019-04-04 02:59:58,630] A3C_AGENT_WORKER-Thread-5 INFO:Local step 217000, global step 3472099: loss 0.1054
[2019-04-04 02:59:58,630] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 217000, global step 3472099: learning rate 0.0001
[2019-04-04 02:59:59,517] A3C_AGENT_WORKER-Thread-15 INFO:Local step 217000, global step 3472428: loss 0.1032
[2019-04-04 02:59:59,517] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 217000, global step 3472428: learning rate 0.0001
[2019-04-04 03:00:00,111] A3C_AGENT_WORKER-Thread-19 INFO:Local step 217500, global step 3472615: loss 0.0189
[2019-04-04 03:00:00,114] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 217500, global step 3472615: learning rate 0.0001
[2019-04-04 03:00:00,454] A3C_AGENT_WORKER-Thread-2 INFO:Local step 217000, global step 3472719: loss 0.1301
[2019-04-04 03:00:00,467] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 217000, global step 3472719: learning rate 0.0001
[2019-04-04 03:00:00,660] A3C_AGENT_WORKER-Thread-3 INFO:Local step 217000, global step 3472775: loss 0.1306
[2019-04-04 03:00:00,660] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 217000, global step 3472775: learning rate 0.0001
[2019-04-04 03:00:01,163] A3C_AGENT_WORKER-Thread-17 INFO:Local step 217500, global step 3472938: loss 0.0163
[2019-04-04 03:00:01,164] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 217500, global step 3472938: learning rate 0.0001
[2019-04-04 03:00:01,267] A3C_AGENT_WORKER-Thread-6 INFO:Local step 217000, global step 3472970: loss 0.1256
[2019-04-04 03:00:01,270] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 217000, global step 3472972: learning rate 0.0001
[2019-04-04 03:00:01,466] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0391119e-26 1.2911780e-21 7.0933022e-25 2.7704898e-22 3.1830535e-22
 2.2255013e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:00:01,467] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2876
[2019-04-04 03:00:01,524] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 59.0, 0.0, 0.0, 26.0, 25.18840978686011, 0.3965364083754925, 1.0, 1.0, 58619.31484583399], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2747400.0000, 
sim time next is 2748000.0000, 
raw observation next is [-5.0, 59.0, 0.0, 0.0, 26.0, 25.14749378237119, 0.3911675956433002, 0.0, 1.0, 76053.55267691765], 
processed observation next is [1.0, 0.8260869565217391, 0.32409972299168976, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5956244818642658, 0.6303891985477668, 0.0, 1.0, 0.3621597746519888], 
reward next is 0.6378, 
noisyNet noise sample is [array([-0.03554035], dtype=float32), -0.14483647]. 
=============================================
[2019-04-04 03:00:01,532] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[79.5434 ]
 [79.33723]
 [79.04374]
 [78.73859]
 [78.93158]], R is [[78.53694153]
 [78.472435  ]
 [78.39105988]
 [78.19703674]
 [78.16283417]].
[2019-04-04 03:00:02,048] A3C_AGENT_WORKER-Thread-10 INFO:Local step 217000, global step 3473245: loss 0.1532
[2019-04-04 03:00:02,050] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 217000, global step 3473246: learning rate 0.0001
[2019-04-04 03:00:02,084] A3C_AGENT_WORKER-Thread-18 INFO:Local step 217000, global step 3473262: loss 0.1370
[2019-04-04 03:00:02,086] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 217000, global step 3473262: learning rate 0.0001
[2019-04-04 03:00:02,638] A3C_AGENT_WORKER-Thread-9 INFO:Local step 217000, global step 3473462: loss 0.1432
[2019-04-04 03:00:02,641] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 217000, global step 3473462: learning rate 0.0001
[2019-04-04 03:00:03,545] A3C_AGENT_WORKER-Thread-8 INFO:Local step 217000, global step 3473793: loss 0.1453
[2019-04-04 03:00:03,545] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 217000, global step 3473793: learning rate 0.0001
[2019-04-04 03:00:05,107] A3C_AGENT_WORKER-Thread-4 INFO:Local step 217000, global step 3474396: loss 0.1283
[2019-04-04 03:00:05,108] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 217000, global step 3474396: learning rate 0.0001
[2019-04-04 03:00:06,200] A3C_AGENT_WORKER-Thread-7 INFO:Local step 217500, global step 3474792: loss 0.1573
[2019-04-04 03:00:06,200] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 217500, global step 3474792: learning rate 0.0001
[2019-04-04 03:00:10,436] A3C_AGENT_WORKER-Thread-20 INFO:Local step 217000, global step 3476517: loss 0.0821
[2019-04-04 03:00:10,437] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 217000, global step 3476517: learning rate 0.0001
[2019-04-04 03:00:14,837] A3C_AGENT_WORKER-Thread-16 INFO:Local step 217000, global step 3478194: loss 0.0059
[2019-04-04 03:00:14,837] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 217000, global step 3478194: learning rate 0.0001
[2019-04-04 03:00:15,567] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.13501777e-25 1.25251826e-20 4.26802859e-24 1.11738732e-21
 6.20398435e-22 3.21876401e-25 1.00000000e+00], sum to 1.0000
[2019-04-04 03:00:15,567] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4133
[2019-04-04 03:00:15,582] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.77573354971461, 0.312352951438599, 0.0, 1.0, 43241.2838328237], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2943600.0000, 
sim time next is 2944200.0000, 
raw observation next is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.79839551445584, 0.3073573801149441, 0.0, 1.0, 43185.62860831132], 
processed observation next is [0.0, 0.043478260869565216, 0.40720221606648205, 0.85, 0.0, 0.0, 0.6666666666666666, 0.5665329595379868, 0.6024524600383147, 0.0, 1.0, 0.2056458505157682], 
reward next is 0.7944, 
noisyNet noise sample is [array([-0.07872664], dtype=float32), 0.94317967]. 
=============================================
[2019-04-04 03:00:18,699] A3C_AGENT_WORKER-Thread-19 INFO:Local step 218000, global step 3479533: loss 0.0407
[2019-04-04 03:00:18,725] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 218000, global step 3479538: learning rate 0.0001
[2019-04-04 03:00:19,260] A3C_AGENT_WORKER-Thread-17 INFO:Local step 218000, global step 3479718: loss 0.0364
[2019-04-04 03:00:19,263] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 218000, global step 3479719: learning rate 0.0001
[2019-04-04 03:00:19,440] A3C_AGENT_WORKER-Thread-5 INFO:Local step 217500, global step 3479799: loss 0.9131
[2019-04-04 03:00:19,442] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 217500, global step 3479799: learning rate 0.0001
[2019-04-04 03:00:19,887] A3C_AGENT_WORKER-Thread-14 INFO:Local step 217500, global step 3479984: loss 0.9641
[2019-04-04 03:00:19,889] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 217500, global step 3479985: learning rate 0.0001
[2019-04-04 03:00:21,564] A3C_AGENT_WORKER-Thread-15 INFO:Local step 217500, global step 3480605: loss 1.0430
[2019-04-04 03:00:21,565] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 217500, global step 3480605: learning rate 0.0001
[2019-04-04 03:00:22,216] A3C_AGENT_WORKER-Thread-2 INFO:Local step 217500, global step 3480830: loss 1.1160
[2019-04-04 03:00:22,216] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 217500, global step 3480830: learning rate 0.0001
[2019-04-04 03:00:22,267] A3C_AGENT_WORKER-Thread-3 INFO:Local step 217500, global step 3480850: loss 1.0717
[2019-04-04 03:00:22,268] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 217500, global step 3480850: learning rate 0.0001
[2019-04-04 03:00:22,662] A3C_AGENT_WORKER-Thread-6 INFO:Local step 217500, global step 3481009: loss 1.0764
[2019-04-04 03:00:22,663] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 217500, global step 3481009: learning rate 0.0001
[2019-04-04 03:00:23,863] A3C_AGENT_WORKER-Thread-10 INFO:Local step 217500, global step 3481523: loss 1.1074
[2019-04-04 03:00:23,864] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 217500, global step 3481523: learning rate 0.0001
[2019-04-04 03:00:24,131] A3C_AGENT_WORKER-Thread-18 INFO:Local step 217500, global step 3481635: loss 1.1685
[2019-04-04 03:00:24,132] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 217500, global step 3481635: learning rate 0.0001
[2019-04-04 03:00:24,301] A3C_AGENT_WORKER-Thread-8 INFO:Local step 217500, global step 3481703: loss 1.1939
[2019-04-04 03:00:24,302] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 217500, global step 3481703: learning rate 0.0001
[2019-04-04 03:00:24,784] A3C_AGENT_WORKER-Thread-9 INFO:Local step 217500, global step 3481868: loss 1.1683
[2019-04-04 03:00:24,785] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 217500, global step 3481868: learning rate 0.0001
[2019-04-04 03:00:24,955] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.2906970e-26 3.3758301e-21 1.8769872e-25 2.5629379e-23 8.4577078e-23
 3.1235302e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:00:24,955] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6188
[2019-04-04 03:00:25,003] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 54.0, 106.0, 759.0, 26.0, 25.13119279548372, 0.3079334918927844, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3061800.0000, 
sim time next is 3062400.0000, 
raw observation next is [-4.0, 54.0, 106.8333333333333, 766.6666666666666, 26.0, 25.07498409860442, 0.3043483448538393, 0.0, 1.0, 35924.53348717919], 
processed observation next is [0.0, 0.43478260869565216, 0.3518005540166205, 0.54, 0.356111111111111, 0.8471454880294659, 0.6666666666666666, 0.5895820082170351, 0.6014494482846131, 0.0, 1.0, 0.17106920708180567], 
reward next is 0.8289, 
noisyNet noise sample is [array([0.54541916], dtype=float32), -0.69487786]. 
=============================================
[2019-04-04 03:00:25,111] A3C_AGENT_WORKER-Thread-7 INFO:Local step 218000, global step 3481972: loss 0.0306
[2019-04-04 03:00:25,112] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 218000, global step 3481972: learning rate 0.0001
[2019-04-04 03:00:25,798] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.9043635e-25 1.3762055e-19 9.9769826e-24 1.7478470e-21 7.6379335e-22
 1.1148438e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 03:00:25,800] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6434
[2019-04-04 03:00:25,813] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 70.0, 0.0, 0.0, 26.0, 23.78757533784637, -0.002851895053407522, 0.0, 1.0, 40185.80136078024], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3045600.0000, 
sim time next is 3046200.0000, 
raw observation next is [-6.0, 71.16666666666667, 0.0, 0.0, 26.0, 23.76000552961344, -0.009181010160241948, 0.0, 1.0, 40215.71112227936], 
processed observation next is [0.0, 0.2608695652173913, 0.296398891966759, 0.7116666666666667, 0.0, 0.0, 0.6666666666666666, 0.4800004608011201, 0.4969396632799194, 0.0, 1.0, 0.19150338629656838], 
reward next is 0.8085, 
noisyNet noise sample is [array([0.21296465], dtype=float32), 0.9721686]. 
=============================================
[2019-04-04 03:00:26,497] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.9897953e-27 3.2122695e-21 1.1382598e-24 6.0124397e-23 2.6915477e-22
 5.0330685e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:00:26,497] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2053
[2019-04-04 03:00:26,517] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.666666666666667, 100.0, 0.0, 0.0, 26.0, 25.21944469956254, 0.2873083733637556, 0.0, 1.0, 54074.08867425823], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3125400.0000, 
sim time next is 3126000.0000, 
raw observation next is [2.733333333333333, 100.0, 0.0, 0.0, 26.0, 25.19728202977266, 0.2924963568600783, 0.0, 1.0, 53979.69019559796], 
processed observation next is [1.0, 0.17391304347826086, 0.538319482917821, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5997735024810549, 0.5974987856200261, 0.0, 1.0, 0.2570461437885617], 
reward next is 0.7430, 
noisyNet noise sample is [array([1.7080551], dtype=float32), 0.6861308]. 
=============================================
[2019-04-04 03:00:26,531] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[84.03075 ]
 [84.07943 ]
 [84.09606 ]
 [84.105705]
 [84.141914]], R is [[83.88134766]
 [83.78504181]
 [83.68863678]
 [83.59029388]
 [83.4838562 ]].
[2019-04-04 03:00:27,022] A3C_AGENT_WORKER-Thread-4 INFO:Local step 217500, global step 3482799: loss 1.2641
[2019-04-04 03:00:27,023] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 217500, global step 3482799: learning rate 0.0001
[2019-04-04 03:00:31,298] A3C_AGENT_WORKER-Thread-20 INFO:Local step 217500, global step 3484927: loss 0.9838
[2019-04-04 03:00:31,300] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 217500, global step 3484927: learning rate 0.0001
[2019-04-04 03:00:31,360] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.2705478e-27 4.7995414e-21 1.4935333e-25 6.2305206e-23 8.6377568e-23
 3.7550697e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:00:31,361] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3105
[2019-04-04 03:00:31,437] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 54.0, 103.6666666666667, 717.6666666666667, 26.0, 25.20018558071985, 0.3178488087000712, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3060600.0000, 
sim time next is 3061200.0000, 
raw observation next is [-4.0, 54.00000000000001, 104.8333333333333, 738.3333333333333, 26.0, 25.17872067477375, 0.3141546340189539, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.3518005540166205, 0.54, 0.34944444444444434, 0.8158379373848986, 0.6666666666666666, 0.5982267228978125, 0.6047182113396513, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.08667964], dtype=float32), -0.82302064]. 
=============================================
[2019-04-04 03:00:32,765] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.1604562e-27 9.9677869e-22 4.3509586e-26 1.4486774e-23 1.3558490e-23
 1.8795934e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:00:32,765] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4132
[2019-04-04 03:00:32,799] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.8333333333333334, 41.5, 102.3333333333333, 785.3333333333334, 26.0, 25.11960416736019, 0.3637409919468075, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3075000.0000, 
sim time next is 3075600.0000, 
raw observation next is [-0.6666666666666667, 41.0, 100.6666666666667, 780.1666666666667, 26.0, 25.12247100041987, 0.3623635776922067, 0.0, 1.0, 18705.60517143038], 
processed observation next is [0.0, 0.6086956521739131, 0.44413665743305636, 0.41, 0.33555555555555566, 0.8620626151012892, 0.6666666666666666, 0.5935392500349893, 0.6207878592307355, 0.0, 1.0, 0.08907431034014467], 
reward next is 0.9109, 
noisyNet noise sample is [array([1.3966913], dtype=float32), -0.7334766]. 
=============================================
[2019-04-04 03:00:35,491] A3C_AGENT_WORKER-Thread-19 INFO:Local step 218500, global step 3486860: loss 3.0331
[2019-04-04 03:00:35,494] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 218500, global step 3486860: learning rate 0.0001
[2019-04-04 03:00:35,872] A3C_AGENT_WORKER-Thread-16 INFO:Local step 217500, global step 3487030: loss 0.8390
[2019-04-04 03:00:35,872] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 217500, global step 3487030: learning rate 0.0001
[2019-04-04 03:00:36,463] A3C_AGENT_WORKER-Thread-17 INFO:Local step 218500, global step 3487303: loss 3.1275
[2019-04-04 03:00:36,465] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 218500, global step 3487303: learning rate 0.0001
[2019-04-04 03:00:37,662] A3C_AGENT_WORKER-Thread-5 INFO:Local step 218000, global step 3487840: loss 0.0696
[2019-04-04 03:00:37,663] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 218000, global step 3487842: learning rate 0.0001
[2019-04-04 03:00:38,120] A3C_AGENT_WORKER-Thread-14 INFO:Local step 218000, global step 3488050: loss 0.0641
[2019-04-04 03:00:38,123] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 218000, global step 3488050: learning rate 0.0001
[2019-04-04 03:00:38,767] A3C_AGENT_WORKER-Thread-15 INFO:Local step 218000, global step 3488358: loss 0.0634
[2019-04-04 03:00:38,776] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 218000, global step 3488361: learning rate 0.0001
[2019-04-04 03:00:40,173] A3C_AGENT_WORKER-Thread-3 INFO:Local step 218000, global step 3488992: loss 0.0713
[2019-04-04 03:00:40,173] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 218000, global step 3488992: learning rate 0.0001
[2019-04-04 03:00:40,177] A3C_AGENT_WORKER-Thread-6 INFO:Local step 218000, global step 3488994: loss 0.0650
[2019-04-04 03:00:40,195] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 218000, global step 3488999: learning rate 0.0001
[2019-04-04 03:00:40,206] A3C_AGENT_WORKER-Thread-2 INFO:Local step 218000, global step 3489004: loss 0.0689
[2019-04-04 03:00:40,207] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 218000, global step 3489004: learning rate 0.0001
[2019-04-04 03:00:41,400] A3C_AGENT_WORKER-Thread-7 INFO:Local step 218500, global step 3489578: loss 3.0877
[2019-04-04 03:00:41,402] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 218500, global step 3489581: learning rate 0.0001
[2019-04-04 03:00:41,461] A3C_AGENT_WORKER-Thread-10 INFO:Local step 218000, global step 3489609: loss 0.0429
[2019-04-04 03:00:41,463] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 218000, global step 3489611: learning rate 0.0001
[2019-04-04 03:00:41,696] A3C_AGENT_WORKER-Thread-18 INFO:Local step 218000, global step 3489743: loss 0.0389
[2019-04-04 03:00:41,696] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 218000, global step 3489743: learning rate 0.0001
[2019-04-04 03:00:42,223] A3C_AGENT_WORKER-Thread-9 INFO:Local step 218000, global step 3490007: loss 0.0366
[2019-04-04 03:00:42,224] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 218000, global step 3490008: learning rate 0.0001
[2019-04-04 03:00:42,548] A3C_AGENT_WORKER-Thread-8 INFO:Local step 218000, global step 3490168: loss 0.0395
[2019-04-04 03:00:42,549] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 218000, global step 3490168: learning rate 0.0001
[2019-04-04 03:00:44,057] A3C_AGENT_WORKER-Thread-4 INFO:Local step 218000, global step 3490926: loss 0.0421
[2019-04-04 03:00:44,058] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 218000, global step 3490927: learning rate 0.0001
[2019-04-04 03:00:46,816] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.2782602e-26 3.5702012e-21 1.8410842e-24 3.4620309e-23 6.2050227e-23
 6.1066696e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:00:46,817] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6895
[2019-04-04 03:00:46,838] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.25588706468866, 0.3642360880912466, 0.0, 1.0, 45837.00538318273], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3480600.0000, 
sim time next is 3481200.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.22024487085626, 0.3507956242875367, 0.0, 1.0, 51733.8073313335], 
processed observation next is [1.0, 0.30434782608695654, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6016870725713549, 0.6169318747625122, 0.0, 1.0, 0.24635146348254047], 
reward next is 0.7536, 
noisyNet noise sample is [array([0.41576833], dtype=float32), 0.6466269]. 
=============================================
[2019-04-04 03:00:48,804] A3C_AGENT_WORKER-Thread-20 INFO:Local step 218000, global step 3493263: loss 0.0900
[2019-04-04 03:00:48,806] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 218000, global step 3493264: learning rate 0.0001
[2019-04-04 03:00:49,095] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.1263870e-26 2.7917533e-20 7.5508709e-24 5.0417648e-22 1.2428511e-21
 7.9564950e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 03:00:49,096] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7752
[2019-04-04 03:00:49,109] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 70.0, 0.0, 0.0, 26.0, 24.53775429480983, 0.2356392945129711, 0.0, 1.0, 40956.02231922626], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3565800.0000, 
sim time next is 3566400.0000, 
raw observation next is [-6.0, 70.0, 0.0, 0.0, 26.0, 24.49150222375233, 0.2263271719078879, 0.0, 1.0, 41029.51704678917], 
processed observation next is [0.0, 0.2608695652173913, 0.296398891966759, 0.7, 0.0, 0.0, 0.6666666666666666, 0.5409585186460276, 0.5754423906359626, 0.0, 1.0, 0.19537865260375797], 
reward next is 0.8046, 
noisyNet noise sample is [array([-0.69737417], dtype=float32), 0.087032355]. 
=============================================
[2019-04-04 03:00:51,123] A3C_AGENT_WORKER-Thread-19 INFO:Local step 219000, global step 3494383: loss 1.8175
[2019-04-04 03:00:51,124] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 219000, global step 3494383: learning rate 0.0001
[2019-04-04 03:00:51,219] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.8465313e-27 6.2789560e-23 9.8902727e-26 8.4155479e-23 5.1960731e-23
 1.4399966e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:00:51,222] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6909
[2019-04-04 03:00:51,240] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 35.33333333333333, 84.33333333333334, 692.6666666666667, 26.0, 26.89140063489771, 0.7764670014463633, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3944400.0000, 
sim time next is 3945000.0000, 
raw observation next is [-4.0, 34.66666666666667, 80.66666666666667, 661.3333333333334, 26.0, 26.9565476984024, 0.7885206678559071, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.3518005540166205, 0.34666666666666673, 0.2688888888888889, 0.730755064456722, 0.6666666666666666, 0.7463789748668667, 0.7628402226186357, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5452516], dtype=float32), -0.95637745]. 
=============================================
[2019-04-04 03:00:51,255] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[82.70129 ]
 [83.03025 ]
 [83.014   ]
 [82.83899 ]
 [82.836174]], R is [[82.63426208]
 [82.80792236]
 [82.97984314]
 [83.1500473 ]
 [82.85046387]].
[2019-04-04 03:00:52,165] A3C_AGENT_WORKER-Thread-17 INFO:Local step 219000, global step 3494931: loss 1.9247
[2019-04-04 03:00:52,166] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 219000, global step 3494931: learning rate 0.0001
[2019-04-04 03:00:52,415] A3C_AGENT_WORKER-Thread-16 INFO:Local step 218000, global step 3495064: loss 0.1197
[2019-04-04 03:00:52,416] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 218000, global step 3495064: learning rate 0.0001
[2019-04-04 03:00:53,576] A3C_AGENT_WORKER-Thread-5 INFO:Local step 218500, global step 3495666: loss 1.7892
[2019-04-04 03:00:53,581] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 218500, global step 3495666: learning rate 0.0001
[2019-04-04 03:00:53,882] A3C_AGENT_WORKER-Thread-14 INFO:Local step 218500, global step 3495802: loss 1.7497
[2019-04-04 03:00:53,883] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 218500, global step 3495802: learning rate 0.0001
[2019-04-04 03:00:54,642] A3C_AGENT_WORKER-Thread-15 INFO:Local step 218500, global step 3496188: loss 1.6395
[2019-04-04 03:00:54,644] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 218500, global step 3496189: learning rate 0.0001
[2019-04-04 03:00:54,830] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.6646016e-27 8.8960236e-22 1.1151069e-25 6.2486307e-24 6.7565646e-23
 6.6966102e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:00:54,838] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1000
[2019-04-04 03:00:54,839] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1214112e-26 9.0807952e-22 9.4513839e-25 1.7756822e-22 1.0772645e-22
 3.5207189e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:00:54,839] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2605
[2019-04-04 03:00:54,853] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.0, 28.0, 106.0, 713.0, 26.0, 25.66735606404694, 0.4686034195709179, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3664800.0000, 
sim time next is 3665400.0000, 
raw observation next is [11.16666666666667, 27.33333333333334, 107.6666666666667, 729.6666666666667, 26.0, 25.65056888328702, 0.4707984550366519, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.7719298245614037, 0.2733333333333334, 0.358888888888889, 0.8062615101289136, 0.6666666666666666, 0.6375474069405849, 0.6569328183455506, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8626746], dtype=float32), 1.5317935]. 
=============================================
[2019-04-04 03:00:54,868] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.25588706468866, 0.3642360880912466, 0.0, 1.0, 45837.00538318273], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3480600.0000, 
sim time next is 3481200.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.22024487085626, 0.3507956242875367, 0.0, 1.0, 51733.8073313335], 
processed observation next is [1.0, 0.30434782608695654, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6016870725713549, 0.6169318747625122, 0.0, 1.0, 0.24635146348254047], 
reward next is 0.7536, 
noisyNet noise sample is [array([-0.04557687], dtype=float32), 0.49816367]. 
=============================================
[2019-04-04 03:00:55,729] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3319781e-26 1.7878418e-22 1.3229498e-25 2.5778222e-23 6.0538890e-23
 3.1750296e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:00:55,730] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8709
[2019-04-04 03:00:55,743] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.333333333333333, 34.33333333333334, 118.1666666666667, 836.8333333333334, 26.0, 26.49082664840711, 0.600381783218983, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4018800.0000, 
sim time next is 4019400.0000, 
raw observation next is [-5.0, 33.0, 118.0, 841.0, 26.0, 26.46573561013935, 0.579173886349824, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.32409972299168976, 0.33, 0.3933333333333333, 0.9292817679558011, 0.6666666666666666, 0.7054779675116126, 0.6930579621166081, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.14632009], dtype=float32), -1.0249622]. 
=============================================
[2019-04-04 03:00:55,914] A3C_AGENT_WORKER-Thread-6 INFO:Local step 218500, global step 3496905: loss 1.6380
[2019-04-04 03:00:55,915] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 218500, global step 3496905: learning rate 0.0001
[2019-04-04 03:00:55,993] A3C_AGENT_WORKER-Thread-3 INFO:Local step 218500, global step 3496951: loss 1.5872
[2019-04-04 03:00:55,995] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 218500, global step 3496951: learning rate 0.0001
[2019-04-04 03:00:56,245] A3C_AGENT_WORKER-Thread-2 INFO:Local step 218500, global step 3497090: loss 1.5243
[2019-04-04 03:00:56,247] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 218500, global step 3497091: learning rate 0.0001
[2019-04-04 03:00:56,420] A3C_AGENT_WORKER-Thread-7 INFO:Local step 219000, global step 3497199: loss 2.0907
[2019-04-04 03:00:56,421] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 219000, global step 3497199: learning rate 0.0001
[2019-04-04 03:00:57,047] A3C_AGENT_WORKER-Thread-10 INFO:Local step 218500, global step 3497594: loss 1.5296
[2019-04-04 03:00:57,049] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 218500, global step 3497595: learning rate 0.0001
[2019-04-04 03:00:57,222] A3C_AGENT_WORKER-Thread-18 INFO:Local step 218500, global step 3497699: loss 1.5853
[2019-04-04 03:00:57,222] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 218500, global step 3497699: learning rate 0.0001
[2019-04-04 03:00:57,932] A3C_AGENT_WORKER-Thread-9 INFO:Local step 218500, global step 3498121: loss 1.5791
[2019-04-04 03:00:57,936] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 218500, global step 3498122: learning rate 0.0001
[2019-04-04 03:00:58,086] A3C_AGENT_WORKER-Thread-8 INFO:Local step 218500, global step 3498215: loss 1.5629
[2019-04-04 03:00:58,087] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 218500, global step 3498215: learning rate 0.0001
[2019-04-04 03:00:58,545] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.8508165e-26 7.6685338e-21 1.3692166e-24 1.4749477e-22 2.4398057e-22
 8.7703220e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:00:58,546] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3538
[2019-04-04 03:00:58,561] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.24012855604513, 0.3792948372184029, 0.0, 1.0, 41600.11595313538], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3480000.0000, 
sim time next is 3480600.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.25588706468866, 0.3642360880912466, 0.0, 1.0, 45837.00538318273], 
processed observation next is [1.0, 0.2608695652173913, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6046572553907218, 0.6214120293637488, 0.0, 1.0, 0.21827145420563204], 
reward next is 0.7817, 
noisyNet noise sample is [array([0.42505893], dtype=float32), -1.1543975]. 
=============================================
[2019-04-04 03:00:59,698] A3C_AGENT_WORKER-Thread-4 INFO:Local step 218500, global step 3499028: loss 1.6610
[2019-04-04 03:00:59,699] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 218500, global step 3499028: learning rate 0.0001
[2019-04-04 03:01:02,199] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.4802271e-26 1.4835304e-22 4.1361294e-25 6.6117916e-23 1.5587589e-22
 5.1931299e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:01:02,199] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8494
[2019-04-04 03:01:02,215] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.3333333333333333, 61.83333333333333, 32.66666666666666, 277.6666666666666, 26.0, 26.20108427603206, 0.5821953554052103, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3777000.0000, 
sim time next is 3777600.0000, 
raw observation next is [-0.6666666666666666, 63.66666666666667, 24.83333333333333, 212.3333333333333, 26.0, 26.29129476255666, 0.536430212650221, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.44413665743305636, 0.6366666666666667, 0.08277777777777776, 0.23462246777163898, 0.6666666666666666, 0.690941230213055, 0.6788100708834071, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6781873], dtype=float32), 0.6709698]. 
=============================================
[2019-04-04 03:01:02,730] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.0422014e-26 6.4701003e-22 8.2201855e-25 4.1080986e-22 2.6205467e-22
 1.4543618e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:01:02,736] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5484
[2019-04-04 03:01:02,761] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 71.0, 0.0, 0.0, 26.0, 25.74087413373565, 0.5021061745895484, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3783600.0000, 
sim time next is 3784200.0000, 
raw observation next is [-2.0, 70.0, 0.0, 0.0, 26.0, 25.57131015840157, 0.4929451503611038, 1.0, 1.0, 74768.59978182681], 
processed observation next is [1.0, 0.8260869565217391, 0.40720221606648205, 0.7, 0.0, 0.0, 0.6666666666666666, 0.6309425132001308, 0.664315050120368, 1.0, 1.0, 0.3560409513420324], 
reward next is 0.6440, 
noisyNet noise sample is [array([-0.61679095], dtype=float32), -0.49355608]. 
=============================================
[2019-04-04 03:01:02,934] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-04 03:01:02,940] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 03:01:02,941] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 03:01:02,941] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:01:02,941] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:01:02,942] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 03:01:02,946] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:01:02,947] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run36
[2019-04-04 03:01:03,018] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run36
[2019-04-04 03:01:03,081] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run36
[2019-04-04 03:04:17,935] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 03:04:48,734] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 03:04:54,815] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 03:04:55,849] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 3500000, evaluation results [3500000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 03:05:00,460] A3C_AGENT_WORKER-Thread-20 INFO:Local step 218500, global step 3501348: loss 1.8982
[2019-04-04 03:05:00,460] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 218500, global step 3501348: learning rate 0.0001
[2019-04-04 03:05:05,584] A3C_AGENT_WORKER-Thread-19 INFO:Local step 219500, global step 3502869: loss 0.0477
[2019-04-04 03:05:05,593] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 219500, global step 3502869: learning rate 0.0001
[2019-04-04 03:05:07,055] A3C_AGENT_WORKER-Thread-17 INFO:Local step 219500, global step 3503360: loss 0.0333
[2019-04-04 03:05:07,058] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 219500, global step 3503362: learning rate 0.0001
[2019-04-04 03:05:07,284] A3C_AGENT_WORKER-Thread-16 INFO:Local step 218500, global step 3503423: loss 2.3766
[2019-04-04 03:05:07,334] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 218500, global step 3503424: learning rate 0.0001
[2019-04-04 03:05:08,086] A3C_AGENT_WORKER-Thread-5 INFO:Local step 219000, global step 3503616: loss 3.8522
[2019-04-04 03:05:08,087] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 219000, global step 3503616: learning rate 0.0001
[2019-04-04 03:05:08,360] A3C_AGENT_WORKER-Thread-14 INFO:Local step 219000, global step 3503686: loss 3.9064
[2019-04-04 03:05:08,362] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 219000, global step 3503686: learning rate 0.0001
[2019-04-04 03:05:09,964] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.12302084e-26 6.50363325e-21 4.42529866e-25 7.71506155e-23
 1.95185759e-22 1.05651944e-25 1.00000000e+00], sum to 1.0000
[2019-04-04 03:05:09,965] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2164
[2019-04-04 03:05:09,983] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.05, 73.5, 0.0, 0.0, 26.0, 25.74505424664915, 0.4370453933015619, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4309800.0000, 
sim time next is 4310400.0000, 
raw observation next is [5.0, 74.0, 0.0, 0.0, 26.0, 25.69303509996219, 0.4251844215632814, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.9130434782608695, 0.6011080332409973, 0.74, 0.0, 0.0, 0.6666666666666666, 0.6410862583301826, 0.6417281405210938, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.61546355], dtype=float32), 0.3378598]. 
=============================================
[2019-04-04 03:05:10,488] A3C_AGENT_WORKER-Thread-15 INFO:Local step 219000, global step 3504296: loss 4.3224
[2019-04-04 03:05:10,489] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 219000, global step 3504296: learning rate 0.0001
[2019-04-04 03:05:12,401] A3C_AGENT_WORKER-Thread-3 INFO:Local step 219000, global step 3504851: loss 4.3104
[2019-04-04 03:05:12,401] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 219000, global step 3504851: learning rate 0.0001
[2019-04-04 03:05:13,154] A3C_AGENT_WORKER-Thread-2 INFO:Local step 219000, global step 3505022: loss 4.4560
[2019-04-04 03:05:13,156] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 219000, global step 3505022: learning rate 0.0001
[2019-04-04 03:05:13,258] A3C_AGENT_WORKER-Thread-6 INFO:Local step 219000, global step 3505050: loss 4.3993
[2019-04-04 03:05:13,258] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 219000, global step 3505050: learning rate 0.0001
[2019-04-04 03:05:14,931] A3C_AGENT_WORKER-Thread-10 INFO:Local step 219000, global step 3505475: loss 4.1748
[2019-04-04 03:05:14,931] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 219000, global step 3505475: learning rate 0.0001
[2019-04-04 03:05:15,097] A3C_AGENT_WORKER-Thread-18 INFO:Local step 219000, global step 3505517: loss 4.2937
[2019-04-04 03:05:15,097] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 219000, global step 3505517: learning rate 0.0001
[2019-04-04 03:05:15,379] A3C_AGENT_WORKER-Thread-7 INFO:Local step 219500, global step 3505595: loss 0.0335
[2019-04-04 03:05:15,379] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 219500, global step 3505595: learning rate 0.0001
[2019-04-04 03:05:15,538] A3C_AGENT_WORKER-Thread-9 INFO:Local step 219000, global step 3505638: loss 4.3831
[2019-04-04 03:05:15,541] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 219000, global step 3505638: learning rate 0.0001
[2019-04-04 03:05:16,854] A3C_AGENT_WORKER-Thread-8 INFO:Local step 219000, global step 3506020: loss 4.9442
[2019-04-04 03:05:16,856] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 219000, global step 3506020: learning rate 0.0001
[2019-04-04 03:05:18,498] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.9162473e-28 4.8143894e-23 2.2697590e-26 6.0482076e-24 1.1909769e-23
 4.8231454e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:05:18,498] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6619
[2019-04-04 03:05:18,521] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.83333333333333, 51.33333333333334, 0.0, 0.0, 26.0, 27.823390163276, 1.002583318206398, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4389000.0000, 
sim time next is 4389600.0000, 
raw observation next is [11.66666666666667, 52.66666666666667, 0.0, 0.0, 26.0, 27.72165536456245, 0.9822995867296952, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.785780240073869, 0.5266666666666667, 0.0, 0.0, 0.6666666666666666, 0.8101379470468709, 0.8274331955765651, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.62508774], dtype=float32), 0.03193223]. 
=============================================
[2019-04-04 03:05:20,150] A3C_AGENT_WORKER-Thread-4 INFO:Local step 219000, global step 3506957: loss 4.8736
[2019-04-04 03:05:20,151] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 219000, global step 3506957: learning rate 0.0001
[2019-04-04 03:05:22,727] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.7902634e-27 1.2438221e-23 2.0307916e-26 3.1454282e-23 9.8421458e-24
 5.2721311e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:05:22,727] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1355
[2019-04-04 03:05:22,738] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 20.0, 96.5, 753.0, 26.0, 26.72075497273509, 0.7203018289818702, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4028400.0000, 
sim time next is 4029000.0000, 
raw observation next is [-1.833333333333333, 20.33333333333334, 94.0, 739.3333333333334, 26.0, 26.88919388687148, 0.7483360577144825, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.41181902123730385, 0.2033333333333334, 0.31333333333333335, 0.8169429097605894, 0.6666666666666666, 0.7407661572392902, 0.7494453525714942, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2307888], dtype=float32), 1.729794]. 
=============================================
[2019-04-04 03:05:22,819] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[85.3341  ]
 [85.3998  ]
 [85.542946]
 [85.93839 ]
 [86.2289  ]], R is [[85.49889374]
 [85.64390564]
 [85.78746796]
 [85.92959595]
 [86.07029724]].
[2019-04-04 03:05:27,199] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.3973974e-27 2.3521545e-22 3.8039597e-26 1.0047488e-23 6.4954168e-24
 3.9989116e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:05:27,199] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0018
[2019-04-04 03:05:27,249] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.666666666666667, 35.33333333333334, 76.66666666666667, 390.8333333333334, 26.0, 25.43336985185105, 0.3753372242767958, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4090800.0000, 
sim time next is 4091400.0000, 
raw observation next is [-3.5, 36.0, 92.0, 469.0, 26.0, 25.57954318165984, 0.4212018277597731, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.36565096952908593, 0.36, 0.30666666666666664, 0.518232044198895, 0.6666666666666666, 0.6316285984716533, 0.6404006092532577, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7930523], dtype=float32), 0.80194354]. 
=============================================
[2019-04-04 03:05:27,495] A3C_AGENT_WORKER-Thread-20 INFO:Local step 219000, global step 3509025: loss 5.0012
[2019-04-04 03:05:27,495] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 219000, global step 3509025: learning rate 0.0001
[2019-04-04 03:05:31,223] A3C_AGENT_WORKER-Thread-19 INFO:Local step 220000, global step 3510165: loss 1.7013
[2019-04-04 03:05:31,226] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 220000, global step 3510165: learning rate 0.0001
[2019-04-04 03:05:31,751] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0007042e-26 2.2292370e-21 1.3365858e-24 1.2426513e-22 1.7256783e-22
 1.7455179e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 03:05:31,751] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2840
[2019-04-04 03:05:31,811] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 41.0, 0.0, 0.0, 26.0, 25.37455134778207, 0.4340385280483436, 0.0, 1.0, 54887.67942544416], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4148400.0000, 
sim time next is 4149000.0000, 
raw observation next is [-1.0, 40.5, 0.0, 0.0, 26.0, 25.37168205379473, 0.4364729069500992, 0.0, 1.0, 46980.92900460962], 
processed observation next is [0.0, 0.0, 0.4349030470914128, 0.405, 0.0, 0.0, 0.6666666666666666, 0.6143068378162274, 0.6454909689833664, 0.0, 1.0, 0.2237187095457601], 
reward next is 0.7763, 
noisyNet noise sample is [array([1.4513278], dtype=float32), -0.11499101]. 
=============================================
[2019-04-04 03:05:31,819] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[81.93051 ]
 [81.88038 ]
 [81.855225]
 [82.02734 ]
 [82.13512 ]], R is [[81.94664001]
 [81.86580658]
 [81.7579422 ]
 [81.67183685]
 [81.65045929]].
[2019-04-04 03:05:32,837] A3C_AGENT_WORKER-Thread-17 INFO:Local step 220000, global step 3510646: loss 1.8142
[2019-04-04 03:05:32,839] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 220000, global step 3510647: learning rate 0.0001
[2019-04-04 03:05:34,107] A3C_AGENT_WORKER-Thread-16 INFO:Local step 219000, global step 3511078: loss 5.6657
[2019-04-04 03:05:34,108] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 219000, global step 3511079: learning rate 0.0001
[2019-04-04 03:05:35,992] A3C_AGENT_WORKER-Thread-5 INFO:Local step 219500, global step 3511579: loss 0.0355
[2019-04-04 03:05:35,993] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 219500, global step 3511579: learning rate 0.0001
[2019-04-04 03:05:36,299] A3C_AGENT_WORKER-Thread-14 INFO:Local step 219500, global step 3511672: loss 0.0319
[2019-04-04 03:05:36,299] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 219500, global step 3511672: learning rate 0.0001
[2019-04-04 03:05:37,405] A3C_AGENT_WORKER-Thread-15 INFO:Local step 219500, global step 3512009: loss 0.0345
[2019-04-04 03:05:37,405] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 219500, global step 3512009: learning rate 0.0001
[2019-04-04 03:05:39,540] A3C_AGENT_WORKER-Thread-3 INFO:Local step 219500, global step 3512664: loss 0.0443
[2019-04-04 03:05:39,565] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 219500, global step 3512672: learning rate 0.0001
[2019-04-04 03:05:41,149] A3C_AGENT_WORKER-Thread-7 INFO:Local step 220000, global step 3513162: loss 1.8006
[2019-04-04 03:05:41,153] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 220000, global step 3513165: learning rate 0.0001
[2019-04-04 03:05:41,234] A3C_AGENT_WORKER-Thread-2 INFO:Local step 219500, global step 3513201: loss 0.0273
[2019-04-04 03:05:41,234] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 219500, global step 3513201: learning rate 0.0001
[2019-04-04 03:05:41,697] A3C_AGENT_WORKER-Thread-6 INFO:Local step 219500, global step 3513350: loss 0.0326
[2019-04-04 03:05:41,698] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 219500, global step 3513350: learning rate 0.0001
[2019-04-04 03:05:42,549] A3C_AGENT_WORKER-Thread-10 INFO:Local step 219500, global step 3513648: loss 0.0204
[2019-04-04 03:05:42,551] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 219500, global step 3513648: learning rate 0.0001
[2019-04-04 03:05:43,218] A3C_AGENT_WORKER-Thread-9 INFO:Local step 219500, global step 3513847: loss 0.0306
[2019-04-04 03:05:43,224] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 219500, global step 3513847: learning rate 0.0001
[2019-04-04 03:05:43,314] A3C_AGENT_WORKER-Thread-18 INFO:Local step 219500, global step 3513872: loss 0.0223
[2019-04-04 03:05:43,316] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 219500, global step 3513873: learning rate 0.0001
[2019-04-04 03:05:44,553] A3C_AGENT_WORKER-Thread-8 INFO:Local step 219500, global step 3514230: loss 0.0176
[2019-04-04 03:05:44,557] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 219500, global step 3514230: learning rate 0.0001
[2019-04-04 03:05:47,591] A3C_AGENT_WORKER-Thread-4 INFO:Local step 219500, global step 3515188: loss 0.0252
[2019-04-04 03:05:47,593] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 219500, global step 3515188: learning rate 0.0001
[2019-04-04 03:05:49,698] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.1645873e-27 1.3087180e-20 8.8741478e-25 7.3913092e-23 8.1584373e-22
 4.0655611e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:05:49,701] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2850
[2019-04-04 03:05:49,712] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 80.0, 0.0, 0.0, 26.0, 25.53709054805183, 0.5071329220740289, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4432800.0000, 
sim time next is 4433400.0000, 
raw observation next is [2.0, 80.0, 0.0, 0.0, 26.0, 25.57521569673165, 0.5048154243686821, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.518005540166205, 0.8, 0.0, 0.0, 0.6666666666666666, 0.6312679747276375, 0.6682718081228941, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.2481933], dtype=float32), -1.001848]. 
=============================================
[2019-04-04 03:05:51,636] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [8.4400840e-29 1.0428860e-23 8.6758108e-28 3.2143360e-24 4.8822971e-25
 1.3363733e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 03:05:51,637] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.0852
[2019-04-04 03:05:51,651] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.6, 29.0, 116.5, 847.5, 26.0, 26.80021988443928, 0.8912619638857318, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4366800.0000, 
sim time next is 4367400.0000, 
raw observation next is [14.58333333333333, 29.33333333333334, 116.0, 845.6666666666666, 26.0, 27.21944034451863, 0.9306769162843377, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.8665743305632503, 0.2933333333333334, 0.38666666666666666, 0.9344383057090239, 0.6666666666666666, 0.7682866953765526, 0.8102256387614459, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6916299], dtype=float32), -0.56079453]. 
=============================================
[2019-04-04 03:05:51,869] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [6.3709101e-29 2.0074415e-23 4.5552603e-27 1.0817356e-24 1.3907839e-24
 1.9573768e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:05:51,869] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4637
[2019-04-04 03:05:51,885] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 92.0, 192.5, 5.0, 26.0, 26.43329049910738, 0.5862933539945291, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4702800.0000, 
sim time next is 4703400.0000, 
raw observation next is [0.0, 92.0, 208.0, 6.0, 26.0, 26.44882807218339, 0.5934658210484952, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.46260387811634357, 0.92, 0.6933333333333334, 0.0066298342541436465, 0.6666666666666666, 0.7040690060152824, 0.6978219403494984, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5898856], dtype=float32), -1.3065689]. 
=============================================
[2019-04-04 03:05:52,550] A3C_AGENT_WORKER-Thread-20 INFO:Local step 219500, global step 3517526: loss 0.0386
[2019-04-04 03:05:52,554] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 219500, global step 3517528: learning rate 0.0001
[2019-04-04 03:05:53,443] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.3354214e-27 3.5847039e-23 4.2941212e-26 1.4436571e-23 1.8387198e-23
 9.1889652e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:05:53,444] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9703
[2019-04-04 03:05:53,457] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 78.0, 56.33333333333333, 0.0, 26.0, 26.30516934017589, 0.6084701117733079, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4464600.0000, 
sim time next is 4465200.0000, 
raw observation next is [0.0, 78.0, 52.66666666666666, 0.0, 26.0, 26.28517520741781, 0.6023588582923595, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.46260387811634357, 0.78, 0.17555555555555552, 0.0, 0.6666666666666666, 0.6904312672848176, 0.7007862860974532, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.33914426], dtype=float32), 0.1319239]. 
=============================================
[2019-04-04 03:05:53,963] A3C_AGENT_WORKER-Thread-19 INFO:Local step 220500, global step 3518233: loss 0.1119
[2019-04-04 03:05:53,964] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 220500, global step 3518234: learning rate 0.0001
[2019-04-04 03:05:54,270] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.0861625e-27 1.1014795e-20 6.9647346e-25 2.5818880e-22 2.3392513e-22
 8.5832516e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:05:54,270] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5303
[2019-04-04 03:05:54,302] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.5, 53.0, 0.0, 0.0, 26.0, 25.40866433839633, 0.4061593716624812, 0.0, 1.0, 46260.44964173336], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4829400.0000, 
sim time next is 4830000.0000, 
raw observation next is [-0.6666666666666666, 53.66666666666667, 0.0, 0.0, 26.0, 25.40624453792563, 0.4036898645271211, 0.0, 1.0, 41882.15027090741], 
processed observation next is [0.0, 0.9130434782608695, 0.44413665743305636, 0.5366666666666667, 0.0, 0.0, 0.6666666666666666, 0.6171870448271358, 0.634563288175707, 0.0, 1.0, 0.19943881081384482], 
reward next is 0.8006, 
noisyNet noise sample is [array([0.17601775], dtype=float32), 0.10480928]. 
=============================================
[2019-04-04 03:05:54,325] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[85.21856]
 [85.32358]
 [85.26069]
 [85.22121]
 [85.18059]], R is [[85.02238464]
 [84.95187378]
 [84.8072052 ]
 [84.66161346]
 [84.5703125 ]].
[2019-04-04 03:05:54,407] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [6.4666780e-27 1.3522347e-21 5.9441552e-25 8.1190662e-23 1.7238493e-22
 1.9058536e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:05:54,409] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.9183
[2019-04-04 03:05:54,429] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.666666666666667, 72.0, 0.0, 0.0, 26.0, 25.52828761628171, 0.516471499148414, 0.0, 1.0, 50350.55355044808], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4429200.0000, 
sim time next is 4429800.0000, 
raw observation next is [2.5, 74.0, 0.0, 0.0, 26.0, 25.50549092837258, 0.5215598352460594, 0.0, 1.0, 52245.96490971497], 
processed observation next is [1.0, 0.2608695652173913, 0.5318559556786704, 0.74, 0.0, 0.0, 0.6666666666666666, 0.6254575773643817, 0.6738532784153531, 0.0, 1.0, 0.2487903090938808], 
reward next is 0.7512, 
noisyNet noise sample is [array([-0.32166848], dtype=float32), 0.8704713]. 
=============================================
[2019-04-04 03:05:55,935] A3C_AGENT_WORKER-Thread-17 INFO:Local step 220500, global step 3519146: loss 0.0960
[2019-04-04 03:05:55,937] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 220500, global step 3519146: learning rate 0.0001
[2019-04-04 03:05:56,818] A3C_AGENT_WORKER-Thread-5 INFO:Local step 220000, global step 3519566: loss 2.0019
[2019-04-04 03:05:56,829] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 220000, global step 3519569: learning rate 0.0001
[2019-04-04 03:05:56,853] A3C_AGENT_WORKER-Thread-14 INFO:Local step 220000, global step 3519575: loss 1.9329
[2019-04-04 03:05:56,856] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 220000, global step 3519575: learning rate 0.0001
[2019-04-04 03:05:56,865] A3C_AGENT_WORKER-Thread-16 INFO:Local step 219500, global step 3519583: loss 0.0221
[2019-04-04 03:05:56,869] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 219500, global step 3519583: learning rate 0.0001
[2019-04-04 03:05:58,032] A3C_AGENT_WORKER-Thread-15 INFO:Local step 220000, global step 3520074: loss 1.9852
[2019-04-04 03:05:58,034] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 220000, global step 3520074: learning rate 0.0001
[2019-04-04 03:05:58,872] A3C_AGENT_WORKER-Thread-3 INFO:Local step 220000, global step 3520514: loss 2.0489
[2019-04-04 03:05:58,874] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 220000, global step 3520514: learning rate 0.0001
[2019-04-04 03:05:59,835] A3C_AGENT_WORKER-Thread-2 INFO:Local step 220000, global step 3520920: loss 1.8873
[2019-04-04 03:05:59,836] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 220000, global step 3520920: learning rate 0.0001
[2019-04-04 03:06:00,512] A3C_AGENT_WORKER-Thread-6 INFO:Local step 220000, global step 3521250: loss 1.9565
[2019-04-04 03:06:00,514] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 220000, global step 3521250: learning rate 0.0001
[2019-04-04 03:06:01,235] A3C_AGENT_WORKER-Thread-7 INFO:Local step 220500, global step 3521634: loss 0.1213
[2019-04-04 03:06:01,237] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 220500, global step 3521635: learning rate 0.0001
[2019-04-04 03:06:01,406] A3C_AGENT_WORKER-Thread-18 INFO:Local step 220000, global step 3521722: loss 2.0858
[2019-04-04 03:06:01,407] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 220000, global step 3521722: learning rate 0.0001
[2019-04-04 03:06:01,648] A3C_AGENT_WORKER-Thread-10 INFO:Local step 220000, global step 3521839: loss 2.0032
[2019-04-04 03:06:01,649] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 220000, global step 3521839: learning rate 0.0001
[2019-04-04 03:06:01,764] A3C_AGENT_WORKER-Thread-9 INFO:Local step 220000, global step 3521881: loss 2.0247
[2019-04-04 03:06:01,764] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 220000, global step 3521881: learning rate 0.0001
[2019-04-04 03:06:02,084] A3C_AGENT_WORKER-Thread-8 INFO:Local step 220000, global step 3522031: loss 1.8502
[2019-04-04 03:06:02,085] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 220000, global step 3522031: learning rate 0.0001
[2019-04-04 03:06:03,484] A3C_AGENT_WORKER-Thread-4 INFO:Local step 220000, global step 3522746: loss 2.0259
[2019-04-04 03:06:03,484] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 220000, global step 3522746: learning rate 0.0001
[2019-04-04 03:06:08,034] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:06:08,037] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:06:08,056] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run27
[2019-04-04 03:06:08,082] A3C_AGENT_WORKER-Thread-20 INFO:Local step 220000, global step 3524952: loss 2.1948
[2019-04-04 03:06:08,085] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 220000, global step 3524952: learning rate 0.0001
[2019-04-04 03:06:10,065] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0695846e-26 2.6266232e-21 1.5202284e-24 2.2914084e-22 8.9254556e-23
 3.9764913e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:06:10,065] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8605
[2019-04-04 03:06:10,073] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:06:10,073] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:06:10,077] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run27
[2019-04-04 03:06:10,107] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.833333333333333, 70.33333333333333, 0.0, 0.0, 26.0, 25.29566456900705, 0.3971516571523019, 0.0, 1.0, 37655.96869173537], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4596000.0000, 
sim time next is 4596600.0000, 
raw observation next is [-1.916666666666667, 70.66666666666667, 0.0, 0.0, 26.0, 25.28316720921138, 0.3959487888137015, 0.0, 1.0, 36614.00606938427], 
processed observation next is [1.0, 0.17391304347826086, 0.4095106186518929, 0.7066666666666667, 0.0, 0.0, 0.6666666666666666, 0.6069306007676151, 0.6319829296045671, 0.0, 1.0, 0.1743524098542108], 
reward next is 0.8256, 
noisyNet noise sample is [array([0.09229172], dtype=float32), -1.3190178]. 
=============================================
[2019-04-04 03:06:13,090] A3C_AGENT_WORKER-Thread-16 INFO:Local step 220000, global step 3526775: loss 2.2885
[2019-04-04 03:06:13,091] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 220000, global step 3526775: learning rate 0.0001
[2019-04-04 03:06:14,429] A3C_AGENT_WORKER-Thread-14 INFO:Local step 220500, global step 3527311: loss 0.0853
[2019-04-04 03:06:14,431] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 220500, global step 3527311: learning rate 0.0001
[2019-04-04 03:06:14,896] A3C_AGENT_WORKER-Thread-5 INFO:Local step 220500, global step 3527502: loss 0.1233
[2019-04-04 03:06:14,898] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 220500, global step 3527502: learning rate 0.0001
[2019-04-04 03:06:15,743] A3C_AGENT_WORKER-Thread-15 INFO:Local step 220500, global step 3527799: loss 0.0963
[2019-04-04 03:06:15,745] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 220500, global step 3527800: learning rate 0.0001
[2019-04-04 03:06:16,438] A3C_AGENT_WORKER-Thread-3 INFO:Local step 220500, global step 3528034: loss 0.0970
[2019-04-04 03:06:16,440] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 220500, global step 3528034: learning rate 0.0001
[2019-04-04 03:06:16,585] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.0890955e-26 6.1685863e-22 4.7304961e-25 8.2801163e-23 5.5014364e-23
 4.4127714e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:06:16,586] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9692
[2019-04-04 03:06:16,638] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0666666666666666, 50.33333333333334, 285.8333333333333, 284.0, 26.0, 24.99449074311573, 0.3450167918472194, 0.0, 1.0, 18716.84457968229], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4879200.0000, 
sim time next is 4879800.0000, 
raw observation next is [0.3, 49.5, 283.0, 308.0, 26.0, 25.02244627031235, 0.3505963730420363, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.47091412742382277, 0.495, 0.9433333333333334, 0.34033149171270716, 0.6666666666666666, 0.5852038558593625, 0.6168654576806788, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.46893737], dtype=float32), -0.85122854]. 
=============================================
[2019-04-04 03:06:16,680] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:06:16,681] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:06:16,705] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run27
[2019-04-04 03:06:17,298] A3C_AGENT_WORKER-Thread-2 INFO:Local step 220500, global step 3528383: loss 0.0832
[2019-04-04 03:06:17,299] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 220500, global step 3528383: learning rate 0.0001
[2019-04-04 03:06:18,482] A3C_AGENT_WORKER-Thread-6 INFO:Local step 220500, global step 3528864: loss 0.0860
[2019-04-04 03:06:18,483] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 220500, global step 3528864: learning rate 0.0001
[2019-04-04 03:06:18,792] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.9573806e-25 1.2490215e-19 4.1547758e-24 8.6887871e-22 2.5578812e-21
 2.7766541e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 03:06:18,792] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4016
[2019-04-04 03:06:18,838] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 47.0, 0.0, 0.0, 26.0, 24.97307284129002, 0.2913881259848979, 0.0, 1.0, 31997.31200273814], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4906800.0000, 
sim time next is 4907400.0000, 
raw observation next is [1.0, 45.83333333333334, 0.0, 0.0, 26.0, 24.97247002444077, 0.3079689306922782, 0.0, 1.0, 29035.4195853215], 
processed observation next is [0.0, 0.8260869565217391, 0.4903047091412743, 0.4583333333333334, 0.0, 0.0, 0.6666666666666666, 0.5810391687033976, 0.6026563102307594, 0.0, 1.0, 0.13826390278724524], 
reward next is 0.8617, 
noisyNet noise sample is [array([-1.1644046], dtype=float32), -0.060753528]. 
=============================================
[2019-04-04 03:06:19,100] A3C_AGENT_WORKER-Thread-18 INFO:Local step 220500, global step 3529107: loss 0.1182
[2019-04-04 03:06:19,102] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 220500, global step 3529107: learning rate 0.0001
[2019-04-04 03:06:19,541] A3C_AGENT_WORKER-Thread-9 INFO:Local step 220500, global step 3529287: loss 0.1195
[2019-04-04 03:06:19,542] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 220500, global step 3529287: learning rate 0.0001
[2019-04-04 03:06:19,718] A3C_AGENT_WORKER-Thread-10 INFO:Local step 220500, global step 3529361: loss 0.1079
[2019-04-04 03:06:19,720] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 220500, global step 3529361: learning rate 0.0001
[2019-04-04 03:06:19,918] A3C_AGENT_WORKER-Thread-8 INFO:Local step 220500, global step 3529439: loss 0.0850
[2019-04-04 03:06:19,919] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 220500, global step 3529440: learning rate 0.0001
[2019-04-04 03:06:21,072] A3C_AGENT_WORKER-Thread-4 INFO:Local step 220500, global step 3529797: loss 0.0770
[2019-04-04 03:06:21,075] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 220500, global step 3529798: learning rate 0.0001
[2019-04-04 03:06:26,788] A3C_AGENT_WORKER-Thread-20 INFO:Local step 220500, global step 3532220: loss 0.1009
[2019-04-04 03:06:26,789] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 220500, global step 3532220: learning rate 0.0001
[2019-04-04 03:06:27,460] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.0103381e-27 8.2732336e-23 1.4133156e-25 7.0042808e-23 9.8638000e-23
 1.0220965e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:06:27,462] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2185
[2019-04-04 03:06:27,471] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.33333333333333, 17.0, 30.0, 243.3333333333333, 26.0, 28.47260883878934, 1.134974158928372, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5074800.0000, 
sim time next is 5075400.0000, 
raw observation next is [11.16666666666667, 17.0, 24.0, 194.6666666666667, 26.0, 28.50684658548913, 1.121711318949016, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.7719298245614037, 0.17, 0.08, 0.21510128913443835, 0.6666666666666666, 0.8755705487907607, 0.8739037729830054, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4316665], dtype=float32), -1.193718]. 
=============================================
[2019-04-04 03:06:29,439] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:06:29,439] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:06:29,453] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run27
[2019-04-04 03:06:29,944] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:06:29,945] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:06:29,964] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run27
[2019-04-04 03:06:30,321] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:06:30,321] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:06:30,325] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run27
[2019-04-04 03:06:31,085] A3C_AGENT_WORKER-Thread-16 INFO:Local step 220500, global step 3534039: loss 0.0704
[2019-04-04 03:06:31,099] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 220500, global step 3534039: learning rate 0.0001
[2019-04-04 03:06:31,801] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:06:31,801] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:06:31,805] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run27
[2019-04-04 03:06:32,042] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:06:32,042] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:06:32,045] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run27
[2019-04-04 03:06:32,691] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.5744471e-28 2.1264610e-23 1.4680683e-26 1.7688792e-24 3.6692350e-24
 5.7229966e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:06:32,691] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0335
[2019-04-04 03:06:32,755] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.166666666666667, 40.16666666666666, 98.0, 612.3333333333333, 26.0, 25.52268391453682, 0.3810636058209161, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4956600.0000, 
sim time next is 4957200.0000, 
raw observation next is [-1.0, 39.0, 100.5, 638.5, 26.0, 25.70443401686995, 0.4102789829749721, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4349030470914128, 0.39, 0.335, 0.7055248618784531, 0.6666666666666666, 0.6420361680724959, 0.6367596609916574, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.65480363], dtype=float32), -0.6558147]. 
=============================================
[2019-04-04 03:06:33,079] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.8140529e-27 9.1732634e-23 2.3461110e-25 2.9257166e-23 4.2819414e-23
 8.0404811e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:06:33,087] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0978
[2019-04-04 03:06:33,098] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.333333333333334, 19.0, 0.0, 0.0, 26.0, 27.20477274652416, 0.8799240333733672, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5085600.0000, 
sim time next is 5086200.0000, 
raw observation next is [9.166666666666666, 19.0, 0.0, 0.0, 26.0, 27.1795677232633, 0.8681626140043132, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.7165281625115422, 0.19, 0.0, 0.0, 0.6666666666666666, 0.7649639769386084, 0.7893875380014377, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8198875], dtype=float32), 0.72052675]. 
=============================================
[2019-04-04 03:06:33,253] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:06:33,253] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:06:33,256] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run27
[2019-04-04 03:06:34,205] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:06:34,205] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:06:34,209] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run27
[2019-04-04 03:06:34,777] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:06:34,777] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:06:34,780] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run27
[2019-04-04 03:06:34,859] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:06:34,860] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:06:34,864] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run27
[2019-04-04 03:06:35,055] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:06:35,055] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:06:35,059] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run27
[2019-04-04 03:06:35,641] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:06:35,641] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:06:35,645] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run27
[2019-04-04 03:06:39,928] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.0856565e-26 8.6387544e-21 6.3907850e-25 6.8348531e-23 1.5438050e-22
 7.3202785e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:06:39,929] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0526
[2019-04-04 03:06:40,034] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.7, 93.0, 45.33333333333333, 0.0, 26.0, 23.38419836032465, -0.1002904867048895, 0.0, 1.0, 58165.86569307836], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 35400.0000, 
sim time next is 36000.0000, 
raw observation next is [7.7, 93.0, 49.0, 0.0, 26.0, 23.48396747548868, -0.07784156757973969, 0.0, 1.0, 57952.71270752177], 
processed observation next is [0.0, 0.43478260869565216, 0.6759002770083103, 0.93, 0.16333333333333333, 0.0, 0.6666666666666666, 0.4569972896240566, 0.47405281080675343, 0.0, 1.0, 0.27596529860724656], 
reward next is 0.7240, 
noisyNet noise sample is [array([-0.6210332], dtype=float32), 1.109494]. 
=============================================
[2019-04-04 03:06:40,037] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[86.54127 ]
 [86.60638 ]
 [86.64574 ]
 [86.687164]
 [86.648605]], R is [[86.32413483]
 [86.18391418]
 [86.04406738]
 [85.90454865]
 [85.76516724]].
[2019-04-04 03:06:42,910] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:06:42,910] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:06:42,913] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run27
[2019-04-04 03:06:46,065] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [4.77789785e-27 1.00742056e-22 4.89247638e-25 4.14409333e-23
 5.09926663e-23 4.50131390e-27 1.00000000e+00], sum to 1.0000
[2019-04-04 03:06:46,066] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8328
[2019-04-04 03:06:46,116] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 26.0, 25.25504657908633, 0.3128313555378108, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 241800.0000, 
sim time next is 242400.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 26.0, 25.35173128684273, 0.282129090733896, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.368421052631579, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6126442739035607, 0.594043030244632, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1417058], dtype=float32), -1.804892]. 
=============================================
[2019-04-04 03:06:49,961] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:06:49,961] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:06:49,967] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run27
[2019-04-04 03:06:53,818] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.6081350e-26 6.3307650e-22 1.7034002e-24 3.4076827e-22 2.9678379e-22
 6.3059988e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:06:53,818] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6762
[2019-04-04 03:06:53,854] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.4, 71.0, 0.0, 0.0, 26.0, 24.21224405446669, 0.1207716649491928, 0.0, 1.0, 45061.14219752914], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 167400.0000, 
sim time next is 168000.0000, 
raw observation next is [-8.4, 71.0, 0.0, 0.0, 26.0, 24.17549276087135, 0.1099824957054454, 0.0, 1.0, 45009.08139946006], 
processed observation next is [1.0, 0.9565217391304348, 0.2299168975069252, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5146243967392792, 0.5366608319018151, 0.0, 1.0, 0.21432895904504792], 
reward next is 0.7857, 
noisyNet noise sample is [array([-1.1157935], dtype=float32), 0.8637572]. 
=============================================
[2019-04-04 03:06:53,860] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[78.38278 ]
 [78.154526]
 [77.87493 ]
 [77.52534 ]
 [77.19457 ]], R is [[78.6342926 ]
 [78.63337708]
 [78.63226318]
 [78.63121796]
 [78.62923431]].
[2019-04-04 03:07:00,548] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [6.8808097e-25 4.3305716e-20 4.0633348e-23 2.1901430e-21 2.5114423e-21
 1.5100272e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 03:07:00,548] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.4340
[2019-04-04 03:07:00,579] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.64423461643237, -0.2686205423049726, 0.0, 1.0, 44963.31874660851], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 196200.0000, 
sim time next is 196800.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.59719105711297, -0.2739333097961998, 0.0, 1.0, 44975.45408805274], 
processed observation next is [1.0, 0.2608695652173913, 0.21606648199445982, 0.78, 0.0, 0.0, 0.6666666666666666, 0.3830992547594141, 0.40868889673460007, 0.0, 1.0, 0.21416882899072734], 
reward next is 0.7858, 
noisyNet noise sample is [array([1.1516346], dtype=float32), -0.67868197]. 
=============================================
[2019-04-04 03:07:09,810] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.7359427e-25 4.1603804e-21 5.2470618e-24 6.1103641e-22 7.3193443e-22
 2.6760129e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 03:07:09,810] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4959
[2019-04-04 03:07:09,863] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.9, 68.33333333333334, 0.0, 0.0, 26.0, 24.08343087015335, 0.0535814927209981, 0.0, 1.0, 45188.94413755413], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 264000.0000, 
sim time next is 264600.0000, 
raw observation next is [-7.0, 69.0, 0.0, 0.0, 26.0, 24.01987610252099, 0.04883353517894815, 0.0, 1.0, 45326.65466958711], 
processed observation next is [1.0, 0.043478260869565216, 0.2686980609418283, 0.69, 0.0, 0.0, 0.6666666666666666, 0.5016563418767491, 0.5162778450596494, 0.0, 1.0, 0.21584121271231957], 
reward next is 0.7842, 
noisyNet noise sample is [array([-1.7690768], dtype=float32), -0.4785437]. 
=============================================
[2019-04-04 03:07:29,838] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.8092006e-26 2.4878886e-22 1.0297177e-24 4.4525339e-22 1.9298244e-22
 4.3564890e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:07:29,839] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9212
[2019-04-04 03:07:29,888] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.8, 72.33333333333334, 0.0, 0.0, 26.0, 25.19447182006785, 0.3113625359440664, 0.0, 1.0, 18775.26655812132], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 332400.0000, 
sim time next is 333000.0000, 
raw observation next is [-12.8, 73.5, 0.0, 0.0, 26.0, 25.12614364416267, 0.2827094422879058, 0.0, 1.0, 24478.74597713857], 
processed observation next is [1.0, 0.8695652173913043, 0.1080332409972299, 0.735, 0.0, 0.0, 0.6666666666666666, 0.5938453036802226, 0.5942364807626352, 0.0, 1.0, 0.11656545703399318], 
reward next is 0.8834, 
noisyNet noise sample is [array([-1.9615371], dtype=float32), -0.4537249]. 
=============================================
[2019-04-04 03:07:29,906] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[76.43583 ]
 [76.71036 ]
 [77.08015 ]
 [77.64352 ]
 [77.145485]], R is [[76.64740753]
 [76.79152679]
 [76.74303436]
 [76.66963959]
 [76.52901459]].
[2019-04-04 03:07:40,259] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.9414658e-25 4.9205831e-20 1.3320005e-23 1.7950043e-21 2.5594232e-21
 2.1855243e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 03:07:40,261] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7744
[2019-04-04 03:07:40,294] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 71.5, 0.0, 0.0, 26.0, 23.89929049976794, 0.02500865469683334, 0.0, 1.0, 44399.16851028537], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 621000.0000, 
sim time next is 621600.0000, 
raw observation next is [-4.5, 70.33333333333333, 0.0, 0.0, 26.0, 23.86334312901314, 0.01831678605294676, 0.0, 1.0, 44443.80314036187], 
processed observation next is [0.0, 0.17391304347826086, 0.3379501385041552, 0.7033333333333333, 0.0, 0.0, 0.6666666666666666, 0.48861192741776155, 0.5061055953509822, 0.0, 1.0, 0.211637157811247], 
reward next is 0.7884, 
noisyNet noise sample is [array([-0.48782972], dtype=float32), 0.40640712]. 
=============================================
[2019-04-04 03:07:55,135] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2402483e-25 1.7828650e-20 1.3673747e-23 9.2424287e-22 1.6856331e-21
 5.6715194e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 03:07:55,135] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4477
[2019-04-04 03:07:55,163] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 76.83333333333333, 0.0, 0.0, 26.0, 24.18815061841411, 0.09002088038973842, 0.0, 1.0, 42681.1738801812], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 615000.0000, 
sim time next is 615600.0000, 
raw observation next is [-3.9, 75.0, 0.0, 0.0, 26.0, 24.15761277742961, 0.08688582191639815, 0.0, 1.0, 42838.92405955317], 
processed observation next is [0.0, 0.13043478260869565, 0.3545706371191136, 0.75, 0.0, 0.0, 0.6666666666666666, 0.5131343981191341, 0.5289619406387994, 0.0, 1.0, 0.2039948764740627], 
reward next is 0.7960, 
noisyNet noise sample is [array([1.5043429], dtype=float32), 0.751803]. 
=============================================
[2019-04-04 03:07:55,816] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.8751408e-28 1.8683676e-22 8.4568884e-26 6.1660863e-24 1.3480718e-23
 3.4132162e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:07:55,817] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6776
[2019-04-04 03:07:55,874] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.366666666666667, 75.0, 36.83333333333333, 0.0, 26.0, 25.75384755964628, 0.3308074057168242, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 808800.0000, 
sim time next is 809400.0000, 
raw observation next is [-6.283333333333333, 75.0, 41.66666666666666, 0.0, 26.0, 25.871694127301, 0.3152053168449394, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.288550323176362, 0.75, 0.13888888888888887, 0.0, 0.6666666666666666, 0.6559745106084168, 0.6050684389483131, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.80878633], dtype=float32), -0.30325603]. 
=============================================
[2019-04-04 03:08:00,690] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.2959259e-27 1.9618819e-23 9.3141886e-26 8.1052295e-24 1.1740656e-23
 6.7448339e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:08:00,690] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1375
[2019-04-04 03:08:00,741] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.899999999999999, 83.33333333333334, 45.66666666666667, 0.0, 26.0, 25.76830698133907, 0.4094998772813001, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 834000.0000, 
sim time next is 834600.0000, 
raw observation next is [-3.9, 82.66666666666667, 42.33333333333334, 0.0, 26.0, 25.9545985033458, 0.4248330232141264, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.3545706371191136, 0.8266666666666667, 0.14111111111111113, 0.0, 0.6666666666666666, 0.66288320861215, 0.6416110077380421, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.29926023], dtype=float32), 1.3635535]. 
=============================================
[2019-04-04 03:08:07,960] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.68950904e-28 1.10719444e-23 1.58551459e-26 2.14384942e-24
 1.38682112e-24 2.60228886e-28 1.00000000e+00], sum to 1.0000
[2019-04-04 03:08:07,961] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5706
[2019-04-04 03:08:07,982] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 26.0, 25.38783436414933, 0.4861293515624119, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1362000.0000, 
sim time next is 1362600.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 26.0, 25.25149059737872, 0.4487308788741926, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.4764542936288089, 0.96, 0.0, 0.0, 0.6666666666666666, 0.6042908831148933, 0.6495769596247308, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.57574934], dtype=float32), 2.6402595]. 
=============================================
[2019-04-04 03:08:14,144] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.0693870e-31 2.7372882e-26 6.8839432e-30 1.4956641e-27 9.0956751e-28
 1.8087423e-31 1.0000000e+00], sum to 1.0000
[2019-04-04 03:08:14,145] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1278
[2019-04-04 03:08:14,150] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [19.4, 49.0, 82.5, 0.0, 26.0, 26.72357767076501, 0.9032164576194494, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1093200.0000, 
sim time next is 1093800.0000, 
raw observation next is [19.4, 49.0, 73.0, 0.0, 26.0, 27.21418040627201, 0.945011141919245, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 1.0, 0.49, 0.24333333333333335, 0.0, 0.6666666666666666, 0.7678483671893342, 0.8150037139730816, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5954921], dtype=float32), -1.6547931]. 
=============================================
[2019-04-04 03:08:15,799] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.3144329e-30 6.8598694e-25 7.9423820e-28 3.9422812e-26 7.4425939e-26
 2.4865041e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 03:08:15,803] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6269
[2019-04-04 03:08:15,811] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.2, 83.0, 35.0, 96.5, 26.0, 26.32289598716714, 0.712696376152601, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1068000.0000, 
sim time next is 1068600.0000, 
raw observation next is [12.2, 83.0, 48.0, 124.0, 26.0, 26.49777368020742, 0.7235465981185861, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.8005540166204987, 0.83, 0.16, 0.13701657458563535, 0.6666666666666666, 0.7081478066839516, 0.741182199372862, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9823083], dtype=float32), 1.0670514]. 
=============================================
[2019-04-04 03:08:18,991] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.2314614e-30 8.4300625e-25 6.2507395e-28 1.8508501e-25 7.4891589e-26
 4.1022564e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 03:08:18,993] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8702
[2019-04-04 03:08:18,999] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 81.0, 0.0, 0.0, 26.0, 26.09443237362697, 0.5721203169892283, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1015200.0000, 
sim time next is 1015800.0000, 
raw observation next is [14.4, 81.0, 0.0, 0.0, 26.0, 25.99844262222411, 0.5712475465055048, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.8614958448753465, 0.81, 0.0, 0.0, 0.6666666666666666, 0.6665368851853426, 0.6904158488351683, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.36006108], dtype=float32), 1.0485178]. 
=============================================
[2019-04-04 03:08:21,192] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.6874111e-29 5.0671941e-23 8.0929781e-28 4.4354028e-26 1.3472376e-25
 7.9971930e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 03:08:21,193] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3765
[2019-04-04 03:08:21,199] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.0, 96.0, 0.0, 0.0, 26.0, 23.64492638944709, 0.179104416639716, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1228200.0000, 
sim time next is 1228800.0000, 
raw observation next is [15.0, 96.0, 0.0, 0.0, 26.0, 23.62993683615814, 0.1755688251008624, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.21739130434782608, 0.8781163434903049, 0.96, 0.0, 0.0, 0.6666666666666666, 0.4691614030131784, 0.5585229417002875, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.54902345], dtype=float32), 1.7051737]. 
=============================================
[2019-04-04 03:08:21,478] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.7324253e-30 1.1533165e-25 1.1309386e-28 4.5631758e-26 4.0710239e-26
 1.2547499e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 03:08:21,479] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4770
[2019-04-04 03:08:21,487] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.86666666666667, 58.33333333333334, 204.8333333333333, 228.1666666666667, 26.0, 27.06612946355922, 0.8197340143727088, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1597200.0000, 
sim time next is 1597800.0000, 
raw observation next is [11.23333333333333, 57.66666666666666, 193.6666666666667, 207.3333333333333, 26.0, 27.13447795594472, 0.8305902077413178, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.7737765466297323, 0.5766666666666665, 0.6455555555555557, 0.22909760589318595, 0.6666666666666666, 0.7612064963287267, 0.7768634025804393, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8331341], dtype=float32), 0.61524993]. 
=============================================
[2019-04-04 03:08:23,039] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.9817239e-28 2.2670367e-23 1.5671264e-26 2.3616644e-24 1.4252108e-24
 2.2754797e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:08:23,043] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6333
[2019-04-04 03:08:23,051] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.26666666666667, 100.0, 24.33333333333333, 0.0, 26.0, 24.607532485985, 0.4210896789086691, 0.0, 1.0, 24169.16340516739], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1268400.0000, 
sim time next is 1269000.0000, 
raw observation next is [13.0, 100.0, 19.0, 0.0, 26.0, 24.60079661398782, 0.4214553514662492, 0.0, 1.0, 26533.59884668281], 
processed observation next is [0.0, 0.6956521739130435, 0.8227146814404434, 1.0, 0.06333333333333334, 0.0, 0.6666666666666666, 0.5500663844989852, 0.6404851171554163, 0.0, 1.0, 0.12635047069848956], 
reward next is 0.8736, 
noisyNet noise sample is [array([0.31418395], dtype=float32), 2.2203588]. 
=============================================
[2019-04-04 03:08:23,060] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[89.67152]
 [89.79436]
 [89.85945]
 [89.84512]
 [89.841  ]], R is [[89.48132324]
 [89.47142029]
 [89.48296356]
 [89.49918365]
 [89.51524353]].
[2019-04-04 03:08:23,793] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.2030686e-28 3.4918983e-24 8.1848510e-27 2.5845361e-24 2.7203233e-24
 1.5051588e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:08:23,797] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4496
[2019-04-04 03:08:23,835] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 26.0, 25.02371666575545, 0.4430952439498727, 1.0, 1.0, 14032.82129157385], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1358400.0000, 
sim time next is 1359000.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 26.0, 25.22924065144755, 0.4475627637174532, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.4764542936288089, 0.96, 0.0, 0.0, 0.6666666666666666, 0.6024367209539626, 0.6491875879058178, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.46146715], dtype=float32), 1.160118]. 
=============================================
[2019-04-04 03:08:23,839] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[84.999374]
 [85.0358  ]
 [85.09572 ]
 [85.19231 ]
 [85.35594 ]], R is [[85.12918091]
 [85.2110672 ]
 [84.41930389]
 [83.96617126]
 [83.99304962]].
[2019-04-04 03:08:27,075] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.7082331e-29 3.1223420e-24 1.2076548e-27 2.3311769e-25 4.8482957e-25
 1.1006097e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:08:27,077] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3161
[2019-04-04 03:08:27,114] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 100.0, 18.0, 0.0, 26.0, 25.55988724041561, 0.4802937192224375, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1413000.0000, 
sim time next is 1413600.0000, 
raw observation next is [-0.6, 100.0, 22.66666666666666, 0.0, 26.0, 25.73793938760602, 0.4959952417330107, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.44598337950138506, 1.0, 0.07555555555555554, 0.0, 0.6666666666666666, 0.6448282823005016, 0.6653317472443369, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.361835], dtype=float32), 1.9233946]. 
=============================================
[2019-04-04 03:08:31,940] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.0545564e-27 6.9253461e-23 4.3133317e-25 4.0580368e-23 3.6979445e-23
 1.8301726e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:08:31,940] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3095
[2019-04-04 03:08:31,963] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.28999250876263, 0.4598468392895613, 0.0, 1.0, 38997.29849649187], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1398600.0000, 
sim time next is 1399200.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.29624496409398, 0.4611154365750187, 0.0, 1.0, 38790.77914731624], 
processed observation next is [1.0, 0.17391304347826086, 0.44598337950138506, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6080204136744983, 0.6537051455250062, 0.0, 1.0, 0.18471799593960117], 
reward next is 0.8153, 
noisyNet noise sample is [array([-0.44721568], dtype=float32), 2.335557]. 
=============================================
[2019-04-04 03:08:32,997] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.6725709e-28 2.5566493e-23 1.0059937e-26 3.0749666e-24 2.1411115e-24
 1.7662593e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:08:32,997] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1308
[2019-04-04 03:08:33,027] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 0.0, 0.0, 26.0, 25.53155896722158, 0.4953461041648859, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1450800.0000, 
sim time next is 1451400.0000, 
raw observation next is [1.1, 92.0, 0.0, 0.0, 26.0, 25.44074470147399, 0.4816385913224221, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.49307479224376743, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6200620584561657, 0.6605461971074741, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2388053], dtype=float32), 0.9312938]. 
=============================================
[2019-04-04 03:08:40,100] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.5726652e-27 4.2707364e-22 2.9067577e-25 1.5564229e-23 2.7358412e-23
 4.9529379e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:08:40,103] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5779
[2019-04-04 03:08:40,127] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.85, 83.0, 0.0, 0.0, 26.0, 25.6360581946529, 0.5183103272681671, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1575000.0000, 
sim time next is 1575600.0000, 
raw observation next is [4.9, 82.66666666666667, 0.0, 0.0, 26.0, 25.59827398656102, 0.5185689893844188, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.5983379501385043, 0.8266666666666667, 0.0, 0.0, 0.6666666666666666, 0.6331894988800849, 0.6728563297948064, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7293787], dtype=float32), 1.4092166]. 
=============================================
[2019-04-04 03:08:40,822] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.9719771e-29 2.2069046e-24 5.4381821e-27 1.7806184e-24 6.3762473e-25
 8.6486111e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 03:08:40,825] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2582
[2019-04-04 03:08:40,886] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 26.0, 24.9919376567396, 0.4351606810403794, 1.0, 1.0, 115350.6804688235], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1363200.0000, 
sim time next is 1363800.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 26.0, 24.82650109826463, 0.4746019686469252, 1.0, 1.0, 173374.3180651607], 
processed observation next is [1.0, 0.782608695652174, 0.4764542936288089, 0.96, 0.0, 0.0, 0.6666666666666666, 0.5688750915220524, 0.6582006562156417, 1.0, 1.0, 0.8255919907864796], 
reward next is 0.1744, 
noisyNet noise sample is [array([-0.1579745], dtype=float32), 0.48218426]. 
=============================================
[2019-04-04 03:08:46,950] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.9653616e-28 5.4866637e-22 2.9984790e-25 8.6150135e-24 5.3830852e-23
 6.9958820e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:08:46,950] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1209
[2019-04-04 03:08:46,972] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.716666666666667, 92.0, 0.0, 0.0, 26.0, 25.58343938406546, 0.534965568940391, 0.0, 1.0, 30748.8560770091], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1667400.0000, 
sim time next is 1668000.0000, 
raw observation next is [4.433333333333334, 92.0, 0.0, 0.0, 26.0, 25.57239642037791, 0.5275154249210822, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.5854108956602032, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6310330350314924, 0.675838474973694, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.30986392], dtype=float32), 2.0986495]. 
=============================================
[2019-04-04 03:08:46,980] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[83.92184 ]
 [84.05164 ]
 [84.160065]
 [84.29908 ]
 [84.47344 ]], R is [[85.16589355]
 [85.16780853]
 [85.18251038]
 [85.24147797]
 [85.29983521]].
[2019-04-04 03:08:48,523] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.0328472e-28 7.1458069e-23 7.9520612e-26 1.5717927e-23 4.9873191e-24
 2.4490404e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:08:48,523] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1069
[2019-04-04 03:08:48,568] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.85, 92.0, 53.0, 0.0, 26.0, 25.88313748051812, 0.5534475131535236, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1675800.0000, 
sim time next is 1676400.0000, 
raw observation next is [1.733333333333333, 92.0, 55.16666666666667, 0.0, 26.0, 25.92848307028386, 0.5577596359608527, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.5106186518928902, 0.92, 0.1838888888888889, 0.0, 0.6666666666666666, 0.660706922523655, 0.6859198786536176, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.03646865], dtype=float32), 0.41773215]. 
=============================================
[2019-04-04 03:08:53,134] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.9391907e-24 4.9112841e-19 4.6282577e-23 5.2570474e-21 3.0206885e-21
 6.4411822e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 03:08:53,135] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9827
[2019-04-04 03:08:53,148] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.066666666666666, 84.33333333333334, 0.0, 0.0, 26.0, 24.13478545455906, 0.1005023963919319, 0.0, 1.0, 46498.7296253738], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1822800.0000, 
sim time next is 1823400.0000, 
raw observation next is [-6.1, 85.0, 0.0, 0.0, 26.0, 24.10114040851075, 0.09202715828417778, 0.0, 1.0, 46559.27911294236], 
processed observation next is [0.0, 0.08695652173913043, 0.29362880886426596, 0.85, 0.0, 0.0, 0.6666666666666666, 0.5084283673758959, 0.5306757194280592, 0.0, 1.0, 0.22171085291877313], 
reward next is 0.7783, 
noisyNet noise sample is [array([1.1927589], dtype=float32), -0.59688693]. 
=============================================
[2019-04-04 03:08:58,946] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.3288075e-24 1.8461454e-19 5.9989685e-23 2.8511940e-21 2.8609887e-21
 1.0187121e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 03:08:58,948] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3398
[2019-04-04 03:08:59,004] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.2, 83.66666666666667, 0.0, 0.0, 26.0, 23.89940333407016, 0.04321750134236348, 0.0, 1.0, 46886.29313407088], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1828200.0000, 
sim time next is 1828800.0000, 
raw observation next is [-6.2, 83.0, 0.0, 0.0, 26.0, 23.85987545821026, 0.04062691469476109, 0.0, 1.0, 46907.03923495772], 
processed observation next is [0.0, 0.17391304347826086, 0.2908587257617729, 0.83, 0.0, 0.0, 0.6666666666666666, 0.48832295485085514, 0.5135423048982537, 0.0, 1.0, 0.22336685349979868], 
reward next is 0.7766, 
noisyNet noise sample is [array([-0.22900274], dtype=float32), -0.6324971]. 
=============================================
[2019-04-04 03:09:02,607] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [5.5256544e-27 2.1697764e-22 6.5440356e-25 4.3603031e-23 1.1691455e-22
 2.5732889e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:09:02,609] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3246
[2019-04-04 03:09:02,621] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.399999999999999, 85.33333333333334, 0.0, 0.0, 26.0, 25.03984600545368, 0.3172658479630137, 0.0, 1.0, 42170.21440662239], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2069400.0000, 
sim time next is 2070000.0000, 
raw observation next is [-4.5, 86.0, 0.0, 0.0, 26.0, 24.98085207279602, 0.306160140129588, 0.0, 1.0, 42287.44611559393], 
processed observation next is [1.0, 1.0, 0.3379501385041552, 0.86, 0.0, 0.0, 0.6666666666666666, 0.5817376727330016, 0.602053380043196, 0.0, 1.0, 0.20136879102663774], 
reward next is 0.7986, 
noisyNet noise sample is [array([-0.6456498], dtype=float32), 1.1605717]. 
=============================================
[2019-04-04 03:09:02,669] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[79.16669 ]
 [79.283134]
 [79.3117  ]
 [79.39516 ]
 [79.41056 ]], R is [[79.07131958]
 [79.07979584]
 [79.08877563]
 [79.09700775]
 [79.10305786]].
[2019-04-04 03:09:05,974] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.1974862e-25 7.5937533e-20 1.5315155e-23 1.7674990e-21 1.1083703e-21
 5.1410424e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 03:09:05,974] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4716
[2019-04-04 03:09:06,020] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.2, 85.0, 0.0, 0.0, 26.0, 23.95955959692031, 0.0611680790502343, 0.0, 1.0, 46830.46814311864], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1827000.0000, 
sim time next is 1827600.0000, 
raw observation next is [-6.199999999999999, 84.33333333333333, 0.0, 0.0, 26.0, 23.93667749345723, 0.05164380521602913, 0.0, 1.0, 46850.61109191907], 
processed observation next is [0.0, 0.13043478260869565, 0.2908587257617729, 0.8433333333333333, 0.0, 0.0, 0.6666666666666666, 0.494723124454769, 0.5172146017386764, 0.0, 1.0, 0.22309814805675748], 
reward next is 0.7769, 
noisyNet noise sample is [array([0.8452778], dtype=float32), -0.91742355]. 
=============================================
[2019-04-04 03:09:11,078] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.8486009e-25 4.0033958e-21 5.1552999e-24 5.4656484e-22 5.3588293e-22
 8.1584872e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 03:09:11,078] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9050
[2019-04-04 03:09:11,150] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.666666666666667, 84.0, 0.0, 0.0, 26.0, 25.03393117210714, 0.2448641321437929, 0.0, 1.0, 30835.66562559727], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1881600.0000, 
sim time next is 1882200.0000, 
raw observation next is [-4.583333333333333, 83.5, 0.0, 0.0, 26.0, 25.02853800540148, 0.2401585197338395, 0.0, 1.0, 37513.58137395676], 
processed observation next is [0.0, 0.782608695652174, 0.3356417359187443, 0.835, 0.0, 0.0, 0.6666666666666666, 0.5857115004501233, 0.5800528399112799, 0.0, 1.0, 0.1786361017807465], 
reward next is 0.8214, 
noisyNet noise sample is [array([-0.14033341], dtype=float32), -0.48543432]. 
=============================================
[2019-04-04 03:09:21,170] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.00689645e-25 1.07136837e-20 2.32696786e-23 1.92680213e-21
 1.94755232e-21 5.01878781e-25 1.00000000e+00], sum to 1.0000
[2019-04-04 03:09:21,171] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5053
[2019-04-04 03:09:21,198] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.7, 78.83333333333333, 0.0, 0.0, 26.0, 23.83248435741245, 0.02462201038530959, 0.0, 1.0, 43529.76299001243], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2098200.0000, 
sim time next is 2098800.0000, 
raw observation next is [-6.7, 78.0, 0.0, 0.0, 26.0, 23.79955480443573, 0.0190198850787707, 0.0, 1.0, 43439.39486182722], 
processed observation next is [1.0, 0.30434782608695654, 0.2770083102493075, 0.78, 0.0, 0.0, 0.6666666666666666, 0.4832962337029774, 0.5063399616929235, 0.0, 1.0, 0.2068542612467963], 
reward next is 0.7931, 
noisyNet noise sample is [array([0.40445617], dtype=float32), -0.7249664]. 
=============================================
[2019-04-04 03:09:35,710] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.7636338e-28 6.8170425e-23 4.0989641e-26 1.3518619e-23 9.4791546e-24
 1.5980772e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:09:35,710] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6790
[2019-04-04 03:09:35,741] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5333333333333334, 43.66666666666666, 123.8333333333333, 57.0, 26.0, 26.29197305884876, 0.4982064880142248, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2299200.0000, 
sim time next is 2299800.0000, 
raw observation next is [0.8166666666666668, 43.33333333333334, 126.6666666666667, 54.0, 26.0, 26.327266771749, 0.4961376734337582, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.4852262234533703, 0.4333333333333334, 0.42222222222222233, 0.05966850828729282, 0.6666666666666666, 0.6939388976457499, 0.6653792244779194, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.12537299], dtype=float32), -0.8248456]. 
=============================================
[2019-04-04 03:09:46,625] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.26768426e-27 9.42960071e-24 1.80501791e-26 1.07889256e-23
 5.65439167e-24 2.22514242e-28 1.00000000e+00], sum to 1.0000
[2019-04-04 03:09:46,626] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5355
[2019-04-04 03:09:46,688] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 49.0, 141.6666666666667, 192.3333333333333, 26.0, 26.04159091902245, 0.4766612274375565, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2648400.0000, 
sim time next is 2649000.0000, 
raw observation next is [0.5, 49.5, 128.3333333333333, 178.6666666666667, 26.0, 26.04084797719316, 0.4618623417278385, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.4764542936288089, 0.495, 0.42777777777777765, 0.19742173112338862, 0.6666666666666666, 0.6700706647660967, 0.6539541139092795, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4235428], dtype=float32), -0.8191449]. 
=============================================
[2019-04-04 03:09:46,709] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[83.24836 ]
 [83.71064 ]
 [83.826515]
 [83.84113 ]
 [83.76966 ]], R is [[82.90164185]
 [83.07262421]
 [83.24189758]
 [83.40947723]
 [83.57538605]].
[2019-04-04 03:09:50,950] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0688143e-25 5.5856049e-21 8.5070807e-24 1.0928196e-21 8.2846702e-22
 2.9611499e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 03:09:50,950] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6403
[2019-04-04 03:09:50,987] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.65, 89.0, 0.0, 0.0, 26.0, 23.97351515289611, 0.04354507667273209, 0.0, 1.0, 43538.50915612203], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2262600.0000, 
sim time next is 2263200.0000, 
raw observation next is [-8.733333333333334, 89.66666666666667, 0.0, 0.0, 26.0, 23.93692052610267, 0.03040749463099974, 0.0, 1.0, 43500.8386443672], 
processed observation next is [1.0, 0.17391304347826086, 0.22068328716528163, 0.8966666666666667, 0.0, 0.0, 0.6666666666666666, 0.4947433771752226, 0.5101358315436666, 0.0, 1.0, 0.20714685068746286], 
reward next is 0.7929, 
noisyNet noise sample is [array([1.5712736], dtype=float32), 0.60457325]. 
=============================================
[2019-04-04 03:09:51,225] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-04 03:09:51,226] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 03:09:51,226] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:09:51,228] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run37
[2019-04-04 03:09:51,253] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 03:09:51,256] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:09:51,258] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run37
[2019-04-04 03:09:51,300] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 03:09:51,300] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:09:51,302] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run37
[2019-04-04 03:12:36,340] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.35466477], dtype=float32), 0.18601839]
[2019-04-04 03:12:36,340] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-3.0, 55.0, 0.0, 0.0, 26.0, 25.54512288480654, 0.4952492178570551, 0.0, 1.0, 27162.38975319732]
[2019-04-04 03:12:36,341] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 03:12:36,342] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [8.9816954e-26 1.4344800e-21 1.6119719e-24 3.2078227e-22 4.3633833e-22
 7.2907382e-26 1.0000000e+00], sampled 0.4923946034669119
[2019-04-04 03:12:47,107] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.35466477], dtype=float32), 0.18601839]
[2019-04-04 03:12:47,108] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [7.6, 79.16666666666667, 0.0, 0.0, 26.0, 25.6543954985989, 0.5409541628955074, 0.0, 1.0, 18726.01698232809]
[2019-04-04 03:12:47,108] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 03:12:47,108] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [5.0597884e-26 4.1794209e-21 2.3482342e-24 2.0808429e-22 4.3566640e-22
 2.5984341e-25 1.0000000e+00], sampled 0.8060248657465262
[2019-04-04 03:13:01,423] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 03:13:25,413] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.35466477], dtype=float32), 0.18601839]
[2019-04-04 03:13:25,413] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [3.0, 45.0, 112.1666666666667, 334.5, 26.0, 25.15587178299585, 0.3760279582760872, 0.0, 1.0, 0.0]
[2019-04-04 03:13:25,413] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 03:13:25,414] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.0369606e-25 3.3383846e-21 2.7230642e-24 5.5455392e-22 4.7369499e-22
 2.0382276e-25 1.0000000e+00], sampled 0.9574270199521249
[2019-04-04 03:13:30,850] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 03:13:37,765] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 03:13:38,808] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 3600000, evaluation results [3600000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 03:13:46,271] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.3994714e-29 1.2158250e-23 4.6781270e-27 3.0115125e-25 3.8864233e-25
 2.9986917e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:13:46,271] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4273
[2019-04-04 03:13:46,423] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-14.5, 87.0, 80.0, 330.0, 26.0, 25.71447886680073, 0.3862299421952788, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2709000.0000, 
sim time next is 2709600.0000, 
raw observation next is [-14.33333333333333, 88.33333333333334, 82.83333333333334, 377.0, 26.0, 25.91862088338155, 0.4092565918935945, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.0655586334256695, 0.8833333333333334, 0.27611111111111114, 0.4165745856353591, 0.6666666666666666, 0.6598850736151292, 0.6364188639645315, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.58442146], dtype=float32), -1.3382932]. 
=============================================
[2019-04-04 03:13:48,173] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [6.3559839e-28 8.5608282e-24 1.2698870e-26 1.4550965e-23 4.4270566e-24
 3.2907811e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:13:48,228] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3565
[2019-04-04 03:13:48,266] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.166666666666666, 64.0, 112.3333333333333, 787.0, 26.0, 25.98686026843815, 0.4714517048107085, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2721000.0000, 
sim time next is 2721600.0000, 
raw observation next is [-8.0, 64.0, 112.5, 790.0, 26.0, 25.94380731560665, 0.4710848062256616, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.24099722991689754, 0.64, 0.375, 0.8729281767955801, 0.6666666666666666, 0.661983942967221, 0.6570282687418872, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4833918], dtype=float32), -1.256937]. 
=============================================
[2019-04-04 03:13:55,459] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.9608786e-28 1.7235656e-23 7.0928079e-27 6.2155380e-24 1.6416283e-24
 5.2011765e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:13:55,460] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3597
[2019-04-04 03:13:55,520] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 48.33333333333334, 133.5, 43.0, 26.0, 25.81121380911717, 0.3013550059277703, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2542800.0000, 
sim time next is 2543400.0000, 
raw observation next is [-0.8999999999999999, 48.0, 133.0, 45.0, 26.0, 25.79521464578109, 0.3001929568921549, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.43767313019390586, 0.48, 0.44333333333333336, 0.049723756906077346, 0.6666666666666666, 0.6496012204817575, 0.6000643189640517, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.60735416], dtype=float32), -0.6676697]. 
=============================================
[2019-04-04 03:14:01,135] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.5003571e-26 2.8999854e-21 1.0577943e-24 9.0205437e-23 4.8072164e-22
 2.2145862e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 03:14:01,135] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5603
[2019-04-04 03:14:01,157] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.2, 36.33333333333334, 0.0, 0.0, 26.0, 25.25568696401628, 0.2730744043571229, 0.0, 1.0, 40028.2375786293], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2495400.0000, 
sim time next is 2496000.0000, 
raw observation next is [-1.2, 35.66666666666667, 0.0, 0.0, 26.0, 25.26894970255266, 0.2731017079695999, 0.0, 1.0, 40058.51177069263], 
processed observation next is [0.0, 0.9130434782608695, 0.42936288088642666, 0.3566666666666667, 0.0, 0.0, 0.6666666666666666, 0.6057458085460551, 0.5910339026565333, 0.0, 1.0, 0.1907548179556792], 
reward next is 0.8092, 
noisyNet noise sample is [array([-1.1663924], dtype=float32), 2.476729]. 
=============================================
[2019-04-04 03:14:01,172] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[81.36434]
 [81.25199]
 [81.18699]
 [81.13851]
 [80.95934]], R is [[81.41600037]
 [81.41123199]
 [81.40661621]
 [81.40187836]
 [81.39707184]].
[2019-04-04 03:14:36,243] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.9113444e-27 2.2288374e-21 4.5071607e-25 9.4874439e-23 5.8563822e-23
 4.4230349e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:14:36,244] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3099
[2019-04-04 03:14:36,297] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 65.0, 217.0, 154.0, 26.0, 24.97850470787592, 0.3390372724516872, 0.0, 1.0, 18743.28119188454], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2977200.0000, 
sim time next is 2977800.0000, 
raw observation next is [-3.0, 65.0, 230.0, 197.3333333333333, 26.0, 24.9963368883564, 0.3424755225193072, 0.0, 1.0, 18739.89428127718], 
processed observation next is [0.0, 0.4782608695652174, 0.3795013850415513, 0.65, 0.7666666666666667, 0.21804788213627987, 0.6666666666666666, 0.5830280740296999, 0.6141585075064357, 0.0, 1.0, 0.08923759181560562], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.46899274], dtype=float32), -0.24294616]. 
=============================================
[2019-04-04 03:14:40,373] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.8054865e-26 4.8136415e-21 1.9403340e-24 1.3881775e-22 5.5249309e-22
 4.8722500e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 03:14:40,373] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9977
[2019-04-04 03:14:40,388] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 71.0, 0.0, 0.0, 26.0, 24.60031698048386, 0.1895443154541918, 0.0, 1.0, 38095.48688355942], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3028800.0000, 
sim time next is 3029400.0000, 
raw observation next is [-5.0, 71.0, 0.0, 0.0, 26.0, 24.55823194302321, 0.1790266988416405, 0.0, 1.0, 38154.42569959466], 
processed observation next is [0.0, 0.043478260869565216, 0.32409972299168976, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5465193285852674, 0.5596755662805468, 0.0, 1.0, 0.18168774142664126], 
reward next is 0.8183, 
noisyNet noise sample is [array([-0.13255501], dtype=float32), 0.22791138]. 
=============================================
[2019-04-04 03:14:44,300] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.5942285e-27 1.4848553e-22 5.9567953e-26 9.9343715e-24 2.0160749e-23
 3.0785769e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:14:44,301] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1875
[2019-04-04 03:14:44,360] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.666666666666667, 58.33333333333334, 0.0, 0.0, 26.0, 25.03835415092971, 0.3428552639723497, 0.0, 1.0, 51008.15246615215], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3001200.0000, 
sim time next is 3001800.0000, 
raw observation next is [-1.833333333333333, 59.16666666666666, 0.0, 0.0, 26.0, 25.09708940797158, 0.3389554105814523, 0.0, 1.0, 18715.62290091659], 
processed observation next is [0.0, 0.7391304347826086, 0.41181902123730385, 0.5916666666666666, 0.0, 0.0, 0.6666666666666666, 0.591424117330965, 0.6129851368604841, 0.0, 1.0, 0.08912201381388853], 
reward next is 0.9109, 
noisyNet noise sample is [array([0.879403], dtype=float32), 0.73690146]. 
=============================================
[2019-04-04 03:14:50,368] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.2455074e-26 8.0508528e-21 2.0411776e-24 3.6735945e-22 3.7364875e-22
 4.8288045e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 03:14:50,368] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8488
[2019-04-04 03:14:50,388] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.666666666666666, 69.0, 0.0, 0.0, 26.0, 24.70775432101414, 0.2154662450038951, 0.0, 1.0, 37866.85600602323], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3026400.0000, 
sim time next is 3027000.0000, 
raw observation next is [-4.833333333333334, 70.0, 0.0, 0.0, 26.0, 24.66737736153852, 0.2066125098914877, 0.0, 1.0, 37899.45947301941], 
processed observation next is [0.0, 0.0, 0.32871652816251157, 0.7, 0.0, 0.0, 0.6666666666666666, 0.55561478012821, 0.5688708366304959, 0.0, 1.0, 0.18047361653818766], 
reward next is 0.8195, 
noisyNet noise sample is [array([-0.15892068], dtype=float32), 0.17433241]. 
=============================================
[2019-04-04 03:14:50,393] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[82.13798]
 [82.29609]
 [82.47977]
 [82.60436]
 [82.70767]], R is [[81.99977875]
 [81.99945831]
 [81.99920654]
 [81.998909  ]
 [81.99848175]].
[2019-04-04 03:14:50,796] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.1154271e-27 3.1683466e-22 2.1145436e-25 2.4991115e-23 6.9901337e-23
 8.4996072e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:14:50,798] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7482
[2019-04-04 03:14:50,801] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.3655048e-29 6.6485883e-24 2.2495541e-26 2.7239369e-24 6.3074486e-24
 3.8720927e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:14:50,805] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1285
[2019-04-04 03:14:50,817] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 93.0, 0.0, 0.0, 26.0, 25.56634244689427, 0.6325458203339445, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3194400.0000, 
sim time next is 3195000.0000, 
raw observation next is [2.0, 93.0, 0.0, 0.0, 26.0, 25.59432046310412, 0.6257093286045742, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.518005540166205, 0.93, 0.0, 0.0, 0.6666666666666666, 0.6328600385920099, 0.7085697762015247, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.05105959], dtype=float32), -1.0845209]. 
=============================================
[2019-04-04 03:14:50,821] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 100.0, 0.0, 0.0, 26.0, 25.43509848397836, 0.5418414529770351, 0.0, 1.0, 102452.4990585727], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3204000.0000, 
sim time next is 3204600.0000, 
raw observation next is [-0.1666666666666667, 100.0, 0.0, 0.0, 26.0, 25.36092244578596, 0.5514699523095034, 0.0, 1.0, 98813.99295274407], 
processed observation next is [1.0, 0.08695652173913043, 0.4579870729455217, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6134102038154966, 0.683823317436501, 0.0, 1.0, 0.47054282358449556], 
reward next is 0.5295, 
noisyNet noise sample is [array([-0.14730965], dtype=float32), 0.31153464]. 
=============================================
[2019-04-04 03:14:50,831] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[82.579094]
 [82.586464]
 [82.75146 ]
 [82.80277 ]
 [82.53154 ]], R is [[82.6460495 ]
 [82.81958771]
 [82.99139404]
 [82.95331573]
 [82.56177521]].
[2019-04-04 03:14:53,145] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4818122e-28 2.7360602e-23 2.7408405e-26 2.7119204e-24 2.0422992e-24
 4.4572784e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:14:53,145] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0773
[2019-04-04 03:14:53,205] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 92.0, 29.0, 178.0, 26.0, 25.57961655384193, 0.5142344332204015, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3225000.0000, 
sim time next is 3225600.0000, 
raw observation next is [-3.0, 92.0, 43.0, 226.0, 26.0, 25.54550507166091, 0.5184660024633451, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.3795013850415513, 0.92, 0.14333333333333334, 0.24972375690607734, 0.6666666666666666, 0.6287920893050757, 0.6728220008211151, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.1927567], dtype=float32), -0.31958038]. 
=============================================
[2019-04-04 03:14:54,574] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.425260e-27 2.863160e-22 4.265227e-25 8.052749e-23 9.957027e-23
 3.218859e-27 1.000000e+00], sum to 1.0000
[2019-04-04 03:14:54,576] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7269
[2019-04-04 03:14:54,642] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.15009625866716, 0.4828489440101838, 1.0, 1.0, 196217.9094192962], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3262800.0000, 
sim time next is 3263400.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.31243677102204, 0.4626895121560047, 1.0, 1.0, 196883.5665705758], 
processed observation next is [1.0, 0.782608695652174, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.52603639758517, 0.6542298373853349, 1.0, 1.0, 0.937540793193218], 
reward next is 0.0625, 
noisyNet noise sample is [array([2.461624], dtype=float32), 0.98942935]. 
=============================================
[2019-04-04 03:14:57,287] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.8923670e-28 2.9513868e-23 3.7734502e-26 4.7754634e-24 7.3748295e-24
 8.7762409e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:14:57,287] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2580
[2019-04-04 03:14:57,370] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.0, 80.5, 86.0, 396.0, 26.0, 25.72936242432628, 0.4591962670261521, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3313800.0000, 
sim time next is 3314400.0000, 
raw observation next is [-9.666666666666668, 79.33333333333333, 89.0, 432.5, 26.0, 25.85623035260561, 0.4722426618437292, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.19482917820867957, 0.7933333333333333, 0.2966666666666667, 0.47790055248618785, 0.6666666666666666, 0.6546858627171342, 0.6574142206145764, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.22179447], dtype=float32), -0.7557786]. 
=============================================
[2019-04-04 03:15:01,284] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.7247923e-27 5.0447298e-21 1.0568425e-24 1.9370228e-22 1.2864673e-22
 4.1083411e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:15:01,284] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3460
[2019-04-04 03:15:01,317] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.83022827786092, 0.2940486682903393, 0.0, 1.0, 41180.0989925426], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3377400.0000, 
sim time next is 3378000.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.81289804778207, 0.2943538687102767, 0.0, 1.0, 41165.25105529713], 
processed observation next is [1.0, 0.08695652173913043, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5677415039818392, 0.5981179562367589, 0.0, 1.0, 0.19602500502522444], 
reward next is 0.8040, 
noisyNet noise sample is [array([1.3827822], dtype=float32), -0.49129364]. 
=============================================
[2019-04-04 03:15:01,344] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[81.758675]
 [81.577934]
 [81.43429 ]
 [81.283104]
 [81.13633 ]], R is [[81.88497162]
 [81.87002563]
 [81.85525513]
 [81.84062195]
 [81.82603455]].
[2019-04-04 03:15:05,093] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.9875448e-28 1.8325367e-23 1.3749046e-26 1.1994299e-23 9.1980322e-24
 2.0352178e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:15:05,096] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0037
[2019-04-04 03:15:05,111] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 49.0, 114.3333333333333, 809.6666666666666, 26.0, 25.85685363697419, 0.5256827731966093, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3417000.0000, 
sim time next is 3417600.0000, 
raw observation next is [3.0, 49.0, 113.6666666666667, 807.8333333333334, 26.0, 25.85978576806452, 0.5738327316486885, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5457063711911359, 0.49, 0.378888888888889, 0.892633517495396, 0.6666666666666666, 0.65498214733871, 0.6912775772162295, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8970779], dtype=float32), -0.85767275]. 
=============================================
[2019-04-04 03:15:09,619] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.3366649e-28 6.7599675e-23 2.3203783e-26 1.5590731e-23 4.9073388e-24
 3.2402072e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:15:09,620] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7851
[2019-04-04 03:15:09,650] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 55.5, 0.0, 0.0, 26.0, 25.06955424277707, 0.4704378707263102, 0.0, 1.0, 198686.1745785816], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3875400.0000, 
sim time next is 3876000.0000, 
raw observation next is [-0.3333333333333333, 57.0, 0.0, 0.0, 26.0, 25.09513000715515, 0.5120927219790975, 0.0, 1.0, 164102.9918570878], 
processed observation next is [1.0, 0.8695652173913043, 0.4533702677747, 0.57, 0.0, 0.0, 0.6666666666666666, 0.5912608339295957, 0.6706975739930324, 0.0, 1.0, 0.7814428183670847], 
reward next is 0.2186, 
noisyNet noise sample is [array([-0.6534448], dtype=float32), -0.46666342]. 
=============================================
[2019-04-04 03:15:09,686] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[82.7555 ]
 [81.86038]
 [81.19734]
 [80.88087]
 [80.94958]], R is [[82.84741211]
 [82.07281494]
 [81.31316376]
 [81.18669128]
 [81.27541351]].
[2019-04-04 03:15:13,251] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.7139903e-27 1.3201949e-21 3.6763926e-25 1.2014179e-22 6.8502012e-23
 4.3385612e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:15:13,252] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9926
[2019-04-04 03:15:13,277] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.34048300735642, 0.4265497896333441, 0.0, 1.0, 44038.23336826492], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3614400.0000, 
sim time next is 3615000.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.43971501398899, 0.4318918341794706, 0.0, 1.0, 18764.23683512882], 
processed observation next is [0.0, 0.8695652173913043, 0.4349030470914128, 0.42, 0.0, 0.0, 0.6666666666666666, 0.6199762511657493, 0.6439639447264902, 0.0, 1.0, 0.08935350873870868], 
reward next is 0.9106, 
noisyNet noise sample is [array([0.23510368], dtype=float32), 0.30897048]. 
=============================================
[2019-04-04 03:15:13,299] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[84.51391]
 [84.4249 ]
 [84.36528]
 [83.94269]
 [83.11232]], R is [[84.60717773]
 [84.55139923]
 [84.44380951]
 [84.1813736 ]
 [83.5657196 ]].
[2019-04-04 03:15:23,674] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.4122698e-28 2.1927741e-23 1.8475691e-26 2.7699071e-23 1.0806720e-23
 8.3266570e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:15:23,674] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8440
[2019-04-04 03:15:23,694] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.666666666666667, 38.0, 118.8333333333333, 820.1666666666666, 26.0, 26.44166647824474, 0.5833882971345098, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4016400.0000, 
sim time next is 4017000.0000, 
raw observation next is [-6.333333333333333, 37.5, 118.6666666666667, 824.3333333333334, 26.0, 26.42399732640986, 0.588165831917534, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.28716528162511545, 0.375, 0.39555555555555566, 0.910865561694291, 0.6666666666666666, 0.7019997772008217, 0.6960552773058447, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4727631], dtype=float32), -1.6174242]. 
=============================================
[2019-04-04 03:15:23,702] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[85.322525]
 [85.44383 ]
 [85.5586  ]
 [85.74506 ]
 [85.974434]], R is [[85.42808533]
 [85.57380676]
 [85.71807098]
 [85.86089325]
 [86.00228882]].
[2019-04-04 03:15:24,533] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.6656471e-28 2.8424924e-23 2.6476918e-26 7.5281209e-24 4.2819228e-24
 5.0351185e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:15:24,533] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1328
[2019-04-04 03:15:24,583] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.0, 58.0, 93.0, 444.0, 26.0, 25.83649952260184, 0.4442952091622723, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4005000.0000, 
sim time next is 4005600.0000, 
raw observation next is [-11.66666666666667, 56.33333333333333, 94.33333333333333, 486.3333333333333, 26.0, 26.12687808499749, 0.4752428830700275, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.13942751615881802, 0.5633333333333332, 0.3144444444444444, 0.5373848987108656, 0.6666666666666666, 0.6772398404164575, 0.6584142943566759, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.43772045], dtype=float32), 1.4155662]. 
=============================================
[2019-04-04 03:15:25,002] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.3013331e-28 4.2597657e-24 8.5166140e-27 5.8425673e-24 5.0855492e-24
 8.3339936e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 03:15:25,002] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7339
[2019-04-04 03:15:25,020] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 49.0, 66.33333333333334, 552.0, 26.0, 26.96337203305146, 0.517504942972744, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3514800.0000, 
sim time next is 3515400.0000, 
raw observation next is [3.0, 49.0, 62.0, 525.0, 26.0, 26.8902782335996, 0.7398299780098004, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.5457063711911359, 0.49, 0.20666666666666667, 0.580110497237569, 0.6666666666666666, 0.7408565194666332, 0.7466099926699336, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.681086], dtype=float32), -1.6616281]. 
=============================================
[2019-04-04 03:15:28,691] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.1407656e-28 1.8312439e-23 1.1481536e-26 1.4510217e-24 1.9892443e-24
 1.4807837e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:15:28,692] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6192
[2019-04-04 03:15:28,737] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 65.5, 101.0, 680.0, 26.0, 26.23515037954987, 0.5550730775399934, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3835800.0000, 
sim time next is 3836400.0000, 
raw observation next is [-2.666666666666667, 63.66666666666667, 102.5, 695.8333333333334, 26.0, 26.32213483966953, 0.5710407369418676, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.38873499538319484, 0.6366666666666667, 0.3416666666666667, 0.768876611418048, 0.6666666666666666, 0.6935112366391275, 0.6903469123139558, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7202264], dtype=float32), 0.33225816]. 
=============================================
[2019-04-04 03:15:29,342] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.2942907e-27 3.4977528e-23 3.2630837e-26 2.1220123e-23 8.0327103e-24
 4.0634329e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:15:29,347] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5735
[2019-04-04 03:15:29,377] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.666666666666667, 69.0, 0.0, 0.0, 26.0, 25.25356791834936, 0.533593792093657, 0.0, 1.0, 91266.70358262879], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3789600.0000, 
sim time next is 3790200.0000, 
raw observation next is [-2.833333333333333, 70.0, 0.0, 0.0, 26.0, 25.45948417819925, 0.5495744759941811, 0.0, 1.0, 18758.371222703], 
processed observation next is [1.0, 0.8695652173913043, 0.3841181902123731, 0.7, 0.0, 0.0, 0.6666666666666666, 0.6216236815166042, 0.6831914919980604, 0.0, 1.0, 0.08932557725096667], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.67575186], dtype=float32), 1.3814082]. 
=============================================
[2019-04-04 03:15:32,644] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.2829062e-27 4.6708766e-24 8.3366096e-27 1.0888155e-23 4.4730602e-24
 4.0355635e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 03:15:32,644] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.6782
[2019-04-04 03:15:32,655] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.333333333333333, 47.0, 90.16666666666666, 727.8333333333333, 26.0, 26.08764807972202, 0.6671580491962552, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3856800.0000, 
sim time next is 3857400.0000, 
raw observation next is [2.5, 46.5, 87.0, 717.0, 26.0, 26.39875676377999, 0.7016478586746663, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5318559556786704, 0.465, 0.29, 0.7922651933701658, 0.6666666666666666, 0.6998963969816657, 0.7338826195582221, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.20644033], dtype=float32), 0.36265826]. 
=============================================
[2019-04-04 03:15:37,246] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.3248882e-26 1.7883824e-20 1.0403624e-24 1.2754289e-22 3.8403743e-22
 5.3010960e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:15:37,246] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1616
[2019-04-04 03:15:37,280] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 49.0, 0.0, 0.0, 26.0, 25.35035098100605, 0.3193929318162274, 0.0, 1.0, 39279.73657328916], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4258200.0000, 
sim time next is 4258800.0000, 
raw observation next is [3.0, 49.0, 0.0, 0.0, 26.0, 25.34476708556436, 0.3180508258358804, 0.0, 1.0, 39216.77030152545], 
processed observation next is [0.0, 0.30434782608695654, 0.5457063711911359, 0.49, 0.0, 0.0, 0.6666666666666666, 0.61206392379703, 0.6060169419452935, 0.0, 1.0, 0.18674652524535928], 
reward next is 0.8133, 
noisyNet noise sample is [array([-1.0018668], dtype=float32), 0.42451534]. 
=============================================
[2019-04-04 03:15:43,128] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.5681282e-28 6.7276275e-24 1.6543695e-26 7.2066309e-24 4.6515128e-24
 2.8627909e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:15:43,128] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1480
[2019-04-04 03:15:43,186] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.666666666666668, 47.0, 102.8333333333333, 711.8333333333334, 26.0, 26.53224643101471, 0.5530006897675276, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4009200.0000, 
sim time next is 4009800.0000, 
raw observation next is [-9.333333333333332, 45.5, 104.6666666666667, 725.6666666666666, 26.0, 26.55517759119067, 0.5581593310182602, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.20406278855032323, 0.455, 0.348888888888889, 0.8018416206261509, 0.6666666666666666, 0.7129314659325559, 0.6860531103394201, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.1687853], dtype=float32), 0.05836211]. 
=============================================
[2019-04-04 03:15:44,419] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.6605828e-28 6.0909553e-22 7.9642953e-26 1.3444050e-23 2.4368838e-23
 2.2144319e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:15:44,419] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3792
[2019-04-04 03:15:44,486] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.333333333333334, 36.33333333333333, 15.33333333333333, 78.16666666666664, 26.0, 25.26867050908424, 0.3119569093327456, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4088400.0000, 
sim time next is 4089000.0000, 
raw observation next is [-4.166666666666667, 35.16666666666667, 30.66666666666666, 156.3333333333333, 26.0, 25.35803340562426, 0.3253649882580697, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.3471837488457987, 0.35166666666666674, 0.1022222222222222, 0.17274401473296497, 0.6666666666666666, 0.6131694504686882, 0.6084549960860232, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.58954996], dtype=float32), 0.55556166]. 
=============================================
[2019-04-04 03:15:44,501] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[87.148224]
 [85.29651 ]
 [83.85202 ]
 [83.322296]
 [83.437454]], R is [[88.69772339]
 [88.81074524]
 [88.92263794]
 [88.06928253]
 [87.99873352]].
[2019-04-04 03:15:46,565] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.4687237e-26 2.9897246e-21 2.3480013e-24 8.4507741e-23 2.9040908e-22
 2.4959827e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:15:46,568] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6311
[2019-04-04 03:15:46,589] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.333333333333334, 54.66666666666667, 0.0, 0.0, 26.0, 25.04759307674608, 0.3623951249292167, 0.0, 1.0, 44096.89882123662], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3972000.0000, 
sim time next is 3972600.0000, 
raw observation next is [-9.5, 55.5, 0.0, 0.0, 26.0, 24.99906137874251, 0.3508805457001007, 0.0, 1.0, 44084.79141663356], 
processed observation next is [1.0, 1.0, 0.1994459833795014, 0.555, 0.0, 0.0, 0.6666666666666666, 0.5832551148952092, 0.6169601819000335, 0.0, 1.0, 0.20992757817444552], 
reward next is 0.7901, 
noisyNet noise sample is [array([1.4401977], dtype=float32), 1.1440368]. 
=============================================
[2019-04-04 03:16:04,185] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.0562962e-29 1.3823105e-24 1.4791416e-27 1.1120574e-24 3.7922836e-25
 5.4096856e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 03:16:04,186] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6533
[2019-04-04 03:16:04,192] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.66666666666667, 52.66666666666667, 0.0, 0.0, 26.0, 27.72165536456245, 0.9822995867296952, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4389600.0000, 
sim time next is 4390200.0000, 
raw observation next is [11.5, 54.0, 0.0, 0.0, 26.0, 27.58395261495759, 0.9657998387888668, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.7811634349030472, 0.54, 0.0, 0.0, 0.6666666666666666, 0.7986627179131324, 0.8219332795962889, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9192487], dtype=float32), 0.38472337]. 
=============================================
[2019-04-04 03:16:05,034] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.2235774e-28 3.2036998e-23 3.5115847e-26 4.8086239e-24 5.7405881e-24
 5.6371079e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:16:05,048] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4119
[2019-04-04 03:16:05,069] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.50678810841445, 0.5399563255586105, 0.0, 1.0, 57598.37979708827], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4485600.0000, 
sim time next is 4486200.0000, 
raw observation next is [-0.05, 72.0, 0.0, 0.0, 26.0, 25.46504218643415, 0.4939283943486331, 0.0, 1.0, 71001.07012544415], 
processed observation next is [1.0, 0.9565217391304348, 0.461218836565097, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6220868488695125, 0.6646427981162111, 0.0, 1.0, 0.33810033393068645], 
reward next is 0.6619, 
noisyNet noise sample is [array([-1.4389164], dtype=float32), -0.5921986]. 
=============================================
[2019-04-04 03:16:05,553] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.3484967e-26 7.0260673e-22 7.8761682e-25 6.3126272e-23 1.1164791e-22
 3.1876889e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:16:05,555] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.8228
[2019-04-04 03:16:05,573] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.666666666666667, 68.0, 0.0, 0.0, 26.0, 25.53889477077428, 0.5599692892413267, 0.0, 1.0, 56638.58018907436], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4425000.0000, 
sim time next is 4425600.0000, 
raw observation next is [3.533333333333334, 68.0, 0.0, 0.0, 26.0, 25.68699971651514, 0.5595283610099938, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.5604801477377656, 0.68, 0.0, 0.0, 0.6666666666666666, 0.6405833097095949, 0.6865094536699979, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.21232797], dtype=float32), -0.48377454]. 
=============================================
[2019-04-04 03:16:08,666] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.0414622e-28 1.1717077e-23 2.7999462e-26 1.4771260e-23 3.4148885e-24
 2.3868835e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:16:08,669] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3520
[2019-04-04 03:16:08,677] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 50.66666666666666, 95.50000000000001, 61.33333333333334, 26.0, 25.43500543983922, 0.4896062646421206, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4552800.0000, 
sim time next is 4553400.0000, 
raw observation next is [2.0, 51.33333333333334, 82.0, 54.66666666666667, 26.0, 25.72537188059517, 0.5060612329315378, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.518005540166205, 0.5133333333333334, 0.2733333333333333, 0.060405156537753225, 0.6666666666666666, 0.6437809900495974, 0.6686870776438459, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1819414], dtype=float32), 1.0349073]. 
=============================================
[2019-04-04 03:16:09,763] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.9932530e-27 3.1949266e-22 3.6041985e-25 2.2333203e-23 5.7817982e-23
 1.8506586e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:16:09,764] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.7826
[2019-04-04 03:16:09,800] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.7333333333333334, 73.0, 0.0, 0.0, 26.0, 25.33887183658305, 0.4438221589772329, 0.0, 1.0, 43165.10114863155], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4501200.0000, 
sim time next is 4501800.0000, 
raw observation next is [-0.8, 73.0, 0.0, 0.0, 26.0, 25.32247291733525, 0.4355065975898428, 0.0, 1.0, 42787.40782452659], 
processed observation next is [1.0, 0.08695652173913043, 0.4404432132963989, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6102060764446042, 0.6451688658632809, 0.0, 1.0, 0.20374956106917425], 
reward next is 0.7963, 
noisyNet noise sample is [array([-0.6823193], dtype=float32), -0.72884154]. 
=============================================
[2019-04-04 03:16:12,394] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [8.8635156e-27 1.2493347e-21 7.8922284e-25 6.0269068e-23 1.1528871e-22
 6.6036403e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:16:12,394] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5936
[2019-04-04 03:16:12,470] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.666666666666667, 75.0, 0.0, 0.0, 26.0, 24.96432186134011, 0.3422328428977399, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4605600.0000, 
sim time next is 4606200.0000, 
raw observation next is [-2.5, 74.0, 0.0, 0.0, 26.0, 25.09691386204256, 0.3641323225708768, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.39335180055401664, 0.74, 0.0, 0.0, 0.6666666666666666, 0.5914094885035466, 0.6213774408569589, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4310755], dtype=float32), 0.85654825]. 
=============================================
[2019-04-04 03:16:17,860] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.9619247e-28 8.0879329e-24 7.1306801e-27 2.3227806e-24 1.5681327e-24
 4.7376220e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:16:17,861] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1339
[2019-04-04 03:16:17,878] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 92.0, 210.5, 6.0, 26.0, 26.47532109893728, 0.5922298287576077, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4705200.0000, 
sim time next is 4705800.0000, 
raw observation next is [0.1666666666666667, 91.0, 211.3333333333333, 6.0, 26.0, 26.43042760141803, 0.5831849760562515, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4672206832871654, 0.91, 0.7044444444444443, 0.0066298342541436465, 0.6666666666666666, 0.7025356334515026, 0.6943949920187505, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2753968], dtype=float32), 0.014221794]. 
=============================================
[2019-04-04 03:16:20,215] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.3698822e-27 1.0194240e-22 1.4062704e-25 1.5242825e-23 1.0655586e-23
 5.3314317e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:16:20,215] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.6659
[2019-04-04 03:16:20,232] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.39880171274698, 0.4291056394000909, 0.0, 1.0, 66585.70249569401], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4689600.0000, 
sim time next is 4690200.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.4309419648604, 0.4324182819300138, 0.0, 1.0, 30457.56732834279], 
processed observation next is [1.0, 0.2608695652173913, 0.4349030470914128, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6192451637383666, 0.6441394273100046, 0.0, 1.0, 0.14503603489687042], 
reward next is 0.8550, 
noisyNet noise sample is [array([-1.0182954], dtype=float32), -0.25722355]. 
=============================================
[2019-04-04 03:16:20,369] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:16:20,369] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:16:20,372] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run28
[2019-04-04 03:16:22,266] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.7505155e-27 4.7412320e-23 1.1838506e-25 2.1164747e-23 3.6193440e-23
 6.2893197e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:16:22,268] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7375
[2019-04-04 03:16:22,301] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.16666666666667, 17.0, 24.0, 194.6666666666667, 26.0, 28.50684658548913, 1.121711318949016, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5075400.0000, 
sim time next is 5076000.0000, 
raw observation next is [11.0, 17.0, 18.0, 146.0, 26.0, 28.56727501970288, 1.110467865862679, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.7673130193905818, 0.17, 0.06, 0.16132596685082873, 0.6666666666666666, 0.8806062516419066, 0.8701559552875597, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.14143437], dtype=float32), 0.17453]. 
=============================================
[2019-04-04 03:16:22,311] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[79.99299]
 [80.24379]
 [80.44496]
 [80.59267]
 [80.92087]], R is [[79.95868683]
 [80.15910339]
 [80.35751343]
 [80.55393982]
 [80.74839783]].
[2019-04-04 03:16:24,405] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:16:24,405] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:16:24,454] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run28
[2019-04-04 03:16:27,960] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:16:27,960] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:16:27,967] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run28
[2019-04-04 03:16:30,763] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.3971885e-26 1.3496226e-20 1.2257810e-24 7.9684102e-23 2.2374059e-22
 1.3408083e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 03:16:30,763] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4806
[2019-04-04 03:16:30,774] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.666666666666667, 67.33333333333333, 0.0, 0.0, 26.0, 24.81127718934649, 0.2322971703744712, 0.0, 1.0, 39244.21998023891], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4855200.0000, 
sim time next is 4855800.0000, 
raw observation next is [-3.833333333333333, 69.16666666666667, 0.0, 0.0, 26.0, 24.78805199118534, 0.2318685166996096, 0.0, 1.0, 39293.23576882304], 
processed observation next is [0.0, 0.17391304347826086, 0.3564173591874424, 0.6916666666666668, 0.0, 0.0, 0.6666666666666666, 0.5656709992654451, 0.5772895055665365, 0.0, 1.0, 0.18711064651820494], 
reward next is 0.8129, 
noisyNet noise sample is [array([-0.29661885], dtype=float32), -0.06574214]. 
=============================================
[2019-04-04 03:16:37,817] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:16:37,817] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:16:37,820] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run28
[2019-04-04 03:16:40,621] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.8701869e-29 3.9960964e-24 7.7616864e-27 1.7268487e-24 1.8608260e-24
 2.6041161e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:16:40,622] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5418
[2019-04-04 03:16:40,628] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.3, 25.0, 0.0, 0.0, 26.0, 26.01536863312196, 0.5727434365154485, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5095200.0000, 
sim time next is 5095800.0000, 
raw observation next is [8.25, 27.5, 0.0, 0.0, 26.0, 25.93534602878092, 0.5558817985628172, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.6911357340720222, 0.275, 0.0, 0.0, 0.6666666666666666, 0.6612788357317433, 0.6852939328542723, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1249341], dtype=float32), -1.5564816]. 
=============================================
[2019-04-04 03:16:41,494] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:16:41,495] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:16:41,500] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run28
[2019-04-04 03:16:41,595] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:16:41,595] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:16:41,599] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run28
[2019-04-04 03:16:41,624] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:16:41,629] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:16:41,637] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run28
[2019-04-04 03:16:41,725] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:16:41,734] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:16:41,772] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run28
[2019-04-04 03:16:41,996] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:16:41,996] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:16:41,999] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run28
[2019-04-04 03:16:42,113] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:16:42,114] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:16:42,117] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run28
[2019-04-04 03:16:46,114] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:16:46,115] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:16:46,133] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run28
[2019-04-04 03:16:46,413] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:16:46,413] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:16:46,416] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run28
[2019-04-04 03:16:47,470] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:16:47,470] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:16:47,488] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run28
[2019-04-04 03:16:49,925] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:16:49,925] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:16:49,928] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run28
[2019-04-04 03:16:53,113] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:16:53,113] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:16:53,117] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run28
[2019-04-04 03:17:00,989] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:17:00,989] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:17:00,992] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run28
[2019-04-04 03:17:07,970] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.9277161e-26 1.0642842e-21 5.7195820e-24 2.5663339e-22 3.6802711e-22
 3.5071819e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 03:17:07,970] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6499
[2019-04-04 03:17:08,007] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.98758475969688, -0.185682409930267, 0.0, 1.0, 44403.1259219667], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 189000.0000, 
sim time next is 189600.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.92349736331452, -0.1938747755881511, 0.0, 1.0, 44493.17285049838], 
processed observation next is [1.0, 0.17391304347826086, 0.21606648199445982, 0.78, 0.0, 0.0, 0.6666666666666666, 0.4102914469428767, 0.4353750748039496, 0.0, 1.0, 0.2118722516690399], 
reward next is 0.7881, 
noisyNet noise sample is [array([-1.0499302], dtype=float32), -0.15696532]. 
=============================================
[2019-04-04 03:17:22,472] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.4795000e-25 2.3872653e-21 3.8716871e-24 4.7521531e-22 4.9101151e-22
 2.2386918e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 03:17:22,473] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8212
[2019-04-04 03:17:22,530] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.2, 83.0, 104.5, 138.6666666666667, 26.0, 24.97345992447614, 0.3237237395282644, 0.0, 1.0, 47249.20296949052], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 573600.0000, 
sim time next is 574200.0000, 
raw observation next is [-1.2, 83.0, 100.0, 73.0, 26.0, 24.96331982358114, 0.3250365299198316, 0.0, 1.0, 45053.95935703665], 
processed observation next is [0.0, 0.6521739130434783, 0.42936288088642666, 0.83, 0.3333333333333333, 0.08066298342541436, 0.6666666666666666, 0.5802766519650951, 0.6083455099732772, 0.0, 1.0, 0.21454266360493643], 
reward next is 0.7855, 
noisyNet noise sample is [array([-1.3287377], dtype=float32), 0.11024217]. 
=============================================
[2019-04-04 03:17:26,002] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.2963214e-26 2.0135483e-21 3.2341260e-24 1.0106538e-22 1.5395764e-22
 1.1150304e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 03:17:26,002] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6786
[2019-04-04 03:17:26,027] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-15.6, 73.0, 0.0, 0.0, 26.0, 22.15075530735789, -0.3704022797251074, 0.0, 1.0, 49233.1877315402], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 362400.0000, 
sim time next is 363000.0000, 
raw observation next is [-15.6, 73.0, 0.0, 0.0, 26.0, 22.11768814488953, -0.3597372947397015, 0.0, 1.0, 49481.32121765371], 
processed observation next is [1.0, 0.17391304347826086, 0.030470914127423816, 0.73, 0.0, 0.0, 0.6666666666666666, 0.34314067874079424, 0.3800875684200995, 0.0, 1.0, 0.23562533913168435], 
reward next is 0.7644, 
noisyNet noise sample is [array([-0.35394478], dtype=float32), -0.6706001]. 
=============================================
[2019-04-04 03:17:26,036] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[80.307755]
 [80.261955]
 [80.23891 ]
 [80.23961 ]
 [80.25906 ]], R is [[80.34635162]
 [80.30844879]
 [80.270401  ]
 [80.23247528]
 [80.19486237]].
[2019-04-04 03:17:30,684] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [4.7239117e-26 6.9499358e-22 7.8829012e-25 2.9707828e-22 1.6491242e-22
 1.3735160e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 03:17:30,687] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9325
[2019-04-04 03:17:30,728] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.2, 80.5, 130.6666666666667, 509.6666666666666, 26.0, 24.999815484291, 0.3477009824114687, 0.0, 1.0, 25931.66447797677], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 569400.0000, 
sim time next is 570000.0000, 
raw observation next is [-1.2, 81.0, 128.8333333333333, 488.3333333333333, 26.0, 24.9946501321666, 0.3471977419198642, 0.0, 1.0, 32031.58099451198], 
processed observation next is [0.0, 0.6086956521739131, 0.42936288088642666, 0.81, 0.4294444444444443, 0.5395948434622467, 0.6666666666666666, 0.5828875110138835, 0.6157325806399547, 0.0, 1.0, 0.15253133806910465], 
reward next is 0.8475, 
noisyNet noise sample is [array([-1.0022849], dtype=float32), -0.120962515]. 
=============================================
[2019-04-04 03:17:30,733] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[80.32484 ]
 [80.38594 ]
 [80.659325]
 [81.14414 ]
 [81.62129 ]], R is [[80.44106293]
 [80.51316833]
 [80.6188736 ]
 [80.72351074]
 [80.82709503]].
[2019-04-04 03:17:46,185] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0108595e-27 1.5754786e-23 3.3380452e-26 5.7791488e-24 1.4809513e-23
 6.0910760e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:17:46,195] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6680
[2019-04-04 03:17:46,250] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.33333333333333, 45.0, 50.33333333333333, 850.3333333333334, 26.0, 26.38819600286435, 0.4128792571188107, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 396600.0000, 
sim time next is 397200.0000, 
raw observation next is [-10.16666666666667, 44.0, 49.16666666666666, 841.1666666666667, 26.0, 25.89400550062027, 0.4246978011192106, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.18097876269621416, 0.44, 0.16388888888888886, 0.9294659300184163, 0.6666666666666666, 0.6578337917183559, 0.6415659337064036, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4177159], dtype=float32), -1.1799852]. 
=============================================
[2019-04-04 03:17:47,740] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.7549228e-28 1.3129564e-23 3.5425842e-26 3.9354937e-24 3.0088500e-24
 7.7732978e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:17:47,742] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4842
[2019-04-04 03:17:47,767] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.2, 81.66666666666667, 0.0, 0.0, 26.0, 24.81111438259661, 0.2513255749680333, 0.0, 1.0, 41484.55922090947], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 858000.0000, 
sim time next is 858600.0000, 
raw observation next is [-3.1, 81.0, 0.0, 0.0, 26.0, 24.78785344617673, 0.2468123486584247, 0.0, 1.0, 41401.11406636181], 
processed observation next is [1.0, 0.9565217391304348, 0.37673130193905824, 0.81, 0.0, 0.0, 0.6666666666666666, 0.5656544538480608, 0.5822707828861415, 0.0, 1.0, 0.19714816222077053], 
reward next is 0.8029, 
noisyNet noise sample is [array([-1.2142669], dtype=float32), -0.5817861]. 
=============================================
[2019-04-04 03:18:00,569] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.6515466e-31 3.6296667e-26 4.6311047e-29 4.6953617e-27 5.6727257e-27
 6.7950499e-31 1.0000000e+00], sum to 1.0000
[2019-04-04 03:18:00,569] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5450
[2019-04-04 03:18:00,576] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [19.2, 50.66666666666667, 132.0, 0.0, 26.0, 27.67506028248892, 0.9912841605443025, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1089600.0000, 
sim time next is 1090200.0000, 
raw observation next is [19.3, 49.83333333333334, 124.0, 0.0, 26.0, 27.74882959348164, 1.0030840572918, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.9972299168975071, 0.4983333333333334, 0.41333333333333333, 0.0, 0.6666666666666666, 0.8124024661234701, 0.8343613524306001, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4202591], dtype=float32), 0.10549056]. 
=============================================
[2019-04-04 03:18:08,204] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.3157425e-28 7.7405828e-25 5.3037226e-27 7.9865392e-25 2.0454335e-24
 6.4972089e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 03:18:08,206] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1656
[2019-04-04 03:18:08,251] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 83.0, 0.0, 0.0, 26.0, 24.90778369893003, 0.2753594970344416, 0.0, 1.0, 42407.1798798204], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 854400.0000, 
sim time next is 855000.0000, 
raw observation next is [-3.4, 83.0, 0.0, 0.0, 26.0, 24.87166616670159, 0.2683075853839145, 0.0, 1.0, 42276.96126628766], 
processed observation next is [1.0, 0.9130434782608695, 0.368421052631579, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5726388472251326, 0.5894358617946381, 0.0, 1.0, 0.20131886317279837], 
reward next is 0.7987, 
noisyNet noise sample is [array([-0.32002798], dtype=float32), -1.4167489]. 
=============================================
[2019-04-04 03:18:08,255] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[84.57401 ]
 [84.34389 ]
 [84.092255]
 [83.89645 ]
 [83.693985]], R is [[84.59091949]
 [84.54306793]
 [84.4945755 ]
 [84.44364166]
 [84.38412476]].
[2019-04-04 03:18:23,570] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.9915820e-30 1.3225109e-25 3.5848044e-28 2.5411660e-26 3.6836486e-26
 8.6863322e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 03:18:23,576] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0259
[2019-04-04 03:18:23,590] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 26.0, 26.10142924071922, 0.6387332914151563, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1039200.0000, 
sim time next is 1039800.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 26.0, 26.06520965562358, 0.6319593920389058, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.8614958448753465, 0.75, 0.0, 0.0, 0.6666666666666666, 0.6721008046352983, 0.7106531306796353, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1769179], dtype=float32), 1.881471]. 
=============================================
[2019-04-04 03:18:26,069] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-04-04 03:18:26,072] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 03:18:26,082] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:18:26,084] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run38
[2019-04-04 03:18:26,131] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 03:18:26,136] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:18:26,137] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 03:18:26,137] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:18:26,145] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run38
[2019-04-04 03:18:26,209] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run38
[2019-04-04 03:19:28,664] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.35900956], dtype=float32), 0.18090412]
[2019-04-04 03:19:28,664] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [5.024146273333334, 96.06776371333335, 0.0, 0.0, 26.0, 25.30110484697931, 0.386637919631952, 0.0, 1.0, 38691.01060724469]
[2019-04-04 03:19:28,664] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 03:19:28,665] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [4.9670859e-29 2.7855458e-24 2.4001147e-27 3.6839173e-25 4.3715570e-25
 1.2741802e-28 1.0000000e+00], sampled 0.7488772042740707
[2019-04-04 03:21:32,634] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 03:22:05,999] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 03:22:12,306] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 03:22:13,344] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 3700000, evaluation results [3700000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 03:22:20,326] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.9633231e-30 7.9673990e-25 1.4781489e-27 2.4985370e-25 1.9724306e-25
 4.6857307e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 03:22:20,346] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6824
[2019-04-04 03:22:20,364] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.0, 82.0, 0.0, 0.0, 26.0, 25.65225850832185, 0.5179638931962525, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1576800.0000, 
sim time next is 1577400.0000, 
raw observation next is [5.083333333333334, 81.50000000000001, 0.0, 0.0, 26.0, 25.69092880130042, 0.5051554535896566, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.6034164358264081, 0.8150000000000002, 0.0, 0.0, 0.6666666666666666, 0.6409107334417016, 0.6683851511965523, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1741947], dtype=float32), 0.73982644]. 
=============================================
[2019-04-04 03:22:21,206] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.9692974e-31 3.7001545e-26 1.5680031e-29 3.2937343e-27 4.0384629e-27
 3.3399598e-31 1.0000000e+00], sum to 1.0000
[2019-04-04 03:22:21,206] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1136
[2019-04-04 03:22:21,263] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 77.0, 0.0, 0.0, 26.0, 24.84753070932601, 0.4273176523760929, 0.0, 1.0, 196650.5451134171], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1024200.0000, 
sim time next is 1024800.0000, 
raw observation next is [14.4, 77.0, 0.0, 0.0, 26.0, 24.83251530042941, 0.4730710096951243, 0.0, 1.0, 198382.711794217], 
processed observation next is [1.0, 0.8695652173913043, 0.8614958448753465, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5693762750357841, 0.6576903365650414, 0.0, 1.0, 0.9446795799724619], 
reward next is 0.0553, 
noisyNet noise sample is [array([0.29026902], dtype=float32), -0.5376674]. 
=============================================
[2019-04-04 03:22:37,361] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.2281058e-29 2.0249753e-25 2.2002341e-28 1.4514306e-25 1.7159015e-25
 9.1664794e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 03:22:37,361] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9483
[2019-04-04 03:22:37,423] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 26.0, 24.95878138857479, 0.5260675042864814, 0.0, 1.0, 117152.109844733], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1369800.0000, 
sim time next is 1370400.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 26.0, 25.06876310895569, 0.549224078145717, 0.0, 1.0, 68019.84295314128], 
processed observation next is [1.0, 0.8695652173913043, 0.4764542936288089, 0.96, 0.0, 0.0, 0.6666666666666666, 0.5890635924129741, 0.683074692715239, 0.0, 1.0, 0.3239040140625775], 
reward next is 0.6761, 
noisyNet noise sample is [array([-0.96291006], dtype=float32), 0.5271217]. 
=============================================
[2019-04-04 03:22:56,886] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.8377362e-25 4.4375635e-21 2.0772798e-24 8.7802099e-23 3.7378133e-22
 2.5137087e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 03:22:56,886] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3040
[2019-04-04 03:22:56,901] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.2, 81.0, 0.0, 0.0, 26.0, 23.78586040340056, 0.01795251803094739, 0.0, 1.0, 46956.49055342565], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1830600.0000, 
sim time next is 1831200.0000, 
raw observation next is [-6.199999999999999, 80.33333333333334, 0.0, 0.0, 26.0, 23.74841624873595, 0.0105619601704026, 0.0, 1.0, 46988.89903281634], 
processed observation next is [0.0, 0.17391304347826086, 0.2908587257617729, 0.8033333333333335, 0.0, 0.0, 0.6666666666666666, 0.4790346873946625, 0.5035206533901342, 0.0, 1.0, 0.2237566620610302], 
reward next is 0.7762, 
noisyNet noise sample is [array([-0.6371765], dtype=float32), -0.2693606]. 
=============================================
[2019-04-04 03:23:02,486] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.1404572e-27 1.5209319e-22 2.3343269e-25 2.8578118e-23 1.6366644e-23
 1.1161447e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:23:02,495] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3734
[2019-04-04 03:23:02,537] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.2, 87.0, 0.0, 0.0, 26.0, 24.03975449421284, 0.02393326081833424, 0.0, 1.0, 41183.24678815856], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2010600.0000, 
sim time next is 2011200.0000, 
raw observation next is [-6.199999999999999, 87.0, 0.0, 0.0, 26.0, 23.94996632264887, 0.02077610546672487, 0.0, 1.0, 41198.77896487354], 
processed observation next is [1.0, 0.2608695652173913, 0.2908587257617729, 0.87, 0.0, 0.0, 0.6666666666666666, 0.49583052688740575, 0.5069253684889082, 0.0, 1.0, 0.19618466173749305], 
reward next is 0.8038, 
noisyNet noise sample is [array([0.696567], dtype=float32), 0.7825496]. 
=============================================
[2019-04-04 03:23:04,450] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.4336149e-27 5.5874747e-23 4.5013527e-25 2.6464538e-23 4.1153744e-23
 3.3272643e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:23:04,453] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9681
[2019-04-04 03:23:04,471] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.25, 91.16666666666667, 0.0, 0.0, 26.0, 25.32768726345083, 0.4483729229428112, 0.0, 1.0, 42904.48344992023], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1734600.0000, 
sim time next is 1735200.0000, 
raw observation next is [0.2, 91.0, 0.0, 0.0, 26.0, 25.30825494827818, 0.4436592557008449, 0.0, 1.0, 42906.89894739845], 
processed observation next is [0.0, 0.08695652173913043, 0.46814404432132967, 0.91, 0.0, 0.0, 0.6666666666666666, 0.6090212456898483, 0.6478864185669483, 0.0, 1.0, 0.2043185664161831], 
reward next is 0.7957, 
noisyNet noise sample is [array([0.9330634], dtype=float32), -0.5079464]. 
=============================================
[2019-04-04 03:23:10,221] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.8207780e-25 8.0796506e-21 7.6653338e-24 1.0014209e-21 1.1035187e-21
 4.5109103e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 03:23:10,221] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1695
[2019-04-04 03:23:10,267] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.3, 82.66666666666666, 0.0, 0.0, 26.0, 25.00334314226739, 0.3161249663589848, 0.0, 1.0, 49064.18548967857], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1795200.0000, 
sim time next is 1795800.0000, 
raw observation next is [-4.399999999999999, 82.83333333333334, 0.0, 0.0, 26.0, 25.00589120090586, 0.3157097888415883, 0.0, 1.0, 48514.59863742439], 
processed observation next is [0.0, 0.782608695652174, 0.3407202216066483, 0.8283333333333335, 0.0, 0.0, 0.6666666666666666, 0.583824266742155, 0.6052365962805294, 0.0, 1.0, 0.23102189827344946], 
reward next is 0.7690, 
noisyNet noise sample is [array([-0.14479516], dtype=float32), 0.1710305]. 
=============================================
[2019-04-04 03:23:11,147] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [3.0665661e-25 4.5000422e-20 1.5170373e-23 1.9585479e-22 6.2682140e-22
 2.0835414e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 03:23:11,147] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.6954
[2019-04-04 03:23:11,170] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.2, 89.66666666666667, 0.0, 0.0, 26.0, 25.19614758419054, 0.4151683833572427, 0.0, 1.0, 43074.70186700125], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1740000.0000, 
sim time next is 1740600.0000, 
raw observation next is [-0.3, 89.0, 0.0, 0.0, 26.0, 25.17291966113001, 0.4097999240423305, 0.0, 1.0, 43110.46512040404], 
processed observation next is [0.0, 0.13043478260869565, 0.4542936288088643, 0.89, 0.0, 0.0, 0.6666666666666666, 0.5977433050941675, 0.6365999746807768, 0.0, 1.0, 0.20528792914478114], 
reward next is 0.7947, 
noisyNet noise sample is [array([-0.23826414], dtype=float32), -0.34740895]. 
=============================================
[2019-04-04 03:23:18,922] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.9616488e-27 9.6874555e-23 3.9677805e-25 2.7829132e-23 3.9237433e-23
 2.1460155e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:23:18,923] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6148
[2019-04-04 03:23:18,935] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.3, 82.0, 0.0, 0.0, 26.0, 24.80076006596724, 0.2686743964070832, 0.0, 1.0, 42204.93641430655], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2156400.0000, 
sim time next is 2157000.0000, 
raw observation next is [-7.3, 82.00000000000001, 0.0, 0.0, 26.0, 24.73039569730869, 0.2553642701049253, 0.0, 1.0, 42245.82307659354], 
processed observation next is [1.0, 1.0, 0.26038781163434904, 0.8200000000000002, 0.0, 0.0, 0.6666666666666666, 0.5608663081090576, 0.5851214233683084, 0.0, 1.0, 0.20117058607901683], 
reward next is 0.7988, 
noisyNet noise sample is [array([0.65878856], dtype=float32), 0.09066179]. 
=============================================
[2019-04-04 03:23:18,939] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[77.97594 ]
 [77.932625]
 [78.018326]
 [78.14309 ]
 [78.24968 ]], R is [[78.12956238]
 [78.14729309]
 [78.16506195]
 [78.18294525]
 [78.19983673]].
[2019-04-04 03:23:26,200] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.4717184e-25 1.0234457e-21 3.3515990e-24 2.2187583e-22 2.6848001e-22
 4.5022629e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 03:23:26,200] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4106
[2019-04-04 03:23:26,247] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 79.0, 167.0, 70.0, 26.0, 25.02572517597086, 0.2847397046276891, 0.0, 1.0, 43144.55794133589], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1867200.0000, 
sim time next is 1867800.0000, 
raw observation next is [-4.5, 81.0, 148.0, 56.00000000000001, 26.0, 25.03170838759691, 0.2859354279134825, 0.0, 1.0, 39539.85861198878], 
processed observation next is [0.0, 0.6086956521739131, 0.3379501385041552, 0.81, 0.49333333333333335, 0.06187845303867404, 0.6666666666666666, 0.5859756989664092, 0.5953118093044941, 0.0, 1.0, 0.18828504100947036], 
reward next is 0.8117, 
noisyNet noise sample is [array([0.8491126], dtype=float32), 1.104488]. 
=============================================
[2019-04-04 03:23:26,925] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.8895863e-27 6.9446051e-23 1.6371060e-25 1.8589894e-23 2.5841234e-23
 6.3390621e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:23:26,925] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2994
[2019-04-04 03:23:27,017] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.45577426068806, 0.1849205436057038, 0.0, 1.0, 42651.76270200649], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1987200.0000, 
sim time next is 1987800.0000, 
raw observation next is [-5.7, 83.66666666666667, 0.0, 0.0, 26.0, 24.43500750633716, 0.1779633877250307, 0.0, 1.0, 42539.22173990975], 
processed observation next is [1.0, 0.0, 0.30470914127423826, 0.8366666666666667, 0.0, 0.0, 0.6666666666666666, 0.5362506255280968, 0.5593211292416769, 0.0, 1.0, 0.2025677225709988], 
reward next is 0.7974, 
noisyNet noise sample is [array([-0.4081563], dtype=float32), 1.2129632]. 
=============================================
[2019-04-04 03:23:40,378] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.7539681e-25 2.6879064e-21 1.8786356e-24 2.9292806e-22 8.0166171e-22
 2.2993116e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 03:23:40,378] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4584
[2019-04-04 03:23:40,411] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.7, 43.83333333333334, 0.0, 0.0, 26.0, 24.55372687203524, 0.1416368753548356, 0.0, 1.0, 43138.21355830986], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2419800.0000, 
sim time next is 2420400.0000, 
raw observation next is [-5.8, 44.66666666666667, 0.0, 0.0, 26.0, 24.52508120054438, 0.1422611338007609, 0.0, 1.0, 43146.58261177186], 
processed observation next is [0.0, 0.0, 0.30193905817174516, 0.4466666666666667, 0.0, 0.0, 0.6666666666666666, 0.5437567667120318, 0.547420377933587, 0.0, 1.0, 0.20545991719891363], 
reward next is 0.7945, 
noisyNet noise sample is [array([2.4660728], dtype=float32), -0.4502092]. 
=============================================
[2019-04-04 03:23:43,086] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4093913e-24 2.3885234e-20 1.7894101e-23 7.1879133e-22 2.6185295e-21
 2.3916277e-24 1.0000000e+00], sum to 1.0000
[2019-04-04 03:23:43,086] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5543
[2019-04-04 03:23:43,203] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.716666666666667, 54.66666666666667, 0.0, 0.0, 26.0, 23.89571443485238, -0.003121688060446525, 0.0, 1.0, 43783.77285202053], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2429400.0000, 
sim time next is 2430000.0000, 
raw observation next is [-7.8, 55.0, 0.0, 0.0, 26.0, 23.84654949357417, -0.01017070406111297, 0.0, 1.0, 43872.65677873355], 
processed observation next is [0.0, 0.13043478260869565, 0.24653739612188366, 0.55, 0.0, 0.0, 0.6666666666666666, 0.48721245779784744, 0.49660976531296236, 0.0, 1.0, 0.20891741323206453], 
reward next is 0.7911, 
noisyNet noise sample is [array([-2.0894463], dtype=float32), 1.4064645]. 
=============================================
[2019-04-04 03:23:43,249] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[77.90277 ]
 [77.956345]
 [78.01724 ]
 [78.12333 ]
 [78.24903 ]], R is [[77.88986206]
 [77.90246582]
 [77.91532898]
 [77.92834473]
 [77.94141388]].
[2019-04-04 03:23:46,256] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.0769675e-27 2.4448915e-23 3.4662542e-26 4.8167567e-24 6.8652395e-24
 1.2642699e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:23:46,256] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7220
[2019-04-04 03:23:46,334] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.700000000000001, 78.0, 0.0, 0.0, 26.0, 24.37081712083968, 0.1707564950661233, 0.0, 1.0, 42569.45129801419], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2167800.0000, 
sim time next is 2168400.0000, 
raw observation next is [-6.700000000000001, 78.0, 0.0, 0.0, 26.0, 24.49007990360808, 0.169993000209038, 0.0, 1.0, 42515.78503846233], 
processed observation next is [1.0, 0.08695652173913043, 0.2770083102493075, 0.78, 0.0, 0.0, 0.6666666666666666, 0.54083999196734, 0.5566643334030127, 0.0, 1.0, 0.202456119230773], 
reward next is 0.7975, 
noisyNet noise sample is [array([-0.24725337], dtype=float32), 1.2044113]. 
=============================================
[2019-04-04 03:23:48,911] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4318890e-28 1.3126860e-23 6.4490400e-27 9.4722818e-25 5.4422345e-25
 1.6321650e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:23:48,911] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3741
[2019-04-04 03:23:48,953] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.1, 71.66666666666667, 107.0, 300.6666666666667, 26.0, 26.02729742006818, 0.4225753403472534, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2195400.0000, 
sim time next is 2196000.0000, 
raw observation next is [-5.0, 71.0, 109.5, 225.5, 26.0, 26.01552172714307, 0.4176920555865027, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.32409972299168976, 0.71, 0.365, 0.24917127071823206, 0.6666666666666666, 0.6679601439285893, 0.6392306851955009, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.01991642], dtype=float32), -0.19311917]. 
=============================================
[2019-04-04 03:23:48,969] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[87.37786]
 [88.21674]
 [89.20952]
 [89.29411]
 [89.30251]], R is [[86.89228058]
 [87.02336121]
 [87.15312958]
 [87.28160095]
 [87.40878296]].
[2019-04-04 03:23:57,190] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.4422092e-29 1.5816500e-24 9.8526108e-27 6.3109866e-24 1.2006333e-24
 9.4077675e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 03:23:57,192] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2606
[2019-04-04 03:23:57,211] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 54.0, 234.5, 159.0, 26.0, 25.75122151300686, 0.3944796894202997, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2635200.0000, 
sim time next is 2635800.0000, 
raw observation next is [-2.016666666666667, 52.83333333333334, 238.0, 155.0, 26.0, 25.75410879292788, 0.3978622046072235, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.4067405355493998, 0.5283333333333334, 0.7933333333333333, 0.1712707182320442, 0.6666666666666666, 0.64617573274399, 0.6326207348690746, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.43303978], dtype=float32), -0.7065743]. 
=============================================
[2019-04-04 03:23:59,547] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.9612202e-27 6.2297393e-22 2.3153484e-25 1.7218833e-23 7.5194001e-23
 3.8988501e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:23:59,547] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0022
[2019-04-04 03:23:59,607] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 69.0, 79.0, 180.0, 26.0, 25.33190534754004, 0.3229324590057953, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2365200.0000, 
sim time next is 2365800.0000, 
raw observation next is [-3.3, 68.33333333333333, 93.0, 240.0, 26.0, 25.33185225843462, 0.3223504808411803, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.37119113573407203, 0.6833333333333332, 0.31, 0.26519337016574585, 0.6666666666666666, 0.610987688202885, 0.6074501602803934, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.01043684], dtype=float32), 0.53826433]. 
=============================================
[2019-04-04 03:24:17,229] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.8986810e-28 1.1870077e-24 8.6938373e-27 3.0212947e-24 1.6335075e-24
 1.8783237e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:24:17,231] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2975
[2019-04-04 03:24:17,292] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.4166666666666667, 35.83333333333333, 0.0, 0.0, 26.0, 25.01278699963433, 0.3195993129907079, 1.0, 1.0, 82167.01313448175], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2573400.0000, 
sim time next is 2574000.0000, 
raw observation next is [-0.6, 36.0, 0.0, 0.0, 26.0, 24.97583633636908, 0.3397244153446112, 1.0, 1.0, 81781.82283234474], 
processed observation next is [1.0, 0.8260869565217391, 0.44598337950138506, 0.36, 0.0, 0.0, 0.6666666666666666, 0.5813196946974234, 0.613241471781537, 1.0, 1.0, 0.389437251582594], 
reward next is 0.6106, 
noisyNet noise sample is [array([-0.89830554], dtype=float32), 0.6655238]. 
=============================================
[2019-04-04 03:24:17,300] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[82.406525]
 [82.216774]
 [82.14912 ]
 [82.25061 ]
 [82.6352  ]], R is [[82.38592529]
 [82.17079163]
 [82.09037018]
 [82.26947021]
 [82.44677734]].
[2019-04-04 03:24:22,700] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.0414559e-26 4.4745498e-22 1.6025108e-25 8.8005599e-24 2.4790863e-23
 1.8658211e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:24:22,700] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9210
[2019-04-04 03:24:22,726] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 42.0, 0.0, 0.0, 26.0, 25.15746206337576, 0.286818246293717, 0.0, 1.0, 43326.67371916811], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2406000.0000, 
sim time next is 2406600.0000, 
raw observation next is [-3.4, 42.0, 0.0, 0.0, 26.0, 25.15863164123137, 0.2810747596082467, 0.0, 1.0, 43113.74704714974], 
processed observation next is [0.0, 0.8695652173913043, 0.368421052631579, 0.42, 0.0, 0.0, 0.6666666666666666, 0.5965526367692808, 0.5936915865360822, 0.0, 1.0, 0.20530355736737974], 
reward next is 0.7947, 
noisyNet noise sample is [array([-0.22303504], dtype=float32), 0.2685089]. 
=============================================
[2019-04-04 03:24:23,051] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.9546124e-27 3.4440130e-22 2.6717900e-25 2.9629606e-23 2.7810455e-23
 8.4277385e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:24:23,052] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9267
[2019-04-04 03:24:23,069] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 44.0, 0.0, 0.0, 26.0, 24.9427864379217, 0.2363330366768966, 0.0, 1.0, 43007.79809687269], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2412000.0000, 
sim time next is 2412600.0000, 
raw observation next is [-4.583333333333333, 43.50000000000001, 0.0, 0.0, 26.0, 24.95842492294453, 0.2322416641016164, 0.0, 1.0, 42981.42256811963], 
processed observation next is [0.0, 0.9565217391304348, 0.3356417359187443, 0.43500000000000005, 0.0, 0.0, 0.6666666666666666, 0.5798687435787109, 0.5774138880338722, 0.0, 1.0, 0.20467344080056968], 
reward next is 0.7953, 
noisyNet noise sample is [array([0.25639775], dtype=float32), -0.56397605]. 
=============================================
[2019-04-04 03:24:26,917] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.8741439e-28 2.7609718e-23 1.1444193e-26 9.1188268e-25 3.2589071e-24
 9.1242761e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:24:26,917] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.9224
[2019-04-04 03:24:26,954] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 65.0, 122.5, 183.0, 26.0, 25.84888873727002, 0.3632664000250728, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2628000.0000, 
sim time next is 2628600.0000, 
raw observation next is [-4.816666666666666, 64.5, 132.3333333333333, 213.6666666666667, 26.0, 25.81087476728168, 0.3637348747227365, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.32917820867959374, 0.645, 0.44111111111111095, 0.2360957642725599, 0.6666666666666666, 0.6509062306068065, 0.6212449582409122, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.22038393], dtype=float32), -0.10378271]. 
=============================================
[2019-04-04 03:24:29,049] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.9148992e-28 3.4647509e-24 1.7645370e-26 7.7789795e-24 2.6343757e-24
 2.5620126e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:24:29,049] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6632
[2019-04-04 03:24:29,146] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.666666666666667, 60.66666666666666, 112.3333333333333, 797.1666666666667, 26.0, 25.29426394596282, 0.4015509190729253, 1.0, 1.0, 166630.5289785643], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2724000.0000, 
sim time next is 2724600.0000, 
raw observation next is [-6.333333333333333, 59.83333333333334, 111.6666666666667, 795.3333333333334, 26.0, 24.58196828002751, 0.4310887986134332, 1.0, 1.0, 198773.4843706625], 
processed observation next is [1.0, 0.5217391304347826, 0.28716528162511545, 0.5983333333333334, 0.37222222222222234, 0.8788213627992634, 0.6666666666666666, 0.5484973566689592, 0.6436962662044777, 1.0, 1.0, 0.9465404017650596], 
reward next is 0.0535, 
noisyNet noise sample is [array([0.08602058], dtype=float32), -4.063331]. 
=============================================
[2019-04-04 03:24:32,792] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.2452889e-27 3.5616821e-23 1.9538809e-25 3.6442815e-23 2.0395519e-23
 1.9357768e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:24:32,792] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7390
[2019-04-04 03:24:32,840] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.333333333333334, 60.66666666666667, 0.0, 0.0, 26.0, 25.195018684665, 0.4244829775713408, 0.0, 1.0, 180518.6023956092], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2751600.0000, 
sim time next is 2752200.0000, 
raw observation next is [-5.5, 61.5, 0.0, 0.0, 26.0, 25.20103272005189, 0.4414903568379375, 0.0, 1.0, 112343.6011780613], 
processed observation next is [1.0, 0.8695652173913043, 0.3102493074792244, 0.615, 0.0, 0.0, 0.6666666666666666, 0.6000860600043243, 0.6471634522793125, 0.0, 1.0, 0.5349695294193395], 
reward next is 0.4650, 
noisyNet noise sample is [array([0.03237234], dtype=float32), 0.5542936]. 
=============================================
[2019-04-04 03:24:34,748] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.67721499e-27 6.64702327e-22 4.11620225e-25 1.20165565e-23
 3.18250565e-23 1.97511003e-26 1.00000000e+00], sum to 1.0000
[2019-04-04 03:24:34,748] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4922
[2019-04-04 03:24:34,786] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.0, 64.0, 0.0, 0.0, 26.0, 24.3254954628516, 0.1144025669363925, 0.0, 1.0, 41105.60024162859], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2783400.0000, 
sim time next is 2784000.0000, 
raw observation next is [-7.0, 64.0, 0.0, 0.0, 26.0, 24.318504843732, 0.1113574496505897, 0.0, 1.0, 41158.01254702439], 
processed observation next is [1.0, 0.21739130434782608, 0.2686980609418283, 0.64, 0.0, 0.0, 0.6666666666666666, 0.5265420703110001, 0.5371191498835299, 0.0, 1.0, 0.19599053593821136], 
reward next is 0.8040, 
noisyNet noise sample is [array([-0.27275777], dtype=float32), -1.3362598]. 
=============================================
[2019-04-04 03:24:34,789] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[81.82217 ]
 [81.907715]
 [82.02859 ]
 [82.13645 ]
 [82.225136]], R is [[81.77256012]
 [81.75909424]
 [81.74617004]
 [81.73383331]
 [81.72198486]].
[2019-04-04 03:24:43,543] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.6971424e-27 6.9021372e-23 1.6431374e-25 1.6846286e-23 1.2754916e-23
 4.2955445e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:24:43,543] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5479
[2019-04-04 03:24:43,597] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 100.0, 87.5, 0.0, 26.0, 25.66150895762087, 0.4382999572098387, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2905200.0000, 
sim time next is 2905800.0000, 
raw observation next is [2.0, 100.0, 86.66666666666666, 0.0, 26.0, 25.82550263267517, 0.4466750237731732, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.518005540166205, 1.0, 0.28888888888888886, 0.0, 0.6666666666666666, 0.6521252193895976, 0.6488916745910577, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.74305606], dtype=float32), 0.22878747]. 
=============================================
[2019-04-04 03:24:50,955] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.0464685e-27 1.7276469e-23 1.9993691e-25 5.4001353e-23 5.0904431e-23
 3.5412448e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:24:50,956] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1381
[2019-04-04 03:24:51,000] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 87.33333333333334, 0.0, 0.0, 26.0, 25.16909837404203, 0.3853225876765645, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2920800.0000, 
sim time next is 2921400.0000, 
raw observation next is [-1.0, 85.0, 0.0, 0.0, 26.0, 25.04050753596379, 0.3775653204153253, 1.0, 1.0, 63032.74195368057], 
processed observation next is [1.0, 0.8260869565217391, 0.4349030470914128, 0.85, 0.0, 0.0, 0.6666666666666666, 0.5867089613303159, 0.6258551068051085, 1.0, 1.0, 0.3001559140651456], 
reward next is 0.6998, 
noisyNet noise sample is [array([0.9522952], dtype=float32), 0.16589363]. 
=============================================
[2019-04-04 03:24:55,409] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0637161e-28 4.3188991e-24 1.9567442e-26 1.9422889e-24 1.8716184e-24
 1.1430657e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:24:55,418] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2202
[2019-04-04 03:24:55,476] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 87.33333333333334, 0.0, 0.0, 26.0, 25.16909837404203, 0.3853225876765645, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2920800.0000, 
sim time next is 2921400.0000, 
raw observation next is [-1.0, 85.0, 0.0, 0.0, 26.0, 25.04050753596379, 0.3775653204153253, 1.0, 1.0, 63032.74195368057], 
processed observation next is [1.0, 0.8260869565217391, 0.4349030470914128, 0.85, 0.0, 0.0, 0.6666666666666666, 0.5867089613303159, 0.6258551068051085, 1.0, 1.0, 0.3001559140651456], 
reward next is 0.6998, 
noisyNet noise sample is [array([-0.2983266], dtype=float32), -0.5626549]. 
=============================================
[2019-04-04 03:24:57,477] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [4.8207752e-27 8.0345231e-23 1.1043629e-25 1.5946276e-23 2.1803293e-23
 2.3560380e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:24:57,478] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.7701
[2019-04-04 03:24:57,495] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.833333333333334, 70.0, 0.0, 0.0, 26.0, 24.66737736153852, 0.2066125098914877, 0.0, 1.0, 37899.45947301941], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3027000.0000, 
sim time next is 3027600.0000, 
raw observation next is [-5.0, 71.0, 0.0, 0.0, 26.0, 24.62572542205321, 0.1974654482698006, 0.0, 1.0, 37953.66652884999], 
processed observation next is [0.0, 0.043478260869565216, 0.32409972299168976, 0.71, 0.0, 0.0, 0.6666666666666666, 0.552143785171101, 0.5658218160899335, 0.0, 1.0, 0.18073174537547615], 
reward next is 0.8193, 
noisyNet noise sample is [array([-0.15283331], dtype=float32), 0.89448977]. 
=============================================
[2019-04-04 03:24:59,017] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.4236185e-29 6.2032207e-24 1.2868034e-26 6.8441345e-25 9.3993931e-25
 4.9083023e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:24:59,017] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6356
[2019-04-04 03:24:59,040] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.0, 100.0, 1.0, 82.0, 26.0, 25.43265117390358, 0.3279781456274151, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3137400.0000, 
sim time next is 3138000.0000, 
raw observation next is [6.0, 100.0, 14.66666666666666, 133.6666666666667, 26.0, 25.47584705962083, 0.3369492431957855, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.6288088642659281, 1.0, 0.04888888888888887, 0.1476979742173113, 0.6666666666666666, 0.6229872549684025, 0.6123164143985952, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7385887], dtype=float32), 0.27724382]. 
=============================================
[2019-04-04 03:24:59,108] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[87.553444]
 [86.24003 ]
 [85.153824]
 [85.09746 ]
 [84.95426 ]], R is [[88.67367554]
 [88.78694153]
 [88.89907074]
 [88.92074585]
 [88.83914185]].
[2019-04-04 03:25:02,830] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.2832240e-28 7.8334740e-24 4.1380999e-26 8.8551814e-24 5.4337942e-24
 4.9053826e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:25:02,831] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3097
[2019-04-04 03:25:02,841] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 93.0, 0.0, 0.0, 26.0, 25.56634244689427, 0.6325458203339445, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3194400.0000, 
sim time next is 3195000.0000, 
raw observation next is [2.0, 93.0, 0.0, 0.0, 26.0, 25.59432046310412, 0.6257093286045742, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.518005540166205, 0.93, 0.0, 0.0, 0.6666666666666666, 0.6328600385920099, 0.7085697762015247, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.49346313], dtype=float32), -1.6271394]. 
=============================================
[2019-04-04 03:25:02,855] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[82.52902 ]
 [82.52122 ]
 [82.658936]
 [82.70265 ]
 [82.45912 ]], R is [[82.65488434]
 [82.82833862]
 [83.00005341]
 [82.96188354]
 [82.57025146]].
[2019-04-04 03:25:12,478] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.9239318e-27 4.6228937e-23 1.8860234e-25 2.3584425e-23 1.4677682e-23
 4.6432445e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:25:12,478] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9148
[2019-04-04 03:25:12,503] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.60614969089775, 0.5235719533134628, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3360000.0000, 
sim time next is 3360600.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.56478975273279, 0.5090533973530371, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6303991460610657, 0.6696844657843458, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0946326], dtype=float32), -0.3225894]. 
=============================================
[2019-04-04 03:25:12,695] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.1327689e-27 7.7171221e-23 6.3351250e-26 5.3488982e-24 7.2047889e-24
 4.1598122e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:25:12,696] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3981
[2019-04-04 03:25:12,770] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 72.0, 61.00000000000001, 331.3333333333334, 26.0, 25.19161835805996, 0.3035227619881227, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3744600.0000, 
sim time next is 3745200.0000, 
raw observation next is [-4.0, 73.0, 75.0, 380.1666666666667, 26.0, 25.18966495568438, 0.3319790735082151, 1.0, 1.0, 9360.018379773128], 
processed observation next is [1.0, 0.34782608695652173, 0.3518005540166205, 0.73, 0.25, 0.42007366482504604, 0.6666666666666666, 0.5991387463070316, 0.610659691169405, 1.0, 1.0, 0.04457151609415775], 
reward next is 0.9554, 
noisyNet noise sample is [array([-1.1227223], dtype=float32), -0.7727599]. 
=============================================
[2019-04-04 03:25:13,385] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.0393116e-27 1.7052150e-23 5.2647603e-26 4.1280610e-24 1.4069694e-23
 1.4051483e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:25:13,386] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9609
[2019-04-04 03:25:13,400] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.1666666666666666, 42.83333333333334, 78.33333333333334, 637.6666666666667, 26.0, 25.35009482877533, 0.4665717693599697, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3599400.0000, 
sim time next is 3600000.0000, 
raw observation next is [0.0, 43.0, 74.5, 607.0, 26.0, 25.33859256288569, 0.46049676187049, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.46260387811634357, 0.43, 0.24833333333333332, 0.6707182320441989, 0.6666666666666666, 0.6115493802404742, 0.6534989206234967, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.11668795], dtype=float32), -1.6484015]. 
=============================================
[2019-04-04 03:25:13,413] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[85.45217 ]
 [85.60028 ]
 [85.76034 ]
 [85.793106]
 [85.81217 ]], R is [[85.49739838]
 [85.64242554]
 [85.78600311]
 [85.92814636]
 [86.06886292]].
[2019-04-04 03:25:14,892] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [8.6876712e-27 7.8436781e-22 1.7399735e-25 4.5482372e-23 4.5651851e-23
 3.4951465e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:25:14,895] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8835
[2019-04-04 03:25:14,923] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 48.33333333333334, 0.0, 0.0, 26.0, 25.31639464522011, 0.3707691383639962, 0.0, 1.0, 38478.60943869619], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3626400.0000, 
sim time next is 3627000.0000, 
raw observation next is [3.0, 42.5, 0.0, 0.0, 26.0, 25.35875459764781, 0.3803946983859938, 0.0, 1.0, 38349.54984108561], 
processed observation next is [0.0, 1.0, 0.5457063711911359, 0.425, 0.0, 0.0, 0.6666666666666666, 0.6132295498039841, 0.6267982327953313, 0.0, 1.0, 0.18261690400516956], 
reward next is 0.8174, 
noisyNet noise sample is [array([0.76483816], dtype=float32), 0.7926182]. 
=============================================
[2019-04-04 03:25:14,949] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[84.6956 ]
 [83.89791]
 [83.47763]
 [83.59851]
 [83.69576]], R is [[85.79543304]
 [85.75424194]
 [85.71300507]
 [85.67031097]
 [85.62177277]].
[2019-04-04 03:25:17,398] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.9462568e-28 1.7731489e-24 1.0580475e-26 5.5326022e-24 2.7659357e-24
 1.0100534e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:25:17,398] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1796
[2019-04-04 03:25:17,407] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.666666666666666, 54.00000000000001, 117.6666666666667, 808.8333333333334, 26.0, 26.36468254809888, 0.5898986169718059, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3327600.0000, 
sim time next is 3328200.0000, 
raw observation next is [-5.5, 54.0, 118.0, 811.0, 26.0, 26.24678252315418, 0.5773457359469171, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.3102493074792244, 0.54, 0.3933333333333333, 0.8961325966850828, 0.6666666666666666, 0.687231876929515, 0.6924485786489724, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5439943], dtype=float32), -0.75226516]. 
=============================================
[2019-04-04 03:25:18,068] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.9398473e-28 5.7299772e-24 1.3947300e-26 3.5484577e-24 1.6581874e-24
 1.0918115e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:25:18,072] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0985
[2019-04-04 03:25:18,095] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 26.0, 25.15815893315217, 0.5120168556426314, 0.0, 1.0, 101774.7258670429], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3444000.0000, 
sim time next is 3444600.0000, 
raw observation next is [1.0, 79.0, 0.0, 0.0, 26.0, 25.29351395095959, 0.5363946758886117, 0.0, 1.0, 62801.50195586943], 
processed observation next is [1.0, 0.8695652173913043, 0.4903047091412743, 0.79, 0.0, 0.0, 0.6666666666666666, 0.6077928292466325, 0.6787982252962039, 0.0, 1.0, 0.2990547712184259], 
reward next is 0.7009, 
noisyNet noise sample is [array([0.7069064], dtype=float32), -1.7532067]. 
=============================================
[2019-04-04 03:25:21,798] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.9988550e-28 1.3348285e-23 5.0209299e-26 8.8415451e-24 5.8267645e-24
 5.7419781e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:25:21,801] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3155
[2019-04-04 03:25:21,814] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 56.66666666666667, 0.0, 0.0, 26.0, 25.75110499856644, 0.563032262123541, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3879600.0000, 
sim time next is 3880200.0000, 
raw observation next is [-1.0, 55.83333333333334, 0.0, 0.0, 26.0, 25.74083874064026, 0.5507731326037043, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.4349030470914128, 0.5583333333333335, 0.0, 0.0, 0.6666666666666666, 0.645069895053355, 0.6835910442012348, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.04540002], dtype=float32), -2.8390818]. 
=============================================
[2019-04-04 03:25:22,325] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [8.1987175e-29 1.1708447e-24 4.0287381e-27 5.2292580e-24 8.9442262e-25
 1.2638594e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:25:22,326] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7002
[2019-04-04 03:25:22,338] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 60.00000000000001, 90.16666666666666, 721.8333333333333, 26.0, 26.79687302229286, 0.6963499324265249, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3770400.0000, 
sim time next is 3771000.0000, 
raw observation next is [0.0, 60.0, 87.0, 711.0, 26.0, 26.82445252085725, 0.6960767746584393, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.46260387811634357, 0.6, 0.29, 0.7856353591160221, 0.6666666666666666, 0.7353710434047708, 0.7320255915528131, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2590003], dtype=float32), 1.4323232]. 
=============================================
[2019-04-04 03:25:22,343] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[84.94564 ]
 [85.064865]
 [85.184555]
 [85.32301 ]
 [85.460655]], R is [[84.9546051 ]
 [85.10505676]
 [85.25400543]
 [85.40146637]
 [85.54745483]].
[2019-04-04 03:25:26,374] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.3395333e-28 6.6788128e-24 7.8602524e-27 2.1431217e-24 8.7084840e-25
 1.1210251e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:25:26,377] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5298
[2019-04-04 03:25:26,400] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 46.33333333333334, 115.3333333333333, 806.1666666666666, 26.0, 26.59255329158398, 0.6201942954823244, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3411600.0000, 
sim time next is 3412200.0000, 
raw observation next is [3.0, 45.66666666666666, 115.6666666666667, 808.3333333333334, 26.0, 26.1855672481751, 0.6058001517900448, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.5457063711911359, 0.45666666666666655, 0.38555555555555565, 0.8931860036832413, 0.6666666666666666, 0.6821306040145917, 0.7019333839300149, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7156742], dtype=float32), -0.08482779]. 
=============================================
[2019-04-04 03:25:29,068] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.4444938e-26 6.3346545e-22 8.0294897e-25 2.5035771e-23 4.4255925e-22
 2.5105739e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:25:29,069] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1553
[2019-04-04 03:25:29,095] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.08425321200666, 0.3425231949454066, 0.0, 1.0, 37446.43955733546], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3609600.0000, 
sim time next is 3610200.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.06397668177818, 0.3496051303539665, 0.0, 1.0, 41979.96632784978], 
processed observation next is [0.0, 0.782608695652174, 0.4349030470914128, 0.42, 0.0, 0.0, 0.6666666666666666, 0.5886647234815149, 0.6165350434513222, 0.0, 1.0, 0.19990460156118944], 
reward next is 0.8001, 
noisyNet noise sample is [array([2.2571864], dtype=float32), 0.7740916]. 
=============================================
[2019-04-04 03:25:34,515] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.5174252e-26 5.5516774e-22 1.0545993e-24 1.2357732e-22 1.7773357e-22
 1.0157813e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 03:25:34,517] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3737
[2019-04-04 03:25:34,540] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.0, 59.0, 0.0, 0.0, 26.0, 25.14782173598343, 0.3408101011779068, 0.0, 1.0, 29200.1143963684], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3693600.0000, 
sim time next is 3694200.0000, 
raw observation next is [4.0, 59.0, 0.0, 0.0, 26.0, 25.11284839973153, 0.3322393911737678, 0.0, 1.0, 36975.20814041291], 
processed observation next is [0.0, 0.782608695652174, 0.5734072022160666, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5927373666442941, 0.6107464637245893, 0.0, 1.0, 0.17607241971625195], 
reward next is 0.8239, 
noisyNet noise sample is [array([0.01700848], dtype=float32), -0.3033223]. 
=============================================
[2019-04-04 03:25:35,155] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5279644e-27 1.3293361e-22 1.4196484e-25 8.0582457e-24 1.0403986e-23
 2.8526324e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:25:35,158] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7174
[2019-04-04 03:25:35,175] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.833333333333334, 40.5, 0.0, 0.0, 26.0, 24.62975774371055, 0.1723898376062818, 0.0, 1.0, 39976.56148640112], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4085400.0000, 
sim time next is 4086000.0000, 
raw observation next is [-5.0, 41.0, 0.0, 0.0, 26.0, 24.60567550565592, 0.1670945690778189, 0.0, 1.0, 39927.34674930925], 
processed observation next is [1.0, 0.30434782608695654, 0.32409972299168976, 0.41, 0.0, 0.0, 0.6666666666666666, 0.5504729588046601, 0.5556981896926063, 0.0, 1.0, 0.19013022261575832], 
reward next is 0.8099, 
noisyNet noise sample is [array([-1.4805211], dtype=float32), 0.3554499]. 
=============================================
[2019-04-04 03:25:35,183] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[84.29822 ]
 [84.4625  ]
 [84.60703 ]
 [84.743126]
 [84.829056]], R is [[84.09572601]
 [84.06440735]
 [84.03314972]
 [84.00198364]
 [83.97097015]].
[2019-04-04 03:25:36,521] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.3618859e-26 3.1812448e-22 5.5435218e-25 2.2611636e-23 7.2219629e-23
 4.0118551e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:25:36,523] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4092
[2019-04-04 03:25:36,537] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 70.0, 0.0, 0.0, 26.0, 25.36871705704424, 0.3857741006193657, 0.0, 1.0, 43145.02623295874], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3714600.0000, 
sim time next is 3715200.0000, 
raw observation next is [-3.0, 71.0, 0.0, 0.0, 26.0, 25.3683308743645, 0.3858237095324888, 0.0, 1.0, 41531.14267666452], 
processed observation next is [1.0, 0.0, 0.3795013850415513, 0.71, 0.0, 0.0, 0.6666666666666666, 0.6140275728637082, 0.6286079031774963, 0.0, 1.0, 0.19776734607935487], 
reward next is 0.8022, 
noisyNet noise sample is [array([-0.41628292], dtype=float32), -1.5525156]. 
=============================================
[2019-04-04 03:25:44,622] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.8226402e-27 6.0119551e-24 7.3699848e-26 2.5328662e-23 2.7981676e-23
 1.9206612e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:25:44,622] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8162
[2019-04-04 03:25:44,637] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 36.0, 0.0, 0.0, 26.0, 25.71966180432339, 0.5897693970202159, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4136400.0000, 
sim time next is 4137000.0000, 
raw observation next is [1.0, 36.66666666666666, 0.0, 0.0, 26.0, 25.86550842223248, 0.5939934594095875, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.4903047091412743, 0.3666666666666666, 0.0, 0.0, 0.6666666666666666, 0.65545903518604, 0.6979978198031959, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.11207841], dtype=float32), 0.7835711]. 
=============================================
[2019-04-04 03:25:44,649] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[81.12565 ]
 [81.18415 ]
 [81.338715]
 [80.88766 ]
 [80.697586]], R is [[81.21523285]
 [81.4030838 ]
 [81.37726593]
 [80.69820404]
 [79.94772339]].
[2019-04-04 03:25:47,224] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.2125415e-27 1.5179410e-22 9.9907271e-25 5.2506086e-23 6.5254208e-23
 3.8105680e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:25:47,224] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1659
[2019-04-04 03:25:47,249] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.333333333333334, 74.0, 0.0, 0.0, 26.0, 25.21223127589737, 0.3529539074027324, 0.0, 1.0, 41019.62105874462], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3906600.0000, 
sim time next is 3907200.0000, 
raw observation next is [-4.666666666666667, 71.0, 0.0, 0.0, 26.0, 25.21442428862093, 0.3415776827169387, 0.0, 1.0, 41100.13441992427], 
processed observation next is [1.0, 0.21739130434782608, 0.3333333333333333, 0.71, 0.0, 0.0, 0.6666666666666666, 0.601202024051744, 0.6138592275723129, 0.0, 1.0, 0.19571492580916316], 
reward next is 0.8043, 
noisyNet noise sample is [array([1.0819765], dtype=float32), -2.3968523]. 
=============================================
[2019-04-04 03:26:00,654] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.4894983e-27 1.0146950e-21 3.8965772e-25 1.9766810e-23 4.5909092e-23
 1.3855664e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:26:00,656] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7304
[2019-04-04 03:26:00,670] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.333333333333333, 39.0, 0.0, 0.0, 26.0, 25.18025508233954, 0.3116088969612595, 0.0, 1.0, 40783.51217574502], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4070400.0000, 
sim time next is 4071000.0000, 
raw observation next is [-5.166666666666667, 38.5, 0.0, 0.0, 26.0, 25.16051225477801, 0.3040764354264973, 0.0, 1.0, 40753.90800941098], 
processed observation next is [1.0, 0.08695652173913043, 0.31948291782086796, 0.385, 0.0, 0.0, 0.6666666666666666, 0.5967093545648341, 0.6013588118088324, 0.0, 1.0, 0.19406622861624276], 
reward next is 0.8059, 
noisyNet noise sample is [array([-1.1202366], dtype=float32), -0.28106412]. 
=============================================
[2019-04-04 03:26:00,675] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[83.80925 ]
 [83.753975]
 [83.72615 ]
 [83.70232 ]
 [83.66658 ]], R is [[83.82394409]
 [83.79149628]
 [83.75924683]
 [83.72717285]
 [83.69527435]].
[2019-04-04 03:26:06,450] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.5110093e-28 2.8973728e-23 9.8203037e-27 2.2170216e-24 8.0500075e-25
 1.1406443e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:26:06,452] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7925
[2019-04-04 03:26:06,495] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.0, 55.0, 162.5, 713.0, 26.0, 25.27084845983894, 0.398165575875906, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4273200.0000, 
sim time next is 4273800.0000, 
raw observation next is [5.333333333333334, 54.5, 148.6666666666667, 749.3333333333334, 26.0, 25.25425645486551, 0.3972033807040876, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.6103416435826409, 0.545, 0.4955555555555557, 0.8279926335174954, 0.6666666666666666, 0.6045213712387923, 0.6324011269013625, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8374895], dtype=float32), -0.3110029]. 
=============================================
[2019-04-04 03:26:08,248] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.4184946e-26 2.7033282e-23 7.2359333e-26 1.3854682e-23 1.5692106e-23
 6.3631443e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:26:08,248] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5299
[2019-04-04 03:26:08,259] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.333333333333333, 39.0, 60.66666666666667, 485.5000000000001, 26.0, 25.5232952207562, 0.4350182510691638, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4207200.0000, 
sim time next is 4207800.0000, 
raw observation next is [2.166666666666667, 39.5, 53.33333333333334, 421.0, 26.0, 25.49846554219806, 0.4198315953103999, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.5226223453370269, 0.395, 0.1777777777777778, 0.46519337016574586, 0.6666666666666666, 0.6248721285165049, 0.6399438651034667, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.55959773], dtype=float32), -1.5532231]. 
=============================================
[2019-04-04 03:26:09,622] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.8737354e-29 1.5379548e-24 9.5414624e-28 1.2385227e-25 1.4991386e-25
 4.7720280e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 03:26:09,625] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9166
[2019-04-04 03:26:09,652] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.3, 57.0, 99.5, 584.0, 26.0, 26.37378845354358, 0.5779263155712183, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4352400.0000, 
sim time next is 4353000.0000, 
raw observation next is [6.916666666666666, 54.5, 102.0, 615.0, 26.0, 26.43580449781494, 0.5946304853599416, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.6542012927054479, 0.545, 0.34, 0.6795580110497238, 0.6666666666666666, 0.7029837081512449, 0.6982101617866472, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.577361], dtype=float32), 0.73887575]. 
=============================================
[2019-04-04 03:26:09,666] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[89.86861]
 [89.41916]
 [89.20933]
 [89.11145]
 [88.43187]], R is [[90.51695251]
 [90.61178589]
 [90.70566559]
 [90.79860687]
 [90.890625  ]].
[2019-04-04 03:26:14,043] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.6829581e-27 4.3961110e-23 7.1761999e-26 6.2115558e-24 1.7011088e-23
 2.3248869e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:26:14,048] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1416
[2019-04-04 03:26:14,061] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.9, 58.0, 116.0, 655.0, 26.0, 25.47654040037177, 0.4641081440772945, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4289400.0000, 
sim time next is 4290000.0000, 
raw observation next is [6.933333333333334, 57.33333333333333, 108.3333333333333, 638.5, 26.0, 25.48446041010245, 0.4655822268135581, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.6546629732225301, 0.5733333333333333, 0.361111111111111, 0.7055248618784531, 0.6666666666666666, 0.6237050341752042, 0.6551940756045194, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.9654275], dtype=float32), 0.31479523]. 
=============================================
[2019-04-04 03:26:14,074] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[85.19968]
 [85.2495 ]
 [85.38185]
 [85.62654]
 [85.93481]], R is [[85.19889832]
 [85.34690857]
 [85.49343872]
 [85.63850403]
 [85.78211975]].
[2019-04-04 03:26:21,969] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.7808415e-26 1.4234255e-21 1.4600053e-24 4.0219897e-23 1.7690361e-22
 1.1521029e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 03:26:21,970] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4636
[2019-04-04 03:26:22,008] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.666666666666667, 67.33333333333333, 0.0, 0.0, 26.0, 24.81127718934649, 0.2322971703744712, 0.0, 1.0, 39244.21998023891], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4855200.0000, 
sim time next is 4855800.0000, 
raw observation next is [-3.833333333333333, 69.16666666666667, 0.0, 0.0, 26.0, 24.78805199118534, 0.2318685166996096, 0.0, 1.0, 39293.23576882304], 
processed observation next is [0.0, 0.17391304347826086, 0.3564173591874424, 0.6916666666666668, 0.0, 0.0, 0.6666666666666666, 0.5656709992654451, 0.5772895055665365, 0.0, 1.0, 0.18711064651820494], 
reward next is 0.8129, 
noisyNet noise sample is [array([-0.72010976], dtype=float32), -0.57246673]. 
=============================================
[2019-04-04 03:26:25,585] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0343014e-29 1.5809073e-25 1.9710587e-28 9.5297953e-26 3.8901443e-26
 6.9434416e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 03:26:25,586] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4998
[2019-04-04 03:26:25,602] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.6666666666666666, 57.33333333333334, 134.8333333333333, 724.0, 26.0, 26.55359774650552, 0.6613902107120356, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4616400.0000, 
sim time next is 4617000.0000, 
raw observation next is [1.0, 56.0, 129.0, 767.0, 26.0, 26.64746032834829, 0.6701905047337519, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4903047091412743, 0.56, 0.43, 0.8475138121546961, 0.6666666666666666, 0.7206216940290243, 0.7233968349112506, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4436885], dtype=float32), 0.107559934]. 
=============================================
[2019-04-04 03:26:25,614] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[91.826324]
 [91.91164 ]
 [92.00841 ]
 [92.18677 ]
 [92.52821 ]], R is [[91.83571625]
 [91.9173584 ]
 [91.9981842 ]
 [92.07820129]
 [92.1574173 ]].
[2019-04-04 03:26:28,523] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:26:28,523] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:26:28,564] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run29
[2019-04-04 03:26:28,803] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:26:28,804] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:26:28,812] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run29
[2019-04-04 03:26:29,172] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.7965804e-28 2.2674939e-24 3.4475286e-26 4.0020460e-24 2.1460669e-24
 2.8591456e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:26:29,175] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9457
[2019-04-04 03:26:29,205] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.5, 81.5, 0.0, 0.0, 26.0, 24.99953747844299, 0.4788736098190463, 0.0, 1.0, 131838.9833938584], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4739400.0000, 
sim time next is 4740000.0000, 
raw observation next is [-1.666666666666667, 82.66666666666667, 0.0, 0.0, 26.0, 25.09898827048732, 0.5020641238172933, 0.0, 1.0, 73966.59840148574], 
processed observation next is [1.0, 0.8695652173913043, 0.4164358264081256, 0.8266666666666667, 0.0, 0.0, 0.6666666666666666, 0.5915823558739435, 0.6673547079390977, 0.0, 1.0, 0.3522218971499321], 
reward next is 0.6478, 
noisyNet noise sample is [array([-0.39335978], dtype=float32), 1.2629507]. 
=============================================
[2019-04-04 03:26:29,221] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[82.07198 ]
 [81.79204 ]
 [81.57157 ]
 [81.786766]
 [82.58324 ]], R is [[81.92269135]
 [81.4756546 ]
 [80.71109772]
 [80.41805267]
 [80.21989441]].
[2019-04-04 03:26:31,911] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.0095885e-27 2.3806709e-22 1.7548047e-25 1.2901034e-23 1.8237846e-23
 9.8048816e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:26:31,916] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1268
[2019-04-04 03:26:31,929] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.1666666666666667, 93.33333333333334, 0.0, 0.0, 26.0, 25.5626617792319, 0.4781399148857495, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4680600.0000, 
sim time next is 4681200.0000, 
raw observation next is [-0.3333333333333333, 94.66666666666667, 0.0, 0.0, 26.0, 25.63847444803928, 0.4681094879118006, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.4533702677747, 0.9466666666666668, 0.0, 0.0, 0.6666666666666666, 0.6365395373366066, 0.6560364959706002, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6579794], dtype=float32), -1.8647661]. 
=============================================
[2019-04-04 03:26:35,409] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:26:35,409] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:26:35,412] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run29
[2019-04-04 03:26:37,629] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-04-04 03:26:37,635] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 03:26:37,636] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 03:26:37,642] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:26:37,644] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run39
[2019-04-04 03:26:37,637] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:26:37,636] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 03:26:37,721] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:26:37,723] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run39
[2019-04-04 03:26:37,777] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run39
[2019-04-04 03:27:04,314] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.36126176], dtype=float32), 0.18098658]
[2019-04-04 03:27:04,314] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-6.1, 84.0, 0.0, 0.0, 26.0, 25.02321854477355, 0.3049732386639709, 0.0, 1.0, 61983.98973124695]
[2019-04-04 03:27:04,314] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 03:27:04,315] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [7.7006090e-27 8.2604932e-23 1.4750523e-25 3.1425978e-23 2.4906507e-23
 7.5192696e-27 1.0000000e+00], sampled 0.5439122284388913
[2019-04-04 03:28:53,659] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.36126176], dtype=float32), 0.18098658]
[2019-04-04 03:28:53,659] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-0.4166666666666667, 53.33333333333333, 0.0, 0.0, 26.0, 25.46731998387422, 0.3937496913595345, 1.0, 1.0, 18702.0947792472]
[2019-04-04 03:28:53,659] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 03:28:53,661] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [8.4796900e-27 5.6030556e-23 1.0152041e-25 2.8742002e-23 1.9570831e-23
 5.2905881e-27 1.0000000e+00], sampled 0.1403751075860581
[2019-04-04 03:29:47,305] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 03:30:17,131] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 03:30:18,552] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.36126176], dtype=float32), 0.18098658]
[2019-04-04 03:30:18,552] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-2.485450268666667, 40.17146249333333, 0.0, 0.0, 26.0, 25.15420684385633, 0.270056999745359, 0.0, 1.0, 37371.06661342517]
[2019-04-04 03:30:18,553] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 03:30:18,553] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [4.3617938e-26 2.7219469e-21 1.6544975e-24 8.3800845e-23 2.1710515e-22
 2.2414861e-25 1.0000000e+00], sampled 0.3349248647409231
[2019-04-04 03:30:22,517] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 03:30:23,544] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 3800000, evaluation results [3800000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 03:30:23,733] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [4.8901293e-16 8.9263183e-13 3.1685635e-15 8.0286770e-14 3.2460325e-14
 2.1870022e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 03:30:23,733] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0676
[2019-04-04 03:30:23,747] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.0, 95.83333333333333, 0.0, 0.0, 26.0, 19.46234971485661, -0.9170681454360184, 0.0, 1.0, 52033.994786758], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3000.0000, 
sim time next is 3600.0000, 
raw observation next is [7.2, 96.0, 0.0, 0.0, 26.0, 19.64114642579297, -0.8903611708336857, 0.0, 1.0, 47427.48457553837], 
processed observation next is [0.0, 0.043478260869565216, 0.662049861495845, 0.96, 0.0, 0.0, 0.6666666666666666, 0.13676220214941429, 0.2032129430554381, 0.0, 1.0, 0.22584516464542082], 
reward next is 0.7742, 
noisyNet noise sample is [array([0.500474], dtype=float32), 1.0240097]. 
=============================================
[2019-04-04 03:30:26,592] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.1116005e-26 3.6654057e-22 5.0658167e-25 5.2878145e-23 6.1174276e-23
 3.3690031e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:30:26,592] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2231
[2019-04-04 03:30:26,639] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.866666666666667, 84.66666666666666, 15.0, 0.0, 26.0, 24.53955699601114, 0.1896098215059848, 0.0, 1.0, 34419.24430637214], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 60000.0000, 
sim time next is 60600.0000, 
raw observation next is [5.683333333333334, 85.33333333333334, 12.0, 0.0, 26.0, 24.54824014586847, 0.1890691172121965, 0.0, 1.0, 34829.88553126116], 
processed observation next is [0.0, 0.6956521739130435, 0.6200369344413666, 0.8533333333333334, 0.04, 0.0, 0.6666666666666666, 0.5456866788223724, 0.5630230390707321, 0.0, 1.0, 0.1658565977679103], 
reward next is 0.8341, 
noisyNet noise sample is [array([-0.5905994], dtype=float32), 0.7289605]. 
=============================================
[2019-04-04 03:30:28,645] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [7.4435029e-26 5.3729557e-21 1.5328826e-24 3.8635154e-23 4.2820905e-22
 4.1439943e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 03:30:28,647] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.1581
[2019-04-04 03:30:28,674] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 60.0, 0.0, 0.0, 26.0, 24.98567167088429, 0.2753736897993791, 0.0, 1.0, 39195.49813963148], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4849800.0000, 
sim time next is 4850400.0000, 
raw observation next is [-3.0, 60.00000000000001, 0.0, 0.0, 26.0, 24.95553235539923, 0.2694432766970734, 0.0, 1.0, 39210.04827775796], 
processed observation next is [0.0, 0.13043478260869565, 0.3795013850415513, 0.6000000000000001, 0.0, 0.0, 0.6666666666666666, 0.5796276962832693, 0.5898144255656911, 0.0, 1.0, 0.18671451560837124], 
reward next is 0.8133, 
noisyNet noise sample is [array([-1.4446558], dtype=float32), 3.07135]. 
=============================================
[2019-04-04 03:30:30,615] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.8776248e-26 1.8809619e-21 6.2583846e-25 5.4925450e-23 2.2792123e-22
 2.0143663e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 03:30:30,619] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2045
[2019-04-04 03:30:30,633] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.333333333333333, 61.66666666666667, 0.0, 0.0, 26.0, 24.60254703648043, 0.1814282145117248, 0.0, 1.0, 39491.55340868184], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4861200.0000, 
sim time next is 4861800.0000, 
raw observation next is [-3.5, 62.5, 0.0, 0.0, 26.0, 24.57684250125638, 0.1767932969518152, 0.0, 1.0, 39480.93899097785], 
processed observation next is [0.0, 0.2608695652173913, 0.36565096952908593, 0.625, 0.0, 0.0, 0.6666666666666666, 0.5480702084380317, 0.5589310989839383, 0.0, 1.0, 0.1880044713856088], 
reward next is 0.8120, 
noisyNet noise sample is [array([0.74384505], dtype=float32), 2.383326]. 
=============================================
[2019-04-04 03:30:34,343] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [8.9124064e-27 1.2279123e-21 8.9504385e-25 7.2665651e-23 8.0118436e-23
 3.0441283e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:30:34,344] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.2746
[2019-04-04 03:30:34,375] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.133333333333334, 74.66666666666667, 0.0, 0.0, 26.0, 23.44119574203863, -0.0518290429873613, 0.0, 1.0, 44691.4968876475], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 106800.0000, 
sim time next is 107400.0000, 
raw observation next is [-6.416666666666667, 74.83333333333333, 0.0, 0.0, 26.0, 23.37269961048132, -0.06395484839273313, 0.0, 1.0, 44885.9793918526], 
processed observation next is [1.0, 0.21739130434782608, 0.2848568790397045, 0.7483333333333333, 0.0, 0.0, 0.6666666666666666, 0.44772496754011, 0.47868171720242225, 0.0, 1.0, 0.2137427590088219], 
reward next is 0.7863, 
noisyNet noise sample is [array([2.1109416], dtype=float32), -1.6611915]. 
=============================================
[2019-04-04 03:30:39,346] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:30:39,346] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:30:39,354] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run29
[2019-04-04 03:30:40,595] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:30:40,595] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:30:40,600] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run29
[2019-04-04 03:30:41,929] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:30:41,929] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:30:41,968] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run29
[2019-04-04 03:30:44,053] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:30:44,053] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:30:44,060] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run29
[2019-04-04 03:30:46,487] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:30:46,487] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:30:46,488] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:30:46,488] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:30:46,492] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run29
[2019-04-04 03:30:46,540] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run29
[2019-04-04 03:30:46,933] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:30:46,933] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:30:46,937] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run29
[2019-04-04 03:30:50,025] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:30:50,025] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:30:50,028] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run29
[2019-04-04 03:30:51,279] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.8506070e-19 1.3768699e-15 4.0020276e-18 7.9723866e-17 3.1272761e-17
 1.2358512e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 03:30:51,280] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6137
[2019-04-04 03:30:51,319] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.200000000000001, 96.0, 0.0, 0.0, 26.0, 19.94391433406363, -0.846351700404918, 0.0, 1.0, 45066.61023796149], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4800.0000, 
sim time next is 5400.0000, 
raw observation next is [7.2, 96.0, 0.0, 0.0, 26.0, 20.05475781599167, -0.8306253765656183, 0.0, 1.0, 44594.31393830113], 
processed observation next is [0.0, 0.043478260869565216, 0.662049861495845, 0.96, 0.0, 0.0, 0.6666666666666666, 0.17122981799930592, 0.22312487447812723, 0.0, 1.0, 0.21235387589667204], 
reward next is 0.7876, 
noisyNet noise sample is [array([-1.7731943], dtype=float32), -0.19744305]. 
=============================================
[2019-04-04 03:30:52,486] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:30:52,486] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:30:52,489] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run29
[2019-04-04 03:30:53,174] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:30:53,175] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:30:53,180] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run29
[2019-04-04 03:30:54,621] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:30:54,621] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:30:54,658] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run29
[2019-04-04 03:30:56,933] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:30:56,933] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:30:56,936] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run29
[2019-04-04 03:31:03,232] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.9411529e-27 5.7015913e-23 6.5667383e-26 7.2135895e-24 6.8483421e-24
 4.3116656e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:31:03,232] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7754
[2019-04-04 03:31:03,303] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-14.86666666666667, 74.0, 44.33333333333333, 736.5, 26.0, 25.47722726430327, 0.2783712597828721, 1.0, 1.0, 56243.41935077996], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 380400.0000, 
sim time next is 381000.0000, 
raw observation next is [-14.68333333333333, 70.0, 49.66666666666666, 735.0, 26.0, 25.61015551171604, 0.2902768599449101, 1.0, 1.0, 54908.36272094578], 
processed observation next is [1.0, 0.391304347826087, 0.05586334256694376, 0.7, 0.1655555555555555, 0.8121546961325967, 0.6666666666666666, 0.6341796259763367, 0.5967589533149701, 1.0, 1.0, 0.26146839390926563], 
reward next is 0.7385, 
noisyNet noise sample is [array([1.4022993], dtype=float32), 2.242255]. 
=============================================
[2019-04-04 03:31:03,311] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[80.99103 ]
 [81.799934]
 [82.44986 ]
 [83.13162 ]
 [83.479034]], R is [[80.33125305]
 [80.26011658]
 [80.20386505]
 [80.22079468]
 [80.32905579]].
[2019-04-04 03:31:06,249] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:31:06,249] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:31:06,252] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run29
[2019-04-04 03:31:13,554] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.27228174e-27 1.59164962e-23 1.26670415e-25 1.25816230e-23
 1.25265495e-23 4.14258325e-27 1.00000000e+00], sum to 1.0000
[2019-04-04 03:31:13,554] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9364
[2019-04-04 03:31:13,581] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.4, 71.0, 0.0, 0.0, 26.0, 23.91270954973291, 0.05040381727173127, 0.0, 1.0, 44700.31617772171], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 171600.0000, 
sim time next is 172200.0000, 
raw observation next is [-8.4, 71.0, 0.0, 0.0, 26.0, 23.87452314386739, 0.04052335222094285, 0.0, 1.0, 44644.84152318563], 
processed observation next is [1.0, 1.0, 0.2299168975069252, 0.71, 0.0, 0.0, 0.6666666666666666, 0.4895435953222824, 0.5135077840736476, 0.0, 1.0, 0.21259448344374107], 
reward next is 0.7874, 
noisyNet noise sample is [array([0.42724085], dtype=float32), -0.28710595]. 
=============================================
[2019-04-04 03:31:18,589] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.0144228e-28 3.7838755e-24 1.2221379e-26 4.9410752e-24 8.7320690e-25
 2.5194361e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:31:18,589] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9575
[2019-04-04 03:31:18,651] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.95, 63.5, 149.0, 0.0, 26.0, 24.3818742310923, 0.2867877150808245, 1.0, 1.0, 200978.6543490316], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 221400.0000, 
sim time next is 222000.0000, 
raw observation next is [-3.766666666666667, 63.0, 143.6666666666667, 0.0, 26.0, 25.3019359012739, 0.3541668102570254, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.358264081255771, 0.63, 0.47888888888888903, 0.0, 0.6666666666666666, 0.6084946584394917, 0.6180556034190084, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1276665], dtype=float32), -0.19422655]. 
=============================================
[2019-04-04 03:31:18,660] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[82.09696 ]
 [81.481865]
 [80.95081 ]
 [81.19943 ]
 [81.26485 ]], R is [[82.06989288]
 [81.2921524 ]
 [80.52905273]
 [80.63461304]
 [80.65660095]].
[2019-04-04 03:31:20,978] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.84021643e-27 3.99418023e-22 4.69644149e-25 4.01040760e-23
 2.82419398e-23 1.00065835e-26 1.00000000e+00], sum to 1.0000
[2019-04-04 03:31:20,978] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5367
[2019-04-04 03:31:21,090] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.55, 68.5, 0.0, 0.0, 26.0, 22.70203702887871, -0.1458374444346605, 1.0, 1.0, 203423.4331543393], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 286200.0000, 
sim time next is 286800.0000, 
raw observation next is [-12.63333333333333, 69.0, 0.0, 0.0, 26.0, 23.29202247530414, -0.02481986599102237, 1.0, 1.0, 164685.916841178], 
processed observation next is [1.0, 0.30434782608695654, 0.11265004616805181, 0.69, 0.0, 0.0, 0.6666666666666666, 0.4410018729420117, 0.4917267113363259, 1.0, 1.0, 0.7842186516246571], 
reward next is 0.2158, 
noisyNet noise sample is [array([-0.9638863], dtype=float32), 1.2562152]. 
=============================================
[2019-04-04 03:31:22,521] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.3687749e-28 5.5217857e-24 4.0564984e-26 3.7700582e-24 4.0087079e-24
 4.9422343e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:31:22,521] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0893
[2019-04-04 03:31:22,583] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 26.0, 25.25504657908633, 0.3128313555378108, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 241800.0000, 
sim time next is 242400.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 26.0, 25.35173128684273, 0.282129090733896, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.368421052631579, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6126442739035607, 0.594043030244632, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.59721935], dtype=float32), 0.8525575]. 
=============================================
[2019-04-04 03:31:35,213] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.06295452e-27 2.19780703e-23 1.34156545e-25 1.27313875e-23
 1.44902010e-23 3.14803881e-27 1.00000000e+00], sum to 1.0000
[2019-04-04 03:31:35,213] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7793
[2019-04-04 03:31:35,278] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-14.31666666666667, 65.0, 60.33333333333333, 732.0, 26.0, 25.78945206923573, 0.3430554131915331, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 382200.0000, 
sim time next is 382800.0000, 
raw observation next is [-14.13333333333333, 64.0, 65.66666666666667, 730.5, 26.0, 25.87096517977423, 0.3554211219245149, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.07109879963065568, 0.64, 0.2188888888888889, 0.8071823204419889, 0.6666666666666666, 0.6559137649811859, 0.6184737073081716, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.420047], dtype=float32), -0.27669322]. 
=============================================
[2019-04-04 03:31:41,219] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.1296605e-26 8.9026091e-22 2.2331319e-24 5.3050688e-23 1.9545846e-22
 1.0138921e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 03:31:41,235] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0028
[2019-04-04 03:31:41,292] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-16.2, 78.0, 0.0, 0.0, 26.0, 21.7826081811242, -0.4751985559176858, 0.0, 1.0, 49027.46503632655], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 369000.0000, 
sim time next is 369600.0000, 
raw observation next is [-16.2, 78.0, 0.0, 0.0, 26.0, 21.70973333539626, -0.493362995174224, 0.0, 1.0, 49150.14669542631], 
processed observation next is [1.0, 0.2608695652173913, 0.013850415512465375, 0.78, 0.0, 0.0, 0.6666666666666666, 0.3091444446163549, 0.33554566827525867, 0.0, 1.0, 0.23404831759726813], 
reward next is 0.7660, 
noisyNet noise sample is [array([-1.6300598], dtype=float32), 0.2247184]. 
=============================================
[2019-04-04 03:31:42,401] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0875762e-27 4.6065375e-24 6.6206409e-26 1.3899360e-23 9.5361695e-24
 2.1327039e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:31:42,401] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9397
[2019-04-04 03:31:42,471] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.816666666666666, 57.16666666666667, 0.0, 0.0, 26.0, 24.98154187192152, 0.2987186756393406, 1.0, 1.0, 96870.94378082962], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 762600.0000, 
sim time next is 763200.0000, 
raw observation next is [-5.0, 58.0, 0.0, 0.0, 26.0, 24.92430657300488, 0.3093445848210389, 1.0, 1.0, 111277.1506140169], 
processed observation next is [1.0, 0.8695652173913043, 0.32409972299168976, 0.58, 0.0, 0.0, 0.6666666666666666, 0.5770255477504067, 0.603114861607013, 1.0, 1.0, 0.5298911934000804], 
reward next is 0.4701, 
noisyNet noise sample is [array([0.56045157], dtype=float32), -0.24197513]. 
=============================================
[2019-04-04 03:31:52,426] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.26074497e-27 1.44551977e-23 9.96252129e-26 4.64206501e-24
 1.28854915e-23 1.14229900e-27 1.00000000e+00], sum to 1.0000
[2019-04-04 03:31:52,427] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7850
[2019-04-04 03:31:52,495] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 79.0, 75.0, 0.0, 26.0, 26.30360629799253, 0.4649924612068742, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 828000.0000, 
sim time next is 828600.0000, 
raw observation next is [-3.9, 80.16666666666667, 69.66666666666666, 0.0, 26.0, 26.30753802702824, 0.4623724381611289, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3545706371191136, 0.8016666666666667, 0.2322222222222222, 0.0, 0.6666666666666666, 0.6922948355856867, 0.6541241460537096, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1224362], dtype=float32), -0.533742]. 
=============================================
[2019-04-04 03:32:04,452] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [9.9465291e-27 1.7719199e-22 2.0077611e-25 2.0239888e-23 2.6472616e-23
 3.0748109e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:32:04,452] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0660
[2019-04-04 03:32:04,497] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.1, 56.5, 18.0, 10.66666666666667, 26.0, 24.8738127618761, 0.2125217635850606, 0.0, 1.0, 51106.98140315043], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 665400.0000, 
sim time next is 666000.0000, 
raw observation next is [-1.2, 57.0, 13.5, 8.5, 26.0, 24.87708899239405, 0.2133208398337729, 0.0, 1.0, 46818.80166719897], 
processed observation next is [0.0, 0.7391304347826086, 0.42936288088642666, 0.57, 0.045, 0.009392265193370166, 0.6666666666666666, 0.5730907493661709, 0.5711069466112576, 0.0, 1.0, 0.22294667460570938], 
reward next is 0.7771, 
noisyNet noise sample is [array([-0.02476316], dtype=float32), -0.26363572]. 
=============================================
[2019-04-04 03:32:04,500] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[81.39619 ]
 [81.408264]
 [81.494415]
 [81.68788 ]
 [81.8689  ]], R is [[81.32092285]
 [81.26434326]
 [81.20276642]
 [81.16469574]
 [81.1721344 ]].
[2019-04-04 03:32:13,162] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.5434038e-28 5.4827663e-24 9.5582609e-27 2.7438234e-25 6.2995345e-25
 6.4067148e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:32:13,175] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8571
[2019-04-04 03:32:13,204] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 76.0, 0.0, 0.0, 26.0, 24.18729244031784, 0.03180908812882842, 0.0, 1.0, 41683.46460290565], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 711600.0000, 
sim time next is 712200.0000, 
raw observation next is [-2.3, 76.0, 0.0, 0.0, 26.0, 24.24286685675699, 0.03671712284704738, 0.0, 1.0, 41705.4822498903], 
processed observation next is [1.0, 0.21739130434782608, 0.3988919667590028, 0.76, 0.0, 0.0, 0.6666666666666666, 0.5202389047297492, 0.5122390409490157, 0.0, 1.0, 0.19859753452328716], 
reward next is 0.8014, 
noisyNet noise sample is [array([0.0955035], dtype=float32), 1.5087816]. 
=============================================
[2019-04-04 03:32:28,082] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.9137891e-29 4.0724570e-24 2.9952062e-27 2.7521991e-25 4.0598754e-25
 1.2111096e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:32:28,086] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6809
[2019-04-04 03:32:28,155] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 24.99289755714832, 0.2482116820843802, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 891000.0000, 
sim time next is 891600.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.13529510005398, 0.2494024886136979, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5946079250044983, 0.5831341628712327, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.02775534], dtype=float32), 1.5480683]. 
=============================================
[2019-04-04 03:32:33,092] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.5807747e-30 1.4292444e-25 5.6773959e-29 1.3235846e-27 7.9019812e-27
 3.3667095e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 03:32:33,098] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2445
[2019-04-04 03:32:33,103] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [18.46666666666667, 64.33333333333333, 24.16666666666667, 0.0, 26.0, 24.97709401130443, 0.4660789281488449, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1183200.0000, 
sim time next is 1183800.0000, 
raw observation next is [18.38333333333333, 64.66666666666667, 19.33333333333334, 0.0, 26.0, 24.95630396905947, 0.4613817278594322, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.9718374884579871, 0.6466666666666667, 0.06444444444444447, 0.0, 0.6666666666666666, 0.5796919974216225, 0.6537939092864774, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.2144048], dtype=float32), 2.2061257]. 
=============================================
[2019-04-04 03:32:40,397] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.4825909e-30 1.2012694e-25 1.9909322e-29 6.2419266e-27 6.7851691e-27
 2.2231039e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 03:32:40,401] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.8370
[2019-04-04 03:32:40,412] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [17.8, 66.33333333333333, 0.0, 0.0, 26.0, 24.75530584054972, 0.4134334500534577, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1191000.0000, 
sim time next is 1191600.0000, 
raw observation next is [17.7, 67.0, 0.0, 0.0, 26.0, 24.73275445313892, 0.4083854902061874, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.8260869565217391, 0.9529085872576178, 0.67, 0.0, 0.0, 0.6666666666666666, 0.56106287109491, 0.6361284967353958, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.20589267], dtype=float32), 1.6182112]. 
=============================================
[2019-04-04 03:32:45,774] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [8.6819971e-29 5.1409639e-24 1.9389557e-26 9.2200534e-25 1.6621899e-24
 4.3388053e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:32:45,774] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.9539
[2019-04-04 03:32:45,783] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.1, 92.0, 0.0, 0.0, 26.0, 25.41299594323612, 0.5527884657345088, 0.0, 1.0, 43429.76518141748], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1311000.0000, 
sim time next is 1311600.0000, 
raw observation next is [2.0, 92.0, 0.0, 0.0, 26.0, 25.45201853674038, 0.5542618968932287, 0.0, 1.0, 18758.29897814685], 
processed observation next is [1.0, 0.17391304347826086, 0.518005540166205, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6210015447283649, 0.6847539656310762, 0.0, 1.0, 0.08932523322927072], 
reward next is 0.9107, 
noisyNet noise sample is [array([0.06030385], dtype=float32), 0.4306237]. 
=============================================
[2019-04-04 03:32:46,699] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.5786693e-27 1.8764745e-22 2.2509286e-25 2.5592058e-23 3.1381294e-23
 3.3142672e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:32:46,699] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5508
[2019-04-04 03:32:46,758] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 87.0, 97.0, 0.0, 26.0, 24.96955792847374, 0.3400224375561655, 0.0, 1.0, 18738.88830610051], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1765800.0000, 
sim time next is 1766400.0000, 
raw observation next is [-2.3, 87.0, 100.6666666666667, 0.0, 26.0, 24.9547816724825, 0.333335327445405, 0.0, 1.0, 33970.08946800172], 
processed observation next is [0.0, 0.43478260869565216, 0.3988919667590028, 0.87, 0.33555555555555566, 0.0, 0.6666666666666666, 0.5795651393735417, 0.611111775815135, 0.0, 1.0, 0.1617623308000082], 
reward next is 0.8382, 
noisyNet noise sample is [array([1.4401071], dtype=float32), 0.55339897]. 
=============================================
[2019-04-04 03:32:48,828] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.0176817e-30 3.2689908e-25 1.2138107e-28 4.4451060e-27 1.1959877e-26
 2.3583274e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 03:32:48,828] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4006
[2019-04-04 03:32:48,857] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [18.3, 64.0, 0.0, 0.0, 26.0, 24.87256115878378, 0.4418427866794986, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1186200.0000, 
sim time next is 1186800.0000, 
raw observation next is [18.3, 63.66666666666667, 0.0, 0.0, 26.0, 24.85237249198265, 0.4380180393866784, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.7391304347826086, 0.9695290858725764, 0.6366666666666667, 0.0, 0.0, 0.6666666666666666, 0.5710310409985541, 0.6460060131288928, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5558583], dtype=float32), 1.2354897]. 
=============================================
[2019-04-04 03:32:49,950] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.6362349e-29 4.7059280e-25 1.5070843e-27 1.6498077e-25 1.9455731e-25
 3.9313873e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 03:32:49,950] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6149
[2019-04-04 03:32:49,960] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 92.0, 110.3333333333333, 0.0, 26.0, 26.10695419515282, 0.591398615179604, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1335000.0000, 
sim time next is 1335600.0000, 
raw observation next is [1.1, 92.0, 114.5, 0.0, 26.0, 26.11485694853901, 0.5918578759193486, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.49307479224376743, 0.92, 0.38166666666666665, 0.0, 0.6666666666666666, 0.6762380790449175, 0.6972859586397829, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5115671], dtype=float32), 1.9335876]. 
=============================================
[2019-04-04 03:33:17,036] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.3751666e-28 1.1051263e-24 4.8128920e-27 1.6332644e-24 4.0323707e-25
 8.7870187e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 03:33:17,042] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7196
[2019-04-04 03:33:17,098] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.75, 66.5, 86.0, 0.0, 26.0, 26.23126169473126, 0.483208446574009, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2129400.0000, 
sim time next is 2130000.0000, 
raw observation next is [-4.666666666666667, 66.0, 76.0, 0.0, 26.0, 26.19650846487978, 0.3698032426078819, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.3333333333333333, 0.66, 0.25333333333333335, 0.0, 0.6666666666666666, 0.6830423720733151, 0.6232677475359606, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.32057425], dtype=float32), 0.5236949]. 
=============================================
[2019-04-04 03:33:17,155] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[82.29751 ]
 [82.23275 ]
 [82.155975]
 [81.96233 ]
 [81.81564 ]], R is [[82.35068512]
 [82.5271759 ]
 [82.7019043 ]
 [82.87488556]
 [83.04613495]].
[2019-04-04 03:33:21,370] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.7188459e-27 1.2232412e-22 7.6232801e-25 3.3900865e-23 5.0071662e-23
 6.1942055e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:33:21,370] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9817
[2019-04-04 03:33:21,417] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 82.0, 0.0, 0.0, 26.0, 23.36196815825983, -0.1522894088950727, 0.0, 1.0, 44855.45429943839], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1922400.0000, 
sim time next is 1923000.0000, 
raw observation next is [-9.0, 83.50000000000001, 0.0, 0.0, 26.0, 23.28357727101462, -0.1585070292027783, 0.0, 1.0, 44772.84372340907], 
processed observation next is [1.0, 0.2608695652173913, 0.21329639889196678, 0.8350000000000002, 0.0, 0.0, 0.6666666666666666, 0.44029810591788515, 0.4471643235990739, 0.0, 1.0, 0.21320401773051936], 
reward next is 0.7868, 
noisyNet noise sample is [array([-0.600252], dtype=float32), -0.8083121]. 
=============================================
[2019-04-04 03:33:21,436] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[79.42414 ]
 [79.42276 ]
 [79.42389 ]
 [79.407295]
 [79.39596 ]], R is [[79.39207458]
 [79.384552  ]
 [79.37675476]
 [79.36871338]
 [79.36039734]].
[2019-04-04 03:33:27,022] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.9593205e-29 1.1338870e-24 1.0848172e-26 2.6361852e-24 8.2014866e-25
 1.7030523e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:33:27,022] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4484
[2019-04-04 03:33:27,070] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.683333333333334, 73.0, 0.0, 0.0, 26.0, 25.292656821005, 0.3643836425862509, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1969800.0000, 
sim time next is 1970400.0000, 
raw observation next is [-4.866666666666667, 75.0, 0.0, 0.0, 26.0, 25.33035619803988, 0.3386403916266121, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.3277931671283472, 0.75, 0.0, 0.0, 0.6666666666666666, 0.6108630165033233, 0.612880130542204, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.02122567], dtype=float32), -1.9839149]. 
=============================================
[2019-04-04 03:33:32,608] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.1900693e-27 3.7694869e-22 3.1000399e-25 9.4244944e-24 3.0518652e-23
 1.3533149e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:33:32,610] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3799
[2019-04-04 03:33:32,718] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.5, 91.0, 0.0, 0.0, 26.0, 23.62482582125511, 0.009840905635030853, 1.0, 1.0, 203280.4378791368], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1927800.0000, 
sim time next is 1928400.0000, 
raw observation next is [-9.5, 91.0, 0.0, 0.0, 26.0, 24.4452978489698, 0.1105834951628297, 1.0, 1.0, 155932.3830473522], 
processed observation next is [1.0, 0.30434782608695654, 0.1994459833795014, 0.91, 0.0, 0.0, 0.6666666666666666, 0.5371081540808168, 0.5368611650542766, 1.0, 1.0, 0.7425351573683437], 
reward next is 0.2575, 
noisyNet noise sample is [array([-1.8375056], dtype=float32), 0.006018754]. 
=============================================
[2019-04-04 03:33:44,979] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.8330321e-28 6.1695920e-24 4.1975292e-26 1.9084746e-24 4.3867277e-24
 1.5548768e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:33:44,979] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2953
[2019-04-04 03:33:45,007] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.333333333333334, 83.0, 0.0, 0.0, 26.0, 25.2418347339725, 0.4065072865468594, 0.0, 1.0, 42514.28222096964], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2151600.0000, 
sim time next is 2152200.0000, 
raw observation next is [-6.516666666666667, 83.0, 0.0, 0.0, 26.0, 25.22880137705562, 0.402811064269501, 0.0, 1.0, 42264.40818521218], 
processed observation next is [1.0, 0.9130434782608695, 0.2820867959372115, 0.83, 0.0, 0.0, 0.6666666666666666, 0.6024001147546351, 0.6342703547565004, 0.0, 1.0, 0.20125908659624847], 
reward next is 0.7987, 
noisyNet noise sample is [array([0.49139428], dtype=float32), -1.3313183]. 
=============================================
[2019-04-04 03:33:53,349] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.3731943e-28 6.5573797e-25 5.9413379e-27 1.2987983e-24 1.1377779e-24
 1.6566540e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:33:53,349] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3014
[2019-04-04 03:33:53,410] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.733333333333333, 70.33333333333334, 0.0, 0.0, 26.0, 25.2443245278082, 0.3543646828154648, 1.0, 1.0, 32414.26670428433], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2229600.0000, 
sim time next is 2230200.0000, 
raw observation next is [-4.8, 70.5, 0.0, 0.0, 26.0, 25.09527870508664, 0.3455202310221422, 1.0, 1.0, 110779.2661981412], 
processed observation next is [1.0, 0.8260869565217391, 0.3296398891966759, 0.705, 0.0, 0.0, 0.6666666666666666, 0.5912732254238865, 0.615173410340714, 1.0, 1.0, 0.5275203152292438], 
reward next is 0.4725, 
noisyNet noise sample is [array([0.1350504], dtype=float32), -0.97403187]. 
=============================================
[2019-04-04 03:33:54,978] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.5650848e-28 4.4924520e-24 2.4627006e-26 3.6178514e-24 4.1493271e-24
 3.4120690e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:33:54,991] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3314
[2019-04-04 03:33:55,022] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.966666666666667, 83.0, 0.0, 0.0, 26.0, 25.27757834205332, 0.413672457834399, 0.0, 1.0, 43086.33272279808], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2150400.0000, 
sim time next is 2151000.0000, 
raw observation next is [-6.15, 83.0, 0.0, 0.0, 26.0, 25.26787101905561, 0.4091321879713989, 0.0, 1.0, 42958.98619060393], 
processed observation next is [1.0, 0.9130434782608695, 0.29224376731301943, 0.83, 0.0, 0.0, 0.6666666666666666, 0.6056559182546343, 0.6363773959904663, 0.0, 1.0, 0.20456660090763776], 
reward next is 0.7954, 
noisyNet noise sample is [array([-0.5333286], dtype=float32), 0.43028983]. 
=============================================
[2019-04-04 03:33:55,044] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[81.05092]
 [81.13879]
 [81.148  ]
 [81.15912]
 [81.19489]], R is [[80.92750549]
 [80.91306305]
 [80.8976593 ]
 [80.87934875]
 [80.850914  ]].
[2019-04-04 03:34:01,579] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.4464995e-28 1.3372079e-24 2.4399870e-26 4.3595522e-24 1.4133324e-24
 6.6485461e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 03:34:01,591] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8827
[2019-04-04 03:34:01,636] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 54.0, 0.0, 0.0, 26.0, 25.12940257575779, 0.3861925235868759, 1.0, 1.0, 83456.2701213297], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2656800.0000, 
sim time next is 2657400.0000, 
raw observation next is [-0.7, 55.0, 0.0, 0.0, 26.0, 25.27725748582148, 0.3903401368759905, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.443213296398892, 0.55, 0.0, 0.0, 0.6666666666666666, 0.6064381238184566, 0.6301133789586635, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.26467815], dtype=float32), 1.5268658]. 
=============================================
[2019-04-04 03:34:03,514] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.0078290e-26 4.8873359e-22 1.4127287e-24 3.7936043e-23 8.5981756e-23
 1.1726366e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 03:34:03,514] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0903
[2019-04-04 03:34:03,531] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.9, 65.66666666666667, 0.0, 0.0, 26.0, 24.44276146063918, 0.1668569174628869, 0.0, 1.0, 40315.23307590793], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2347800.0000, 
sim time next is 2348400.0000, 
raw observation next is [-3.0, 66.33333333333334, 0.0, 0.0, 26.0, 24.41773018779126, 0.158446449286571, 0.0, 1.0, 40426.02928522837], 
processed observation next is [0.0, 0.17391304347826086, 0.3795013850415513, 0.6633333333333334, 0.0, 0.0, 0.6666666666666666, 0.534810848982605, 0.5528154830955236, 0.0, 1.0, 0.1925049013582303], 
reward next is 0.8075, 
noisyNet noise sample is [array([-0.389049], dtype=float32), -0.5316263]. 
=============================================
[2019-04-04 03:34:07,019] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [3.4446201e-28 2.2357401e-24 8.7106684e-27 2.8904055e-24 9.8131240e-25
 1.6424644e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:34:07,019] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.8214
[2019-04-04 03:34:07,066] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 54.66666666666667, 0.0, 0.0, 26.0, 25.1238071399149, 0.4064353542722763, 0.0, 1.0, 62956.43839989592], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2320800.0000, 
sim time next is 2321400.0000, 
raw observation next is [-1.7, 54.33333333333333, 0.0, 0.0, 26.0, 25.24313763277943, 0.425879630555791, 0.0, 1.0, 47588.00769012819], 
processed observation next is [1.0, 0.8695652173913043, 0.4155124653739613, 0.5433333333333333, 0.0, 0.0, 0.6666666666666666, 0.6035948027316191, 0.6419598768519303, 0.0, 1.0, 0.22660956042918184], 
reward next is 0.7734, 
noisyNet noise sample is [array([-1.311797], dtype=float32), 0.49911413]. 
=============================================
[2019-04-04 03:34:08,331] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0899186e-27 1.9568143e-23 1.6556393e-25 7.2605488e-24 1.7320681e-23
 4.7266311e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:34:08,331] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8879
[2019-04-04 03:34:08,370] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.4, 70.0, 0.0, 0.0, 26.0, 25.26503298660168, 0.4048237439515546, 0.0, 1.0, 44742.02131163482], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2238000.0000, 
sim time next is 2238600.0000, 
raw observation next is [-5.5, 70.5, 0.0, 0.0, 26.0, 25.24350301546981, 0.4005872186574309, 0.0, 1.0, 44673.05609532035], 
processed observation next is [1.0, 0.9130434782608695, 0.3102493074792244, 0.705, 0.0, 0.0, 0.6666666666666666, 0.6036252512891508, 0.6335290728858103, 0.0, 1.0, 0.21272883854914454], 
reward next is 0.7873, 
noisyNet noise sample is [array([0.7666247], dtype=float32), 0.68376696]. 
=============================================
[2019-04-04 03:34:09,012] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [5.1940833e-26 1.7122172e-21 2.1422062e-24 8.0295589e-23 1.6410227e-22
 2.4519969e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 03:34:09,012] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.5964
[2019-04-04 03:34:09,032] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 66.33333333333333, 0.0, 0.0, 26.0, 24.17204015059864, 0.1080277332314757, 0.0, 1.0, 41138.57092740813], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2353200.0000, 
sim time next is 2353800.0000, 
raw observation next is [-2.9, 65.66666666666667, 0.0, 0.0, 26.0, 24.14957927692748, 0.1030538797508116, 0.0, 1.0, 41174.56090611978], 
processed observation next is [0.0, 0.21739130434782608, 0.38227146814404434, 0.6566666666666667, 0.0, 0.0, 0.6666666666666666, 0.5124649397439566, 0.5343512932502705, 0.0, 1.0, 0.19606933764818943], 
reward next is 0.8039, 
noisyNet noise sample is [array([-0.78222895], dtype=float32), -0.81166965]. 
=============================================
[2019-04-04 03:34:10,104] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.6997815e-28 6.0862834e-23 3.3171259e-26 2.9387463e-24 5.8829574e-24
 1.2661136e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:34:10,104] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9494
[2019-04-04 03:34:10,148] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.7, 27.83333333333334, 88.0, 836.3333333333334, 26.0, 24.96081604033909, 0.2741734925257777, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2466600.0000, 
sim time next is 2467200.0000, 
raw observation next is [1.8, 27.66666666666667, 87.5, 834.1666666666667, 26.0, 24.96967213798414, 0.2724280746982401, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.5124653739612189, 0.2766666666666667, 0.2916666666666667, 0.921731123388582, 0.6666666666666666, 0.5808060114986784, 0.5908093582327467, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0559449], dtype=float32), 0.41214618]. 
=============================================
[2019-04-04 03:34:12,525] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.4431036e-27 3.5495289e-23 2.4717642e-26 3.8559012e-24 2.3110787e-24
 2.8120834e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:34:12,526] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1616
[2019-04-04 03:34:12,597] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 38.33333333333334, 77.66666666666667, 785.6666666666666, 26.0, 24.9590171809638, 0.2305152568739033, 0.0, 1.0, 41246.86446869657], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2457600.0000, 
sim time next is 2458200.0000, 
raw observation next is [-2.85, 37.16666666666666, 79.33333333333333, 794.3333333333334, 26.0, 24.93356929170071, 0.2437915227193076, 0.0, 1.0, 40261.85011497299], 
processed observation next is [0.0, 0.43478260869565216, 0.3836565096952909, 0.3716666666666666, 0.2644444444444444, 0.8777163904235727, 0.6666666666666666, 0.5777974409750591, 0.5812638409064359, 0.0, 1.0, 0.1917230957855857], 
reward next is 0.8083, 
noisyNet noise sample is [array([0.42192134], dtype=float32), 0.70314395]. 
=============================================
[2019-04-04 03:34:18,149] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0034894e-29 2.4593328e-24 3.0764637e-27 3.3058720e-25 4.4790684e-25
 1.5412406e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:34:18,152] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4709
[2019-04-04 03:34:18,184] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.333333333333333, 93.0, 95.66666666666666, 130.0, 26.0, 25.27218574100274, 0.3249576294998061, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2882400.0000, 
sim time next is 2883000.0000, 
raw observation next is [1.166666666666667, 93.0, 86.33333333333334, 104.0, 26.0, 25.38223872261973, 0.3167644610154963, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.49492151431209613, 0.93, 0.2877777777777778, 0.11491712707182321, 0.6666666666666666, 0.6151865602183108, 0.605588153671832, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7261912], dtype=float32), 0.012601612]. 
=============================================
[2019-04-04 03:34:18,187] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[86.09422 ]
 [87.16285 ]
 [87.03171 ]
 [86.977325]
 [86.98869 ]], R is [[85.15840149]
 [85.3068161 ]
 [85.45375061]
 [85.17625427]
 [85.23535919]].
[2019-04-04 03:34:19,837] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.8641634e-26 2.5204437e-22 1.6056445e-24 4.7307174e-23 1.2371317e-22
 4.7439929e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:34:19,838] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8729
[2019-04-04 03:34:19,864] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.100000000000001, 74.66666666666667, 0.0, 0.0, 26.0, 24.74027480056822, 0.1973623970027864, 0.0, 1.0, 41916.35562082512], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2603400.0000, 
sim time next is 2604000.0000, 
raw observation next is [-5.2, 75.33333333333334, 0.0, 0.0, 26.0, 24.70459247064187, 0.1868456984455138, 0.0, 1.0, 42000.083713449], 
processed observation next is [1.0, 0.13043478260869565, 0.31855955678670367, 0.7533333333333334, 0.0, 0.0, 0.6666666666666666, 0.5587160392201559, 0.5622818994818379, 0.0, 1.0, 0.20000039863547142], 
reward next is 0.8000, 
noisyNet noise sample is [array([0.6439779], dtype=float32), 0.21266606]. 
=============================================
[2019-04-04 03:34:19,885] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[80.350876]
 [80.32666 ]
 [80.27223 ]
 [80.21898 ]
 [80.15349 ]], R is [[80.24040222]
 [80.23839569]
 [80.23679352]
 [80.2355423 ]
 [80.23451233]].
[2019-04-04 03:34:22,778] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.2869281e-29 8.3764012e-25 3.0822315e-27 2.3082405e-25 9.0535259e-25
 9.5673718e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 03:34:22,781] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0797
[2019-04-04 03:34:22,841] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 55.66666666666667, 79.33333333333333, 23.0, 26.0, 25.60794135596133, 0.2601653890635837, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2537400.0000, 
sim time next is 2538000.0000, 
raw observation next is [-2.8, 56.0, 93.5, 25.5, 26.0, 25.60038111788484, 0.2689656965354875, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.38504155124653744, 0.56, 0.31166666666666665, 0.0281767955801105, 0.6666666666666666, 0.63336509315707, 0.5896552321784959, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.0288522], dtype=float32), -0.40164044]. 
=============================================
[2019-04-04 03:34:22,845] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[85.440216]
 [85.34431 ]
 [85.09908 ]
 [85.04326 ]
 [85.01466 ]], R is [[85.63619232]
 [85.77983093]
 [85.92203522]
 [86.06281281]
 [86.14271545]].
[2019-04-04 03:34:29,415] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.6258984e-28 4.9424540e-23 1.6487883e-25 2.2874510e-23 8.1027564e-24
 5.4579256e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:34:29,415] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7190
[2019-04-04 03:34:29,427] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.100000000000001, 74.66666666666667, 0.0, 0.0, 26.0, 24.74027480056822, 0.1973623970027864, 0.0, 1.0, 41916.35562082512], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2603400.0000, 
sim time next is 2604000.0000, 
raw observation next is [-5.2, 75.33333333333334, 0.0, 0.0, 26.0, 24.70459247064187, 0.1868456984455138, 0.0, 1.0, 42000.083713449], 
processed observation next is [1.0, 0.13043478260869565, 0.31855955678670367, 0.7533333333333334, 0.0, 0.0, 0.6666666666666666, 0.5587160392201559, 0.5622818994818379, 0.0, 1.0, 0.20000039863547142], 
reward next is 0.8000, 
noisyNet noise sample is [array([0.88269264], dtype=float32), -0.37481582]. 
=============================================
[2019-04-04 03:34:29,450] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[82.88261 ]
 [82.88924 ]
 [82.86929 ]
 [82.842705]
 [82.80207 ]], R is [[82.75054169]
 [82.72343445]
 [82.69698334]
 [82.67113495]
 [82.64574432]].
[2019-04-04 03:34:34,244] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.5758603e-27 4.4193818e-23 2.3274005e-25 3.4358622e-23 4.7131732e-23
 5.5241372e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:34:34,245] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1754
[2019-04-04 03:34:34,261] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 59.83333333333334, 0.0, 0.0, 26.0, 25.33914115679265, 0.441118485421102, 0.0, 1.0, 51549.16203217742], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2757000.0000, 
sim time next is 2757600.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 25.34743712311475, 0.4394078466682749, 0.0, 1.0, 48973.43977221643], 
processed observation next is [1.0, 0.9565217391304348, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.6122864269262293, 0.6464692822227583, 0.0, 1.0, 0.23320685605817346], 
reward next is 0.7668, 
noisyNet noise sample is [array([0.9427382], dtype=float32), -0.40589905]. 
=============================================
[2019-04-04 03:34:38,945] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.0373661e-28 4.8138545e-24 4.8526230e-26 4.3481753e-24 3.2399172e-24
 5.4781349e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:34:38,946] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2868
[2019-04-04 03:34:38,996] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.4, 26.0, 75.0, 63.33333333333334, 26.0, 25.58841175966843, 0.3377719914785254, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2823600.0000, 
sim time next is 2824200.0000, 
raw observation next is [6.3, 26.5, 71.0, 76.0, 26.0, 24.80753731241297, 0.3045046034034991, 1.0, 1.0, 177996.0021858393], 
processed observation next is [1.0, 0.6956521739130435, 0.6371191135734073, 0.265, 0.23666666666666666, 0.08397790055248619, 0.6666666666666666, 0.567294776034414, 0.601501534467833, 1.0, 1.0, 0.8476000104087585], 
reward next is 0.1524, 
noisyNet noise sample is [array([-0.19738047], dtype=float32), -0.6205984]. 
=============================================
[2019-04-04 03:34:45,455] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.5134213e-27 4.5109687e-22 7.7241237e-25 3.5335333e-23 5.9780435e-23
 2.7193362e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 03:34:45,456] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8600
[2019-04-04 03:34:45,469] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 84.0, 0.0, 0.0, 26.0, 24.28780752491831, 0.1935089505011843, 0.0, 1.0, 42892.14186863689], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2955600.0000, 
sim time next is 2956200.0000, 
raw observation next is [-3.166666666666667, 82.83333333333333, 0.0, 0.0, 26.0, 24.2849136261236, 0.1887786813028078, 0.0, 1.0, 42819.63314037053], 
processed observation next is [0.0, 0.21739130434782608, 0.3748845798707295, 0.8283333333333333, 0.0, 0.0, 0.6666666666666666, 0.5237428021769667, 0.562926227100936, 0.0, 1.0, 0.2039030149541454], 
reward next is 0.7961, 
noisyNet noise sample is [array([-1.0205772], dtype=float32), -0.7031721]. 
=============================================
[2019-04-04 03:34:47,030] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.9496023e-27 3.4416865e-23 1.3346082e-25 6.0577670e-24 2.7618462e-23
 1.0434177e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:34:47,030] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.3178
[2019-04-04 03:34:47,046] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.333333333333333, 97.66666666666667, 0.0, 0.0, 26.0, 24.77493697744571, 0.2199079602462973, 0.0, 1.0, 55431.75990371163], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2874000.0000, 
sim time next is 2874600.0000, 
raw observation next is [1.5, 96.5, 0.0, 0.0, 26.0, 24.73168222363366, 0.2261876861687102, 0.0, 1.0, 55379.38884654026], 
processed observation next is [1.0, 0.2608695652173913, 0.5041551246537397, 0.965, 0.0, 0.0, 0.6666666666666666, 0.5609735186361382, 0.57539589538957, 0.0, 1.0, 0.2637113754597155], 
reward next is 0.7363, 
noisyNet noise sample is [array([0.43079177], dtype=float32), 0.33754587]. 
=============================================
[2019-04-04 03:34:54,380] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.8398948e-27 1.3851042e-22 6.2597900e-26 5.9961972e-24 1.5610847e-23
 1.3643507e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:34:54,380] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7164
[2019-04-04 03:34:54,417] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 65.0, 169.1666666666667, 709.1666666666667, 26.0, 25.05054027155332, 0.3961856520499302, 0.0, 1.0, 30595.95339748564], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2983200.0000, 
sim time next is 2983800.0000, 
raw observation next is [-3.0, 65.0, 157.3333333333333, 727.3333333333334, 26.0, 25.0600081472442, 0.3996871222216677, 0.0, 1.0, 27291.65754245599], 
processed observation next is [0.0, 0.5217391304347826, 0.3795013850415513, 0.65, 0.5244444444444443, 0.8036832412523021, 0.6666666666666666, 0.5883340122703501, 0.6332290407405559, 0.0, 1.0, 0.1299602740116952], 
reward next is 0.8700, 
noisyNet noise sample is [array([0.12497376], dtype=float32), 0.21400094]. 
=============================================
[2019-04-04 03:34:56,291] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.1723492e-27 5.1389078e-23 2.5581103e-25 6.3396763e-24 9.9991204e-24
 2.8312119e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:34:56,291] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4012
[2019-04-04 03:34:56,307] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.833333333333333, 100.0, 0.0, 0.0, 26.0, 25.38944096592018, 0.3078523009753407, 0.0, 1.0, 85065.83354989595], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3131400.0000, 
sim time next is 3132000.0000, 
raw observation next is [4.0, 100.0, 0.0, 0.0, 26.0, 25.39636090446602, 0.311497648574787, 0.0, 1.0, 64119.01223906534], 
processed observation next is [1.0, 0.2608695652173913, 0.5734072022160666, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6163634087055018, 0.6038325495249289, 0.0, 1.0, 0.305328629709835], 
reward next is 0.6947, 
noisyNet noise sample is [array([0.52354676], dtype=float32), -0.24811198]. 
=============================================
[2019-04-04 03:34:56,330] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[82.71874 ]
 [82.52642 ]
 [82.44814 ]
 [82.43655 ]
 [82.513664]], R is [[82.75931549]
 [82.52664948]
 [82.44380188]
 [82.51012421]
 [82.54718781]].
[2019-04-04 03:35:09,660] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.9036937e-30 1.5399492e-26 2.6354358e-28 3.1748493e-26 7.0089207e-26
 2.0609664e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 03:35:09,660] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1555
[2019-04-04 03:35:09,670] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.0, 100.0, 112.3333333333333, 811.6666666666666, 26.0, 27.31598668936194, 0.856802131636443, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3157800.0000, 
sim time next is 3158400.0000, 
raw observation next is [7.0, 100.0, 112.1666666666667, 808.8333333333334, 26.0, 27.33096829769233, 0.869076725621039, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.6565096952908588, 1.0, 0.373888888888889, 0.8937384898710866, 0.6666666666666666, 0.7775806914743608, 0.7896922418736797, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9381371], dtype=float32), -1.6382756]. 
=============================================
[2019-04-04 03:35:10,920] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.7635044e-28 9.5192279e-25 4.1953361e-26 7.7404389e-24 3.0539715e-24
 2.1575873e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:35:10,921] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5439
[2019-04-04 03:35:10,936] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.0, 100.0, 0.0, 0.0, 26.0, 26.71841129377468, 0.8654679145053209, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3174600.0000, 
sim time next is 3175200.0000, 
raw observation next is [6.0, 100.0, 0.0, 0.0, 26.0, 26.92412069699157, 0.8689484737004403, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.6288088642659281, 1.0, 0.0, 0.0, 0.6666666666666666, 0.7436767247492974, 0.78964949123348, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.56530684], dtype=float32), -0.03945282]. 
=============================================
[2019-04-04 03:35:16,937] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.23589153e-29 5.89924730e-25 1.39721085e-27 4.17549254e-25
 1.63482819e-25 6.35668405e-29 1.00000000e+00], sum to 1.0000
[2019-04-04 03:35:16,938] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8476
[2019-04-04 03:35:17,020] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 92.0, 101.0, 653.0, 26.0, 26.17361055906806, 0.6515622328560254, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3231000.0000, 
sim time next is 3231600.0000, 
raw observation next is [-3.0, 92.0, 102.3333333333333, 669.5, 26.0, 26.27145000348268, 0.6628012050223094, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.3795013850415513, 0.92, 0.341111111111111, 0.7397790055248619, 0.6666666666666666, 0.6892875002902233, 0.7209337350074364, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3363446], dtype=float32), -0.7331899]. 
=============================================
[2019-04-04 03:35:22,944] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.8827524e-29 3.2162601e-25 2.4624459e-27 4.9848057e-25 9.6917705e-25
 1.7568306e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 03:35:22,982] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1453
[2019-04-04 03:35:23,018] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.333333333333333, 47.0, 90.16666666666666, 727.8333333333333, 26.0, 26.08764807972202, 0.6671580491962552, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3856800.0000, 
sim time next is 3857400.0000, 
raw observation next is [2.5, 46.5, 87.0, 717.0, 26.0, 26.39875676377999, 0.7016478586746663, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5318559556786704, 0.465, 0.29, 0.7922651933701658, 0.6666666666666666, 0.6998963969816657, 0.7338826195582221, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.27802458], dtype=float32), -0.41403645]. 
=============================================
[2019-04-04 03:35:27,899] A3C_AGENT_WORKER-Thread-6 INFO:Evaluating...
[2019-04-04 03:35:27,901] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 03:35:27,901] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:35:27,901] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 03:35:27,902] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:35:27,902] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 03:35:27,905] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:35:27,906] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run40
[2019-04-04 03:35:27,957] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run40
[2019-04-04 03:35:27,983] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run40
[2019-04-04 03:38:41,426] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 03:39:12,909] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 03:39:20,701] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 03:39:21,747] A3C_AGENT_WORKER-Thread-6 INFO:Global step: 3900000, evaluation results [3900000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 03:39:23,632] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.3038436e-29 6.9875641e-26 6.6115986e-28 4.5811081e-25 6.9654218e-26
 1.8377746e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 03:39:23,632] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8401
[2019-04-04 03:39:23,658] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 56.16666666666666, 115.6666666666667, 818.6666666666667, 26.0, 25.18743664129374, 0.5275577351396238, 1.0, 1.0, 65786.76487610437], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3499800.0000, 
sim time next is 3500400.0000, 
raw observation next is [2.0, 55.33333333333334, 115.8333333333333, 820.8333333333334, 26.0, 25.62551801954501, 0.5505783075491518, 1.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.5217391304347826, 0.518005540166205, 0.5533333333333335, 0.386111111111111, 0.9069981583793739, 0.6666666666666666, 0.635459834962084, 0.6835261025163839, 1.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-1.1127481], dtype=float32), 0.20174508]. 
=============================================
[2019-04-04 03:39:47,308] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.0450923e-27 2.5482173e-23 7.6061383e-26 2.6223040e-24 1.4331659e-23
 2.6568672e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:39:47,312] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2295
[2019-04-04 03:39:47,356] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.3333333333333333, 40.0, 22.16666666666666, 204.1666666666667, 26.0, 25.12506713944044, 0.3852698937643111, 0.0, 1.0, 52045.48087596208], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3604800.0000, 
sim time next is 3605400.0000, 
raw observation next is [-0.5, 40.5, 14.0, 142.0, 26.0, 25.10677882319484, 0.3783306039977253, 0.0, 1.0, 48604.05293110362], 
processed observation next is [0.0, 0.7391304347826086, 0.44875346260387816, 0.405, 0.04666666666666667, 0.1569060773480663, 0.6666666666666666, 0.59223156859957, 0.6261102013325751, 0.0, 1.0, 0.23144787110049345], 
reward next is 0.7686, 
noisyNet noise sample is [array([-0.5457427], dtype=float32), -0.564625]. 
=============================================
[2019-04-04 03:39:47,830] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0491281e-28 4.4189049e-25 1.6383351e-27 1.1710324e-24 1.8127819e-25
 3.3213372e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 03:39:47,831] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3867
[2019-04-04 03:39:47,850] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.5, 68.0, 115.0, 822.0, 26.0, 26.42962210926085, 0.5864292973054416, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3756600.0000, 
sim time next is 3757200.0000, 
raw observation next is [-2.333333333333333, 67.0, 115.6666666666667, 823.1666666666666, 26.0, 26.46200068963748, 0.5943524528362522, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.3979686057248385, 0.67, 0.38555555555555565, 0.9095764272559852, 0.6666666666666666, 0.7051667241364568, 0.6981174842787508, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.14575192], dtype=float32), 0.6833343]. 
=============================================
[2019-04-04 03:39:48,436] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.1360272e-29 1.7419184e-26 1.2391493e-28 4.9365301e-26 2.0241402e-26
 1.6610059e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 03:39:48,437] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7539
[2019-04-04 03:39:48,447] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.0, 35.0, 100.0, 744.5, 26.0, 27.09305948784415, 0.7676522397180808, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4114800.0000, 
sim time next is 4115400.0000, 
raw observation next is [4.0, 35.0, 98.0, 728.0, 26.0, 27.15546292257693, 0.7817550602941606, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5734072022160666, 0.35, 0.32666666666666666, 0.8044198895027624, 0.6666666666666666, 0.7629552435480775, 0.7605850200980536, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.49175474], dtype=float32), 0.8894166]. 
=============================================
[2019-04-04 03:39:56,824] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.4155633e-28 3.1373396e-23 1.5882144e-26 9.5620641e-25 2.5862621e-24
 4.8934795e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:39:56,828] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8655
[2019-04-04 03:39:56,907] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 53.0, 146.0, 92.0, 26.0, 25.72009862780573, 0.3974995281013938, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4266000.0000, 
sim time next is 4266600.0000, 
raw observation next is [3.166666666666667, 53.16666666666667, 158.0, 105.0, 26.0, 25.67595431620773, 0.3991027139090369, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.5503231763619576, 0.5316666666666667, 0.5266666666666666, 0.11602209944751381, 0.6666666666666666, 0.6396628596839774, 0.633034237969679, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4602891], dtype=float32), 1.7784212]. 
=============================================
[2019-04-04 03:39:59,684] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.5393972e-26 2.2245128e-22 1.2227313e-24 1.6174795e-23 1.0142031e-22
 3.1008073e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:39:59,684] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1022
[2019-04-04 03:39:59,704] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.166666666666666, 59.83333333333334, 0.0, 0.0, 26.0, 24.90839451530307, 0.295918252242036, 0.0, 1.0, 41901.63964919668], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3910200.0000, 
sim time next is 3910800.0000, 
raw observation next is [-6.333333333333333, 60.66666666666667, 0.0, 0.0, 26.0, 24.92799566016194, 0.2920715495732042, 0.0, 1.0, 42025.06065439757], 
processed observation next is [1.0, 0.2608695652173913, 0.28716528162511545, 0.6066666666666667, 0.0, 0.0, 0.6666666666666666, 0.5773329716801617, 0.5973571831910681, 0.0, 1.0, 0.20011933644951221], 
reward next is 0.7999, 
noisyNet noise sample is [array([-0.67970115], dtype=float32), 0.05969943]. 
=============================================
[2019-04-04 03:40:05,387] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.02101703e-27 1.01166773e-23 9.76397918e-26 1.17654494e-23
 4.16653650e-24 2.27620884e-27 1.00000000e+00], sum to 1.0000
[2019-04-04 03:40:05,389] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5052
[2019-04-04 03:40:05,405] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.333333333333334, 46.33333333333334, 0.0, 0.0, 26.0, 25.49149766570373, 0.5083847918267971, 0.0, 1.0, 49747.09863012226], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3964800.0000, 
sim time next is 3965400.0000, 
raw observation next is [-7.5, 47.0, 0.0, 0.0, 26.0, 25.47680666212933, 0.5045266735127245, 0.0, 1.0, 50586.42600469835], 
processed observation next is [1.0, 0.9130434782608695, 0.2548476454293629, 0.47, 0.0, 0.0, 0.6666666666666666, 0.6230672218441109, 0.6681755578375749, 0.0, 1.0, 0.24088774287951595], 
reward next is 0.7591, 
noisyNet noise sample is [array([-0.65242326], dtype=float32), -0.9299528]. 
=============================================
[2019-04-04 03:40:07,081] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.9711249e-28 4.0857321e-23 9.3149706e-26 4.5178869e-24 9.7507010e-24
 5.8974864e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:40:07,089] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8958
[2019-04-04 03:40:07,130] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 80.0, 0.0, 0.0, 26.0, 25.61494848365923, 0.5138844977000021, 0.0, 1.0, 18735.6487910902], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4431600.0000, 
sim time next is 4432200.0000, 
raw observation next is [2.0, 80.0, 0.0, 0.0, 26.0, 25.58359069485099, 0.5047793785115325, 0.0, 1.0, 30181.95782463135], 
processed observation next is [1.0, 0.30434782608695654, 0.518005540166205, 0.8, 0.0, 0.0, 0.6666666666666666, 0.6319658912375825, 0.6682597928371775, 0.0, 1.0, 0.14372360868872072], 
reward next is 0.8563, 
noisyNet noise sample is [array([0.01529076], dtype=float32), -0.9389052]. 
=============================================
[2019-04-04 03:40:18,448] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.8191365e-27 7.6976330e-22 1.9555438e-25 7.5344413e-24 1.6399767e-23
 1.3647827e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:40:18,450] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0019
[2019-04-04 03:40:18,479] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 53.16666666666666, 30.66666666666666, 162.6666666666666, 26.0, 24.63528992600519, 0.2207685764860946, 0.0, 1.0, 40190.72173323314], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4175400.0000, 
sim time next is 4176000.0000, 
raw observation next is [-5.0, 54.0, 46.0, 244.0, 26.0, 24.60516388070349, 0.230038203003901, 0.0, 1.0, 39990.32891703129], 
processed observation next is [0.0, 0.34782608695652173, 0.32409972299168976, 0.54, 0.15333333333333332, 0.2696132596685083, 0.6666666666666666, 0.5504303233919575, 0.5766794010013003, 0.0, 1.0, 0.19043013770014902], 
reward next is 0.8096, 
noisyNet noise sample is [array([-0.32104227], dtype=float32), -0.25959992]. 
=============================================
[2019-04-04 03:40:18,482] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[85.20585]
 [84.11065]
 [83.01495]
 [82.9473 ]
 [82.8785 ]], R is [[85.92818451]
 [85.8775177 ]
 [85.82689667]
 [85.7769928 ]
 [85.72688293]].
[2019-04-04 03:40:20,088] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.1831704e-28 6.2953337e-24 1.4666522e-26 4.3108172e-24 3.3381884e-24
 2.9737784e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:40:20,089] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8748
[2019-04-04 03:40:20,120] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 35.0, 0.0, 0.0, 26.0, 26.53998194703529, 0.5539523506263906, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4126800.0000, 
sim time next is 4127400.0000, 
raw observation next is [3.0, 35.5, 0.0, 0.0, 26.0, 25.8399790302912, 0.5129828560165571, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.5457063711911359, 0.355, 0.0, 0.0, 0.6666666666666666, 0.6533315858576, 0.6709942853388524, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.23201992], dtype=float32), -1.1920949]. 
=============================================
[2019-04-04 03:40:23,680] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.5289730e-28 1.7618830e-23 1.4914089e-26 1.5048806e-24 2.6086453e-24
 1.9786762e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:40:23,686] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2838
[2019-04-04 03:40:23,724] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.5, 53.5, 182.0, 131.0, 26.0, 25.70410420268236, 0.4069295128259844, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4267800.0000, 
sim time next is 4268400.0000, 
raw observation next is [3.666666666666667, 53.66666666666667, 185.6666666666667, 209.8333333333333, 26.0, 25.69800641862553, 0.4032184567330062, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.564173591874423, 0.5366666666666667, 0.618888888888889, 0.23186003683241246, 0.6666666666666666, 0.6415005348854607, 0.6344061522443354, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.95497316], dtype=float32), 1.0755054]. 
=============================================
[2019-04-04 03:40:24,267] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.35466234e-27 1.12909646e-23 2.64549096e-26 1.84158277e-24
 2.64281064e-24 1.14368114e-27 1.00000000e+00], sum to 1.0000
[2019-04-04 03:40:24,267] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2155
[2019-04-04 03:40:24,283] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.9, 58.5, 229.0, 385.0, 26.0, 25.42774409428822, 0.4434226385205982, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4285800.0000, 
sim time next is 4286400.0000, 
raw observation next is [6.866666666666667, 59.0, 210.1666666666667, 430.0, 26.0, 25.4262242588991, 0.4520746135299463, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.6528162511542014, 0.59, 0.7005555555555557, 0.47513812154696133, 0.6666666666666666, 0.618852021574925, 0.6506915378433155, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4931257], dtype=float32), 0.3042133]. 
=============================================
[2019-04-04 03:40:27,720] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.8697618e-27 6.2329963e-22 3.6481168e-25 9.6381306e-24 4.8507426e-23
 1.4703438e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:40:27,720] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7970
[2019-04-04 03:40:27,731] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.1, 73.0, 0.0, 0.0, 26.0, 25.50169170808444, 0.3564580509204561, 0.0, 1.0, 41800.18459727104], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4343400.0000, 
sim time next is 4344000.0000, 
raw observation next is [3.033333333333333, 73.66666666666667, 0.0, 0.0, 26.0, 25.41277053232727, 0.3583224584454687, 0.0, 1.0, 85519.45668198958], 
processed observation next is [1.0, 0.2608695652173913, 0.5466297322253002, 0.7366666666666667, 0.0, 0.0, 0.6666666666666666, 0.6177308776939391, 0.619440819481823, 0.0, 1.0, 0.4072355080094742], 
reward next is 0.5928, 
noisyNet noise sample is [array([-0.4756951], dtype=float32), 0.31614578]. 
=============================================
[2019-04-04 03:40:27,750] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[82.8963 ]
 [82.83103]
 [82.91582]
 [83.04756]
 [83.06217]], R is [[82.79722595]
 [82.77020264]
 [82.85324097]
 [83.02471161]
 [83.1051712 ]].
[2019-04-04 03:40:32,076] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.48566593e-27 2.13001810e-22 5.03641034e-26 1.49847695e-23
 9.23748468e-24 1.00514985e-26 1.00000000e+00], sum to 1.0000
[2019-04-04 03:40:32,077] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7612
[2019-04-04 03:40:32,091] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 60.00000000000001, 0.0, 0.0, 26.0, 25.19567657049034, 0.3314249725347352, 0.0, 1.0, 39303.09353778009], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4843200.0000, 
sim time next is 4843800.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.17913279593735, 0.3270825052428999, 0.0, 1.0, 39257.59662467085], 
processed observation next is [0.0, 0.043478260869565216, 0.40720221606648205, 0.6, 0.0, 0.0, 0.6666666666666666, 0.5982610663281124, 0.6090275017476333, 0.0, 1.0, 0.1869409363079564], 
reward next is 0.8131, 
noisyNet noise sample is [array([0.64375466], dtype=float32), 1.0855082]. 
=============================================
[2019-04-04 03:40:34,713] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0309935e-27 5.0214931e-23 3.6341833e-26 1.8047798e-23 2.1821587e-24
 3.3904159e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:40:34,714] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6614
[2019-04-04 03:40:34,744] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.2, 46.0, 281.0, 390.0, 26.0, 25.11479792016505, 0.3603881890241822, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4883400.0000, 
sim time next is 4884000.0000, 
raw observation next is [1.266666666666667, 45.66666666666667, 279.5, 389.6666666666666, 26.0, 25.09601903386104, 0.3545961203162108, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.4976915974145891, 0.4566666666666667, 0.9316666666666666, 0.4305709023941067, 0.6666666666666666, 0.59133491948842, 0.6181987067720702, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5172137], dtype=float32), -0.8592795]. 
=============================================
[2019-04-04 03:40:34,763] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[86.5666 ]
 [86.68766]
 [86.77861]
 [86.86883]
 [86.90933]], R is [[86.45478058]
 [86.59023285]
 [86.72433472]
 [86.85709381]
 [86.98852539]].
[2019-04-04 03:40:38,387] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.1491813e-29 4.9642543e-25 3.9797366e-27 5.6476416e-25 4.2361209e-25
 4.9241470e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 03:40:38,390] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0317
[2019-04-04 03:40:38,409] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 52.0, 28.0, 28.0, 26.0, 25.98068381545369, 0.4714277571868601, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4555800.0000, 
sim time next is 4556400.0000, 
raw observation next is [2.0, 52.0, 23.33333333333334, 23.33333333333334, 26.0, 25.74786600621344, 0.4956911991582501, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.518005540166205, 0.52, 0.07777777777777779, 0.025782688766114188, 0.6666666666666666, 0.6456555005177865, 0.6652303997194168, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2694395], dtype=float32), 1.1814822]. 
=============================================
[2019-04-04 03:40:42,672] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.2766267e-29 1.5763907e-25 3.9844590e-28 5.3450781e-26 1.0836431e-25
 2.5663152e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 03:40:42,673] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.2854
[2019-04-04 03:40:42,699] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 61.0, 172.0, 9.0, 26.0, 26.17010211475501, 0.5343850763934475, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4528800.0000, 
sim time next is 4529400.0000, 
raw observation next is [1.166666666666667, 60.33333333333334, 188.3333333333333, 12.0, 26.0, 26.18746959390825, 0.5358016082595808, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.49492151431209613, 0.6033333333333334, 0.6277777777777777, 0.013259668508287293, 0.6666666666666666, 0.6822891328256876, 0.6786005360865269, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.20603733], dtype=float32), -0.4860135]. 
=============================================
[2019-04-04 03:40:46,846] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:40:46,846] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:40:46,851] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run30
[2019-04-04 03:40:47,440] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:40:47,441] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:40:47,448] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run30
[2019-04-04 03:40:48,105] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:40:48,105] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:40:48,108] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run30
[2019-04-04 03:41:01,401] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0148745e-26 3.7838361e-22 1.9597492e-24 3.1483092e-23 3.6707768e-23
 7.2197758e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:41:01,404] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9006
[2019-04-04 03:41:01,421] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 49.33333333333334, 0.0, 0.0, 26.0, 25.1859450047348, 0.2708520909280713, 0.0, 1.0, 38351.86405521686], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4939800.0000, 
sim time next is 4940400.0000, 
raw observation next is [-2.0, 48.66666666666667, 0.0, 0.0, 26.0, 25.14852297642786, 0.272285558324682, 0.0, 1.0, 38383.20992265364], 
processed observation next is [1.0, 0.17391304347826086, 0.40720221606648205, 0.4866666666666667, 0.0, 0.0, 0.6666666666666666, 0.595710248035655, 0.590761852774894, 0.0, 1.0, 0.18277719010787447], 
reward next is 0.8172, 
noisyNet noise sample is [array([0.97045016], dtype=float32), -1.9300083]. 
=============================================
[2019-04-04 03:41:01,702] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.8034827e-27 2.5525676e-22 1.9125039e-25 1.7872680e-23 1.2930842e-23
 1.1552709e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:41:01,707] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5954
[2019-04-04 03:41:01,727] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.666666666666667, 48.66666666666666, 0.0, 0.0, 26.0, 25.10936306507998, 0.2378977209856268, 0.0, 1.0, 38600.98620530344], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4945200.0000, 
sim time next is 4945800.0000, 
raw observation next is [-2.833333333333333, 49.33333333333334, 0.0, 0.0, 26.0, 25.03745137651805, 0.2320528959865091, 0.0, 1.0, 38646.89735431648], 
processed observation next is [1.0, 0.21739130434782608, 0.3841181902123731, 0.4933333333333334, 0.0, 0.0, 0.6666666666666666, 0.5864542813765041, 0.5773509653288363, 0.0, 1.0, 0.1840328445443642], 
reward next is 0.8160, 
noisyNet noise sample is [array([1.6776073], dtype=float32), 0.6108663]. 
=============================================
[2019-04-04 03:41:06,663] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:41:06,663] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:41:06,667] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run30
[2019-04-04 03:41:07,552] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:41:07,553] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:41:07,586] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run30
[2019-04-04 03:41:09,637] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:41:09,637] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:41:09,652] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run30
[2019-04-04 03:41:10,617] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:41:10,617] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:41:10,621] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run30
[2019-04-04 03:41:11,065] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:41:11,065] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:41:11,069] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run30
[2019-04-04 03:41:11,164] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [3.7740343e-29 2.1935260e-24 9.7411310e-27 9.9926707e-25 2.3628989e-24
 6.7199409e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 03:41:11,166] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.9556
[2019-04-04 03:41:11,187] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.166666666666667, 35.66666666666667, 0.0, 0.0, 26.0, 25.78685543002248, 0.5740874304693787, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5003400.0000, 
sim time next is 5004000.0000, 
raw observation next is [3.0, 37.0, 0.0, 0.0, 26.0, 25.80009934946773, 0.5672273438275154, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.5457063711911359, 0.37, 0.0, 0.0, 0.6666666666666666, 0.6500082791223107, 0.6890757812758385, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6440283], dtype=float32), 1.8585362]. 
=============================================
[2019-04-04 03:41:11,190] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[84.19873]
 [84.32697]
 [84.52446]
 [84.61913]
 [84.50831]], R is [[84.25598145]
 [84.41342163]
 [84.56929016]
 [84.72359467]
 [84.63988495]].
[2019-04-04 03:41:11,429] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:41:11,429] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:41:11,433] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run30
[2019-04-04 03:41:11,805] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:41:11,805] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:41:11,809] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run30
[2019-04-04 03:41:12,020] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.9215786e-28 2.4735791e-23 2.7648462e-26 4.3922702e-24 3.6021109e-24
 3.7375043e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:41:12,021] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4151
[2019-04-04 03:41:12,051] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 36.0, 0.0, 0.0, 26.0, 25.42007737750374, 0.3687573127522527, 0.0, 1.0, 59980.25037548656], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4915200.0000, 
sim time next is 4915800.0000, 
raw observation next is [1.0, 36.0, 0.0, 0.0, 26.0, 25.41016231090414, 0.3703650056722856, 0.0, 1.0, 51749.92396301865], 
processed observation next is [0.0, 0.9130434782608695, 0.4903047091412743, 0.36, 0.0, 0.0, 0.6666666666666666, 0.6175135259086785, 0.6234550018907619, 0.0, 1.0, 0.24642820934770784], 
reward next is 0.7536, 
noisyNet noise sample is [array([0.26325625], dtype=float32), -0.018208096]. 
=============================================
[2019-04-04 03:41:12,459] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.6130106e-28 3.1412658e-24 1.2383945e-26 3.4663373e-24 2.6731982e-24
 4.2437650e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:41:12,459] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3709
[2019-04-04 03:41:12,496] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.0, 26.0, 0.0, 0.0, 26.0, 25.84312514588501, 0.5395104206158676, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4998600.0000, 
sim time next is 4999200.0000, 
raw observation next is [4.666666666666666, 27.0, 0.0, 0.0, 26.0, 25.7654162115691, 0.5229933635874905, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.5918744228993538, 0.27, 0.0, 0.0, 0.6666666666666666, 0.6471180176307584, 0.6743311211958302, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.09676106], dtype=float32), -1.302426]. 
=============================================
[2019-04-04 03:41:16,933] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:41:16,933] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:41:16,938] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run30
[2019-04-04 03:41:17,568] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.5521767e-27 3.2033226e-22 3.6179461e-25 1.1161507e-23 3.0896775e-23
 3.0972371e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:41:17,568] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8128
[2019-04-04 03:41:17,607] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.5, 70.0, 0.0, 0.0, 26.0, 23.41080842201293, -0.09330872821628405, 0.0, 1.0, 46524.46015133633], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 273600.0000, 
sim time next is 274200.0000, 
raw observation next is [-9.683333333333334, 69.5, 0.0, 0.0, 26.0, 23.3323253809277, -0.1032875480473283, 0.0, 1.0, 46717.53386556815], 
processed observation next is [1.0, 0.17391304347826086, 0.19436749769159742, 0.695, 0.0, 0.0, 0.6666666666666666, 0.4443604484106416, 0.46557081731755723, 0.0, 1.0, 0.22246444697889595], 
reward next is 0.7775, 
noisyNet noise sample is [array([-0.58871484], dtype=float32), 0.04355084]. 
=============================================
[2019-04-04 03:41:18,929] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:41:18,931] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:41:18,936] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run30
[2019-04-04 03:41:19,359] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:41:19,359] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:41:19,362] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run30
[2019-04-04 03:41:19,685] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:41:19,686] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:41:19,739] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run30
[2019-04-04 03:41:21,811] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:41:21,812] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:41:21,817] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run30
[2019-04-04 03:41:25,521] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:41:25,521] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:41:25,526] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run30
[2019-04-04 03:41:28,231] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.6572031e-26 2.5423541e-22 1.2366586e-24 2.7562892e-23 6.4048002e-23
 8.8398835e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:41:28,231] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8944
[2019-04-04 03:41:28,246] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.25, 89.0, 0.0, 0.0, 26.0, 24.23082506461778, 0.1113484315665553, 0.0, 1.0, 42428.88192590918], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 95400.0000, 
sim time next is 96000.0000, 
raw observation next is [-2.433333333333333, 88.33333333333334, 0.0, 0.0, 26.0, 24.19240022771296, 0.09729269250240746, 0.0, 1.0, 42594.51462153073], 
processed observation next is [1.0, 0.08695652173913043, 0.3951985226223454, 0.8833333333333334, 0.0, 0.0, 0.6666666666666666, 0.5160333523094133, 0.5324308975008024, 0.0, 1.0, 0.2028310220072892], 
reward next is 0.7972, 
noisyNet noise sample is [array([-1.5885466], dtype=float32), 0.20628743]. 
=============================================
[2019-04-04 03:41:28,257] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[77.91348 ]
 [77.91896 ]
 [77.91638 ]
 [77.912346]
 [77.95353 ]], R is [[77.95940399]
 [77.97776794]
 [77.99663544]
 [78.01608276]
 [78.03605652]].
[2019-04-04 03:41:32,941] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.5612246e-27 7.6821074e-24 1.2856951e-25 1.5045483e-23 4.0256279e-23
 2.8224650e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:41:32,941] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0408
[2019-04-04 03:41:33,003] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.5, 40.0, 0.0, 0.0, 26.0, 25.56189653091911, 0.3671376187320992, 1.0, 1.0, 34523.80684485524], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 411000.0000, 
sim time next is 411600.0000, 
raw observation next is [-9.5, 40.0, 0.0, 0.0, 26.0, 25.60628907505891, 0.3549069539082217, 1.0, 1.0, 41764.94630336553], 
processed observation next is [1.0, 0.782608695652174, 0.1994459833795014, 0.4, 0.0, 0.0, 0.6666666666666666, 0.6338574229215759, 0.6183023179694073, 1.0, 1.0, 0.19888069668269298], 
reward next is 0.8011, 
noisyNet noise sample is [array([-0.17392097], dtype=float32), -0.91742224]. 
=============================================
[2019-04-04 03:41:42,406] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.4254160e-27 9.2584400e-24 5.1241597e-26 9.2148893e-24 2.2664043e-24
 8.6730307e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:41:42,407] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6686
[2019-04-04 03:41:42,499] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 26.0, 24.8270477040896, 0.2483352095889506, 1.0, 1.0, 198252.2047524743], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 234000.0000, 
sim time next is 234600.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 26.0, 24.35222092240665, 0.2673168159477525, 1.0, 1.0, 199754.1877266978], 
processed observation next is [1.0, 0.7391304347826086, 0.368421052631579, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5293517435338876, 0.5891056053159175, 1.0, 1.0, 0.95121041774618], 
reward next is 0.0488, 
noisyNet noise sample is [array([0.84950036], dtype=float32), -0.12498309]. 
=============================================
[2019-04-04 03:42:10,386] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [7.5988221e-28 3.8544179e-23 6.4261065e-26 9.6703192e-24 2.9825437e-24
 4.5155885e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:42:10,387] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3596
[2019-04-04 03:42:10,401] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.466666666666667, 71.0, 0.0, 0.0, 26.0, 24.21914652531844, 0.116268503880695, 0.0, 1.0, 41576.60701682534], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 782400.0000, 
sim time next is 783000.0000, 
raw observation next is [-7.55, 71.0, 0.0, 0.0, 26.0, 24.20673328394815, 0.1122078271517264, 0.0, 1.0, 41562.75883908949], 
processed observation next is [1.0, 0.043478260869565216, 0.25346260387811637, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5172277736623458, 0.5374026090505755, 0.0, 1.0, 0.19791789923375946], 
reward next is 0.8021, 
noisyNet noise sample is [array([0.04938509], dtype=float32), 1.3308792]. 
=============================================
[2019-04-04 03:42:10,434] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[80.24584 ]
 [80.23961 ]
 [80.153496]
 [80.039696]
 [79.90993 ]], R is [[80.29338837]
 [80.29247284]
 [80.29154968]
 [80.29060364]
 [80.28959656]].
[2019-04-04 03:42:23,031] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0814374e-26 1.5585269e-22 3.1383776e-25 4.7105848e-23 3.3995532e-23
 5.4185543e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:42:23,033] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6913
[2019-04-04 03:42:23,127] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.45, 85.0, 41.0, 45.0, 26.0, 24.97017561923451, 0.3040882156890815, 0.0, 1.0, 45546.85599563065], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 577800.0000, 
sim time next is 578400.0000, 
raw observation next is [-1.533333333333333, 85.66666666666667, 34.16666666666667, 37.5, 26.0, 24.95605252552194, 0.3027443074381306, 0.0, 1.0, 50127.51130480439], 
processed observation next is [0.0, 0.6956521739130435, 0.42012927054478305, 0.8566666666666667, 0.1138888888888889, 0.04143646408839779, 0.6666666666666666, 0.579671043793495, 0.6009147691460436, 0.0, 1.0, 0.2387024347847828], 
reward next is 0.7613, 
noisyNet noise sample is [array([-1.0972018], dtype=float32), -0.4259967]. 
=============================================
[2019-04-04 03:42:33,987] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.3562542e-28 1.6650966e-24 8.2364998e-27 1.3784975e-25 7.4708150e-25
 7.4589577e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:42:33,988] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3136
[2019-04-04 03:42:34,011] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.56666666666667, 78.0, 39.66666666666666, 0.0, 26.0, 25.73413726967065, 0.6198123225452976, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1154400.0000, 
sim time next is 1155000.0000, 
raw observation next is [15.03333333333333, 76.5, 48.33333333333333, 0.0, 26.0, 25.7391352170314, 0.6153846718970863, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.879039704524469, 0.765, 0.1611111111111111, 0.0, 0.6666666666666666, 0.6449279347526167, 0.7051282239656954, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.48503393], dtype=float32), -2.3582966]. 
=============================================
[2019-04-04 03:42:34,020] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[85.03822]
 [84.72054]
 [84.5795 ]
 [84.52347]
 [84.38563]], R is [[85.5632782 ]
 [85.70764923]
 [85.85057068]
 [85.99206543]
 [85.96697998]].
[2019-04-04 03:42:36,960] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.1268694e-29 4.6229773e-25 9.9375356e-28 1.3578145e-25 1.3672534e-25
 4.4436774e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 03:42:36,960] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4753
[2019-04-04 03:42:36,972] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.3, 82.33333333333334, 0.0, 0.0, 26.0, 24.83370966923195, 0.2567677284626809, 0.0, 1.0, 41560.48373720986], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 857400.0000, 
sim time next is 858000.0000, 
raw observation next is [-3.2, 81.66666666666667, 0.0, 0.0, 26.0, 24.81111438259661, 0.2513255749680333, 0.0, 1.0, 41484.55922090947], 
processed observation next is [1.0, 0.9565217391304348, 0.37396121883656513, 0.8166666666666668, 0.0, 0.0, 0.6666666666666666, 0.5675928652163842, 0.5837751916560111, 0.0, 1.0, 0.1975455200995689], 
reward next is 0.8025, 
noisyNet noise sample is [array([0.6036185], dtype=float32), -1.2402729]. 
=============================================
[2019-04-04 03:42:36,986] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[84.882164]
 [84.85393 ]
 [84.715965]
 [84.69736 ]
 [84.69401 ]], R is [[84.84081268]
 [84.79450226]
 [84.74835968]
 [84.70264435]
 [84.65634155]].
[2019-04-04 03:42:40,095] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [4.2468421e-28 1.2193224e-23 2.4099232e-26 1.1447087e-24 1.6628304e-24
 1.6601913e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:42:40,096] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.6504
[2019-04-04 03:42:40,115] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.383333333333333, 71.0, 0.0, 0.0, 26.0, 24.26091500837919, 0.1218029288757944, 0.0, 1.0, 41579.78014081237], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 781800.0000, 
sim time next is 782400.0000, 
raw observation next is [-7.466666666666667, 71.0, 0.0, 0.0, 26.0, 24.21914652531844, 0.116268503880695, 0.0, 1.0, 41576.60701682534], 
processed observation next is [1.0, 0.043478260869565216, 0.25577100646352724, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5182622104432033, 0.5387561679602316, 0.0, 1.0, 0.19798384293726354], 
reward next is 0.8020, 
noisyNet noise sample is [array([-1.5598549], dtype=float32), 0.34769216]. 
=============================================
[2019-04-04 03:42:44,935] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.9496964e-31 5.9252679e-27 3.6756000e-29 1.9572218e-26 8.9977744e-27
 7.9037137e-31 1.0000000e+00], sum to 1.0000
[2019-04-04 03:42:44,935] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3400
[2019-04-04 03:42:44,963] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.95, 78.0, 57.0, 0.0, 26.0, 26.48826124909874, 0.7036004561591905, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1006200.0000, 
sim time next is 1006800.0000, 
raw observation next is [15.13333333333333, 77.0, 51.66666666666666, 0.0, 26.0, 26.77404791922958, 0.7312037796154102, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.8818097876269622, 0.77, 0.1722222222222222, 0.0, 0.6666666666666666, 0.7311706599357984, 0.7437345932051368, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.31275392], dtype=float32), 0.58632505]. 
=============================================
[2019-04-04 03:42:51,773] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.0215745e-31 4.0750771e-27 3.9736343e-29 3.3048854e-27 4.5125925e-27
 6.9940263e-31 1.0000000e+00], sum to 1.0000
[2019-04-04 03:42:51,773] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8796
[2019-04-04 03:42:51,788] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 77.0, 0.0, 0.0, 26.0, 25.9812211168066, 0.6254163956159512, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1051200.0000, 
sim time next is 1051800.0000, 
raw observation next is [14.3, 77.16666666666667, 0.0, 0.0, 26.0, 25.95898696840225, 0.6159443721084826, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.8587257617728533, 0.7716666666666667, 0.0, 0.0, 0.6666666666666666, 0.6632489140335208, 0.7053147907028275, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7863416], dtype=float32), -1.8232032]. 
=============================================
[2019-04-04 03:42:54,080] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.8631785e-30 1.5778406e-25 1.1515650e-28 1.2304740e-26 1.0472698e-26
 9.0938267e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 03:42:54,084] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4067
[2019-04-04 03:42:54,089] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.0, 96.0, 0.0, 0.0, 26.0, 23.60667048239833, 0.1686142619788823, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1230600.0000, 
sim time next is 1231200.0000, 
raw observation next is [15.0, 96.0, 0.0, 0.0, 26.0, 23.58347076702589, 0.1641174065662103, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.2608695652173913, 0.8781163434903049, 0.96, 0.0, 0.0, 0.6666666666666666, 0.4652892305854908, 0.5547058021887368, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.23231001], dtype=float32), 3.1535802]. 
=============================================
[2019-04-04 03:42:54,576] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.2875779e-31 3.4574855e-26 1.5232264e-28 3.8450720e-26 1.6798067e-26
 6.1160760e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 03:42:54,618] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2787
[2019-04-04 03:42:54,628] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.23333333333333, 73.0, 0.0, 0.0, 26.0, 25.69369264196057, 0.6387205606193155, 0.0, 1.0, 59485.80764628713], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1124400.0000, 
sim time next is 1125000.0000, 
raw observation next is [11.05, 74.0, 0.0, 0.0, 26.0, 25.67121097274541, 0.6402287445864445, 0.0, 1.0, 53247.10227817594], 
processed observation next is [0.0, 0.0, 0.7686980609418284, 0.74, 0.0, 0.0, 0.6666666666666666, 0.6392675810621175, 0.7134095815288148, 0.0, 1.0, 0.2535576298960759], 
reward next is 0.7464, 
noisyNet noise sample is [array([0.8535149], dtype=float32), 0.7679686]. 
=============================================
[2019-04-04 03:42:54,673] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[90.26076 ]
 [90.26516 ]
 [90.338036]
 [90.6555  ]
 [90.68795 ]], R is [[90.08823395]
 [89.90409088]
 [90.00505066]
 [90.10500336]
 [90.2039566 ]].
[2019-04-04 03:42:56,297] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.5784339e-30 2.2834633e-26 2.2393534e-29 1.1612662e-27 2.2174751e-27
 1.3160707e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 03:42:56,301] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4184
[2019-04-04 03:42:56,305] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [18.38333333333334, 64.66666666666667, 96.0, 0.0, 26.0, 25.06704685860541, 0.4960938057167865, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1177800.0000, 
sim time next is 1178400.0000, 
raw observation next is [18.46666666666667, 64.33333333333334, 88.0, 0.0, 26.0, 25.05931077393005, 0.4921197692192287, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.9741458910433982, 0.6433333333333334, 0.29333333333333333, 0.0, 0.6666666666666666, 0.5882758978275042, 0.6640399230730762, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.43800813], dtype=float32), 0.70316094]. 
=============================================
[2019-04-04 03:42:58,698] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.9986757e-31 1.3887263e-25 3.1337670e-28 1.8266433e-26 2.8560527e-26
 9.3867922e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 03:42:58,700] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9787
[2019-04-04 03:42:58,713] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.166666666666667, 81.0, 0.0, 0.0, 26.0, 25.63333331967036, 0.4874068363726511, 0.0, 1.0, 22136.11749005866], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1578000.0000, 
sim time next is 1578600.0000, 
raw observation next is [5.25, 80.5, 0.0, 0.0, 26.0, 25.54559118834849, 0.4849176258494902, 0.0, 1.0, 70936.36000230153], 
processed observation next is [1.0, 0.2608695652173913, 0.60803324099723, 0.805, 0.0, 0.0, 0.6666666666666666, 0.6287992656957074, 0.6616392086164967, 0.0, 1.0, 0.33779219048715015], 
reward next is 0.6622, 
noisyNet noise sample is [array([-0.25436684], dtype=float32), 0.31290329]. 
=============================================
[2019-04-04 03:43:01,042] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.43432580e-31 1.25429315e-26 1.37231464e-28 5.28687568e-27
 1.73952134e-26 1.96486326e-30 1.00000000e+00], sum to 1.0000
[2019-04-04 03:43:01,042] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8029
[2019-04-04 03:43:01,064] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.2, 82.0, 0.0, 0.0, 26.0, 25.5898401825248, 0.583613518286071, 0.0, 1.0, 93418.84051648348], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1638000.0000, 
sim time next is 1638600.0000, 
raw observation next is [7.2, 82.66666666666667, 0.0, 0.0, 26.0, 25.53142472217041, 0.6088248626438326, 0.0, 1.0, 91388.5079616094], 
processed observation next is [1.0, 1.0, 0.662049861495845, 0.8266666666666667, 0.0, 0.0, 0.6666666666666666, 0.6276187268475342, 0.7029416208812775, 0.0, 1.0, 0.43518337124575907], 
reward next is 0.5648, 
noisyNet noise sample is [array([-0.6564864], dtype=float32), 0.78806555]. 
=============================================
[2019-04-04 03:43:04,452] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.1451681e-30 3.0032065e-25 2.7566818e-28 1.7514265e-26 7.7760047e-26
 9.0968111e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 03:43:04,465] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4414
[2019-04-04 03:43:04,476] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.516666666666667, 92.0, 0.0, 0.0, 26.0, 25.40570659932159, 0.5130729119506064, 0.0, 1.0, 81657.49092201325], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1318200.0000, 
sim time next is 1318800.0000, 
raw observation next is [1.433333333333334, 92.0, 0.0, 0.0, 26.0, 25.33685704321579, 0.5142852841007115, 0.0, 1.0, 77920.50857822575], 
processed observation next is [1.0, 0.2608695652173913, 0.502308402585411, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6114047536013159, 0.6714284280335705, 0.0, 1.0, 0.37105004084869403], 
reward next is 0.6289, 
noisyNet noise sample is [array([2.778918], dtype=float32), -1.1218932]. 
=============================================
[2019-04-04 03:43:14,510] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2692071e-29 1.2246686e-24 7.0574516e-27 4.0866010e-25 5.3907648e-25
 1.4185364e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:43:14,511] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2362
[2019-04-04 03:43:14,526] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.266666666666667, 92.0, 0.0, 0.0, 26.0, 25.3541770853876, 0.5299962511917439, 0.0, 1.0, 45991.24918938802], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1320000.0000, 
sim time next is 1320600.0000, 
raw observation next is [1.183333333333333, 92.0, 0.0, 0.0, 26.0, 25.41026722667187, 0.5278991594501431, 0.0, 1.0, 21569.52710518911], 
processed observation next is [1.0, 0.2608695652173913, 0.49538319482917825, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6175222688893225, 0.675966386483381, 0.0, 1.0, 0.10271203383423386], 
reward next is 0.8973, 
noisyNet noise sample is [array([-1.6757374], dtype=float32), 0.45210806]. 
=============================================
[2019-04-04 03:43:17,012] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [8.6589857e-27 7.5406558e-23 3.6384715e-25 1.8358882e-23 1.7554492e-23
 7.2794001e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:43:17,012] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4983
[2019-04-04 03:43:17,064] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.75, 71.0, 120.0, 0.0, 26.0, 24.96569670071434, 0.2507682187919532, 0.0, 1.0, 52881.06580605587], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1859400.0000, 
sim time next is 1860000.0000, 
raw observation next is [-4.666666666666667, 71.0, 128.3333333333333, 6.666666666666665, 26.0, 24.95513774760073, 0.2551423364577226, 0.0, 1.0, 54035.30031438558], 
processed observation next is [0.0, 0.5217391304347826, 0.3333333333333333, 0.71, 0.42777777777777765, 0.00736648250460405, 0.6666666666666666, 0.5795948123000608, 0.5850474454859075, 0.0, 1.0, 0.2573109538780266], 
reward next is 0.7427, 
noisyNet noise sample is [array([0.94336337], dtype=float32), 1.9281417]. 
=============================================
[2019-04-04 03:43:17,085] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[79.70589]
 [79.87245]
 [80.06519]
 [80.33077]
 [80.6945 ]], R is [[80.03677368]
 [79.98458862]
 [79.9743042 ]
 [80.0174408 ]
 [80.06976318]].
[2019-04-04 03:43:17,168] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.3093462e-29 2.6919851e-25 1.2312482e-27 3.8870460e-25 2.9911559e-25
 5.3258975e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 03:43:17,169] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2396
[2019-04-04 03:43:17,186] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.6, 76.0, 0.0, 0.0, 26.0, 26.11655770960757, 0.6917118072088183, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1548000.0000, 
sim time next is 1548600.0000, 
raw observation next is [6.416666666666667, 77.0, 0.0, 0.0, 26.0, 26.0733046707324, 0.6296122677314592, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.6403508771929826, 0.77, 0.0, 0.0, 0.6666666666666666, 0.6727753892277001, 0.7098707559104863, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.62966764], dtype=float32), -0.08018379]. 
=============================================
[2019-04-04 03:43:39,762] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.45862412e-27 1.04194414e-22 2.40295709e-25 5.44587128e-24
 1.23070592e-23 1.51823908e-26 1.00000000e+00], sum to 1.0000
[2019-04-04 03:43:39,762] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6996
[2019-04-04 03:43:39,798] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 83.33333333333334, 0.0, 0.0, 26.0, 24.77856812355, 0.2454401931784066, 0.0, 1.0, 45698.64618853736], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1809600.0000, 
sim time next is 1810200.0000, 
raw observation next is [-5.0, 82.66666666666667, 0.0, 0.0, 26.0, 24.75033799253831, 0.2388588845515238, 0.0, 1.0, 45643.30429965832], 
processed observation next is [0.0, 0.9565217391304348, 0.32409972299168976, 0.8266666666666667, 0.0, 0.0, 0.6666666666666666, 0.5625281660448591, 0.5796196281838413, 0.0, 1.0, 0.21734906809361104], 
reward next is 0.7827, 
noisyNet noise sample is [array([0.7904306], dtype=float32), 0.053971156]. 
=============================================
[2019-04-04 03:43:40,444] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.8020536e-25 1.9002613e-21 3.2698904e-24 7.2006712e-23 5.6693002e-22
 2.7643942e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 03:43:40,446] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6208
[2019-04-04 03:43:40,465] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.9, 79.0, 0.0, 0.0, 26.0, 24.76138725471395, 0.1802179228437144, 0.0, 1.0, 44748.96879417], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1891800.0000, 
sim time next is 1892400.0000, 
raw observation next is [-6.0, 77.66666666666667, 0.0, 0.0, 26.0, 24.72419584845051, 0.1725511036929352, 0.0, 1.0, 44765.79111581334], 
processed observation next is [0.0, 0.9130434782608695, 0.296398891966759, 0.7766666666666667, 0.0, 0.0, 0.6666666666666666, 0.5603496540375424, 0.5575170345643118, 0.0, 1.0, 0.21317043388482546], 
reward next is 0.7868, 
noisyNet noise sample is [array([-0.6038524], dtype=float32), -0.008771473]. 
=============================================
[2019-04-04 03:43:48,936] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.1878370e-26 3.2838189e-22 7.5858876e-25 2.0409616e-22 6.4363956e-23
 7.2922401e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:43:48,937] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9800
[2019-04-04 03:43:48,986] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.833333333333334, 85.0, 0.0, 0.0, 26.0, 25.01412686548381, 0.2470434658567346, 0.0, 1.0, 42433.77974203333], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1880400.0000, 
sim time next is 1881000.0000, 
raw observation next is [-4.75, 84.5, 0.0, 0.0, 26.0, 25.01824448817991, 0.2478340842463581, 0.0, 1.0, 38676.10905627751], 
processed observation next is [0.0, 0.782608695652174, 0.3310249307479225, 0.845, 0.0, 0.0, 0.6666666666666666, 0.5848537073483259, 0.5826113614154527, 0.0, 1.0, 0.18417194788703578], 
reward next is 0.8158, 
noisyNet noise sample is [array([-0.07276846], dtype=float32), -0.044499207]. 
=============================================
[2019-04-04 03:43:48,993] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[77.196724]
 [77.10686 ]
 [76.99108 ]
 [76.87174 ]
 [76.73716 ]], R is [[77.34564972]
 [77.37013245]
 [77.36152649]
 [77.32067871]
 [77.30808258]].
[2019-04-04 03:44:05,583] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-04 03:44:05,584] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 03:44:05,584] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:44:05,585] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 03:44:05,585] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:44:05,587] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run41
[2019-04-04 03:44:05,623] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 03:44:05,624] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:44:05,627] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run41
[2019-04-04 03:44:05,667] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run41
[2019-04-04 03:44:36,115] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.34865117], dtype=float32), 0.19276884]
[2019-04-04 03:44:36,115] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-7.409646143333334, 39.25634543666667, 37.22590891, 688.4678739166667, 26.0, 25.77307556295186, 0.3866500655719837, 1.0, 1.0, 0.0]
[2019-04-04 03:44:36,115] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 03:44:36,116] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [4.0398187e-27 7.1057364e-24 2.3747340e-26 6.9656678e-24 8.1923915e-24
 1.2146728e-27 1.0000000e+00], sampled 0.6442100493932672
[2019-04-04 03:46:20,235] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.34865117], dtype=float32), 0.19276884]
[2019-04-04 03:46:20,236] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-1.033333333333333, 70.66666666666667, 0.0, 0.0, 26.0, 24.62345971023871, 0.2080839519253333, 0.0, 1.0, 97753.89279832368]
[2019-04-04 03:46:20,236] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 03:46:20,237] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [2.9413116e-26 5.3150984e-22 1.2740758e-24 5.3625893e-23 9.9480295e-23
 1.4223478e-25 1.0000000e+00], sampled 0.7650920730510039
[2019-04-04 03:47:17,097] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 03:47:44,465] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 03:47:49,077] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 03:47:50,130] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 4000000, evaluation results [4000000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 03:47:52,809] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.5179098e-27 5.1256730e-23 1.6444354e-25 2.1847992e-24 8.7900246e-24
 6.8256151e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:47:52,809] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3109
[2019-04-04 03:47:52,849] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.7, 80.5, 0.0, 0.0, 26.0, 23.91855928716343, 0.03999057634350158, 0.0, 1.0, 43631.33120207255], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2097000.0000, 
sim time next is 2097600.0000, 
raw observation next is [-6.700000000000001, 79.66666666666667, 0.0, 0.0, 26.0, 23.87897140928377, 0.03193400653815419, 0.0, 1.0, 43594.9564209496], 
processed observation next is [1.0, 0.2608695652173913, 0.2770083102493075, 0.7966666666666667, 0.0, 0.0, 0.6666666666666666, 0.48991428410698096, 0.5106446688460514, 0.0, 1.0, 0.2075950305759505], 
reward next is 0.7924, 
noisyNet noise sample is [array([0.5577117], dtype=float32), 0.6804675]. 
=============================================
[2019-04-04 03:47:54,062] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.7903010e-27 3.3762388e-23 2.3793404e-26 2.2310458e-24 3.7266475e-24
 5.1526183e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:47:54,063] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1583
[2019-04-04 03:47:54,252] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.166666666666666, 45.33333333333333, 63.5, 683.6666666666667, 26.0, 25.26752740538911, 0.2518065774858636, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2454000.0000, 
sim time next is 2454600.0000, 
raw observation next is [-5.883333333333333, 44.16666666666667, 66.0, 702.3333333333333, 26.0, 25.22071208438101, 0.2465256550473158, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.2996306555863343, 0.4416666666666667, 0.22, 0.7760589318600367, 0.6666666666666666, 0.6017260070317508, 0.5821752183491052, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.06784691], dtype=float32), -0.75317454]. 
=============================================
[2019-04-04 03:47:58,594] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.0770188e-28 1.9548884e-24 5.7312798e-27 6.0007939e-25 2.0748316e-25
 2.0354506e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:47:58,594] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8561
[2019-04-04 03:47:58,688] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.116666666666667, 85.5, 82.33333333333334, 31.33333333333334, 26.0, 25.64342517002868, 0.3027720382361976, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2279400.0000, 
sim time next is 2280000.0000, 
raw observation next is [-7.833333333333334, 84.0, 91.66666666666667, 35.16666666666666, 26.0, 25.60224602895041, 0.300337264534932, 1.0, 1.0, 18741.36186802325], 
processed observation next is [1.0, 0.391304347826087, 0.2456140350877193, 0.84, 0.3055555555555556, 0.038858195211786364, 0.6666666666666666, 0.6335205024125342, 0.600112421511644, 1.0, 1.0, 0.08924458032392024], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.83998156], dtype=float32), 1.6194887]. 
=============================================
[2019-04-04 03:47:58,691] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[84.96005 ]
 [84.85981 ]
 [84.77839 ]
 [84.77067 ]
 [84.658455]], R is [[85.05752563]
 [85.20695496]
 [85.35488892]
 [85.50134277]
 [85.64633179]].
[2019-04-04 03:47:59,365] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.3757014e-29 5.6260529e-25 8.2003524e-27 3.1016369e-25 7.8637092e-25
 3.1253251e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 03:47:59,365] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0888
[2019-04-04 03:47:59,438] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.533333333333333, 68.66666666666667, 0.0, 0.0, 26.0, 25.86854018152923, 0.4811061108272527, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2226000.0000, 
sim time next is 2226600.0000, 
raw observation next is [-4.55, 69.0, 0.0, 0.0, 26.0, 25.85488005648553, 0.4684737004294818, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.3365650969529086, 0.69, 0.0, 0.0, 0.6666666666666666, 0.6545733380404609, 0.6561579001431607, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9803584], dtype=float32), 0.9634077]. 
=============================================
[2019-04-04 03:48:06,754] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.3297872e-30 9.0230077e-26 1.3093619e-28 4.5631585e-26 2.4840903e-26
 4.1629175e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 03:48:06,754] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3457
[2019-04-04 03:48:06,846] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.566666666666666, 71.33333333333333, 175.1666666666667, 60.0, 26.0, 25.73251123426163, 0.3479477692854727, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2284800.0000, 
sim time next is 2285400.0000, 
raw observation next is [-5.283333333333333, 69.66666666666667, 172.3333333333333, 70.0, 26.0, 25.74319499846047, 0.3495057331877049, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.31625115420129274, 0.6966666666666668, 0.5744444444444443, 0.07734806629834254, 0.6666666666666666, 0.6452662498717059, 0.6165019110625684, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.51651347], dtype=float32), -0.8390202]. 
=============================================
[2019-04-04 03:48:09,017] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.6112063e-28 1.6817641e-24 3.6614853e-26 2.6310515e-24 1.7767847e-24
 3.1942588e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:48:09,019] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1625
[2019-04-04 03:48:09,128] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.15, 40.0, 0.0, 0.0, 26.0, 25.10267519125579, 0.3278620009727205, 0.0, 1.0, 18709.45943026739], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2575800.0000, 
sim time next is 2576400.0000, 
raw observation next is [-1.333333333333333, 41.33333333333334, 0.0, 0.0, 26.0, 25.01223537869182, 0.3252990395828078, 0.0, 1.0, 72571.96969759038], 
processed observation next is [1.0, 0.8260869565217391, 0.42566943674976926, 0.41333333333333344, 0.0, 0.0, 0.6666666666666666, 0.5843529482243183, 0.6084330131942693, 0.0, 1.0, 0.34558080808376374], 
reward next is 0.6544, 
noisyNet noise sample is [array([1.4954497], dtype=float32), 0.38622922]. 
=============================================
[2019-04-04 03:48:10,276] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.6187502e-29 5.9381071e-24 1.5098644e-26 5.0712885e-25 1.0309282e-24
 3.5732267e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:48:10,277] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4333
[2019-04-04 03:48:10,338] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.399999999999999, 61.5, 0.0, 0.0, 26.0, 24.8511693542735, 0.2644473504060884, 0.0, 1.0, 41886.93867219034], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2591400.0000, 
sim time next is 2592000.0000, 
raw observation next is [-4.5, 62.0, 0.0, 0.0, 26.0, 24.80601395118446, 0.2561451597604474, 0.0, 1.0, 41898.09662132264], 
processed observation next is [1.0, 0.0, 0.3379501385041552, 0.62, 0.0, 0.0, 0.6666666666666666, 0.5671678292653718, 0.5853817199201491, 0.0, 1.0, 0.1995147458158221], 
reward next is 0.8005, 
noisyNet noise sample is [array([-1.2579163], dtype=float32), -0.9705319]. 
=============================================
[2019-04-04 03:48:10,363] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[83.01421 ]
 [83.03221 ]
 [83.06172 ]
 [83.16479 ]
 [83.284744]], R is [[82.95693207]
 [82.92790222]
 [82.89915466]
 [82.87060547]
 [82.84212494]].
[2019-04-04 03:48:20,390] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.1079525e-28 6.1881153e-25 1.1859926e-26 1.6598330e-24 2.9214215e-24
 1.8388343e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:48:20,391] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3974
[2019-04-04 03:48:20,432] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.833333333333333, 66.33333333333334, 0.0, 0.0, 26.0, 25.48282836991365, 0.4510419422067921, 0.0, 1.0, 18757.16575054029], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2668800.0000, 
sim time next is 2669400.0000, 
raw observation next is [-2.15, 67.0, 0.0, 0.0, 26.0, 25.46109691301452, 0.4427315418962319, 0.0, 1.0, 31339.54885192225], 
processed observation next is [1.0, 0.9130434782608695, 0.4030470914127424, 0.67, 0.0, 0.0, 0.6666666666666666, 0.6217580760845433, 0.6475771806320773, 0.0, 1.0, 0.14923594691391548], 
reward next is 0.8508, 
noisyNet noise sample is [array([-0.80266315], dtype=float32), -0.015237499]. 
=============================================
[2019-04-04 03:48:48,571] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.8701527e-27 1.0507849e-23 2.5697380e-26 3.2036128e-24 5.2558564e-24
 2.3768161e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:48:48,572] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9992
[2019-04-04 03:48:48,603] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.433333333333333, 52.0, 0.0, 0.0, 26.0, 25.11085527817831, 0.4046251918533503, 0.0, 1.0, 61779.19744075568], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2580000.0000, 
sim time next is 2580600.0000, 
raw observation next is [-2.616666666666667, 54.0, 0.0, 0.0, 26.0, 25.28989237578768, 0.4157496878829272, 0.0, 1.0, 48658.19523439195], 
processed observation next is [1.0, 0.8695652173913043, 0.3901200369344414, 0.54, 0.0, 0.0, 0.6666666666666666, 0.60749103131564, 0.6385832292943091, 0.0, 1.0, 0.2317056915923426], 
reward next is 0.7683, 
noisyNet noise sample is [array([-0.54555976], dtype=float32), 0.1261941]. 
=============================================
[2019-04-04 03:49:03,712] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.66439530e-29 5.16642066e-25 1.04889704e-26 1.09550179e-24
 2.29508036e-24 1.91478591e-28 1.00000000e+00], sum to 1.0000
[2019-04-04 03:49:03,725] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5632
[2019-04-04 03:49:03,750] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.833333333333333, 63.66666666666667, 0.0, 0.0, 26.0, 25.23039108213945, 0.3548893402030302, 0.0, 1.0, 43937.55701729131], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2848200.0000, 
sim time next is 2848800.0000, 
raw observation next is [1.666666666666667, 65.33333333333334, 0.0, 0.0, 26.0, 25.22931550665036, 0.3525981238629425, 0.0, 1.0, 43155.42125199507], 
processed observation next is [1.0, 1.0, 0.5087719298245615, 0.6533333333333334, 0.0, 0.0, 0.6666666666666666, 0.6024429588875299, 0.6175327079543141, 0.0, 1.0, 0.20550200596188128], 
reward next is 0.7945, 
noisyNet noise sample is [array([-0.3699613], dtype=float32), -0.6232429]. 
=============================================
[2019-04-04 03:49:07,303] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.0032706e-28 5.9273344e-24 2.6006442e-26 8.1788044e-25 3.0096994e-24
 9.7501234e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:49:07,303] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2200
[2019-04-04 03:49:07,320] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.76854295212603, 0.2719883519393703, 0.0, 1.0, 44134.15580724889], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2764200.0000, 
sim time next is 2764800.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.7323638617833, 0.2635584372672933, 0.0, 1.0, 43861.30102329209], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5610303218152749, 0.587852812422431, 0.0, 1.0, 0.20886333820615283], 
reward next is 0.7911, 
noisyNet noise sample is [array([2.091378], dtype=float32), -2.2554896]. 
=============================================
[2019-04-04 03:49:14,581] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.7654132e-28 8.5231234e-25 4.3931178e-27 6.8044607e-25 9.6100406e-25
 3.1689899e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:49:14,582] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3967
[2019-04-04 03:49:14,630] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.5, 46.0, 109.0, 806.0, 26.0, 25.10572168245595, 0.3593671628727835, 0.0, 1.0, 18713.06802973763], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3072600.0000, 
sim time next is 3073200.0000, 
raw observation next is [-1.333333333333333, 44.66666666666667, 107.3333333333333, 800.8333333333334, 26.0, 25.10802997689351, 0.3609231561962134, 0.0, 1.0, 18711.65534988897], 
processed observation next is [0.0, 0.5652173913043478, 0.42566943674976926, 0.4466666666666667, 0.3577777777777777, 0.8848987108655617, 0.6666666666666666, 0.5923358314077923, 0.6203077187320711, 0.0, 1.0, 0.089103120713757], 
reward next is 0.9109, 
noisyNet noise sample is [array([-0.9390637], dtype=float32), -0.21780558]. 
=============================================
[2019-04-04 03:49:15,723] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.6357605e-26 6.9558236e-22 1.2162001e-24 1.2730520e-22 1.0888533e-22
 1.3256628e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 03:49:15,724] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6165
[2019-04-04 03:49:15,737] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.2113661826549, 0.09871443371297679, 0.0, 1.0, 39621.6907900564], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3036000.0000, 
sim time next is 3036600.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.18252824625887, 0.09052494890158751, 0.0, 1.0, 39745.01637977498], 
processed observation next is [0.0, 0.13043478260869565, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.515210687188239, 0.5301749829671959, 0.0, 1.0, 0.18926198276083323], 
reward next is 0.8107, 
noisyNet noise sample is [array([0.7199224], dtype=float32), -0.501268]. 
=============================================
[2019-04-04 03:49:16,832] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.52811679e-26 5.19454985e-22 7.51216709e-25 1.30679534e-23
 6.91885035e-23 2.33363249e-25 1.00000000e+00], sum to 1.0000
[2019-04-04 03:49:16,833] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3819
[2019-04-04 03:49:16,862] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 77.0, 0.0, 0.0, 26.0, 24.135237355238, 0.1312922337417096, 0.0, 1.0, 47978.00283519884], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2959800.0000, 
sim time next is 2960400.0000, 
raw observation next is [-4.0, 77.0, 0.0, 0.0, 26.0, 24.05684603231575, 0.1168749166602233, 0.0, 1.0, 54514.84568551352], 
processed observation next is [0.0, 0.2608695652173913, 0.3518005540166205, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5047371693596459, 0.5389583055534077, 0.0, 1.0, 0.25959450326435013], 
reward next is 0.7404, 
noisyNet noise sample is [array([1.1258296], dtype=float32), 0.7115024]. 
=============================================
[2019-04-04 03:49:17,041] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.5909067e-26 3.0467734e-21 9.5126567e-25 6.3930558e-24 3.3769728e-23
 5.9324405e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:49:17,043] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2273
[2019-04-04 03:49:17,059] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.14882948909623, 0.08254440193817912, 0.0, 1.0, 39861.80825189553], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3037200.0000, 
sim time next is 3037800.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.11406771180776, 0.07546812518156033, 0.0, 1.0, 39962.04879645998], 
processed observation next is [0.0, 0.13043478260869565, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5095056426506467, 0.5251560417271868, 0.0, 1.0, 0.19029547045933323], 
reward next is 0.8097, 
noisyNet noise sample is [array([-1.4079844], dtype=float32), 0.6677357]. 
=============================================
[2019-04-04 03:49:24,059] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.6073493e-30 1.7477757e-26 2.3841898e-28 1.2423976e-26 1.4306128e-26
 3.3192203e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 03:49:24,059] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6918
[2019-04-04 03:49:24,124] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.666666666666666, 100.0, 85.66666666666667, 434.5, 26.0, 25.98033147509004, 0.4692270018139912, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3141600.0000, 
sim time next is 3142200.0000, 
raw observation next is [6.833333333333334, 100.0, 88.33333333333334, 477.0, 26.0, 26.07387803987232, 0.5029197105045956, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.651892890120037, 1.0, 0.29444444444444445, 0.5270718232044199, 0.6666666666666666, 0.67282316998936, 0.6676399035015318, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.21958615], dtype=float32), 0.19945516]. 
=============================================
[2019-04-04 03:49:37,419] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.0867822e-27 1.9287793e-22 5.8242059e-25 1.1623411e-23 5.1913974e-23
 6.0647638e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:49:37,421] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9167
[2019-04-04 03:49:37,435] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.083333333333334, 76.83333333333334, 0.0, 0.0, 26.0, 24.66028106469301, 0.2671806405980895, 0.0, 1.0, 43998.25219366558], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3298200.0000, 
sim time next is 3298800.0000, 
raw observation next is [-9.266666666666667, 76.66666666666667, 0.0, 0.0, 26.0, 24.65952195806871, 0.2604934506617724, 0.0, 1.0, 43749.08787626091], 
processed observation next is [1.0, 0.17391304347826086, 0.20590951061865187, 0.7666666666666667, 0.0, 0.0, 0.6666666666666666, 0.5549601631723924, 0.5868311502205908, 0.0, 1.0, 0.20832898988695672], 
reward next is 0.7917, 
noisyNet noise sample is [array([-1.0147803], dtype=float32), -0.009108404]. 
=============================================
[2019-04-04 03:49:37,696] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [9.1178733e-28 3.0894317e-22 1.2522618e-25 2.9909748e-24 1.1914495e-23
 2.7788673e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:49:37,696] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1752
[2019-04-04 03:49:37,710] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 71.0, 0.0, 0.0, 26.0, 25.36818814896056, 0.3964003562589438, 0.0, 1.0, 41793.53916371293], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3717600.0000, 
sim time next is 3718200.0000, 
raw observation next is [-3.0, 71.0, 0.0, 0.0, 26.0, 25.49731616549926, 0.4009323513425718, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.3795013850415513, 0.71, 0.0, 0.0, 0.6666666666666666, 0.6247763471249383, 0.6336441171141906, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.79181916], dtype=float32), 1.8504689]. 
=============================================
[2019-04-04 03:49:38,180] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.5063492e-27 3.5200526e-23 7.0645391e-26 7.1010214e-24 1.3684602e-23
 4.6431205e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:49:38,180] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7461
[2019-04-04 03:49:38,197] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 67.33333333333333, 0.0, 0.0, 26.0, 25.41443848365369, 0.4020189582364753, 0.0, 1.0, 45464.70628726168], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3710400.0000, 
sim time next is 3711000.0000, 
raw observation next is [-2.5, 66.16666666666667, 0.0, 0.0, 26.0, 25.41028353834636, 0.3969843997724484, 0.0, 1.0, 44218.4853087352], 
processed observation next is [0.0, 0.9565217391304348, 0.39335180055401664, 0.6616666666666667, 0.0, 0.0, 0.6666666666666666, 0.61752362819553, 0.6323281332574828, 0.0, 1.0, 0.2105642157558819], 
reward next is 0.7894, 
noisyNet noise sample is [array([0.61173815], dtype=float32), 0.8218016]. 
=============================================
[2019-04-04 03:49:38,230] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[81.01188 ]
 [81.38953 ]
 [81.80697 ]
 [82.13706 ]
 [82.366974]], R is [[80.62275696]
 [80.60002899]
 [80.61647034]
 [80.64955139]
 [80.65335846]].
[2019-04-04 03:49:49,314] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.9837990e-29 1.3195736e-24 1.0626871e-26 4.7289445e-25 4.0276972e-25
 3.1996858e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:49:49,315] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5348
[2019-04-04 03:49:49,347] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 69.5, 0.0, 0.0, 26.0, 25.2684955938171, 0.3958823247713399, 0.0, 1.0, 48435.90254868819], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3472200.0000, 
sim time next is 3472800.0000, 
raw observation next is [0.3333333333333334, 70.33333333333334, 0.0, 0.0, 26.0, 25.25780518216423, 0.39407206959019, 0.0, 1.0, 43696.17377954846], 
processed observation next is [1.0, 0.17391304347826086, 0.4718374884579871, 0.7033333333333335, 0.0, 0.0, 0.6666666666666666, 0.6048170985136858, 0.6313573565300633, 0.0, 1.0, 0.2080770179978498], 
reward next is 0.7919, 
noisyNet noise sample is [array([1.6210186], dtype=float32), -0.1638759]. 
=============================================
[2019-04-04 03:49:51,169] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.4585161e-28 3.2386956e-23 3.8805399e-26 2.2424109e-24 1.3939229e-24
 8.5692731e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:49:51,175] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.0690
[2019-04-04 03:49:51,273] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.3333333333333333, 71.66666666666667, 0.0, 0.0, 26.0, 25.15233571014652, 0.3851577295998216, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3482400.0000, 
sim time next is 3483000.0000, 
raw observation next is [-0.5, 71.5, 3.0, 107.0, 26.0, 25.37705207028172, 0.4032276070388975, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.44875346260387816, 0.715, 0.01, 0.11823204419889503, 0.6666666666666666, 0.6147543391901434, 0.6344092023462992, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.42598182], dtype=float32), 0.97259814]. 
=============================================
[2019-04-04 03:49:51,288] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[84.3987 ]
 [84.51927]
 [84.51523]
 [84.47491]
 [84.42724]], R is [[86.36128235]
 [86.49767303]
 [86.36972046]
 [86.25967407]
 [86.17881012]].
[2019-04-04 03:49:55,181] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.4666694e-30 5.6777050e-27 2.4044662e-28 8.2599764e-26 5.2980835e-26
 4.9031698e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 03:49:55,183] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7252
[2019-04-04 03:49:55,254] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 49.0, 92.66666666666666, 718.3333333333333, 26.0, 26.81053803790298, 0.6269992932645246, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3510600.0000, 
sim time next is 3511200.0000, 
raw observation next is [3.0, 49.0, 90.33333333333334, 702.6666666666666, 26.0, 26.2472917296294, 0.6691519747389106, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5457063711911359, 0.49, 0.30111111111111116, 0.776427255985267, 0.6666666666666666, 0.6872743108024499, 0.7230506582463035, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4533836], dtype=float32), -0.60597605]. 
=============================================
[2019-04-04 03:49:58,306] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.8395561e-28 1.9227251e-23 6.9018895e-26 3.1517204e-24 3.3014251e-24
 5.3175796e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:49:58,307] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8815
[2019-04-04 03:49:58,423] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 77.0, 0.0, 0.0, 26.0, 24.65749344941921, 0.2806811660272314, 1.0, 1.0, 196447.4790284253], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3828000.0000, 
sim time next is 3828600.0000, 
raw observation next is [-5.0, 77.0, 5.0, 149.0, 26.0, 24.88219526250876, 0.3390872298389849, 1.0, 1.0, 18778.05797971062], 
processed observation next is [1.0, 0.30434782608695654, 0.32409972299168976, 0.77, 0.016666666666666666, 0.16464088397790055, 0.6666666666666666, 0.57351627187573, 0.613029076612995, 1.0, 1.0, 0.0894193237129077], 
reward next is 0.9106, 
noisyNet noise sample is [array([-0.9420234], dtype=float32), 0.9444185]. 
=============================================
[2019-04-04 03:50:03,640] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.3239302e-28 1.9695905e-23 2.6922199e-26 2.6071829e-24 6.1392828e-24
 1.8309887e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:50:03,640] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9159
[2019-04-04 03:50:03,667] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.4148687440496, 0.4352082460343886, 0.0, 1.0, 56389.93696283193], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4147200.0000, 
sim time next is 4147800.0000, 
raw observation next is [-1.0, 41.5, 0.0, 0.0, 26.0, 25.38724420683905, 0.4341621081977253, 0.0, 1.0, 60733.81480205471], 
processed observation next is [0.0, 0.0, 0.4349030470914128, 0.415, 0.0, 0.0, 0.6666666666666666, 0.6156036839032542, 0.6447207027325751, 0.0, 1.0, 0.2892086419145462], 
reward next is 0.7108, 
noisyNet noise sample is [array([-1.4730772], dtype=float32), 0.8034466]. 
=============================================
[2019-04-04 03:50:05,396] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.1961463e-29 6.9741424e-26 2.9113363e-28 8.8716727e-26 6.8495899e-26
 1.9105520e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 03:50:05,397] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8540
[2019-04-04 03:50:05,423] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 36.66666666666667, 90.83333333333334, 734.6666666666667, 26.0, 26.28330134472143, 0.7296007367118653, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3943200.0000, 
sim time next is 3943800.0000, 
raw observation next is [-4.0, 36.0, 88.0, 724.0, 26.0, 26.70950171116037, 0.758299609460844, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.3518005540166205, 0.36, 0.29333333333333333, 0.8, 0.6666666666666666, 0.7257918092633643, 0.752766536486948, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.17621468], dtype=float32), 1.3541003]. 
=============================================
[2019-04-04 03:50:08,721] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.9674114e-28 7.3625499e-23 1.3763738e-26 2.2770636e-24 1.7234530e-24
 2.4466313e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:50:08,723] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5719
[2019-04-04 03:50:08,737] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.833333333333333, 45.5, 0.0, 0.0, 26.0, 25.42011266536825, 0.34599613668297, 0.0, 1.0, 29994.69383306903], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4240200.0000, 
sim time next is 4240800.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 26.0, 25.42496675981354, 0.3440142848873828, 0.0, 1.0, 31800.97381037433], 
processed observation next is [0.0, 0.08695652173913043, 0.5457063711911359, 0.45, 0.0, 0.0, 0.6666666666666666, 0.6187472299844616, 0.6146714282957942, 0.0, 1.0, 0.15143320862083015], 
reward next is 0.8486, 
noisyNet noise sample is [array([1.7992022], dtype=float32), 0.4898797]. 
=============================================
[2019-04-04 03:50:09,071] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.9546733e-29 8.5805670e-26 1.4826554e-27 4.5340969e-25 3.6103494e-25
 9.8567312e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 03:50:09,072] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3849
[2019-04-04 03:50:09,088] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.166666666666667, 34.66666666666667, 73.33333333333334, 598.6666666666666, 26.0, 27.11364420785407, 0.8041768978943796, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3946200.0000, 
sim time next is 3946800.0000, 
raw observation next is [-4.333333333333334, 35.33333333333334, 69.66666666666666, 567.3333333333334, 26.0, 27.21722892827036, 0.4912751474781973, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.3425669436749769, 0.35333333333333344, 0.2322222222222222, 0.6268876611418048, 0.6666666666666666, 0.7681024106891966, 0.6637583824927324, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.17722104], dtype=float32), -1.5208204]. 
=============================================
[2019-04-04 03:50:10,065] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.8622264e-30 3.1360306e-26 1.0240591e-28 2.6020534e-26 2.9188676e-26
 1.8115399e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 03:50:10,065] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7183
[2019-04-04 03:50:10,092] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 48.0, 102.8333333333333, 771.1666666666667, 26.0, 26.88494127168119, 0.7497018347685565, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3854400.0000, 
sim time next is 3855000.0000, 
raw observation next is [2.0, 48.0, 99.66666666666666, 760.3333333333333, 26.0, 26.95834924470662, 0.6416084354882713, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.518005540166205, 0.48, 0.3322222222222222, 0.840147329650092, 0.6666666666666666, 0.7465291037255518, 0.7138694784960905, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.20969929], dtype=float32), -0.4851747]. 
=============================================
[2019-04-04 03:50:10,119] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[87.59149 ]
 [87.7566  ]
 [87.79362 ]
 [87.757484]
 [87.68835 ]], R is [[87.36541748]
 [87.49176788]
 [87.61685181]
 [87.74068451]
 [87.86328125]].
[2019-04-04 03:50:10,574] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.8514472e-30 4.0502362e-26 1.3220196e-28 1.9065662e-26 4.2297569e-26
 2.5235503e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 03:50:10,575] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8879
[2019-04-04 03:50:10,622] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.666666666666667, 63.66666666666667, 102.5, 695.8333333333334, 26.0, 26.32213483966953, 0.5710407369418676, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3836400.0000, 
sim time next is 3837000.0000, 
raw observation next is [-2.333333333333333, 61.83333333333333, 104.0, 711.6666666666666, 26.0, 26.39740304411487, 0.5804476922698273, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.3979686057248385, 0.6183333333333333, 0.3466666666666667, 0.7863720073664825, 0.6666666666666666, 0.6997835870095725, 0.6934825640899424, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.35928488], dtype=float32), 0.615289]. 
=============================================
[2019-04-04 03:50:10,624] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[88.95727 ]
 [88.90968 ]
 [88.678955]
 [88.5739  ]
 [88.59986 ]], R is [[89.12997437]
 [89.23867798]
 [89.34629059]
 [89.45282745]
 [89.55830383]].
[2019-04-04 03:50:12,930] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.4998544e-27 3.6984122e-22 1.0904984e-24 4.0657222e-23 6.4061444e-23
 7.2178486e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:50:12,933] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2492
[2019-04-04 03:50:12,975] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.5, 61.5, 0.0, 0.0, 26.0, 24.91647843306871, 0.2834527412304775, 0.0, 1.0, 42172.9947809158], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3911400.0000, 
sim time next is 3912000.0000, 
raw observation next is [-6.666666666666666, 62.33333333333333, 0.0, 0.0, 26.0, 24.88489138401096, 0.2680382634821208, 0.0, 1.0, 42333.27091130289], 
processed observation next is [1.0, 0.2608695652173913, 0.2779316712834719, 0.6233333333333333, 0.0, 0.0, 0.6666666666666666, 0.57374094866758, 0.5893460878273736, 0.0, 1.0, 0.20158700433953758], 
reward next is 0.7984, 
noisyNet noise sample is [array([-0.05614492], dtype=float32), -2.4537394]. 
=============================================
[2019-04-04 03:50:12,979] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[78.14856]
 [78.20407]
 [78.3269 ]
 [78.63147]
 [79.02817]], R is [[78.12094116]
 [78.13890839]
 [78.15740204]
 [78.17630005]
 [78.19570923]].
[2019-04-04 03:50:18,542] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.7849067e-28 2.3618278e-23 6.2034277e-26 2.3232857e-24 2.4909630e-24
 3.4549523e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:50:18,542] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8476
[2019-04-04 03:50:18,555] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 38.5, 0.0, 0.0, 26.0, 25.12860856317331, 0.2982881565930677, 0.0, 1.0, 40661.04650984435], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4072200.0000, 
sim time next is 4072800.0000, 
raw observation next is [-5.0, 39.0, 0.0, 0.0, 26.0, 25.11652898864116, 0.2867724634682304, 0.0, 1.0, 40613.37803579748], 
processed observation next is [1.0, 0.13043478260869565, 0.32409972299168976, 0.39, 0.0, 0.0, 0.6666666666666666, 0.5930440823867634, 0.5955908211560769, 0.0, 1.0, 0.1933970382657023], 
reward next is 0.8066, 
noisyNet noise sample is [array([0.7055153], dtype=float32), -0.11766523]. 
=============================================
[2019-04-04 03:50:21,349] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.3552509e-28 1.5959543e-23 1.5149761e-26 1.9933919e-24 7.7319654e-25
 7.0055721e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:50:21,352] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8615
[2019-04-04 03:50:21,437] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-13.5, 66.0, 0.0, 0.0, 26.0, 23.56233043235398, 0.07339330325314407, 1.0, 1.0, 202955.1771653006], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4001400.0000, 
sim time next is 4002000.0000, 
raw observation next is [-13.33333333333333, 65.0, 15.5, 73.99999999999999, 26.0, 24.12768308345118, 0.1932730319634381, 1.0, 1.0, 152490.8549105871], 
processed observation next is [1.0, 0.30434782608695654, 0.09325946445060027, 0.65, 0.051666666666666666, 0.08176795580110496, 0.6666666666666666, 0.5106402569542651, 0.5644243439878127, 1.0, 1.0, 0.7261469281456528], 
reward next is 0.2739, 
noisyNet noise sample is [array([-0.320352], dtype=float32), 0.23540142]. 
=============================================
[2019-04-04 03:50:21,448] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[86.56551]
 [84.6519 ]
 [82.66603]
 [82.61543]
 [82.64573]], R is [[88.19769287]
 [87.34926605]
 [86.51199341]
 [86.44268799]
 [86.37360382]].
[2019-04-04 03:50:22,424] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.3507606e-28 9.2231497e-25 1.0931837e-26 2.0955696e-24 8.0732873e-25
 5.9303271e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 03:50:22,425] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7641
[2019-04-04 03:50:22,448] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 31.0, 0.0, 0.0, 26.0, 25.46098558328369, 0.4853110379540038, 0.0, 1.0, 64010.81647600716], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4053600.0000, 
sim time next is 4054200.0000, 
raw observation next is [-5.166666666666667, 32.0, 0.0, 0.0, 26.0, 25.45984417683852, 0.4394693407686881, 0.0, 1.0, 48412.21203549852], 
processed observation next is [1.0, 0.9565217391304348, 0.31948291782086796, 0.32, 0.0, 0.0, 0.6666666666666666, 0.62165368140321, 0.6464897802562294, 0.0, 1.0, 0.23053434302618345], 
reward next is 0.7695, 
noisyNet noise sample is [array([2.22009], dtype=float32), 0.55503815]. 
=============================================
[2019-04-04 03:50:32,784] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.2231208e-28 3.5637596e-24 1.0089492e-26 7.3328595e-25 8.5605953e-25
 8.0396222e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:50:32,784] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4183
[2019-04-04 03:50:32,805] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.5, 76.0, 0.0, 0.0, 26.0, 25.45326070069806, 0.4091276563107401, 0.0, 1.0, 59603.33556110971], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4320000.0000, 
sim time next is 4320600.0000, 
raw observation next is [4.45, 75.83333333333334, 0.0, 0.0, 26.0, 25.4983260206708, 0.4146625724478257, 0.0, 1.0, 21810.99189676814], 
processed observation next is [1.0, 0.0, 0.5858725761772854, 0.7583333333333334, 0.0, 0.0, 0.6666666666666666, 0.6248605017225666, 0.6382208574826086, 0.0, 1.0, 0.10386186617508639], 
reward next is 0.8961, 
noisyNet noise sample is [array([-0.6044845], dtype=float32), 0.14230303]. 
=============================================
[2019-04-04 03:50:34,714] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.8105469e-28 5.6298745e-25 7.2049432e-27 6.5719041e-25 5.3378066e-25
 1.2632816e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:50:34,714] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.2246
[2019-04-04 03:50:34,735] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 43.0, 0.0, 0.0, 26.0, 25.3330776894664, 0.4070362479393317, 0.0, 1.0, 46650.28535476003], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4219200.0000, 
sim time next is 4219800.0000, 
raw observation next is [1.0, 43.0, 0.0, 0.0, 26.0, 25.47917825096469, 0.4157366616927704, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.4903047091412743, 0.43, 0.0, 0.0, 0.6666666666666666, 0.6232648542470575, 0.6385788872309235, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.91101784], dtype=float32), 0.8709905]. 
=============================================
[2019-04-04 03:50:35,311] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.1441950e-29 1.4424392e-25 1.0277971e-27 1.9951330e-25 1.1086641e-25
 1.3837473e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 03:50:35,313] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5883
[2019-04-04 03:50:35,345] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 72.0, 76.33333333333334, 16.66666666666666, 26.0, 25.00057114212286, 0.443161358988682, 1.0, 1.0, 170719.191767522], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4726200.0000, 
sim time next is 4726800.0000, 
raw observation next is [1.0, 72.0, 64.5, 19.5, 26.0, 25.42567177108764, 0.4967021836291559, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.4903047091412743, 0.72, 0.215, 0.02154696132596685, 0.6666666666666666, 0.61880598092397, 0.6655673945430519, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.059762], dtype=float32), 0.4282249]. 
=============================================
[2019-04-04 03:50:42,713] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.4036313e-30 1.9072582e-25 1.1311333e-27 2.3110498e-26 1.1366667e-25
 7.7743871e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 03:50:42,715] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3175
[2019-04-04 03:50:42,758] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 71.0, 129.8333333333333, 227.3333333333333, 26.0, 25.91954378591563, 0.5038773388002669, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4610400.0000, 
sim time next is 4611000.0000, 
raw observation next is [-2.0, 71.0, 136.6666666666667, 283.6666666666666, 26.0, 26.07005804061876, 0.5170418252592595, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.40720221606648205, 0.71, 0.4555555555555557, 0.3134438305709023, 0.6666666666666666, 0.6725048367182301, 0.6723472750864198, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.16014336], dtype=float32), 0.16633542]. 
=============================================
[2019-04-04 03:50:42,770] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[88.66463 ]
 [88.48608 ]
 [88.15649 ]
 [87.771126]
 [87.26434 ]], R is [[88.97288513]
 [89.0831604 ]
 [89.19232941]
 [89.30040741]
 [89.40740204]].
[2019-04-04 03:50:47,286] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.3854260e-30 1.3201116e-25 3.5148243e-28 8.2998367e-26 4.0779095e-26
 6.3193749e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 03:50:47,288] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9555
[2019-04-04 03:50:47,332] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.07149146610091, 0.4684769014240351, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4477200.0000, 
sim time next is 4477800.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.06175902812878, 0.4644170265468352, 0.0, 1.0, 18706.2261540012], 
processed observation next is [1.0, 0.8260869565217391, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5884799190107316, 0.6548056755156118, 0.0, 1.0, 0.08907726740000571], 
reward next is 0.9109, 
noisyNet noise sample is [array([-0.20188957], dtype=float32), -0.49048716]. 
=============================================
[2019-04-04 03:50:54,179] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.7220492e-29 2.2713973e-25 7.0905892e-27 6.8784741e-25 5.0094536e-25
 1.1977397e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:50:54,180] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5488
[2019-04-04 03:50:54,210] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.333333333333333, 84.66666666666667, 0.0, 0.0, 26.0, 25.50837384306576, 0.5235875018962333, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4742400.0000, 
sim time next is 4743000.0000, 
raw observation next is [-2.5, 84.5, 0.0, 0.0, 26.0, 25.49880180946165, 0.514621369112285, 0.0, 1.0, 24657.4114034725], 
processed observation next is [1.0, 0.9130434782608695, 0.39335180055401664, 0.845, 0.0, 0.0, 0.6666666666666666, 0.6249001507884708, 0.6715404563707615, 0.0, 1.0, 0.11741624477844047], 
reward next is 0.8826, 
noisyNet noise sample is [array([0.74361664], dtype=float32), -2.233977]. 
=============================================
[2019-04-04 03:50:54,219] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[82.11645 ]
 [82.28208 ]
 [82.446304]
 [82.40678 ]
 [82.43386 ]], R is [[82.06519318]
 [82.24454498]
 [82.42210388]
 [82.50853729]
 [82.43318176]].
[2019-04-04 03:50:57,554] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:50:57,555] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:50:57,558] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run31
[2019-04-04 03:50:57,828] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:50:57,828] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:50:57,832] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run31
[2019-04-04 03:50:57,944] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.5522057e-29 2.2819679e-24 2.8696368e-27 4.0201912e-25 8.7005102e-26
 6.0591181e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:50:57,944] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6501
[2019-04-04 03:50:58,020] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.5, 84.5, 124.0, 419.0, 26.0, 24.31781378082753, 0.3489413017274095, 0.0, 1.0, 202145.2252079068], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4782600.0000, 
sim time next is 4783200.0000, 
raw observation next is [-5.333333333333333, 82.0, 132.3333333333333, 419.3333333333334, 26.0, 25.08999939929086, 0.4268419235486307, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.3148661126500462, 0.82, 0.44111111111111095, 0.46335174953959496, 0.6666666666666666, 0.5908332832742383, 0.6422806411828769, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.73490393], dtype=float32), 1.6039593]. 
=============================================
[2019-04-04 03:50:58,293] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:50:58,293] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:50:58,301] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run31
[2019-04-04 03:50:58,978] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.6901173e-29 4.0650205e-25 6.3792820e-27 4.7576601e-25 7.8163957e-25
 6.2766857e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 03:50:58,978] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4820
[2019-04-04 03:50:58,995] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.166666666666667, 52.33333333333334, 0.0, 0.0, 26.0, 26.34600706413043, 0.6796335708924649, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4647000.0000, 
sim time next is 4647600.0000, 
raw observation next is [3.0, 53.0, 0.0, 0.0, 26.0, 26.25513555105322, 0.65639811591833, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.5457063711911359, 0.53, 0.0, 0.0, 0.6666666666666666, 0.6879279625877682, 0.7187993719727767, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.56218565], dtype=float32), -0.6891527]. 
=============================================
[2019-04-04 03:51:00,351] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.5232042e-29 3.8572382e-25 3.0102116e-27 2.5977404e-25 3.7546850e-25
 5.5745133e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 03:51:00,353] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3643
[2019-04-04 03:51:00,403] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 78.0, 0.0, 0.0, 26.0, 25.14371949618502, 0.423772834711716, 0.0, 1.0, 26466.84550367529], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4737000.0000, 
sim time next is 4737600.0000, 
raw observation next is [-1.0, 78.0, 0.0, 0.0, 26.0, 25.04108275064648, 0.4173758454975736, 0.0, 1.0, 82735.61278444852], 
processed observation next is [1.0, 0.8695652173913043, 0.4349030470914128, 0.78, 0.0, 0.0, 0.6666666666666666, 0.5867568958872068, 0.6391252818325245, 0.0, 1.0, 0.3939791084973739], 
reward next is 0.6060, 
noisyNet noise sample is [array([-1.3018833], dtype=float32), 1.6530483]. 
=============================================
[2019-04-04 03:51:01,229] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.7645622e-28 7.2403310e-24 2.9380483e-26 9.9397874e-25 1.4434197e-24
 2.1401529e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:51:01,230] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2796
[2019-04-04 03:51:01,243] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.59355077598256, 0.4596468463091535, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4686600.0000, 
sim time next is 4687200.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.5938897577379, 0.4506667087063628, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.4349030470914128, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6328241464781584, 0.6502222362354543, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.51841944], dtype=float32), -0.11864102]. 
=============================================
[2019-04-04 03:51:01,503] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.6146465e-27 1.9743834e-22 8.1275754e-26 4.6001808e-24 9.6801366e-24
 9.1958779e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:51:01,510] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8940
[2019-04-04 03:51:01,533] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.333333333333333, 63.66666666666667, 0.0, 0.0, 26.0, 24.70952359415261, 0.2034980601257555, 0.0, 1.0, 39562.05470774141], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4858800.0000, 
sim time next is 4859400.0000, 
raw observation next is [-3.166666666666667, 61.83333333333333, 0.0, 0.0, 26.0, 24.67987780416417, 0.1980413185090211, 0.0, 1.0, 39561.34959837276], 
processed observation next is [0.0, 0.21739130434782608, 0.3748845798707295, 0.6183333333333333, 0.0, 0.0, 0.6666666666666666, 0.5566564836803476, 0.5660137728363404, 0.0, 1.0, 0.1883873790398703], 
reward next is 0.8116, 
noisyNet noise sample is [array([1.6644621], dtype=float32), -1.36157]. 
=============================================
[2019-04-04 03:51:07,596] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.1608842e-26 7.3309368e-23 1.3033390e-25 1.5409036e-23 3.5875359e-23
 1.3035919e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:51:07,619] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8308
[2019-04-04 03:51:07,626] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 37.0, 105.5, 729.5, 26.0, 25.18554610781176, 0.4440344088789673, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4807200.0000, 
sim time next is 4807800.0000, 
raw observation next is [3.0, 37.0, 97.0, 727.0, 26.0, 25.18501721192654, 0.4432620315263363, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.5457063711911359, 0.37, 0.3233333333333333, 0.8033149171270718, 0.6666666666666666, 0.5987514343272116, 0.6477540105087788, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4257144], dtype=float32), -0.6006287]. 
=============================================
[2019-04-04 03:51:08,888] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.31495371e-27 1.62714600e-23 7.54435632e-26 1.05027994e-23
 1.06762557e-23 8.41514520e-27 1.00000000e+00], sum to 1.0000
[2019-04-04 03:51:08,890] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4303
[2019-04-04 03:51:08,945] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.233333333333333, 83.33333333333334, 23.33333333333333, 0.0, 26.0, 24.5220054204006, 0.1773502594089305, 0.0, 1.0, 50689.92145719513], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 58800.0000, 
sim time next is 59400.0000, 
raw observation next is [6.05, 84.0, 18.0, 0.0, 26.0, 24.50422910401001, 0.1860511567913048, 0.0, 1.0, 58935.20283097521], 
processed observation next is [0.0, 0.6956521739130435, 0.6301939058171746, 0.84, 0.06, 0.0, 0.6666666666666666, 0.5420190920008343, 0.5620170522637683, 0.0, 1.0, 0.28064382300464386], 
reward next is 0.7194, 
noisyNet noise sample is [array([-2.1152215], dtype=float32), 0.44903648]. 
=============================================
[2019-04-04 03:51:10,345] A3C_AGENT_WORKER-Thread-7 INFO:Local step 255000, global step 4074996: loss 0.0766
[2019-04-04 03:51:10,346] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 255000, global step 4074996: learning rate 0.0001
[2019-04-04 03:51:10,428] A3C_AGENT_WORKER-Thread-17 INFO:Local step 255000, global step 4075027: loss 0.0766
[2019-04-04 03:51:10,429] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 255000, global step 4075028: learning rate 0.0001
[2019-04-04 03:51:11,185] A3C_AGENT_WORKER-Thread-19 INFO:Local step 255000, global step 4075409: loss 0.1133
[2019-04-04 03:51:11,185] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 255000, global step 4075409: learning rate 0.0001
[2019-04-04 03:51:12,126] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:51:12,128] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:51:12,137] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run31
[2019-04-04 03:51:13,497] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:51:13,497] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:51:13,501] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run31
[2019-04-04 03:51:15,194] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.8561932e-30 8.2558138e-27 1.6497044e-28 8.9868020e-26 1.1900992e-25
 1.6776902e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 03:51:15,195] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0743
[2019-04-04 03:51:15,211] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.0, 19.0, 103.5, 782.0, 26.0, 28.55247690706437, 1.108481634662004, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5065200.0000, 
sim time next is 5065800.0000, 
raw observation next is [12.0, 19.0, 101.0, 769.6666666666667, 26.0, 28.64044736531397, 1.126138800875722, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.7950138504155125, 0.19, 0.33666666666666667, 0.8504604051565379, 0.6666666666666666, 0.8867039471094976, 0.8753796002919074, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.13970421], dtype=float32), -1.5334405]. 
=============================================
[2019-04-04 03:51:16,905] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:51:16,905] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:51:16,910] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run31
[2019-04-04 03:51:17,349] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:51:17,349] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:51:17,352] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run31
[2019-04-04 03:51:17,517] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:51:17,517] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:51:17,522] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run31
[2019-04-04 03:51:17,685] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:51:17,685] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:51:17,690] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run31
[2019-04-04 03:51:17,878] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:51:17,878] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:51:17,881] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run31
[2019-04-04 03:51:23,757] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:51:23,757] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:51:23,762] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run31
[2019-04-04 03:51:24,660] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:51:24,661] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:51:24,666] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run31
[2019-04-04 03:51:24,901] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:51:24,901] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:51:24,905] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run31
[2019-04-04 03:51:25,572] A3C_AGENT_WORKER-Thread-5 INFO:Local step 255000, global step 4079500: loss 0.1445
[2019-04-04 03:51:25,573] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 255000, global step 4079500: learning rate 0.0001
[2019-04-04 03:51:27,268] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:51:27,269] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:51:27,270] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:51:27,270] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:51:27,277] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run31
[2019-04-04 03:51:27,274] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run31
[2019-04-04 03:51:27,429] A3C_AGENT_WORKER-Thread-14 INFO:Local step 255000, global step 4079895: loss 0.1205
[2019-04-04 03:51:27,431] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 255000, global step 4079895: learning rate 0.0001
[2019-04-04 03:51:27,488] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.8064774e-27 2.5212405e-23 7.5069212e-26 1.4660335e-23 1.2671694e-23
 7.2310457e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:51:27,489] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2691
[2019-04-04 03:51:27,535] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.7, 89.0, 0.0, 0.0, 26.0, 24.76432427285463, 0.2419582743495171, 0.0, 1.0, 42309.77853506793], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 72000.0000, 
sim time next is 72600.0000, 
raw observation next is [2.516666666666667, 88.33333333333334, 0.0, 0.0, 26.0, 24.76193669398639, 0.2398501884180917, 0.0, 1.0, 41556.10709449992], 
processed observation next is [0.0, 0.8695652173913043, 0.5323176361957526, 0.8833333333333334, 0.0, 0.0, 0.6666666666666666, 0.5634947244988657, 0.5799500628060306, 0.0, 1.0, 0.19788622425952343], 
reward next is 0.8021, 
noisyNet noise sample is [array([0.8011047], dtype=float32), 0.9065485]. 
=============================================
[2019-04-04 03:51:27,730] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.6817520e-28 2.5401510e-24 5.0833137e-26 2.4354515e-24 3.2197727e-24
 6.2094350e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:51:27,730] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7012
[2019-04-04 03:51:27,788] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 26.0, 24.78816660034418, 0.224419554168731, 0.0, 1.0, 44883.81135981878], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 248400.0000, 
sim time next is 249000.0000, 
raw observation next is [-3.483333333333333, 66.66666666666667, 0.0, 0.0, 26.0, 24.73134698482353, 0.2137034001661745, 0.0, 1.0, 44754.75766822971], 
processed observation next is [1.0, 0.9130434782608695, 0.3661126500461681, 0.6666666666666667, 0.0, 0.0, 0.6666666666666666, 0.5609455820686277, 0.5712344667220582, 0.0, 1.0, 0.2131178936582367], 
reward next is 0.7869, 
noisyNet noise sample is [array([-1.0611415], dtype=float32), -0.76015437]. 
=============================================
[2019-04-04 03:51:27,796] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[79.417534]
 [79.24737 ]
 [79.23589 ]
 [79.38192 ]
 [79.25958 ]], R is [[79.5530014 ]
 [79.54373932]
 [79.53243256]
 [79.51449585]
 [79.47948456]].
[2019-04-04 03:51:28,849] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 03:51:28,849] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:51:28,863] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run31
[2019-04-04 03:51:30,832] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.5045457e-26 8.0739421e-23 6.2378387e-25 4.6392348e-23 3.8657564e-23
 3.9072324e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:51:30,832] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1609
[2019-04-04 03:51:30,912] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.416666666666667, 82.66666666666667, 28.66666666666666, 0.0, 26.0, 24.54627051744927, 0.1787405372246849, 0.0, 1.0, 28616.51865161897], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 58200.0000, 
sim time next is 58800.0000, 
raw observation next is [6.233333333333333, 83.33333333333334, 23.33333333333333, 0.0, 26.0, 24.5220054204006, 0.1773502594089305, 0.0, 1.0, 50689.92145719513], 
processed observation next is [0.0, 0.6956521739130435, 0.6352723915050786, 0.8333333333333335, 0.07777777777777777, 0.0, 0.6666666666666666, 0.54350045170005, 0.5591167531363102, 0.0, 1.0, 0.24138057836759585], 
reward next is 0.7586, 
noisyNet noise sample is [array([-1.9602238], dtype=float32), 0.24728154]. 
=============================================
[2019-04-04 03:51:31,820] A3C_AGENT_WORKER-Thread-3 INFO:Local step 255000, global step 4080568: loss 0.1090
[2019-04-04 03:51:31,822] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 255000, global step 4080568: learning rate 0.0001
[2019-04-04 03:51:32,199] A3C_AGENT_WORKER-Thread-15 INFO:Local step 255000, global step 4080641: loss 0.0968
[2019-04-04 03:51:32,199] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 255000, global step 4080641: learning rate 0.0001
[2019-04-04 03:51:32,338] A3C_AGENT_WORKER-Thread-9 INFO:Local step 255000, global step 4080675: loss 0.1020
[2019-04-04 03:51:32,339] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 255000, global step 4080675: learning rate 0.0001
[2019-04-04 03:51:32,538] A3C_AGENT_WORKER-Thread-6 INFO:Local step 255000, global step 4080724: loss 0.0974
[2019-04-04 03:51:32,538] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 255000, global step 4080724: learning rate 0.0001
[2019-04-04 03:51:32,673] A3C_AGENT_WORKER-Thread-2 INFO:Local step 255000, global step 4080762: loss 0.0975
[2019-04-04 03:51:32,702] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 255000, global step 4080762: learning rate 0.0001
[2019-04-04 03:51:37,495] A3C_AGENT_WORKER-Thread-4 INFO:Local step 255000, global step 4081918: loss 0.0649
[2019-04-04 03:51:37,496] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 255000, global step 4081918: learning rate 0.0001
[2019-04-04 03:51:38,888] A3C_AGENT_WORKER-Thread-20 INFO:Local step 255000, global step 4082322: loss 0.0680
[2019-04-04 03:51:38,888] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 255000, global step 4082322: learning rate 0.0001
[2019-04-04 03:51:38,953] A3C_AGENT_WORKER-Thread-10 INFO:Local step 255000, global step 4082340: loss 0.0675
[2019-04-04 03:51:38,958] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 255000, global step 4082341: learning rate 0.0001
[2019-04-04 03:51:39,786] A3C_AGENT_WORKER-Thread-19 INFO:Local step 255500, global step 4082595: loss 1.2907
[2019-04-04 03:51:39,786] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 255500, global step 4082595: learning rate 0.0001
[2019-04-04 03:51:39,855] A3C_AGENT_WORKER-Thread-7 INFO:Local step 255500, global step 4082607: loss 1.2981
[2019-04-04 03:51:39,855] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 255500, global step 4082607: learning rate 0.0001
[2019-04-04 03:51:39,977] A3C_AGENT_WORKER-Thread-17 INFO:Local step 255500, global step 4082628: loss 1.3186
[2019-04-04 03:51:39,978] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 255500, global step 4082628: learning rate 0.0001
[2019-04-04 03:51:41,677] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.4078098e-26 6.1959635e-23 9.5945464e-25 2.5521476e-23 5.9644221e-23
 9.6021573e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:51:41,677] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2927
[2019-04-04 03:51:41,743] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.683333333333334, 85.33333333333334, 12.0, 0.0, 26.0, 24.54824014586847, 0.1890691172121965, 0.0, 1.0, 34829.88553126116], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 60600.0000, 
sim time next is 61200.0000, 
raw observation next is [5.5, 86.0, 0.0, 0.0, 26.0, 24.54659692194853, 0.1947913318725229, 0.0, 1.0, 42206.84572453996], 
processed observation next is [0.0, 0.7391304347826086, 0.6149584487534627, 0.86, 0.0, 0.0, 0.6666666666666666, 0.545549743495711, 0.5649304439575077, 0.0, 1.0, 0.2009849796406665], 
reward next is 0.7990, 
noisyNet noise sample is [array([2.5216837], dtype=float32), 2.9006348]. 
=============================================
[2019-04-04 03:51:41,916] A3C_AGENT_WORKER-Thread-8 INFO:Local step 255000, global step 4083078: loss 0.0562
[2019-04-04 03:51:41,917] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 255000, global step 4083078: learning rate 0.0001
[2019-04-04 03:51:42,017] A3C_AGENT_WORKER-Thread-18 INFO:Local step 255000, global step 4083108: loss 0.0597
[2019-04-04 03:51:42,019] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 255000, global step 4083108: learning rate 0.0001
[2019-04-04 03:51:42,119] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.5596529e-26 4.2866153e-23 8.6411094e-25 3.0306922e-23 1.8053522e-22
 3.1328378e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:51:42,119] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1620
[2019-04-04 03:51:42,190] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.4, 68.5, 0.0, 0.0, 26.0, 24.60760041399288, 0.2050969217860293, 0.0, 1.0, 45836.30398297768], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 162600.0000, 
sim time next is 163200.0000, 
raw observation next is [-8.4, 69.0, 0.0, 0.0, 26.0, 24.53584637074417, 0.1906810797279336, 0.0, 1.0, 45854.47657743898], 
processed observation next is [1.0, 0.9130434782608695, 0.2299168975069252, 0.69, 0.0, 0.0, 0.6666666666666666, 0.5446538642286809, 0.5635603599093112, 0.0, 1.0, 0.21835465036875706], 
reward next is 0.7816, 
noisyNet noise sample is [array([-1.0848346], dtype=float32), -0.47618014]. 
=============================================
[2019-04-04 03:51:43,056] A3C_AGENT_WORKER-Thread-16 INFO:Local step 255000, global step 4083434: loss 0.0765
[2019-04-04 03:51:43,056] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 255000, global step 4083434: learning rate 0.0001
[2019-04-04 03:51:46,114] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.0466382e-27 1.4401143e-24 3.7604309e-26 6.6404773e-24 1.6541756e-24
 5.9144933e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:51:46,118] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.8046
[2019-04-04 03:51:46,200] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.8, 86.0, 187.0, 24.5, 26.0, 25.32297745251308, 0.3035808216663944, 1.0, 1.0, 41655.99005941557], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 126000.0000, 
sim time next is 126600.0000, 
raw observation next is [-7.9, 81.83333333333334, 186.0, 20.66666666666666, 26.0, 25.34419145045703, 0.3109378772440958, 1.0, 1.0, 44145.7819909243], 
processed observation next is [1.0, 0.4782608695652174, 0.24376731301939059, 0.8183333333333335, 0.62, 0.022836095764272552, 0.6666666666666666, 0.6120159542047524, 0.6036459590813653, 1.0, 1.0, 0.21021800948059188], 
reward next is 0.7898, 
noisyNet noise sample is [array([-0.69460595], dtype=float32), 1.4574349]. 
=============================================
[2019-04-04 03:51:53,627] A3C_AGENT_WORKER-Thread-5 INFO:Local step 255500, global step 4086358: loss 1.0267
[2019-04-04 03:51:53,629] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 255500, global step 4086358: learning rate 0.0001
[2019-04-04 03:51:56,883] A3C_AGENT_WORKER-Thread-14 INFO:Local step 255500, global step 4087240: loss 1.0029
[2019-04-04 03:51:56,884] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 255500, global step 4087240: learning rate 0.0001
[2019-04-04 03:51:59,991] A3C_AGENT_WORKER-Thread-3 INFO:Local step 255500, global step 4088131: loss 0.9213
[2019-04-04 03:51:59,991] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 255500, global step 4088131: learning rate 0.0001
[2019-04-04 03:52:00,604] A3C_AGENT_WORKER-Thread-9 INFO:Local step 255500, global step 4088353: loss 0.8628
[2019-04-04 03:52:00,605] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 255500, global step 4088353: learning rate 0.0001
[2019-04-04 03:52:00,652] A3C_AGENT_WORKER-Thread-15 INFO:Local step 255500, global step 4088368: loss 0.8945
[2019-04-04 03:52:00,653] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 255500, global step 4088368: learning rate 0.0001
[2019-04-04 03:52:00,996] A3C_AGENT_WORKER-Thread-6 INFO:Local step 255500, global step 4088473: loss 0.9117
[2019-04-04 03:52:00,997] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 255500, global step 4088473: learning rate 0.0001
[2019-04-04 03:52:01,196] A3C_AGENT_WORKER-Thread-2 INFO:Local step 255500, global step 4088516: loss 0.9173
[2019-04-04 03:52:01,206] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 255500, global step 4088517: learning rate 0.0001
[2019-04-04 03:52:02,387] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [9.42922204e-28 1.01453926e-23 5.59772740e-26 3.24326829e-24
 3.06671984e-24 1.06967796e-27 1.00000000e+00], sum to 1.0000
[2019-04-04 03:52:02,388] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.5166
[2019-04-04 03:52:02,459] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.8, 63.66666666666666, 92.33333333333334, 426.0, 26.0, 25.75508582514959, 0.3512868785192806, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 294600.0000, 
sim time next is 295200.0000, 
raw observation next is [-11.7, 63.0, 91.0, 447.5, 26.0, 25.79444508085086, 0.3558713007912011, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.13850415512465375, 0.63, 0.30333333333333334, 0.494475138121547, 0.6666666666666666, 0.649537090070905, 0.6186237669304003, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.71546644], dtype=float32), 0.72994566]. 
=============================================
[2019-04-04 03:52:05,100] A3C_AGENT_WORKER-Thread-4 INFO:Local step 255500, global step 4089546: loss 0.8285
[2019-04-04 03:52:05,101] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 255500, global step 4089546: learning rate 0.0001
[2019-04-04 03:52:07,084] A3C_AGENT_WORKER-Thread-20 INFO:Local step 255500, global step 4090032: loss 0.8409
[2019-04-04 03:52:07,085] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 255500, global step 4090032: learning rate 0.0001
[2019-04-04 03:52:07,559] A3C_AGENT_WORKER-Thread-10 INFO:Local step 255500, global step 4090171: loss 0.8494
[2019-04-04 03:52:07,562] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 255500, global step 4090172: learning rate 0.0001
[2019-04-04 03:52:08,091] A3C_AGENT_WORKER-Thread-19 INFO:Local step 256000, global step 4090349: loss 0.0273
[2019-04-04 03:52:08,092] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 256000, global step 4090349: learning rate 0.0001
[2019-04-04 03:52:08,669] A3C_AGENT_WORKER-Thread-7 INFO:Local step 256000, global step 4090563: loss 0.0280
[2019-04-04 03:52:08,670] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 256000, global step 4090563: learning rate 0.0001
[2019-04-04 03:52:09,367] A3C_AGENT_WORKER-Thread-8 INFO:Local step 255500, global step 4090818: loss 0.7713
[2019-04-04 03:52:09,369] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 255500, global step 4090818: learning rate 0.0001
[2019-04-04 03:52:09,731] A3C_AGENT_WORKER-Thread-17 INFO:Local step 256000, global step 4090917: loss 0.0306
[2019-04-04 03:52:09,732] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 256000, global step 4090917: learning rate 0.0001
[2019-04-04 03:52:09,934] A3C_AGENT_WORKER-Thread-18 INFO:Local step 255500, global step 4090973: loss 0.7277
[2019-04-04 03:52:09,977] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 255500, global step 4090977: learning rate 0.0001
[2019-04-04 03:52:10,357] A3C_AGENT_WORKER-Thread-16 INFO:Local step 255500, global step 4091085: loss 0.7112
[2019-04-04 03:52:10,357] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 255500, global step 4091085: learning rate 0.0001
[2019-04-04 03:52:21,663] A3C_AGENT_WORKER-Thread-5 INFO:Local step 256000, global step 4094491: loss 0.0751
[2019-04-04 03:52:21,664] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 256000, global step 4094491: learning rate 0.0001
[2019-04-04 03:52:24,079] A3C_AGENT_WORKER-Thread-14 INFO:Local step 256000, global step 4095441: loss 0.0977
[2019-04-04 03:52:24,080] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 256000, global step 4095441: learning rate 0.0001
[2019-04-04 03:52:24,228] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.9422597e-30 4.8953676e-26 2.6155465e-27 3.8442235e-25 1.1117303e-25
 5.3759066e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 03:52:24,228] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2208
[2019-04-04 03:52:24,241] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.3, 96.0, 0.0, 0.0, 26.0, 24.83544992004674, 0.2334300721471567, 0.0, 1.0, 40390.95562304365], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 514800.0000, 
sim time next is 515400.0000, 
raw observation next is [3.383333333333333, 96.16666666666666, 0.0, 0.0, 26.0, 24.83801304605708, 0.233477655566811, 0.0, 1.0, 40247.05321314817], 
processed observation next is [1.0, 1.0, 0.5563250230840259, 0.9616666666666666, 0.0, 0.0, 0.6666666666666666, 0.5698344205047565, 0.577825885188937, 0.0, 1.0, 0.1916526343483246], 
reward next is 0.8083, 
noisyNet noise sample is [array([-0.09928703], dtype=float32), -0.11696491]. 
=============================================
[2019-04-04 03:52:27,830] A3C_AGENT_WORKER-Thread-15 INFO:Local step 256000, global step 4096271: loss 0.0870
[2019-04-04 03:52:27,831] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 256000, global step 4096271: learning rate 0.0001
[2019-04-04 03:52:28,650] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.2453167e-28 1.6790791e-23 1.9648373e-26 9.5251483e-25 2.0883716e-24
 4.7324405e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:52:28,650] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7478
[2019-04-04 03:52:28,739] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.75, 59.5, 0.0, 0.0, 26.0, 24.88820359395857, 0.207177378094991, 0.0, 1.0, 44038.8786757627], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 671400.0000, 
sim time next is 672000.0000, 
raw observation next is [-1.933333333333333, 60.33333333333333, 0.0, 0.0, 26.0, 24.89505475335285, 0.2084350863645628, 0.0, 1.0, 44414.51249606263], 
processed observation next is [0.0, 0.782608695652174, 0.40904893813481075, 0.6033333333333333, 0.0, 0.0, 0.6666666666666666, 0.5745878961127374, 0.5694783621215209, 0.0, 1.0, 0.21149767855267917], 
reward next is 0.7885, 
noisyNet noise sample is [array([-1.2827168], dtype=float32), 0.99017656]. 
=============================================
[2019-04-04 03:52:28,753] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[82.29139 ]
 [82.339455]
 [82.40203 ]
 [82.4404  ]
 [82.36779 ]], R is [[82.21645355]
 [82.18457794]
 [82.14076233]
 [82.0745163 ]
 [81.99150085]].
[2019-04-04 03:52:28,754] A3C_AGENT_WORKER-Thread-3 INFO:Local step 256000, global step 4096441: loss 0.0933
[2019-04-04 03:52:28,770] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 256000, global step 4096441: learning rate 0.0001
[2019-04-04 03:52:29,540] A3C_AGENT_WORKER-Thread-9 INFO:Local step 256000, global step 4096579: loss 0.0704
[2019-04-04 03:52:29,540] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 256000, global step 4096579: learning rate 0.0001
[2019-04-04 03:52:30,453] A3C_AGENT_WORKER-Thread-6 INFO:Local step 256000, global step 4096748: loss 0.0656
[2019-04-04 03:52:30,509] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 256000, global step 4096758: learning rate 0.0001
[2019-04-04 03:52:31,244] A3C_AGENT_WORKER-Thread-2 INFO:Local step 256000, global step 4096890: loss 0.0665
[2019-04-04 03:52:31,245] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 256000, global step 4096890: learning rate 0.0001
[2019-04-04 03:52:35,876] A3C_AGENT_WORKER-Thread-19 INFO:Local step 256500, global step 4097863: loss 0.0190
[2019-04-04 03:52:35,895] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 256500, global step 4097870: learning rate 0.0001
[2019-04-04 03:52:36,105] A3C_AGENT_WORKER-Thread-7 INFO:Local step 256500, global step 4097914: loss 0.0168
[2019-04-04 03:52:36,106] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 256500, global step 4097914: learning rate 0.0001
[2019-04-04 03:52:36,157] A3C_AGENT_WORKER-Thread-4 INFO:Local step 256000, global step 4097930: loss 0.0800
[2019-04-04 03:52:36,158] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 256000, global step 4097930: learning rate 0.0001
[2019-04-04 03:52:38,715] A3C_AGENT_WORKER-Thread-17 INFO:Local step 256500, global step 4098468: loss 0.0328
[2019-04-04 03:52:38,715] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 256500, global step 4098468: learning rate 0.0001
[2019-04-04 03:52:40,166] A3C_AGENT_WORKER-Thread-20 INFO:Local step 256000, global step 4098738: loss 0.0780
[2019-04-04 03:52:40,167] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 256000, global step 4098738: learning rate 0.0001
[2019-04-04 03:52:40,766] A3C_AGENT_WORKER-Thread-10 INFO:Local step 256000, global step 4098855: loss 0.0696
[2019-04-04 03:52:40,766] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 256000, global step 4098855: learning rate 0.0001
[2019-04-04 03:52:43,526] A3C_AGENT_WORKER-Thread-18 INFO:Local step 256000, global step 4099539: loss 0.0586
[2019-04-04 03:52:43,526] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 256000, global step 4099539: learning rate 0.0001
[2019-04-04 03:52:43,722] A3C_AGENT_WORKER-Thread-8 INFO:Local step 256000, global step 4099581: loss 0.0852
[2019-04-04 03:52:43,736] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 256000, global step 4099587: learning rate 0.0001
[2019-04-04 03:52:45,011] A3C_AGENT_WORKER-Thread-16 INFO:Local step 256000, global step 4099917: loss 0.0487
[2019-04-04 03:52:45,018] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 256000, global step 4099917: learning rate 0.0001
[2019-04-04 03:52:45,389] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-04 03:52:45,406] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 03:52:45,406] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:52:45,426] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run42
[2019-04-04 03:52:45,457] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 03:52:45,457] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:52:45,458] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 03:52:45,459] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 03:52:45,461] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run42
[2019-04-04 03:52:45,523] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run42
[2019-04-04 03:54:03,484] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.3604168], dtype=float32), 0.21135487]
[2019-04-04 03:54:03,484] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [4.9, 80.5, 113.0, 174.0, 26.0, 25.70161294244483, 0.5592773662620835, 1.0, 1.0, 0.0]
[2019-04-04 03:54:03,484] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 03:54:03,485] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [4.1528813e-28 3.8637934e-24 1.4680963e-26 1.1025240e-24 1.5709647e-24
 1.3902354e-27 1.0000000e+00], sampled 0.618233724060482
[2019-04-04 03:54:03,532] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.3604168], dtype=float32), 0.21135487]
[2019-04-04 03:54:03,532] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [4.800000000000001, 76.0, 157.1666666666667, 207.3333333333333, 26.0, 25.92260521320205, 0.5885796799395677, 1.0, 1.0, 0.0]
[2019-04-04 03:54:03,532] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 03:54:03,533] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [4.6421970e-28 1.2098977e-24 6.8020918e-27 1.0436776e-24 8.3940579e-25
 5.7799088e-28 1.0000000e+00], sampled 0.40844225151089486
[2019-04-04 03:54:42,124] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.3604168], dtype=float32), 0.21135487]
[2019-04-04 03:54:42,124] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [3.4, 68.0, 39.5, 169.5, 26.0, 25.57528300531359, 0.3527927518515961, 1.0, 1.0, 18680.41167023054]
[2019-04-04 03:54:42,124] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 03:54:42,125] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.8515968e-27 3.0202461e-24 3.0768644e-26 5.7858325e-24 3.2459295e-24
 1.4057541e-27 1.0000000e+00], sampled 0.7947745319195908
[2019-04-04 03:55:38,145] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.3604168], dtype=float32), 0.21135487]
[2019-04-04 03:55:38,145] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-3.0, 71.0, 0.0, 0.0, 26.0, 25.49731616549926, 0.4009323513425718, 0.0, 1.0, 0.0]
[2019-04-04 03:55:38,145] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 03:55:38,146] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [3.5716479e-27 3.0688160e-23 1.2200284e-25 8.8901114e-24 1.4560793e-23
 1.6543442e-26 1.0000000e+00], sampled 0.5824065204129345
[2019-04-04 03:55:51,528] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 03:56:25,309] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 03:56:31,682] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 03:56:32,731] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 4100000, evaluation results [4100000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 03:56:41,507] A3C_AGENT_WORKER-Thread-5 INFO:Local step 256500, global step 4101896: loss 0.0126
[2019-04-04 03:56:41,511] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 256500, global step 4101896: learning rate 0.0001
[2019-04-04 03:56:45,885] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.4449201e-30 5.4716633e-27 5.9571728e-29 8.0888773e-27 8.8547965e-27
 1.5360856e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 03:56:45,886] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6041
[2019-04-04 03:56:45,914] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.5, 76.5, 25.0, 0.0, 26.0, 25.94885173989908, 0.6332677426625071, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1009800.0000, 
sim time next is 1010400.0000, 
raw observation next is [15.5, 77.0, 20.83333333333334, 0.0, 26.0, 26.45133463895038, 0.6776089465763605, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.8919667590027703, 0.77, 0.06944444444444446, 0.0, 0.6666666666666666, 0.7042778865791984, 0.7258696488587869, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4459586], dtype=float32), -2.4983451]. 
=============================================
[2019-04-04 03:56:45,975] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5716560e-29 2.5123943e-26 7.4873765e-29 3.1689552e-27 1.2516117e-26
 7.3534571e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 03:56:45,975] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8610
[2019-04-04 03:56:46,001] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [17.7, 67.0, 0.0, 0.0, 26.0, 24.56962419734215, 0.3710771119317997, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1198800.0000, 
sim time next is 1199400.0000, 
raw observation next is [17.51666666666667, 68.33333333333334, 0.0, 0.0, 26.0, 24.55067922493936, 0.3654513520551457, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.9130434782608695, 0.9478301015697139, 0.6833333333333335, 0.0, 0.0, 0.6666666666666666, 0.5458899354116132, 0.6218171173517152, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6521697], dtype=float32), 2.0905087]. 
=============================================
[2019-04-04 03:56:47,486] A3C_AGENT_WORKER-Thread-14 INFO:Local step 256500, global step 4103235: loss 0.0241
[2019-04-04 03:56:47,486] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 256500, global step 4103235: learning rate 0.0001
[2019-04-04 03:56:51,759] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.9322970e-31 6.2860370e-27 2.9350083e-29 7.2263814e-28 1.7162008e-27
 1.3373875e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 03:56:51,759] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7705
[2019-04-04 03:56:51,815] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.6, 100.0, 80.83333333333333, 0.0, 26.0, 24.05622076363828, 0.3842208301628278, 0.0, 1.0, 64126.6824289393], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1248000.0000, 
sim time next is 1248600.0000, 
raw observation next is [14.5, 100.0, 83.66666666666667, 0.0, 26.0, 24.45726140577369, 0.4259845501270266, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.8642659279778394, 1.0, 0.2788888888888889, 0.0, 0.6666666666666666, 0.5381051171478074, 0.6419948500423421, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.338271], dtype=float32), -0.4193787]. 
=============================================
[2019-04-04 03:56:52,994] A3C_AGENT_WORKER-Thread-15 INFO:Local step 256500, global step 4104425: loss 0.0305
[2019-04-04 03:56:52,995] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 256500, global step 4104425: learning rate 0.0001
[2019-04-04 03:56:53,586] A3C_AGENT_WORKER-Thread-3 INFO:Local step 256500, global step 4104550: loss 0.0210
[2019-04-04 03:56:53,586] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 256500, global step 4104550: learning rate 0.0001
[2019-04-04 03:56:53,733] A3C_AGENT_WORKER-Thread-9 INFO:Local step 256500, global step 4104582: loss 0.0217
[2019-04-04 03:56:53,735] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 256500, global step 4104582: learning rate 0.0001
[2019-04-04 03:56:54,085] A3C_AGENT_WORKER-Thread-19 INFO:Local step 257000, global step 4104648: loss 1.3123
[2019-04-04 03:56:54,101] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 257000, global step 4104648: learning rate 0.0001
[2019-04-04 03:56:54,961] A3C_AGENT_WORKER-Thread-7 INFO:Local step 257000, global step 4104837: loss 1.4146
[2019-04-04 03:56:55,005] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 257000, global step 4104837: learning rate 0.0001
[2019-04-04 03:56:55,532] A3C_AGENT_WORKER-Thread-17 INFO:Local step 257000, global step 4104968: loss 1.3950
[2019-04-04 03:56:55,544] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 257000, global step 4104968: learning rate 0.0001
[2019-04-04 03:56:56,899] A3C_AGENT_WORKER-Thread-6 INFO:Local step 256500, global step 4105329: loss 0.0379
[2019-04-04 03:56:56,900] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 256500, global step 4105329: learning rate 0.0001
[2019-04-04 03:56:57,424] A3C_AGENT_WORKER-Thread-2 INFO:Local step 256500, global step 4105483: loss 0.0314
[2019-04-04 03:56:57,429] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 256500, global step 4105483: learning rate 0.0001
[2019-04-04 03:56:57,892] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [7.6551159e-29 1.2662217e-24 3.9728807e-27 4.6743854e-25 4.9682706e-25
 3.1977943e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:56:57,892] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8843
[2019-04-04 03:56:57,980] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.716666666666666, 92.83333333333333, 0.0, 0.0, 26.0, 25.56207739344113, 0.5885318818971411, 0.0, 1.0, 18743.15084342029], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1300200.0000, 
sim time next is 1300800.0000, 
raw observation next is [3.633333333333334, 92.66666666666667, 0.0, 0.0, 26.0, 25.60140710580352, 0.5646036972630579, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.5632502308402586, 0.9266666666666667, 0.0, 0.0, 0.6666666666666666, 0.6334505921502934, 0.6882012324210193, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3686707], dtype=float32), 0.7643467]. 
=============================================
[2019-04-04 03:56:58,228] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.2677417e-31 2.0395099e-26 5.8333102e-29 6.9482476e-27 6.0366044e-27
 2.6912377e-31 1.0000000e+00], sum to 1.0000
[2019-04-04 03:56:58,228] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2222
[2019-04-04 03:56:58,271] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.0, 98.0, 0.0, 0.0, 26.0, 25.1194885560965, 0.3899087077552632, 0.0, 1.0, 40377.29841162066], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 941400.0000, 
sim time next is 942000.0000, 
raw observation next is [5.0, 97.33333333333334, 0.0, 0.0, 26.0, 25.14254202696572, 0.3930804697720537, 0.0, 1.0, 40157.11741611649], 
processed observation next is [1.0, 0.9130434782608695, 0.6011080332409973, 0.9733333333333334, 0.0, 0.0, 0.6666666666666666, 0.5952118355804767, 0.6310268232573512, 0.0, 1.0, 0.19122436864817377], 
reward next is 0.8088, 
noisyNet noise sample is [array([-0.6562662], dtype=float32), 0.8210611]. 
=============================================
[2019-04-04 03:56:58,293] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[86.77258]
 [86.82475]
 [86.7079 ]
 [86.66048]
 [86.63527]], R is [[86.64082336]
 [86.58213806]
 [86.52272034]
 [86.46154785]
 [86.39474487]].
[2019-04-04 03:57:01,185] A3C_AGENT_WORKER-Thread-4 INFO:Local step 256500, global step 4106394: loss 0.0694
[2019-04-04 03:57:01,187] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 256500, global step 4106394: learning rate 0.0001
[2019-04-04 03:57:01,935] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5516215e-30 4.9737865e-26 2.6391179e-28 7.1371019e-27 1.4304601e-26
 2.9573606e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 03:57:01,935] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1242
[2019-04-04 03:57:02,017] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 92.0, 45.0, 0.0, 26.0, 26.03730379980853, 0.588597221355959, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1330200.0000, 
sim time next is 1330800.0000, 
raw observation next is [0.5, 92.0, 54.5, 0.0, 26.0, 26.08792225195037, 0.5895144679483473, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4764542936288089, 0.92, 0.18166666666666667, 0.0, 0.6666666666666666, 0.6739935209958642, 0.696504822649449, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.71287686], dtype=float32), 0.8993627]. 
=============================================
[2019-04-04 03:57:03,643] A3C_AGENT_WORKER-Thread-10 INFO:Local step 256500, global step 4107049: loss 0.0702
[2019-04-04 03:57:03,643] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 256500, global step 4107049: learning rate 0.0001
[2019-04-04 03:57:03,876] A3C_AGENT_WORKER-Thread-20 INFO:Local step 256500, global step 4107093: loss 0.0699
[2019-04-04 03:57:03,886] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 256500, global step 4107093: learning rate 0.0001
[2019-04-04 03:57:06,276] A3C_AGENT_WORKER-Thread-8 INFO:Local step 256500, global step 4108225: loss 0.0649
[2019-04-04 03:57:06,277] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 256500, global step 4108225: learning rate 0.0001
[2019-04-04 03:57:06,430] A3C_AGENT_WORKER-Thread-18 INFO:Local step 256500, global step 4108311: loss 0.0605
[2019-04-04 03:57:06,434] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 256500, global step 4108312: learning rate 0.0001
[2019-04-04 03:57:06,457] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.32606694e-32 2.07158687e-27 1.10886485e-29 8.35414580e-28
 1.06026912e-27 1.60929350e-31 1.00000000e+00], sum to 1.0000
[2019-04-04 03:57:06,465] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8583
[2019-04-04 03:57:06,472] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 81.0, 0.0, 0.0, 26.0, 25.65853643939625, 0.5225276721938997, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1017600.0000, 
sim time next is 1018200.0000, 
raw observation next is [14.4, 81.0, 0.0, 0.0, 26.0, 25.6141118760364, 0.5168688400659217, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.8614958448753465, 0.81, 0.0, 0.0, 0.6666666666666666, 0.6345093230030333, 0.6722896133553072, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0154263], dtype=float32), -0.4104668]. 
=============================================
[2019-04-04 03:57:07,571] A3C_AGENT_WORKER-Thread-5 INFO:Local step 257000, global step 4108945: loss 1.8355
[2019-04-04 03:57:07,572] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 257000, global step 4108945: learning rate 0.0001
[2019-04-04 03:57:07,578] A3C_AGENT_WORKER-Thread-16 INFO:Local step 256500, global step 4108950: loss 0.0687
[2019-04-04 03:57:07,579] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 256500, global step 4108950: learning rate 0.0001
[2019-04-04 03:57:08,194] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.9599377e-32 3.5802691e-27 7.2375748e-30 6.1145425e-28 1.2650418e-27
 1.8312365e-31 1.0000000e+00], sum to 1.0000
[2019-04-04 03:57:08,198] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1932
[2019-04-04 03:57:08,207] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 95.0, 84.0, 0.0, 26.0, 25.81351680817056, 0.4775429834776723, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1422600.0000, 
sim time next is 1423200.0000, 
raw observation next is [0.0, 95.0, 87.0, 0.0, 26.0, 25.75344646066129, 0.4639896409521698, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.46260387811634357, 0.95, 0.29, 0.0, 0.6666666666666666, 0.6461205383884409, 0.6546632136507232, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5749141], dtype=float32), -1.0047961]. 
=============================================
[2019-04-04 03:57:09,853] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.7679546e-34 2.5300000e-29 5.6233934e-32 5.0988788e-30 5.4196180e-30
 6.5055503e-34 1.0000000e+00], sum to 1.0000
[2019-04-04 03:57:09,856] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3070
[2019-04-04 03:57:09,863] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [19.4, 49.0, 116.0, 0.0, 26.0, 27.80621957410134, 1.001892969901353, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1090800.0000, 
sim time next is 1091400.0000, 
raw observation next is [19.4, 49.0, 108.0, 0.0, 26.0, 27.771708576678, 0.9964216242371721, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 1.0, 0.49, 0.36, 0.0, 0.6666666666666666, 0.8143090480565002, 0.8321405414123907, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6775397], dtype=float32), 0.37919694]. 
=============================================
[2019-04-04 03:57:10,245] A3C_AGENT_WORKER-Thread-14 INFO:Local step 257000, global step 4110513: loss 1.8779
[2019-04-04 03:57:10,246] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 257000, global step 4110513: learning rate 0.0001
[2019-04-04 03:57:11,242] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.3551152e-30 3.0761603e-26 5.7880918e-29 3.7853421e-27 5.0261398e-27
 1.4965236e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 03:57:11,246] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5786
[2019-04-04 03:57:11,269] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 26.0, 25.37202989916993, 0.4846600251041966, 0.0, 1.0, 46644.29145911751], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1468200.0000, 
sim time next is 1468800.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 26.0, 25.3558234533264, 0.4832139204532866, 0.0, 1.0, 42359.32267590512], 
processed observation next is [1.0, 0.0, 0.5069252077562327, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6129852877772001, 0.6610713068177622, 0.0, 1.0, 0.20171106036145295], 
reward next is 0.7983, 
noisyNet noise sample is [array([0.01982247], dtype=float32), 0.96649605]. 
=============================================
[2019-04-04 03:57:12,044] A3C_AGENT_WORKER-Thread-3 INFO:Local step 257000, global step 4111578: loss 1.7813
[2019-04-04 03:57:12,045] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 257000, global step 4111579: learning rate 0.0001
[2019-04-04 03:57:12,261] A3C_AGENT_WORKER-Thread-9 INFO:Local step 257000, global step 4111716: loss 1.9127
[2019-04-04 03:57:12,261] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 257000, global step 4111717: learning rate 0.0001
[2019-04-04 03:57:12,273] A3C_AGENT_WORKER-Thread-15 INFO:Local step 257000, global step 4111724: loss 1.8893
[2019-04-04 03:57:12,275] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 257000, global step 4111725: learning rate 0.0001
[2019-04-04 03:57:13,511] A3C_AGENT_WORKER-Thread-6 INFO:Local step 257000, global step 4112371: loss 1.7483
[2019-04-04 03:57:13,519] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 257000, global step 4112371: learning rate 0.0001
[2019-04-04 03:57:14,055] A3C_AGENT_WORKER-Thread-2 INFO:Local step 257000, global step 4112710: loss 1.9159
[2019-04-04 03:57:14,057] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 257000, global step 4112711: learning rate 0.0001
[2019-04-04 03:57:14,472] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.7845300e-29 3.2232874e-24 4.0174275e-27 4.3635410e-26 1.0326083e-25
 7.1907676e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 03:57:14,473] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2068
[2019-04-04 03:57:14,493] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.7, 92.0, 0.0, 0.0, 26.0, 25.4269419483258, 0.5425751728150076, 0.0, 1.0, 56002.25267370819], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1313400.0000, 
sim time next is 1314000.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 26.0, 25.44406321659527, 0.5634002874657053, 0.0, 1.0, 39164.60673135251], 
processed observation next is [1.0, 0.21739130434782608, 0.5069252077562327, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6203386013829393, 0.6878000958219017, 0.0, 1.0, 0.18649812729215481], 
reward next is 0.8135, 
noisyNet noise sample is [array([-0.265255], dtype=float32), -0.407122]. 
=============================================
[2019-04-04 03:57:14,502] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[87.21769 ]
 [87.274605]
 [87.3686  ]
 [87.4259  ]
 [87.4611  ]], R is [[87.16149902]
 [87.02320862]
 [87.06367493]
 [87.10372162]
 [87.14336395]].
[2019-04-04 03:57:15,570] A3C_AGENT_WORKER-Thread-4 INFO:Local step 257000, global step 4113585: loss 1.8015
[2019-04-04 03:57:15,572] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 257000, global step 4113585: learning rate 0.0001
[2019-04-04 03:57:16,323] A3C_AGENT_WORKER-Thread-19 INFO:Local step 257500, global step 4114019: loss 0.0290
[2019-04-04 03:57:16,324] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 257500, global step 4114019: learning rate 0.0001
[2019-04-04 03:57:16,757] A3C_AGENT_WORKER-Thread-10 INFO:Local step 257000, global step 4114270: loss 1.8726
[2019-04-04 03:57:16,758] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 257000, global step 4114270: learning rate 0.0001
[2019-04-04 03:57:16,767] A3C_AGENT_WORKER-Thread-20 INFO:Local step 257000, global step 4114275: loss 1.8823
[2019-04-04 03:57:16,781] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 257000, global step 4114277: learning rate 0.0001
[2019-04-04 03:57:17,062] A3C_AGENT_WORKER-Thread-7 INFO:Local step 257500, global step 4114423: loss 0.0411
[2019-04-04 03:57:17,066] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 257500, global step 4114424: learning rate 0.0001
[2019-04-04 03:57:17,175] A3C_AGENT_WORKER-Thread-17 INFO:Local step 257500, global step 4114468: loss 0.0365
[2019-04-04 03:57:17,178] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 257500, global step 4114468: learning rate 0.0001
[2019-04-04 03:57:18,861] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.0327794e-30 5.6662837e-26 5.7777484e-28 4.5705950e-27 5.2489584e-26
 1.0428590e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 03:57:18,861] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8814
[2019-04-04 03:57:18,877] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.533333333333334, 92.0, 0.0, 0.0, 26.0, 25.48007174391674, 0.5472168749560865, 0.0, 1.0, 52633.65287221607], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1308000.0000, 
sim time next is 1308600.0000, 
raw observation next is [2.45, 92.0, 0.0, 0.0, 26.0, 25.42526903540245, 0.5538109978672241, 0.0, 1.0, 72954.79521996416], 
processed observation next is [1.0, 0.13043478260869565, 0.5304709141274239, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6187724196168709, 0.6846036659557413, 0.0, 1.0, 0.3474037867617341], 
reward next is 0.6526, 
noisyNet noise sample is [array([0.32038283], dtype=float32), 1.1655207]. 
=============================================
[2019-04-04 03:57:19,068] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.77832791e-32 2.20574811e-27 1.17141815e-29 1.78107767e-27
 7.79863298e-28 1.89453367e-31 1.00000000e+00], sum to 1.0000
[2019-04-04 03:57:19,069] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1599
[2019-04-04 03:57:19,071] A3C_AGENT_WORKER-Thread-8 INFO:Local step 257000, global step 4115375: loss 1.9148
[2019-04-04 03:57:19,078] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 257000, global step 4115375: learning rate 0.0001
[2019-04-04 03:57:19,086] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.8, 49.0, 122.6666666666667, 0.0, 26.0, 26.88958143679467, 0.7844970403192129, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1608600.0000, 
sim time next is 1609200.0000, 
raw observation next is [13.8, 49.0, 111.5, 0.0, 26.0, 26.11157826484739, 0.7123247739705945, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.844875346260388, 0.49, 0.37166666666666665, 0.0, 0.6666666666666666, 0.6759648554039493, 0.7374415913235315, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.17064661], dtype=float32), 0.45791033]. 
=============================================
[2019-04-04 03:57:19,096] A3C_AGENT_WORKER-Thread-18 INFO:Local step 257000, global step 4115385: loss 1.8199
[2019-04-04 03:57:19,097] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 257000, global step 4115385: learning rate 0.0001
[2019-04-04 03:57:20,400] A3C_AGENT_WORKER-Thread-16 INFO:Local step 257000, global step 4116008: loss 1.7350
[2019-04-04 03:57:20,409] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 257000, global step 4116008: learning rate 0.0001
[2019-04-04 03:57:25,383] A3C_AGENT_WORKER-Thread-5 INFO:Local step 257500, global step 4118047: loss 0.0338
[2019-04-04 03:57:25,385] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 257500, global step 4118047: learning rate 0.0001
[2019-04-04 03:57:27,539] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.0047176e-30 7.1506189e-27 3.9581730e-29 3.9053101e-26 5.4601955e-27
 7.5597495e-31 1.0000000e+00], sum to 1.0000
[2019-04-04 03:57:27,539] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8243
[2019-04-04 03:57:27,546] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.066666666666666, 66.33333333333334, 83.33333333333334, 694.6666666666667, 26.0, 26.38150085396911, 0.6844326538871474, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1518000.0000, 
sim time next is 1518600.0000, 
raw observation next is [9.533333333333333, 64.66666666666666, 81.66666666666666, 688.3333333333333, 26.0, 26.49890315135366, 0.7083654259055728, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.7266851338873501, 0.6466666666666666, 0.2722222222222222, 0.7605893186003683, 0.6666666666666666, 0.7082419292794716, 0.7361218086351909, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9564398], dtype=float32), -0.71897715]. 
=============================================
[2019-04-04 03:57:27,573] A3C_AGENT_WORKER-Thread-14 INFO:Local step 257500, global step 4118983: loss 0.0226
[2019-04-04 03:57:27,575] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 257500, global step 4118983: learning rate 0.0001
[2019-04-04 03:57:27,717] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.5622380e-29 2.0750227e-24 2.0358876e-26 7.9841934e-25 4.4383163e-25
 3.4975971e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:57:27,717] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6701
[2019-04-04 03:57:27,733] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.20619445740169, 0.4356784889876606, 0.0, 1.0, 38473.10655435454], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1402800.0000, 
sim time next is 1403400.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.1975866993153, 0.4278938523279037, 0.0, 1.0, 38480.02803496933], 
processed observation next is [1.0, 0.21739130434782608, 0.44598337950138506, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5997988916096082, 0.6426312841093013, 0.0, 1.0, 0.18323822873794918], 
reward next is 0.8168, 
noisyNet noise sample is [array([-0.43127677], dtype=float32), 1.2082367]. 
=============================================
[2019-04-04 03:57:29,697] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.8147570e-29 3.1465773e-25 1.2447689e-27 2.5687298e-25 2.2626885e-25
 1.2829930e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 03:57:29,697] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2001
[2019-04-04 03:57:29,716] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.45, 73.5, 0.0, 0.0, 26.0, 25.2964552057658, 0.5534028149206998, 0.0, 1.0, 196664.0695617451], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1542600.0000, 
sim time next is 1543200.0000, 
raw observation next is [7.533333333333333, 73.66666666666666, 0.0, 0.0, 26.0, 25.26567745959006, 0.5811164690012379, 0.0, 1.0, 198415.2365098056], 
processed observation next is [1.0, 0.8695652173913043, 0.6712834718374886, 0.7366666666666666, 0.0, 0.0, 0.6666666666666666, 0.6054731216325049, 0.6937054896670793, 0.0, 1.0, 0.9448344595705028], 
reward next is 0.0552, 
noisyNet noise sample is [array([0.06037207], dtype=float32), 0.68329513]. 
=============================================
[2019-04-04 03:57:29,737] A3C_AGENT_WORKER-Thread-9 INFO:Local step 257500, global step 4119926: loss 0.0186
[2019-04-04 03:57:29,743] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 257500, global step 4119926: learning rate 0.0001
[2019-04-04 03:57:30,200] A3C_AGENT_WORKER-Thread-15 INFO:Local step 257500, global step 4120124: loss 0.0178
[2019-04-04 03:57:30,201] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 257500, global step 4120124: learning rate 0.0001
[2019-04-04 03:57:30,312] A3C_AGENT_WORKER-Thread-3 INFO:Local step 257500, global step 4120175: loss 0.0168
[2019-04-04 03:57:30,318] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 257500, global step 4120175: learning rate 0.0001
[2019-04-04 03:57:31,228] A3C_AGENT_WORKER-Thread-6 INFO:Local step 257500, global step 4120537: loss 0.0163
[2019-04-04 03:57:31,230] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 257500, global step 4120537: learning rate 0.0001
[2019-04-04 03:57:32,391] A3C_AGENT_WORKER-Thread-2 INFO:Local step 257500, global step 4121067: loss 0.0212
[2019-04-04 03:57:32,394] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 257500, global step 4121069: learning rate 0.0001
[2019-04-04 03:57:32,473] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.5959173e-31 1.7528160e-27 2.2818836e-29 4.1330965e-27 1.5954701e-27
 2.3453701e-31 1.0000000e+00], sum to 1.0000
[2019-04-04 03:57:32,481] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8893
[2019-04-04 03:57:32,494] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.666666666666666, 71.33333333333334, 90.0, 700.6666666666666, 26.0, 25.82762013924738, 0.5912763100424173, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1516200.0000, 
sim time next is 1516800.0000, 
raw observation next is [8.133333333333333, 69.66666666666667, 87.5, 700.8333333333334, 26.0, 25.96050469659746, 0.6304922057379434, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.687903970452447, 0.6966666666666668, 0.2916666666666667, 0.774401473296501, 0.6666666666666666, 0.6633753913831217, 0.7101640685793145, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.25350302], dtype=float32), -0.6855112]. 
=============================================
[2019-04-04 03:57:33,575] A3C_AGENT_WORKER-Thread-4 INFO:Local step 257500, global step 4121683: loss 0.0211
[2019-04-04 03:57:33,579] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 257500, global step 4121683: learning rate 0.0001
[2019-04-04 03:57:33,836] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.1336200e-30 1.5870576e-26 1.2479062e-28 2.8360006e-26 1.3702237e-26
 3.0256452e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 03:57:33,844] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2271
[2019-04-04 03:57:33,850] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.616666666666667, 74.33333333333334, 0.0, 0.0, 26.0, 25.48761857925103, 0.5596153273749805, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1627800.0000, 
sim time next is 1628400.0000, 
raw observation next is [7.533333333333333, 74.66666666666667, 0.0, 0.0, 26.0, 25.39618208207875, 0.5592340427234507, 0.0, 1.0, 196217.9094192961], 
processed observation next is [1.0, 0.8695652173913043, 0.6712834718374886, 0.7466666666666667, 0.0, 0.0, 0.6666666666666666, 0.6163485068398957, 0.6864113475744835, 0.0, 1.0, 0.9343709972347434], 
reward next is 0.0656, 
noisyNet noise sample is [array([1.4085788], dtype=float32), 0.24547283]. 
=============================================
[2019-04-04 03:57:34,316] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.1753463e-29 1.7929204e-25 2.1758132e-27 3.1911701e-25 3.4463602e-25
 6.5238331e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 03:57:34,324] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7767
[2019-04-04 03:57:34,363] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.333333333333334, 82.0, 0.0, 0.0, 26.0, 25.58434253957916, 0.5739347260831248, 0.0, 1.0, 18739.46116390277], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1552800.0000, 
sim time next is 1553400.0000, 
raw observation next is [5.25, 82.0, 0.0, 0.0, 26.0, 25.57274552742539, 0.5644402270737611, 0.0, 1.0, 18738.2382081111], 
processed observation next is [1.0, 1.0, 0.60803324099723, 0.82, 0.0, 0.0, 0.6666666666666666, 0.6310621272854492, 0.6881467423579203, 0.0, 1.0, 0.08922970575290999], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.00692494], dtype=float32), 1.2112619]. 
=============================================
[2019-04-04 03:57:35,137] A3C_AGENT_WORKER-Thread-10 INFO:Local step 257500, global step 4122416: loss 0.0207
[2019-04-04 03:57:35,138] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 257500, global step 4122416: learning rate 0.0001
[2019-04-04 03:57:35,246] A3C_AGENT_WORKER-Thread-20 INFO:Local step 257500, global step 4122464: loss 0.0206
[2019-04-04 03:57:35,246] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 257500, global step 4122464: learning rate 0.0001
[2019-04-04 03:57:36,897] A3C_AGENT_WORKER-Thread-19 INFO:Local step 258000, global step 4123155: loss 0.0245
[2019-04-04 03:57:36,898] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 258000, global step 4123155: learning rate 0.0001
[2019-04-04 03:57:37,372] A3C_AGENT_WORKER-Thread-8 INFO:Local step 257500, global step 4123353: loss 0.0272
[2019-04-04 03:57:37,375] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 257500, global step 4123354: learning rate 0.0001
[2019-04-04 03:57:37,521] A3C_AGENT_WORKER-Thread-18 INFO:Local step 257500, global step 4123411: loss 0.0292
[2019-04-04 03:57:37,522] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 257500, global step 4123411: learning rate 0.0001
[2019-04-04 03:57:37,571] A3C_AGENT_WORKER-Thread-7 INFO:Local step 258000, global step 4123433: loss 0.0248
[2019-04-04 03:57:37,573] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 258000, global step 4123433: learning rate 0.0001
[2019-04-04 03:57:38,028] A3C_AGENT_WORKER-Thread-17 INFO:Local step 258000, global step 4123612: loss 0.0204
[2019-04-04 03:57:38,028] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 258000, global step 4123612: learning rate 0.0001
[2019-04-04 03:57:39,208] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.9153484e-28 5.4681846e-25 7.9142894e-27 1.2049095e-24 1.2069473e-24
 2.6701557e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:57:39,210] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7570
[2019-04-04 03:57:39,235] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.45, 91.83333333333334, 0.0, 0.0, 26.0, 25.35846936288074, 0.4593602434269536, 0.0, 1.0, 44214.86413182768], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1732200.0000, 
sim time next is 1732800.0000, 
raw observation next is [0.4, 91.66666666666667, 0.0, 0.0, 26.0, 25.34224321497358, 0.455689660952909, 0.0, 1.0, 43483.40263982167], 
processed observation next is [0.0, 0.043478260869565216, 0.4736842105263158, 0.9166666666666667, 0.0, 0.0, 0.6666666666666666, 0.6118536012477982, 0.6518965536509697, 0.0, 1.0, 0.2070638220943889], 
reward next is 0.7929, 
noisyNet noise sample is [array([0.08343302], dtype=float32), 0.54152274]. 
=============================================
[2019-04-04 03:57:39,456] A3C_AGENT_WORKER-Thread-16 INFO:Local step 257500, global step 4124159: loss 0.0297
[2019-04-04 03:57:39,457] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 257500, global step 4124159: learning rate 0.0001
[2019-04-04 03:57:39,464] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.2411791e-30 1.6342920e-25 7.3783883e-28 3.3001879e-26 8.3793366e-26
 9.9161679e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 03:57:39,467] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9417
[2019-04-04 03:57:39,478] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.5166314e-28 1.2964521e-24 1.8606397e-26 4.0908922e-24 2.6915039e-24
 1.8643117e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:57:39,479] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3260
[2019-04-04 03:57:39,488] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 88.0, 15.5, 0.0, 26.0, 25.74591497964901, 0.5234089911970701, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1702800.0000, 
sim time next is 1703400.0000, 
raw observation next is [1.1, 88.00000000000001, 10.66666666666666, 0.0, 26.0, 25.88814625839878, 0.5343937241585534, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.49307479224376743, 0.8800000000000001, 0.035555555555555535, 0.0, 0.6666666666666666, 0.6573455215332317, 0.6781312413861844, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0295311], dtype=float32), -0.6952513]. 
=============================================
[2019-04-04 03:57:39,504] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.3, 91.33333333333334, 0.0, 0.0, 26.0, 25.34037666456886, 0.4534682181767955, 0.0, 1.0, 42946.15967838938], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1734000.0000, 
sim time next is 1734600.0000, 
raw observation next is [0.25, 91.16666666666667, 0.0, 0.0, 26.0, 25.32768726345083, 0.4483729229428112, 0.0, 1.0, 42904.48344992023], 
processed observation next is [0.0, 0.043478260869565216, 0.46952908587257625, 0.9116666666666667, 0.0, 0.0, 0.6666666666666666, 0.6106406052875691, 0.6494576409809371, 0.0, 1.0, 0.2043070640472392], 
reward next is 0.7957, 
noisyNet noise sample is [array([-0.9539706], dtype=float32), -1.4163055]. 
=============================================
[2019-04-04 03:57:45,821] A3C_AGENT_WORKER-Thread-5 INFO:Local step 258000, global step 4126322: loss 0.0141
[2019-04-04 03:57:45,822] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 258000, global step 4126322: learning rate 0.0001
[2019-04-04 03:57:48,941] A3C_AGENT_WORKER-Thread-14 INFO:Local step 258000, global step 4127374: loss 0.0173
[2019-04-04 03:57:48,942] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 258000, global step 4127374: learning rate 0.0001
[2019-04-04 03:57:50,962] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [7.5754035e-29 9.9326995e-25 7.3194652e-27 2.8461027e-25 5.9349334e-25
 9.7315392e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 03:57:50,962] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8983
[2019-04-04 03:57:51,013] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 77.66666666666667, 154.6666666666667, 0.0, 26.0, 25.35074112643948, 0.2998232188013544, 1.0, 1.0, 43344.24290241934], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2032800.0000, 
sim time next is 2033400.0000, 
raw observation next is [-4.5, 78.33333333333334, 153.3333333333333, 0.0, 26.0, 25.32275660983174, 0.3100822329044441, 1.0, 1.0, 35237.52014046651], 
processed observation next is [1.0, 0.5217391304347826, 0.3379501385041552, 0.7833333333333334, 0.511111111111111, 0.0, 0.6666666666666666, 0.6102297174859782, 0.6033607443014813, 1.0, 1.0, 0.16779771495460244], 
reward next is 0.8322, 
noisyNet noise sample is [array([-0.43832195], dtype=float32), 1.96713]. 
=============================================
[2019-04-04 03:57:51,365] A3C_AGENT_WORKER-Thread-9 INFO:Local step 258000, global step 4127978: loss 0.0233
[2019-04-04 03:57:51,366] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 258000, global step 4127978: learning rate 0.0001
[2019-04-04 03:57:51,853] A3C_AGENT_WORKER-Thread-15 INFO:Local step 258000, global step 4128100: loss 0.0249
[2019-04-04 03:57:51,854] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 258000, global step 4128100: learning rate 0.0001
[2019-04-04 03:57:52,361] A3C_AGENT_WORKER-Thread-3 INFO:Local step 258000, global step 4128229: loss 0.0201
[2019-04-04 03:57:52,362] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 258000, global step 4128229: learning rate 0.0001
[2019-04-04 03:57:53,233] A3C_AGENT_WORKER-Thread-6 INFO:Local step 258000, global step 4128452: loss 0.0203
[2019-04-04 03:57:53,234] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 258000, global step 4128452: learning rate 0.0001
[2019-04-04 03:57:54,361] A3C_AGENT_WORKER-Thread-2 INFO:Local step 258000, global step 4128757: loss 0.0195
[2019-04-04 03:57:54,362] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 258000, global step 4128757: learning rate 0.0001
[2019-04-04 03:57:56,855] A3C_AGENT_WORKER-Thread-4 INFO:Local step 258000, global step 4129596: loss 0.0252
[2019-04-04 03:57:56,856] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 258000, global step 4129596: learning rate 0.0001
[2019-04-04 03:57:58,402] A3C_AGENT_WORKER-Thread-20 INFO:Local step 258000, global step 4130016: loss 0.0178
[2019-04-04 03:57:58,403] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 258000, global step 4130016: learning rate 0.0001
[2019-04-04 03:57:58,411] A3C_AGENT_WORKER-Thread-10 INFO:Local step 258000, global step 4130020: loss 0.0207
[2019-04-04 03:57:58,412] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 258000, global step 4130020: learning rate 0.0001
[2019-04-04 03:58:00,541] A3C_AGENT_WORKER-Thread-8 INFO:Local step 258000, global step 4130583: loss 0.0173
[2019-04-04 03:58:00,541] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 258000, global step 4130583: learning rate 0.0001
[2019-04-04 03:58:01,753] A3C_AGENT_WORKER-Thread-18 INFO:Local step 258000, global step 4130882: loss 0.0171
[2019-04-04 03:58:01,754] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 258000, global step 4130882: learning rate 0.0001
[2019-04-04 03:58:03,708] A3C_AGENT_WORKER-Thread-19 INFO:Local step 258500, global step 4131491: loss 0.0039
[2019-04-04 03:58:03,709] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 258500, global step 4131491: learning rate 0.0001
[2019-04-04 03:58:03,804] A3C_AGENT_WORKER-Thread-16 INFO:Local step 258000, global step 4131521: loss 0.0159
[2019-04-04 03:58:03,804] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 258000, global step 4131521: learning rate 0.0001
[2019-04-04 03:58:04,597] A3C_AGENT_WORKER-Thread-7 INFO:Local step 258500, global step 4131790: loss 0.0097
[2019-04-04 03:58:04,598] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 258500, global step 4131790: learning rate 0.0001
[2019-04-04 03:58:04,982] A3C_AGENT_WORKER-Thread-17 INFO:Local step 258500, global step 4131905: loss 0.0189
[2019-04-04 03:58:04,984] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 258500, global step 4131905: learning rate 0.0001
[2019-04-04 03:58:12,716] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.2632543e-27 9.6668202e-23 4.7896183e-25 1.0085043e-23 1.4329691e-23
 2.0789425e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:58:12,718] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9864
[2019-04-04 03:58:12,739] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.7, 78.0, 0.0, 0.0, 26.0, 23.79955480443573, 0.0190198850787707, 0.0, 1.0, 43439.39486182722], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2098800.0000, 
sim time next is 2099400.0000, 
raw observation next is [-6.800000000000001, 78.16666666666667, 0.0, 0.0, 26.0, 23.77552735957714, 0.01076023100295946, 0.0, 1.0, 43331.41048358567], 
processed observation next is [1.0, 0.30434782608695654, 0.2742382271468144, 0.7816666666666667, 0.0, 0.0, 0.6666666666666666, 0.4812939466314283, 0.5035867436676532, 0.0, 1.0, 0.20634004992183652], 
reward next is 0.7937, 
noisyNet noise sample is [array([0.75472206], dtype=float32), 1.0447825]. 
=============================================
[2019-04-04 03:58:13,899] A3C_AGENT_WORKER-Thread-5 INFO:Local step 258500, global step 4134436: loss 0.0006
[2019-04-04 03:58:13,902] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 258500, global step 4134436: learning rate 0.0001
[2019-04-04 03:58:16,228] A3C_AGENT_WORKER-Thread-14 INFO:Local step 258500, global step 4135027: loss 0.0080
[2019-04-04 03:58:16,229] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 258500, global step 4135027: learning rate 0.0001
[2019-04-04 03:58:18,253] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.4535182e-28 1.3833550e-24 5.3224322e-26 9.0671822e-24 3.8902668e-24
 1.0943473e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:58:18,253] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6933
[2019-04-04 03:58:18,271] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.7, 83.0, 0.0, 0.0, 26.0, 25.21737653277829, 0.3976263854053873, 0.0, 1.0, 42206.82806102424], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2152800.0000, 
sim time next is 2153400.0000, 
raw observation next is [-6.800000000000001, 82.83333333333334, 0.0, 0.0, 26.0, 25.19147858450103, 0.3490897065955352, 0.0, 1.0, 42519.48520479897], 
processed observation next is [1.0, 0.9565217391304348, 0.2742382271468144, 0.8283333333333335, 0.0, 0.0, 0.6666666666666666, 0.5992898820417526, 0.616363235531845, 0.0, 1.0, 0.20247373907047128], 
reward next is 0.7975, 
noisyNet noise sample is [array([-1.0191056], dtype=float32), -0.16979645]. 
=============================================
[2019-04-04 03:58:18,904] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.7226207e-27 9.0198506e-24 1.9393932e-25 8.2066549e-24 9.8955659e-24
 1.0457209e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:58:18,904] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7985
[2019-04-04 03:58:18,937] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.616666666666667, 83.66666666666667, 0.0, 0.0, 26.0, 24.13590745055667, 0.07610600722437852, 0.0, 1.0, 43500.84823573153], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2094600.0000, 
sim time next is 2095200.0000, 
raw observation next is [-6.7, 83.0, 0.0, 0.0, 26.0, 24.06241220357136, 0.07261119835244921, 0.0, 1.0, 43563.55716902344], 
processed observation next is [1.0, 0.2608695652173913, 0.2770083102493075, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5052010169642799, 0.5242037327841497, 0.0, 1.0, 0.20744551032868305], 
reward next is 0.7926, 
noisyNet noise sample is [array([-0.55899334], dtype=float32), 0.09987985]. 
=============================================
[2019-04-04 03:58:19,078] A3C_AGENT_WORKER-Thread-9 INFO:Local step 258500, global step 4135847: loss 0.0011
[2019-04-04 03:58:19,082] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 258500, global step 4135848: learning rate 0.0001
[2019-04-04 03:58:19,325] A3C_AGENT_WORKER-Thread-15 INFO:Local step 258500, global step 4135920: loss 0.0026
[2019-04-04 03:58:19,330] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 258500, global step 4135920: learning rate 0.0001
[2019-04-04 03:58:20,262] A3C_AGENT_WORKER-Thread-3 INFO:Local step 258500, global step 4136243: loss 0.0007
[2019-04-04 03:58:20,263] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 258500, global step 4136243: learning rate 0.0001
[2019-04-04 03:58:20,356] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.5837557e-29 1.2476803e-24 2.9334889e-27 1.9084519e-25 2.7548356e-25
 1.1412705e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:58:20,356] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4695
[2019-04-04 03:58:20,410] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.8, 82.0, 185.3333333333333, 98.66666666666667, 26.0, 25.79682429116654, 0.3973865403668149, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2109000.0000, 
sim time next is 2109600.0000, 
raw observation next is [-7.8, 82.0, 191.0, 89.0, 26.0, 25.84290754779257, 0.4027765732773818, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.24653739612188366, 0.82, 0.6366666666666667, 0.09834254143646409, 0.6666666666666666, 0.6535756289827143, 0.6342588577591273, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.939336], dtype=float32), 0.053281877]. 
=============================================
[2019-04-04 03:58:21,120] A3C_AGENT_WORKER-Thread-6 INFO:Local step 258500, global step 4136518: loss 0.0021
[2019-04-04 03:58:21,124] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 258500, global step 4136518: learning rate 0.0001
[2019-04-04 03:58:21,340] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.7371173e-28 5.4435271e-23 1.1345527e-25 4.1219710e-24 3.6212616e-24
 1.0183181e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 03:58:21,340] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2000
[2019-04-04 03:58:21,359] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.533333333333334, 77.0, 0.0, 0.0, 26.0, 24.14886341213963, 0.08516802762487825, 0.0, 1.0, 42074.23045600582], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2175600.0000, 
sim time next is 2176200.0000, 
raw observation next is [-6.45, 76.5, 0.0, 0.0, 26.0, 24.12438756386508, 0.07403210653297965, 0.0, 1.0, 42051.50385924979], 
processed observation next is [1.0, 0.17391304347826086, 0.28393351800554023, 0.765, 0.0, 0.0, 0.6666666666666666, 0.5103656303220901, 0.5246773688443266, 0.0, 1.0, 0.20024525647261804], 
reward next is 0.7998, 
noisyNet noise sample is [array([0.7787617], dtype=float32), -0.07139421]. 
=============================================
[2019-04-04 03:58:21,808] A3C_AGENT_WORKER-Thread-2 INFO:Local step 258500, global step 4136720: loss 0.0007
[2019-04-04 03:58:21,809] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 258500, global step 4136720: learning rate 0.0001
[2019-04-04 03:58:24,886] A3C_AGENT_WORKER-Thread-4 INFO:Local step 258500, global step 4137498: loss 0.0010
[2019-04-04 03:58:24,887] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 258500, global step 4137498: learning rate 0.0001
[2019-04-04 03:58:26,273] A3C_AGENT_WORKER-Thread-20 INFO:Local step 258500, global step 4137890: loss 0.0015
[2019-04-04 03:58:26,275] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 258500, global step 4137890: learning rate 0.0001
[2019-04-04 03:58:26,424] A3C_AGENT_WORKER-Thread-10 INFO:Local step 258500, global step 4137943: loss 0.0010
[2019-04-04 03:58:26,426] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 258500, global step 4137943: learning rate 0.0001
[2019-04-04 03:58:27,182] A3C_AGENT_WORKER-Thread-8 INFO:Local step 258500, global step 4138232: loss 0.0022
[2019-04-04 03:58:27,183] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 258500, global step 4138232: learning rate 0.0001
[2019-04-04 03:58:29,008] A3C_AGENT_WORKER-Thread-18 INFO:Local step 258500, global step 4138823: loss 0.0028
[2019-04-04 03:58:29,014] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 258500, global step 4138823: learning rate 0.0001
[2019-04-04 03:58:29,959] A3C_AGENT_WORKER-Thread-19 INFO:Local step 259000, global step 4139102: loss 0.0070
[2019-04-04 03:58:29,964] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 259000, global step 4139103: learning rate 0.0001
[2019-04-04 03:58:31,006] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.7318684e-30 1.1463899e-26 2.3597955e-28 7.4344785e-26 3.4217048e-26
 2.7811309e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 03:58:31,007] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3736
[2019-04-04 03:58:31,064] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.9166666666666667, 43.16666666666667, 132.3333333333333, 48.0, 26.0, 26.31915576063546, 0.3974217330195591, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2301000.0000, 
sim time next is 2301600.0000, 
raw observation next is [0.7333333333333335, 43.33333333333334, 135.1666666666667, 45.0, 26.0, 25.8458250894587, 0.4285893580609319, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.4829178208679595, 0.4333333333333334, 0.4505555555555557, 0.049723756906077346, 0.6666666666666666, 0.6538187574548916, 0.6428631193536439, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0212768], dtype=float32), -1.1985713]. 
=============================================
[2019-04-04 03:58:31,776] A3C_AGENT_WORKER-Thread-7 INFO:Local step 259000, global step 4139604: loss 0.0035
[2019-04-04 03:58:31,777] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 259000, global step 4139604: learning rate 0.0001
[2019-04-04 03:58:31,940] A3C_AGENT_WORKER-Thread-16 INFO:Local step 258500, global step 4139656: loss 0.0044
[2019-04-04 03:58:31,942] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 258500, global step 4139656: learning rate 0.0001
[2019-04-04 03:58:32,102] A3C_AGENT_WORKER-Thread-17 INFO:Local step 259000, global step 4139708: loss 0.0044
[2019-04-04 03:58:32,114] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 259000, global step 4139710: learning rate 0.0001
[2019-04-04 03:58:36,759] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.5479711e-30 2.4143137e-25 7.2567129e-28 4.2027203e-26 2.1142361e-26
 1.3965913e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 03:58:36,759] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4893
[2019-04-04 03:58:36,819] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 56.0, 93.5, 25.5, 26.0, 25.60038111788484, 0.2689656965354875, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2538000.0000, 
sim time next is 2538600.0000, 
raw observation next is [-2.533333333333334, 54.83333333333334, 107.6666666666667, 28.0, 26.0, 25.64517903468457, 0.274340040757456, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.39242843951985223, 0.5483333333333335, 0.358888888888889, 0.030939226519337018, 0.6666666666666666, 0.6370982528903809, 0.5914466802524854, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9734476], dtype=float32), 0.77292]. 
=============================================
[2019-04-04 03:58:40,592] A3C_AGENT_WORKER-Thread-5 INFO:Local step 259000, global step 4142456: loss 0.0047
[2019-04-04 03:58:40,614] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 259000, global step 4142460: learning rate 0.0001
[2019-04-04 03:58:42,921] A3C_AGENT_WORKER-Thread-14 INFO:Local step 259000, global step 4143264: loss 0.0048
[2019-04-04 03:58:42,922] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 259000, global step 4143264: learning rate 0.0001
[2019-04-04 03:58:45,422] A3C_AGENT_WORKER-Thread-9 INFO:Local step 259000, global step 4144004: loss 0.0022
[2019-04-04 03:58:45,424] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 259000, global step 4144004: learning rate 0.0001
[2019-04-04 03:58:45,450] A3C_AGENT_WORKER-Thread-15 INFO:Local step 259000, global step 4144011: loss 0.0024
[2019-04-04 03:58:45,450] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 259000, global step 4144011: learning rate 0.0001
[2019-04-04 03:58:46,700] A3C_AGENT_WORKER-Thread-3 INFO:Local step 259000, global step 4144426: loss 0.0025
[2019-04-04 03:58:46,701] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 259000, global step 4144426: learning rate 0.0001
[2019-04-04 03:58:47,211] A3C_AGENT_WORKER-Thread-6 INFO:Local step 259000, global step 4144620: loss 0.0029
[2019-04-04 03:58:47,220] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 259000, global step 4144620: learning rate 0.0001
[2019-04-04 03:58:47,895] A3C_AGENT_WORKER-Thread-2 INFO:Local step 259000, global step 4144900: loss 0.0049
[2019-04-04 03:58:47,901] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 259000, global step 4144900: learning rate 0.0001
[2019-04-04 03:58:50,208] A3C_AGENT_WORKER-Thread-4 INFO:Local step 259000, global step 4145724: loss 0.0042
[2019-04-04 03:58:50,208] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 259000, global step 4145724: learning rate 0.0001
[2019-04-04 03:58:51,832] A3C_AGENT_WORKER-Thread-20 INFO:Local step 259000, global step 4146262: loss 0.0039
[2019-04-04 03:58:51,835] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 259000, global step 4146262: learning rate 0.0001
[2019-04-04 03:58:52,065] A3C_AGENT_WORKER-Thread-10 INFO:Local step 259000, global step 4146341: loss 0.0039
[2019-04-04 03:58:52,067] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 259000, global step 4146342: learning rate 0.0001
[2019-04-04 03:58:52,648] A3C_AGENT_WORKER-Thread-8 INFO:Local step 259000, global step 4146562: loss 0.0026
[2019-04-04 03:58:52,649] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 259000, global step 4146562: learning rate 0.0001
[2019-04-04 03:58:53,342] A3C_AGENT_WORKER-Thread-19 INFO:Local step 259500, global step 4146814: loss 0.0001
[2019-04-04 03:58:53,343] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 259500, global step 4146814: learning rate 0.0001
[2019-04-04 03:58:54,813] A3C_AGENT_WORKER-Thread-7 INFO:Local step 259500, global step 4147381: loss 0.0097
[2019-04-04 03:58:54,813] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 259500, global step 4147381: learning rate 0.0001
[2019-04-04 03:58:54,958] A3C_AGENT_WORKER-Thread-18 INFO:Local step 259000, global step 4147450: loss 0.0031
[2019-04-04 03:58:54,974] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 259000, global step 4147451: learning rate 0.0001
[2019-04-04 03:58:55,147] A3C_AGENT_WORKER-Thread-17 INFO:Local step 259500, global step 4147525: loss 0.0126
[2019-04-04 03:58:55,149] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 259500, global step 4147525: learning rate 0.0001
[2019-04-04 03:58:57,457] A3C_AGENT_WORKER-Thread-16 INFO:Local step 259000, global step 4148330: loss 0.0033
[2019-04-04 03:58:57,458] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 259000, global step 4148330: learning rate 0.0001
[2019-04-04 03:58:59,663] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.7722305e-28 4.7601652e-25 8.8203424e-27 1.5655148e-24 4.1525392e-25
 8.6729934e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 03:58:59,664] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4889
[2019-04-04 03:58:59,707] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.433333333333333, 52.0, 0.0, 0.0, 26.0, 25.11085527817831, 0.4046251918533503, 0.0, 1.0, 61779.19744075568], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2580000.0000, 
sim time next is 2580600.0000, 
raw observation next is [-2.616666666666667, 54.0, 0.0, 0.0, 26.0, 25.28989237578768, 0.4157496878829272, 0.0, 1.0, 48658.19523439195], 
processed observation next is [1.0, 0.8695652173913043, 0.3901200369344414, 0.54, 0.0, 0.0, 0.6666666666666666, 0.60749103131564, 0.6385832292943091, 0.0, 1.0, 0.2317056915923426], 
reward next is 0.7683, 
noisyNet noise sample is [array([0.08228845], dtype=float32), 1.1069798]. 
=============================================
[2019-04-04 03:59:03,182] A3C_AGENT_WORKER-Thread-5 INFO:Local step 259500, global step 4150400: loss 0.0044
[2019-04-04 03:59:03,183] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 259500, global step 4150400: learning rate 0.0001
[2019-04-04 03:59:05,909] A3C_AGENT_WORKER-Thread-14 INFO:Local step 259500, global step 4151244: loss 0.0006
[2019-04-04 03:59:05,910] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 259500, global step 4151244: learning rate 0.0001
[2019-04-04 03:59:07,268] A3C_AGENT_WORKER-Thread-15 INFO:Local step 259500, global step 4151763: loss 0.0032
[2019-04-04 03:59:07,269] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 259500, global step 4151763: learning rate 0.0001
[2019-04-04 03:59:08,811] A3C_AGENT_WORKER-Thread-3 INFO:Local step 259500, global step 4152307: loss 0.0006
[2019-04-04 03:59:08,811] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 259500, global step 4152307: learning rate 0.0001
[2019-04-04 03:59:09,019] A3C_AGENT_WORKER-Thread-9 INFO:Local step 259500, global step 4152370: loss 0.0018
[2019-04-04 03:59:09,036] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 259500, global step 4152370: learning rate 0.0001
[2019-04-04 03:59:09,221] A3C_AGENT_WORKER-Thread-6 INFO:Local step 259500, global step 4152428: loss 0.0012
[2019-04-04 03:59:09,222] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 259500, global step 4152428: learning rate 0.0001
[2019-04-04 03:59:10,683] A3C_AGENT_WORKER-Thread-2 INFO:Local step 259500, global step 4152885: loss 0.0129
[2019-04-04 03:59:10,684] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 259500, global step 4152885: learning rate 0.0001
[2019-04-04 03:59:11,279] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.0278575e-27 5.3886556e-22 3.4544306e-24 7.7265480e-23 7.9764389e-23
 1.4145776e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 03:59:11,279] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8482
[2019-04-04 03:59:11,302] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-15.0, 91.0, 0.0, 0.0, 26.0, 23.51180627460512, -0.01140833228865865, 0.0, 1.0, 44500.55273092505], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2692800.0000, 
sim time next is 2693400.0000, 
raw observation next is [-15.0, 89.66666666666667, 0.0, 0.0, 26.0, 23.43901901650056, -0.01876596742700646, 0.0, 1.0, 44479.33604797773], 
processed observation next is [1.0, 0.17391304347826086, 0.04709141274238226, 0.8966666666666667, 0.0, 0.0, 0.6666666666666666, 0.45325158470837995, 0.4937446775243312, 0.0, 1.0, 0.2118063621332273], 
reward next is 0.7882, 
noisyNet noise sample is [array([-1.2054658], dtype=float32), -0.01641907]. 
=============================================
[2019-04-04 03:59:12,349] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [5.9184203e-28 5.8150369e-25 1.1475932e-26 1.7427444e-24 1.4107254e-24
 8.9512397e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 03:59:12,349] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.1365
[2019-04-04 03:59:12,390] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 50.0, 28.5, 253.5, 26.0, 25.59610119693251, 0.4543076756240847, 1.0, 1.0, 172573.9780704277], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2739600.0000, 
sim time next is 2740200.0000, 
raw observation next is [-3.166666666666667, 50.66666666666667, 20.0, 186.6666666666666, 26.0, 25.72521747142292, 0.4622105285282825, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.3748845798707295, 0.5066666666666667, 0.06666666666666667, 0.20626151012891336, 0.6666666666666666, 0.6437681226185766, 0.6540701761760942, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.36400187], dtype=float32), -1.4082831]. 
=============================================
[2019-04-04 03:59:12,863] A3C_AGENT_WORKER-Thread-4 INFO:Local step 259500, global step 4153600: loss 0.0067
[2019-04-04 03:59:12,866] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 259500, global step 4153601: learning rate 0.0001
[2019-04-04 03:59:14,465] A3C_AGENT_WORKER-Thread-20 INFO:Local step 259500, global step 4154186: loss 0.0065
[2019-04-04 03:59:14,468] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 259500, global step 4154188: learning rate 0.0001
[2019-04-04 03:59:14,961] A3C_AGENT_WORKER-Thread-10 INFO:Local step 259500, global step 4154379: loss 0.0072
[2019-04-04 03:59:14,961] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 259500, global step 4154379: learning rate 0.0001
[2019-04-04 03:59:15,003] A3C_AGENT_WORKER-Thread-8 INFO:Local step 259500, global step 4154396: loss 0.0062
[2019-04-04 03:59:15,003] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 259500, global step 4154396: learning rate 0.0001
[2019-04-04 03:59:16,254] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.2653809e-27 1.3071786e-24 3.7096517e-26 9.9505290e-24 1.0105222e-23
 7.2382447e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:59:16,254] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7965
[2019-04-04 03:59:16,290] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.333333333333333, 28.0, 139.8333333333333, 28.83333333333333, 26.0, 25.8754464797689, 0.4280916104874987, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2816400.0000, 
sim time next is 2817000.0000, 
raw observation next is [6.5, 27.0, 118.0, 0.0, 26.0, 25.92091289639496, 0.4121785671108053, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.6426592797783934, 0.27, 0.3933333333333333, 0.0, 0.6666666666666666, 0.66007607469958, 0.6373928557036018, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.768362], dtype=float32), 0.48587027]. 
=============================================
[2019-04-04 03:59:16,324] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[78.7365 ]
 [78.59169]
 [78.63728]
 [78.96038]
 [79.69707]], R is [[79.05626678]
 [79.26570129]
 [79.47304535]
 [79.67831421]
 [79.88153076]].
[2019-04-04 03:59:16,420] A3C_AGENT_WORKER-Thread-19 INFO:Local step 260000, global step 4154899: loss 0.1153
[2019-04-04 03:59:16,421] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 260000, global step 4154899: learning rate 0.0001
[2019-04-04 03:59:17,842] A3C_AGENT_WORKER-Thread-7 INFO:Local step 260000, global step 4155349: loss 0.1088
[2019-04-04 03:59:17,846] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 260000, global step 4155353: learning rate 0.0001
[2019-04-04 03:59:17,935] A3C_AGENT_WORKER-Thread-18 INFO:Local step 259500, global step 4155382: loss 0.0012
[2019-04-04 03:59:17,938] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 259500, global step 4155384: learning rate 0.0001
[2019-04-04 03:59:18,200] A3C_AGENT_WORKER-Thread-17 INFO:Local step 260000, global step 4155487: loss 0.1013
[2019-04-04 03:59:18,201] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 260000, global step 4155487: learning rate 0.0001
[2019-04-04 03:59:20,058] A3C_AGENT_WORKER-Thread-16 INFO:Local step 259500, global step 4156176: loss 0.0032
[2019-04-04 03:59:20,059] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 259500, global step 4156176: learning rate 0.0001
[2019-04-04 03:59:20,899] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.1353255e-29 3.8750391e-25 3.3376998e-27 4.0916083e-25 4.0897357e-25
 6.3148234e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 03:59:20,899] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1476
[2019-04-04 03:59:20,961] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 96.5, 71.0, 54.0, 26.0, 25.15613702378743, 0.4209336089274429, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2910600.0000, 
sim time next is 2911200.0000, 
raw observation next is [2.0, 95.33333333333334, 60.16666666666667, 51.83333333333333, 26.0, 25.58478147182302, 0.4462653556776039, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.518005540166205, 0.9533333333333335, 0.20055555555555557, 0.057274401473296495, 0.6666666666666666, 0.6320651226519184, 0.6487551185592013, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.0764721], dtype=float32), -0.22867075]. 
=============================================
[2019-04-04 03:59:26,315] A3C_AGENT_WORKER-Thread-5 INFO:Local step 260000, global step 4158542: loss 0.0639
[2019-04-04 03:59:26,315] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 260000, global step 4158542: learning rate 0.0001
[2019-04-04 03:59:28,299] A3C_AGENT_WORKER-Thread-14 INFO:Local step 260000, global step 4159296: loss 0.0549
[2019-04-04 03:59:28,299] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 260000, global step 4159296: learning rate 0.0001
[2019-04-04 03:59:29,889] A3C_AGENT_WORKER-Thread-15 INFO:Local step 260000, global step 4159920: loss 0.0604
[2019-04-04 03:59:29,889] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 260000, global step 4159920: learning rate 0.0001
[2019-04-04 03:59:31,689] A3C_AGENT_WORKER-Thread-3 INFO:Local step 260000, global step 4160547: loss 0.0444
[2019-04-04 03:59:31,704] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 260000, global step 4160547: learning rate 0.0001
[2019-04-04 03:59:31,811] A3C_AGENT_WORKER-Thread-9 INFO:Local step 260000, global step 4160587: loss 0.0516
[2019-04-04 03:59:31,813] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 260000, global step 4160588: learning rate 0.0001
[2019-04-04 03:59:32,059] A3C_AGENT_WORKER-Thread-6 INFO:Local step 260000, global step 4160667: loss 0.0439
[2019-04-04 03:59:32,060] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 260000, global step 4160667: learning rate 0.0001
[2019-04-04 03:59:33,030] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2317689e-28 9.7929301e-25 1.7018395e-26 6.7443520e-25 1.9375899e-24
 4.3464917e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:59:33,041] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8917
[2019-04-04 03:59:33,070] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 100.0, 0.0, 0.0, 26.0, 25.48563922086829, 0.2997385861561997, 0.0, 1.0, 23191.11797153286], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3121200.0000, 
sim time next is 3121800.0000, 
raw observation next is [2.1, 100.0, 0.0, 0.0, 26.0, 25.3606906563835, 0.2977899908975266, 0.0, 1.0, 99007.51993933818], 
processed observation next is [1.0, 0.13043478260869565, 0.5207756232686982, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6133908880319584, 0.5992633302991756, 0.0, 1.0, 0.47146438066351515], 
reward next is 0.5285, 
noisyNet noise sample is [array([1.0904768], dtype=float32), 0.156892]. 
=============================================
[2019-04-04 03:59:33,481] A3C_AGENT_WORKER-Thread-2 INFO:Local step 260000, global step 4161199: loss 0.0381
[2019-04-04 03:59:33,483] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 260000, global step 4161200: learning rate 0.0001
[2019-04-04 03:59:33,867] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.2566635e-28 1.3694419e-23 1.4419433e-26 8.9258546e-25 3.2062780e-24
 2.3032422e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:59:33,867] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8724
[2019-04-04 03:59:33,888] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 92.0, 0.0, 0.0, 26.0, 25.47446964913158, 0.3717866900861554, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3096000.0000, 
sim time next is 3096600.0000, 
raw observation next is [-1.0, 93.33333333333334, 0.0, 0.0, 26.0, 25.52111309391782, 0.3645129516836749, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.4349030470914128, 0.9333333333333335, 0.0, 0.0, 0.6666666666666666, 0.6267594244931516, 0.6215043172278917, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.7276427], dtype=float32), -0.08913413]. 
=============================================
[2019-04-04 03:59:34,948] A3C_AGENT_WORKER-Thread-4 INFO:Local step 260000, global step 4161843: loss 0.0507
[2019-04-04 03:59:34,948] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 260000, global step 4161843: learning rate 0.0001
[2019-04-04 03:59:34,975] A3C_AGENT_WORKER-Thread-19 INFO:Local step 260500, global step 4161855: loss 0.1734
[2019-04-04 03:59:34,976] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 260500, global step 4161855: learning rate 0.0001
[2019-04-04 03:59:36,128] A3C_AGENT_WORKER-Thread-7 INFO:Local step 260500, global step 4162322: loss 0.2073
[2019-04-04 03:59:36,137] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 260500, global step 4162322: learning rate 0.0001
[2019-04-04 03:59:37,126] A3C_AGENT_WORKER-Thread-20 INFO:Local step 260000, global step 4162726: loss 0.0357
[2019-04-04 03:59:37,128] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 260000, global step 4162727: learning rate 0.0001
[2019-04-04 03:59:37,212] A3C_AGENT_WORKER-Thread-10 INFO:Local step 260000, global step 4162764: loss 0.0344
[2019-04-04 03:59:37,213] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 260000, global step 4162765: learning rate 0.0001
[2019-04-04 03:59:37,427] A3C_AGENT_WORKER-Thread-17 INFO:Local step 260500, global step 4162857: loss 0.2283
[2019-04-04 03:59:37,427] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 260500, global step 4162857: learning rate 0.0001
[2019-04-04 03:59:37,513] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.5676468e-29 1.0137611e-24 1.6403309e-26 6.7776996e-25 1.0622950e-24
 1.6330875e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 03:59:37,513] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9413
[2019-04-04 03:59:37,528] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.333333333333333, 100.0, 0.0, 0.0, 26.0, 25.29639825323102, 0.5054039611662144, 0.0, 1.0, 59437.98741654179], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3212400.0000, 
sim time next is 3213000.0000, 
raw observation next is [-1.5, 100.0, 0.0, 0.0, 26.0, 25.28647435386438, 0.5063269993313723, 0.0, 1.0, 46900.32316017478], 
processed observation next is [1.0, 0.17391304347826086, 0.4210526315789474, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6072061961553651, 0.6687756664437908, 0.0, 1.0, 0.2233348721913085], 
reward next is 0.7767, 
noisyNet noise sample is [array([1.6617512], dtype=float32), 1.1828603]. 
=============================================
[2019-04-04 03:59:37,533] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[83.33775]
 [83.32221]
 [83.06859]
 [82.7855 ]
 [82.69761]], R is [[83.27133179]
 [83.15558624]
 [82.9063797 ]
 [82.58989716]
 [82.57376862]].
[2019-04-04 03:59:37,684] A3C_AGENT_WORKER-Thread-8 INFO:Local step 260000, global step 4162972: loss 0.0230
[2019-04-04 03:59:37,684] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 260000, global step 4162972: learning rate 0.0001
[2019-04-04 03:59:40,097] A3C_AGENT_WORKER-Thread-18 INFO:Local step 260000, global step 4164112: loss 0.0353
[2019-04-04 03:59:40,097] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 260000, global step 4164112: learning rate 0.0001
[2019-04-04 03:59:40,254] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0391773e-30 5.2237643e-28 2.7827984e-29 1.0588752e-26 4.3057484e-27
 4.0054683e-31 1.0000000e+00], sum to 1.0000
[2019-04-04 03:59:40,269] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3422
[2019-04-04 03:59:40,303] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.0, 100.0, 112.3333333333333, 811.6666666666666, 26.0, 27.31598668936194, 0.856802131636443, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3157800.0000, 
sim time next is 3158400.0000, 
raw observation next is [7.0, 100.0, 112.1666666666667, 808.8333333333334, 26.0, 27.33096829769233, 0.869076725621039, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.6565096952908588, 1.0, 0.373888888888889, 0.8937384898710866, 0.6666666666666666, 0.7775806914743608, 0.7896922418736797, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.41294366], dtype=float32), -0.8177761]. 
=============================================
[2019-04-04 03:59:42,213] A3C_AGENT_WORKER-Thread-16 INFO:Local step 260000, global step 4165022: loss 0.0289
[2019-04-04 03:59:42,214] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 260000, global step 4165022: learning rate 0.0001
[2019-04-04 03:59:44,207] A3C_AGENT_WORKER-Thread-5 INFO:Local step 260500, global step 4165928: loss 0.1008
[2019-04-04 03:59:44,209] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 260500, global step 4165929: learning rate 0.0001
[2019-04-04 03:59:44,785] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.30259896e-30 4.37587615e-26 1.44149092e-28 1.23926404e-26
 2.56278737e-26 5.12078855e-30 1.00000000e+00], sum to 1.0000
[2019-04-04 03:59:44,789] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5128
[2019-04-04 03:59:44,803] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.1666666666666666, 61.83333333333333, 102.3333333333333, 703.3333333333334, 26.0, 26.01452750198677, 0.5421911438660643, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3491400.0000, 
sim time next is 3492000.0000, 
raw observation next is [0.0, 60.0, 104.0, 720.0, 26.0, 26.13614660064907, 0.5618829339627793, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.46260387811634357, 0.6, 0.3466666666666667, 0.7955801104972375, 0.6666666666666666, 0.6780122167207558, 0.6872943113209264, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.12345922], dtype=float32), -1.008606]. 
=============================================
[2019-04-04 03:59:44,818] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[89.21972]
 [89.31029]
 [89.43521]
 [89.27554]
 [89.24059]], R is [[89.28842926]
 [89.39554596]
 [89.50159454]
 [89.60658264]
 [89.71051788]].
[2019-04-04 03:59:46,985] A3C_AGENT_WORKER-Thread-14 INFO:Local step 260500, global step 4167155: loss 0.0983
[2019-04-04 03:59:46,987] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 260500, global step 4167155: learning rate 0.0001
[2019-04-04 03:59:47,784] A3C_AGENT_WORKER-Thread-15 INFO:Local step 260500, global step 4167526: loss 0.0907
[2019-04-04 03:59:47,785] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 260500, global step 4167527: learning rate 0.0001
[2019-04-04 03:59:48,818] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.1853532e-28 3.3467952e-24 4.8198554e-27 3.0120982e-25 4.5150937e-25
 5.0254662e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 03:59:48,819] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.2270
[2019-04-04 03:59:48,857] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.166666666666667, 55.83333333333333, 112.0, 777.3333333333333, 26.0, 25.43067850607588, 0.4628231785046248, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3581400.0000, 
sim time next is 3582000.0000, 
raw observation next is [-4.0, 54.0, 112.5, 787.0, 26.0, 25.36320656070966, 0.4529625221767962, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.3518005540166205, 0.54, 0.375, 0.8696132596685083, 0.6666666666666666, 0.613600546725805, 0.6509875073922654, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.04594601], dtype=float32), 1.2428474]. 
=============================================
[2019-04-04 03:59:48,864] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[86.820564]
 [87.11895 ]
 [87.31126 ]
 [87.44535 ]
 [87.543816]], R is [[86.60710907]
 [86.74103546]
 [86.87362671]
 [87.00489044]
 [87.13484192]].
[2019-04-04 03:59:49,268] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.2534211e-29 5.5629470e-25 6.0451305e-27 1.2367664e-25 1.3542769e-24
 9.0855444e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 03:59:49,268] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2621
[2019-04-04 03:59:49,287] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.50999011415411, 0.536482076258521, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3358800.0000, 
sim time next is 3359400.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.59385864104139, 0.5330749416413166, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6328215534201158, 0.6776916472137722, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.757435], dtype=float32), 0.46788272]. 
=============================================
[2019-04-04 03:59:49,873] A3C_AGENT_WORKER-Thread-6 INFO:Local step 260500, global step 4168382: loss 0.0984
[2019-04-04 03:59:49,876] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 260500, global step 4168385: learning rate 0.0001
[2019-04-04 03:59:50,028] A3C_AGENT_WORKER-Thread-9 INFO:Local step 260500, global step 4168455: loss 0.0781
[2019-04-04 03:59:50,030] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 260500, global step 4168456: learning rate 0.0001
[2019-04-04 03:59:50,148] A3C_AGENT_WORKER-Thread-3 INFO:Local step 260500, global step 4168517: loss 0.0623
[2019-04-04 03:59:50,151] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 260500, global step 4168517: learning rate 0.0001
[2019-04-04 03:59:51,291] A3C_AGENT_WORKER-Thread-2 INFO:Local step 260500, global step 4169021: loss 0.1075
[2019-04-04 03:59:51,292] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 260500, global step 4169021: learning rate 0.0001
[2019-04-04 03:59:52,978] A3C_AGENT_WORKER-Thread-19 INFO:Local step 261000, global step 4169783: loss 14.1888
[2019-04-04 03:59:52,984] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 261000, global step 4169786: learning rate 0.0001
[2019-04-04 03:59:53,252] A3C_AGENT_WORKER-Thread-7 INFO:Local step 261000, global step 4169915: loss 14.0427
[2019-04-04 03:59:53,253] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 261000, global step 4169916: learning rate 0.0001
[2019-04-04 03:59:53,258] A3C_AGENT_WORKER-Thread-4 INFO:Local step 260500, global step 4169920: loss 0.0681
[2019-04-04 03:59:53,259] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 260500, global step 4169920: learning rate 0.0001
[2019-04-04 03:59:54,433] A3C_AGENT_WORKER-Thread-17 INFO:Local step 261000, global step 4170476: loss 14.4469
[2019-04-04 03:59:54,436] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 261000, global step 4170478: learning rate 0.0001
[2019-04-04 03:59:54,883] A3C_AGENT_WORKER-Thread-10 INFO:Local step 260500, global step 4170683: loss 0.1017
[2019-04-04 03:59:54,893] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 260500, global step 4170684: learning rate 0.0001
[2019-04-04 03:59:55,219] A3C_AGENT_WORKER-Thread-20 INFO:Local step 260500, global step 4170856: loss 0.0503
[2019-04-04 03:59:55,220] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 260500, global step 4170856: learning rate 0.0001
[2019-04-04 03:59:55,317] A3C_AGENT_WORKER-Thread-8 INFO:Local step 260500, global step 4170906: loss 0.0401
[2019-04-04 03:59:55,319] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 260500, global step 4170906: learning rate 0.0001
[2019-04-04 03:59:57,870] A3C_AGENT_WORKER-Thread-18 INFO:Local step 260500, global step 4172028: loss 0.0283
[2019-04-04 03:59:57,871] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 260500, global step 4172029: learning rate 0.0001
[2019-04-04 03:59:59,915] A3C_AGENT_WORKER-Thread-16 INFO:Local step 260500, global step 4173003: loss 0.0952
[2019-04-04 03:59:59,917] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 260500, global step 4173004: learning rate 0.0001
[2019-04-04 04:00:01,162] A3C_AGENT_WORKER-Thread-5 INFO:Local step 261000, global step 4173614: loss 14.0607
[2019-04-04 04:00:01,163] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 261000, global step 4173614: learning rate 0.0001
[2019-04-04 04:00:03,236] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1230491e-29 8.0858890e-26 9.4953805e-29 2.8868322e-26 3.2812078e-26
 3.9913788e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:00:03,236] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3726
[2019-04-04 04:00:03,253] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.0, 25.0, 0.0, 0.0, 26.0, 25.60078620067299, 0.3734109537857981, 0.0, 1.0, 18735.48893980787], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3631800.0000, 
sim time next is 3632400.0000, 
raw observation next is [9.0, 25.0, 0.0, 0.0, 26.0, 25.56355117718372, 0.3672650490254382, 0.0, 1.0, 37560.19093003786], 
processed observation next is [0.0, 0.043478260869565216, 0.7119113573407203, 0.25, 0.0, 0.0, 0.6666666666666666, 0.6302959314319766, 0.6224216830084793, 0.0, 1.0, 0.17885805204779934], 
reward next is 0.8211, 
noisyNet noise sample is [array([1.9910549], dtype=float32), -0.16084808]. 
=============================================
[2019-04-04 04:00:04,507] A3C_AGENT_WORKER-Thread-14 INFO:Local step 261000, global step 4175160: loss 14.2297
[2019-04-04 04:00:04,508] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 261000, global step 4175160: learning rate 0.0001
[2019-04-04 04:00:05,082] A3C_AGENT_WORKER-Thread-15 INFO:Local step 261000, global step 4175444: loss 14.0841
[2019-04-04 04:00:05,082] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 261000, global step 4175444: learning rate 0.0001
[2019-04-04 04:00:05,233] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.3002294e-28 8.1529847e-24 8.3715368e-27 8.9847835e-25 2.9762925e-24
 8.1776998e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:00:05,234] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5647
[2019-04-04 04:00:05,276] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.06397668177818, 0.3496051303539665, 0.0, 1.0, 41979.96632784978], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3610200.0000, 
sim time next is 3610800.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.11703279948885, 0.3475624633871848, 0.0, 1.0, 18708.4840392683], 
processed observation next is [0.0, 0.8260869565217391, 0.4349030470914128, 0.42, 0.0, 0.0, 0.6666666666666666, 0.5930860666240708, 0.6158541544623949, 0.0, 1.0, 0.08908801923461095], 
reward next is 0.9109, 
noisyNet noise sample is [array([0.20209731], dtype=float32), 1.4244846]. 
=============================================
[2019-04-04 04:00:06,655] A3C_AGENT_WORKER-Thread-6 INFO:Local step 261000, global step 4176256: loss 13.7465
[2019-04-04 04:00:06,660] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 261000, global step 4176258: learning rate 0.0001
[2019-04-04 04:00:06,665] A3C_AGENT_WORKER-Thread-3 INFO:Local step 261000, global step 4176262: loss 13.7687
[2019-04-04 04:00:06,665] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 261000, global step 4176262: learning rate 0.0001
[2019-04-04 04:00:07,617] A3C_AGENT_WORKER-Thread-9 INFO:Local step 261000, global step 4176705: loss 13.5455
[2019-04-04 04:00:07,620] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 261000, global step 4176706: learning rate 0.0001
[2019-04-04 04:00:08,514] A3C_AGENT_WORKER-Thread-2 INFO:Local step 261000, global step 4177114: loss 13.4618
[2019-04-04 04:00:08,515] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 261000, global step 4177114: learning rate 0.0001
[2019-04-04 04:00:08,601] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.8860432e-27 5.1027278e-24 1.3035372e-26 2.7096453e-24 1.2449987e-24
 5.5637237e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:00:08,604] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7485
[2019-04-04 04:00:08,642] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.12888205450825, 0.355689727156426, 0.0, 1.0, 18704.81231649502], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3608400.0000, 
sim time next is 3609000.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.10883962178169, 0.3476602795969421, 0.0, 1.0, 24777.55560123369], 
processed observation next is [0.0, 0.782608695652174, 0.4349030470914128, 0.42, 0.0, 0.0, 0.6666666666666666, 0.5924033018151409, 0.6158867598656473, 0.0, 1.0, 0.1179883600058747], 
reward next is 0.8820, 
noisyNet noise sample is [array([-1.1501303], dtype=float32), -0.36878002]. 
=============================================
[2019-04-04 04:00:08,655] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[83.18678]
 [82.98411]
 [82.65705]
 [82.5011 ]
 [83.41141]], R is [[83.27698517]
 [83.35514069]
 [83.43251801]
 [83.43404388]
 [83.44012451]].
[2019-04-04 04:00:08,862] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.8731295e-28 1.6897702e-24 8.4019295e-27 4.7178270e-25 1.0974676e-24
 2.8836484e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:00:08,862] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1212
[2019-04-04 04:00:08,875] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.0, 43.0, 108.5, 797.0, 26.0, 25.3234273494535, 0.4680431926033817, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3679200.0000, 
sim time next is 3679800.0000, 
raw observation next is [6.0, 43.66666666666667, 107.0, 790.0, 26.0, 25.37336021540277, 0.4694607535598136, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.6288088642659281, 0.4366666666666667, 0.3566666666666667, 0.8729281767955801, 0.6666666666666666, 0.6144466846168974, 0.6564869178532712, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0456733], dtype=float32), -0.7597423]. 
=============================================
[2019-04-04 04:00:09,418] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.29202996e-29 9.73664083e-26 9.39633101e-28 1.48325658e-25
 7.55377273e-26 1.21281496e-29 1.00000000e+00], sum to 1.0000
[2019-04-04 04:00:09,418] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9818
[2019-04-04 04:00:09,428] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.166666666666667, 66.0, 116.3333333333333, 824.3333333333334, 26.0, 26.49326282704165, 0.5943016467144335, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3757800.0000, 
sim time next is 3758400.0000, 
raw observation next is [-2.0, 65.0, 117.0, 825.5, 26.0, 26.47693814238087, 0.5986487725730729, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.40720221606648205, 0.65, 0.39, 0.9121546961325967, 0.6666666666666666, 0.7064115118650726, 0.699549590857691, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.14772674], dtype=float32), -0.42833638]. 
=============================================
[2019-04-04 04:00:10,132] A3C_AGENT_WORKER-Thread-4 INFO:Local step 261000, global step 4177879: loss 13.3858
[2019-04-04 04:00:10,133] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 261000, global step 4177879: learning rate 0.0001
[2019-04-04 04:00:10,170] A3C_AGENT_WORKER-Thread-19 INFO:Local step 261500, global step 4177896: loss 0.2279
[2019-04-04 04:00:10,170] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 261500, global step 4177896: learning rate 0.0001
[2019-04-04 04:00:10,403] A3C_AGENT_WORKER-Thread-7 INFO:Local step 261500, global step 4178000: loss 0.1763
[2019-04-04 04:00:10,404] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 261500, global step 4178000: learning rate 0.0001
[2019-04-04 04:00:10,925] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.4504925e-29 1.5242061e-24 9.2799548e-28 3.0847386e-26 5.1106733e-25
 1.1461482e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:00:10,926] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7449
[2019-04-04 04:00:10,961] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.5, 52.5, 118.0, 823.0, 26.0, 25.18007679361539, 0.4491950383833944, 0.0, 1.0, 18710.10697724189], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3587400.0000, 
sim time next is 3588000.0000, 
raw observation next is [-2.333333333333333, 51.66666666666666, 117.3333333333333, 821.1666666666667, 26.0, 25.18669180435771, 0.4511873066794712, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.3979686057248385, 0.5166666666666666, 0.391111111111111, 0.9073664825046042, 0.6666666666666666, 0.5988909836964759, 0.6503957688931571, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.973493], dtype=float32), 0.32486698]. 
=============================================
[2019-04-04 04:00:10,982] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[86.515495]
 [86.46195 ]
 [86.366356]
 [86.30299 ]
 [86.2613  ]], R is [[86.58592987]
 [86.63097382]
 [86.67556   ]
 [86.71970367]
 [86.76339722]].
[2019-04-04 04:00:11,238] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.49971406e-28 4.24064801e-25 1.05948126e-26 3.00491555e-24
 1.63127196e-24 1.55380284e-28 1.00000000e+00], sum to 1.0000
[2019-04-04 04:00:11,241] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7399
[2019-04-04 04:00:11,266] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 76.0, 0.0, 0.0, 26.0, 25.49697682943904, 0.5091684355768097, 0.0, 1.0, 71478.01482979127], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3793800.0000, 
sim time next is 3794400.0000, 
raw observation next is [-3.0, 77.0, 0.0, 0.0, 26.0, 25.45429081399476, 0.510920921717458, 0.0, 1.0, 76027.90069229504], 
processed observation next is [1.0, 0.9565217391304348, 0.3795013850415513, 0.77, 0.0, 0.0, 0.6666666666666666, 0.62119090116623, 0.6703069739058193, 0.0, 1.0, 0.36203762234426207], 
reward next is 0.6380, 
noisyNet noise sample is [array([0.01050687], dtype=float32), -1.3017753]. 
=============================================
[2019-04-04 04:00:11,602] A3C_AGENT_WORKER-Thread-17 INFO:Local step 261500, global step 4178628: loss 0.2275
[2019-04-04 04:00:11,604] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 261500, global step 4178629: learning rate 0.0001
[2019-04-04 04:00:11,768] A3C_AGENT_WORKER-Thread-10 INFO:Local step 261000, global step 4178708: loss 13.2009
[2019-04-04 04:00:11,772] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 261000, global step 4178708: learning rate 0.0001
[2019-04-04 04:00:12,367] A3C_AGENT_WORKER-Thread-20 INFO:Local step 261000, global step 4178975: loss 13.2763
[2019-04-04 04:00:12,368] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 261000, global step 4178975: learning rate 0.0001
[2019-04-04 04:00:12,643] A3C_AGENT_WORKER-Thread-8 INFO:Local step 261000, global step 4179093: loss 13.3593
[2019-04-04 04:00:12,643] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 261000, global step 4179093: learning rate 0.0001
[2019-04-04 04:00:15,010] A3C_AGENT_WORKER-Thread-18 INFO:Local step 261000, global step 4180269: loss 12.8602
[2019-04-04 04:00:15,012] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 261000, global step 4180271: learning rate 0.0001
[2019-04-04 04:00:16,253] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3655676e-28 2.8663371e-24 1.9858835e-26 9.6668840e-25 9.8960895e-25
 7.8502099e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:00:16,254] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8321
[2019-04-04 04:00:16,274] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.39937152118684, 0.378174844387545, 0.0, 1.0, 89377.67665274073], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3898800.0000, 
sim time next is 3899400.0000, 
raw observation next is [-2.166666666666667, 66.0, 0.0, 0.0, 26.0, 25.26837789253005, 0.394262127666376, 0.0, 1.0, 82587.37047570689], 
processed observation next is [1.0, 0.13043478260869565, 0.4025854108956602, 0.66, 0.0, 0.0, 0.6666666666666666, 0.6056981577108376, 0.6314207092221253, 0.0, 1.0, 0.3932731927414614], 
reward next is 0.6067, 
noisyNet noise sample is [array([0.86872894], dtype=float32), -0.44544083]. 
=============================================
[2019-04-04 04:00:16,703] A3C_AGENT_WORKER-Thread-16 INFO:Local step 261000, global step 4181133: loss 13.0455
[2019-04-04 04:00:16,704] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 261000, global step 4181133: learning rate 0.0001
[2019-04-04 04:00:17,790] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.1286830e-28 1.5314980e-23 1.1673700e-25 3.9450232e-24 3.7862290e-24
 3.3340983e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 04:00:17,792] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8904
[2019-04-04 04:00:17,809] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 38.5, 0.0, 0.0, 26.0, 25.12860856317331, 0.2982881565930677, 0.0, 1.0, 40661.04650984435], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4072200.0000, 
sim time next is 4072800.0000, 
raw observation next is [-5.0, 39.0, 0.0, 0.0, 26.0, 25.11652898864116, 0.2867724634682304, 0.0, 1.0, 40613.37803579748], 
processed observation next is [1.0, 0.13043478260869565, 0.32409972299168976, 0.39, 0.0, 0.0, 0.6666666666666666, 0.5930440823867634, 0.5955908211560769, 0.0, 1.0, 0.1933970382657023], 
reward next is 0.8066, 
noisyNet noise sample is [array([0.20534278], dtype=float32), 0.32264742]. 
=============================================
[2019-04-04 04:00:17,853] A3C_AGENT_WORKER-Thread-5 INFO:Local step 261500, global step 4181702: loss 0.0998
[2019-04-04 04:00:17,856] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 261500, global step 4181703: learning rate 0.0001
[2019-04-04 04:00:18,813] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.7301523e-28 1.4699199e-23 4.8522714e-26 4.1077340e-24 1.6302392e-24
 3.0286521e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 04:00:18,815] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4843
[2019-04-04 04:00:18,827] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 77.0, 0.0, 0.0, 26.0, 25.2667299339836, 0.4023148131331706, 0.0, 1.0, 44178.81176154229], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3806400.0000, 
sim time next is 3807000.0000, 
raw observation next is [-4.0, 77.0, 0.0, 0.0, 26.0, 25.30862239525782, 0.4026604670861376, 0.0, 1.0, 44162.28474514392], 
processed observation next is [1.0, 0.043478260869565216, 0.3518005540166205, 0.77, 0.0, 0.0, 0.6666666666666666, 0.6090518662714851, 0.6342201556953792, 0.0, 1.0, 0.21029659402449488], 
reward next is 0.7897, 
noisyNet noise sample is [array([-1.439448], dtype=float32), -0.2042131]. 
=============================================
[2019-04-04 04:00:18,835] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[80.89935 ]
 [80.792984]
 [80.67831 ]
 [80.64285 ]
 [80.6296  ]], R is [[81.00764465]
 [80.98719788]
 [80.9670639 ]
 [80.9475174 ]
 [80.92871094]].
[2019-04-04 04:00:20,809] A3C_AGENT_WORKER-Thread-14 INFO:Local step 261500, global step 4183093: loss 0.0897
[2019-04-04 04:00:20,810] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 261500, global step 4183093: learning rate 0.0001
[2019-04-04 04:00:20,854] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.7646664e-30 4.7496243e-26 1.8555060e-28 2.7968613e-26 5.2225945e-26
 5.3782628e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 04:00:20,871] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5589
[2019-04-04 04:00:20,896] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.0, 40.0, 115.5, 798.5, 26.0, 26.56591163649881, 0.5814053621988381, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4014000.0000, 
sim time next is 4014600.0000, 
raw observation next is [-7.666666666666667, 39.5, 116.6666666666667, 804.3333333333334, 26.0, 26.49550303229624, 0.5812792328548869, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.2502308402585411, 0.395, 0.388888888888889, 0.8887661141804789, 0.6666666666666666, 0.7079585860246868, 0.6937597442849622, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.47082868], dtype=float32), 1.8501661]. 
=============================================
[2019-04-04 04:00:21,650] A3C_AGENT_WORKER-Thread-15 INFO:Local step 261500, global step 4183504: loss 0.1366
[2019-04-04 04:00:21,650] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 261500, global step 4183504: learning rate 0.0001
[2019-04-04 04:00:22,219] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.9790289e-29 1.1248699e-24 4.0245754e-27 2.8700668e-25 4.2110194e-25
 2.7658985e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:00:22,223] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5412
[2019-04-04 04:00:22,243] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 60.0, 0.0, 0.0, 26.0, 25.46565745605039, 0.5660325337742216, 0.0, 1.0, 18759.25425434499], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3877200.0000, 
sim time next is 3877800.0000, 
raw observation next is [-1.0, 59.16666666666666, 0.0, 0.0, 26.0, 25.63943777345906, 0.5748000150630587, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.4349030470914128, 0.5916666666666666, 0.0, 0.0, 0.6666666666666666, 0.6366198144549218, 0.6916000050210195, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.469428], dtype=float32), -1.6688436]. 
=============================================
[2019-04-04 04:00:22,681] A3C_AGENT_WORKER-Thread-6 INFO:Local step 261500, global step 4183987: loss 0.1031
[2019-04-04 04:00:22,681] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 261500, global step 4183987: learning rate 0.0001
[2019-04-04 04:00:22,892] A3C_AGENT_WORKER-Thread-3 INFO:Local step 261500, global step 4184089: loss 0.0643
[2019-04-04 04:00:22,893] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 261500, global step 4184089: learning rate 0.0001
[2019-04-04 04:00:24,586] A3C_AGENT_WORKER-Thread-9 INFO:Local step 261500, global step 4184834: loss 0.0840
[2019-04-04 04:00:24,588] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 261500, global step 4184834: learning rate 0.0001
[2019-04-04 04:00:24,851] A3C_AGENT_WORKER-Thread-2 INFO:Local step 261500, global step 4184949: loss 0.0920
[2019-04-04 04:00:24,851] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 261500, global step 4184949: learning rate 0.0001
[2019-04-04 04:00:26,387] A3C_AGENT_WORKER-Thread-4 INFO:Local step 261500, global step 4185613: loss 0.0859
[2019-04-04 04:00:26,387] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 261500, global step 4185613: learning rate 0.0001
[2019-04-04 04:00:27,665] A3C_AGENT_WORKER-Thread-7 INFO:Local step 262000, global step 4186203: loss 0.4835
[2019-04-04 04:00:27,666] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 262000, global step 4186203: learning rate 0.0001
[2019-04-04 04:00:27,723] A3C_AGENT_WORKER-Thread-19 INFO:Local step 262000, global step 4186232: loss 0.4930
[2019-04-04 04:00:27,725] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 262000, global step 4186234: learning rate 0.0001
[2019-04-04 04:00:27,936] A3C_AGENT_WORKER-Thread-10 INFO:Local step 261500, global step 4186323: loss 0.0677
[2019-04-04 04:00:27,937] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 261500, global step 4186323: learning rate 0.0001
[2019-04-04 04:00:28,657] A3C_AGENT_WORKER-Thread-20 INFO:Local step 261500, global step 4186664: loss 0.0828
[2019-04-04 04:00:28,658] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 261500, global step 4186664: learning rate 0.0001
[2019-04-04 04:00:28,680] A3C_AGENT_WORKER-Thread-17 INFO:Local step 262000, global step 4186675: loss 0.5290
[2019-04-04 04:00:28,681] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 262000, global step 4186675: learning rate 0.0001
[2019-04-04 04:00:29,235] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.5081630e-28 1.1914677e-23 3.3366320e-26 1.4622292e-24 1.0667259e-23
 1.7694568e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 04:00:29,238] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1418
[2019-04-04 04:00:29,266] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.15, 73.0, 0.0, 0.0, 26.0, 25.81289900825547, 0.4599606856444589, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4308600.0000, 
sim time next is 4309200.0000, 
raw observation next is [5.1, 73.0, 0.0, 0.0, 26.0, 25.78695217962955, 0.4490501155051628, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.9130434782608695, 0.6038781163434903, 0.73, 0.0, 0.0, 0.6666666666666666, 0.648912681635796, 0.6496833718350542, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6842447], dtype=float32), 1.5748888]. 
=============================================
[2019-04-04 04:00:29,593] A3C_AGENT_WORKER-Thread-8 INFO:Local step 261500, global step 4187122: loss 0.0404
[2019-04-04 04:00:29,594] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 261500, global step 4187122: learning rate 0.0001
[2019-04-04 04:00:31,590] A3C_AGENT_WORKER-Thread-18 INFO:Local step 261500, global step 4188043: loss 0.0804
[2019-04-04 04:00:31,591] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 261500, global step 4188043: learning rate 0.0001
[2019-04-04 04:00:32,876] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.3775806e-28 7.4048235e-24 1.7024306e-26 3.5969052e-25 1.8621184e-24
 7.8750144e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:00:32,877] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2247
[2019-04-04 04:00:32,899] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 36.0, 0.0, 0.0, 26.0, 24.82899811774097, 0.2207144359255359, 0.0, 1.0, 40190.81714542563], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4080600.0000, 
sim time next is 4081200.0000, 
raw observation next is [-4.0, 36.66666666666666, 0.0, 0.0, 26.0, 24.81842458596958, 0.2128651160840288, 0.0, 1.0, 40176.57983001737], 
processed observation next is [1.0, 0.21739130434782608, 0.3518005540166205, 0.3666666666666666, 0.0, 0.0, 0.6666666666666666, 0.5682020488307984, 0.5709550386946763, 0.0, 1.0, 0.1913170468096065], 
reward next is 0.8087, 
noisyNet noise sample is [array([0.3177416], dtype=float32), -1.8991108]. 
=============================================
[2019-04-04 04:00:33,206] A3C_AGENT_WORKER-Thread-16 INFO:Local step 261500, global step 4188843: loss 0.0737
[2019-04-04 04:00:33,208] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 261500, global step 4188845: learning rate 0.0001
[2019-04-04 04:00:34,665] A3C_AGENT_WORKER-Thread-5 INFO:Local step 262000, global step 4189584: loss 0.5783
[2019-04-04 04:00:34,668] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 262000, global step 4189584: learning rate 0.0001
[2019-04-04 04:00:38,001] A3C_AGENT_WORKER-Thread-14 INFO:Local step 262000, global step 4191275: loss 0.7839
[2019-04-04 04:00:38,002] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 262000, global step 4191276: learning rate 0.0001
[2019-04-04 04:00:38,741] A3C_AGENT_WORKER-Thread-15 INFO:Local step 262000, global step 4191646: loss 0.8050
[2019-04-04 04:00:38,744] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 262000, global step 4191647: learning rate 0.0001
[2019-04-04 04:00:39,399] A3C_AGENT_WORKER-Thread-3 INFO:Local step 262000, global step 4191990: loss 0.8431
[2019-04-04 04:00:39,400] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 262000, global step 4191990: learning rate 0.0001
[2019-04-04 04:00:39,976] A3C_AGENT_WORKER-Thread-6 INFO:Local step 262000, global step 4192309: loss 0.8197
[2019-04-04 04:00:39,979] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 262000, global step 4192309: learning rate 0.0001
[2019-04-04 04:00:41,814] A3C_AGENT_WORKER-Thread-7 INFO:Local step 262500, global step 4193303: loss 0.0008
[2019-04-04 04:00:41,815] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 262500, global step 4193303: learning rate 0.0001
[2019-04-04 04:00:41,927] A3C_AGENT_WORKER-Thread-9 INFO:Local step 262000, global step 4193363: loss 0.8737
[2019-04-04 04:00:41,931] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 262000, global step 4193364: learning rate 0.0001
[2019-04-04 04:00:41,940] A3C_AGENT_WORKER-Thread-2 INFO:Local step 262000, global step 4193371: loss 0.8414
[2019-04-04 04:00:41,940] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 262000, global step 4193371: learning rate 0.0001
[2019-04-04 04:00:42,481] A3C_AGENT_WORKER-Thread-19 INFO:Local step 262500, global step 4193680: loss 0.0046
[2019-04-04 04:00:42,482] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 262500, global step 4193680: learning rate 0.0001
[2019-04-04 04:00:43,351] A3C_AGENT_WORKER-Thread-4 INFO:Local step 262000, global step 4194159: loss 0.8356
[2019-04-04 04:00:43,352] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 262000, global step 4194160: learning rate 0.0001
[2019-04-04 04:00:43,397] A3C_AGENT_WORKER-Thread-17 INFO:Local step 262500, global step 4194181: loss 0.0195
[2019-04-04 04:00:43,400] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 262500, global step 4194181: learning rate 0.0001
[2019-04-04 04:00:44,440] A3C_AGENT_WORKER-Thread-10 INFO:Local step 262000, global step 4194768: loss 0.7488
[2019-04-04 04:00:44,442] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 262000, global step 4194768: learning rate 0.0001
[2019-04-04 04:00:45,572] A3C_AGENT_WORKER-Thread-20 INFO:Local step 262000, global step 4195453: loss 0.6227
[2019-04-04 04:00:45,577] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 262000, global step 4195453: learning rate 0.0001
[2019-04-04 04:00:46,316] A3C_AGENT_WORKER-Thread-8 INFO:Local step 262000, global step 4195908: loss 0.5339
[2019-04-04 04:00:46,318] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 262000, global step 4195908: learning rate 0.0001
[2019-04-04 04:00:48,314] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.8807020e-28 5.5422286e-26 3.6340153e-27 8.8907513e-25 5.4797316e-25
 6.8806122e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:00:48,315] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9751
[2019-04-04 04:00:48,357] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 52.0, 18.66666666666667, 18.66666666666667, 26.0, 25.56865384433846, 0.345527051538691, 1.0, 1.0, 28310.95432949887], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4557000.0000, 
sim time next is 4557600.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 25.53289610481542, 0.451113378670124, 1.0, 1.0, 19875.61658156659], 
processed observation next is [1.0, 0.782608695652174, 0.518005540166205, 0.52, 0.0, 0.0, 0.6666666666666666, 0.6277413420679517, 0.6503711262233747, 1.0, 1.0, 0.0946457932455552], 
reward next is 0.9054, 
noisyNet noise sample is [array([-0.05861117], dtype=float32), 0.9063321]. 
=============================================
[2019-04-04 04:00:48,364] A3C_AGENT_WORKER-Thread-18 INFO:Local step 262000, global step 4196851: loss 0.4592
[2019-04-04 04:00:48,364] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 262000, global step 4196851: learning rate 0.0001
[2019-04-04 04:00:48,657] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.1664626e-29 1.9690777e-25 5.2117652e-27 2.3661178e-25 9.4488606e-25
 2.7523092e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:00:48,660] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5612
[2019-04-04 04:00:48,684] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.0, 36.0, 49.66666666666666, 0.0, 26.0, 27.9090928109323, 1.052176595408906, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4378800.0000, 
sim time next is 4379400.0000, 
raw observation next is [13.0, 36.5, 39.0, 0.0, 26.0, 28.25508368328948, 1.078321659857876, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.8227146814404434, 0.365, 0.13, 0.0, 0.6666666666666666, 0.8545903069407901, 0.8594405532859586, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5941246], dtype=float32), 1.5616418]. 
=============================================
[2019-04-04 04:00:49,686] A3C_AGENT_WORKER-Thread-5 INFO:Local step 262500, global step 4197273: loss 0.0048
[2019-04-04 04:00:49,686] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 262500, global step 4197273: learning rate 0.0001
[2019-04-04 04:00:51,686] A3C_AGENT_WORKER-Thread-16 INFO:Local step 262000, global step 4197833: loss 0.4555
[2019-04-04 04:00:51,687] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 262000, global step 4197833: learning rate 0.0001
[2019-04-04 04:00:55,610] A3C_AGENT_WORKER-Thread-14 INFO:Local step 262500, global step 4198924: loss 0.0026
[2019-04-04 04:00:55,637] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 262500, global step 4198924: learning rate 0.0001
[2019-04-04 04:00:56,507] A3C_AGENT_WORKER-Thread-15 INFO:Local step 262500, global step 4199195: loss 0.0222
[2019-04-04 04:00:56,520] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 262500, global step 4199195: learning rate 0.0001
[2019-04-04 04:00:57,595] A3C_AGENT_WORKER-Thread-3 INFO:Local step 262500, global step 4199542: loss 0.0125
[2019-04-04 04:00:57,595] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 262500, global step 4199542: learning rate 0.0001
[2019-04-04 04:00:58,524] A3C_AGENT_WORKER-Thread-6 INFO:Local step 262500, global step 4199835: loss 0.0037
[2019-04-04 04:00:58,524] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 262500, global step 4199835: learning rate 0.0001
[2019-04-04 04:00:59,130] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-04 04:00:59,149] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 04:00:59,150] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:00:59,152] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run43
[2019-04-04 04:00:59,194] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 04:00:59,195] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:00:59,197] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run43
[2019-04-04 04:00:59,256] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 04:00:59,257] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:00:59,269] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run43
[2019-04-04 04:03:42,349] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.36434537], dtype=float32), 0.21589541]
[2019-04-04 04:03:42,349] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [0.1, 60.0, 80.5, 586.5, 26.0, 25.3885466511102, 0.4155883163272809, 1.0, 1.0, 0.0]
[2019-04-04 04:03:42,349] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 04:03:42,350] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [2.8265443e-30 8.1066126e-26 8.3940405e-29 1.1128541e-26 1.3636535e-26
 1.2035053e-29 1.0000000e+00], sampled 0.8571027389276082
[2019-04-04 04:04:11,293] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 04:04:41,352] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 04:04:44,340] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 04:04:45,369] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 4200000, evaluation results [4200000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 04:04:45,661] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.2227139e-30 7.2938503e-27 3.2562023e-29 3.1851590e-27 4.3218423e-27
 4.2418649e-31 1.0000000e+00], sum to 1.0000
[2019-04-04 04:04:45,662] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9143
[2019-04-04 04:04:45,749] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.66666666666667, 28.83333333333334, 117.0, 849.3333333333334, 26.0, 27.65435093386403, 0.9486084692514133, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4366200.0000, 
sim time next is 4366800.0000, 
raw observation next is [14.6, 29.0, 116.5, 847.5, 26.0, 26.80021988443928, 0.8912619638857318, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.8670360110803325, 0.29, 0.3883333333333333, 0.93646408839779, 0.6666666666666666, 0.7333516570366067, 0.797087321295244, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2898167], dtype=float32), 0.76077914]. 
=============================================
[2019-04-04 04:04:47,520] A3C_AGENT_WORKER-Thread-2 INFO:Local step 262500, global step 4200712: loss 0.0014
[2019-04-04 04:04:47,520] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 262500, global step 4200712: learning rate 0.0001
[2019-04-04 04:04:48,558] A3C_AGENT_WORKER-Thread-9 INFO:Local step 262500, global step 4201067: loss 0.0013
[2019-04-04 04:04:48,562] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 262500, global step 4201068: learning rate 0.0001
[2019-04-04 04:04:50,367] A3C_AGENT_WORKER-Thread-7 INFO:Local step 263000, global step 4201600: loss 0.2688
[2019-04-04 04:04:50,369] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 263000, global step 4201600: learning rate 0.0001
[2019-04-04 04:04:51,747] A3C_AGENT_WORKER-Thread-4 INFO:Local step 262500, global step 4202019: loss 0.0034
[2019-04-04 04:04:51,750] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 262500, global step 4202021: learning rate 0.0001
[2019-04-04 04:04:51,940] A3C_AGENT_WORKER-Thread-19 INFO:Local step 263000, global step 4202068: loss 0.2902
[2019-04-04 04:04:51,942] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 263000, global step 4202068: learning rate 0.0001
[2019-04-04 04:04:53,119] A3C_AGENT_WORKER-Thread-10 INFO:Local step 262500, global step 4202437: loss 0.0043
[2019-04-04 04:04:53,121] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 262500, global step 4202437: learning rate 0.0001
[2019-04-04 04:04:53,839] A3C_AGENT_WORKER-Thread-17 INFO:Local step 263000, global step 4202699: loss 0.2225
[2019-04-04 04:04:53,839] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 263000, global step 4202699: learning rate 0.0001
[2019-04-04 04:04:54,721] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [7.2642153e-29 4.6819166e-25 3.0873385e-27 5.7910407e-25 1.9997963e-25
 8.5393128e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:04:54,727] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0548
[2019-04-04 04:04:54,757] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.666666666666667, 52.66666666666667, 0.0, 0.0, 26.0, 26.05518100750793, 0.6233009045685032, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4648800.0000, 
sim time next is 4649400.0000, 
raw observation next is [2.5, 52.5, 0.0, 0.0, 26.0, 25.94221655922227, 0.6045156069428929, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.5318559556786704, 0.525, 0.0, 0.0, 0.6666666666666666, 0.6618513799351892, 0.7015052023142977, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3730009], dtype=float32), 0.28909343]. 
=============================================
[2019-04-04 04:04:55,329] A3C_AGENT_WORKER-Thread-20 INFO:Local step 262500, global step 4203207: loss 0.0051
[2019-04-04 04:04:55,335] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 262500, global step 4203208: learning rate 0.0001
[2019-04-04 04:04:56,783] A3C_AGENT_WORKER-Thread-8 INFO:Local step 262500, global step 4203668: loss 0.0038
[2019-04-04 04:04:56,786] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 262500, global step 4203668: learning rate 0.0001
[2019-04-04 04:04:59,982] A3C_AGENT_WORKER-Thread-18 INFO:Local step 262500, global step 4204599: loss 0.0287
[2019-04-04 04:05:00,001] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 262500, global step 4204599: learning rate 0.0001
[2019-04-04 04:05:02,491] A3C_AGENT_WORKER-Thread-16 INFO:Local step 262500, global step 4205450: loss 0.0580
[2019-04-04 04:05:02,498] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 262500, global step 4205450: learning rate 0.0001
[2019-04-04 04:05:02,797] A3C_AGENT_WORKER-Thread-5 INFO:Local step 263000, global step 4205540: loss 0.1196
[2019-04-04 04:05:02,798] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 263000, global step 4205540: learning rate 0.0001
[2019-04-04 04:05:03,236] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4055182e-27 1.2224895e-23 3.4429551e-26 6.5135050e-24 1.9995455e-24
 3.4856247e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 04:05:03,237] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4001
[2019-04-04 04:05:03,264] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 37.0, 97.0, 727.0, 26.0, 25.18501721192654, 0.4432620315263363, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4807800.0000, 
sim time next is 4808400.0000, 
raw observation next is [3.0, 37.0, 94.5, 697.3333333333334, 26.0, 25.18534690642576, 0.4426012744189542, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.5457063711911359, 0.37, 0.315, 0.7705340699815838, 0.6666666666666666, 0.5987789088688134, 0.6475337581396514, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.32321188], dtype=float32), 0.60900813]. 
=============================================
[2019-04-04 04:05:07,113] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.2450957e-28 2.6026912e-24 1.0762765e-26 8.5062329e-25 1.2309635e-24
 4.9522741e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:05:07,113] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5966
[2019-04-04 04:05:07,121] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.333333333333333, 39.0, 211.6666666666667, 604.5, 26.0, 25.16945125145121, 0.4348369194666806, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4800000.0000, 
sim time next is 4800600.0000, 
raw observation next is [2.5, 38.5, 220.0, 568.0, 26.0, 25.15466387951673, 0.4329663060762329, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.5318559556786704, 0.385, 0.7333333333333333, 0.6276243093922652, 0.6666666666666666, 0.5962219899597274, 0.644322102025411, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.44476563], dtype=float32), -0.022199506]. 
=============================================
[2019-04-04 04:05:07,642] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4060852e-29 4.6378273e-27 2.0526049e-28 5.2548885e-26 7.4739468e-26
 5.2341517e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 04:05:07,643] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1009
[2019-04-04 04:05:07,654] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.0, 20.0, 114.5, 839.5, 26.0, 27.52131170307823, 0.9527841666404541, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5061600.0000, 
sim time next is 5062200.0000, 
raw observation next is [11.16666666666667, 19.83333333333334, 113.3333333333333, 832.6666666666667, 26.0, 27.89055155940179, 0.9952708416659256, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.7719298245614037, 0.1983333333333334, 0.37777777777777766, 0.9200736648250462, 0.6666666666666666, 0.8242126299501491, 0.8317569472219751, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2144381], dtype=float32), -1.2456784]. 
=============================================
[2019-04-04 04:05:07,734] A3C_AGENT_WORKER-Thread-14 INFO:Local step 263000, global step 4207044: loss 0.0963
[2019-04-04 04:05:07,735] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 263000, global step 4207044: learning rate 0.0001
[2019-04-04 04:05:09,107] A3C_AGENT_WORKER-Thread-15 INFO:Local step 263000, global step 4207561: loss 0.0904
[2019-04-04 04:05:09,108] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 263000, global step 4207561: learning rate 0.0001
[2019-04-04 04:05:09,424] A3C_AGENT_WORKER-Thread-3 INFO:Local step 263000, global step 4207666: loss 0.0844
[2019-04-04 04:05:09,425] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 263000, global step 4207666: learning rate 0.0001
[2019-04-04 04:05:09,690] A3C_AGENT_WORKER-Thread-6 INFO:Local step 263000, global step 4207751: loss 0.0907
[2019-04-04 04:05:09,690] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 263000, global step 4207751: learning rate 0.0001
[2019-04-04 04:05:09,875] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:05:09,875] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:05:09,887] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run32
[2019-04-04 04:05:10,701] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:05:10,701] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:05:10,706] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run32
[2019-04-04 04:05:12,296] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:05:12,297] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:05:12,300] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run32
[2019-04-04 04:05:12,398] A3C_AGENT_WORKER-Thread-2 INFO:Local step 263000, global step 4208478: loss 0.0830
[2019-04-04 04:05:12,400] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 263000, global step 4208478: learning rate 0.0001
[2019-04-04 04:05:14,155] A3C_AGENT_WORKER-Thread-9 INFO:Local step 263000, global step 4208854: loss 0.0798
[2019-04-04 04:05:14,156] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 263000, global step 4208854: learning rate 0.0001
[2019-04-04 04:05:17,402] A3C_AGENT_WORKER-Thread-4 INFO:Local step 263000, global step 4209718: loss 0.0791
[2019-04-04 04:05:17,402] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 263000, global step 4209718: learning rate 0.0001
[2019-04-04 04:05:18,350] A3C_AGENT_WORKER-Thread-10 INFO:Local step 263000, global step 4210099: loss 0.0772
[2019-04-04 04:05:18,351] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 263000, global step 4210100: learning rate 0.0001
[2019-04-04 04:05:20,242] A3C_AGENT_WORKER-Thread-8 INFO:Local step 263000, global step 4210875: loss 0.0761
[2019-04-04 04:05:20,243] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 263000, global step 4210875: learning rate 0.0001
[2019-04-04 04:05:20,404] A3C_AGENT_WORKER-Thread-20 INFO:Local step 263000, global step 4210938: loss 0.0745
[2019-04-04 04:05:20,404] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 263000, global step 4210938: learning rate 0.0001
[2019-04-04 04:05:21,662] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:05:21,662] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:05:21,665] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run32
[2019-04-04 04:05:23,379] A3C_AGENT_WORKER-Thread-18 INFO:Local step 263000, global step 4212049: loss 0.0725
[2019-04-04 04:05:23,385] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 263000, global step 4212049: learning rate 0.0001
[2019-04-04 04:05:23,923] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:05:23,923] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:05:23,933] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run32
[2019-04-04 04:05:25,089] A3C_AGENT_WORKER-Thread-16 INFO:Local step 263000, global step 4212725: loss 0.0709
[2019-04-04 04:05:25,093] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 263000, global step 4212725: learning rate 0.0001
[2019-04-04 04:05:25,724] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:05:25,725] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:05:25,728] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run32
[2019-04-04 04:05:25,865] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:05:25,866] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:05:25,873] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run32
[2019-04-04 04:05:26,225] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:05:26,225] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:05:26,231] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run32
[2019-04-04 04:05:27,641] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.0004350e-28 1.9859461e-24 1.1470898e-26 4.5937777e-25 9.0453789e-25
 8.0498725e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:05:27,642] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4913
[2019-04-04 04:05:27,685] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 44.66666666666667, 0.0, 0.0, 26.0, 25.08355786655643, 0.32238127028083, 0.0, 1.0, 199026.9581515002], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4908000.0000, 
sim time next is 4908600.0000, 
raw observation next is [1.0, 43.5, 0.0, 0.0, 26.0, 25.12713939175858, 0.3479835325957756, 0.0, 1.0, 152652.7670043305], 
processed observation next is [0.0, 0.8260869565217391, 0.4903047091412743, 0.435, 0.0, 0.0, 0.6666666666666666, 0.5939282826465483, 0.6159945108652586, 0.0, 1.0, 0.7269179381158595], 
reward next is 0.2731, 
noisyNet noise sample is [array([-0.65569293], dtype=float32), -1.0835838]. 
=============================================
[2019-04-04 04:05:28,129] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:05:28,129] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:05:28,132] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run32
[2019-04-04 04:05:30,145] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:05:30,145] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:05:30,148] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run32
[2019-04-04 04:05:33,343] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:05:33,343] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:05:33,351] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run32
[2019-04-04 04:05:34,193] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:05:34,193] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:05:34,199] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run32
[2019-04-04 04:05:36,201] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:05:36,201] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:05:36,208] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run32
[2019-04-04 04:05:37,145] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:05:37,145] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:05:37,186] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run32
[2019-04-04 04:05:40,023] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:05:40,024] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:05:40,027] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run32
[2019-04-04 04:05:43,466] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.9225111e-27 4.1732354e-23 8.9501952e-26 2.7664528e-24 8.7656480e-24
 4.5563030e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 04:05:43,466] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3266
[2019-04-04 04:05:43,473] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:05:43,473] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:05:43,476] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run32
[2019-04-04 04:05:43,513] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.516666666666667, 88.33333333333334, 0.0, 0.0, 26.0, 24.76193669398639, 0.2398501884180917, 0.0, 1.0, 41556.10709449992], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 72600.0000, 
sim time next is 73200.0000, 
raw observation next is [2.333333333333333, 87.66666666666667, 0.0, 0.0, 26.0, 24.74847378207282, 0.2366040340322584, 0.0, 1.0, 41315.55032488138], 
processed observation next is [0.0, 0.8695652173913043, 0.5272391505078486, 0.8766666666666667, 0.0, 0.0, 0.6666666666666666, 0.5623728151727351, 0.5788680113440862, 0.0, 1.0, 0.19674071583276845], 
reward next is 0.8033, 
noisyNet noise sample is [array([-0.07996411], dtype=float32), 1.8350652]. 
=============================================
[2019-04-04 04:05:46,312] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.9450226e-27 3.5663859e-23 4.3807719e-25 3.2081515e-23 2.1234618e-23
 4.5626189e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 04:05:46,313] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3648
[2019-04-04 04:05:46,364] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.2, 86.0, 64.5, 0.0, 26.0, 24.472653285526, 0.1620706657024818, 0.0, 1.0, 46211.44080813207], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 54000.0000, 
sim time next is 54600.0000, 
raw observation next is [7.1, 85.33333333333334, 59.66666666666666, 0.0, 26.0, 24.45557447856088, 0.1660952768867109, 0.0, 1.0, 53856.95906287082], 
processed observation next is [0.0, 0.6521739130434783, 0.6592797783933518, 0.8533333333333334, 0.19888888888888887, 0.0, 0.6666666666666666, 0.5379645398800733, 0.5553650922955703, 0.0, 1.0, 0.2564617098231944], 
reward next is 0.7435, 
noisyNet noise sample is [array([0.5544078], dtype=float32), -0.84850204]. 
=============================================
[2019-04-04 04:05:50,224] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.5420788e-27 2.8693799e-23 3.7739845e-25 4.0782266e-23 5.2639855e-23
 6.9271817e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 04:05:50,224] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4563
[2019-04-04 04:05:50,277] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.4, 70.5, 0.0, 0.0, 26.0, 24.34317931302077, 0.1537389125694532, 0.0, 1.0, 45709.14642020054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 165000.0000, 
sim time next is 165600.0000, 
raw observation next is [-8.4, 71.0, 0.0, 0.0, 26.0, 24.29287266623632, 0.1440526799793897, 0.0, 1.0, 45292.55173545384], 
processed observation next is [1.0, 0.9565217391304348, 0.2299168975069252, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5244060555196933, 0.54801755999313, 0.0, 1.0, 0.21567881778787543], 
reward next is 0.7843, 
noisyNet noise sample is [array([-0.6320629], dtype=float32), 0.58557343]. 
=============================================
[2019-04-04 04:05:53,484] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.50576356e-27 2.49097377e-23 1.19385507e-25 1.01846345e-23
 1.90274733e-23 3.42102597e-26 1.00000000e+00], sum to 1.0000
[2019-04-04 04:05:53,485] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7106
[2019-04-04 04:05:53,547] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.25, 87.5, 0.0, 0.0, 26.0, 24.64453812820157, 0.2319213816388229, 0.0, 1.0, 75792.94237259592], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 70200.0000, 
sim time next is 70800.0000, 
raw observation next is [3.066666666666667, 88.0, 0.0, 0.0, 26.0, 24.68501333288506, 0.2396993119939093, 0.0, 1.0, 53056.50876382782], 
processed observation next is [0.0, 0.8260869565217391, 0.5475530932594646, 0.88, 0.0, 0.0, 0.6666666666666666, 0.5570844444070883, 0.5798997706646364, 0.0, 1.0, 0.25265004173251343], 
reward next is 0.7473, 
noisyNet noise sample is [array([-0.79008096], dtype=float32), -1.3640403]. 
=============================================
[2019-04-04 04:06:01,424] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.7519193e-28 2.0500266e-24 8.2480384e-27 1.3809244e-24 1.1878320e-24
 9.0832615e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:06:01,424] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6512
[2019-04-04 04:06:01,513] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.483333333333333, 75.5, 94.0, 0.0, 26.0, 25.39066981908058, 0.211145755858431, 1.0, 1.0, 18738.8136536837], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 208200.0000, 
sim time next is 208800.0000, 
raw observation next is [-7.3, 75.0, 101.5, 0.0, 26.0, 25.3365173608052, 0.2127622456632948, 1.0, 1.0, 18734.92729576059], 
processed observation next is [1.0, 0.43478260869565216, 0.26038781163434904, 0.75, 0.3383333333333333, 0.0, 0.6666666666666666, 0.6113764467337667, 0.5709207485544315, 1.0, 1.0, 0.08921393950362186], 
reward next is 0.9108, 
noisyNet noise sample is [array([-1.8030158], dtype=float32), -0.46050632]. 
=============================================
[2019-04-04 04:06:03,620] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.3716176e-28 6.3448536e-24 3.8200281e-26 5.8996777e-24 5.5398881e-24
 4.3071613e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 04:06:03,621] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6926
[2019-04-04 04:06:03,646] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.733333333333334, 73.0, 0.0, 0.0, 26.0, 23.77890500024309, 0.009321873032790676, 0.0, 1.0, 44402.24777808877], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 175200.0000, 
sim time next is 175800.0000, 
raw observation next is [-8.816666666666666, 73.5, 0.0, 0.0, 26.0, 23.73807492215903, 0.002853394037237481, 0.0, 1.0, 44402.62783155789], 
processed observation next is [1.0, 0.0, 0.21837488457987075, 0.735, 0.0, 0.0, 0.6666666666666666, 0.4781729101799191, 0.5009511313457459, 0.0, 1.0, 0.21144108491218042], 
reward next is 0.7886, 
noisyNet noise sample is [array([0.4603795], dtype=float32), 1.6424185]. 
=============================================
[2019-04-04 04:06:06,981] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0336792e-27 8.6371260e-24 4.6528391e-26 3.9177286e-24 7.8623936e-24
 8.8750829e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:06:06,981] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7136
[2019-04-04 04:06:07,033] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.4, 68.5, 0.0, 0.0, 26.0, 24.60760041399288, 0.2050969217860293, 0.0, 1.0, 45836.30398297768], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 162600.0000, 
sim time next is 163200.0000, 
raw observation next is [-8.4, 69.0, 0.0, 0.0, 26.0, 24.53584637074417, 0.1906810797279336, 0.0, 1.0, 45854.47657743898], 
processed observation next is [1.0, 0.9130434782608695, 0.2299168975069252, 0.69, 0.0, 0.0, 0.6666666666666666, 0.5446538642286809, 0.5635603599093112, 0.0, 1.0, 0.21835465036875706], 
reward next is 0.7816, 
noisyNet noise sample is [array([-0.04820428], dtype=float32), 0.76515156]. 
=============================================
[2019-04-04 04:06:09,985] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.4969325e-28 1.3221685e-24 2.5788497e-27 8.8447795e-25 7.4318453e-25
 6.5633724e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:06:09,986] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2541
[2019-04-04 04:06:10,029] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.7333333333333334, 35.0, 50.00000000000001, 0.0, 26.0, 25.70764079200234, 0.2757293911178156, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 488400.0000, 
sim time next is 489000.0000, 
raw observation next is [0.9166666666666667, 34.5, 44.0, 0.0, 26.0, 25.65796226086802, 0.2721899327335164, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.48799630655586346, 0.345, 0.14666666666666667, 0.0, 0.6666666666666666, 0.6381635217390015, 0.5907299775778388, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.37946093], dtype=float32), -0.81466454]. 
=============================================
[2019-04-04 04:06:10,036] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[81.763824]
 [81.938324]
 [81.985374]
 [82.17996 ]
 [82.235374]], R is [[81.94937134]
 [82.12987518]
 [82.30857849]
 [82.48549652]
 [82.66064453]].
[2019-04-04 04:06:27,586] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.1957923e-28 1.5528410e-23 7.8201495e-26 2.5404029e-24 2.4781868e-24
 4.2219362e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 04:06:27,586] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1235
[2019-04-04 04:06:27,613] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 76.0, 0.0, 0.0, 26.0, 24.25095308869876, 0.02894175085831643, 0.0, 1.0, 41956.42196727282], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 715200.0000, 
sim time next is 715800.0000, 
raw observation next is [-2.3, 76.0, 0.0, 0.0, 26.0, 24.2438440065476, 0.01718394564063278, 0.0, 1.0, 41981.82350274587], 
processed observation next is [1.0, 0.2608695652173913, 0.3988919667590028, 0.76, 0.0, 0.0, 0.6666666666666666, 0.5203203338789667, 0.505727981880211, 0.0, 1.0, 0.19991344525117083], 
reward next is 0.8001, 
noisyNet noise sample is [array([-0.76446146], dtype=float32), -0.47635692]. 
=============================================
[2019-04-04 04:06:33,971] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.21123985e-28 1.42311375e-24 1.34242494e-26 6.81521539e-25
 9.47275219e-25 5.36648438e-28 1.00000000e+00], sum to 1.0000
[2019-04-04 04:06:33,971] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8918
[2019-04-04 04:06:33,988] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 83.66666666666667, 0.0, 0.0, 26.0, 24.77973156719639, 0.2208446561378385, 0.0, 1.0, 42543.73788096876], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 601800.0000, 
sim time next is 602400.0000, 
raw observation next is [-3.4, 84.33333333333334, 0.0, 0.0, 26.0, 24.74756090783536, 0.2131001567948232, 0.0, 1.0, 42475.8980498567], 
processed observation next is [0.0, 1.0, 0.368421052631579, 0.8433333333333334, 0.0, 0.0, 0.6666666666666666, 0.5622967423196134, 0.5710333855982744, 0.0, 1.0, 0.20226618118979384], 
reward next is 0.7977, 
noisyNet noise sample is [array([-0.02395894], dtype=float32), -0.62748927]. 
=============================================
[2019-04-04 04:06:38,361] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.6130434e-27 2.2105118e-23 1.1091124e-25 3.8033106e-24 7.9405200e-24
 2.0101358e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 04:06:38,371] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8202
[2019-04-04 04:06:38,397] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 92.0, 12.0, 37.99999999999999, 26.0, 24.31860810675678, 0.1464576016374901, 0.0, 1.0, 41086.84912184721], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 546600.0000, 
sim time next is 547200.0000, 
raw observation next is [0.5, 92.0, 17.5, 54.5, 26.0, 24.34837812765612, 0.1476811022201212, 0.0, 1.0, 41025.87807236706], 
processed observation next is [0.0, 0.34782608695652173, 0.4764542936288089, 0.92, 0.058333333333333334, 0.06022099447513812, 0.6666666666666666, 0.5290315106380099, 0.5492270340733737, 0.0, 1.0, 0.19536132415412885], 
reward next is 0.8046, 
noisyNet noise sample is [array([0.8527723], dtype=float32), 0.50647694]. 
=============================================
[2019-04-04 04:06:41,615] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4312617e-27 1.6702822e-23 5.4161569e-26 2.5051808e-24 1.1177272e-23
 5.3094310e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 04:06:41,622] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8892
[2019-04-04 04:06:41,666] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 69.0, 0.0, 0.0, 26.0, 24.89289054758281, 0.191450671759657, 0.0, 1.0, 42266.57265494853], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 681600.0000, 
sim time next is 682200.0000, 
raw observation next is [-3.4, 69.0, 0.0, 0.0, 26.0, 24.85303713939475, 0.184685454245594, 0.0, 1.0, 42208.08234663724], 
processed observation next is [0.0, 0.9130434782608695, 0.368421052631579, 0.69, 0.0, 0.0, 0.6666666666666666, 0.5710864282828959, 0.5615618180818647, 0.0, 1.0, 0.2009908683173202], 
reward next is 0.7990, 
noisyNet noise sample is [array([-0.05185797], dtype=float32), -1.1497885]. 
=============================================
[2019-04-04 04:06:48,280] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.3771287e-30 4.7294859e-25 2.1581397e-27 8.4164014e-26 4.4031218e-25
 5.8093715e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:06:48,282] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7490
[2019-04-04 04:06:48,312] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.6, 82.0, 0.0, 0.0, 26.0, 25.55978519492045, 0.4567112556129091, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 957600.0000, 
sim time next is 958200.0000, 
raw observation next is [6.783333333333333, 81.66666666666667, 0.0, 0.0, 26.0, 25.56930492550499, 0.4482436101148818, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.6505078485687905, 0.8166666666666668, 0.0, 0.0, 0.6666666666666666, 0.6307754104587492, 0.6494145367049606, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.89010364], dtype=float32), 0.27640447]. 
=============================================
[2019-04-04 04:06:54,360] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [5.3114692e-30 7.5848961e-27 6.1975047e-29 2.4369731e-26 5.0277314e-27
 9.1597980e-31 1.0000000e+00], sum to 1.0000
[2019-04-04 04:06:54,362] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.8185
[2019-04-04 04:06:54,375] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 58.5, 99.66666666666666, 669.0, 26.0, 25.86301252543569, 0.3895616733027364, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 733800.0000, 
sim time next is 734400.0000, 
raw observation next is [-0.6, 57.0, 107.5, 614.0, 26.0, 25.84235977200898, 0.3889771677855018, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.44598337950138506, 0.57, 0.35833333333333334, 0.6784530386740332, 0.6666666666666666, 0.6535299810007483, 0.6296590559285006, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.42723194], dtype=float32), -0.72601897]. 
=============================================
[2019-04-04 04:06:55,614] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.3262180e-31 5.9940614e-27 1.1080916e-28 9.2416650e-27 8.0316263e-27
 3.9586087e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 04:06:55,618] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0810
[2019-04-04 04:06:55,631] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 84.0, 83.66666666666666, 0.0, 26.0, 25.45274447085762, 0.2912276182903884, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 903000.0000, 
sim time next is 903600.0000, 
raw observation next is [1.1, 84.0, 87.0, 0.0, 26.0, 25.44417228833298, 0.2831390469846868, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.49307479224376743, 0.84, 0.29, 0.0, 0.6666666666666666, 0.6203476906944149, 0.5943796823282289, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.71534437], dtype=float32), -0.97242314]. 
=============================================
[2019-04-04 04:07:00,493] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [6.3603919e-30 1.2390750e-26 6.6257593e-29 2.7582610e-27 1.9200019e-27
 8.0268660e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 04:07:00,494] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1825
[2019-04-04 04:07:00,499] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.0, 100.0, 51.0, 0.0, 26.0, 23.34192033361787, 0.1241297437032605, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1242000.0000, 
sim time next is 1242600.0000, 
raw observation next is [15.0, 100.0, 58.66666666666667, 0.0, 26.0, 23.335652256433, 0.1238507700989025, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.8781163434903049, 1.0, 0.19555555555555557, 0.0, 0.6666666666666666, 0.4446376880360834, 0.5412835900329674, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.49236608], dtype=float32), 1.4864498]. 
=============================================
[2019-04-04 04:07:07,364] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.7414308e-29 9.9244046e-25 8.2295863e-28 5.7693632e-26 7.5167209e-26
 1.1321213e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:07:07,364] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5012
[2019-04-04 04:07:07,379] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.616666666666667, 78.50000000000001, 0.0, 0.0, 26.0, 24.58480593631119, 0.1803160307072794, 0.0, 1.0, 39304.19869682054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 875400.0000, 
sim time next is 876000.0000, 
raw observation next is [-1.533333333333333, 78.0, 0.0, 0.0, 26.0, 24.57566399822124, 0.1843559917088464, 0.0, 1.0, 39268.00146015653], 
processed observation next is [1.0, 0.13043478260869565, 0.42012927054478305, 0.78, 0.0, 0.0, 0.6666666666666666, 0.5479719998517701, 0.5614519972362821, 0.0, 1.0, 0.1869904831436025], 
reward next is 0.8130, 
noisyNet noise sample is [array([0.8210622], dtype=float32), -0.9495088]. 
=============================================
[2019-04-04 04:07:07,398] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[88.448   ]
 [88.46864 ]
 [88.50197 ]
 [88.563736]
 [88.63323 ]], R is [[88.38261414]
 [88.31162262]
 [88.24118042]
 [88.17129517]
 [88.1020813 ]].
[2019-04-04 04:07:16,725] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.1044923e-33 6.2154983e-29 1.1000918e-31 4.0663596e-30 5.3181595e-30
 1.4665519e-33 1.0000000e+00], sum to 1.0000
[2019-04-04 04:07:16,725] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8037
[2019-04-04 04:07:16,739] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.68333333333333, 69.16666666666667, 207.3333333333333, 143.3333333333333, 26.0, 27.28235791757907, 0.9343740406609488, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1077000.0000, 
sim time next is 1077600.0000, 
raw observation next is [15.86666666666667, 68.33333333333334, 230.6666666666667, 179.1666666666667, 26.0, 27.3641311709984, 0.9629047030957963, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.9021237303785783, 0.6833333333333335, 0.7688888888888891, 0.19797421731123394, 0.6666666666666666, 0.7803442642498668, 0.8209682343652654, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.79779917], dtype=float32), 0.2508486]. 
=============================================
[2019-04-04 04:07:21,724] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.5839637e-32 8.1580081e-28 1.0434673e-30 5.0874378e-29 1.0268599e-28
 4.8293216e-32 1.0000000e+00], sum to 1.0000
[2019-04-04 04:07:21,726] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7812
[2019-04-04 04:07:21,732] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [17.7, 67.0, 0.0, 0.0, 26.0, 24.56310063450866, 0.3752251825763632, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1198200.0000, 
sim time next is 1198800.0000, 
raw observation next is [17.7, 67.0, 0.0, 0.0, 26.0, 24.56962419734215, 0.3710771119317997, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.9130434782608695, 0.9529085872576178, 0.67, 0.0, 0.0, 0.6666666666666666, 0.5474686831118459, 0.6236923706439332, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.8488357], dtype=float32), 0.7068755]. 
=============================================
[2019-04-04 04:07:21,966] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.21938600e-30 5.01386741e-26 2.77452164e-28 1.95507281e-26
 1.06188474e-26 6.29800458e-30 1.00000000e+00], sum to 1.0000
[2019-04-04 04:07:21,966] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2686
[2019-04-04 04:07:21,986] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 26.0, 25.28457464495722, 0.4981197988760955, 0.0, 1.0, 40775.94494718486], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1382400.0000, 
sim time next is 1383000.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 26.0, 25.27586230230212, 0.4931913466793625, 0.0, 1.0, 40592.40545103702], 
processed observation next is [1.0, 0.0, 0.46260387811634357, 0.95, 0.0, 0.0, 0.6666666666666666, 0.6063218585251766, 0.6643971155597875, 0.0, 1.0, 0.193297168814462], 
reward next is 0.8067, 
noisyNet noise sample is [array([0.4319187], dtype=float32), -1.2686272]. 
=============================================
[2019-04-04 04:07:22,004] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[87.22401]
 [87.18172]
 [87.1814 ]
 [87.19967]
 [87.12372]], R is [[87.27204895]
 [87.20516205]
 [87.13796997]
 [87.07049561]
 [87.00269318]].
[2019-04-04 04:07:23,920] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [7.4763269e-30 4.2510928e-26 4.5423477e-28 1.5684901e-26 3.5858748e-26
 2.4674927e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:07:23,922] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.9722
[2019-04-04 04:07:23,942] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.2, 100.0, 9.5, 0.0, 26.0, 24.59283117520657, 0.4239649499376067, 0.0, 1.0, 26902.94546961281], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1270800.0000, 
sim time next is 1271400.0000, 
raw observation next is [11.55, 99.33333333333334, 6.333333333333332, 0.0, 26.0, 24.58320774206833, 0.4243766439037286, 0.0, 1.0, 41897.84233436369], 
processed observation next is [0.0, 0.7391304347826086, 0.7825484764542937, 0.9933333333333334, 0.02111111111111111, 0.0, 0.6666666666666666, 0.5486006451723607, 0.6414588813012428, 0.0, 1.0, 0.19951353492554139], 
reward next is 0.8005, 
noisyNet noise sample is [array([0.3307768], dtype=float32), 1.7110652]. 
=============================================
[2019-04-04 04:07:27,974] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.8368134e-30 8.1636824e-26 3.2578439e-28 1.4188842e-26 2.2185169e-26
 1.7499147e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:07:27,975] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5679
[2019-04-04 04:07:28,020] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.2, 92.33333333333334, 0.0, 0.0, 26.0, 25.3610620694359, 0.4809079003057499, 0.0, 1.0, 45407.00488567488], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1476600.0000, 
sim time next is 1477200.0000, 
raw observation next is [2.2, 92.66666666666667, 0.0, 0.0, 26.0, 25.43554764086767, 0.4889172875880263, 0.0, 1.0, 18764.61214494732], 
processed observation next is [1.0, 0.08695652173913043, 0.5235457063711911, 0.9266666666666667, 0.0, 0.0, 0.6666666666666666, 0.619628970072306, 0.6629724291960087, 0.0, 1.0, 0.08935529592832057], 
reward next is 0.9106, 
noisyNet noise sample is [array([-0.09511194], dtype=float32), 1.5557374]. 
=============================================
[2019-04-04 04:07:29,259] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [8.2731405e-30 4.2774124e-26 8.1600621e-28 8.2171092e-26 2.9922955e-26
 8.7104222e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 04:07:29,259] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8516
[2019-04-04 04:07:29,301] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 18.0, 0.0, 26.0, 25.80606929791018, 0.4981458413407355, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1441800.0000, 
sim time next is 1442400.0000, 
raw observation next is [1.1, 92.0, 15.0, 0.0, 26.0, 25.83594541847344, 0.4996866579892578, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.49307479224376743, 0.92, 0.05, 0.0, 0.6666666666666666, 0.6529954515394533, 0.6665622193297526, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.14782327], dtype=float32), -2.2570164]. 
=============================================
[2019-04-04 04:07:29,849] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.6310650e-29 1.7394616e-26 7.4918656e-28 2.6420330e-25 9.8705229e-26
 1.6234893e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:07:29,849] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0794
[2019-04-04 04:07:29,869] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 100.1666666666667, 0.0, 26.0, 25.69319787048423, 0.5343136338309148, 1.0, 1.0, 26896.59772116946], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1345200.0000, 
sim time next is 1345800.0000, 
raw observation next is [1.1, 92.0, 94.33333333333333, 0.0, 26.0, 25.73526231480488, 0.5418785984617771, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.49307479224376743, 0.92, 0.3144444444444444, 0.0, 0.6666666666666666, 0.6446051929004067, 0.680626199487259, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.23288406], dtype=float32), 1.4012772]. 
=============================================
[2019-04-04 04:07:34,314] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.8360201e-31 3.3511208e-27 1.0474005e-28 3.4960651e-27 4.3359778e-27
 3.6856371e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 04:07:34,315] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3974
[2019-04-04 04:07:34,328] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.699999999999999, 91.83333333333333, 0.0, 0.0, 26.0, 25.64448107875081, 0.5820271375966611, 0.0, 1.0, 20144.90892849684], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1644600.0000, 
sim time next is 1645200.0000, 
raw observation next is [6.6, 93.0, 0.0, 0.0, 26.0, 25.62783776066102, 0.5848993508354791, 0.0, 1.0, 33347.55825022262], 
processed observation next is [1.0, 0.043478260869565216, 0.6454293628808865, 0.93, 0.0, 0.0, 0.6666666666666666, 0.6356531467217517, 0.694966450278493, 0.0, 1.0, 0.15879789642963152], 
reward next is 0.8412, 
noisyNet noise sample is [array([0.9503426], dtype=float32), 0.6171199]. 
=============================================
[2019-04-04 04:07:36,532] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.0006764e-30 2.3797761e-26 3.2085596e-28 1.9743643e-26 6.1095595e-26
 4.2374518e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 04:07:36,533] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5210
[2019-04-04 04:07:36,551] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.699999999999999, 84.33333333333334, 0.0, 0.0, 26.0, 26.07773941404336, 0.7180296338465721, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1633800.0000, 
sim time next is 1634400.0000, 
raw observation next is [6.6, 86.0, 0.0, 0.0, 26.0, 26.06690919481409, 0.7110992303113286, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.6454293628808865, 0.86, 0.0, 0.0, 0.6666666666666666, 0.6722424329011742, 0.7370330767704428, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0347086], dtype=float32), 0.2639682]. 
=============================================
[2019-04-04 04:07:38,909] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.0597565e-30 2.6817545e-26 3.0104509e-28 3.8784682e-26 5.6523160e-26
 5.8718192e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 04:07:38,909] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8340
[2019-04-04 04:07:38,921] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 88.0, 83.0, 0.0, 26.0, 25.87953999082011, 0.5280300205967204, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1683000.0000, 
sim time next is 1683600.0000, 
raw observation next is [1.1, 86.66666666666667, 87.0, 0.0, 26.0, 25.87070826411628, 0.5256002907836532, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.49307479224376743, 0.8666666666666667, 0.29, 0.0, 0.6666666666666666, 0.6558923553430235, 0.6752000969278843, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.15184045], dtype=float32), 0.32798702]. 
=============================================
[2019-04-04 04:07:41,363] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.6988469e-29 3.2983897e-25 1.1406139e-27 3.0892974e-25 9.6862585e-26
 2.0715375e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:07:41,371] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4630
[2019-04-04 04:07:41,396] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.7000000000000001, 90.66666666666666, 0.0, 0.0, 26.0, 25.50436320993163, 0.5342512432571993, 0.0, 1.0, 92892.06423388641], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1716000.0000, 
sim time next is 1716600.0000, 
raw observation next is [0.6, 91.33333333333334, 0.0, 0.0, 26.0, 25.44381200261099, 0.538222191773175, 0.0, 1.0, 92259.48243953697], 
processed observation next is [1.0, 0.8695652173913043, 0.479224376731302, 0.9133333333333334, 0.0, 0.0, 0.6666666666666666, 0.6203176668842492, 0.6794073972577249, 0.0, 1.0, 0.43933086875969984], 
reward next is 0.5607, 
noisyNet noise sample is [array([0.56153595], dtype=float32), 0.12456523]. 
=============================================
[2019-04-04 04:07:42,300] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.8759689e-29 4.3905760e-25 5.2947069e-27 8.4272324e-25 6.0120667e-25
 1.0523266e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:07:42,303] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3519
[2019-04-04 04:07:42,337] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.4166666666666667, 92.5, 0.0, 0.0, 26.0, 25.35711319317116, 0.4754876445726777, 0.0, 1.0, 43508.28728098986], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1727400.0000, 
sim time next is 1728000.0000, 
raw observation next is [0.5, 92.0, 0.0, 0.0, 26.0, 25.35457301732419, 0.4735240581696719, 0.0, 1.0, 43295.09364771321], 
processed observation next is [0.0, 0.0, 0.4764542936288089, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6128810847770157, 0.657841352723224, 0.0, 1.0, 0.20616711260815815], 
reward next is 0.7938, 
noisyNet noise sample is [array([-0.39956033], dtype=float32), -0.11914131]. 
=============================================
[2019-04-04 04:07:42,341] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[81.57039 ]
 [81.528885]
 [81.49269 ]
 [81.49062 ]
 [81.505035]], R is [[81.4630661 ]
 [81.44125366]
 [81.41687775]
 [81.38495636]
 [81.33532715]].
[2019-04-04 04:08:00,021] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.6955358e-29 1.6983004e-24 8.5028829e-27 4.9472401e-25 2.6350678e-25
 1.7389424e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:08:00,037] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7220
[2019-04-04 04:08:00,091] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.3, 89.33333333333334, 27.83333333333333, 17.66666666666667, 26.0, 25.1612069262851, 0.2099862819923267, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1930800.0000, 
sim time next is 1931400.0000, 
raw observation next is [-9.2, 88.5, 33.0, 21.0, 26.0, 25.36110054095347, 0.2106020658134306, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.20775623268698065, 0.885, 0.11, 0.023204419889502764, 0.6666666666666666, 0.6134250450794557, 0.5702006886044769, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3093299], dtype=float32), -1.409842]. 
=============================================
[2019-04-04 04:08:00,107] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [4.7481208e-26 8.6649219e-22 2.9449185e-24 1.1823541e-22 1.0500201e-22
 4.3649419e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 04:08:00,112] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.1338
[2019-04-04 04:08:00,142] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.199999999999999, 79.0, 0.0, 0.0, 26.0, 23.66575062099458, -0.01370085113833766, 0.0, 1.0, 47075.1249657478], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1834800.0000, 
sim time next is 1835400.0000, 
raw observation next is [-6.2, 79.0, 0.0, 0.0, 26.0, 23.63940594835257, -0.0219774711502735, 0.0, 1.0, 47079.64049845625], 
processed observation next is [0.0, 0.21739130434782608, 0.2908587257617729, 0.79, 0.0, 0.0, 0.6666666666666666, 0.46995049569604763, 0.4926741762832421, 0.0, 1.0, 0.22418876427836307], 
reward next is 0.7758, 
noisyNet noise sample is [array([-1.3864081], dtype=float32), -1.9602064]. 
=============================================
[2019-04-04 04:08:01,639] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [8.5132279e-29 1.9800976e-25 5.6876332e-27 3.7030225e-25 1.7792732e-25
 1.0669918e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:08:01,640] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5107
[2019-04-04 04:08:01,712] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.366666666666667, 64.0, 150.6666666666667, 111.6666666666667, 26.0, 25.54855082263671, 0.3767714158484545, 1.0, 1.0, 152343.5299324043], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2119200.0000, 
sim time next is 2119800.0000, 
raw observation next is [-6.283333333333333, 64.0, 150.3333333333333, 89.33333333333334, 26.0, 25.48677449955727, 0.4011643313366673, 1.0, 1.0, 94591.73351152381], 
processed observation next is [1.0, 0.5217391304347826, 0.288550323176362, 0.64, 0.501111111111111, 0.0987108655616943, 0.6666666666666666, 0.6238978749631059, 0.6337214437788891, 1.0, 1.0, 0.4504368262453515], 
reward next is 0.5496, 
noisyNet noise sample is [array([0.8265237], dtype=float32), -1.1331406]. 
=============================================
[2019-04-04 04:08:01,764] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.87613227e-27 3.42450451e-23 2.57746570e-25 8.48151533e-24
 1.11845228e-23 1.20881224e-26 1.00000000e+00], sum to 1.0000
[2019-04-04 04:08:01,766] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2256
[2019-04-04 04:08:01,839] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.300000000000001, 79.66666666666667, 0.0, 0.0, 26.0, 24.1052778876759, 0.05266449111506832, 0.0, 1.0, 45079.62538300424], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1902000.0000, 
sim time next is 1902600.0000, 
raw observation next is [-7.3, 78.5, 0.0, 0.0, 26.0, 24.13183527723151, 0.05253248866487997, 0.0, 1.0, 45088.71466653408], 
processed observation next is [1.0, 0.0, 0.26038781163434904, 0.785, 0.0, 0.0, 0.6666666666666666, 0.5109862731026258, 0.51751082955496, 0.0, 1.0, 0.2147081650787337], 
reward next is 0.7853, 
noisyNet noise sample is [array([0.3847033], dtype=float32), -0.09006532]. 
=============================================
[2019-04-04 04:08:05,702] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.2904212e-29 4.7020700e-25 2.0730505e-26 1.4961058e-24 1.6355089e-24
 4.4452225e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:08:05,703] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3416
[2019-04-04 04:08:05,719] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.1, 86.33333333333333, 0.0, 0.0, 26.0, 24.34979759879714, 0.1584151576461038, 0.0, 1.0, 42109.4935449344], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1990200.0000, 
sim time next is 1990800.0000, 
raw observation next is [-6.2, 87.0, 0.0, 0.0, 26.0, 24.34865421782067, 0.1508961795616905, 0.0, 1.0, 42007.70013087132], 
processed observation next is [1.0, 0.043478260869565216, 0.2908587257617729, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5290545181517224, 0.5502987265205636, 0.0, 1.0, 0.20003666728986344], 
reward next is 0.8000, 
noisyNet noise sample is [array([0.23446415], dtype=float32), -1.7141154]. 
=============================================
[2019-04-04 04:08:06,741] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [6.6938228e-28 2.7079522e-23 1.5707303e-25 9.1054483e-24 8.3295965e-24
 8.8599326e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 04:08:06,759] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.8940
[2019-04-04 04:08:06,780] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.3, 78.5, 0.0, 0.0, 26.0, 24.13183527723151, 0.05253248866487997, 0.0, 1.0, 45088.71466653408], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1902600.0000, 
sim time next is 1903200.0000, 
raw observation next is [-7.3, 77.33333333333334, 0.0, 0.0, 26.0, 24.13928361517537, 0.04695683202578276, 0.0, 1.0, 45104.12624540675], 
processed observation next is [1.0, 0.0, 0.26038781163434904, 0.7733333333333334, 0.0, 0.0, 0.6666666666666666, 0.5116069679312808, 0.5156522773419275, 0.0, 1.0, 0.21478155354955594], 
reward next is 0.7852, 
noisyNet noise sample is [array([0.12261788], dtype=float32), 0.10686469]. 
=============================================
[2019-04-04 04:08:10,698] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0099070e-28 4.1255399e-25 8.8480434e-27 1.9235227e-25 5.8690349e-25
 7.3285655e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:08:10,699] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0013
[2019-04-04 04:08:10,748] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 82.0, 32.0, 0.0, 26.0, 25.81851380572238, 0.4026359148166687, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2046600.0000, 
sim time next is 2047200.0000, 
raw observation next is [-3.899999999999999, 82.0, 27.0, 0.0, 26.0, 25.83010999102411, 0.2542311670059296, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.35457063711911363, 0.82, 0.09, 0.0, 0.6666666666666666, 0.6525091659186758, 0.5847437223353099, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2750357], dtype=float32), -1.4681771]. 
=============================================
[2019-04-04 04:08:18,613] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.4786098e-29 3.3227998e-26 1.1498881e-28 4.0894993e-26 3.5269158e-26
 5.1829694e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 04:08:18,616] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8924
[2019-04-04 04:08:18,647] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.95, 52.16666666666666, 248.3333333333333, 72.33333333333333, 26.0, 25.32673803455797, 0.3167552644156448, 1.0, 1.0, 18697.59342053906], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2292600.0000, 
sim time next is 2293200.0000, 
raw observation next is [-1.7, 51.0, 241.5, 71.5, 26.0, 25.22074535228762, 0.3205781990161612, 1.0, 1.0, 18696.98945455885], 
processed observation next is [1.0, 0.5652173913043478, 0.4155124653739613, 0.51, 0.805, 0.07900552486187845, 0.6666666666666666, 0.6017287793573015, 0.6068593996720537, 1.0, 1.0, 0.0890332831169469], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.06290427], dtype=float32), -0.42452964]. 
=============================================
[2019-04-04 04:08:19,140] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.3721481e-29 3.9355065e-25 2.3714195e-27 3.7174875e-25 1.6104897e-25
 2.0404173e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:08:19,140] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9814
[2019-04-04 04:08:19,220] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.633333333333333, 77.66666666666667, 0.0, 0.0, 26.0, 25.49605243267775, 0.348666740919426, 1.0, 1.0, 18715.60489475307], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1964400.0000, 
sim time next is 1965000.0000, 
raw observation next is [-4.816666666666666, 78.33333333333334, 0.0, 0.0, 26.0, 25.33151688927054, 0.3483495085572961, 1.0, 1.0, 24675.21412799677], 
processed observation next is [1.0, 0.7391304347826086, 0.32917820867959374, 0.7833333333333334, 0.0, 0.0, 0.6666666666666666, 0.6109597407725449, 0.616116502852432, 1.0, 1.0, 0.11750101965712748], 
reward next is 0.8825, 
noisyNet noise sample is [array([1.4834867], dtype=float32), 0.037006076]. 
=============================================
[2019-04-04 04:08:19,233] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[82.48676 ]
 [82.529465]
 [82.95006 ]
 [83.20118 ]
 [83.01117 ]], R is [[82.51532745]
 [82.60105896]
 [82.7750473 ]
 [82.94729614]
 [83.11782074]].
[2019-04-04 04:08:21,714] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.0097157e-29 1.8966654e-25 9.5171070e-28 1.8153770e-25 1.2930090e-25
 3.3369324e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:08:21,714] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1576
[2019-04-04 04:08:21,736] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 54.66666666666667, 0.0, 0.0, 26.0, 25.45055753928966, 0.4287674455916695, 0.0, 1.0, 18759.99925482402], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2323200.0000, 
sim time next is 2323800.0000, 
raw observation next is [-1.7, 55.0, 0.0, 0.0, 26.0, 25.43301926057366, 0.4261490673601802, 0.0, 1.0, 32336.74252528141], 
processed observation next is [1.0, 0.9130434782608695, 0.4155124653739613, 0.55, 0.0, 0.0, 0.6666666666666666, 0.6194182717144715, 0.6420496891200601, 0.0, 1.0, 0.15398448821562577], 
reward next is 0.8460, 
noisyNet noise sample is [array([0.8529906], dtype=float32), 0.52260554]. 
=============================================
[2019-04-04 04:08:31,203] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.1360888e-29 9.3664586e-26 1.1060229e-27 1.7355056e-25 9.3338226e-26
 3.2977036e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:08:31,204] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6149
[2019-04-04 04:08:31,246] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 68.5, 75.66666666666666, 94.99999999999999, 26.0, 26.13650490378787, 0.4711837366778238, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2218200.0000, 
sim time next is 2218800.0000, 
raw observation next is [-4.1, 69.0, 55.33333333333333, 47.49999999999999, 26.0, 26.1345482429473, 0.3105792095256618, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.3490304709141275, 0.69, 0.18444444444444444, 0.05248618784530386, 0.6666666666666666, 0.6778790202456083, 0.6035264031752207, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.58098936], dtype=float32), 0.06558347]. 
=============================================
[2019-04-04 04:08:33,699] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.2744037e-28 1.4962934e-25 2.5675123e-27 3.1728412e-25 3.1705062e-25
 1.2661949e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:08:33,707] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5892
[2019-04-04 04:08:33,725] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.700000000000001, 77.5, 0.0, 0.0, 26.0, 24.417795168362, 0.1932522347808426, 0.0, 1.0, 44309.1788440236], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2247000.0000, 
sim time next is 2247600.0000, 
raw observation next is [-6.700000000000001, 77.0, 0.0, 0.0, 26.0, 24.39091398764574, 0.1913457990522367, 0.0, 1.0, 44341.99237110727], 
processed observation next is [1.0, 0.0, 0.2770083102493075, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5325761656371449, 0.5637819330174122, 0.0, 1.0, 0.21115234462432034], 
reward next is 0.7888, 
noisyNet noise sample is [array([-0.08025301], dtype=float32), -0.15505159]. 
=============================================
[2019-04-04 04:08:40,154] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.3847617e-29 3.3149518e-25 1.7521941e-27 5.6776211e-26 3.6654701e-25
 4.9373510e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:08:40,155] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6505
[2019-04-04 04:08:40,196] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.1, 58.0, 0.0, 0.0, 26.0, 25.26992018373328, 0.3552117471373228, 0.0, 1.0, 39136.10283157401], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2328000.0000, 
sim time next is 2328600.0000, 
raw observation next is [-2.2, 58.5, 0.0, 0.0, 26.0, 25.18886297456008, 0.3423647455157446, 0.0, 1.0, 38787.40772606265], 
processed observation next is [1.0, 0.9565217391304348, 0.4016620498614959, 0.585, 0.0, 0.0, 0.6666666666666666, 0.5990719145466734, 0.6141215818385816, 0.0, 1.0, 0.1847019415526793], 
reward next is 0.8153, 
noisyNet noise sample is [array([-1.2759326], dtype=float32), -0.9380272]. 
=============================================
[2019-04-04 04:08:46,118] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0495204e-28 1.2719724e-25 8.9074741e-28 1.0813967e-25 4.2982699e-25
 1.6308269e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:08:46,118] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5431
[2019-04-04 04:08:46,131] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.3, 26.66666666666667, 64.66666666666667, 697.3333333333334, 26.0, 24.97409056221018, 0.2804224218277619, 0.0, 1.0, 18697.68870803133], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2474400.0000, 
sim time next is 2475000.0000, 
raw observation next is [3.3, 26.5, 62.0, 680.0, 26.0, 24.97513901394997, 0.2804122218231922, 0.0, 1.0, 18697.52007355263], 
processed observation next is [0.0, 0.6521739130434783, 0.554016620498615, 0.265, 0.20666666666666667, 0.7513812154696132, 0.6666666666666666, 0.5812615844958309, 0.5934707406077308, 0.0, 1.0, 0.08903580987406015], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.2019261], dtype=float32), -0.96833616]. 
=============================================
[2019-04-04 04:08:46,141] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[84.76445 ]
 [84.88618 ]
 [85.0346  ]
 [85.10917 ]
 [85.198204]], R is [[84.73460388]
 [84.7982254 ]
 [84.95024109]
 [85.10073853]
 [85.16067505]].
[2019-04-04 04:08:52,405] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.5092294e-30 3.3971109e-26 2.2089047e-28 2.8515023e-26 3.5784410e-26
 2.3704573e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 04:08:52,406] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0194
[2019-04-04 04:08:52,448] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.133333333333333, 54.33333333333333, 96.66666666666667, 693.3333333333334, 26.0, 26.77187813220095, 0.5559290625408814, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2731800.0000, 
sim time next is 2732400.0000, 
raw observation next is [-4.0, 54.0, 94.0, 673.5, 26.0, 26.03710124918757, 0.5715649199430146, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.3518005540166205, 0.54, 0.31333333333333335, 0.7441988950276243, 0.6666666666666666, 0.6697584374322977, 0.6905216399810049, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.26551846], dtype=float32), 0.49161097]. 
=============================================
[2019-04-04 04:09:07,274] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.6796833e-28 2.4327297e-25 3.7331017e-27 4.3823432e-25 8.1530116e-25
 4.7995950e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:09:07,275] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3050
[2019-04-04 04:09:07,298] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.7, 29.0, 38.5, 83.5, 26.0, 25.75911909512832, 0.3955770754742485, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2566800.0000, 
sim time next is 2567400.0000, 
raw observation next is [2.333333333333334, 30.0, 28.0, 57.33333333333332, 26.0, 25.82678832664019, 0.3954606309302025, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.5272391505078486, 0.3, 0.09333333333333334, 0.06335174953959483, 0.6666666666666666, 0.6522323605533492, 0.6318202103100675, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.24943602], dtype=float32), -1.3712616]. 
=============================================
[2019-04-04 04:09:11,078] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [7.7417561e-27 4.7076029e-23 1.5251716e-24 7.8277721e-23 8.8551524e-23
 8.6113252e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 04:09:11,079] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.2007
[2019-04-04 04:09:11,114] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-15.83333333333333, 83.0, 0.0, 0.0, 26.0, 23.08757945947819, -0.1099431370389052, 0.0, 1.0, 43425.96799931899], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2699400.0000, 
sim time next is 2700000.0000, 
raw observation next is [-16.0, 83.0, 0.0, 0.0, 26.0, 23.08813308392492, -0.1239352117427715, 0.0, 1.0, 43343.73676134676], 
processed observation next is [1.0, 0.2608695652173913, 0.01939058171745151, 0.83, 0.0, 0.0, 0.6666666666666666, 0.4240110903270766, 0.45868826275240954, 0.0, 1.0, 0.20639874648260362], 
reward next is 0.7936, 
noisyNet noise sample is [array([0.7630505], dtype=float32), 1.6762725]. 
=============================================
[2019-04-04 04:09:11,117] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[76.709885]
 [76.6845  ]
 [76.65008 ]
 [76.56997 ]
 [76.48693 ]], R is [[76.75895691]
 [76.78458405]
 [76.80964661]
 [76.83404541]
 [76.85764313]].
[2019-04-04 04:09:14,394] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.8631023e-28 1.1376997e-24 1.4122100e-26 1.8872480e-24 5.8823938e-25
 1.1612700e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:09:14,394] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2699
[2019-04-04 04:09:14,426] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 100.0, 78.0, 27.0, 26.0, 25.92084114481721, 0.3538867630415987, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2908800.0000, 
sim time next is 2909400.0000, 
raw observation next is [2.0, 98.83333333333334, 75.66666666666666, 36.00000000000001, 26.0, 25.44042317157564, 0.3764351671068822, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.518005540166205, 0.9883333333333334, 0.2522222222222222, 0.03977900552486189, 0.6666666666666666, 0.6200352642979702, 0.6254783890356274, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.88540643], dtype=float32), -0.35304236]. 
=============================================
[2019-04-04 04:09:16,173] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.13280869e-27 1.58803346e-25 1.17439526e-26 4.16439120e-24
 4.04712071e-24 4.49727849e-28 1.00000000e+00], sum to 1.0000
[2019-04-04 04:09:16,173] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.3127
[2019-04-04 04:09:16,255] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.333333333333333, 51.33333333333334, 11.5, 119.8333333333333, 26.0, 25.86881986861039, 0.2906147474600818, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2740800.0000, 
sim time next is 2741400.0000, 
raw observation next is [-3.5, 52.0, 3.0, 53.0, 26.0, 25.59499499415945, 0.388817702986069, 1.0, 1.0, 142475.9379075124], 
processed observation next is [1.0, 0.7391304347826086, 0.36565096952908593, 0.52, 0.01, 0.05856353591160221, 0.6666666666666666, 0.6329162495132875, 0.6296059009953563, 1.0, 1.0, 0.6784568471786304], 
reward next is 0.3215, 
noisyNet noise sample is [array([1.5725367], dtype=float32), -1.4891294]. 
=============================================
[2019-04-04 04:09:20,841] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.3615296e-29 6.2134931e-26 3.2751159e-27 3.5349454e-25 1.0601165e-25
 6.1785734e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:09:20,841] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9165
[2019-04-04 04:09:20,868] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 100.0, 151.6666666666667, 0.0, 26.0, 25.39682945652768, 0.338529879753328, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2900400.0000, 
sim time next is 2901000.0000, 
raw observation next is [2.0, 100.0, 139.3333333333333, 0.0, 26.0, 25.41603693524968, 0.3416881387318187, 1.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.5652173913043478, 0.518005540166205, 1.0, 0.46444444444444427, 0.0, 0.6666666666666666, 0.6180030779374732, 0.6138960462439396, 1.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-1.1337494], dtype=float32), 0.79356796]. 
=============================================
[2019-04-04 04:09:20,912] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[81.68892 ]
 [82.12269 ]
 [82.209274]
 [82.26897 ]
 [82.31516 ]], R is [[81.36371613]
 [81.55007935]
 [81.73458099]
 [81.91723633]
 [82.09806824]].
[2019-04-04 04:09:21,000] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.1565247e-29 4.5480242e-25 2.1894345e-27 1.9264527e-25 6.4547533e-26
 8.0732708e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:09:21,000] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9315
[2019-04-04 04:09:21,068] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.833333333333333, 63.66666666666667, 0.0, 0.0, 26.0, 25.23039108213945, 0.3548893402030302, 0.0, 1.0, 43937.55701729131], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2848200.0000, 
sim time next is 2848800.0000, 
raw observation next is [1.666666666666667, 65.33333333333334, 0.0, 0.0, 26.0, 25.22931550665036, 0.3525981238629425, 0.0, 1.0, 43155.42125199507], 
processed observation next is [1.0, 1.0, 0.5087719298245615, 0.6533333333333334, 0.0, 0.0, 0.6666666666666666, 0.6024429588875299, 0.6175327079543141, 0.0, 1.0, 0.20550200596188128], 
reward next is 0.7945, 
noisyNet noise sample is [array([-1.1549133], dtype=float32), -1.023992]. 
=============================================
[2019-04-04 04:09:26,438] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.4151942e-28 4.5866730e-23 1.1688494e-25 1.2603088e-24 4.3948175e-24
 6.5984748e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 04:09:26,449] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5921
[2019-04-04 04:09:26,483] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 97.66666666666666, 0.0, 0.0, 26.0, 24.99192311093055, 0.2441483376127192, 0.0, 1.0, 55925.1354633003], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2871600.0000, 
sim time next is 2872200.0000, 
raw observation next is [1.0, 98.83333333333334, 0.0, 0.0, 26.0, 24.89310544857761, 0.2448947600178673, 0.0, 1.0, 55708.69950202957], 
processed observation next is [1.0, 0.21739130434782608, 0.4903047091412743, 0.9883333333333334, 0.0, 0.0, 0.6666666666666666, 0.5744254540481343, 0.5816315866726224, 0.0, 1.0, 0.2652795214382361], 
reward next is 0.7347, 
noisyNet noise sample is [array([1.1777269], dtype=float32), -0.5120019]. 
=============================================
[2019-04-04 04:09:31,452] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.6760207e-29 8.0683264e-26 1.8136723e-27 3.0942744e-25 8.6557202e-26
 6.0839435e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:09:31,452] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1756
[2019-04-04 04:09:31,536] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.3333333333333333, 92.33333333333333, 0.0, 0.0, 26.0, 24.91900329407454, 0.3910486897722302, 1.0, 1.0, 121877.1629080823], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2918400.0000, 
sim time next is 2919000.0000, 
raw observation next is [-0.6666666666666667, 92.16666666666667, 0.0, 0.0, 26.0, 25.00380131112324, 0.4033841315444391, 1.0, 1.0, 36206.56696090028], 
processed observation next is [1.0, 0.782608695652174, 0.44413665743305636, 0.9216666666666667, 0.0, 0.0, 0.6666666666666666, 0.5836501092602701, 0.6344613771814797, 1.0, 1.0, 0.17241222362333467], 
reward next is 0.8276, 
noisyNet noise sample is [array([-0.04025658], dtype=float32), -0.7303181]. 
=============================================
[2019-04-04 04:09:31,591] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[80.87335 ]
 [80.801346]
 [80.96242 ]
 [81.327675]
 [81.370995]], R is [[80.62654114]
 [80.23990631]
 [80.03197479]
 [80.23165894]
 [80.42934418]].
[2019-04-04 04:09:34,072] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.2589897e-28 1.4927252e-24 3.7131610e-27 7.7715308e-25 3.3086478e-25
 3.4537909e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:09:34,072] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1884
[2019-04-04 04:09:34,086] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.84008398536729, 0.246376479061709, 0.0, 1.0, 37931.16879212891], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3024000.0000, 
sim time next is 3024600.0000, 
raw observation next is [-4.166666666666667, 66.0, 0.0, 0.0, 26.0, 24.80523955170657, 0.2405401219831908, 0.0, 1.0, 37891.24811552476], 
processed observation next is [0.0, 0.0, 0.3471837488457987, 0.66, 0.0, 0.0, 0.6666666666666666, 0.5671032959755475, 0.5801800406610637, 0.0, 1.0, 0.18043451483583217], 
reward next is 0.8196, 
noisyNet noise sample is [array([0.16830833], dtype=float32), -0.27588156]. 
=============================================
[2019-04-04 04:09:48,991] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.0883870e-27 6.7114563e-23 1.1191382e-25 4.5873532e-24 3.0158251e-24
 1.2232900e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 04:09:48,991] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2932
[2019-04-04 04:09:49,014] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 23.90984109903956, 0.02958920391666374, 0.0, 1.0, 40198.08265195469], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3042000.0000, 
sim time next is 3042600.0000, 
raw observation next is [-6.0, 75.83333333333334, 0.0, 0.0, 26.0, 23.88464341453021, 0.02359580570904125, 0.0, 1.0, 40180.42283557937], 
processed observation next is [0.0, 0.21739130434782608, 0.296398891966759, 0.7583333333333334, 0.0, 0.0, 0.6666666666666666, 0.4903869512108508, 0.5078652685696804, 0.0, 1.0, 0.1913353468360922], 
reward next is 0.8087, 
noisyNet noise sample is [array([1.0399737], dtype=float32), 1.0112259]. 
=============================================
[2019-04-04 04:09:55,017] A3C_AGENT_WORKER-Thread-4 INFO:Evaluating...
[2019-04-04 04:09:55,018] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 04:09:55,019] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:09:55,020] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 04:09:55,022] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:09:55,028] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run44
[2019-04-04 04:09:55,029] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 04:09:55,084] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run44
[2019-04-04 04:09:55,084] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:09:55,155] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run44
[2019-04-04 04:11:00,645] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.36715293], dtype=float32), 0.22445396]
[2019-04-04 04:11:00,645] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [1.1, 73.0, 110.0, 451.0, 26.0, 25.00153845767694, 0.3822491658947073, 0.0, 1.0, 45779.35312324821]
[2019-04-04 04:11:00,645] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 04:11:00,646] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [9.4541048e-29 4.4189390e-25 1.6738124e-27 1.9223638e-25 2.2962262e-25
 1.2522935e-28 1.0000000e+00], sampled 0.5673058884523527
[2019-04-04 04:11:14,120] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.36715293], dtype=float32), 0.22445396]
[2019-04-04 04:11:14,121] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [4.316666666666666, 76.5, 211.3333333333333, 216.6666666666667, 26.0, 25.52833499432035, 0.532167146496982, 1.0, 1.0, 0.0]
[2019-04-04 04:11:14,121] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 04:11:14,122] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.8121836e-29 4.0121000e-26 2.7847327e-28 6.5860302e-26 3.2059079e-26
 1.5875760e-29 1.0000000e+00], sampled 0.7737919504431151
[2019-04-04 04:13:09,932] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 04:13:32,155] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.36715293], dtype=float32), 0.22445396]
[2019-04-04 04:13:32,156] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-0.540096091, 61.74827611, 0.0, 0.0, 26.0, 25.48698874449217, 0.5501689352484717, 0.0, 1.0, 21951.49334459001]
[2019-04-04 04:13:32,156] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 04:13:32,157] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [5.1747561e-28 9.8648062e-25 9.2942013e-27 1.1972488e-24 1.1545760e-24
 4.1691674e-28 1.0000000e+00], sampled 0.977244693607175
[2019-04-04 04:13:41,616] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 04:13:46,191] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 04:13:47,229] A3C_AGENT_WORKER-Thread-4 INFO:Global step: 4300000, evaluation results [4300000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 04:13:51,491] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [8.3193741e-31 1.4791022e-27 3.5475961e-29 5.3106467e-27 2.9890427e-27
 1.3963874e-31 1.0000000e+00], sum to 1.0000
[2019-04-04 04:13:51,491] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.2794
[2019-04-04 04:13:51,514] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.666666666666666, 95.33333333333334, 113.8333333333333, 808.0, 26.0, 27.15433281740924, 0.7877372371714492, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3152400.0000, 
sim time next is 3153000.0000, 
raw observation next is [7.833333333333334, 94.16666666666666, 113.6666666666667, 811.0, 26.0, 27.16585768630426, 0.8023743668491043, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.6795937211449677, 0.9416666666666665, 0.378888888888889, 0.8961325966850828, 0.6666666666666666, 0.7638214738586884, 0.7674581222830348, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6896499], dtype=float32), 0.44806182]. 
=============================================
[2019-04-04 04:13:51,560] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[86.819115]
 [86.958305]
 [87.021416]
 [87.14585 ]
 [87.33413 ]], R is [[86.8555603 ]
 [86.98700714]
 [87.11713409]
 [87.24596405]
 [87.37350464]].
[2019-04-04 04:13:59,305] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.6414041e-30 8.1593856e-26 3.3709665e-28 1.5827046e-26 3.4814920e-27
 1.1757876e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:13:59,346] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9839
[2019-04-04 04:13:59,406] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 92.0, 43.0, 226.0, 26.0, 25.54550507166091, 0.5184660024633451, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3225600.0000, 
sim time next is 3226200.0000, 
raw observation next is [-3.0, 92.0, 57.00000000000001, 274.0, 26.0, 25.5623352367891, 0.5168645596369369, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.3795013850415513, 0.92, 0.19000000000000003, 0.3027624309392265, 0.6666666666666666, 0.6301946030657583, 0.6722881865456456, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7400271], dtype=float32), -1.4747189]. 
=============================================
[2019-04-04 04:13:59,755] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.4538045e-31 1.8272070e-27 9.9237362e-30 1.7557336e-27 1.0788822e-27
 1.1816858e-31 1.0000000e+00], sum to 1.0000
[2019-04-04 04:13:59,755] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0190
[2019-04-04 04:13:59,773] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.0, 100.0, 111.0, 775.5, 26.0, 27.03661860448754, 0.7427995284861121, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3150000.0000, 
sim time next is 3150600.0000, 
raw observation next is [7.166666666666667, 98.83333333333334, 112.0, 785.3333333333334, 26.0, 27.07482250864806, 0.7569433004677717, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.6611265004616806, 0.9883333333333334, 0.37333333333333335, 0.8677716390423573, 0.6666666666666666, 0.7562352090540051, 0.7523144334892572, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.38431674], dtype=float32), -1.3344254]. 
=============================================
[2019-04-04 04:14:08,587] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.1531602e-29 4.8756316e-26 4.8517666e-28 3.7242703e-26 4.0810372e-26
 1.6190857e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:14:08,587] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6250
[2019-04-04 04:14:08,615] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.166666666666667, 77.0, 0.0, 0.0, 26.0, 25.17686605181428, 0.4678758360979512, 1.0, 1.0, 18682.27693586107], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3437400.0000, 
sim time next is 3438000.0000, 
raw observation next is [1.0, 79.0, 0.0, 0.0, 26.0, 25.29219433239937, 0.46137473375451, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.4903047091412743, 0.79, 0.0, 0.0, 0.6666666666666666, 0.6076828610332807, 0.65379157791817, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6506789], dtype=float32), 1.8045316]. 
=============================================
[2019-04-04 04:14:08,632] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[83.02413]
 [82.94159]
 [82.77414]
 [82.53641]
 [82.08565]], R is [[83.19442749]
 [83.27352142]
 [83.00189209]
 [82.88717651]
 [82.80200195]].
[2019-04-04 04:14:13,567] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [6.7772981e-29 1.2917130e-24 2.7221150e-27 5.4582408e-26 4.5473801e-26
 3.9839118e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:14:13,569] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5971
[2019-04-04 04:14:13,624] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 71.0, 47.0, 282.5, 26.0, 25.25316173021312, 0.301844734431631, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3744000.0000, 
sim time next is 3744600.0000, 
raw observation next is [-4.0, 72.0, 61.00000000000001, 331.3333333333334, 26.0, 25.19161835805996, 0.3035227619881227, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.3518005540166205, 0.72, 0.20333333333333337, 0.3661141804788215, 0.6666666666666666, 0.5993015298383298, 0.6011742539960409, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.30325672], dtype=float32), 1.0353903]. 
=============================================
[2019-04-04 04:14:15,150] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.7658247e-28 7.1279436e-24 9.7612185e-27 9.3863502e-25 2.7693248e-24
 2.5744560e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 04:14:15,155] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2070
[2019-04-04 04:14:15,177] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.166666666666667, 66.0, 0.0, 0.0, 26.0, 25.20654143314968, 0.392213861107859, 0.0, 1.0, 41023.09906558692], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3553800.0000, 
sim time next is 3554400.0000, 
raw observation next is [-3.333333333333333, 67.0, 0.0, 0.0, 26.0, 25.17656483835031, 0.3825522573968878, 0.0, 1.0, 41110.61505693721], 
processed observation next is [0.0, 0.13043478260869565, 0.37026777469990774, 0.67, 0.0, 0.0, 0.6666666666666666, 0.5980470698625258, 0.627517419132296, 0.0, 1.0, 0.19576483360446292], 
reward next is 0.8042, 
noisyNet noise sample is [array([-1.5229043], dtype=float32), -1.9755808]. 
=============================================
[2019-04-04 04:14:17,036] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.4578868e-29 2.0530307e-25 4.8961473e-28 2.0829114e-26 1.0083841e-25
 5.3556019e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:14:17,038] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7889
[2019-04-04 04:14:17,055] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.5, 65.5, 99.0, 670.0, 26.0, 25.72918537935858, 0.4989944164272253, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3490200.0000, 
sim time next is 3490800.0000, 
raw observation next is [-0.3333333333333334, 63.66666666666667, 100.6666666666667, 686.6666666666667, 26.0, 25.85387625279866, 0.5189949064705648, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4533702677747, 0.6366666666666667, 0.33555555555555566, 0.7587476979742174, 0.6666666666666666, 0.6544896877332217, 0.6729983021568549, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.5611064], dtype=float32), 0.44678596]. 
=============================================
[2019-04-04 04:14:27,480] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.4787952e-30 5.0483916e-27 4.4269272e-29 1.9916417e-26 9.1204870e-27
 1.7309720e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 04:14:27,481] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1526
[2019-04-04 04:14:27,493] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.666666666666667, 49.0, 111.8333333333333, 817.0, 26.0, 25.55411446068755, 0.5847837386208455, 1.0, 1.0, 20808.50269566059], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3850800.0000, 
sim time next is 3851400.0000, 
raw observation next is [1.833333333333333, 48.5, 110.6666666666667, 810.0, 26.0, 25.99444504042791, 0.6242094179710765, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5133887349953832, 0.485, 0.368888888888889, 0.8950276243093923, 0.6666666666666666, 0.6662037533689924, 0.7080698059903588, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.48900998], dtype=float32), 0.8363372]. 
=============================================
[2019-04-04 04:14:31,991] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.7660433e-30 2.8769915e-27 1.0662675e-28 5.7136853e-27 1.8199385e-26
 4.2804211e-31 1.0000000e+00], sum to 1.0000
[2019-04-04 04:14:31,999] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4557
[2019-04-04 04:14:32,010] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.5, 60.0, 115.0, 818.0, 26.0, 26.22333361838097, 0.6072738174020901, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3763800.0000, 
sim time next is 3764400.0000, 
raw observation next is [-0.3333333333333334, 60.0, 113.5, 811.0, 26.0, 26.37469784386521, 0.6264907489607064, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.4533702677747, 0.6, 0.37833333333333335, 0.8961325966850828, 0.6666666666666666, 0.6978914869887675, 0.7088302496535688, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.67531675], dtype=float32), 1.4083519]. 
=============================================
[2019-04-04 04:14:38,459] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.0599627e-30 7.1655826e-27 2.2914714e-28 4.4082792e-26 6.6608685e-26
 2.2534488e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 04:14:38,459] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7949
[2019-04-04 04:14:38,470] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 35.33333333333333, 84.33333333333334, 692.6666666666667, 26.0, 26.89140063489771, 0.7764670014463633, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3944400.0000, 
sim time next is 3945000.0000, 
raw observation next is [-4.0, 34.66666666666667, 80.66666666666667, 661.3333333333334, 26.0, 26.9565476984024, 0.7885206678559071, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.3518005540166205, 0.34666666666666673, 0.2688888888888889, 0.730755064456722, 0.6666666666666666, 0.7463789748668667, 0.7628402226186357, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8251724], dtype=float32), -1.8335754]. 
=============================================
[2019-04-04 04:14:38,481] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[84.98779]
 [85.25038]
 [85.26818]
 [85.20095]
 [85.10312]], R is [[84.89555359]
 [85.04660034]
 [85.19613647]
 [85.34417725]
 [85.02265167]].
[2019-04-04 04:14:44,117] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.0312388e-28 2.9380642e-23 1.9088077e-26 6.5278541e-25 1.2855879e-24
 1.5215373e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 04:14:44,118] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3405
[2019-04-04 04:14:44,132] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.33333333333333, 65.0, 0.0, 0.0, 26.0, 24.07974829941636, 0.09828364358355646, 0.0, 1.0, 43705.91617426986], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3990000.0000, 
sim time next is 3990600.0000, 
raw observation next is [-12.5, 66.0, 0.0, 0.0, 26.0, 24.05354233622288, 0.08391329806530168, 0.0, 1.0, 43720.17626365295], 
processed observation next is [1.0, 0.17391304347826086, 0.11634349030470914, 0.66, 0.0, 0.0, 0.6666666666666666, 0.5044618613519066, 0.5279710993551006, 0.0, 1.0, 0.20819131554120454], 
reward next is 0.7918, 
noisyNet noise sample is [array([1.4482093], dtype=float32), 0.25541934]. 
=============================================
[2019-04-04 04:14:52,571] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.8170529e-28 1.4683275e-24 2.4576692e-27 1.5205467e-25 4.3645090e-25
 3.7039352e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:14:52,572] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6439
[2019-04-04 04:14:52,592] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.666666666666667, 39.33333333333333, 144.6666666666667, 540.0, 26.0, 25.35755113229998, 0.439389323687092, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4203600.0000, 
sim time next is 4204200.0000, 
raw observation next is [2.833333333333333, 38.16666666666667, 129.3333333333333, 542.0, 26.0, 25.36100364117926, 0.4369635989185024, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.541089566020314, 0.3816666666666667, 0.43111111111111095, 0.5988950276243094, 0.6666666666666666, 0.6134169700982716, 0.6456545329728341, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9497447], dtype=float32), 0.41919717]. 
=============================================
[2019-04-04 04:15:04,251] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.2218310e-29 4.3046034e-25 7.5109809e-28 3.1835951e-25 9.3193420e-26
 7.9514419e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:15:04,252] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7655
[2019-04-04 04:15:04,268] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.0, 53.5, 121.0, 822.0, 26.0, 25.21759589717154, 0.3952088647809995, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4275000.0000, 
sim time next is 4275600.0000, 
raw observation next is [6.333333333333333, 53.0, 120.8333333333333, 826.1666666666666, 26.0, 25.20962818711787, 0.3959995971075088, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.6380424746075716, 0.53, 0.4027777777777777, 0.9128913443830571, 0.6666666666666666, 0.600802348926489, 0.6319998657025029, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1949598], dtype=float32), -0.5654167]. 
=============================================
[2019-04-04 04:15:16,784] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.9255094e-28 7.4010678e-24 9.6295534e-27 2.7856081e-25 2.1041246e-24
 3.4034632e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:15:16,785] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9443
[2019-04-04 04:15:16,796] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 55.0, 0.0, 0.0, 26.0, 25.36318799178657, 0.3884201963013729, 0.0, 1.0, 49950.99718044487], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4833600.0000, 
sim time next is 4834200.0000, 
raw observation next is [-1.0, 55.0, 0.0, 0.0, 26.0, 25.35164688489126, 0.3868827447784208, 0.0, 1.0, 44001.76980581765], 
processed observation next is [0.0, 0.9565217391304348, 0.4349030470914128, 0.55, 0.0, 0.0, 0.6666666666666666, 0.6126372404076049, 0.6289609149261403, 0.0, 1.0, 0.20953223717056021], 
reward next is 0.7905, 
noisyNet noise sample is [array([-0.33949044], dtype=float32), -1.2639308]. 
=============================================
[2019-04-04 04:15:17,166] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.3414560e-28 6.5270359e-24 5.6583317e-27 3.0140063e-25 2.1622468e-24
 1.1123825e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 04:15:17,168] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5584
[2019-04-04 04:15:17,249] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.5, 68.0, 141.0, 313.0, 26.0, 24.80509418951284, 0.3384438834282639, 0.0, 1.0, 8346.298220997533], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4869000.0000, 
sim time next is 4869600.0000, 
raw observation next is [-3.333333333333333, 67.0, 152.0, 290.6666666666667, 26.0, 25.28709075522744, 0.3763758335705075, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.37026777469990774, 0.67, 0.5066666666666667, 0.3211786372007367, 0.6666666666666666, 0.6072575629356199, 0.6254586111901692, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.32589817], dtype=float32), -0.5826728]. 
=============================================
[2019-04-04 04:15:17,425] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.2896265e-28 8.0048092e-25 2.4382735e-27 2.9061367e-25 3.3555503e-25
 2.0065559e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:15:17,425] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1517
[2019-04-04 04:15:17,454] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 37.0, 148.0, 742.0, 26.0, 25.14003225894111, 0.4387172676695759, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4804200.0000, 
sim time next is 4804800.0000, 
raw observation next is [3.0, 37.0, 139.5, 739.5, 26.0, 25.13864692412885, 0.4369948590713654, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.5457063711911359, 0.37, 0.465, 0.8171270718232044, 0.6666666666666666, 0.5948872436774041, 0.6456649530237885, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.55349284], dtype=float32), -0.06474849]. 
=============================================
[2019-04-04 04:15:25,076] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.2526132e-28 7.0352327e-24 2.1366544e-26 5.0404687e-25 1.0257189e-24
 6.5708661e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:15:25,079] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1152
[2019-04-04 04:15:25,100] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 50.0, 0.0, 0.0, 26.0, 25.41465357185336, 0.2967461376526185, 0.0, 1.0, 65475.10942325972], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4934400.0000, 
sim time next is 4935000.0000, 
raw observation next is [-1.0, 50.0, 0.0, 0.0, 26.0, 25.30702256213702, 0.298429718990426, 0.0, 1.0, 75984.14160278325], 
processed observation next is [1.0, 0.08695652173913043, 0.4349030470914128, 0.5, 0.0, 0.0, 0.6666666666666666, 0.6089185468447518, 0.5994765729968087, 0.0, 1.0, 0.3618292457275393], 
reward next is 0.6382, 
noisyNet noise sample is [array([-0.8589087], dtype=float32), -0.62979454]. 
=============================================
[2019-04-04 04:15:25,124] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[82.97186 ]
 [82.90641 ]
 [83.129875]
 [83.37577 ]
 [83.54896 ]], R is [[82.96993256]
 [82.82845306]
 [83.00016785]
 [83.17016602]
 [83.33846283]].
[2019-04-04 04:15:30,642] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:15:30,642] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:15:30,686] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run33
[2019-04-04 04:15:30,796] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.9955922e-30 1.0027792e-26 6.8961160e-29 1.7439795e-26 3.3386419e-27
 4.1274341e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 04:15:30,798] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0377
[2019-04-04 04:15:30,828] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.3333333333333334, 36.0, 105.5, 690.8333333333333, 26.0, 26.00615102632463, 0.4489912368381963, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4958400.0000, 
sim time next is 4959000.0000, 
raw observation next is [0.0, 34.5, 108.0, 717.0, 26.0, 26.08216394692588, 0.4704168524656509, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.46260387811634357, 0.345, 0.36, 0.7922651933701658, 0.6666666666666666, 0.6735136622438235, 0.6568056174885503, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6586466], dtype=float32), 0.15260203]. 
=============================================
[2019-04-04 04:15:30,848] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[91.119705]
 [91.23204 ]
 [91.23329 ]
 [91.25456 ]
 [91.48888 ]], R is [[90.96063232]
 [91.05102539]
 [91.14051819]
 [91.22911072]
 [91.31681824]].
[2019-04-04 04:15:32,469] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:15:32,469] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:15:32,473] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run33
[2019-04-04 04:15:35,597] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:15:35,597] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:15:35,606] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run33
[2019-04-04 04:15:39,212] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:15:39,237] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:15:39,251] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run33
[2019-04-04 04:15:43,015] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:15:43,015] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:15:43,019] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run33
[2019-04-04 04:15:44,640] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:15:44,641] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:15:44,644] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run33
[2019-04-04 04:15:45,604] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:15:45,604] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:15:45,608] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run33
[2019-04-04 04:15:45,706] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:15:45,706] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:15:45,711] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run33
[2019-04-04 04:15:48,280] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:15:48,280] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:15:48,284] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run33
[2019-04-04 04:15:48,699] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:15:48,699] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:15:48,703] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run33
[2019-04-04 04:15:51,045] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:15:51,047] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:15:51,050] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run33
[2019-04-04 04:15:51,550] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:15:51,550] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:15:51,554] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run33
[2019-04-04 04:15:53,926] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:15:53,927] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:15:53,931] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run33
[2019-04-04 04:15:55,708] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:15:55,709] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:15:55,712] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run33
[2019-04-04 04:15:56,089] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:15:56,089] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:15:56,118] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run33
[2019-04-04 04:15:58,381] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.68391274e-27 2.99962592e-23 1.88028374e-25 1.31184997e-23
 2.19961873e-23 1.19056174e-26 1.00000000e+00], sum to 1.0000
[2019-04-04 04:15:58,381] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9919
[2019-04-04 04:15:58,479] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.2, 86.0, 64.5, 0.0, 26.0, 24.472653285526, 0.1620706657024818, 0.0, 1.0, 46211.44080813207], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 54000.0000, 
sim time next is 54600.0000, 
raw observation next is [7.1, 85.33333333333334, 59.66666666666666, 0.0, 26.0, 24.45557447856088, 0.1660952768867109, 0.0, 1.0, 53856.95906287082], 
processed observation next is [0.0, 0.6521739130434783, 0.6592797783933518, 0.8533333333333334, 0.19888888888888887, 0.0, 0.6666666666666666, 0.5379645398800733, 0.5553650922955703, 0.0, 1.0, 0.2564617098231944], 
reward next is 0.7435, 
noisyNet noise sample is [array([-0.09178845], dtype=float32), -0.17002651]. 
=============================================
[2019-04-04 04:15:59,999] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.9810728e-27 1.1164360e-23 1.4322900e-25 2.8718314e-24 7.0814363e-24
 1.4400962e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 04:15:59,999] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2678
[2019-04-04 04:16:00,028] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.9666666666666667, 91.0, 0.0, 0.0, 26.0, 24.32998788235721, 0.1452704966806129, 0.0, 1.0, 41215.53058806681], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 91200.0000, 
sim time next is 91800.0000, 
raw observation next is [-1.15, 91.0, 0.0, 0.0, 26.0, 24.35063198306668, 0.1370149550293152, 0.0, 1.0, 41418.3595285769], 
processed observation next is [1.0, 0.043478260869565216, 0.4307479224376732, 0.91, 0.0, 0.0, 0.6666666666666666, 0.5292193319222234, 0.5456716516764384, 0.0, 1.0, 0.1972302834694138], 
reward next is 0.8028, 
noisyNet noise sample is [array([-0.07091368], dtype=float32), 1.3709396]. 
=============================================
[2019-04-04 04:16:00,921] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:16:00,921] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:16:00,924] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run33
[2019-04-04 04:16:03,583] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.6543750e-27 3.8320291e-24 9.8035030e-26 1.1214685e-23 6.3165020e-24
 1.7936299e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 04:16:03,583] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9772
[2019-04-04 04:16:03,609] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.4, 71.0, 0.0, 0.0, 26.0, 24.25062868400297, 0.1334393292707116, 0.0, 1.0, 45092.01685133071], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 166200.0000, 
sim time next is 166800.0000, 
raw observation next is [-8.4, 71.0, 0.0, 0.0, 26.0, 24.21407745904903, 0.1291643945587169, 0.0, 1.0, 45104.45373343657], 
processed observation next is [1.0, 0.9565217391304348, 0.2299168975069252, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5178397882540858, 0.5430547981862389, 0.0, 1.0, 0.2147831130163646], 
reward next is 0.7852, 
noisyNet noise sample is [array([-1.9273095], dtype=float32), 0.36527362]. 
=============================================
[2019-04-04 04:16:10,900] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.6646488e-28 8.9981261e-25 1.5220664e-26 2.5141033e-24 7.3226004e-25
 2.8988356e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:16:10,900] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8088
[2019-04-04 04:16:10,973] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.0, 77.66666666666667, 185.0, 16.83333333333333, 26.0, 25.3771824656761, 0.3185155094839756, 1.0, 1.0, 45541.42234657752], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 127200.0000, 
sim time next is 127800.0000, 
raw observation next is [-8.1, 73.5, 184.0, 13.0, 26.0, 25.40651119557062, 0.3215370130039465, 1.0, 1.0, 46251.000549406], 
processed observation next is [1.0, 0.4782608695652174, 0.23822714681440446, 0.735, 0.6133333333333333, 0.014364640883977901, 0.6666666666666666, 0.6172092662975516, 0.6071790043346489, 1.0, 1.0, 0.2202428597590762], 
reward next is 0.7798, 
noisyNet noise sample is [array([2.1913059], dtype=float32), -0.47948307]. 
=============================================
[2019-04-04 04:16:18,658] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.9357859e-28 8.6071913e-25 2.7709491e-26 1.7387800e-24 2.1190587e-24
 5.2067545e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:16:18,658] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7577
[2019-04-04 04:16:18,741] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.5, 40.0, 0.0, 0.0, 26.0, 25.60628907505891, 0.3549069539082217, 1.0, 1.0, 41764.94630336553], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 411600.0000, 
sim time next is 412200.0000, 
raw observation next is [-9.5, 40.0, 0.0, 0.0, 26.0, 25.52947609459156, 0.3412758905877049, 1.0, 1.0, 45226.46459877052], 
processed observation next is [1.0, 0.782608695652174, 0.1994459833795014, 0.4, 0.0, 0.0, 0.6666666666666666, 0.6274563412159632, 0.6137586301959016, 1.0, 1.0, 0.2153641171370025], 
reward next is 0.7846, 
noisyNet noise sample is [array([-0.24107012], dtype=float32), 0.79319775]. 
=============================================
[2019-04-04 04:16:33,677] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.8301521e-27 4.3299370e-23 1.3340840e-25 3.2553288e-24 7.1669331e-24
 1.0025229e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 04:16:33,677] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5597
[2019-04-04 04:16:33,705] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-16.1, 77.16666666666666, 0.0, 0.0, 26.0, 21.9875273075273, -0.4363105681234463, 0.0, 1.0, 48660.89965763258], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 366600.0000, 
sim time next is 367200.0000, 
raw observation next is [-16.2, 78.0, 0.0, 0.0, 26.0, 21.88158139127582, -0.4563679281572835, 0.0, 1.0, 48767.95328910237], 
processed observation next is [1.0, 0.2608695652173913, 0.013850415512465375, 0.78, 0.0, 0.0, 0.6666666666666666, 0.3234651159396516, 0.3478773572809055, 0.0, 1.0, 0.23222834899572556], 
reward next is 0.7678, 
noisyNet noise sample is [array([-0.54897606], dtype=float32), -1.1695192]. 
=============================================
[2019-04-04 04:16:35,057] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.8358945e-28 9.2717958e-25 1.2586308e-26 1.9287625e-24 1.2415366e-24
 1.3070163e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:16:35,058] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7474
[2019-04-04 04:16:35,180] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.41666666666667, 47.83333333333334, 16.0, 162.0, 26.0, 25.48358208083948, 0.4236909948247199, 1.0, 1.0, 174129.8525643432], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 319800.0000, 
sim time next is 320400.0000, 
raw observation next is [-10.6, 49.0, 12.0, 123.0, 26.0, 25.73980139223423, 0.3037176839206601, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.1689750692520776, 0.49, 0.04, 0.13591160220994475, 0.6666666666666666, 0.6449834493528526, 0.6012392279735533, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6682343], dtype=float32), 1.2619516]. 
=============================================
[2019-04-04 04:16:59,409] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [6.5316802e-28 2.4483622e-24 3.7976528e-26 1.6347480e-24 1.8773677e-24
 3.6261775e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 04:16:59,410] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.0187
[2019-04-04 04:16:59,466] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 87.0, 0.0, 0.0, 26.0, 24.97592635739803, 0.3012205881477752, 0.0, 1.0, 79433.76675624374], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 588600.0000, 
sim time next is 589200.0000, 
raw observation next is [-2.8, 87.0, 0.0, 0.0, 26.0, 25.02209767219758, 0.3059074048109434, 0.0, 1.0, 55787.61756989258], 
processed observation next is [0.0, 0.8260869565217391, 0.38504155124653744, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5851748060164651, 0.6019691349369811, 0.0, 1.0, 0.2656553217613932], 
reward next is 0.7343, 
noisyNet noise sample is [array([-0.8321698], dtype=float32), 0.20866452]. 
=============================================
[2019-04-04 04:17:03,095] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1205469e-27 1.4555351e-23 3.7530366e-26 1.9001642e-24 2.6799881e-24
 5.5433044e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 04:17:03,095] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3964
[2019-04-04 04:17:03,123] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 80.5, 0.0, 0.0, 26.0, 24.22128922904358, 0.1033258356827847, 0.0, 1.0, 42430.7967819286], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 613800.0000, 
sim time next is 614400.0000, 
raw observation next is [-3.899999999999999, 78.66666666666667, 0.0, 0.0, 26.0, 24.217308654015, 0.09729659183748807, 0.0, 1.0, 42541.65688489065], 
processed observation next is [0.0, 0.08695652173913043, 0.35457063711911363, 0.7866666666666667, 0.0, 0.0, 0.6666666666666666, 0.51810905450125, 0.5324321972791627, 0.0, 1.0, 0.2025793184994793], 
reward next is 0.7974, 
noisyNet noise sample is [array([0.3565942], dtype=float32), 0.56975466]. 
=============================================
[2019-04-04 04:17:11,712] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.26119955e-30 9.67916153e-27 1.21307936e-28 5.20157586e-27
 9.61237721e-27 1.75852601e-30 1.00000000e+00], sum to 1.0000
[2019-04-04 04:17:11,712] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8628
[2019-04-04 04:17:11,744] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.3, 75.5, 0.0, 0.0, 26.0, 26.04438381511459, 0.6293779687063353, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1041000.0000, 
sim time next is 1041600.0000, 
raw observation next is [14.2, 76.0, 0.0, 0.0, 26.0, 26.02685733059183, 0.6200976319829445, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.8559556786703602, 0.76, 0.0, 0.0, 0.6666666666666666, 0.6689047775493192, 0.7066992106609815, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7877383], dtype=float32), -0.594478]. 
=============================================
[2019-04-04 04:17:12,046] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.0966069e-29 3.7167359e-25 3.2290421e-27 2.9998158e-26 5.1497545e-25
 8.9177026e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:17:12,053] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3585
[2019-04-04 04:17:12,065] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 74.66666666666667, 0.0, 0.0, 26.0, 24.71410161706071, 0.1821757032946421, 0.0, 1.0, 39009.24921612224], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 879600.0000, 
sim time next is 880200.0000, 
raw observation next is [-0.8999999999999999, 74.0, 0.0, 0.0, 26.0, 24.64434960797146, 0.1757762266129836, 0.0, 1.0, 39031.10242158404], 
processed observation next is [1.0, 0.17391304347826086, 0.43767313019390586, 0.74, 0.0, 0.0, 0.6666666666666666, 0.5536958006642884, 0.5585920755376612, 0.0, 1.0, 0.1858623924837335], 
reward next is 0.8141, 
noisyNet noise sample is [array([-0.04914555], dtype=float32), -0.59191513]. 
=============================================
[2019-04-04 04:17:12,825] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.11681172e-29 1.02906924e-25 1.04395071e-27 6.75838461e-26
 8.28345242e-26 4.32843757e-29 1.00000000e+00], sum to 1.0000
[2019-04-04 04:17:12,825] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1275
[2019-04-04 04:17:12,865] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.3, 71.0, 0.0, 0.0, 26.0, 24.36194490612996, 0.1583037248834525, 0.0, 1.0, 41881.33905127856], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 777600.0000, 
sim time next is 778200.0000, 
raw observation next is [-7.3, 71.0, 0.0, 0.0, 26.0, 24.32511007063034, 0.1499228247635609, 0.0, 1.0, 41810.71164456981], 
processed observation next is [1.0, 0.0, 0.26038781163434904, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5270925058858618, 0.549974274921187, 0.0, 1.0, 0.19909862687890387], 
reward next is 0.8009, 
noisyNet noise sample is [array([-1.5181054], dtype=float32), 0.51351583]. 
=============================================
[2019-04-04 04:17:13,664] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.3988363e-31 4.1296294e-27 9.5909042e-29 3.1253879e-27 7.3204705e-27
 5.5510195e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 04:17:13,667] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0149
[2019-04-04 04:17:13,690] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 26.0, 25.60585274232056, 0.5860012699815808, 0.0, 1.0, 34251.74005806723], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1036800.0000, 
sim time next is 1037400.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 26.0, 25.62811346957547, 0.6082423338234634, 0.0, 1.0, 18730.29253464766], 
processed observation next is [1.0, 0.0, 0.8614958448753465, 0.75, 0.0, 0.0, 0.6666666666666666, 0.6356761224646226, 0.7027474446078211, 0.0, 1.0, 0.0891918692126079], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.6928582], dtype=float32), 0.2510612]. 
=============================================
[2019-04-04 04:17:13,936] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.5190884e-29 1.2813130e-25 1.3231150e-27 5.2899046e-26 6.9177579e-26
 1.4882702e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:17:13,937] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5222
[2019-04-04 04:17:13,958] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.300000000000001, 71.0, 0.0, 0.0, 26.0, 24.28866115930475, 0.1416268004420537, 0.0, 1.0, 41753.68684611459], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 778800.0000, 
sim time next is 779400.0000, 
raw observation next is [-7.3, 71.0, 0.0, 0.0, 26.0, 24.25277695427169, 0.1386421519142427, 0.0, 1.0, 41705.45994597278], 
processed observation next is [1.0, 0.0, 0.26038781163434904, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5210647461893073, 0.5462140506380809, 0.0, 1.0, 0.1985974283141561], 
reward next is 0.8014, 
noisyNet noise sample is [array([1.6108681], dtype=float32), 1.1222751]. 
=============================================
[2019-04-04 04:17:14,421] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.9616299e-28 2.6494323e-24 1.9783752e-26 6.0305340e-25 4.4436185e-25
 6.6511625e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:17:14,421] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7141
[2019-04-04 04:17:14,435] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.8, 73.5, 0.0, 0.0, 26.0, 23.8994677868201, 0.04185673211411756, 0.0, 1.0, 41388.60063178797], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 787800.0000, 
sim time next is 788400.0000, 
raw observation next is [-7.8, 74.0, 0.0, 0.0, 26.0, 23.87076509924893, 0.04655306624437628, 0.0, 1.0, 41332.08139678056], 
processed observation next is [1.0, 0.13043478260869565, 0.24653739612188366, 0.74, 0.0, 0.0, 0.6666666666666666, 0.4892304249374109, 0.5155176887481254, 0.0, 1.0, 0.19681943522276457], 
reward next is 0.8032, 
noisyNet noise sample is [array([-0.2858272], dtype=float32), -0.5557212]. 
=============================================
[2019-04-04 04:17:21,230] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.7719880e-29 3.1448236e-26 5.3980619e-28 3.9096926e-26 4.0691609e-26
 6.2033914e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 04:17:21,231] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9368
[2019-04-04 04:17:21,279] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.733333333333333, 85.0, 0.0, 0.0, 26.0, 25.16248379120529, 0.2941370464990105, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 847200.0000, 
sim time next is 847800.0000, 
raw observation next is [-3.65, 84.5, 0.0, 0.0, 26.0, 25.04763481759518, 0.2681150388315321, 1.0, 1.0, 24720.62871611482], 
processed observation next is [1.0, 0.8260869565217391, 0.3614958448753463, 0.845, 0.0, 0.0, 0.6666666666666666, 0.5873029014662651, 0.5893716796105107, 1.0, 1.0, 0.11771727960054677], 
reward next is 0.8823, 
noisyNet noise sample is [array([0.24503058], dtype=float32), 0.7595466]. 
=============================================
[2019-04-04 04:17:21,879] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.8907494e-30 1.3921945e-26 2.7566921e-28 3.3830408e-26 1.3076262e-26
 1.9463922e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 04:17:21,879] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4750
[2019-04-04 04:17:21,923] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.9, 92.16666666666667, 12.0, 0.0, 26.0, 24.51399361889501, 0.310557693130901, 1.0, 1.0, 196475.6107603647], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 924600.0000, 
sim time next is 925200.0000, 
raw observation next is [5.0, 92.0, 9.0, 0.0, 26.0, 25.00417326812636, 0.3543358689796134, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.6011080332409973, 0.92, 0.03, 0.0, 0.6666666666666666, 0.5836811056771968, 0.6181119563265378, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0404105], dtype=float32), -0.28970784]. 
=============================================
[2019-04-04 04:17:24,227] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.8869172e-31 2.2204558e-26 1.2719365e-28 3.0916403e-27 4.2388000e-27
 2.7148621e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 04:17:24,228] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1603
[2019-04-04 04:17:24,249] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.7, 82.5, 0.0, 0.0, 26.0, 25.47566013077125, 0.4553487707229321, 0.0, 1.0, 18756.06842844894], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 964200.0000, 
sim time next is 964800.0000, 
raw observation next is [7.7, 83.0, 0.0, 0.0, 26.0, 25.53948608622502, 0.4476349900520018, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.6759002770083103, 0.83, 0.0, 0.0, 0.6666666666666666, 0.6282905071854182, 0.6492116633506673, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.7541248], dtype=float32), 1.553914]. 
=============================================
[2019-04-04 04:17:35,109] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.9270193e-31 8.5024938e-27 1.5443296e-29 8.4219174e-28 1.8384775e-27
 2.6515476e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 04:17:35,110] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3124
[2019-04-04 04:17:35,125] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.51666666666667, 80.5, 0.0, 0.0, 26.0, 25.65641263715488, 0.6139218740931556, 0.0, 1.0, 18727.07530330078], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1147800.0000, 
sim time next is 1148400.0000, 
raw observation next is [12.7, 80.0, 0.0, 0.0, 26.0, 25.666165853522, 0.6142114277117167, 0.0, 1.0, 18726.33814551247], 
processed observation next is [0.0, 0.30434782608695654, 0.8144044321329641, 0.8, 0.0, 0.0, 0.6666666666666666, 0.6388471544601666, 0.7047371425705723, 0.0, 1.0, 0.08917303878815462], 
reward next is 0.9108, 
noisyNet noise sample is [array([-0.05572626], dtype=float32), 0.79259735]. 
=============================================
[2019-04-04 04:17:39,142] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.4392106e-30 6.3141345e-26 1.1744350e-27 5.6159507e-26 4.1983618e-26
 2.5917369e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:17:39,143] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5333
[2019-04-04 04:17:39,155] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.3, 97.5, 0.0, 0.0, 26.0, 25.48076951863624, 0.4664960025111042, 0.0, 1.0, 9378.596273429177], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1395000.0000, 
sim time next is 1395600.0000, 
raw observation next is [-0.4, 98.33333333333333, 0.0, 0.0, 26.0, 25.35517058604645, 0.4634982105434542, 0.0, 1.0, 73014.46563423058], 
processed observation next is [1.0, 0.13043478260869565, 0.45152354570637127, 0.9833333333333333, 0.0, 0.0, 0.6666666666666666, 0.6129308821705376, 0.6544994035144848, 0.0, 1.0, 0.3476879315915742], 
reward next is 0.6523, 
noisyNet noise sample is [array([-0.48054242], dtype=float32), 1.3552483]. 
=============================================
[2019-04-04 04:18:01,079] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.9991687e-28 3.2275076e-24 5.8312694e-27 1.5916732e-24 1.4664187e-24
 2.3475488e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:18:01,079] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1183
[2019-04-04 04:18:01,112] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.35, 91.5, 0.0, 0.0, 26.0, 25.32528733129514, 0.456620992497031, 0.0, 1.0, 43096.09348582705], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1733400.0000, 
sim time next is 1734000.0000, 
raw observation next is [0.3, 91.33333333333334, 0.0, 0.0, 26.0, 25.34037666456886, 0.4534682181767955, 0.0, 1.0, 42946.15967838938], 
processed observation next is [0.0, 0.043478260869565216, 0.47091412742382277, 0.9133333333333334, 0.0, 0.0, 0.6666666666666666, 0.6116980553807384, 0.6511560727255985, 0.0, 1.0, 0.20450552227804464], 
reward next is 0.7955, 
noisyNet noise sample is [array([-0.38128722], dtype=float32), -0.42806876]. 
=============================================
[2019-04-04 04:18:01,167] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[82.29867]
 [82.33129]
 [82.39654]
 [82.39457]
 [82.38727]], R is [[82.20877838]
 [82.18146515]
 [82.15258789]
 [82.12051392]
 [82.09096527]].
[2019-04-04 04:18:07,337] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.1096627e-31 9.6921625e-28 4.5252408e-29 4.3295321e-27 6.3097013e-27
 4.9600370e-31 1.0000000e+00], sum to 1.0000
[2019-04-04 04:18:07,338] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4329
[2019-04-04 04:18:07,466] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.35, 57.5, 0.0, 0.0, 26.0, 26.8092742177253, 0.7372736782181959, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1618200.0000, 
sim time next is 1618800.0000, 
raw observation next is [11.06666666666667, 58.66666666666667, 0.0, 0.0, 26.0, 26.77745517967717, 0.7527409961793613, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.7691597414589106, 0.5866666666666667, 0.0, 0.0, 0.6666666666666666, 0.7314545983064308, 0.7509136653931204, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4125856], dtype=float32), -0.36329284]. 
=============================================
[2019-04-04 04:18:22,789] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-04 04:18:22,792] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 04:18:22,792] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:18:22,794] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run45
[2019-04-04 04:18:22,844] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 04:18:22,844] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:18:22,846] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run45
[2019-04-04 04:18:22,911] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 04:18:22,911] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:18:22,914] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run45
[2019-04-04 04:19:33,610] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.36556494], dtype=float32), 0.22468479]
[2019-04-04 04:19:33,611] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-0.3, 97.5, 46.0, 0.0, 26.0, 25.94719382034214, 0.5155366606126869, 1.0, 1.0, 0.0]
[2019-04-04 04:19:33,611] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 04:19:33,612] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [8.4103230e-29 6.9020251e-25 3.1570824e-27 2.3002854e-25 2.4683828e-25
 2.5865528e-28 1.0000000e+00], sampled 0.93144353332896
[2019-04-04 04:20:10,024] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.36556494], dtype=float32), 0.22468479]
[2019-04-04 04:20:10,024] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-4.460405854499999, 83.51705010500001, 0.0, 0.0, 26.0, 23.96880568437567, 0.04590242979843361, 0.0, 1.0, 41688.72128427175]
[2019-04-04 04:20:10,025] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 04:20:10,026] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.3281529e-27 3.5992009e-23 1.0945196e-25 3.3607530e-24 6.2216584e-24
 1.5970236e-26 1.0000000e+00], sampled 0.5598028333544028
[2019-04-04 04:20:50,530] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.36556494], dtype=float32), 0.22468479]
[2019-04-04 04:20:50,530] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-3.9, 55.83333333333334, 0.0, 0.0, 26.0, 25.6350961737907, 0.5015092742894915, 1.0, 1.0, 0.0]
[2019-04-04 04:20:50,530] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 04:20:50,531] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [7.70909938e-28 1.05557734e-24 1.25400126e-26 1.73121521e-24
 1.60044051e-24 6.45437721e-28 1.00000000e+00], sampled 0.7939057407956941
[2019-04-04 04:21:30,033] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 04:22:02,593] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 04:22:05,404] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 04:22:06,431] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 4400000, evaluation results [4400000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 04:22:09,893] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.6362062e-27 2.9403402e-23 5.1469072e-25 1.3247338e-23 2.3805878e-23
 3.1017656e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 04:22:09,926] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9728
[2019-04-04 04:22:09,987] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.733333333333334, 80.66666666666667, 0.0, 0.0, 26.0, 23.64073059260665, -0.0847732708615251, 0.0, 1.0, 45300.62290424366], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1917600.0000, 
sim time next is 1918200.0000, 
raw observation next is [-8.816666666666666, 81.33333333333334, 0.0, 0.0, 26.0, 23.6238452081551, -0.08296313703098428, 0.0, 1.0, 45285.87087959125], 
processed observation next is [1.0, 0.17391304347826086, 0.21837488457987075, 0.8133333333333335, 0.0, 0.0, 0.6666666666666666, 0.4686537673462583, 0.4723456209896719, 0.0, 1.0, 0.21564700418852978], 
reward next is 0.7844, 
noisyNet noise sample is [array([1.996691], dtype=float32), -0.915233]. 
=============================================
[2019-04-04 04:22:25,836] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.2087741e-28 5.0055187e-25 1.0962029e-26 2.9138185e-25 1.3558328e-24
 6.3341034e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:22:25,836] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6149
[2019-04-04 04:22:25,849] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 85.66666666666667, 0.0, 0.0, 26.0, 24.37351436662654, 0.1543059894539502, 0.0, 1.0, 41817.4681339063], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1992000.0000, 
sim time next is 1992600.0000, 
raw observation next is [-5.9, 85.0, 0.0, 0.0, 26.0, 24.3671772717119, 0.1432403617732327, 0.0, 1.0, 41756.16565721299], 
processed observation next is [1.0, 0.043478260869565216, 0.2991689750692521, 0.85, 0.0, 0.0, 0.6666666666666666, 0.5305981059759916, 0.5477467872577443, 0.0, 1.0, 0.1988388840819666], 
reward next is 0.8012, 
noisyNet noise sample is [array([1.9747447], dtype=float32), 0.5540517]. 
=============================================
[2019-04-04 04:22:28,643] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [8.6472256e-29 3.7432155e-25 3.5867895e-27 4.4486391e-25 2.0235622e-25
 5.8866019e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:22:28,643] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.9541
[2019-04-04 04:22:28,735] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.816666666666666, 78.33333333333334, 0.0, 0.0, 26.0, 25.33151688927054, 0.3483495085572961, 1.0, 1.0, 24675.21412799677], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1965000.0000, 
sim time next is 1965600.0000, 
raw observation next is [-5.0, 79.0, 0.0, 0.0, 26.0, 25.37753790425285, 0.3459471264915204, 1.0, 1.0, 33762.73584754415], 
processed observation next is [1.0, 0.782608695652174, 0.32409972299168976, 0.79, 0.0, 0.0, 0.6666666666666666, 0.6147948253544042, 0.6153157088305068, 1.0, 1.0, 0.1607749326073531], 
reward next is 0.8392, 
noisyNet noise sample is [array([0.95531684], dtype=float32), -0.16527572]. 
=============================================
[2019-04-04 04:22:39,492] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.7475666e-29 8.2719585e-26 1.3591501e-27 9.1121804e-26 4.6230984e-26
 4.4185399e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 04:22:39,492] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9737
[2019-04-04 04:22:39,532] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.1, 63.0, 161.0, 110.0, 26.0, 25.69440480523112, 0.3492804294450551, 1.0, 1.0, 50780.41963802768], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2287800.0000, 
sim time next is 2288400.0000, 
raw observation next is [-3.8, 61.33333333333333, 177.8333333333333, 104.0, 26.0, 25.69052960603648, 0.3550978272269131, 1.0, 1.0, 32497.78292782655], 
processed observation next is [1.0, 0.4782608695652174, 0.3573407202216067, 0.6133333333333333, 0.5927777777777776, 0.11491712707182321, 0.6666666666666666, 0.6408774671697067, 0.618365942408971, 1.0, 1.0, 0.15475134727536452], 
reward next is 0.8452, 
noisyNet noise sample is [array([-0.89176947], dtype=float32), -0.15237118]. 
=============================================
[2019-04-04 04:22:48,543] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.1852134e-28 5.9925385e-24 2.6972877e-26 1.1774828e-24 1.5458659e-24
 8.5337157e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:22:48,543] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8142
[2019-04-04 04:22:48,593] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.2, 79.0, 0.0, 0.0, 26.0, 23.77279941898912, 0.007508376851335319, 0.0, 1.0, 41901.31589277943], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2181600.0000, 
sim time next is 2182200.0000, 
raw observation next is [-6.100000000000001, 78.33333333333334, 0.0, 0.0, 26.0, 23.75849940046304, -0.0003213479278438523, 0.0, 1.0, 41891.66792788614], 
processed observation next is [1.0, 0.2608695652173913, 0.2936288088642659, 0.7833333333333334, 0.0, 0.0, 0.6666666666666666, 0.47987495003858677, 0.499892884024052, 0.0, 1.0, 0.199484132989934], 
reward next is 0.8005, 
noisyNet noise sample is [array([1.1218463], dtype=float32), -1.3394231]. 
=============================================
[2019-04-04 04:22:58,781] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.6951938e-31 1.8391720e-27 3.2567737e-29 6.7659652e-27 1.7878236e-27
 5.5909911e-31 1.0000000e+00], sum to 1.0000
[2019-04-04 04:22:58,781] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1258
[2019-04-04 04:22:58,797] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.03333333333333333, 44.33333333333334, 215.5, 66.66666666666667, 26.0, 25.6720165675737, 0.3071277931425497, 1.0, 1.0, 18682.0374422987], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2546400.0000, 
sim time next is 2547000.0000, 
raw observation next is [0.2500000000000001, 43.0, 232.0, 71.0, 26.0, 25.68301449811121, 0.319259056854339, 1.0, 1.0, 18682.31023118947], 
processed observation next is [1.0, 0.4782608695652174, 0.46952908587257625, 0.43, 0.7733333333333333, 0.07845303867403315, 0.6666666666666666, 0.6402512081759341, 0.606419685618113, 1.0, 1.0, 0.08896338205328319], 
reward next is 0.9110, 
noisyNet noise sample is [array([-1.3628246], dtype=float32), -0.9856168]. 
=============================================
[2019-04-04 04:22:58,804] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[87.2076  ]
 [87.17466 ]
 [87.1555  ]
 [87.070595]
 [86.83984 ]], R is [[87.38295746]
 [87.42017365]
 [87.54597473]
 [87.67051697]
 [87.79381561]].
[2019-04-04 04:23:04,763] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.1552472e-29 8.0845627e-26 4.1233154e-28 3.2371961e-26 9.7961382e-26
 3.1548881e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:23:04,764] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4551
[2019-04-04 04:23:04,790] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.2, 49.0, 134.5, 39.0, 26.0, 25.8239492257968, 0.3076850075449724, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2541600.0000, 
sim time next is 2542200.0000, 
raw observation next is [-1.1, 48.66666666666667, 134.0, 41.0, 26.0, 25.84144305470994, 0.3043989087774404, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4321329639889197, 0.4866666666666667, 0.44666666666666666, 0.045303867403314914, 0.6666666666666666, 0.6534535878924951, 0.6014663029258135, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2377108], dtype=float32), 0.98524046]. 
=============================================
[2019-04-04 04:23:06,336] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.5922769e-30 1.0496380e-25 8.6258156e-28 9.3578514e-26 8.2911660e-26
 1.7839657e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:23:06,336] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0340
[2019-04-04 04:23:06,383] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.9, 57.0, 0.0, 0.0, 26.0, 25.37372297773946, 0.3746837254952223, 0.0, 1.0, 38929.22209528348], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2326800.0000, 
sim time next is 2327400.0000, 
raw observation next is [-2.0, 57.5, 0.0, 0.0, 26.0, 25.33105744933168, 0.3653145147663617, 0.0, 1.0, 39646.36398118924], 
processed observation next is [1.0, 0.9565217391304348, 0.40720221606648205, 0.575, 0.0, 0.0, 0.6666666666666666, 0.6109214541109734, 0.6217715049221205, 0.0, 1.0, 0.1887922094342345], 
reward next is 0.8112, 
noisyNet noise sample is [array([-0.16376404], dtype=float32), -0.3966068]. 
=============================================
[2019-04-04 04:23:24,475] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.2183400e-29 7.6543975e-26 2.5341271e-27 1.9456622e-25 6.3055655e-26
 2.4642569e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:23:24,475] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1128
[2019-04-04 04:23:24,482] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 100.0, 169.5, 0.0, 26.0, 25.20088421720227, 0.3151777457167335, 1.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2898000.0000, 
sim time next is 2898600.0000, 
raw observation next is [2.0, 100.0, 167.6666666666667, 0.0, 26.0, 25.25210464605741, 0.3257126479772009, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.518005540166205, 1.0, 0.5588888888888891, 0.0, 0.6666666666666666, 0.6043420538381176, 0.608570882659067, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.30895403], dtype=float32), 0.50200266]. 
=============================================
[2019-04-04 04:23:25,489] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.0331027e-29 2.8557258e-26 7.4834964e-28 1.6815836e-25 4.9663355e-26
 2.1105424e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:23:25,512] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.3231
[2019-04-04 04:23:25,530] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.833333333333333, 56.66666666666667, 227.5, 167.0, 26.0, 25.71635767182471, 0.3867700690522657, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2634000.0000, 
sim time next is 2634600.0000, 
raw observation next is [-2.566666666666666, 55.33333333333333, 231.0, 163.0, 26.0, 25.72680383329119, 0.3928679337030537, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.39150507848568794, 0.5533333333333332, 0.77, 0.18011049723756906, 0.6666666666666666, 0.6439003194409324, 0.6309559779010179, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.823265], dtype=float32), -0.59735817]. 
=============================================
[2019-04-04 04:23:37,415] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.1168732e-27 1.0782060e-22 1.5084145e-25 6.4198408e-24 7.3557801e-24
 2.4553277e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 04:23:37,415] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2055
[2019-04-04 04:23:37,434] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 74.66666666666667, 0.0, 0.0, 26.0, 23.85988515543255, 0.02146623851106124, 0.0, 1.0, 40163.87637566785], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3043200.0000, 
sim time next is 3043800.0000, 
raw observation next is [-6.0, 73.5, 0.0, 0.0, 26.0, 23.86261493293577, 0.01665380014424284, 0.0, 1.0, 40147.55442159386], 
processed observation next is [0.0, 0.21739130434782608, 0.296398891966759, 0.735, 0.0, 0.0, 0.6666666666666666, 0.4885512444113142, 0.5055512667147476, 0.0, 1.0, 0.19117883057901836], 
reward next is 0.8088, 
noisyNet noise sample is [array([0.9039583], dtype=float32), -0.81343675]. 
=============================================
[2019-04-04 04:23:39,972] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.6703121e-29 1.0881168e-25 4.2831742e-27 3.2327429e-25 2.5115236e-25
 1.0264292e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:23:39,972] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4095
[2019-04-04 04:23:40,027] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 92.5, 0.0, 0.0, 26.0, 25.00831032712875, 0.3672727387030481, 1.0, 1.0, 85162.16511579743], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2917800.0000, 
sim time next is 2918400.0000, 
raw observation next is [-0.3333333333333333, 92.33333333333333, 0.0, 0.0, 26.0, 24.91900329407454, 0.3910486897722302, 1.0, 1.0, 121877.1629080823], 
processed observation next is [1.0, 0.782608695652174, 0.4533702677747, 0.9233333333333333, 0.0, 0.0, 0.6666666666666666, 0.5765836078395449, 0.63034956325741, 1.0, 1.0, 0.5803674424194395], 
reward next is 0.4196, 
noisyNet noise sample is [array([0.818702], dtype=float32), -0.46603024]. 
=============================================
[2019-04-04 04:23:43,644] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.4543388e-27 1.1589938e-23 3.0717982e-26 1.0042157e-24 2.5803100e-24
 1.9892944e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 04:23:43,646] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1601
[2019-04-04 04:23:43,702] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.666666666666667, 58.16666666666667, 93.66666666666667, 543.0, 26.0, 25.39736984913916, 0.3248465214218544, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3057000.0000, 
sim time next is 3057600.0000, 
raw observation next is [-5.333333333333333, 57.33333333333334, 96.33333333333333, 589.0000000000001, 26.0, 25.37026755062598, 0.3244001449059064, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.3148661126500462, 0.5733333333333335, 0.32111111111111107, 0.650828729281768, 0.6666666666666666, 0.6141889625521649, 0.6081333816353022, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6816125], dtype=float32), -0.7216906]. 
=============================================
[2019-04-04 04:23:43,825] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [9.9686266e-29 7.8460904e-25 2.1595326e-26 4.7218966e-25 6.6706872e-25
 2.9003843e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:23:43,825] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6164
[2019-04-04 04:23:43,880] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 93.0, 77.0, 78.0, 26.0, 25.34201775090492, 0.3161023597650016, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2883600.0000, 
sim time next is 2884200.0000, 
raw observation next is [0.8333333333333334, 94.16666666666666, 67.66666666666666, 51.99999999999999, 26.0, 25.3906590342895, 0.3090531820027545, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4856879039704525, 0.9416666666666665, 0.22555555555555554, 0.0574585635359116, 0.6666666666666666, 0.6158882528574582, 0.6030177273342515, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7710641], dtype=float32), -0.41894332]. 
=============================================
[2019-04-04 04:23:45,280] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.2094555e-28 4.3375560e-24 2.7900201e-26 1.5758924e-24 2.7034608e-24
 1.4940560e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 04:23:45,280] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0067
[2019-04-04 04:23:45,327] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 65.0, 145.5, 745.5, 26.0, 25.0713955448958, 0.4028408129671823, 0.0, 1.0, 24086.00603802831], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2984400.0000, 
sim time next is 2985000.0000, 
raw observation next is [-2.833333333333333, 64.16666666666667, 133.6666666666667, 763.6666666666666, 26.0, 25.08230936783137, 0.4050532403610243, 0.0, 1.0, 18724.10559347588], 
processed observation next is [0.0, 0.5652173913043478, 0.3841181902123731, 0.6416666666666667, 0.4455555555555557, 0.8438305709023941, 0.6666666666666666, 0.5901924473192809, 0.6350177467870081, 0.0, 1.0, 0.08916240758798039], 
reward next is 0.9108, 
noisyNet noise sample is [array([-0.39207336], dtype=float32), 0.66532]. 
=============================================
[2019-04-04 04:23:45,358] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[81.18431 ]
 [81.381134]
 [81.75083 ]
 [82.21022 ]
 [82.330086]], R is [[81.21019745]
 [81.28340149]
 [81.34060669]
 [81.38150024]
 [81.43293762]].
[2019-04-04 04:23:49,504] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.93740353e-30 1.21332585e-26 3.33389307e-28 1.07699112e-26
 6.50413428e-27 1.90682431e-30 1.00000000e+00], sum to 1.0000
[2019-04-04 04:23:49,506] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7627
[2019-04-04 04:23:49,538] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 88.0, 107.6666666666667, 735.5, 26.0, 26.39842961992017, 0.6947931754550002, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3234000.0000, 
sim time next is 3234600.0000, 
raw observation next is [-2.7, 86.0, 109.0, 752.0, 26.0, 26.41783753803377, 0.6985344871061603, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.38781163434903054, 0.86, 0.36333333333333334, 0.830939226519337, 0.6666666666666666, 0.7014864615028141, 0.7328448290353867, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5124061], dtype=float32), 0.97917867]. 
=============================================
[2019-04-04 04:23:58,643] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.3574862e-28 3.3894106e-25 3.5706807e-27 2.3764662e-25 2.9287287e-25
 4.0562935e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:23:58,643] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4497
[2019-04-04 04:23:58,658] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 97.33333333333333, 0.0, 0.0, 26.0, 25.41048023751869, 0.3436127322029615, 0.0, 1.0, 70680.17944055932], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3098400.0000, 
sim time next is 3099000.0000, 
raw observation next is [-1.0, 98.66666666666667, 0.0, 0.0, 26.0, 25.3872557147299, 0.3437938083438688, 0.0, 1.0, 66917.8077670496], 
processed observation next is [0.0, 0.8695652173913043, 0.4349030470914128, 0.9866666666666667, 0.0, 0.0, 0.6666666666666666, 0.6156046428941583, 0.6145979361146229, 0.0, 1.0, 0.31865622746214095], 
reward next is 0.6813, 
noisyNet noise sample is [array([0.33511117], dtype=float32), -0.7271253]. 
=============================================
[2019-04-04 04:23:58,667] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[83.81789 ]
 [83.67622 ]
 [83.67062 ]
 [83.68885 ]
 [83.824615]], R is [[83.85684204]
 [83.68170166]
 [83.64608765]
 [83.72033691]
 [83.88313293]].
[2019-04-04 04:23:58,667] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.3480222e-28 1.3991758e-24 2.1913826e-26 3.7168493e-25 9.0057840e-25
 6.3500953e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:23:58,668] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9389
[2019-04-04 04:23:58,683] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.82780537830124, 0.2769812020430907, 0.0, 1.0, 41144.1849516783], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3382200.0000, 
sim time next is 3382800.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.85077324029807, 0.274203041431712, 0.0, 1.0, 41187.59130488877], 
processed observation next is [1.0, 0.13043478260869565, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5708977700248393, 0.5914010138105706, 0.0, 1.0, 0.19613138716613698], 
reward next is 0.8039, 
noisyNet noise sample is [array([-0.38260418], dtype=float32), -1.347745]. 
=============================================
[2019-04-04 04:24:10,198] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.0676005e-30 1.8028787e-25 2.2869578e-27 4.5596086e-26 7.4043927e-26
 7.4009925e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:24:10,198] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1313
[2019-04-04 04:24:10,217] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.666666666666667, 100.0, 0.0, 0.0, 26.0, 25.21944469956254, 0.2873083733637556, 0.0, 1.0, 54074.08867425823], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3125400.0000, 
sim time next is 3126000.0000, 
raw observation next is [2.733333333333333, 100.0, 0.0, 0.0, 26.0, 25.19728202977266, 0.2924963568600783, 0.0, 1.0, 53979.69019559796], 
processed observation next is [1.0, 0.17391304347826086, 0.538319482917821, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5997735024810549, 0.5974987856200261, 0.0, 1.0, 0.2570461437885617], 
reward next is 0.7430, 
noisyNet noise sample is [array([-1.0997267], dtype=float32), -0.638276]. 
=============================================
[2019-04-04 04:24:10,241] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[85.54251]
 [85.56819]
 [85.55236]
 [85.52527]
 [85.55454]], R is [[85.39600372]
 [85.28455353]
 [85.17315674]
 [85.05996704]
 [84.93882751]].
[2019-04-04 04:24:22,594] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [4.5022025e-29 4.6652115e-25 3.1539766e-27 3.4690735e-25 1.9773576e-25
 9.8743114e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:24:22,610] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.7189
[2019-04-04 04:24:22,627] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 26.0, 25.56379045140961, 0.4486918080607731, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3462600.0000, 
sim time next is 3463200.0000, 
raw observation next is [1.0, 79.0, 0.0, 0.0, 26.0, 25.52304570007948, 0.4316777886790957, 0.0, 1.0, 26074.66702076684], 
processed observation next is [1.0, 0.08695652173913043, 0.4903047091412743, 0.79, 0.0, 0.0, 0.6666666666666666, 0.6269204750066232, 0.6438925962263652, 0.0, 1.0, 0.12416508105127066], 
reward next is 0.8758, 
noisyNet noise sample is [array([1.3381969], dtype=float32), 2.1924758]. 
=============================================
[2019-04-04 04:24:24,437] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [9.4388256e-29 1.4539638e-24 8.2114642e-27 2.8046727e-25 3.0961992e-25
 2.1702168e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:24:24,447] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5229
[2019-04-04 04:24:24,463] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 77.0, 0.0, 0.0, 26.0, 24.87363420458988, 0.276344402431641, 0.0, 1.0, 43404.34320938815], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3823200.0000, 
sim time next is 3823800.0000, 
raw observation next is [-5.0, 77.0, 0.0, 0.0, 26.0, 24.91793662001787, 0.2684428548406161, 0.0, 1.0, 43252.58654204633], 
processed observation next is [1.0, 0.2608695652173913, 0.32409972299168976, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5764947183348225, 0.5894809516135388, 0.0, 1.0, 0.20596469781926824], 
reward next is 0.7940, 
noisyNet noise sample is [array([0.03548085], dtype=float32), -0.97652876]. 
=============================================
[2019-04-04 04:24:28,986] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1672569e-28 6.3133914e-25 7.7784043e-27 2.6606225e-25 5.9717007e-25
 6.7999093e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:24:29,005] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9948
[2019-04-04 04:24:29,019] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.36522951713013, 0.4470787477215231, 0.0, 1.0, 56671.10350364526], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3546000.0000, 
sim time next is 3546600.0000, 
raw observation next is [-2.166666666666667, 61.83333333333333, 0.0, 0.0, 26.0, 25.33771420791134, 0.4425421275368251, 0.0, 1.0, 48188.34168120848], 
processed observation next is [0.0, 0.043478260869565216, 0.4025854108956602, 0.6183333333333333, 0.0, 0.0, 0.6666666666666666, 0.6114761839926116, 0.6475140425122751, 0.0, 1.0, 0.2294682937200404], 
reward next is 0.7705, 
noisyNet noise sample is [array([-2.8198535], dtype=float32), 0.42360345]. 
=============================================
[2019-04-04 04:24:35,992] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.8881546e-30 6.0831144e-27 8.1697449e-29 1.3183898e-26 1.1137545e-26
 8.5549545e-31 1.0000000e+00], sum to 1.0000
[2019-04-04 04:24:35,997] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5343
[2019-04-04 04:24:36,035] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.666666666666667, 28.0, 114.6666666666667, 831.8333333333334, 26.0, 25.72185177863083, 0.6144264541652384, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4022400.0000, 
sim time next is 4023000.0000, 
raw observation next is [-3.5, 27.5, 114.0, 830.0, 26.0, 26.30503091981505, 0.662308395504141, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.36565096952908593, 0.275, 0.38, 0.9171270718232044, 0.6666666666666666, 0.6920859099845874, 0.720769465168047, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.352547], dtype=float32), -0.07551453]. 
=============================================
[2019-04-04 04:24:36,041] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[87.3347  ]
 [87.112526]
 [86.6816  ]
 [86.476845]
 [86.63227 ]], R is [[87.38671112]
 [87.5128479 ]
 [86.81013489]
 [86.43569183]
 [86.57133484]].
[2019-04-04 04:24:39,232] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.6673037e-29 4.6035815e-26 9.2305624e-28 1.1746154e-25 2.4886158e-25
 4.4996761e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 04:24:39,232] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.1333
[2019-04-04 04:24:39,255] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 60.00000000000001, 67.83333333333333, 567.6666666666666, 26.0, 26.94264666283271, 0.4443733955049413, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3774000.0000, 
sim time next is 3774600.0000, 
raw observation next is [0.0, 60.0, 64.0, 539.0, 26.0, 26.817561089176, 0.6639444867789087, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.46260387811634357, 0.6, 0.21333333333333335, 0.5955801104972376, 0.6666666666666666, 0.7347967574313333, 0.7213148289263028, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5377375], dtype=float32), 0.27588037]. 
=============================================
[2019-04-04 04:24:40,718] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.1904776e-28 1.3980988e-23 2.4831619e-26 5.4197145e-25 1.0504156e-24
 7.7193400e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:24:40,721] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7244
[2019-04-04 04:24:40,742] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 41.0, 0.0, 0.0, 26.0, 25.12340783229716, 0.3374208911349341, 0.0, 1.0, 40818.53046298795], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4066200.0000, 
sim time next is 4066800.0000, 
raw observation next is [-6.0, 41.0, 0.0, 0.0, 26.0, 25.18714140761337, 0.3354875598597832, 0.0, 1.0, 40816.78917764825], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.41, 0.0, 0.0, 0.6666666666666666, 0.5989284506344476, 0.6118291866199277, 0.0, 1.0, 0.19436566275070596], 
reward next is 0.8056, 
noisyNet noise sample is [array([-0.19543311], dtype=float32), -1.3661776]. 
=============================================
[2019-04-04 04:24:42,633] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.0944511e-28 2.2917733e-24 1.1031497e-26 8.4604099e-25 2.1681029e-24
 5.9843482e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:24:42,639] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7188
[2019-04-04 04:24:42,666] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.333333333333334, 44.0, 116.8333333333333, 826.8333333333334, 26.0, 25.38756660775856, 0.4575829391170367, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3673200.0000, 
sim time next is 3673800.0000, 
raw observation next is [4.5, 43.5, 117.0, 829.0, 26.0, 25.3557492549885, 0.4553640189633629, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.5872576177285319, 0.435, 0.39, 0.9160220994475138, 0.6666666666666666, 0.6129791045823749, 0.651788006321121, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8136506], dtype=float32), -1.1431646]. 
=============================================
[2019-04-04 04:24:48,864] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.47593040e-30 2.71161615e-27 7.42214039e-29 1.15033246e-26
 9.82300153e-27 9.00184170e-31 1.00000000e+00], sum to 1.0000
[2019-04-04 04:24:48,864] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2219
[2019-04-04 04:24:48,909] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 38.0, 96.5, 756.0, 26.0, 26.27767855871401, 0.6879997545111266, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3942000.0000, 
sim time next is 3942600.0000, 
raw observation next is [-4.0, 37.33333333333334, 93.66666666666666, 745.3333333333334, 26.0, 25.6607467555966, 0.6670631736645082, 1.0, 1.0, 98298.62137997994], 
processed observation next is [1.0, 0.6521739130434783, 0.3518005540166205, 0.3733333333333334, 0.3122222222222222, 0.823572744014733, 0.6666666666666666, 0.6383955629663832, 0.7223543912215028, 1.0, 1.0, 0.46808867323799974], 
reward next is 0.5319, 
noisyNet noise sample is [array([-1.3049434], dtype=float32), 0.89391494]. 
=============================================
[2019-04-04 04:24:49,735] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.3825248e-29 3.8311669e-24 2.5195046e-27 1.1028515e-25 2.2238901e-25
 2.0652341e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:24:49,736] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8643
[2019-04-04 04:24:49,804] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.0, 58.0, 48.5, 314.5, 26.0, 25.70954889667954, 0.4402178677905227, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3916800.0000, 
sim time next is 3917400.0000, 
raw observation next is [-8.0, 57.16666666666667, 62.66666666666667, 365.0, 26.0, 25.70941695776311, 0.4379131184670081, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.24099722991689754, 0.5716666666666668, 0.2088888888888889, 0.40331491712707185, 0.6666666666666666, 0.6424514131469259, 0.6459710394890027, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.63174516], dtype=float32), -1.0008886]. 
=============================================
[2019-04-04 04:24:53,706] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1373242e-28 3.3627922e-24 1.2149608e-26 3.3232070e-25 8.6387369e-25
 8.1377425e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:24:53,710] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6062
[2019-04-04 04:24:53,721] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.95, 70.5, 0.0, 0.0, 26.0, 25.66630557920007, 0.4025411950909819, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4332600.0000, 
sim time next is 4333200.0000, 
raw observation next is [3.933333333333333, 70.33333333333333, 0.0, 0.0, 26.0, 25.62178946724628, 0.3867960948165127, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.5715604801477379, 0.7033333333333333, 0.0, 0.0, 0.6666666666666666, 0.6351491222705233, 0.6289320316055043, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.16766588], dtype=float32), -1.8363007]. 
=============================================
[2019-04-04 04:24:56,109] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.0719508e-28 7.3028814e-24 2.8575130e-26 1.8710331e-24 2.7870657e-24
 2.1191687e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 04:24:56,113] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9488
[2019-04-04 04:24:56,152] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.166666666666667, 49.83333333333334, 0.0, 0.0, 26.0, 24.78182683347869, 0.2512509759561343, 0.0, 1.0, 39678.36219543718], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4169400.0000, 
sim time next is 4170000.0000, 
raw observation next is [-4.333333333333334, 49.66666666666667, 0.0, 0.0, 26.0, 24.77356968530453, 0.2447861568828066, 0.0, 1.0, 39750.67718149976], 
processed observation next is [0.0, 0.2608695652173913, 0.3425669436749769, 0.4966666666666667, 0.0, 0.0, 0.6666666666666666, 0.5644641404420442, 0.5815953856276023, 0.0, 1.0, 0.18928893895952267], 
reward next is 0.8107, 
noisyNet noise sample is [array([0.47979143], dtype=float32), -0.6160894]. 
=============================================
[2019-04-04 04:24:56,161] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[82.28928]
 [82.3274 ]
 [82.32223]
 [82.32098]
 [82.32835]], R is [[82.22062683]
 [82.20948029]
 [82.19877625]
 [82.18848419]
 [82.17853546]].
[2019-04-04 04:24:57,695] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.1399720e-28 8.5830265e-25 1.3649337e-26 2.3785611e-25 1.1075191e-24
 4.9796666e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:24:57,695] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1483
[2019-04-04 04:24:57,707] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 38.0, 0.0, 0.0, 26.0, 24.71024340140844, 0.1949178310500141, 0.0, 1.0, 40164.89355677031], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4082400.0000, 
sim time next is 4083000.0000, 
raw observation next is [-4.166666666666667, 38.5, 0.0, 0.0, 26.0, 24.6851568521959, 0.1920669973210452, 0.0, 1.0, 40143.74606340181], 
processed observation next is [1.0, 0.2608695652173913, 0.3471837488457987, 0.385, 0.0, 0.0, 0.6666666666666666, 0.5570964043496582, 0.5640223324403484, 0.0, 1.0, 0.19116069554000864], 
reward next is 0.8088, 
noisyNet noise sample is [array([-0.9292123], dtype=float32), 0.47053176]. 
=============================================
[2019-04-04 04:24:57,718] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[83.59673 ]
 [83.62655 ]
 [83.65709 ]
 [83.674805]
 [83.68796 ]], R is [[83.50818634]
 [83.48184204]
 [83.45575714]
 [83.42988586]
 [83.40420532]].
[2019-04-04 04:25:04,388] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [3.7916974e-29 9.3193464e-25 8.8375484e-28 1.1828800e-25 1.4244111e-25
 1.2666685e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:25:04,389] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.0555
[2019-04-04 04:25:04,416] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.3333333333333334, 33.33333333333334, 118.1666666666667, 814.0, 26.0, 25.13207899321517, 0.3865265967903277, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4188000.0000, 
sim time next is 4188600.0000, 
raw observation next is [0.0, 32.5, 119.0, 822.0, 26.0, 25.11500641410059, 0.3811733805558144, 0.0, 1.0, 9350.951144204759], 
processed observation next is [0.0, 0.4782608695652174, 0.46260387811634357, 0.325, 0.39666666666666667, 0.9082872928176795, 0.6666666666666666, 0.5929172011750493, 0.6270577935186048, 0.0, 1.0, 0.04452833878192742], 
reward next is 0.9555, 
noisyNet noise sample is [array([0.2733963], dtype=float32), -0.77704173]. 
=============================================
[2019-04-04 04:25:08,557] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.4956464e-30 6.9561810e-26 3.1263881e-28 9.4495714e-27 1.8248186e-26
 1.3350587e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:25:08,558] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1272
[2019-04-04 04:25:08,596] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 71.0, 123.0, 171.0, 26.0, 25.63566599506286, 0.4738534092556028, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4609800.0000, 
sim time next is 4610400.0000, 
raw observation next is [-2.0, 71.0, 129.8333333333333, 227.3333333333333, 26.0, 25.91954378591563, 0.5038773388002669, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.40720221606648205, 0.71, 0.4327777777777776, 0.2511970534069981, 0.6666666666666666, 0.6599619821596358, 0.6679591129334224, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7251136], dtype=float32), 1.0415554]. 
=============================================
[2019-04-04 04:25:14,161] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.8544003e-30 1.5484022e-26 7.6737698e-29 2.9586612e-28 4.4292290e-27
 3.9926100e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 04:25:14,161] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6108
[2019-04-04 04:25:14,207] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 71.0, 136.6666666666667, 283.6666666666666, 26.0, 26.07005804061876, 0.5170418252592595, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4611000.0000, 
sim time next is 4611600.0000, 
raw observation next is [-2.0, 71.0, 143.5, 340.0, 26.0, 26.09284538978513, 0.5234936235737083, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.40720221606648205, 0.71, 0.47833333333333333, 0.3756906077348066, 0.6666666666666666, 0.6744037824820941, 0.6744978745245694, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.20725371], dtype=float32), -0.75601053]. 
=============================================
[2019-04-04 04:25:24,624] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.4997174e-29 1.0291404e-24 4.2758279e-27 1.1585726e-25 5.3051645e-25
 1.9978412e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:25:24,624] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7246
[2019-04-04 04:25:24,637] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 62.0, 0.0, 0.0, 26.0, 25.63348347897109, 0.4876803952457934, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4675200.0000, 
sim time next is 4675800.0000, 
raw observation next is [2.0, 62.0, 0.0, 0.0, 26.0, 25.61653559418387, 0.485542799813776, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.518005540166205, 0.62, 0.0, 0.0, 0.6666666666666666, 0.6347112995153225, 0.6618475999379253, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.54347926], dtype=float32), 0.9812933]. 
=============================================
[2019-04-04 04:25:36,422] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:25:36,423] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:25:36,467] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run34
[2019-04-04 04:25:37,536] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:25:37,536] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:25:37,540] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run34
[2019-04-04 04:25:38,673] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:25:38,673] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:25:38,676] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run34
[2019-04-04 04:25:41,873] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:25:41,873] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:25:41,885] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run34
[2019-04-04 04:25:43,982] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:25:43,983] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:25:43,996] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run34
[2019-04-04 04:25:47,812] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:25:47,813] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:25:47,828] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run34
[2019-04-04 04:25:47,850] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:25:47,851] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:25:47,877] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run34
[2019-04-04 04:25:48,238] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.5479485e-28 2.1523141e-24 8.3479387e-27 2.8934378e-25 4.4821790e-25
 1.1702045e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 04:25:48,239] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5110
[2019-04-04 04:25:48,294] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.7, 93.0, 85.5, 0.0, 26.0, 24.30500146643375, 0.09495160068802604, 0.0, 1.0, 35711.57516565673], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 43200.0000, 
sim time next is 43800.0000, 
raw observation next is [7.800000000000001, 91.83333333333333, 89.0, 0.0, 26.0, 24.3048746577388, 0.1010041400136539, 0.0, 1.0, 39155.29926228961], 
processed observation next is [0.0, 0.5217391304347826, 0.6786703601108034, 0.9183333333333333, 0.2966666666666667, 0.0, 0.6666666666666666, 0.5254062214782333, 0.5336680466712179, 0.0, 1.0, 0.18645380601090292], 
reward next is 0.8135, 
noisyNet noise sample is [array([-0.6780573], dtype=float32), 0.9453496]. 
=============================================
[2019-04-04 04:25:49,590] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:25:49,590] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:25:49,618] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run34
[2019-04-04 04:25:53,765] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:25:53,765] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:25:53,768] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run34
[2019-04-04 04:25:54,514] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:25:54,515] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:25:54,518] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run34
[2019-04-04 04:25:55,377] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:25:55,377] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:25:55,384] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run34
[2019-04-04 04:25:56,085] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:25:56,085] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:25:56,089] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run34
[2019-04-04 04:25:56,366] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:25:56,366] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:25:56,369] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run34
[2019-04-04 04:25:57,949] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:25:57,949] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:25:57,954] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run34
[2019-04-04 04:26:00,081] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:26:00,081] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:26:00,084] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run34
[2019-04-04 04:26:04,955] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:26:04,955] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:26:04,961] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run34
[2019-04-04 04:26:13,879] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.3906191e-28 4.9466366e-25 4.3192569e-26 7.1884816e-24 3.5922722e-24
 5.0270960e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:26:13,879] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5606
[2019-04-04 04:26:13,949] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.71666666666667, 68.83333333333333, 0.0, 0.0, 26.0, 25.17548648077241, 0.3336481615253, 1.0, 1.0, 78524.94347607948], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 330600.0000, 
sim time next is 331200.0000, 
raw observation next is [-12.8, 70.0, 0.0, 0.0, 26.0, 25.18780429457048, 0.3340146913082676, 0.0, 1.0, 64252.69398521925], 
processed observation next is [1.0, 0.8695652173913043, 0.1080332409972299, 0.7, 0.0, 0.0, 0.6666666666666666, 0.5989836912142067, 0.6113382304360891, 0.0, 1.0, 0.30596520945342504], 
reward next is 0.6940, 
noisyNet noise sample is [array([0.31916195], dtype=float32), -1.227741]. 
=============================================
[2019-04-04 04:26:25,481] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.5916619e-28 3.0142624e-23 8.9029307e-26 4.3019954e-24 3.0744388e-24
 5.2856658e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 04:26:25,481] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3993
[2019-04-04 04:26:25,535] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 26.0, 23.36110842084121, -0.1025044071657327, 0.0, 1.0, 44088.86109029114], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 184200.0000, 
sim time next is 184800.0000, 
raw observation next is [-8.900000000000002, 78.0, 0.0, 0.0, 26.0, 23.31624409276012, -0.1102817337009118, 0.0, 1.0, 44126.68170926346], 
processed observation next is [1.0, 0.13043478260869565, 0.2160664819944598, 0.78, 0.0, 0.0, 0.6666666666666666, 0.44302034106334326, 0.46323942209969604, 0.0, 1.0, 0.21012705575839744], 
reward next is 0.7899, 
noisyNet noise sample is [array([-1.632313], dtype=float32), 0.54731774]. 
=============================================
[2019-04-04 04:26:31,814] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.8320949e-28 4.3217809e-25 2.5595339e-26 7.8957818e-25 6.2100605e-25
 9.4810161e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:26:31,814] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9768
[2019-04-04 04:26:31,829] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.4, 71.0, 0.0, 0.0, 26.0, 23.99280043999308, 0.06963118568244465, 0.0, 1.0, 44811.70005641523], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 170400.0000, 
sim time next is 171000.0000, 
raw observation next is [-8.4, 71.0, 0.0, 0.0, 26.0, 23.95617247812843, 0.05982449196822043, 0.0, 1.0, 44755.5227471297], 
processed observation next is [1.0, 1.0, 0.2299168975069252, 0.71, 0.0, 0.0, 0.6666666666666666, 0.4963477065107025, 0.5199414973227402, 0.0, 1.0, 0.21312153689109378], 
reward next is 0.7869, 
noisyNet noise sample is [array([-0.578789], dtype=float32), 0.38935244]. 
=============================================
[2019-04-04 04:26:31,942] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[79.07866 ]
 [79.05664 ]
 [79.00291 ]
 [78.90844 ]
 [78.791084]], R is [[79.05727386]
 [79.05331421]
 [79.04914093]
 [79.04477692]
 [79.04022217]].
[2019-04-04 04:26:59,667] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4049714e-27 3.5805477e-24 4.2988741e-26 2.2459036e-24 3.0160205e-24
 5.0945897e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 04:26:59,667] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7363
[2019-04-04 04:26:59,800] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 69.0, 0.0, 0.0, 26.0, 24.95486878349629, 0.2056499695810395, 0.0, 1.0, 42391.70018764104], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 680400.0000, 
sim time next is 681000.0000, 
raw observation next is [-3.4, 69.0, 0.0, 0.0, 26.0, 24.93169811083233, 0.1982939963085799, 0.0, 1.0, 42324.179092329], 
processed observation next is [0.0, 0.9130434782608695, 0.368421052631579, 0.69, 0.0, 0.0, 0.6666666666666666, 0.5776415092360274, 0.5660979987695266, 0.0, 1.0, 0.20154370996347143], 
reward next is 0.7985, 
noisyNet noise sample is [array([1.5665425], dtype=float32), -0.065345034]. 
=============================================
[2019-04-04 04:26:59,827] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[79.23053 ]
 [79.26803 ]
 [79.34368 ]
 [79.42308 ]
 [79.530396]], R is [[79.22379303]
 [79.22969055]
 [79.23522949]
 [79.2405777 ]
 [79.24465179]].
[2019-04-04 04:27:00,365] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-04-04 04:27:00,377] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 04:27:00,377] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:27:00,379] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run46
[2019-04-04 04:27:00,447] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 04:27:00,448] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:27:00,450] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run46
[2019-04-04 04:27:00,487] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 04:27:00,487] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:27:00,532] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run46
[2019-04-04 04:27:19,192] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.35946637], dtype=float32), 0.23128398]
[2019-04-04 04:27:19,192] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [8.3, 93.0, 0.0, 0.0, 26.0, 24.57093769796942, 0.3088677278369752, 0.0, 1.0, 40447.60576255531]
[2019-04-04 04:27:19,193] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 04:27:19,193] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.5393721e-28 2.3612668e-25 3.6717500e-27 4.1501637e-25 3.8302441e-25
 1.4306660e-28 1.0000000e+00], sampled 0.09410315881106512
[2019-04-04 04:28:04,980] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.35946637], dtype=float32), 0.23128398]
[2019-04-04 04:28:04,980] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [7.863837962, 100.0, 0.0, 0.0, 26.0, 25.40494520661819, 0.4126874159681045, 1.0, 1.0, 0.0]
[2019-04-04 04:28:04,980] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 04:28:04,982] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.6382812e-29 2.8940008e-25 1.7948003e-27 6.6669951e-26 1.3250155e-25
 1.4704488e-28 1.0000000e+00], sampled 0.054464728002333485
[2019-04-04 04:29:42,695] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.35946637], dtype=float32), 0.23128398]
[2019-04-04 04:29:42,695] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-2.2, 80.33333333333333, 0.0, 0.0, 26.0, 25.08479304813413, 0.4486992085237642, 1.0, 1.0, 41502.64368313296]
[2019-04-04 04:29:42,695] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 04:29:42,696] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [3.0101984e-28 4.5381807e-25 6.9228230e-27 7.7667892e-25 8.7070227e-25
 3.7826410e-28 1.0000000e+00], sampled 0.13395976576020707
[2019-04-04 04:30:07,010] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 04:30:38,680] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 04:30:44,490] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 04:30:45,542] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 4500000, evaluation results [4500000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 04:30:51,150] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.5667145e-29 2.2311723e-25 9.1307214e-27 1.5532777e-25 9.0600212e-25
 3.0750777e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:30:51,150] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9526
[2019-04-04 04:30:51,259] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 76.0, 41.0, 8.166666666666664, 26.0, 25.67482222566394, 0.291812185746611, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 722400.0000, 
sim time next is 723000.0000, 
raw observation next is [-2.3, 76.0, 52.99999999999999, 16.33333333333333, 26.0, 25.70856098890648, 0.3040692575263489, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.3988919667590028, 0.76, 0.17666666666666664, 0.018047882136279923, 0.6666666666666666, 0.6423800824088733, 0.6013564191754496, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6693442], dtype=float32), 0.02719199]. 
=============================================
[2019-04-04 04:30:51,418] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[81.95287 ]
 [82.17929 ]
 [82.74376 ]
 [83.32533 ]
 [83.872314]], R is [[82.0842514 ]
 [82.26341248]
 [82.44078064]
 [82.61637115]
 [82.79020691]].
[2019-04-04 04:30:55,949] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2354395e-28 1.8068032e-25 2.4536411e-27 4.2317605e-25 2.7178844e-25
 9.6023990e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:30:55,949] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7451
[2019-04-04 04:30:55,998] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.6, 48.33333333333333, 0.0, 0.0, 26.0, 24.45264075790766, 0.1410772028426763, 0.0, 1.0, 45160.04931476731], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 423600.0000, 
sim time next is 424200.0000, 
raw observation next is [-10.6, 48.66666666666666, 0.0, 0.0, 26.0, 24.39861733054365, 0.1286303989315176, 0.0, 1.0, 44914.29303314945], 
processed observation next is [1.0, 0.9130434782608695, 0.1689750692520776, 0.4866666666666666, 0.0, 0.0, 0.6666666666666666, 0.5332181108786376, 0.5428767996438392, 0.0, 1.0, 0.21387758587214023], 
reward next is 0.7861, 
noisyNet noise sample is [array([-0.53665787], dtype=float32), 1.6996099]. 
=============================================
[2019-04-04 04:31:05,512] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.0162777e-29 8.4646988e-26 1.2031577e-27 1.5197754e-25 7.3049919e-26
 1.2953859e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:31:05,512] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6834
[2019-04-04 04:31:05,590] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.7000000000000001, 70.33333333333334, 0.0, 0.0, 26.0, 25.16742778846995, 0.1963432320220378, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 495600.0000, 
sim time next is 496200.0000, 
raw observation next is [0.6, 77.16666666666667, 0.0, 0.0, 26.0, 25.03181213992662, 0.03726468471581279, 1.0, 1.0, 47442.92794667481], 
processed observation next is [1.0, 0.7391304347826086, 0.479224376731302, 0.7716666666666667, 0.0, 0.0, 0.6666666666666666, 0.5859843449938849, 0.5124215615719376, 1.0, 1.0, 0.2259187045079753], 
reward next is 0.7741, 
noisyNet noise sample is [array([-0.01124152], dtype=float32), -0.5506945]. 
=============================================
[2019-04-04 04:31:08,407] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.1332187e-29 6.0813573e-25 5.5381895e-27 3.0063817e-25 6.3067719e-25
 2.0374550e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:31:08,408] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6410
[2019-04-04 04:31:08,461] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.533333333333333, 66.0, 0.0, 0.0, 26.0, 24.62260968390205, 0.2208489905353493, 0.0, 1.0, 42931.40748044642], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 772800.0000, 
sim time next is 773400.0000, 
raw observation next is [-6.616666666666667, 66.5, 0.0, 0.0, 26.0, 24.59085504386332, 0.2137159059543678, 0.0, 1.0, 42783.71985238007], 
processed observation next is [1.0, 0.9565217391304348, 0.2793167128347184, 0.665, 0.0, 0.0, 0.6666666666666666, 0.5492379203219434, 0.5712386353181226, 0.0, 1.0, 0.20373199929704794], 
reward next is 0.7963, 
noisyNet noise sample is [array([-1.2010133], dtype=float32), 0.7315344]. 
=============================================
[2019-04-04 04:31:12,818] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.8679709e-29 3.1306056e-25 3.7859777e-27 7.3905972e-25 5.8062815e-25
 1.1353476e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:31:12,818] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6132
[2019-04-04 04:31:12,840] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.2, 46.33333333333334, 84.16666666666667, 144.8333333333333, 26.0, 25.69086463420798, 0.3848448749561064, 1.0, 1.0, 74120.40541372853], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 746400.0000, 
sim time next is 747000.0000, 
raw observation next is [-0.3, 46.0, 85.0, 31.0, 26.0, 25.36794852157735, 0.3583244765325751, 1.0, 1.0, 45121.10770575175], 
processed observation next is [1.0, 0.6521739130434783, 0.4542936288088643, 0.46, 0.2833333333333333, 0.03425414364640884, 0.6666666666666666, 0.6139957101314458, 0.619441492177525, 1.0, 1.0, 0.21486241764643688], 
reward next is 0.7851, 
noisyNet noise sample is [array([-0.66181815], dtype=float32), 1.7049944]. 
=============================================
[2019-04-04 04:31:12,849] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[79.13118 ]
 [78.87737 ]
 [79.17337 ]
 [79.49648 ]
 [80.098625]], R is [[79.10803986]
 [78.96401215]
 [79.17436981]
 [79.38262939]
 [79.58880615]].
[2019-04-04 04:31:18,473] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.1592433e-27 6.9585004e-23 1.4367444e-25 2.6942363e-24 1.5371286e-23
 1.4918756e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 04:31:18,476] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3803
[2019-04-04 04:31:18,516] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 73.83333333333333, 0.0, 0.0, 26.0, 23.98367002589843, 0.03999155261563465, 0.0, 1.0, 44156.54556964591], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 619800.0000, 
sim time next is 620400.0000, 
raw observation next is [-4.5, 72.66666666666667, 0.0, 0.0, 26.0, 23.93923793738436, 0.03220285335861329, 0.0, 1.0, 44304.03093296679], 
processed observation next is [0.0, 0.17391304347826086, 0.3379501385041552, 0.7266666666666667, 0.0, 0.0, 0.6666666666666666, 0.4949364947820299, 0.5107342844528712, 0.0, 1.0, 0.2109715758712704], 
reward next is 0.7890, 
noisyNet noise sample is [array([1.455488], dtype=float32), -1.4974719]. 
=============================================
[2019-04-04 04:31:20,399] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.5559874e-30 1.8870876e-26 5.4007188e-28 9.1399612e-26 2.2233374e-26
 1.5799876e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:31:20,399] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4185
[2019-04-04 04:31:20,441] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 58.5, 99.66666666666666, 669.0, 26.0, 25.86301252543569, 0.3895616733027364, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 733800.0000, 
sim time next is 734400.0000, 
raw observation next is [-0.6, 57.0, 107.5, 614.0, 26.0, 25.84235977200898, 0.3889771677855018, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.44598337950138506, 0.57, 0.35833333333333334, 0.6784530386740332, 0.6666666666666666, 0.6535299810007483, 0.6296590559285006, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2633437], dtype=float32), -0.038199607]. 
=============================================
[2019-04-04 04:31:24,636] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.18308917e-30 1.65173447e-25 2.61852011e-28 5.98327847e-27
 1.24451313e-26 1.17440695e-29 1.00000000e+00], sum to 1.0000
[2019-04-04 04:31:24,638] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5283
[2019-04-04 04:31:24,735] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.366666666666667, 75.0, 36.83333333333333, 0.0, 26.0, 25.75384755964628, 0.3308074057168242, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 808800.0000, 
sim time next is 809400.0000, 
raw observation next is [-6.283333333333333, 75.0, 41.66666666666666, 0.0, 26.0, 25.871694127301, 0.3152053168449394, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.288550323176362, 0.75, 0.13888888888888887, 0.0, 0.6666666666666666, 0.6559745106084168, 0.6050684389483131, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.13173187], dtype=float32), 0.12880212]. 
=============================================
[2019-04-04 04:31:25,103] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.21383872e-29 5.06830744e-26 5.30764626e-28 4.45809919e-26
 3.59988200e-26 1.21596506e-29 1.00000000e+00], sum to 1.0000
[2019-04-04 04:31:25,103] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8115
[2019-04-04 04:31:25,125] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.8, 62.0, 0.0, 0.0, 26.0, 24.9250012787001, 0.2865554844696554, 0.0, 1.0, 44143.81030173526], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 768000.0000, 
sim time next is 768600.0000, 
raw observation next is [-5.9, 62.5, 0.0, 0.0, 26.0, 24.90347920361462, 0.2770424011899555, 0.0, 1.0, 44053.95495300784], 
processed observation next is [1.0, 0.9130434782608695, 0.2991689750692521, 0.625, 0.0, 0.0, 0.6666666666666666, 0.5752899336345516, 0.5923474670633185, 0.0, 1.0, 0.2097807378714659], 
reward next is 0.7902, 
noisyNet noise sample is [array([0.2224343], dtype=float32), -1.4510006]. 
=============================================
[2019-04-04 04:31:25,782] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.8781549e-30 1.5871554e-25 9.8297808e-28 6.0631245e-26 1.0245178e-25
 7.0385704e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:31:25,782] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3301
[2019-04-04 04:31:25,832] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.1833333333333333, 73.33333333333333, 19.33333333333334, 0.0, 26.0, 24.92862839406163, 0.2199285592904818, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 893400.0000, 
sim time next is 894000.0000, 
raw observation next is [0.3666666666666667, 74.66666666666667, 24.16666666666667, 0.0, 26.0, 24.90941937703329, 0.2508761708785909, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.4727608494921515, 0.7466666666666667, 0.08055555555555557, 0.0, 0.6666666666666666, 0.5757849480861076, 0.5836253902928636, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3969145], dtype=float32), -0.013724339]. 
=============================================
[2019-04-04 04:31:25,837] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[87.676926]
 [87.94286 ]
 [88.221886]
 [88.22236 ]
 [88.17958 ]], R is [[87.60778046]
 [87.73170471]
 [87.85438538]
 [87.97584534]
 [88.09608459]].
[2019-04-04 04:31:27,467] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.5064019e-31 1.9558477e-27 4.8497287e-29 5.7017318e-27 4.4244500e-27
 8.7958631e-31 1.0000000e+00], sum to 1.0000
[2019-04-04 04:31:27,467] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0381
[2019-04-04 04:31:27,500] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.600000000000001, 94.66666666666667, 0.0, 0.0, 26.0, 25.34320688888258, 0.327370925972723, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 927600.0000, 
sim time next is 928200.0000, 
raw observation next is [4.5, 95.33333333333333, 0.0, 0.0, 26.0, 25.19585159205904, 0.3158075948779777, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.5872576177285319, 0.9533333333333333, 0.0, 0.0, 0.6666666666666666, 0.5996542993382533, 0.6052691982926592, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.07231539], dtype=float32), 0.92441386]. 
=============================================
[2019-04-04 04:31:35,853] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.5984157e-32 1.9536916e-28 4.1405545e-30 1.3960334e-27 6.9611645e-28
 1.1978977e-31 1.0000000e+00], sum to 1.0000
[2019-04-04 04:31:35,855] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3966
[2019-04-04 04:31:35,876] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.4, 93.0, 36.0, 0.0, 26.0, 25.80429893368935, 0.4057066860082339, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 921600.0000, 
sim time next is 922200.0000, 
raw observation next is [4.5, 92.83333333333333, 30.0, 0.0, 26.0, 25.78394147640775, 0.4027152533576151, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.5872576177285319, 0.9283333333333332, 0.1, 0.0, 0.6666666666666666, 0.6486617897006459, 0.6342384177858716, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.35263315], dtype=float32), 2.8833783]. 
=============================================
[2019-04-04 04:31:37,956] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.79763442e-30 1.76026114e-26 1.84479232e-28 1.94172411e-26
 1.23228725e-26 2.44956923e-30 1.00000000e+00], sum to 1.0000
[2019-04-04 04:31:37,958] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5214
[2019-04-04 04:31:38,004] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 26.0, 25.17462222497971, 0.5008515283835687, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1359600.0000, 
sim time next is 1360200.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 26.0, 25.18834936744118, 0.502407025496897, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.4764542936288089, 0.96, 0.0, 0.0, 0.6666666666666666, 0.5990291139534317, 0.6674690084989656, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.65006256], dtype=float32), 0.46877772]. 
=============================================
[2019-04-04 04:31:48,095] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.6539393e-32 5.3305952e-28 8.8888250e-31 3.4961680e-29 8.7389493e-29
 4.3186694e-31 1.0000000e+00], sum to 1.0000
[2019-04-04 04:31:48,097] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5689
[2019-04-04 04:31:48,160] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 100.0, 86.5, 0.0, 26.0, 24.70938401756385, 0.4496120801522759, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1249200.0000, 
sim time next is 1249800.0000, 
raw observation next is [14.4, 99.33333333333334, 89.33333333333334, 0.0, 26.0, 24.89117898571879, 0.4656925621488011, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.8614958448753465, 0.9933333333333334, 0.2977777777777778, 0.0, 0.6666666666666666, 0.5742649154765657, 0.6552308540496004, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.65235126], dtype=float32), -0.73347396]. 
=============================================
[2019-04-04 04:31:50,292] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1502116e-30 4.9014378e-27 7.4392047e-29 2.0996964e-26 1.2362799e-26
 1.0195054e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 04:31:50,293] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6186
[2019-04-04 04:31:50,324] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 26.0, 25.38783436414933, 0.4861293515624119, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1362000.0000, 
sim time next is 1362600.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 26.0, 25.25149059737872, 0.4487308788741926, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.4764542936288089, 0.96, 0.0, 0.0, 0.6666666666666666, 0.6042908831148933, 0.6495769596247308, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.03028002], dtype=float32), -0.7871743]. 
=============================================
[2019-04-04 04:31:51,137] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.6733172e-30 1.1282471e-25 2.4498520e-28 2.4786002e-26 4.3918299e-26
 2.9397145e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:31:51,142] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2401
[2019-04-04 04:31:51,156] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.7, 98.66666666666666, 0.0, 0.0, 26.0, 25.50047278851907, 0.6086856166769475, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1284000.0000, 
sim time next is 1284600.0000, 
raw observation next is [5.600000000000001, 99.33333333333334, 0.0, 0.0, 26.0, 25.52826701024134, 0.602905717538565, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.6177285318559558, 0.9933333333333334, 0.0, 0.0, 0.6666666666666666, 0.6273555841867783, 0.700968572512855, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4129261], dtype=float32), -1.8490454]. 
=============================================
[2019-04-04 04:31:55,784] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.0306121e-30 7.4155014e-26 3.0240442e-28 5.1257994e-27 2.0378302e-26
 8.0193368e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 04:31:55,784] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5122
[2019-04-04 04:31:55,797] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.5, 85.5, 0.0, 0.0, 26.0, 25.5454881757229, 0.5077496492412733, 0.0, 1.0, 49668.69428651478], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1567800.0000, 
sim time next is 1568400.0000, 
raw observation next is [4.533333333333333, 85.33333333333334, 0.0, 0.0, 26.0, 25.48338700745914, 0.5143972643350075, 0.0, 1.0, 72497.11905179311], 
processed observation next is [1.0, 0.13043478260869565, 0.5881809787626964, 0.8533333333333334, 0.0, 0.0, 0.6666666666666666, 0.6236155839549283, 0.6714657547783358, 0.0, 1.0, 0.34522437643711007], 
reward next is 0.6548, 
noisyNet noise sample is [array([-1.3986048], dtype=float32), 1.6153944]. 
=============================================
[2019-04-04 04:31:56,355] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.4399130e-33 6.2723024e-30 1.3467710e-31 8.4180905e-29 1.0325668e-29
 4.9670000e-33 1.0000000e+00], sum to 1.0000
[2019-04-04 04:31:56,356] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2023
[2019-04-04 04:31:56,364] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.43333333333334, 50.33333333333334, 158.0, 82.66666666666667, 26.0, 25.89628161470464, 0.6939506492156906, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1601400.0000, 
sim time next is 1602000.0000, 
raw observation next is [13.8, 49.0, 162.5, 62.0, 26.0, 26.32445505338043, 0.7422680961632647, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.844875346260388, 0.49, 0.5416666666666666, 0.06850828729281767, 0.6666666666666666, 0.6937045877817024, 0.7474226987210882, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.13570006], dtype=float32), -0.79611105]. 
=============================================
[2019-04-04 04:31:56,369] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[93.550095]
 [93.53793 ]
 [93.40706 ]
 [93.6307  ]
 [93.63083 ]], R is [[93.7509613 ]
 [93.81345367]
 [93.87532043]
 [93.93656921]
 [93.99720764]].
[2019-04-04 04:32:16,125] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.6034292e-27 3.8824243e-24 8.1604444e-26 3.7320106e-24 6.8820998e-24
 1.3879735e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 04:32:16,125] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2741
[2019-04-04 04:32:16,173] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 85.66666666666667, 93.5, 0.0, 26.0, 24.95447858253507, 0.3427841864435955, 0.0, 1.0, 60846.71760336556], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1780800.0000, 
sim time next is 1781400.0000, 
raw observation next is [-2.8, 86.33333333333333, 88.00000000000001, 0.0, 26.0, 24.96462405665156, 0.3461872857564092, 0.0, 1.0, 45229.52841826906], 
processed observation next is [0.0, 0.6086956521739131, 0.38504155124653744, 0.8633333333333333, 0.2933333333333334, 0.0, 0.6666666666666666, 0.5803853380542966, 0.6153957619188031, 0.0, 1.0, 0.2153787067536622], 
reward next is 0.7846, 
noisyNet noise sample is [array([-0.22316904], dtype=float32), 0.60618144]. 
=============================================
[2019-04-04 04:32:21,940] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.9697427e-28 1.0888362e-23 2.8728145e-26 3.2867732e-24 6.8118884e-25
 2.8543195e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 04:32:21,940] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2259
[2019-04-04 04:32:21,980] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.600000000000001, 83.0, 0.0, 0.0, 26.0, 24.25499127323788, 0.1237438874724506, 0.0, 1.0, 41729.64080314956], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1995000.0000, 
sim time next is 1995600.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.25813438357073, 0.1318255587536841, 0.0, 1.0, 41703.85576488706], 
processed observation next is [1.0, 0.08695652173913043, 0.30747922437673136, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5215111986308942, 0.5439418529178947, 0.0, 1.0, 0.19858978935660507], 
reward next is 0.8014, 
noisyNet noise sample is [array([-0.8640996], dtype=float32), -0.6301613]. 
=============================================
[2019-04-04 04:32:23,207] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.7146443e-26 2.8847782e-23 3.5291113e-25 1.2662127e-23 2.6223153e-23
 2.2416986e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 04:32:23,207] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3496
[2019-04-04 04:32:23,236] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 86.0, 0.0, 0.0, 26.0, 25.02800880712413, 0.2376209972204582, 0.0, 1.0, 45327.08267313833], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1886400.0000, 
sim time next is 1887000.0000, 
raw observation next is [-5.600000000000001, 85.5, 0.0, 0.0, 26.0, 24.99774696322585, 0.2302543796385635, 0.0, 1.0, 44909.35934324843], 
processed observation next is [0.0, 0.8695652173913043, 0.3074792243767313, 0.855, 0.0, 0.0, 0.6666666666666666, 0.5831455802688209, 0.5767514598795211, 0.0, 1.0, 0.2138540921107068], 
reward next is 0.7861, 
noisyNet noise sample is [array([-1.5579377], dtype=float32), -0.08922356]. 
=============================================
[2019-04-04 04:32:23,239] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[77.18771]
 [77.39282]
 [77.58815]
 [77.70818]
 [77.70704]], R is [[77.05480957]
 [77.06842041]
 [77.07244873]
 [77.04284668]
 [76.91651154]].
[2019-04-04 04:32:26,311] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.0388166e-27 1.8706558e-23 1.5747559e-25 3.2237055e-24 3.6118805e-24
 8.2332642e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 04:32:26,311] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4285
[2019-04-04 04:32:26,330] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.716666666666667, 77.5, 0.0, 0.0, 26.0, 24.05541235920311, 0.01605240647698288, 0.0, 1.0, 45072.13526853644], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1907400.0000, 
sim time next is 1908000.0000, 
raw observation next is [-7.8, 78.0, 0.0, 0.0, 26.0, 24.02642378052837, 0.00942343089310121, 0.0, 1.0, 44998.02777500107], 
processed observation next is [1.0, 0.08695652173913043, 0.24653739612188366, 0.78, 0.0, 0.0, 0.6666666666666666, 0.5022019817106974, 0.5031411436310337, 0.0, 1.0, 0.21427632273810035], 
reward next is 0.7857, 
noisyNet noise sample is [array([-1.2209998], dtype=float32), -1.1743386]. 
=============================================
[2019-04-04 04:32:26,353] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[79.06019 ]
 [78.980865]
 [78.8943  ]
 [78.81663 ]
 [78.74993 ]], R is [[79.12010193]
 [79.11427307]
 [79.1081543 ]
 [79.10186768]
 [79.09560394]].
[2019-04-04 04:32:36,405] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.3702510e-29 4.6243906e-24 1.6824169e-26 2.9968094e-25 9.6032978e-25
 9.7824232e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:32:36,406] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7466
[2019-04-04 04:32:36,420] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.33137021355545, 0.1340124439971207, 0.0, 1.0, 41640.45409929976], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1996200.0000, 
sim time next is 1996800.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.35721537142524, 0.1337888760391078, 0.0, 1.0, 41581.57025222336], 
processed observation next is [1.0, 0.08695652173913043, 0.30747922437673136, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5297679476187701, 0.5445962920130359, 0.0, 1.0, 0.19800747739153982], 
reward next is 0.8020, 
noisyNet noise sample is [array([0.76799005], dtype=float32), 0.37962675]. 
=============================================
[2019-04-04 04:32:40,047] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.6995394e-28 4.8565539e-24 3.8958469e-26 1.1008934e-24 1.0921303e-24
 9.9269652e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:32:40,048] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.9542
[2019-04-04 04:32:40,107] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.2, 87.0, 0.0, 0.0, 26.0, 24.24099296882595, 0.07411455812404721, 0.0, 1.0, 41065.97827217368], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2007000.0000, 
sim time next is 2007600.0000, 
raw observation next is [-6.199999999999999, 87.0, 0.0, 0.0, 26.0, 24.21321926624926, 0.0638995299894536, 0.0, 1.0, 41079.6130000487], 
processed observation next is [1.0, 0.21739130434782608, 0.2908587257617729, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5177682721874385, 0.5212998433298178, 0.0, 1.0, 0.19561720476213668], 
reward next is 0.8044, 
noisyNet noise sample is [array([-0.42948607], dtype=float32), -0.970249]. 
=============================================
[2019-04-04 04:32:50,592] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.7692165e-28 2.5723383e-25 2.6055583e-27 4.2795527e-25 3.6662811e-25
 1.0822740e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:32:50,592] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1377
[2019-04-04 04:32:50,656] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.716666666666667, 80.83333333333334, 196.6666666666667, 79.33333333333333, 26.0, 25.87500907578895, 0.3994172016371669, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2110200.0000, 
sim time next is 2110800.0000, 
raw observation next is [-7.633333333333333, 79.66666666666667, 202.3333333333333, 69.66666666666666, 26.0, 25.84980128394227, 0.3975033557110924, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.2511542012927055, 0.7966666666666667, 0.6744444444444443, 0.07697974217311233, 0.6666666666666666, 0.6541501069951892, 0.6325011185703642, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.44209576], dtype=float32), 1.0323523]. 
=============================================
[2019-04-04 04:32:57,004] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.2159361e-28 8.6357718e-25 1.5356552e-26 1.0654077e-24 1.6510550e-24
 7.4216630e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:32:57,005] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4532
[2019-04-04 04:32:57,016] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.8335804942405, 0.2654516646893542, 0.0, 1.0, 38530.42486251384], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2337000.0000, 
sim time next is 2337600.0000, 
raw observation next is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.81583128467948, 0.2568578157225973, 0.0, 1.0, 38572.7311341381], 
processed observation next is [0.0, 0.043478260869565216, 0.3988919667590028, 0.62, 0.0, 0.0, 0.6666666666666666, 0.5679859403899566, 0.5856192719075325, 0.0, 1.0, 0.18367967206732427], 
reward next is 0.8163, 
noisyNet noise sample is [array([-0.09767001], dtype=float32), -1.4389794]. 
=============================================
[2019-04-04 04:33:06,451] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.7323951e-26 1.2604207e-22 5.7121655e-25 1.7205964e-23 5.4820999e-23
 2.7043466e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 04:33:06,456] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8279
[2019-04-04 04:33:06,508] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.3, 60.66666666666667, 0.0, 0.0, 26.0, 23.07434229941386, -0.1901171284559354, 0.0, 1.0, 44038.86235203341], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2443200.0000, 
sim time next is 2443800.0000, 
raw observation next is [-9.4, 60.83333333333334, 0.0, 0.0, 26.0, 23.03151772895281, -0.1996443887961127, 0.0, 1.0, 44056.21939400317], 
processed observation next is [0.0, 0.2608695652173913, 0.20221606648199447, 0.6083333333333334, 0.0, 0.0, 0.6666666666666666, 0.4192931440794008, 0.43345187040129574, 0.0, 1.0, 0.20979152092382464], 
reward next is 0.7902, 
noisyNet noise sample is [array([-0.8819271], dtype=float32), -1.1004354]. 
=============================================
[2019-04-04 04:33:07,763] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1767266e-27 7.4087229e-24 1.3114677e-26 1.0678327e-24 1.0529792e-24
 1.7888195e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 04:33:07,764] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6305
[2019-04-04 04:33:07,802] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 39.0, 0.0, 0.0, 26.0, 25.19064883523308, 0.2443667880841464, 0.0, 1.0, 39292.43838657329], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2507400.0000, 
sim time next is 2508000.0000, 
raw observation next is [-1.7, 39.33333333333333, 0.0, 0.0, 26.0, 25.24406354948085, 0.2350176709359714, 0.0, 1.0, 39072.18639699488], 
processed observation next is [1.0, 0.0, 0.4155124653739613, 0.3933333333333333, 0.0, 0.0, 0.6666666666666666, 0.6036719624567374, 0.5783392236453239, 0.0, 1.0, 0.18605803046188038], 
reward next is 0.8139, 
noisyNet noise sample is [array([-0.5537498], dtype=float32), -1.0439061]. 
=============================================
[2019-04-04 04:33:07,847] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[82.404  ]
 [82.3148 ]
 [82.1648 ]
 [82.08013]
 [81.86228]], R is [[82.41596985]
 [82.40470886]
 [82.39186096]
 [82.38017273]
 [82.36837006]].
[2019-04-04 04:33:10,232] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.7432120e-29 4.3091194e-26 5.2262959e-28 2.7291762e-26 6.1522177e-26
 6.9029813e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 04:33:10,232] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.9722
[2019-04-04 04:33:10,305] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.616666666666667, 55.66666666666667, 0.0, 0.0, 26.0, 24.94506129678298, 0.3537184093086396, 1.0, 1.0, 43631.20550018981], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2317800.0000, 
sim time next is 2318400.0000, 
raw observation next is [-1.7, 56.0, 0.0, 0.0, 26.0, 25.00152839940588, 0.3570679732289589, 0.0, 1.0, 18730.56746857888], 
processed observation next is [1.0, 0.8695652173913043, 0.4155124653739613, 0.56, 0.0, 0.0, 0.6666666666666666, 0.5834606999504901, 0.6190226577429864, 0.0, 1.0, 0.08919317842180419], 
reward next is 0.9108, 
noisyNet noise sample is [array([-2.19027], dtype=float32), 1.6700567]. 
=============================================
[2019-04-04 04:33:20,317] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.4896294e-27 2.8712399e-24 2.2707982e-26 1.1389547e-24 4.7965504e-24
 4.0275398e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 04:33:20,317] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9757
[2019-04-04 04:33:20,379] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.2, 33.0, 0.0, 0.0, 26.0, 25.28047900236282, 0.2638931449116221, 0.0, 1.0, 40216.08888121878], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2498400.0000, 
sim time next is 2499000.0000, 
raw observation next is [-1.1, 33.33333333333334, 0.0, 0.0, 26.0, 25.25058943919131, 0.2590894485793602, 0.0, 1.0, 40278.8053116081], 
processed observation next is [0.0, 0.9565217391304348, 0.4321329639889197, 0.3333333333333334, 0.0, 0.0, 0.6666666666666666, 0.6042157865992758, 0.5863631495264534, 0.0, 1.0, 0.1918038348171814], 
reward next is 0.8082, 
noisyNet noise sample is [array([0.76828337], dtype=float32), 0.5048796]. 
=============================================
[2019-04-04 04:33:20,384] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[81.22672 ]
 [81.198456]
 [81.130974]
 [81.00822 ]
 [80.8885  ]], R is [[81.23616028]
 [81.23229218]
 [81.22882843]
 [81.2256546 ]
 [81.22260284]].
[2019-04-04 04:33:39,322] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.1679289e-27 2.4632212e-23 3.3110213e-25 1.1712385e-23 2.2217300e-23
 1.4322947e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 04:33:39,324] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0322
[2019-04-04 04:33:39,338] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.333333333333334, 70.0, 0.0, 0.0, 26.0, 24.56699027915509, 0.2350758367545724, 0.0, 1.0, 44417.12844139731], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2680800.0000, 
sim time next is 2681400.0000, 
raw observation next is [-8.666666666666666, 69.5, 0.0, 0.0, 26.0, 24.56304797790205, 0.2274264213631529, 0.0, 1.0, 44412.2874613992], 
processed observation next is [1.0, 0.0, 0.22253000923361038, 0.695, 0.0, 0.0, 0.6666666666666666, 0.5469206648251707, 0.575808807121051, 0.0, 1.0, 0.21148708314952], 
reward next is 0.7885, 
noisyNet noise sample is [array([0.33256066], dtype=float32), 0.9414797]. 
=============================================
[2019-04-04 04:33:46,403] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.5536131e-28 1.9148263e-24 1.7587041e-26 9.3359282e-25 9.8974108e-25
 5.5885898e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:33:46,404] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1749
[2019-04-04 04:33:46,422] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.64554662655244, 0.2544611136274689, 0.0, 1.0, 42918.24202176111], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2767200.0000, 
sim time next is 2767800.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.72411067863519, 0.2562760719155276, 0.0, 1.0, 42694.63821931047], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5603425565529324, 0.5854253573051759, 0.0, 1.0, 0.20330780104433557], 
reward next is 0.7967, 
noisyNet noise sample is [array([-0.79197997], dtype=float32), -1.4048407]. 
=============================================
[2019-04-04 04:33:49,715] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6352133e-27 3.3677987e-24 1.5933907e-26 2.3528879e-24 1.4379843e-24
 2.9138265e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 04:33:49,715] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2858
[2019-04-04 04:33:49,792] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.666666666666667, 69.0, 174.0, 42.0, 26.0, 24.84692871960326, 0.3061027346643935, 0.0, 1.0, 97469.73404623813], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2974800.0000, 
sim time next is 2975400.0000, 
raw observation next is [-3.5, 68.0, 178.0, 24.0, 26.0, 24.88819128498533, 0.3160545643893864, 0.0, 1.0, 43768.29940431967], 
processed observation next is [0.0, 0.43478260869565216, 0.36565096952908593, 0.68, 0.5933333333333334, 0.026519337016574586, 0.6666666666666666, 0.574015940415444, 0.6053515214631288, 0.0, 1.0, 0.20842047335390318], 
reward next is 0.7916, 
noisyNet noise sample is [array([-0.94546306], dtype=float32), 0.21509601]. 
=============================================
[2019-04-04 04:33:58,277] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.2011445e-27 6.7336607e-24 5.6877444e-26 1.1156818e-24 1.6769339e-24
 1.4736231e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 04:33:58,277] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.9191
[2019-04-04 04:33:58,314] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 77.0, 0.0, 0.0, 26.0, 23.88586607555141, 0.08692345320719057, 0.0, 1.0, 59879.26740965401], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2961600.0000, 
sim time next is 2962200.0000, 
raw observation next is [-4.0, 77.0, 0.0, 0.0, 26.0, 23.81566998378192, 0.07489316288812199, 0.0, 1.0, 60450.56888520911], 
processed observation next is [0.0, 0.2608695652173913, 0.3518005540166205, 0.77, 0.0, 0.0, 0.6666666666666666, 0.48463916531516, 0.524964387629374, 0.0, 1.0, 0.2878598518343291], 
reward next is 0.7121, 
noisyNet noise sample is [array([1.0037203], dtype=float32), -2.0900352]. 
=============================================
[2019-04-04 04:33:58,877] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.4037803e-31 2.4821363e-28 5.2831787e-30 3.0280630e-27 1.4756643e-27
 8.9013416e-32 1.0000000e+00], sum to 1.0000
[2019-04-04 04:33:58,878] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0871
[2019-04-04 04:33:58,927] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.333333333333333, 38.0, 189.8333333333333, 599.6666666666667, 26.0, 24.76443877426243, 0.3882156926175347, 1.0, 1.0, 181310.4967913419], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2810400.0000, 
sim time next is 2811000.0000, 
raw observation next is [3.666666666666667, 36.5, 201.6666666666667, 514.3333333333334, 26.0, 25.22454040321242, 0.4379456356537516, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.564173591874423, 0.365, 0.6722222222222224, 0.5683241252302026, 0.6666666666666666, 0.6020450336010349, 0.6459818785512506, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.33424675], dtype=float32), 0.67553896]. 
=============================================
[2019-04-04 04:33:58,933] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[89.54403 ]
 [89.55601 ]
 [89.30212 ]
 [89.405464]
 [89.48805 ]], R is [[89.04824066]
 [88.29438019]
 [87.47706604]
 [87.49108124]
 [87.61617279]].
[2019-04-04 04:34:00,036] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.4387227e-28 2.6323981e-23 4.3540965e-26 2.0589139e-24 3.0087121e-24
 8.8645979e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 04:34:00,037] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2245
[2019-04-04 04:34:00,062] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.14882948909623, 0.08254440193817912, 0.0, 1.0, 39861.80825189553], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3037200.0000, 
sim time next is 3037800.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.11406771180776, 0.07546812518156033, 0.0, 1.0, 39962.04879645998], 
processed observation next is [0.0, 0.13043478260869565, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5095056426506467, 0.5251560417271868, 0.0, 1.0, 0.19029547045933323], 
reward next is 0.8097, 
noisyNet noise sample is [array([1.7432352], dtype=float32), -0.4180745]. 
=============================================
[2019-04-04 04:34:06,712] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.3527367e-31 1.4481993e-27 3.9547921e-29 1.6974557e-27 5.0334310e-27
 3.2622912e-31 1.0000000e+00], sum to 1.0000
[2019-04-04 04:34:06,714] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0955
[2019-04-04 04:34:06,725] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.0, 100.0, 106.5, 729.5, 26.0, 26.80680792637616, 0.6775357402980914, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3147600.0000, 
sim time next is 3148200.0000, 
raw observation next is [7.0, 100.0, 108.0, 746.0, 26.0, 26.89040471629321, 0.6987153005025308, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.6565096952908588, 1.0, 0.36, 0.8243093922651934, 0.6666666666666666, 0.740867059691101, 0.7329051001675103, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5860296], dtype=float32), 0.54220736]. 
=============================================
[2019-04-04 04:34:08,824] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [9.9713977e-30 1.3042684e-26 1.1549222e-27 1.0541563e-25 9.2375809e-26
 7.1448312e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:34:08,825] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2208
[2019-04-04 04:34:08,842] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.333333333333333, 100.0, 0.0, 0.0, 26.0, 25.86027098292335, 0.712808675311203, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3188400.0000, 
sim time next is 3189000.0000, 
raw observation next is [2.166666666666667, 100.0, 0.0, 0.0, 26.0, 25.84811050546541, 0.7114718237774659, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.5226223453370269, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6540092087887842, 0.7371572745924886, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7798245], dtype=float32), -0.6434162]. 
=============================================
[2019-04-04 04:34:08,862] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[82.214745]
 [82.19805 ]
 [82.2425  ]
 [82.17458 ]
 [82.17649 ]], R is [[82.45059204]
 [82.62608337]
 [82.79981995]
 [82.97182465]
 [83.1421051 ]].
[2019-04-04 04:34:12,450] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.0626901e-29 1.1140185e-25 1.3513643e-27 1.3033241e-25 1.0811326e-25
 8.3373640e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:34:12,451] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4694
[2019-04-04 04:34:12,466] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.333333333333334, 100.0, 0.0, 0.0, 26.0, 25.38772292844255, 0.3087437522058671, 0.0, 1.0, 59903.08556857277], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3132600.0000, 
sim time next is 3133200.0000, 
raw observation next is [4.666666666666667, 100.0, 0.0, 0.0, 26.0, 25.35622834776907, 0.3107643894317293, 0.0, 1.0, 63508.21569445484], 
processed observation next is [1.0, 0.2608695652173913, 0.5918744228993538, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6130190289807557, 0.6035881298105764, 0.0, 1.0, 0.30242007473549926], 
reward next is 0.6976, 
noisyNet noise sample is [array([1.0897001], dtype=float32), 1.0112585]. 
=============================================
[2019-04-04 04:34:13,840] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.0496893e-29 2.4438272e-24 1.1871604e-26 1.3374878e-25 8.0623616e-25
 2.5582722e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:34:13,840] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7405
[2019-04-04 04:34:13,871] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.2, 100.0, 0.0, 0.0, 26.0, 25.29821525672278, 0.2981036287009481, 0.0, 1.0, 77793.39354088478], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3122400.0000, 
sim time next is 3123000.0000, 
raw observation next is [2.3, 100.0, 0.0, 0.0, 26.0, 25.28186964175538, 0.2926892863162612, 0.0, 1.0, 62645.92852978713], 
processed observation next is [1.0, 0.13043478260869565, 0.5263157894736843, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6068224701462818, 0.5975630954387537, 0.0, 1.0, 0.2983139453799387], 
reward next is 0.7017, 
noisyNet noise sample is [array([-0.51258945], dtype=float32), -0.018128084]. 
=============================================
[2019-04-04 04:34:13,882] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[84.36589]
 [84.2267 ]
 [83.98328]
 [83.96222]
 [84.028  ]], R is [[84.25514221]
 [84.04214478]
 [83.73025513]
 [83.78251648]
 [83.8553772 ]].
[2019-04-04 04:34:16,931] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.7908464e-28 1.6917309e-24 4.2559280e-26 6.8781330e-25 2.0624198e-24
 2.1415247e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 04:34:16,932] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5143
[2019-04-04 04:34:16,946] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.166666666666667, 100.0, 0.0, 0.0, 26.0, 25.35666602636819, 0.5049432921417821, 0.0, 1.0, 40665.95458741377], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3215400.0000, 
sim time next is 3216000.0000, 
raw observation next is [-2.333333333333333, 100.0, 0.0, 0.0, 26.0, 25.35122355476976, 0.4969229862247934, 0.0, 1.0, 40590.4851680076], 
processed observation next is [1.0, 0.21739130434782608, 0.3979686057248385, 1.0, 0.0, 0.0, 0.6666666666666666, 0.61260196289748, 0.6656409954082645, 0.0, 1.0, 0.19328802460956002], 
reward next is 0.8067, 
noisyNet noise sample is [array([-1.2767204], dtype=float32), -0.055710923]. 
=============================================
[2019-04-04 04:34:16,954] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[82.5444 ]
 [82.73347]
 [82.86084]
 [82.96834]
 [82.99428]], R is [[82.38796234]
 [82.37043762]
 [82.35221863]
 [82.33213806]
 [82.30591583]].
[2019-04-04 04:34:17,386] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.2190902e-30 7.1273928e-26 2.0562254e-28 9.2119587e-27 1.1401447e-26
 7.5168203e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 04:34:17,386] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5846
[2019-04-04 04:34:17,436] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.0, 80.5, 86.0, 396.0, 26.0, 25.72936242432628, 0.4591962670261521, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3313800.0000, 
sim time next is 3314400.0000, 
raw observation next is [-9.666666666666668, 79.33333333333333, 89.0, 432.5, 26.0, 25.85623035260561, 0.4722426618437292, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.19482917820867957, 0.7933333333333333, 0.2966666666666667, 0.47790055248618785, 0.6666666666666666, 0.6546858627171342, 0.6574142206145764, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.18344182], dtype=float32), 1.6521441]. 
=============================================
[2019-04-04 04:34:18,011] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.4853020e-30 3.9025384e-27 1.2918088e-28 4.8257638e-26 2.4704728e-26
 1.1267088e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 04:34:18,011] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3199
[2019-04-04 04:34:18,035] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 71.0, 114.0, 817.0, 26.0, 26.63800912173961, 0.7595149352770812, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3240000.0000, 
sim time next is 3240600.0000, 
raw observation next is [-2.0, 73.33333333333334, 114.3333333333333, 819.0, 26.0, 26.66906271743749, 0.7655589425682305, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.40720221606648205, 0.7333333333333334, 0.381111111111111, 0.9049723756906077, 0.6666666666666666, 0.7224218931197909, 0.7551863141894102, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.442849], dtype=float32), -0.0641351]. 
=============================================
[2019-04-04 04:34:24,478] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.83862476e-30 6.28162659e-27 1.39168324e-28 1.13269695e-26
 6.03250678e-27 9.28659815e-31 1.00000000e+00], sum to 1.0000
[2019-04-04 04:34:24,482] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8866
[2019-04-04 04:34:24,517] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 26.0, 25.0767191972281, 0.456934192410438, 0.0, 1.0, 198590.6135249597], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3442800.0000, 
sim time next is 3443400.0000, 
raw observation next is [1.0, 79.0, 0.0, 0.0, 26.0, 25.09750664945245, 0.4839227792317282, 0.0, 1.0, 183667.2044460197], 
processed observation next is [1.0, 0.8695652173913043, 0.4903047091412743, 0.79, 0.0, 0.0, 0.6666666666666666, 0.5914588874543707, 0.6613075930772427, 0.0, 1.0, 0.8746057354572366], 
reward next is 0.1254, 
noisyNet noise sample is [array([0.93263525], dtype=float32), 1.6534581]. 
=============================================
[2019-04-04 04:34:26,374] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.5628907e-30 4.4742650e-27 7.8200484e-29 4.1397714e-27 7.7317220e-27
 7.1303155e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 04:34:26,376] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4725
[2019-04-04 04:34:26,384] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.0, 45.66666666666667, 101.1666666666667, 764.1666666666667, 26.0, 25.50049207200338, 0.4858114260102029, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3681600.0000, 
sim time next is 3682200.0000, 
raw observation next is [6.0, 46.33333333333334, 98.33333333333334, 752.3333333333333, 26.0, 25.49571053286211, 0.4852664115450744, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.6288088642659281, 0.46333333333333343, 0.32777777777777783, 0.8313075506445672, 0.6666666666666666, 0.6246425444051757, 0.6617554705150248, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5873811], dtype=float32), -1.1475728]. 
=============================================
[2019-04-04 04:34:31,311] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.10469715e-30 7.62187700e-27 2.13590253e-28 1.73492885e-26
 1.31219339e-26 3.82839401e-30 1.00000000e+00], sum to 1.0000
[2019-04-04 04:34:31,311] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4903
[2019-04-04 04:34:31,324] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 86.0, 0.0, 0.0, 26.0, 25.32017394606766, 0.4705100514240397, 0.0, 1.0, 57577.32838748945], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3451800.0000, 
sim time next is 3452400.0000, 
raw observation next is [1.0, 86.0, 0.0, 0.0, 26.0, 25.35414748331831, 0.4752441141724841, 0.0, 1.0, 47020.69152333237], 
processed observation next is [1.0, 1.0, 0.4903047091412743, 0.86, 0.0, 0.0, 0.6666666666666666, 0.6128456236098593, 0.6584147047241614, 0.0, 1.0, 0.22390805487301127], 
reward next is 0.7761, 
noisyNet noise sample is [array([0.63990825], dtype=float32), -1.9512187]. 
=============================================
[2019-04-04 04:34:32,745] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [4.3836528e-30 1.3966946e-26 1.2229667e-28 4.4879255e-26 2.6349960e-26
 6.1810927e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 04:34:32,746] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.6946
[2019-04-04 04:34:32,754] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 74.0, 111.0, 769.0, 26.0, 26.34119722485291, 0.5505675109896396, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3753000.0000, 
sim time next is 3753600.0000, 
raw observation next is [-3.0, 73.0, 111.6666666666667, 777.8333333333334, 26.0, 26.37608264090255, 0.5580858117730952, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.3795013850415513, 0.73, 0.37222222222222234, 0.8594843462246777, 0.6666666666666666, 0.698006886741879, 0.686028603924365, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.22522566], dtype=float32), 0.5142002]. 
=============================================
[2019-04-04 04:34:36,971] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.5954735e-30 1.0087957e-25 3.5795298e-28 1.0909092e-26 6.0887588e-26
 1.5366146e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:34:36,982] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7751
[2019-04-04 04:34:36,995] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.166666666666666, 31.16666666666667, 32.33333333333333, 215.0, 26.0, 25.46169849063133, 0.3835480248315149, 0.0, 1.0, 29651.87678953702], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3657000.0000, 
sim time next is 3657600.0000, 
raw observation next is [8.0, 32.0, 46.5, 262.0, 26.0, 25.48169022511965, 0.3969666850992357, 0.0, 1.0, 18753.17327691974], 
processed observation next is [0.0, 0.34782608695652173, 0.6842105263157896, 0.32, 0.155, 0.28950276243093925, 0.6666666666666666, 0.6234741854266375, 0.6323222283664119, 0.0, 1.0, 0.08930082512818924], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.9116007], dtype=float32), -0.83567774]. 
=============================================
[2019-04-04 04:34:39,760] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.7832309e-29 2.0763359e-25 1.4515177e-27 4.6145352e-26 1.3076517e-25
 4.8179388e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:34:39,760] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2272
[2019-04-04 04:34:39,808] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 72.0, 32.99999999999999, 233.6666666666666, 26.0, 25.25838864699619, 0.3102797900131691, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3743400.0000, 
sim time next is 3744000.0000, 
raw observation next is [-4.0, 71.0, 47.0, 282.5, 26.0, 25.25316173021312, 0.301844734431631, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.3518005540166205, 0.71, 0.15666666666666668, 0.31215469613259667, 0.6666666666666666, 0.6044301441844265, 0.6006149114772104, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3398878], dtype=float32), 0.4204858]. 
=============================================
[2019-04-04 04:34:39,814] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[86.84807 ]
 [86.5523  ]
 [85.671036]
 [83.770096]
 [82.674904]], R is [[85.67873383]
 [85.82194519]
 [85.96372986]
 [86.10409546]
 [86.24305725]].
[2019-04-04 04:34:48,572] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.5160176e-30 1.3021556e-26 5.3997510e-28 2.4767288e-26 4.8168254e-26
 1.3521777e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:34:48,573] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6098
[2019-04-04 04:34:48,611] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.13619429621621, 0.44870075760736, 1.0, 1.0, 64097.48702938536], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3787200.0000, 
sim time next is 3787800.0000, 
raw observation next is [-2.166666666666667, 66.0, 0.0, 0.0, 26.0, 25.12006068086737, 0.4515967234592686, 0.0, 1.0, 59682.5288635636], 
processed observation next is [1.0, 0.8695652173913043, 0.4025854108956602, 0.66, 0.0, 0.0, 0.6666666666666666, 0.5933383900722807, 0.6505322411530895, 0.0, 1.0, 0.28420251839792193], 
reward next is 0.7158, 
noisyNet noise sample is [array([-1.0367004], dtype=float32), 1.0593908]. 
=============================================
[2019-04-04 04:34:50,928] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.3286020e-28 7.5101614e-25 2.0064814e-26 5.6408811e-25 6.9903667e-25
 5.2231470e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:34:50,929] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9951
[2019-04-04 04:34:50,958] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.333333333333334, 36.33333333333333, 0.0, 0.0, 26.0, 24.84516892751611, 0.2286926485071905, 0.0, 1.0, 40387.37840132401], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4077600.0000, 
sim time next is 4078200.0000, 
raw observation next is [-4.166666666666667, 35.16666666666667, 0.0, 0.0, 26.0, 24.78929686555816, 0.2317953975124562, 0.0, 1.0, 40362.70866115568], 
processed observation next is [1.0, 0.17391304347826086, 0.3471837488457987, 0.35166666666666674, 0.0, 0.0, 0.6666666666666666, 0.5657747387965134, 0.5772651325041521, 0.0, 1.0, 0.19220337457693182], 
reward next is 0.8078, 
noisyNet noise sample is [array([-0.1317659], dtype=float32), -0.051714048]. 
=============================================
[2019-04-04 04:34:51,635] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.9564981e-30 7.2656394e-26 1.0170714e-27 5.2394360e-26 1.3606922e-25
 1.2999004e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:34:51,638] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3059
[2019-04-04 04:34:51,659] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6666666666666667, 58.5, 0.0, 0.0, 26.0, 25.21627409588835, 0.5447488504835446, 0.0, 1.0, 87126.31150101422], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3876600.0000, 
sim time next is 3877200.0000, 
raw observation next is [-1.0, 60.0, 0.0, 0.0, 26.0, 25.46565745605039, 0.5660325337742216, 0.0, 1.0, 18759.25425434499], 
processed observation next is [1.0, 0.9130434782608695, 0.4349030470914128, 0.6, 0.0, 0.0, 0.6666666666666666, 0.6221381213375325, 0.6886775112580739, 0.0, 1.0, 0.08932978216354756], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.6565196], dtype=float32), -1.5379333]. 
=============================================
[2019-04-04 04:34:56,873] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.5087335e-29 1.5960256e-25 3.4393286e-28 1.8325408e-26 4.3548274e-26
 6.5515653e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:34:56,876] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8429
[2019-04-04 04:34:56,920] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.333333333333333, 36.66666666666666, 110.0, 698.0, 26.0, 25.45266056755997, 0.4086666392988421, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4182600.0000, 
sim time next is 4183200.0000, 
raw observation next is [-2.0, 35.0, 111.0, 717.0, 26.0, 25.40086080118352, 0.4025114967482836, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.40720221606648205, 0.35, 0.37, 0.7922651933701658, 0.6666666666666666, 0.6167384000986266, 0.6341704989160946, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.9353683], dtype=float32), -0.62717396]. 
=============================================
[2019-04-04 04:34:57,269] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.1860818e-28 1.7351556e-24 2.4682780e-26 1.1572570e-24 3.1812173e-24
 5.1198561e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:34:57,270] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6083
[2019-04-04 04:34:57,298] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 37.66666666666667, 0.0, 0.0, 26.0, 25.10183265488864, 0.3488259957282535, 0.0, 1.0, 40675.46960616737], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4061400.0000, 
sim time next is 4062000.0000, 
raw observation next is [-6.0, 38.33333333333334, 0.0, 0.0, 26.0, 25.07641342097462, 0.3414266920193625, 0.0, 1.0, 40671.1404261312], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.3833333333333334, 0.0, 0.0, 0.6666666666666666, 0.5897011184145518, 0.6138088973397875, 0.0, 1.0, 0.19367209726729143], 
reward next is 0.8063, 
noisyNet noise sample is [array([-1.846946], dtype=float32), -0.47153953]. 
=============================================
[2019-04-04 04:34:57,312] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[80.75416]
 [80.59712]
 [80.52678]
 [80.46094]
 [80.37994]], R is [[80.93470764]
 [80.93166351]
 [80.92856598]
 [80.92536926]
 [80.92204285]].
[2019-04-04 04:34:58,867] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.48856313e-31 4.81559177e-27 5.10034036e-29 1.19293025e-26
 7.82524827e-27 1.40345327e-30 1.00000000e+00], sum to 1.0000
[2019-04-04 04:34:58,867] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0721
[2019-04-04 04:34:58,962] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 60.0, 105.5, 727.5, 26.0, 26.42990178295419, 0.590694916482538, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3837600.0000, 
sim time next is 3838200.0000, 
raw observation next is [-1.833333333333333, 60.0, 107.0, 743.3333333333334, 26.0, 26.4690897601978, 0.602621595714813, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.41181902123730385, 0.6, 0.3566666666666667, 0.8213627992633518, 0.6666666666666666, 0.7057574800164833, 0.700873865238271, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.40258884], dtype=float32), 1.9175637]. 
=============================================
[2019-04-04 04:35:07,534] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1406095e-27 8.1267772e-24 4.1281353e-26 9.8759508e-25 3.0298238e-24
 4.3396513e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 04:35:07,535] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0420
[2019-04-04 04:35:07,564] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 26.0, 25.41920638981872, 0.338066682289865, 0.0, 1.0, 31983.10151986276], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4245000.0000, 
sim time next is 4245600.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 26.0, 25.42140899829792, 0.3353295743878276, 0.0, 1.0, 33853.47715452191], 
processed observation next is [0.0, 0.13043478260869565, 0.5457063711911359, 0.45, 0.0, 0.0, 0.6666666666666666, 0.6184507498581601, 0.6117765247959426, 0.0, 1.0, 0.16120703406915196], 
reward next is 0.8388, 
noisyNet noise sample is [array([-0.14121374], dtype=float32), -0.1350672]. 
=============================================
[2019-04-04 04:35:11,591] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.3511151e-29 4.4885954e-25 1.0607304e-27 8.5067802e-26 1.0112809e-25
 3.4223784e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:35:11,592] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0446
[2019-04-04 04:35:11,642] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6666666666666667, 34.16666666666667, 117.3333333333333, 806.0, 26.0, 25.11545053542289, 0.3892565273496273, 0.0, 1.0, 18705.58955302931], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4187400.0000, 
sim time next is 4188000.0000, 
raw observation next is [-0.3333333333333334, 33.33333333333334, 118.1666666666667, 814.0, 26.0, 25.13207899321517, 0.3865265967903277, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.4533702677747, 0.3333333333333334, 0.393888888888889, 0.8994475138121547, 0.6666666666666666, 0.5943399161012642, 0.6288421989301093, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8704691], dtype=float32), -1.4842417]. 
=============================================
[2019-04-04 04:35:11,650] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[87.20829 ]
 [87.26627 ]
 [87.386375]
 [87.58741 ]
 [87.76905 ]], R is [[87.28408813]
 [87.32217407]
 [87.35987854]
 [87.48628235]
 [87.61141968]].
[2019-04-04 04:35:20,150] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-04 04:35:20,150] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 04:35:20,150] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:35:20,153] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run47
[2019-04-04 04:35:20,210] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 04:35:20,211] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:35:20,213] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run47
[2019-04-04 04:35:20,280] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 04:35:20,282] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:35:20,297] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run47
[2019-04-04 04:36:05,009] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.36085644], dtype=float32), 0.24415193]
[2019-04-04 04:36:05,010] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-4.5, 75.0, 0.0, 0.0, 26.0, 24.03307346456412, 0.04851413633639745, 0.0, 1.0, 43963.65879555471]
[2019-04-04 04:36:05,010] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 04:36:05,011] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [6.72486781e-28 1.02378045e-23 2.95229437e-26 9.07814917e-25
 1.94205189e-24 5.03174158e-27 1.00000000e+00], sampled 0.25167378508902427
[2019-04-04 04:38:32,358] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 04:39:02,869] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 04:39:05,587] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 04:39:06,625] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 4600000, evaluation results [4600000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 04:39:10,287] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.3905678e-29 4.2621994e-26 4.3540923e-28 4.8322294e-26 1.1014599e-25
 4.1851315e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:39:10,318] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6409
[2019-04-04 04:39:10,331] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 44.0, 173.5, 313.5, 26.0, 25.00997361016111, 0.3874280953327778, 0.0, 1.0, 26651.53363485365], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4201200.0000, 
sim time next is 4201800.0000, 
raw observation next is [2.166666666666667, 42.83333333333334, 169.0, 388.3333333333334, 26.0, 25.08959415918396, 0.4064570084743561, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.5226223453370269, 0.42833333333333345, 0.5633333333333334, 0.42909760589318613, 0.6666666666666666, 0.59079951326533, 0.635485669491452, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.22430854], dtype=float32), -2.161825]. 
=============================================
[2019-04-04 04:39:13,211] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.0759663e-30 2.6900525e-27 4.8750661e-29 8.1759262e-27 1.1381370e-26
 7.3797846e-31 1.0000000e+00], sum to 1.0000
[2019-04-04 04:39:13,212] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2859
[2019-04-04 04:39:13,233] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.166666666666667, 31.66666666666667, 109.3333333333333, 806.0, 26.0, 26.63457215665083, 0.6937527007454812, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4111800.0000, 
sim time next is 4112400.0000, 
raw observation next is [3.333333333333333, 32.33333333333334, 107.6666666666667, 800.0, 26.0, 26.86202403625707, 0.7233813160644081, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5549399815327793, 0.3233333333333334, 0.358888888888889, 0.8839779005524862, 0.6666666666666666, 0.7385020030214223, 0.7411271053548028, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.71844643], dtype=float32), -0.7773206]. 
=============================================
[2019-04-04 04:39:16,333] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.8456799e-31 2.4477701e-27 7.2220513e-29 1.8340365e-27 1.5596657e-26
 6.3337799e-31 1.0000000e+00], sum to 1.0000
[2019-04-04 04:39:16,337] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3764
[2019-04-04 04:39:16,353] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.5, 54.0, 0.0, 0.0, 26.0, 27.58395261495759, 0.9657998387888668, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4390200.0000, 
sim time next is 4390800.0000, 
raw observation next is [11.33333333333333, 55.33333333333333, 0.0, 0.0, 26.0, 27.48801167645907, 0.947884539681323, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.7765466297322253, 0.5533333333333332, 0.0, 0.0, 0.6666666666666666, 0.7906676397049225, 0.8159615132271076, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.32671735], dtype=float32), -0.38799673]. 
=============================================
[2019-04-04 04:39:16,552] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6467044e-29 6.5649882e-25 3.1583350e-27 2.8089020e-25 1.8302364e-25
 1.1180015e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:39:16,555] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1451
[2019-04-04 04:39:16,579] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.5333333333333333, 73.0, 0.0, 0.0, 26.0, 25.36341890054383, 0.4717689142017412, 0.0, 1.0, 49891.25726278517], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4494000.0000, 
sim time next is 4494600.0000, 
raw observation next is [-0.55, 73.0, 0.0, 0.0, 26.0, 25.37101066150809, 0.4939353317380669, 0.0, 1.0, 46975.42924133546], 
processed observation next is [1.0, 0.0, 0.44736842105263164, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6142508884590075, 0.6646451105793556, 0.0, 1.0, 0.22369252019683553], 
reward next is 0.7763, 
noisyNet noise sample is [array([-0.73042184], dtype=float32), -0.8503001]. 
=============================================
[2019-04-04 04:39:25,661] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.9058554e-29 3.4829168e-25 3.9041316e-27 1.9382394e-25 1.6264245e-25
 1.5814538e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:39:25,661] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9769
[2019-04-04 04:39:25,673] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.8, 73.0, 0.0, 0.0, 26.0, 25.32247291733525, 0.4355065975898428, 0.0, 1.0, 42787.40782452659], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4501800.0000, 
sim time next is 4502400.0000, 
raw observation next is [-0.8666666666666667, 73.0, 0.0, 0.0, 26.0, 25.28925414934671, 0.4422032187008544, 0.0, 1.0, 42637.09595340549], 
processed observation next is [1.0, 0.08695652173913043, 0.4385964912280702, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6074378457788926, 0.6474010729002848, 0.0, 1.0, 0.20303379025431187], 
reward next is 0.7970, 
noisyNet noise sample is [array([-0.25548822], dtype=float32), 0.8113257]. 
=============================================
[2019-04-04 04:39:32,108] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.5248374e-30 1.4459588e-26 2.0566960e-28 1.0758456e-26 2.1823063e-26
 2.8995406e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 04:39:32,108] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8311
[2019-04-04 04:39:32,143] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.06175902812878, 0.4644170265468352, 0.0, 1.0, 18706.2261540012], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4477800.0000, 
sim time next is 4478400.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.06249717222639, 0.4575665358630409, 1.0, 1.0, 23591.84888744531], 
processed observation next is [1.0, 0.8695652173913043, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5885414310188658, 0.6525221786210137, 1.0, 1.0, 0.11234213755926338], 
reward next is 0.8877, 
noisyNet noise sample is [array([-0.24928828], dtype=float32), -0.031400472]. 
=============================================
[2019-04-04 04:39:36,430] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.6452899e-31 9.7141114e-28 1.5846958e-29 3.8130817e-27 5.8809263e-28
 1.2177277e-31 1.0000000e+00], sum to 1.0000
[2019-04-04 04:39:36,430] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7878
[2019-04-04 04:39:36,438] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 45.66666666666666, 227.0, 79.33333333333334, 26.0, 25.78756771950579, 0.5513632684362519, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4546200.0000, 
sim time next is 4546800.0000, 
raw observation next is [3.0, 45.0, 208.5, 62.5, 26.0, 26.09374122446678, 0.5745268200569723, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5457063711911359, 0.45, 0.695, 0.06906077348066299, 0.6666666666666666, 0.6744784353722316, 0.6915089400189908, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2894902], dtype=float32), 0.98387444]. 
=============================================
[2019-04-04 04:39:41,427] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.06232582e-27 1.42183390e-23 4.75616966e-26 2.96553571e-25
 3.02925763e-24 1.00748384e-26 1.00000000e+00], sum to 1.0000
[2019-04-04 04:39:41,429] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5192
[2019-04-04 04:39:41,444] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.38119535871673, 0.2268209585137052, 0.0, 1.0, 41009.92712833046], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4767000.0000, 
sim time next is 4767600.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.38841005547334, 0.2203201275771762, 0.0, 1.0, 41072.98989092105], 
processed observation next is [0.0, 0.17391304347826086, 0.296398891966759, 0.92, 0.0, 0.0, 0.6666666666666666, 0.5323675046227784, 0.5734400425257254, 0.0, 1.0, 0.1955856661472431], 
reward next is 0.8044, 
noisyNet noise sample is [array([1.5636725], dtype=float32), 0.13155209]. 
=============================================
[2019-04-04 04:39:41,804] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.9288533e-29 2.9384406e-26 1.3960387e-27 1.6205720e-25 1.6246449e-25
 8.6083516e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 04:39:41,806] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2306
[2019-04-04 04:39:41,820] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.333333333333334, 45.5, 91.33333333333334, 146.3333333333333, 26.0, 27.32437773877921, 0.8473032854843295, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4639800.0000, 
sim time next is 4640400.0000, 
raw observation next is [5.2, 46.0, 78.5, 146.0, 26.0, 27.43391879743504, 0.8497876673942267, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.6066481994459835, 0.46, 0.26166666666666666, 0.16132596685082873, 0.6666666666666666, 0.7861598997862534, 0.7832625557980756, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3329161], dtype=float32), 1.2371342]. 
=============================================
[2019-04-04 04:39:45,143] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3310979e-28 7.0170568e-25 1.3491552e-26 3.8784258e-25 3.0905938e-25
 2.2207162e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:39:45,144] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6490
[2019-04-04 04:39:45,170] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.5830897435604, 0.4188339325918073, 0.0, 1.0, 23945.44626980651], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4683600.0000, 
sim time next is 4684200.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.42787264414369, 0.4097084403791789, 0.0, 1.0, 108326.5479850814], 
processed observation next is [1.0, 0.21739130434782608, 0.4349030470914128, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6189893870119741, 0.6365694801263929, 0.0, 1.0, 0.5158407046908638], 
reward next is 0.4842, 
noisyNet noise sample is [array([-0.02041531], dtype=float32), 0.71309143]. 
=============================================
[2019-04-04 04:39:45,325] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.0266106e-28 2.7411153e-24 2.1964459e-26 1.3715160e-25 4.7539228e-25
 6.1675793e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:39:45,327] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5489
[2019-04-04 04:39:45,355] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.3, 68.0, 0.0, 0.0, 26.0, 25.47147141795237, 0.4330141496297703, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4591800.0000, 
sim time next is 4592400.0000, 
raw observation next is [-1.366666666666667, 68.33333333333334, 0.0, 0.0, 26.0, 25.47452026744691, 0.424218001037105, 0.0, 1.0, 18758.56380777569], 
processed observation next is [1.0, 0.13043478260869565, 0.42474607571560485, 0.6833333333333335, 0.0, 0.0, 0.6666666666666666, 0.6228766889539091, 0.6414060003457017, 0.0, 1.0, 0.08932649432274137], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.5773606], dtype=float32), -0.8869783]. 
=============================================
[2019-04-04 04:39:50,220] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:39:50,221] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:39:50,271] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run35
[2019-04-04 04:39:51,729] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.8710644e-29 2.9884855e-26 4.6009961e-28 3.2720582e-26 5.9357001e-26
 2.4891233e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:39:51,730] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3945
[2019-04-04 04:39:51,740] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.166666666666667, 39.5, 203.3333333333333, 641.0, 26.0, 25.18033812500995, 0.4395382137572437, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4799400.0000, 
sim time next is 4800000.0000, 
raw observation next is [2.333333333333333, 39.0, 211.6666666666667, 604.5, 26.0, 25.16945125145121, 0.4348369194666806, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.5272391505078486, 0.39, 0.7055555555555557, 0.6679558011049723, 0.6666666666666666, 0.5974542709542675, 0.6449456398222269, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4446228], dtype=float32), 0.0021093735]. 
=============================================
[2019-04-04 04:39:51,754] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[85.313065]
 [85.34659 ]
 [85.431656]
 [85.56708 ]
 [85.73747 ]], R is [[85.44860077]
 [85.59411621]
 [85.73817444]
 [85.88079071]
 [86.02198029]].
[2019-04-04 04:39:52,333] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:39:52,333] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:39:52,342] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run35
[2019-04-04 04:39:54,155] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [6.7238678e-28 7.3227994e-24 1.9802402e-26 5.0530978e-25 1.4969164e-24
 1.1209445e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 04:39:54,155] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.5847
[2019-04-04 04:39:54,202] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.166666666666667, 46.5, 0.0, 0.0, 26.0, 24.97048942761053, 0.2932562018247335, 0.0, 1.0, 33858.23416950955], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4906200.0000, 
sim time next is 4906800.0000, 
raw observation next is [1.0, 47.0, 0.0, 0.0, 26.0, 24.97307284129002, 0.2913881259848979, 0.0, 1.0, 31997.31200273814], 
processed observation next is [0.0, 0.8260869565217391, 0.4903047091412743, 0.47, 0.0, 0.0, 0.6666666666666666, 0.5810894034408349, 0.5971293753282992, 0.0, 1.0, 0.15236815239399115], 
reward next is 0.8476, 
noisyNet noise sample is [array([0.22563823], dtype=float32), 0.85910344]. 
=============================================
[2019-04-04 04:39:54,377] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:39:54,377] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:39:54,386] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run35
[2019-04-04 04:39:54,427] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:39:54,428] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:39:54,439] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run35
[2019-04-04 04:39:58,825] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:39:58,825] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:39:58,828] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run35
[2019-04-04 04:40:00,368] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.7507582e-29 2.9690774e-26 6.9520885e-28 1.5015425e-25 1.4953804e-25
 6.3204356e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 04:40:00,368] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1632
[2019-04-04 04:40:00,402] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.83333333333333, 17.0, 49.33333333333333, 389.6666666666666, 26.0, 28.8669398719788, 1.175852070523924, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5073000.0000, 
sim time next is 5073600.0000, 
raw observation next is [11.66666666666667, 17.0, 42.66666666666666, 340.8333333333333, 26.0, 29.05819799478462, 0.9868948916428573, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.785780240073869, 0.17, 0.1422222222222222, 0.3766114180478821, 0.6666666666666666, 0.9215164995653851, 0.8289649638809524, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.01885255], dtype=float32), -1.2723632]. 
=============================================
[2019-04-04 04:40:02,176] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:40:02,176] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:40:02,181] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run35
[2019-04-04 04:40:02,463] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:40:02,463] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:40:02,467] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run35
[2019-04-04 04:40:02,605] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:40:02,605] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:40:02,608] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run35
[2019-04-04 04:40:04,253] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:40:04,253] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:40:04,261] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run35
[2019-04-04 04:40:06,674] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:40:06,675] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:40:06,679] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run35
[2019-04-04 04:40:07,342] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:40:07,342] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:40:07,352] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run35
[2019-04-04 04:40:08,613] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:40:08,613] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:40:08,618] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run35
[2019-04-04 04:40:11,692] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:40:11,692] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:40:11,705] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run35
[2019-04-04 04:40:12,349] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:40:12,349] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:40:12,352] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run35
[2019-04-04 04:40:12,682] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:40:12,682] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:40:12,685] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run35
[2019-04-04 04:40:20,421] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:40:20,421] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:40:20,424] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run35
[2019-04-04 04:40:21,814] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.8517835e-30 1.7798637e-25 1.0367793e-27 3.8834723e-26 3.0933171e-26
 1.6800414e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:40:21,814] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3620
[2019-04-04 04:40:21,901] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 78.0, 11.33333333333333, 110.6666666666666, 26.0, 23.93666262079129, 0.06289755577489771, 1.0, 1.0, 109116.2438965528], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 201000.0000, 
sim time next is 201600.0000, 
raw observation next is [-8.9, 78.0, 17.0, 157.0, 26.0, 24.40427705977291, 0.1259352286072122, 1.0, 1.0, 90169.12504313787], 
processed observation next is [1.0, 0.34782608695652173, 0.21606648199445982, 0.78, 0.056666666666666664, 0.1734806629834254, 0.6666666666666666, 0.5336897549810757, 0.5419784095357374, 1.0, 1.0, 0.42937678591970413], 
reward next is 0.5706, 
noisyNet noise sample is [array([-0.31330657], dtype=float32), 0.12209473]. 
=============================================
[2019-04-04 04:40:29,568] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [9.2484974e-29 2.1116579e-25 7.0627842e-27 1.6050864e-25 2.2918682e-25
 1.7133153e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:40:29,568] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5781
[2019-04-04 04:40:29,646] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.466666666666667, 65.66666666666667, 30.83333333333334, 7.5, 26.0, 25.02803481863253, 0.2512005557983419, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 116400.0000, 
sim time next is 117000.0000, 
raw observation next is [-7.55, 64.5, 37.0, 9.0, 26.0, 25.16772768728775, 0.2668540931174303, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.25346260387811637, 0.645, 0.12333333333333334, 0.009944751381215469, 0.6666666666666666, 0.5973106406073126, 0.5889513643724767, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.01764734], dtype=float32), 0.31078607]. 
=============================================
[2019-04-04 04:40:29,659] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[83.73624]
 [84.05305]
 [84.20833]
 [83.67404]
 [82.61505]], R is [[83.3481369 ]
 [83.51465607]
 [83.67951202]
 [83.84272003]
 [83.48069   ]].
[2019-04-04 04:40:45,044] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.1999165e-28 1.3095487e-25 3.2689498e-27 8.6355736e-25 6.8145656e-25
 7.6749993e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:40:45,044] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.5999
[2019-04-04 04:40:45,087] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.6, 50.83333333333334, 98.33333333333333, 691.3333333333334, 26.0, 25.71794882284061, 0.4203006007855087, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 301800.0000, 
sim time next is 302400.0000, 
raw observation next is [-10.6, 49.0, 94.5, 708.0, 26.0, 25.91247653886598, 0.4385979486473259, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.1689750692520776, 0.49, 0.315, 0.7823204419889502, 0.6666666666666666, 0.6593730449054984, 0.6461993162157753, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5391], dtype=float32), -0.20566426]. 
=============================================
[2019-04-04 04:40:57,128] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.8232479e-27 4.1114673e-23 8.7003445e-26 2.1771782e-24 4.3546656e-24
 7.1866615e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 04:40:57,128] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9725
[2019-04-04 04:40:57,147] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-16.0, 76.33333333333334, 0.0, 0.0, 26.0, 22.0801167179802, -0.4147324806918082, 0.0, 1.0, 48610.76102454025], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 366000.0000, 
sim time next is 366600.0000, 
raw observation next is [-16.1, 77.16666666666666, 0.0, 0.0, 26.0, 21.9875273075273, -0.4363105681234463, 0.0, 1.0, 48660.89965763258], 
processed observation next is [1.0, 0.21739130434782608, 0.01662049861495839, 0.7716666666666666, 0.0, 0.0, 0.6666666666666666, 0.33229394229394177, 0.35456314395885125, 0.0, 1.0, 0.2317185697982504], 
reward next is 0.7683, 
noisyNet noise sample is [array([-0.17199591], dtype=float32), 1.8042125]. 
=============================================
[2019-04-04 04:41:11,188] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.1924189e-29 8.6144508e-25 3.3086190e-27 4.0708551e-25 3.0844811e-25
 2.3076822e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:41:11,188] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5281
[2019-04-04 04:41:11,239] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 76.0, 9.666666666666664, 0.0, 26.0, 25.49463964297966, 0.2790805702847839, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 719400.0000, 
sim time next is 720000.0000, 
raw observation next is [-2.3, 76.0, 14.5, 0.0, 26.0, 25.6168682738347, 0.2850177096609972, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.3988919667590028, 0.76, 0.04833333333333333, 0.0, 0.6666666666666666, 0.6347390228195584, 0.5950059032203324, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.189105], dtype=float32), -0.89483804]. 
=============================================
[2019-04-04 04:41:11,245] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[85.39319 ]
 [85.5186  ]
 [85.43212 ]
 [84.048935]
 [82.456894]], R is [[85.11213684]
 [85.26101685]
 [85.40840912]
 [84.66998291]
 [83.85939026]].
[2019-04-04 04:41:12,115] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.5783198e-28 1.3549021e-24 8.1237898e-27 2.0746734e-25 3.0599054e-25
 4.2761033e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:41:12,116] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9971
[2019-04-04 04:41:12,154] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.316666666666666, 43.83333333333334, 0.0, 0.0, 26.0, 22.50416957045627, -0.3218370096216475, 0.0, 1.0, 46556.55271461963], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 454200.0000, 
sim time next is 454800.0000, 
raw observation next is [-9.133333333333333, 43.66666666666667, 0.0, 0.0, 26.0, 22.50815835960205, -0.3303536651065994, 0.0, 1.0, 46479.65978812299], 
processed observation next is [1.0, 0.2608695652173913, 0.20960295475530935, 0.4366666666666667, 0.0, 0.0, 0.6666666666666666, 0.3756798633001708, 0.3898821116311335, 0.0, 1.0, 0.22133171327677614], 
reward next is 0.7787, 
noisyNet noise sample is [array([1.2736825], dtype=float32), 0.733774]. 
=============================================
[2019-04-04 04:41:15,610] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.5002985e-27 8.8187076e-24 3.1904869e-26 2.8618795e-24 1.6979766e-24
 4.1791532e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 04:41:15,610] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2923
[2019-04-04 04:41:15,708] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.05, 63.0, 89.0, 38.0, 26.0, 24.84900592234366, 0.2076093521417634, 0.0, 1.0, 57717.92078557186], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 646200.0000, 
sim time next is 646800.0000, 
raw observation next is [-2.933333333333334, 62.33333333333333, 92.83333333333334, 48.33333333333333, 26.0, 24.84607951113493, 0.2131142364524765, 0.0, 1.0, 52108.13162204412], 
processed observation next is [0.0, 0.4782608695652174, 0.38134810710988, 0.6233333333333333, 0.30944444444444447, 0.05340699815837937, 0.6666666666666666, 0.5705066259279109, 0.5710380788174921, 0.0, 1.0, 0.24813396010497202], 
reward next is 0.7519, 
noisyNet noise sample is [array([0.3199191], dtype=float32), 0.8151976]. 
=============================================
[2019-04-04 04:41:32,434] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.7877944e-29 6.6105965e-26 1.9249668e-27 2.2418197e-25 1.7023014e-25
 1.6562677e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:41:32,435] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8084
[2019-04-04 04:41:32,459] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.7, 61.5, 0.0, 0.0, 26.0, 24.9190388811414, 0.2960682432452119, 0.0, 1.0, 44290.44143123835], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 767400.0000, 
sim time next is 768000.0000, 
raw observation next is [-5.8, 62.0, 0.0, 0.0, 26.0, 24.9250012787001, 0.2865554844696554, 0.0, 1.0, 44143.81030173526], 
processed observation next is [1.0, 0.9130434782608695, 0.30193905817174516, 0.62, 0.0, 0.0, 0.6666666666666666, 0.5770834398916751, 0.5955184948232185, 0.0, 1.0, 0.21020862048445363], 
reward next is 0.7898, 
noisyNet noise sample is [array([1.3961495], dtype=float32), -1.4672558]. 
=============================================
[2019-04-04 04:41:32,466] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[81.54707]
 [81.31637]
 [81.08954]
 [81.11945]
 [81.43834]], R is [[81.86155701]
 [81.83203888]
 [81.80134583]
 [81.76652527]
 [81.71767426]].
[2019-04-04 04:41:33,079] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.7901605e-31 8.3852438e-28 7.1952670e-29 1.6979027e-27 2.5735331e-27
 1.3467666e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 04:41:33,079] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6276
[2019-04-04 04:41:33,113] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 26.0, 25.62811346957547, 0.6082423338234634, 0.0, 1.0, 18730.29253464766], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1037400.0000, 
sim time next is 1038000.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 26.0, 25.76586278691668, 0.6354028026233002, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.8614958448753465, 0.75, 0.0, 0.0, 0.6666666666666666, 0.6471552322430568, 0.7118009342077668, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.68198633], dtype=float32), -0.087012686]. 
=============================================
[2019-04-04 04:41:33,145] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[86.24507 ]
 [86.257454]
 [86.3362  ]
 [86.28313 ]
 [86.10669 ]], R is [[86.37807465]
 [86.42510223]
 [86.39775085]
 [86.26197052]
 [86.06034088]].
[2019-04-04 04:41:35,965] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.15542725e-30 1.70667653e-26 1.18281963e-28 8.21876660e-27
 2.47476451e-26 4.36076995e-30 1.00000000e+00], sum to 1.0000
[2019-04-04 04:41:35,980] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2545
[2019-04-04 04:41:36,018] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 81.33333333333334, 44.83333333333333, 0.0, 26.0, 25.49576327600653, 0.2904038053016585, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 897600.0000, 
sim time next is 898200.0000, 
raw observation next is [1.1, 82.0, 48.0, 0.0, 26.0, 25.44412123393289, 0.2931815274754219, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.49307479224376743, 0.82, 0.16, 0.0, 0.6666666666666666, 0.6203434361610741, 0.5977271758251407, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.62585616], dtype=float32), 0.98134166]. 
=============================================
[2019-04-04 04:41:41,817] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.2844215e-32 7.8670236e-29 2.5911721e-30 5.4648604e-28 4.1787523e-28
 8.1222199e-32 1.0000000e+00], sum to 1.0000
[2019-04-04 04:41:41,820] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4678
[2019-04-04 04:41:41,838] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.5, 93.0, 78.0, 0.0, 26.0, 26.47276495812752, 0.6299986621124463, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 986400.0000, 
sim time next is 987000.0000, 
raw observation next is [10.68333333333333, 91.83333333333333, 84.0, 0.0, 26.0, 26.52269950750698, 0.639751271942474, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.7585410895660203, 0.9183333333333333, 0.28, 0.0, 0.6666666666666666, 0.7102249589589151, 0.7132504239808247, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6522822], dtype=float32), 0.9172248]. 
=============================================
[2019-04-04 04:41:41,845] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[91.645355]
 [91.61589 ]
 [91.520325]
 [91.43564 ]
 [91.368225]], R is [[91.80029297]
 [91.8822937 ]
 [91.96347046]
 [92.0438385 ]
 [92.12339783]].
[2019-04-04 04:41:45,576] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.98175505e-33 1.60455748e-28 5.58962610e-31 4.58900416e-29
 8.73201831e-29 1.07959164e-32 1.00000000e+00], sum to 1.0000
[2019-04-04 04:41:45,577] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8977
[2019-04-04 04:41:45,584] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.11666666666667, 80.5, 104.6666666666667, 156.0, 26.0, 26.84453088739691, 0.8202778937479978, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1072200.0000, 
sim time next is 1072800.0000, 
raw observation next is [13.3, 80.0, 107.0, 117.0, 26.0, 26.95312563801329, 0.8352418899979117, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.8310249307479226, 0.8, 0.3566666666666667, 0.1292817679558011, 0.6666666666666666, 0.7460938031677742, 0.7784139633326372, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4681968], dtype=float32), 0.9852779]. 
=============================================
[2019-04-04 04:41:56,055] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.6817397e-30 5.6074278e-27 1.5961936e-28 6.0208738e-27 6.1004967e-27
 1.2344113e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 04:41:56,055] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7776
[2019-04-04 04:41:56,079] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 26.0, 25.38690795567233, 0.4886755752241098, 0.0, 1.0, 38180.31502685676], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1467000.0000, 
sim time next is 1467600.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 26.0, 25.39293120120312, 0.4875456049413745, 0.0, 1.0, 35202.84062433129], 
processed observation next is [1.0, 1.0, 0.5069252077562327, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6160776001002599, 0.6625152016471249, 0.0, 1.0, 0.16763257440157758], 
reward next is 0.8324, 
noisyNet noise sample is [array([-1.1086314], dtype=float32), 0.97706586]. 
=============================================
[2019-04-04 04:41:59,568] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.3502073e-31 5.8098207e-27 9.6454574e-30 3.2482019e-28 6.3028657e-28
 1.0346679e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 04:41:59,576] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2391
[2019-04-04 04:41:59,587] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.0, 96.0, 0.0, 0.0, 26.0, 23.45096544400141, 0.1378830058280725, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1236600.0000, 
sim time next is 1237200.0000, 
raw observation next is [15.0, 96.0, 0.0, 0.0, 26.0, 23.43141006263212, 0.1402564947286637, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.30434782608695654, 0.8781163434903049, 0.96, 0.0, 0.0, 0.6666666666666666, 0.45261750521934346, 0.5467521649095546, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.27557087], dtype=float32), 0.5675969]. 
=============================================
[2019-04-04 04:42:09,574] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.4783715e-29 1.2122122e-24 4.0741292e-27 5.3565085e-26 2.9438837e-25
 5.6486456e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:42:09,575] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8390
[2019-04-04 04:42:09,588] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 86.33333333333333, 0.0, 0.0, 26.0, 25.07678731633122, 0.3891389999760537, 0.0, 1.0, 43279.59381557771], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1743000.0000, 
sim time next is 1743600.0000, 
raw observation next is [-0.6, 85.66666666666667, 0.0, 0.0, 26.0, 25.05453702888274, 0.3846517031207497, 0.0, 1.0, 43325.28732294629], 
processed observation next is [0.0, 0.17391304347826086, 0.44598337950138506, 0.8566666666666667, 0.0, 0.0, 0.6666666666666666, 0.5878780857402285, 0.6282172343735832, 0.0, 1.0, 0.20631089201402997], 
reward next is 0.7937, 
noisyNet noise sample is [array([1.1166074], dtype=float32), 0.89943266]. 
=============================================
[2019-04-04 04:42:09,904] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.93502766e-31 7.56810349e-27 8.04860929e-29 2.97663990e-27
 4.99441282e-27 1.32736465e-30 1.00000000e+00], sum to 1.0000
[2019-04-04 04:42:09,904] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9639
[2019-04-04 04:42:09,923] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.9, 94.5, 0.0, 0.0, 26.0, 25.61634398898589, 0.5887075946301761, 0.0, 1.0, 33647.67002965443], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1647000.0000, 
sim time next is 1647600.0000, 
raw observation next is [7.0, 95.0, 0.0, 0.0, 26.0, 25.69235463489474, 0.599020800087236, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.6565096952908588, 0.95, 0.0, 0.0, 0.6666666666666666, 0.6410295529078951, 0.6996736000290786, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.34511065], dtype=float32), -0.34745678]. 
=============================================
[2019-04-04 04:42:16,505] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1727186e-30 1.2690438e-27 8.4605212e-29 3.0454857e-27 7.0884530e-27
 9.5351531e-31 1.0000000e+00], sum to 1.0000
[2019-04-04 04:42:16,506] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4128
[2019-04-04 04:42:16,530] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 15.0, 0.0, 26.0, 25.83594541847344, 0.4996866579892578, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1442400.0000, 
sim time next is 1443000.0000, 
raw observation next is [1.1, 92.0, 12.0, 0.0, 26.0, 25.85200607246324, 0.5032279217539041, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.49307479224376743, 0.92, 0.04, 0.0, 0.6666666666666666, 0.6543338393719367, 0.6677426405846347, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0870961], dtype=float32), 0.8406084]. 
=============================================
[2019-04-04 04:42:16,554] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[84.27413]
 [84.23118]
 [84.26594]
 [84.13753]
 [83.67129]], R is [[84.39035797]
 [84.54645538]
 [84.70098877]
 [84.85398102]
 [85.00543976]].
[2019-04-04 04:42:27,841] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.0949457e-26 3.2599904e-23 2.3964026e-25 5.4081105e-24 1.9207091e-23
 4.5835179e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 04:42:27,841] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6905
[2019-04-04 04:42:27,856] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.283333333333333, 78.83333333333334, 0.0, 0.0, 26.0, 23.5627916583503, -0.03732287942367261, 0.0, 1.0, 47067.67103482843], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1836600.0000, 
sim time next is 1837200.0000, 
raw observation next is [-6.366666666666667, 78.66666666666667, 0.0, 0.0, 26.0, 23.52472462646916, -0.04494160968814705, 0.0, 1.0, 47063.60924292093], 
processed observation next is [0.0, 0.2608695652173913, 0.28624192059095105, 0.7866666666666667, 0.0, 0.0, 0.6666666666666666, 0.46039371887242986, 0.48501946343728436, 0.0, 1.0, 0.22411242496629014], 
reward next is 0.7759, 
noisyNet noise sample is [array([0.91355807], dtype=float32), -0.19317523]. 
=============================================
[2019-04-04 04:42:28,344] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.7872312e-26 4.2447798e-23 2.5652046e-25 1.6545804e-23 1.8636187e-23
 1.4140812e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 04:42:28,344] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2478
[2019-04-04 04:42:28,494] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.700000000000001, 78.0, 22.66666666666667, 0.0, 26.0, 23.17077767944449, -0.0414509882462037, 0.0, 1.0, 202310.3868408478], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1844400.0000, 
sim time next is 1845000.0000, 
raw observation next is [-6.7, 78.0, 27.0, 0.0, 26.0, 23.50548875717431, 0.05706513801021209, 0.0, 1.0, 203468.6302052165], 
processed observation next is [0.0, 0.34782608695652173, 0.2770083102493075, 0.78, 0.09, 0.0, 0.6666666666666666, 0.45879072976452573, 0.5190217126700707, 0.0, 1.0, 0.9688982390724595], 
reward next is 0.0311, 
noisyNet noise sample is [array([-0.6188957], dtype=float32), 0.5228021]. 
=============================================
[2019-04-04 04:42:28,500] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[78.38985]
 [78.44347]
 [78.41713]
 [78.30499]
 [78.04867]], R is [[77.93914795]
 [77.19637299]
 [77.20038605]
 [77.20410919]
 [77.20755005]].
[2019-04-04 04:42:38,491] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.6559710e-30 3.8175641e-27 2.4309540e-28 6.9868708e-26 1.4231884e-26
 3.0037947e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 04:42:38,492] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1048
[2019-04-04 04:42:38,533] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.266666666666667, 65.0, 212.0, 3.333333333333333, 26.0, 25.59660003855037, 0.3231061144412721, 1.0, 1.0, 33028.97887147822], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1946400.0000, 
sim time next is 1947000.0000, 
raw observation next is [-4.083333333333333, 65.0, 197.0, 2.666666666666667, 26.0, 25.57463536907563, 0.3232821883294509, 1.0, 1.0, 25048.88026014361], 
processed observation next is [1.0, 0.5217391304347826, 0.34949215143120965, 0.65, 0.6566666666666666, 0.002946593001841621, 0.6666666666666666, 0.6312196140896358, 0.6077607294431503, 1.0, 1.0, 0.11928038219116005], 
reward next is 0.8807, 
noisyNet noise sample is [array([0.23589621], dtype=float32), -0.002916419]. 
=============================================
[2019-04-04 04:42:38,553] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[82.71205 ]
 [83.019   ]
 [83.262955]
 [83.54239 ]
 [83.83618 ]], R is [[82.51789856]
 [82.53543854]
 [82.50608063]
 [82.68102264]
 [82.8542099 ]].
[2019-04-04 04:42:46,664] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.6662187e-28 1.9402304e-24 7.5678767e-26 8.1393076e-25 4.0636115e-24
 1.8314219e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 04:42:46,665] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3991
[2019-04-04 04:42:46,693] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.199999999999999, 87.0, 0.0, 0.0, 26.0, 24.21321926624926, 0.0638995299894536, 0.0, 1.0, 41079.6130000487], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2007600.0000, 
sim time next is 2008200.0000, 
raw observation next is [-6.2, 87.0, 0.0, 0.0, 26.0, 24.15917574545531, 0.05870301665495665, 0.0, 1.0, 41111.63103439956], 
processed observation next is [1.0, 0.21739130434782608, 0.2908587257617729, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5132646454546093, 0.5195676722183189, 0.0, 1.0, 0.19576967159237885], 
reward next is 0.8042, 
noisyNet noise sample is [array([-0.70804626], dtype=float32), 0.9863122]. 
=============================================
[2019-04-04 04:43:13,683] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.2450102e-29 3.0084926e-25 6.3218920e-27 1.4678787e-25 2.6195933e-25
 1.5356185e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:43:13,686] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8505
[2019-04-04 04:43:13,708] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.7, 78.0, 0.0, 0.0, 26.0, 24.49425636089506, 0.161493453998913, 0.0, 1.0, 42439.06314931958], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2170200.0000, 
sim time next is 2170800.0000, 
raw observation next is [-6.7, 78.0, 0.0, 0.0, 26.0, 24.49888018357656, 0.1600654390824777, 0.0, 1.0, 42362.12942320872], 
processed observation next is [1.0, 0.13043478260869565, 0.2770083102493075, 0.78, 0.0, 0.0, 0.6666666666666666, 0.54157334863138, 0.5533551463608258, 0.0, 1.0, 0.2017244258248034], 
reward next is 0.7983, 
noisyNet noise sample is [array([-0.71199083], dtype=float32), -0.1703092]. 
=============================================
[2019-04-04 04:43:20,932] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.9869340e-27 5.5756547e-24 8.3385852e-26 4.0472288e-24 6.7876132e-24
 1.4188842e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 04:43:20,932] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.3354
[2019-04-04 04:43:20,960] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.9, 45.5, 0.0, 0.0, 26.0, 24.55085865280743, 0.134430591915672, 0.0, 1.0, 43152.82445401153], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2421000.0000, 
sim time next is 2421600.0000, 
raw observation next is [-6.0, 46.33333333333334, 0.0, 0.0, 26.0, 24.5056884397177, 0.1233325215974282, 0.0, 1.0, 43191.64768203745], 
processed observation next is [0.0, 0.0, 0.296398891966759, 0.46333333333333343, 0.0, 0.0, 0.6666666666666666, 0.5421407033098085, 0.541110840532476, 0.0, 1.0, 0.20567451277160692], 
reward next is 0.7943, 
noisyNet noise sample is [array([0.30101338], dtype=float32), -0.6394329]. 
=============================================
[2019-04-04 04:43:35,618] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.5673760e-28 9.0198809e-25 3.4664509e-27 1.0760800e-25 2.0982574e-24
 5.0232047e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:43:35,618] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9308
[2019-04-04 04:43:35,801] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 38.66666666666667, 0.0, 0.0, 26.0, 25.02289631806049, 0.2389719353384915, 0.0, 1.0, 39649.20202221813], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2506800.0000, 
sim time next is 2507400.0000, 
raw observation next is [-1.7, 39.0, 0.0, 0.0, 26.0, 25.19064883523308, 0.2443667880841464, 0.0, 1.0, 39292.43838657329], 
processed observation next is [1.0, 0.0, 0.4155124653739613, 0.39, 0.0, 0.0, 0.6666666666666666, 0.5992207362694234, 0.5814555960280489, 0.0, 1.0, 0.1871068494598728], 
reward next is 0.8129, 
noisyNet noise sample is [array([1.1684352], dtype=float32), -0.04519499]. 
=============================================
[2019-04-04 04:43:52,425] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.1484919e-27 1.4829866e-23 8.4266177e-26 3.7158307e-24 9.6703185e-24
 2.5669443e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 04:43:52,425] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5127
[2019-04-04 04:43:52,543] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.55, 29.0, 0.0, 0.0, 26.0, 24.91554849102632, 0.2113485216110403, 0.0, 1.0, 41154.06603762173], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2485800.0000, 
sim time next is 2486400.0000, 
raw observation next is [0.3666666666666668, 29.33333333333334, 0.0, 0.0, 26.0, 24.90555538474279, 0.2109580907247238, 0.0, 1.0, 46503.78432833819], 
processed observation next is [0.0, 0.782608695652174, 0.4727608494921515, 0.2933333333333334, 0.0, 0.0, 0.6666666666666666, 0.5754629487285658, 0.5703193635749079, 0.0, 1.0, 0.22144659203970565], 
reward next is 0.7786, 
noisyNet noise sample is [array([-0.9912662], dtype=float32), -0.2465561]. 
=============================================
[2019-04-04 04:43:54,683] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.0438003e-27 3.2603886e-23 1.2456014e-25 1.0717791e-24 4.8798975e-24
 1.9591492e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 04:43:54,683] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2450
[2019-04-04 04:43:54,729] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.5, 60.0, 0.0, 0.0, 26.0, 22.91118571837527, -0.2239645284725656, 0.0, 1.0, 44171.00750017277], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2445600.0000, 
sim time next is 2446200.0000, 
raw observation next is [-9.5, 59.5, 0.0, 0.0, 26.0, 22.88092063536656, -0.2324413760878249, 0.0, 1.0, 44219.6161881244], 
processed observation next is [0.0, 0.30434782608695654, 0.1994459833795014, 0.595, 0.0, 0.0, 0.6666666666666666, 0.40674338628054674, 0.42251954130405833, 0.0, 1.0, 0.21056960089583046], 
reward next is 0.7894, 
noisyNet noise sample is [array([0.6171421], dtype=float32), 0.53902316]. 
=============================================
[2019-04-04 04:44:10,927] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.4042238e-28 3.2069737e-25 7.9168863e-27 1.6645060e-24 8.1243868e-25
 7.6035449e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:44:10,927] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9087
[2019-04-04 04:44:10,981] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 59.0, 0.0, 0.0, 26.0, 25.11107012385821, 0.3914384855026853, 1.0, 1.0, 84199.29921381727], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2748600.0000, 
sim time next is 2749200.0000, 
raw observation next is [-5.0, 59.0, 0.0, 0.0, 26.0, 25.09173423741802, 0.3884670913963812, 0.0, 1.0, 79195.5097463796], 
processed observation next is [1.0, 0.8260869565217391, 0.32409972299168976, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5909778531181683, 0.6294890304654605, 0.0, 1.0, 0.37712147498276], 
reward next is 0.6229, 
noisyNet noise sample is [array([0.20446904], dtype=float32), -0.5990759]. 
=============================================
[2019-04-04 04:44:17,133] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [9.0962559e-30 8.7660026e-27 2.4615335e-28 8.2215614e-26 2.1983738e-26
 3.5212356e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 04:44:17,134] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3965
[2019-04-04 04:44:17,202] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.533333333333333, 55.33333333333334, 103.1666666666667, 742.1666666666667, 26.0, 26.66551651425414, 0.652406935405158, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2730000.0000, 
sim time next is 2730600.0000, 
raw observation next is [-4.4, 55.0, 102.0, 733.0, 26.0, 26.71360305330706, 0.6604479534888882, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3407202216066482, 0.55, 0.34, 0.8099447513812155, 0.6666666666666666, 0.7261335877755885, 0.7201493178296294, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.30820942], dtype=float32), -0.69143856]. 
=============================================
[2019-04-04 04:44:23,793] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.9033294e-26 7.9239721e-23 4.6463676e-25 1.6503191e-23 3.2191110e-23
 1.1503067e-25 1.0000000e+00], sum to 1.0000
[2019-04-04 04:44:23,793] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9234
[2019-04-04 04:44:23,811] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 72.33333333333333, 0.0, 0.0, 26.0, 23.84151617166037, 0.00982698332444333, 0.0, 1.0, 40147.6477611446], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3044400.0000, 
sim time next is 3045000.0000, 
raw observation next is [-6.0, 71.16666666666667, 0.0, 0.0, 26.0, 23.81500344886235, 0.003339265060000216, 0.0, 1.0, 40163.68999954766], 
processed observation next is [0.0, 0.21739130434782608, 0.296398891966759, 0.7116666666666667, 0.0, 0.0, 0.6666666666666666, 0.48458362073852906, 0.5011130883533333, 0.0, 1.0, 0.19125566666451269], 
reward next is 0.8087, 
noisyNet noise sample is [array([1.638136], dtype=float32), -0.4809809]. 
=============================================
[2019-04-04 04:44:23,829] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[80.47093 ]
 [80.48251 ]
 [80.471466]
 [80.45269 ]
 [80.43003 ]], R is [[80.45412445]
 [80.45840454]
 [80.46264648]
 [80.46676636]
 [80.47076416]].
[2019-04-04 04:44:25,691] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-04 04:44:25,693] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 04:44:25,694] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 04:44:25,695] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:44:25,695] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 04:44:25,697] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:44:25,701] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run48
[2019-04-04 04:44:25,728] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:44:25,739] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run48
[2019-04-04 04:44:25,765] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run48
[2019-04-04 04:46:03,612] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.36355177], dtype=float32), 0.24544951]
[2019-04-04 04:46:03,612] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [1.0, 79.0, 0.0, 0.0, 26.0, 24.98235813754905, 0.4360535231200394, 1.0, 1.0, 59151.31660126976]
[2019-04-04 04:46:03,612] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 04:46:03,612] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.7929456e-28 1.7803866e-25 4.2437346e-27 5.0535988e-25 4.4426526e-25
 1.5893401e-28 1.0000000e+00], sampled 0.5975423186045553
[2019-04-04 04:46:14,777] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 04:46:34,358] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 04:46:38,077] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 04:46:39,099] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 4700000, evaluation results [4700000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 04:46:39,805] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.5348571e-28 9.3137765e-23 6.3111486e-26 7.1312858e-25 3.3765976e-24
 8.1837894e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 04:46:39,805] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7458
[2019-04-04 04:46:39,824] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 72.33333333333334, 0.0, 0.0, 26.0, 23.73304636665446, -0.0154576877729091, 0.0, 1.0, 40257.19456582331], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3046800.0000, 
sim time next is 3047400.0000, 
raw observation next is [-6.0, 73.5, 0.0, 0.0, 26.0, 23.70667593914112, -0.0200800353785007, 0.0, 1.0, 40309.24722992616], 
processed observation next is [0.0, 0.2608695652173913, 0.296398891966759, 0.735, 0.0, 0.0, 0.6666666666666666, 0.4755563282617601, 0.49330665487383313, 0.0, 1.0, 0.1919487963329817], 
reward next is 0.8081, 
noisyNet noise sample is [array([-0.83418584], dtype=float32), -1.5572449]. 
=============================================
[2019-04-04 04:46:44,753] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.7857107e-27 1.6628559e-24 3.5533848e-26 4.4840968e-24 1.7977495e-24
 6.1571453e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 04:46:44,757] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9893
[2019-04-04 04:46:44,773] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 71.0, 0.0, 0.0, 26.0, 24.60031698048386, 0.1895443154541918, 0.0, 1.0, 38095.48688355942], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3028800.0000, 
sim time next is 3029400.0000, 
raw observation next is [-5.0, 71.0, 0.0, 0.0, 26.0, 24.55823194302321, 0.1790266988416405, 0.0, 1.0, 38154.42569959466], 
processed observation next is [0.0, 0.043478260869565216, 0.32409972299168976, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5465193285852674, 0.5596755662805468, 0.0, 1.0, 0.18168774142664126], 
reward next is 0.8183, 
noisyNet noise sample is [array([0.5888818], dtype=float32), -0.68363184]. 
=============================================
[2019-04-04 04:46:49,538] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.9354098e-29 3.4851364e-25 1.6385727e-27 1.2374649e-25 1.3548549e-25
 8.8002354e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:46:49,540] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9566
[2019-04-04 04:46:49,584] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 65.0, 256.0, 284.0, 26.0, 24.9644654129473, 0.3548680661672626, 0.0, 1.0, 49133.55438706467], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2979000.0000, 
sim time next is 2979600.0000, 
raw observation next is [-3.0, 65.0, 243.5, 351.8333333333333, 26.0, 24.96632192637955, 0.3648661732393372, 0.0, 1.0, 44537.14328897791], 
processed observation next is [0.0, 0.4782608695652174, 0.3795013850415513, 0.65, 0.8116666666666666, 0.3887661141804788, 0.6666666666666666, 0.5805268271982958, 0.6216220577464457, 0.0, 1.0, 0.21208163470941863], 
reward next is 0.7879, 
noisyNet noise sample is [array([1.1848981], dtype=float32), 1.2094742]. 
=============================================
[2019-04-04 04:46:53,278] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.9360950e-28 1.3686585e-23 3.7512327e-26 1.8355022e-24 2.9453005e-24
 4.5932474e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 04:46:53,278] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2304
[2019-04-04 04:46:53,302] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 70.5, 0.0, 0.0, 26.0, 23.63688093836484, -0.04756457574431707, 0.0, 1.0, 40782.63756015559], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3051000.0000, 
sim time next is 3051600.0000, 
raw observation next is [-6.0, 68.33333333333333, 14.66666666666666, 118.1666666666667, 26.0, 23.60125769312616, -0.04193116196571851, 0.0, 1.0, 40809.09686901498], 
processed observation next is [0.0, 0.30434782608695654, 0.296398891966759, 0.6833333333333332, 0.04888888888888887, 0.13057090239410685, 0.6666666666666666, 0.4667714744271801, 0.48602294601142715, 0.0, 1.0, 0.19432903270959515], 
reward next is 0.8057, 
noisyNet noise sample is [array([0.33139792], dtype=float32), -1.5745757]. 
=============================================
[2019-04-04 04:47:05,113] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.1096440e-31 2.6119670e-27 5.0084115e-29 4.0620089e-27 3.3554572e-27
 1.1950913e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 04:47:05,113] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3270
[2019-04-04 04:47:05,153] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 92.0, 57.00000000000001, 274.0, 26.0, 25.5623352367891, 0.5168645596369369, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3226200.0000, 
sim time next is 3226800.0000, 
raw observation next is [-3.0, 92.0, 71.00000000000001, 322.0000000000001, 26.0, 25.53967535685825, 0.5276735657996724, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.3795013850415513, 0.92, 0.23666666666666672, 0.3558011049723758, 0.6666666666666666, 0.6283062797381875, 0.6758911885998908, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0381442], dtype=float32), 1.2544428]. 
=============================================
[2019-04-04 04:47:10,679] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.7554062e-30 9.3632027e-27 1.8538858e-28 2.7135838e-26 4.0898272e-26
 2.5232039e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 04:47:10,680] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0704
[2019-04-04 04:47:10,712] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 67.0, 0.0, 0.0, 26.0, 25.71794982184244, 0.3730156912699203, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3433800.0000, 
sim time next is 3434400.0000, 
raw observation next is [2.0, 67.0, 0.0, 0.0, 26.0, 25.73247613922467, 0.4904372516254664, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.518005540166205, 0.67, 0.0, 0.0, 0.6666666666666666, 0.6443730116020557, 0.6634790838751555, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.3825727], dtype=float32), 0.062050324]. 
=============================================
[2019-04-04 04:47:18,761] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.5178976e-29 4.3141851e-26 2.5415330e-27 7.6446844e-25 1.9773878e-25
 7.7042741e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:47:18,768] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8357
[2019-04-04 04:47:18,799] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.3333333333333333, 68.66666666666666, 0.0, 0.0, 26.0, 25.10038256035005, 0.4461173675141495, 0.0, 1.0, 91031.95761190004], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3527400.0000, 
sim time next is 3528000.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.03506243903738, 0.456141600769205, 0.0, 1.0, 103012.3548216757], 
processed observation next is [1.0, 0.8695652173913043, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5862552032531149, 0.6520472002564016, 0.0, 1.0, 0.4905350229603605], 
reward next is 0.5095, 
noisyNet noise sample is [array([-0.15804833], dtype=float32), 0.3482841]. 
=============================================
[2019-04-04 04:47:18,815] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[79.57409 ]
 [79.64151 ]
 [79.999115]
 [80.73629 ]
 [81.75736 ]], R is [[79.53872681]
 [79.3098526 ]
 [79.23085785]
 [79.29736328]
 [79.36151886]].
[2019-04-04 04:47:18,940] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [8.7138783e-30 1.8456670e-26 7.9147381e-28 9.0342354e-26 5.4097147e-26
 1.7965452e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:47:18,944] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.4645
[2019-04-04 04:47:18,957] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.8333333333333334, 77.0, 0.0, 0.0, 26.0, 25.8590077419985, 0.5809542529609356, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3534600.0000, 
sim time next is 3535200.0000, 
raw observation next is [-1.0, 78.0, 0.0, 0.0, 26.0, 25.78840036169179, 0.5639609908622033, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.4349030470914128, 0.78, 0.0, 0.0, 0.6666666666666666, 0.6490333634743157, 0.6879869969540677, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.09121161], dtype=float32), -0.21824135]. 
=============================================
[2019-04-04 04:47:20,459] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.8652939e-29 5.9368129e-25 1.6512856e-27 6.8802291e-26 1.8168873e-25
 3.3064221e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:47:20,459] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0325
[2019-04-04 04:47:20,515] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.333333333333333, 66.66666666666666, 101.8333333333333, 689.6666666666666, 26.0, 25.86295704968023, 0.5216357898017118, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3577200.0000, 
sim time next is 3577800.0000, 
raw observation next is [-5.166666666666667, 65.83333333333334, 103.6666666666667, 703.3333333333334, 26.0, 25.81619406882663, 0.5144090744836186, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.31948291782086796, 0.6583333333333334, 0.34555555555555567, 0.7771639042357275, 0.6666666666666666, 0.6513495057355524, 0.6714696914945395, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.75099033], dtype=float32), 0.9469517]. 
=============================================
[2019-04-04 04:47:20,922] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.5954777e-29 4.5103596e-25 1.2923469e-27 2.0281680e-25 2.8579395e-25
 2.2651724e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:47:20,922] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2254
[2019-04-04 04:47:20,961] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.166666666666667, 55.83333333333333, 112.0, 777.3333333333333, 26.0, 25.43067850607588, 0.4628231785046248, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3581400.0000, 
sim time next is 3582000.0000, 
raw observation next is [-4.0, 54.0, 112.5, 787.0, 26.0, 25.36320656070966, 0.4529625221767962, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.3518005540166205, 0.54, 0.375, 0.8696132596685083, 0.6666666666666666, 0.613600546725805, 0.6509875073922654, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4866506], dtype=float32), -1.6449653]. 
=============================================
[2019-04-04 04:47:20,966] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[85.111855]
 [85.40637 ]
 [85.67286 ]
 [85.89941 ]
 [86.09805 ]], R is [[84.97181702]
 [85.12210083]
 [85.27088165]
 [85.41817474]
 [85.56399536]].
[2019-04-04 04:47:21,336] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.6032308e-28 2.6005561e-25 3.6957652e-27 2.2452515e-25 7.0199485e-25
 3.3908411e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:47:21,340] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5623
[2019-04-04 04:47:21,391] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.11703279948885, 0.3475624633871848, 0.0, 1.0, 18708.4840392683], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3610800.0000, 
sim time next is 3611400.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.11186532655789, 0.3391489445647, 0.0, 1.0, 18708.18457163546], 
processed observation next is [0.0, 0.8260869565217391, 0.4349030470914128, 0.42, 0.0, 0.0, 0.6666666666666666, 0.5926554438798242, 0.6130496481882334, 0.0, 1.0, 0.08908659319826409], 
reward next is 0.9109, 
noisyNet noise sample is [array([0.15612896], dtype=float32), -0.13839552]. 
=============================================
[2019-04-04 04:47:31,159] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.3115497e-28 1.0859611e-24 1.5897358e-26 1.2800484e-24 1.6708677e-24
 2.0776967e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 04:47:31,160] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8095
[2019-04-04 04:47:31,186] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.5, 68.5, 0.0, 0.0, 26.0, 25.42899569122917, 0.4046751132321587, 0.0, 1.0, 37287.8279490995], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3709800.0000, 
sim time next is 3710400.0000, 
raw observation next is [-2.0, 67.33333333333333, 0.0, 0.0, 26.0, 25.41443848365369, 0.4020189582364753, 0.0, 1.0, 45464.70628726168], 
processed observation next is [0.0, 0.9565217391304348, 0.40720221606648205, 0.6733333333333333, 0.0, 0.0, 0.6666666666666666, 0.6178698736378075, 0.6340063194121585, 0.0, 1.0, 0.21649860136791277], 
reward next is 0.7835, 
noisyNet noise sample is [array([0.1032813], dtype=float32), 0.4070514]. 
=============================================
[2019-04-04 04:47:39,997] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2248437e-28 1.3316535e-25 5.2992332e-27 5.5574960e-25 5.5313541e-25
 4.4375646e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:47:39,997] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4408
[2019-04-04 04:47:40,014] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 41.0, 0.0, 0.0, 26.0, 25.37455134778207, 0.4340385280483436, 0.0, 1.0, 54887.67942544416], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4148400.0000, 
sim time next is 4149000.0000, 
raw observation next is [-1.0, 40.5, 0.0, 0.0, 26.0, 25.37168205379473, 0.4364729069500992, 0.0, 1.0, 46980.92900460962], 
processed observation next is [0.0, 0.0, 0.4349030470914128, 0.405, 0.0, 0.0, 0.6666666666666666, 0.6143068378162274, 0.6454909689833664, 0.0, 1.0, 0.2237187095457601], 
reward next is 0.7763, 
noisyNet noise sample is [array([-1.9963863], dtype=float32), 1.3701065]. 
=============================================
[2019-04-04 04:47:40,059] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[80.57188 ]
 [80.55849 ]
 [80.51157 ]
 [80.755165]
 [80.87275 ]], R is [[80.57223511]
 [80.50514221]
 [80.41088104]
 [80.33824921]
 [80.33020782]].
[2019-04-04 04:47:47,750] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0824475e-28 3.5477808e-24 1.6636473e-26 2.4577092e-25 1.2694573e-24
 6.7751562e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:47:47,751] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6473
[2019-04-04 04:47:47,782] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.0, 63.0, 0.0, 0.0, 26.0, 24.18022256450548, 0.1137711891745468, 0.0, 1.0, 43643.89971858619], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3988800.0000, 
sim time next is 3989400.0000, 
raw observation next is [-12.16666666666667, 64.0, 0.0, 0.0, 26.0, 24.11401463913138, 0.1050442692036519, 0.0, 1.0, 43686.3789886024], 
processed observation next is [1.0, 0.17391304347826086, 0.12557710064635264, 0.64, 0.0, 0.0, 0.6666666666666666, 0.509501219927615, 0.5350147564012173, 0.0, 1.0, 0.20803037613620193], 
reward next is 0.7920, 
noisyNet noise sample is [array([-0.11467152], dtype=float32), 0.020372262]. 
=============================================
[2019-04-04 04:47:49,450] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.8014393e-28 2.9930847e-25 3.0556799e-27 1.4268693e-25 6.9290914e-25
 1.9416479e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:47:49,450] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.2688
[2019-04-04 04:47:49,468] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 43.66666666666667, 0.0, 0.0, 26.0, 25.4605922537011, 0.3754897795999305, 0.0, 1.0, 63657.87193689325], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4223400.0000, 
sim time next is 4224000.0000, 
raw observation next is [1.0, 44.33333333333334, 0.0, 0.0, 26.0, 25.4327301756918, 0.3769240224889601, 0.0, 1.0, 62938.51109537486], 
processed observation next is [0.0, 0.9130434782608695, 0.4903047091412743, 0.4433333333333334, 0.0, 0.0, 0.6666666666666666, 0.6193941813076499, 0.6256413408296534, 0.0, 1.0, 0.29970719569226123], 
reward next is 0.7003, 
noisyNet noise sample is [array([0.47157553], dtype=float32), -0.88774836]. 
=============================================
[2019-04-04 04:47:49,483] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[83.11132]
 [82.95049]
 [82.81053]
 [82.84739]
 [82.96237]], R is [[83.06202698]
 [82.92827606]
 [82.86802673]
 [82.95009613]
 [83.12059784]].
[2019-04-04 04:48:03,996] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.4844310e-29 3.6782002e-26 3.1200512e-27 1.8460834e-25 2.1440855e-25
 9.2706094e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:48:03,999] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5044
[2019-04-04 04:48:04,039] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 73.0, 0.0, 0.0, 26.0, 25.57266697065237, 0.4723028009517899, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4497000.0000, 
sim time next is 4497600.0000, 
raw observation next is [-0.6, 73.0, 0.0, 0.0, 26.0, 25.53701836909476, 0.441501536019045, 0.0, 1.0, 29232.47208240464], 
processed observation next is [1.0, 0.043478260869565216, 0.44598337950138506, 0.73, 0.0, 0.0, 0.6666666666666666, 0.62808486409123, 0.647167178673015, 0.0, 1.0, 0.13920224801145067], 
reward next is 0.8608, 
noisyNet noise sample is [array([0.3677428], dtype=float32), -0.5890859]. 
=============================================
[2019-04-04 04:48:08,174] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.9114620e-29 1.5968417e-25 4.3386915e-27 8.0590363e-26 4.0476436e-25
 2.6523594e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:48:08,174] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4931
[2019-04-04 04:48:08,187] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.15, 73.0, 0.0, 0.0, 26.0, 25.81289900825547, 0.4599606856444589, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4308600.0000, 
sim time next is 4309200.0000, 
raw observation next is [5.1, 73.0, 0.0, 0.0, 26.0, 25.78695217962955, 0.4490501155051628, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.9130434782608695, 0.6038781163434903, 0.73, 0.0, 0.0, 0.6666666666666666, 0.648912681635796, 0.6496833718350542, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7122658], dtype=float32), -1.893165]. 
=============================================
[2019-04-04 04:48:10,936] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2757310e-29 1.1586302e-25 1.1675202e-27 4.6848797e-26 1.0934804e-25
 8.3048597e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:48:10,937] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6669
[2019-04-04 04:48:10,949] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.25, 73.0, 0.0, 0.0, 26.0, 25.78729451476448, 0.473415845427186, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4307400.0000, 
sim time next is 4308000.0000, 
raw observation next is [5.199999999999999, 73.0, 0.0, 0.0, 26.0, 25.80819900145363, 0.4680271936802159, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.6066481994459835, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6506832501211358, 0.6560090645600719, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2156205], dtype=float32), 0.9420588]. 
=============================================
[2019-04-04 04:48:10,972] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[85.33746 ]
 [85.577484]
 [85.78749 ]
 [86.1171  ]
 [86.16607 ]], R is [[85.36850739]
 [85.51482391]
 [85.6596756 ]
 [85.8030777 ]
 [85.65850067]].
[2019-04-04 04:48:17,830] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.1581848e-31 1.5061352e-28 3.3298734e-30 6.8094099e-28 5.2471913e-28
 1.1052066e-31 1.0000000e+00], sum to 1.0000
[2019-04-04 04:48:17,832] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7408
[2019-04-04 04:48:17,862] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.666666666666667, 50.0, 121.6666666666667, 837.3333333333334, 26.0, 26.68503846367531, 0.7140385586662207, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4621200.0000, 
sim time next is 4621800.0000, 
raw observation next is [2.833333333333333, 49.5, 121.3333333333333, 841.6666666666666, 26.0, 26.39314211878886, 0.7062874159345678, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.541089566020314, 0.495, 0.40444444444444433, 0.9300184162062615, 0.6666666666666666, 0.6994285098990716, 0.7354291386448559, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4133964], dtype=float32), -0.7054149]. 
=============================================
[2019-04-04 04:48:19,372] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1796963e-29 3.0731343e-26 7.9653506e-28 4.4461258e-26 3.5299714e-26
 6.9331718e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 04:48:19,373] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8712
[2019-04-04 04:48:19,380] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.833333333333333, 52.83333333333334, 0.0, 0.0, 26.0, 26.12868891041919, 0.6414891887360449, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4648200.0000, 
sim time next is 4648800.0000, 
raw observation next is [2.666666666666667, 52.66666666666667, 0.0, 0.0, 26.0, 26.05518100750793, 0.6233009045685032, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.5364727608494922, 0.5266666666666667, 0.0, 0.0, 0.6666666666666666, 0.6712650839589941, 0.707766968189501, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.02735338], dtype=float32), -0.06721575]. 
=============================================
[2019-04-04 04:48:19,906] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.5253478e-30 1.7503369e-27 3.6868501e-28 2.7866163e-26 3.5074879e-26
 6.0971606e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 04:48:19,907] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4029
[2019-04-04 04:48:19,912] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.933333333333334, 59.66666666666667, 0.0, 0.0, 26.0, 26.75638897936856, 0.8241747747618776, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4396800.0000, 
sim time next is 4397400.0000, 
raw observation next is [9.8, 60.0, 0.0, 0.0, 26.0, 26.70083944653212, 0.8124031992566279, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.7340720221606649, 0.6, 0.0, 0.0, 0.6666666666666666, 0.7250699538776768, 0.7708010664188759, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.7305801], dtype=float32), -0.66209966]. 
=============================================
[2019-04-04 04:48:26,926] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.9314009e-29 2.5861227e-25 1.0718919e-27 6.2513899e-26 1.4811249e-25
 9.9357492e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:48:26,944] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3713
[2019-04-04 04:48:26,961] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.6666666666666667, 37.0, 0.0, 0.0, 26.0, 25.42808167324989, 0.3662770259017269, 0.0, 1.0, 42986.36922649428], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4918800.0000, 
sim time next is 4919400.0000, 
raw observation next is [0.5, 37.5, 0.0, 0.0, 26.0, 25.46122862772531, 0.3625058834467534, 0.0, 1.0, 21793.27463765453], 
processed observation next is [0.0, 0.9565217391304348, 0.4764542936288089, 0.375, 0.0, 0.0, 0.6666666666666666, 0.6217690523104423, 0.6208352944822512, 0.0, 1.0, 0.10377749827454538], 
reward next is 0.8962, 
noisyNet noise sample is [array([-0.53483003], dtype=float32), 0.7819027]. 
=============================================
[2019-04-04 04:48:27,108] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.0452609e-28 5.1397873e-24 3.5662038e-26 1.4699640e-24 2.2701683e-24
 2.7273119e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 04:48:27,110] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7539
[2019-04-04 04:48:27,132] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.35731626691095, 0.2236887321930442, 0.0, 1.0, 41127.7293927049], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4768200.0000, 
sim time next is 4768800.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.40878652011274, 0.2210887770015194, 0.0, 1.0, 41156.66989949167], 
processed observation next is [0.0, 0.17391304347826086, 0.296398891966759, 0.92, 0.0, 0.0, 0.6666666666666666, 0.5340655433427285, 0.5736962590005065, 0.0, 1.0, 0.1959841423785318], 
reward next is 0.8040, 
noisyNet noise sample is [array([0.75768894], dtype=float32), -0.502361]. 
=============================================
[2019-04-04 04:48:29,153] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.0964887e-31 1.6646558e-27 6.2494222e-30 1.5828660e-28 2.0473654e-28
 2.3812140e-31 1.0000000e+00], sum to 1.0000
[2019-04-04 04:48:29,154] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9414
[2019-04-04 04:48:29,213] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.166666666666667, 40.16666666666666, 98.0, 612.3333333333333, 26.0, 25.52268391453682, 0.3810636058209161, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4956600.0000, 
sim time next is 4957200.0000, 
raw observation next is [-1.0, 39.0, 100.5, 638.5, 26.0, 25.70443401686995, 0.4102789829749721, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4349030470914128, 0.39, 0.335, 0.7055248618784531, 0.6666666666666666, 0.6420361680724959, 0.6367596609916574, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.975157], dtype=float32), -0.086482696]. 
=============================================
[2019-04-04 04:48:29,519] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.0901428e-27 2.9045089e-24 1.2112402e-26 1.5358618e-24 2.9276914e-24
 9.3581273e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:48:29,526] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1301
[2019-04-04 04:48:29,556] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.333333333333333, 42.0, 0.0, 0.0, 26.0, 25.02042626856862, 0.339442212343933, 0.0, 1.0, 19654.07437361286], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4819200.0000, 
sim time next is 4819800.0000, 
raw observation next is [1.166666666666667, 42.5, 0.0, 0.0, 26.0, 25.01017142553801, 0.3332252937592092, 0.0, 1.0, 31028.96719910811], 
processed observation next is [0.0, 0.782608695652174, 0.49492151431209613, 0.425, 0.0, 0.0, 0.6666666666666666, 0.5841809521281677, 0.6110750979197364, 0.0, 1.0, 0.14775698666241957], 
reward next is 0.8522, 
noisyNet noise sample is [array([0.15889755], dtype=float32), 2.2245824]. 
=============================================
[2019-04-04 04:48:30,646] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.9307622e-31 5.0968642e-27 3.7408667e-29 5.1563739e-27 7.7602659e-27
 9.8725689e-31 1.0000000e+00], sum to 1.0000
[2019-04-04 04:48:30,650] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0035
[2019-04-04 04:48:30,662] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.5, 27.0, 122.0, 845.0, 26.0, 26.82753252111416, 0.6637460303462425, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4966200.0000, 
sim time next is 4966800.0000, 
raw observation next is [5.0, 26.33333333333333, 122.1666666666667, 848.3333333333334, 26.0, 26.9087933177935, 0.681842083812452, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.6011080332409973, 0.2633333333333333, 0.4072222222222223, 0.9373848987108656, 0.6666666666666666, 0.7423994431494583, 0.7272806946041507, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.04122658], dtype=float32), -0.81291664]. 
=============================================
[2019-04-04 04:48:32,504] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:48:32,504] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:48:32,512] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run36
[2019-04-04 04:48:33,976] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.1532788e-28 1.6732915e-24 7.8115888e-27 2.7469653e-25 4.6690034e-25
 1.4873402e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 04:48:33,977] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5505
[2019-04-04 04:48:33,997] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.066666666666666, 92.33333333333334, 0.0, 0.0, 26.0, 24.09174086750239, 0.1410417408938902, 0.0, 1.0, 41648.13431327002], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4774800.0000, 
sim time next is 4775400.0000, 
raw observation next is [-6.1, 92.5, 0.0, 0.0, 26.0, 24.05196759549414, 0.1329015117225329, 0.0, 1.0, 41703.0471194319], 
processed observation next is [0.0, 0.2608695652173913, 0.29362880886426596, 0.925, 0.0, 0.0, 0.6666666666666666, 0.5043306329578451, 0.544300503907511, 0.0, 1.0, 0.19858593866396143], 
reward next is 0.8014, 
noisyNet noise sample is [array([1.6389939], dtype=float32), -0.8756205]. 
=============================================
[2019-04-04 04:48:36,237] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:48:36,238] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:48:36,252] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run36
[2019-04-04 04:48:36,329] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:48:36,330] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:48:36,343] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run36
[2019-04-04 04:48:38,437] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:48:38,437] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:48:38,447] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:48:38,447] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:48:38,449] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run36
[2019-04-04 04:48:38,492] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run36
[2019-04-04 04:48:45,020] A3C_AGENT_WORKER-Thread-17 INFO:Local step 297500, global step 4756589: loss 0.0377
[2019-04-04 04:48:45,021] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 297500, global step 4756589: learning rate 0.0001
[2019-04-04 04:48:45,243] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:48:45,243] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:48:45,251] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run36
[2019-04-04 04:48:45,657] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:48:45,658] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:48:45,674] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run36
[2019-04-04 04:48:46,097] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:48:46,098] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:48:46,101] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run36
[2019-04-04 04:48:46,123] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:48:46,127] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:48:46,136] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run36
[2019-04-04 04:48:46,673] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:48:46,673] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:48:46,729] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run36
[2019-04-04 04:48:49,017] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:48:49,017] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:48:49,025] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run36
[2019-04-04 04:48:49,267] A3C_AGENT_WORKER-Thread-19 INFO:Local step 297500, global step 4757733: loss 0.0730
[2019-04-04 04:48:49,268] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 297500, global step 4757733: learning rate 0.0001
[2019-04-04 04:48:49,893] A3C_AGENT_WORKER-Thread-7 INFO:Local step 297500, global step 4757861: loss 0.0771
[2019-04-04 04:48:49,895] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 297500, global step 4757861: learning rate 0.0001
[2019-04-04 04:48:51,643] A3C_AGENT_WORKER-Thread-14 INFO:Local step 297500, global step 4758215: loss 0.0694
[2019-04-04 04:48:51,663] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 297500, global step 4758215: learning rate 0.0001
[2019-04-04 04:48:52,582] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:48:52,583] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:48:52,585] A3C_AGENT_WORKER-Thread-5 INFO:Local step 297500, global step 4758390: loss 0.0597
[2019-04-04 04:48:52,702] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 297500, global step 4758426: learning rate 0.0001
[2019-04-04 04:48:53,061] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run36
[2019-04-04 04:48:53,741] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:48:53,741] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:48:53,747] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run36
[2019-04-04 04:48:54,957] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:48:54,957] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:48:54,960] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run36
[2019-04-04 04:48:55,191] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:48:55,191] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:48:55,207] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run36
[2019-04-04 04:48:55,861] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3710215e-29 4.1455892e-26 4.7676454e-28 8.8119054e-26 2.1723228e-26
 4.5971143e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 04:48:55,862] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4284
[2019-04-04 04:48:55,948] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.4, 61.0, 157.0, 308.0, 26.0, 25.56785283841909, 0.3903668561274127, 1.0, 1.0, 47615.25983126828], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 129600.0000, 
sim time next is 130200.0000, 
raw observation next is [-8.3, 61.0, 148.0, 406.3333333333334, 26.0, 25.70727071211084, 0.4028175739444828, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.23268698060941828, 0.61, 0.49333333333333335, 0.44898710865561703, 0.6666666666666666, 0.6422725593425701, 0.634272524648161, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5431931], dtype=float32), -0.110977374]. 
=============================================
[2019-04-04 04:49:00,058] A3C_AGENT_WORKER-Thread-3 INFO:Local step 297500, global step 4759982: loss 0.1213
[2019-04-04 04:49:00,059] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 297500, global step 4759982: learning rate 0.0001
[2019-04-04 04:49:00,529] A3C_AGENT_WORKER-Thread-6 INFO:Local step 297500, global step 4760119: loss 0.1208
[2019-04-04 04:49:00,530] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 297500, global step 4760119: learning rate 0.0001
[2019-04-04 04:49:00,686] A3C_AGENT_WORKER-Thread-15 INFO:Local step 297500, global step 4760174: loss 0.1140
[2019-04-04 04:49:00,689] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 297500, global step 4760174: learning rate 0.0001
[2019-04-04 04:49:00,917] A3C_AGENT_WORKER-Thread-2 INFO:Local step 297500, global step 4760254: loss 0.1149
[2019-04-04 04:49:00,919] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 297500, global step 4760254: learning rate 0.0001
[2019-04-04 04:49:01,281] A3C_AGENT_WORKER-Thread-9 INFO:Local step 297500, global step 4760413: loss 0.1270
[2019-04-04 04:49:01,282] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 297500, global step 4760413: learning rate 0.0001
[2019-04-04 04:49:01,937] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:49:01,937] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:49:01,942] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run36
[2019-04-04 04:49:03,507] A3C_AGENT_WORKER-Thread-8 INFO:Local step 297500, global step 4761075: loss 0.1384
[2019-04-04 04:49:03,512] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 297500, global step 4761075: learning rate 0.0001
[2019-04-04 04:49:06,988] A3C_AGENT_WORKER-Thread-10 INFO:Local step 297500, global step 4761864: loss 0.1646
[2019-04-04 04:49:06,988] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 297500, global step 4761864: learning rate 0.0001
[2019-04-04 04:49:07,291] A3C_AGENT_WORKER-Thread-4 INFO:Local step 297500, global step 4761958: loss 0.1676
[2019-04-04 04:49:07,311] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 297500, global step 4761959: learning rate 0.0001
[2019-04-04 04:49:09,327] A3C_AGENT_WORKER-Thread-20 INFO:Local step 297500, global step 4762701: loss 0.1166
[2019-04-04 04:49:09,328] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 297500, global step 4762701: learning rate 0.0001
[2019-04-04 04:49:09,394] A3C_AGENT_WORKER-Thread-18 INFO:Local step 297500, global step 4762729: loss 0.1038
[2019-04-04 04:49:09,395] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 297500, global step 4762729: learning rate 0.0001
[2019-04-04 04:49:10,779] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1754076e-27 5.2137616e-24 1.4404652e-25 2.7591280e-24 6.2801264e-24
 1.4789900e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 04:49:10,780] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4686
[2019-04-04 04:49:10,822] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-16.0, 76.33333333333334, 0.0, 0.0, 26.0, 22.0801167179802, -0.4147324806918082, 0.0, 1.0, 48610.76102454025], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 366000.0000, 
sim time next is 366600.0000, 
raw observation next is [-16.1, 77.16666666666666, 0.0, 0.0, 26.0, 21.9875273075273, -0.4363105681234463, 0.0, 1.0, 48660.89965763258], 
processed observation next is [1.0, 0.21739130434782608, 0.01662049861495839, 0.7716666666666666, 0.0, 0.0, 0.6666666666666666, 0.33229394229394177, 0.35456314395885125, 0.0, 1.0, 0.2317185697982504], 
reward next is 0.7683, 
noisyNet noise sample is [array([-1.0261968], dtype=float32), -0.17295678]. 
=============================================
[2019-04-04 04:49:13,361] A3C_AGENT_WORKER-Thread-17 INFO:Local step 298000, global step 4763741: loss 0.3591
[2019-04-04 04:49:13,381] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 298000, global step 4763741: learning rate 0.0001
[2019-04-04 04:49:13,385] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.7518355e-27 4.9516610e-24 1.8190024e-25 1.0351808e-23 2.0951391e-23
 1.3724731e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 04:49:13,387] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4017
[2019-04-04 04:49:13,448] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.8, 83.33333333333334, 44.66666666666667, 0.0, 26.0, 24.50110650266059, 0.1817570585806212, 0.0, 1.0, 20765.34044412126], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 56400.0000, 
sim time next is 57000.0000, 
raw observation next is [6.699999999999999, 82.66666666666667, 39.33333333333334, 0.0, 26.0, 24.51504275655866, 0.1854598810933138, 0.0, 1.0, 22896.91173825506], 
processed observation next is [0.0, 0.6521739130434783, 0.6481994459833795, 0.8266666666666667, 0.13111111111111115, 0.0, 0.6666666666666666, 0.5429202297132217, 0.5618199603644379, 0.0, 1.0, 0.10903291303930981], 
reward next is 0.8910, 
noisyNet noise sample is [array([-0.6991908], dtype=float32), 1.3345581]. 
=============================================
[2019-04-04 04:49:13,473] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[76.45205 ]
 [76.609344]
 [76.653046]
 [76.65121 ]
 [76.633606]], R is [[76.42564392]
 [76.5625    ]
 [76.60945129]
 [76.6072998 ]
 [76.58476257]].
[2019-04-04 04:49:16,829] A3C_AGENT_WORKER-Thread-16 INFO:Local step 297500, global step 4764449: loss 0.1117
[2019-04-04 04:49:16,839] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 297500, global step 4764449: learning rate 0.0001
[2019-04-04 04:49:20,125] A3C_AGENT_WORKER-Thread-19 INFO:Local step 298000, global step 4764957: loss 0.2834
[2019-04-04 04:49:20,130] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 298000, global step 4764958: learning rate 0.0001
[2019-04-04 04:49:22,671] A3C_AGENT_WORKER-Thread-7 INFO:Local step 298000, global step 4765352: loss 0.2488
[2019-04-04 04:49:22,702] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 298000, global step 4765352: learning rate 0.0001
[2019-04-04 04:49:24,713] A3C_AGENT_WORKER-Thread-14 INFO:Local step 298000, global step 4765675: loss 0.2061
[2019-04-04 04:49:24,714] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 298000, global step 4765676: learning rate 0.0001
[2019-04-04 04:49:27,550] A3C_AGENT_WORKER-Thread-5 INFO:Local step 298000, global step 4766256: loss 0.1431
[2019-04-04 04:49:27,550] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 298000, global step 4766256: learning rate 0.0001
[2019-04-04 04:49:30,557] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.8046168e-28 2.2842747e-25 8.0437065e-27 4.1239664e-25 1.0444023e-24
 4.4970382e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:49:30,557] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8359
[2019-04-04 04:49:30,640] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.483333333333334, 71.5, 0.0, 0.0, 26.0, 23.7869450901235, 0.02456220456886705, 0.0, 1.0, 44537.80594488809], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 173400.0000, 
sim time next is 174000.0000, 
raw observation next is [-8.566666666666666, 72.0, 0.0, 0.0, 26.0, 23.7726079272146, 0.02181906267209607, 0.0, 1.0, 44482.8852986491], 
processed observation next is [1.0, 0.0, 0.22530009233610343, 0.72, 0.0, 0.0, 0.6666666666666666, 0.48105066060121676, 0.5072730208906987, 0.0, 1.0, 0.21182326332690046], 
reward next is 0.7882, 
noisyNet noise sample is [array([1.188324], dtype=float32), -1.1406804]. 
=============================================
[2019-04-04 04:49:30,848] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[80.03906 ]
 [80.11232 ]
 [80.2699  ]
 [80.28752 ]
 [80.308624]], R is [[79.98616791]
 [79.97422791]
 [79.96215057]
 [79.94993591]
 [79.93757629]].
[2019-04-04 04:49:34,826] A3C_AGENT_WORKER-Thread-3 INFO:Local step 298000, global step 4767589: loss 0.1617
[2019-04-04 04:49:34,827] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 298000, global step 4767589: learning rate 0.0001
[2019-04-04 04:49:36,680] A3C_AGENT_WORKER-Thread-2 INFO:Local step 298000, global step 4767931: loss 0.1369
[2019-04-04 04:49:36,684] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 298000, global step 4767931: learning rate 0.0001
[2019-04-04 04:49:37,227] A3C_AGENT_WORKER-Thread-6 INFO:Local step 298000, global step 4768048: loss 0.1259
[2019-04-04 04:49:37,230] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 298000, global step 4768048: learning rate 0.0001
[2019-04-04 04:49:37,395] A3C_AGENT_WORKER-Thread-15 INFO:Local step 298000, global step 4768077: loss 0.1301
[2019-04-04 04:49:37,396] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 298000, global step 4768077: learning rate 0.0001
[2019-04-04 04:49:38,924] A3C_AGENT_WORKER-Thread-9 INFO:Local step 298000, global step 4768406: loss 0.0860
[2019-04-04 04:49:38,924] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 298000, global step 4768406: learning rate 0.0001
[2019-04-04 04:49:40,482] A3C_AGENT_WORKER-Thread-8 INFO:Local step 298000, global step 4768749: loss 0.0873
[2019-04-04 04:49:40,482] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 298000, global step 4768749: learning rate 0.0001
[2019-04-04 04:49:45,176] A3C_AGENT_WORKER-Thread-4 INFO:Local step 298000, global step 4769795: loss 0.1001
[2019-04-04 04:49:45,177] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 298000, global step 4769795: learning rate 0.0001
[2019-04-04 04:49:46,655] A3C_AGENT_WORKER-Thread-10 INFO:Local step 298000, global step 4770131: loss 0.0801
[2019-04-04 04:49:46,656] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 298000, global step 4770131: learning rate 0.0001
[2019-04-04 04:49:48,739] A3C_AGENT_WORKER-Thread-18 INFO:Local step 298000, global step 4770536: loss 0.0811
[2019-04-04 04:49:48,740] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 298000, global step 4770536: learning rate 0.0001
[2019-04-04 04:49:49,865] A3C_AGENT_WORKER-Thread-20 INFO:Local step 298000, global step 4770774: loss 0.0646
[2019-04-04 04:49:49,868] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 298000, global step 4770774: learning rate 0.0001
[2019-04-04 04:49:52,094] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.8114569e-29 2.9105285e-26 1.4967657e-27 5.7478161e-25 2.4326555e-25
 2.9405219e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:49:52,095] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6011
[2019-04-04 04:49:52,139] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.2, 39.0, 37.0, 707.0, 26.0, 26.33558924052229, 0.3557428223438013, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 401400.0000, 
sim time next is 402000.0000, 
raw observation next is [-9.100000000000001, 38.66666666666667, 34.33333333333334, 656.3333333333334, 26.0, 26.20811688913422, 0.4764981768221332, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.21052631578947364, 0.3866666666666667, 0.11444444444444447, 0.7252302025782689, 0.6666666666666666, 0.684009740761185, 0.6588327256073777, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3376788], dtype=float32), 1.4464008]. 
=============================================
[2019-04-04 04:49:52,142] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[80.15787 ]
 [80.362625]
 [80.48858 ]
 [80.544304]
 [80.64063 ]], R is [[79.94142914]
 [80.14201355]
 [80.34059143]
 [80.53718567]
 [80.73181152]].
[2019-04-04 04:49:53,700] A3C_AGENT_WORKER-Thread-17 INFO:Local step 298500, global step 4771620: loss 0.0173
[2019-04-04 04:49:53,701] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 298500, global step 4771620: learning rate 0.0001
[2019-04-04 04:49:57,022] A3C_AGENT_WORKER-Thread-19 INFO:Local step 298500, global step 4772271: loss 0.0224
[2019-04-04 04:49:57,046] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 298500, global step 4772271: learning rate 0.0001
[2019-04-04 04:49:58,847] A3C_AGENT_WORKER-Thread-16 INFO:Local step 298000, global step 4772703: loss 0.1185
[2019-04-04 04:49:58,848] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 298000, global step 4772703: learning rate 0.0001
[2019-04-04 04:49:59,382] A3C_AGENT_WORKER-Thread-7 INFO:Local step 298500, global step 4772804: loss 0.0130
[2019-04-04 04:49:59,383] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 298500, global step 4772804: learning rate 0.0001
[2019-04-04 04:50:00,804] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.3774020e-28 1.1897548e-24 4.5075343e-27 1.9932464e-25 2.3898208e-25
 7.5558112e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:50:00,804] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.0876
[2019-04-04 04:50:00,862] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.25, 91.5, 34.0, 104.0, 26.0, 24.62283313580432, 0.2713850057909315, 0.0, 1.0, 10432.47593509457], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 549000.0000, 
sim time next is 549600.0000, 
raw observation next is [0.1666666666666667, 91.33333333333334, 52.33333333333333, 103.8333333333333, 26.0, 25.00134839822407, 0.2932608035857546, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.4672206832871654, 0.9133333333333334, 0.17444444444444443, 0.11473296500920807, 0.6666666666666666, 0.5834456998520059, 0.5977536011952516, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.08254899], dtype=float32), 0.99940753]. 
=============================================
[2019-04-04 04:50:01,904] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.8758094e-29 1.8263229e-26 2.9999503e-28 2.3653495e-26 2.1266120e-26
 1.1376144e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:50:01,904] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3050
[2019-04-04 04:50:02,005] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 30.33333333333334, 120.6666666666667, 0.0, 26.0, 24.97879704819736, 0.200490595321976, 1.0, 1.0, 35590.49131209732], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 480000.0000, 
sim time next is 480600.0000, 
raw observation next is [-0.8999999999999999, 31.5, 119.0, 0.0, 26.0, 24.95089509436038, 0.2028666009498425, 1.0, 1.0, 46392.28670483363], 
processed observation next is [1.0, 0.5652173913043478, 0.43767313019390586, 0.315, 0.39666666666666667, 0.0, 0.6666666666666666, 0.579241257863365, 0.5676222003166141, 1.0, 1.0, 0.22091565097539823], 
reward next is 0.7791, 
noisyNet noise sample is [array([0.1858048], dtype=float32), -0.26772273]. 
=============================================
[2019-04-04 04:50:03,891] A3C_AGENT_WORKER-Thread-14 INFO:Local step 298500, global step 4773804: loss 0.0097
[2019-04-04 04:50:03,892] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 298500, global step 4773804: learning rate 0.0001
[2019-04-04 04:50:05,416] A3C_AGENT_WORKER-Thread-5 INFO:Local step 298500, global step 4774151: loss 0.0127
[2019-04-04 04:50:05,417] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 298500, global step 4774151: learning rate 0.0001
[2019-04-04 04:50:09,398] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.0093521e-28 4.9561772e-24 1.1573883e-26 5.6028580e-25 1.0589329e-24
 2.0149098e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 04:50:09,398] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2836
[2019-04-04 04:50:09,412] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 79.0, 9.5, 0.0, 26.0, 23.45535208591805, -0.07952867824251297, 0.0, 1.0, 43972.51526968516], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 633600.0000, 
sim time next is 634200.0000, 
raw observation next is [-4.4, 77.66666666666667, 12.66666666666667, 0.0, 26.0, 23.43182657570526, -0.08399418876199528, 0.0, 1.0, 43984.58184318279], 
processed observation next is [0.0, 0.34782608695652173, 0.3407202216066482, 0.7766666666666667, 0.04222222222222223, 0.0, 0.6666666666666666, 0.45265221464210487, 0.4720019370793349, 0.0, 1.0, 0.20945038972944183], 
reward next is 0.7905, 
noisyNet noise sample is [array([0.5071022], dtype=float32), 1.6428862]. 
=============================================
[2019-04-04 04:50:13,457] A3C_AGENT_WORKER-Thread-3 INFO:Local step 298500, global step 4775787: loss 0.0278
[2019-04-04 04:50:13,492] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 298500, global step 4775787: learning rate 0.0001
[2019-04-04 04:50:13,935] A3C_AGENT_WORKER-Thread-2 INFO:Local step 298500, global step 4775914: loss 0.0325
[2019-04-04 04:50:13,938] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 298500, global step 4775914: learning rate 0.0001
[2019-04-04 04:50:13,954] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.3696820e-28 2.2793927e-24 1.8810647e-26 1.1849582e-24 2.1189455e-24
 1.2229348e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 04:50:13,955] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7407
[2019-04-04 04:50:13,979] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.816666666666666, 71.16666666666667, 0.0, 0.0, 26.0, 24.44670637114744, 0.1037598294071145, 0.0, 1.0, 41050.14463001297], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 691800.0000, 
sim time next is 692400.0000, 
raw observation next is [-3.733333333333333, 71.33333333333334, 0.0, 0.0, 26.0, 24.4236352722201, 0.09745405913226435, 0.0, 1.0, 40991.49900836598], 
processed observation next is [1.0, 0.0, 0.35918744228993543, 0.7133333333333334, 0.0, 0.0, 0.6666666666666666, 0.535302939351675, 0.5324846863774214, 0.0, 1.0, 0.19519761432555227], 
reward next is 0.8048, 
noisyNet noise sample is [array([0.02662468], dtype=float32), -0.7351153]. 
=============================================
[2019-04-04 04:50:15,666] A3C_AGENT_WORKER-Thread-6 INFO:Local step 298500, global step 4776346: loss 0.0254
[2019-04-04 04:50:15,669] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 298500, global step 4776346: learning rate 0.0001
[2019-04-04 04:50:16,187] A3C_AGENT_WORKER-Thread-15 INFO:Local step 298500, global step 4776475: loss 0.0400
[2019-04-04 04:50:16,187] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 298500, global step 4776475: learning rate 0.0001
[2019-04-04 04:50:16,531] A3C_AGENT_WORKER-Thread-9 INFO:Local step 298500, global step 4776568: loss 0.0326
[2019-04-04 04:50:16,576] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 298500, global step 4776568: learning rate 0.0001
[2019-04-04 04:50:18,799] A3C_AGENT_WORKER-Thread-8 INFO:Local step 298500, global step 4777044: loss 0.0269
[2019-04-04 04:50:18,825] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 298500, global step 4777044: learning rate 0.0001
[2019-04-04 04:50:20,179] A3C_AGENT_WORKER-Thread-4 INFO:Local step 298500, global step 4777396: loss 0.0313
[2019-04-04 04:50:20,180] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 298500, global step 4777396: learning rate 0.0001
[2019-04-04 04:50:23,976] A3C_AGENT_WORKER-Thread-10 INFO:Local step 298500, global step 4778360: loss 0.0232
[2019-04-04 04:50:23,977] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 298500, global step 4778360: learning rate 0.0001
[2019-04-04 04:50:25,435] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0834095e-29 3.3649318e-26 5.1602871e-28 5.0399084e-26 6.7447860e-26
 1.0559008e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:50:25,436] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1744
[2019-04-04 04:50:25,502] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.4, 60.0, 0.0, 0.0, 26.0, 25.01415549662105, 0.3138983219119877, 0.0, 1.0, 48549.39408051515], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 765600.0000, 
sim time next is 766200.0000, 
raw observation next is [-5.5, 60.5, 0.0, 0.0, 26.0, 24.99096304383018, 0.3062672255715344, 0.0, 1.0, 45529.57231842505], 
processed observation next is [1.0, 0.8695652173913043, 0.3102493074792244, 0.605, 0.0, 0.0, 0.6666666666666666, 0.5825802536525151, 0.6020890751905115, 0.0, 1.0, 0.21680748723059548], 
reward next is 0.7832, 
noisyNet noise sample is [array([-0.6100682], dtype=float32), -0.5762636]. 
=============================================
[2019-04-04 04:50:26,476] A3C_AGENT_WORKER-Thread-18 INFO:Local step 298500, global step 4779102: loss 0.0226
[2019-04-04 04:50:26,480] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 298500, global step 4779102: learning rate 0.0001
[2019-04-04 04:50:26,889] A3C_AGENT_WORKER-Thread-20 INFO:Local step 298500, global step 4779201: loss 0.0242
[2019-04-04 04:50:26,889] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 298500, global step 4779201: learning rate 0.0001
[2019-04-04 04:50:27,778] A3C_AGENT_WORKER-Thread-17 INFO:Local step 299000, global step 4779407: loss 0.1378
[2019-04-04 04:50:27,778] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 299000, global step 4779407: learning rate 0.0001
[2019-04-04 04:50:29,832] A3C_AGENT_WORKER-Thread-19 INFO:Local step 299000, global step 4779890: loss 0.1326
[2019-04-04 04:50:29,833] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 299000, global step 4779890: learning rate 0.0001
[2019-04-04 04:50:32,381] A3C_AGENT_WORKER-Thread-7 INFO:Local step 299000, global step 4780455: loss 0.1824
[2019-04-04 04:50:32,382] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 299000, global step 4780455: learning rate 0.0001
[2019-04-04 04:50:33,129] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.1961586e-30 1.4132385e-27 9.9122775e-29 8.2294016e-27 7.0030110e-27
 2.4594191e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 04:50:33,129] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5699
[2019-04-04 04:50:33,180] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.899999999999999, 83.33333333333334, 45.66666666666667, 0.0, 26.0, 25.76830698133907, 0.4094998772813001, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 834000.0000, 
sim time next is 834600.0000, 
raw observation next is [-3.9, 82.66666666666667, 42.33333333333334, 0.0, 26.0, 25.9545985033458, 0.4248330232141264, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.3545706371191136, 0.8266666666666667, 0.14111111111111113, 0.0, 0.6666666666666666, 0.66288320861215, 0.6416110077380421, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.0145934], dtype=float32), -0.48425078]. 
=============================================
[2019-04-04 04:50:35,437] A3C_AGENT_WORKER-Thread-14 INFO:Local step 299000, global step 4781345: loss 0.1890
[2019-04-04 04:50:35,471] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 299000, global step 4781345: learning rate 0.0001
[2019-04-04 04:50:35,988] A3C_AGENT_WORKER-Thread-16 INFO:Local step 298500, global step 4781495: loss 0.0074
[2019-04-04 04:50:35,988] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 298500, global step 4781495: learning rate 0.0001
[2019-04-04 04:50:37,950] A3C_AGENT_WORKER-Thread-5 INFO:Local step 299000, global step 4781943: loss 0.2669
[2019-04-04 04:50:37,974] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 299000, global step 4781951: learning rate 0.0001
[2019-04-04 04:50:44,093] A3C_AGENT_WORKER-Thread-3 INFO:Local step 299000, global step 4783619: loss 0.2927
[2019-04-04 04:50:44,094] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 299000, global step 4783620: learning rate 0.0001
[2019-04-04 04:50:45,158] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.7903466e-32 1.3523589e-28 2.6369415e-30 2.8766609e-28 3.9613147e-29
 2.0840497e-32 1.0000000e+00], sum to 1.0000
[2019-04-04 04:50:45,169] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5508
[2019-04-04 04:50:45,179] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.11666666666667, 81.83333333333333, 110.6666666666667, 0.0, 26.0, 26.54773120096637, 0.6813825032078479, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1000200.0000, 
sim time next is 1000800.0000, 
raw observation next is [14.4, 81.0, 106.5, 0.0, 26.0, 26.61579783310861, 0.6913163659584974, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.8614958448753465, 0.81, 0.355, 0.0, 0.6666666666666666, 0.7179831527590507, 0.7304387886528324, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.43248], dtype=float32), 2.2439346]. 
=============================================
[2019-04-04 04:50:45,375] A3C_AGENT_WORKER-Thread-2 INFO:Local step 299000, global step 4784154: loss 0.2620
[2019-04-04 04:50:45,376] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 299000, global step 4784156: learning rate 0.0001
[2019-04-04 04:50:47,319] A3C_AGENT_WORKER-Thread-15 INFO:Local step 299000, global step 4784873: loss 0.2949
[2019-04-04 04:50:47,320] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 299000, global step 4784873: learning rate 0.0001
[2019-04-04 04:50:47,612] A3C_AGENT_WORKER-Thread-6 INFO:Local step 299000, global step 4784977: loss 0.3065
[2019-04-04 04:50:47,613] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 299000, global step 4784977: learning rate 0.0001
[2019-04-04 04:50:48,587] A3C_AGENT_WORKER-Thread-9 INFO:Local step 299000, global step 4785308: loss 0.3418
[2019-04-04 04:50:48,588] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 299000, global step 4785308: learning rate 0.0001
[2019-04-04 04:50:49,416] A3C_AGENT_WORKER-Thread-17 INFO:Local step 299500, global step 4785618: loss 0.1033
[2019-04-04 04:50:49,418] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 299500, global step 4785618: learning rate 0.0001
[2019-04-04 04:50:49,606] A3C_AGENT_WORKER-Thread-8 INFO:Local step 299000, global step 4785693: loss 0.3175
[2019-04-04 04:50:49,606] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 299000, global step 4785693: learning rate 0.0001
[2019-04-04 04:50:50,862] A3C_AGENT_WORKER-Thread-4 INFO:Local step 299000, global step 4786129: loss 0.2936
[2019-04-04 04:50:50,899] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 299000, global step 4786129: learning rate 0.0001
[2019-04-04 04:50:50,916] A3C_AGENT_WORKER-Thread-19 INFO:Local step 299500, global step 4786144: loss 0.0940
[2019-04-04 04:50:50,935] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 299500, global step 4786145: learning rate 0.0001
[2019-04-04 04:50:53,300] A3C_AGENT_WORKER-Thread-10 INFO:Local step 299000, global step 4787013: loss 0.3160
[2019-04-04 04:50:53,301] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 299000, global step 4787013: learning rate 0.0001
[2019-04-04 04:50:53,315] A3C_AGENT_WORKER-Thread-7 INFO:Local step 299500, global step 4787019: loss 0.0934
[2019-04-04 04:50:53,316] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 299500, global step 4787019: learning rate 0.0001
[2019-04-04 04:50:56,558] A3C_AGENT_WORKER-Thread-14 INFO:Local step 299500, global step 4788209: loss 0.1238
[2019-04-04 04:50:56,558] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 299500, global step 4788209: learning rate 0.0001
[2019-04-04 04:50:57,033] A3C_AGENT_WORKER-Thread-20 INFO:Local step 299000, global step 4788414: loss 0.3175
[2019-04-04 04:50:57,034] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 299000, global step 4788414: learning rate 0.0001
[2019-04-04 04:50:57,254] A3C_AGENT_WORKER-Thread-18 INFO:Local step 299000, global step 4788521: loss 0.2905
[2019-04-04 04:50:57,255] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 299000, global step 4788521: learning rate 0.0001
[2019-04-04 04:50:58,189] A3C_AGENT_WORKER-Thread-5 INFO:Local step 299500, global step 4788985: loss 0.0862
[2019-04-04 04:50:58,192] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 299500, global step 4788985: learning rate 0.0001
[2019-04-04 04:51:02,106] A3C_AGENT_WORKER-Thread-3 INFO:Local step 299500, global step 4790829: loss 0.0866
[2019-04-04 04:51:02,106] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 299500, global step 4790829: learning rate 0.0001
[2019-04-04 04:51:03,381] A3C_AGENT_WORKER-Thread-16 INFO:Local step 299000, global step 4791446: loss 0.2297
[2019-04-04 04:51:03,385] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 299000, global step 4791446: learning rate 0.0001
[2019-04-04 04:51:03,387] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0204893e-31 1.2778169e-27 2.6700410e-30 5.5161715e-29 2.7588487e-28
 1.9689666e-31 1.0000000e+00], sum to 1.0000
[2019-04-04 04:51:03,389] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9170
[2019-04-04 04:51:03,420] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.0, 100.0, 75.33333333333333, 0.0, 26.0, 23.30909433454293, 0.123287726885682, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1245000.0000, 
sim time next is 1245600.0000, 
raw observation next is [15.0, 100.0, 76.0, 0.0, 26.0, 23.30472534653298, 0.1230740888890166, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.8781163434903049, 1.0, 0.25333333333333335, 0.0, 0.6666666666666666, 0.442060445544415, 0.5410246962963389, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5936693], dtype=float32), 1.4820071]. 
=============================================
[2019-04-04 04:51:03,433] A3C_AGENT_WORKER-Thread-2 INFO:Local step 299500, global step 4791470: loss 0.0592
[2019-04-04 04:51:03,434] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 299500, global step 4791470: learning rate 0.0001
[2019-04-04 04:51:03,987] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.3924617e-31 1.8858273e-27 7.6455108e-30 2.9729502e-28 2.3767798e-28
 2.9249683e-31 1.0000000e+00], sum to 1.0000
[2019-04-04 04:51:03,988] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.9527
[2019-04-04 04:51:04,002] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.0, 100.0, 75.33333333333333, 0.0, 26.0, 23.30909433454293, 0.123287726885682, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1245000.0000, 
sim time next is 1245600.0000, 
raw observation next is [15.0, 100.0, 76.0, 0.0, 26.0, 23.30472534653298, 0.1230740888890166, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.8781163434903049, 1.0, 0.25333333333333335, 0.0, 0.6666666666666666, 0.442060445544415, 0.5410246962963389, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6636018], dtype=float32), 0.612971]. 
=============================================
[2019-04-04 04:51:04,834] A3C_AGENT_WORKER-Thread-6 INFO:Local step 299500, global step 4792141: loss 0.0968
[2019-04-04 04:51:04,835] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 299500, global step 4792141: learning rate 0.0001
[2019-04-04 04:51:04,980] A3C_AGENT_WORKER-Thread-15 INFO:Local step 299500, global step 4792214: loss 0.0795
[2019-04-04 04:51:04,981] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 299500, global step 4792214: learning rate 0.0001
[2019-04-04 04:51:05,549] A3C_AGENT_WORKER-Thread-9 INFO:Local step 299500, global step 4792516: loss 0.0789
[2019-04-04 04:51:05,553] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 299500, global step 4792516: learning rate 0.0001
[2019-04-04 04:51:06,366] A3C_AGENT_WORKER-Thread-8 INFO:Local step 299500, global step 4792926: loss 0.0845
[2019-04-04 04:51:06,369] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 299500, global step 4792926: learning rate 0.0001
[2019-04-04 04:51:06,908] A3C_AGENT_WORKER-Thread-4 INFO:Local step 299500, global step 4793211: loss 0.0691
[2019-04-04 04:51:06,909] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 299500, global step 4793212: learning rate 0.0001
[2019-04-04 04:51:09,089] A3C_AGENT_WORKER-Thread-10 INFO:Local step 299500, global step 4794288: loss 0.0957
[2019-04-04 04:51:09,090] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 299500, global step 4794288: learning rate 0.0001
[2019-04-04 04:51:09,387] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.3266762e-30 1.7784579e-26 1.4558368e-28 3.1352872e-27 1.6152942e-26
 5.7341359e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 04:51:09,388] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3945
[2019-04-04 04:51:09,399] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.5, 100.0, 0.0, 0.0, 26.0, 25.45865744063256, 0.6000880125633808, 0.0, 1.0, 27896.39346716968], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1290000.0000, 
sim time next is 1290600.0000, 
raw observation next is [5.5, 100.0, 0.0, 0.0, 26.0, 25.50432957085948, 0.5964899583831936, 0.0, 1.0, 18752.32145978201], 
processed observation next is [0.0, 0.9565217391304348, 0.6149584487534627, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6253607975716232, 0.6988299861277313, 0.0, 1.0, 0.08929676885610481], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.26491207], dtype=float32), 0.6060527]. 
=============================================
[2019-04-04 04:51:10,558] A3C_AGENT_WORKER-Thread-17 INFO:Local step 300000, global step 4794984: loss 2.7389
[2019-04-04 04:51:10,558] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 300000, global step 4794985: learning rate 0.0001
[2019-04-04 04:51:10,996] A3C_AGENT_WORKER-Thread-19 INFO:Local step 300000, global step 4795149: loss 2.8135
[2019-04-04 04:51:10,998] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 300000, global step 4795149: learning rate 0.0001
[2019-04-04 04:51:11,150] A3C_AGENT_WORKER-Thread-20 INFO:Local step 299500, global step 4795217: loss 0.0769
[2019-04-04 04:51:11,152] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 299500, global step 4795217: learning rate 0.0001
[2019-04-04 04:51:11,655] A3C_AGENT_WORKER-Thread-18 INFO:Local step 299500, global step 4795457: loss 0.0664
[2019-04-04 04:51:11,659] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 299500, global step 4795459: learning rate 0.0001
[2019-04-04 04:51:12,483] A3C_AGENT_WORKER-Thread-7 INFO:Local step 300000, global step 4795873: loss 2.9470
[2019-04-04 04:51:12,483] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 300000, global step 4795873: learning rate 0.0001
[2019-04-04 04:51:14,696] A3C_AGENT_WORKER-Thread-14 INFO:Local step 300000, global step 4796970: loss 3.0800
[2019-04-04 04:51:14,696] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 300000, global step 4796970: learning rate 0.0001
[2019-04-04 04:51:16,508] A3C_AGENT_WORKER-Thread-5 INFO:Local step 300000, global step 4797763: loss 3.2858
[2019-04-04 04:51:16,511] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 300000, global step 4797766: learning rate 0.0001
[2019-04-04 04:51:16,620] A3C_AGENT_WORKER-Thread-16 INFO:Local step 299500, global step 4797806: loss 0.1242
[2019-04-04 04:51:16,621] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 299500, global step 4797806: learning rate 0.0001
[2019-04-04 04:51:19,698] A3C_AGENT_WORKER-Thread-3 INFO:Local step 300000, global step 4799184: loss 3.9318
[2019-04-04 04:51:19,701] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 300000, global step 4799186: learning rate 0.0001
[2019-04-04 04:51:20,154] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.8287024e-29 5.2883337e-25 1.9140712e-27 5.8368139e-26 9.1038764e-26
 1.3152189e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:51:20,154] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4606
[2019-04-04 04:51:20,177] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.1, 91.0, 0.0, 0.0, 26.0, 25.2696662513741, 0.4377085569046205, 0.0, 1.0, 42942.19173520658], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1737000.0000, 
sim time next is 1737600.0000, 
raw observation next is [0.06666666666666668, 91.0, 0.0, 0.0, 26.0, 25.2758266283212, 0.4335134147952147, 0.0, 1.0, 42954.52738263577], 
processed observation next is [0.0, 0.08695652173913043, 0.46445060018467227, 0.91, 0.0, 0.0, 0.6666666666666666, 0.6063188856934335, 0.6445044715984048, 0.0, 1.0, 0.20454536848874177], 
reward next is 0.7955, 
noisyNet noise sample is [array([-0.5390881], dtype=float32), -0.64589655]. 
=============================================
[2019-04-04 04:51:20,988] A3C_AGENT_WORKER-Thread-2 INFO:Local step 300000, global step 4799705: loss 3.9538
[2019-04-04 04:51:20,988] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 300000, global step 4799705: learning rate 0.0001
[2019-04-04 04:51:21,717] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-04 04:51:21,719] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 04:51:21,720] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:51:21,727] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 04:51:21,728] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:51:21,730] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 04:51:21,734] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:51:21,747] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run49
[2019-04-04 04:51:21,771] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run49
[2019-04-04 04:51:21,799] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run49
[2019-04-04 04:54:05,552] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 04:54:24,926] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 04:54:28,646] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 04:54:29,670] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 4800000, evaluation results [4800000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 04:54:30,319] A3C_AGENT_WORKER-Thread-15 INFO:Local step 300000, global step 4800321: loss 4.4050
[2019-04-04 04:54:30,321] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 300000, global step 4800321: learning rate 0.0001
[2019-04-04 04:54:30,655] A3C_AGENT_WORKER-Thread-6 INFO:Local step 300000, global step 4800456: loss 4.2893
[2019-04-04 04:54:30,655] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 300000, global step 4800456: learning rate 0.0001
[2019-04-04 04:54:31,178] A3C_AGENT_WORKER-Thread-9 INFO:Local step 300000, global step 4800696: loss 4.2104
[2019-04-04 04:54:31,182] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 300000, global step 4800700: learning rate 0.0001
[2019-04-04 04:54:32,155] A3C_AGENT_WORKER-Thread-4 INFO:Local step 300000, global step 4801155: loss 4.5431
[2019-04-04 04:54:32,157] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 300000, global step 4801155: learning rate 0.0001
[2019-04-04 04:54:32,468] A3C_AGENT_WORKER-Thread-8 INFO:Local step 300000, global step 4801294: loss 4.4376
[2019-04-04 04:54:32,469] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 300000, global step 4801294: learning rate 0.0001
[2019-04-04 04:54:32,552] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.2641238e-29 6.8453578e-26 1.5646706e-27 7.4640601e-26 6.6708365e-26
 2.3134544e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:54:32,552] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8153
[2019-04-04 04:54:32,580] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.8, 90.0, 0.0, 0.0, 26.0, 25.59001105207806, 0.5352749009877834, 0.0, 1.0, 63300.28903280217], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1715400.0000, 
sim time next is 1716000.0000, 
raw observation next is [0.7000000000000001, 90.66666666666666, 0.0, 0.0, 26.0, 25.50436320993163, 0.5342512432571993, 0.0, 1.0, 92892.06423388641], 
processed observation next is [1.0, 0.8695652173913043, 0.4819944598337951, 0.9066666666666666, 0.0, 0.0, 0.6666666666666666, 0.6253636008276358, 0.6780837477523999, 0.0, 1.0, 0.44234316301850674], 
reward next is 0.5577, 
noisyNet noise sample is [array([1.4632796], dtype=float32), -0.5234036]. 
=============================================
[2019-04-04 04:54:32,587] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[82.383385]
 [82.18899 ]
 [82.281906]
 [82.86455 ]
 [83.77183 ]], R is [[82.14946747]
 [82.02654266]
 [81.93322754]
 [82.11389923]
 [82.29276276]].
[2019-04-04 04:54:34,641] A3C_AGENT_WORKER-Thread-10 INFO:Local step 300000, global step 4802329: loss 5.0348
[2019-04-04 04:54:34,649] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 300000, global step 4802331: learning rate 0.0001
[2019-04-04 04:54:36,748] A3C_AGENT_WORKER-Thread-20 INFO:Local step 300000, global step 4803196: loss 5.3911
[2019-04-04 04:54:36,748] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 300000, global step 4803196: learning rate 0.0001
[2019-04-04 04:54:37,484] A3C_AGENT_WORKER-Thread-18 INFO:Local step 300000, global step 4803474: loss 5.5861
[2019-04-04 04:54:37,484] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 300000, global step 4803474: learning rate 0.0001
[2019-04-04 04:54:39,379] A3C_AGENT_WORKER-Thread-17 INFO:Local step 300500, global step 4804145: loss 0.1774
[2019-04-04 04:54:39,381] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 300500, global step 4804146: learning rate 0.0001
[2019-04-04 04:54:40,152] A3C_AGENT_WORKER-Thread-19 INFO:Local step 300500, global step 4804464: loss 0.1534
[2019-04-04 04:54:40,153] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 300500, global step 4804464: learning rate 0.0001
[2019-04-04 04:54:41,940] A3C_AGENT_WORKER-Thread-7 INFO:Local step 300500, global step 4805159: loss 0.1509
[2019-04-04 04:54:41,941] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 300500, global step 4805159: learning rate 0.0001
[2019-04-04 04:54:42,502] A3C_AGENT_WORKER-Thread-16 INFO:Local step 300000, global step 4805311: loss 6.2293
[2019-04-04 04:54:42,504] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 300000, global step 4805312: learning rate 0.0001
[2019-04-04 04:54:44,127] A3C_AGENT_WORKER-Thread-14 INFO:Local step 300500, global step 4805772: loss 0.1701
[2019-04-04 04:54:44,130] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 300500, global step 4805772: learning rate 0.0001
[2019-04-04 04:54:45,138] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.4371957e-30 1.8412711e-27 1.2029597e-28 1.4894645e-26 1.8677437e-26
 1.1456225e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 04:54:45,142] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5623
[2019-04-04 04:54:45,151] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.983333333333333, 72.66666666666667, 0.0, 0.0, 26.0, 25.65166767937008, 0.591841409681305, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1626600.0000, 
sim time next is 1627200.0000, 
raw observation next is [7.7, 74.0, 0.0, 0.0, 26.0, 25.57808109625306, 0.5760091901266039, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.6759002770083103, 0.74, 0.0, 0.0, 0.6666666666666666, 0.6315067580210885, 0.6920030633755346, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4186803], dtype=float32), 0.5614271]. 
=============================================
[2019-04-04 04:54:47,011] A3C_AGENT_WORKER-Thread-5 INFO:Local step 300500, global step 4806696: loss 0.1701
[2019-04-04 04:54:47,013] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 300500, global step 4806696: learning rate 0.0001
[2019-04-04 04:54:50,372] A3C_AGENT_WORKER-Thread-3 INFO:Local step 300500, global step 4807697: loss 0.1506
[2019-04-04 04:54:50,372] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 300500, global step 4807697: learning rate 0.0001
[2019-04-04 04:54:51,543] A3C_AGENT_WORKER-Thread-2 INFO:Local step 300500, global step 4808001: loss 0.1429
[2019-04-04 04:54:51,567] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 300500, global step 4808003: learning rate 0.0001
[2019-04-04 04:54:52,995] A3C_AGENT_WORKER-Thread-15 INFO:Local step 300500, global step 4808404: loss 0.1548
[2019-04-04 04:54:52,995] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 300500, global step 4808404: learning rate 0.0001
[2019-04-04 04:54:53,564] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.0677970e-27 7.2407176e-24 1.7492041e-25 7.2877984e-24 8.1634411e-24
 1.4347556e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 04:54:53,579] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2540
[2019-04-04 04:54:53,622] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 86.0, 0.0, 0.0, 26.0, 25.06138405856493, 0.303930167683113, 0.0, 1.0, 45983.24765839949], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1804200.0000, 
sim time next is 1804800.0000, 
raw observation next is [-5.0, 86.0, 0.0, 0.0, 26.0, 25.02944356982434, 0.2949050996298186, 0.0, 1.0, 45965.71833335696], 
processed observation next is [0.0, 0.9130434782608695, 0.32409972299168976, 0.86, 0.0, 0.0, 0.6666666666666666, 0.5857869641520285, 0.5983016998766062, 0.0, 1.0, 0.21888437301598554], 
reward next is 0.7811, 
noisyNet noise sample is [array([-0.03517608], dtype=float32), -0.50290346]. 
=============================================
[2019-04-04 04:54:53,797] A3C_AGENT_WORKER-Thread-6 INFO:Local step 300500, global step 4808647: loss 0.1716
[2019-04-04 04:54:53,797] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 300500, global step 4808647: learning rate 0.0001
[2019-04-04 04:54:53,862] A3C_AGENT_WORKER-Thread-9 INFO:Local step 300500, global step 4808667: loss 0.1591
[2019-04-04 04:54:53,863] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 300500, global step 4808667: learning rate 0.0001
[2019-04-04 04:54:54,565] A3C_AGENT_WORKER-Thread-4 INFO:Local step 300500, global step 4808891: loss 0.1780
[2019-04-04 04:54:54,567] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 300500, global step 4808891: learning rate 0.0001
[2019-04-04 04:54:55,847] A3C_AGENT_WORKER-Thread-8 INFO:Local step 300500, global step 4809249: loss 0.1502
[2019-04-04 04:54:55,848] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 300500, global step 4809250: learning rate 0.0001
[2019-04-04 04:54:58,854] A3C_AGENT_WORKER-Thread-10 INFO:Local step 300500, global step 4810028: loss 0.1754
[2019-04-04 04:54:58,854] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 300500, global step 4810028: learning rate 0.0001
[2019-04-04 04:55:00,281] A3C_AGENT_WORKER-Thread-20 INFO:Local step 300500, global step 4810429: loss 0.1800
[2019-04-04 04:55:00,283] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 300500, global step 4810430: learning rate 0.0001
[2019-04-04 04:55:01,528] A3C_AGENT_WORKER-Thread-18 INFO:Local step 300500, global step 4810817: loss 0.1526
[2019-04-04 04:55:01,529] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 300500, global step 4810817: learning rate 0.0001
[2019-04-04 04:55:05,870] A3C_AGENT_WORKER-Thread-17 INFO:Local step 301000, global step 4812053: loss 0.0135
[2019-04-04 04:55:05,871] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 301000, global step 4812053: learning rate 0.0001
[2019-04-04 04:55:06,824] A3C_AGENT_WORKER-Thread-16 INFO:Local step 300500, global step 4812271: loss 0.1890
[2019-04-04 04:55:06,824] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 300500, global step 4812271: learning rate 0.0001
[2019-04-04 04:55:08,400] A3C_AGENT_WORKER-Thread-19 INFO:Local step 301000, global step 4812695: loss 0.0076
[2019-04-04 04:55:08,408] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 301000, global step 4812695: learning rate 0.0001
[2019-04-04 04:55:09,271] A3C_AGENT_WORKER-Thread-7 INFO:Local step 301000, global step 4812933: loss 0.0150
[2019-04-04 04:55:09,289] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 301000, global step 4812933: learning rate 0.0001
[2019-04-04 04:55:11,355] A3C_AGENT_WORKER-Thread-14 INFO:Local step 301000, global step 4813531: loss 0.0155
[2019-04-04 04:55:11,369] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 301000, global step 4813531: learning rate 0.0001
[2019-04-04 04:55:13,511] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [4.0744823e-29 4.5985266e-26 5.0638214e-27 2.2788784e-25 2.3096146e-25
 5.6813387e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:55:13,511] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.3891
[2019-04-04 04:55:13,528] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 90.16666666666667, 0.0, 0.0, 26.0, 24.75020727658442, 0.2614209294493259, 0.0, 1.0, 42669.79333486462], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2073000.0000, 
sim time next is 2073600.0000, 
raw observation next is [-4.5, 91.0, 0.0, 0.0, 26.0, 24.71242524942515, 0.2535448426576029, 0.0, 1.0, 42695.21773464028], 
processed observation next is [1.0, 0.0, 0.3379501385041552, 0.91, 0.0, 0.0, 0.6666666666666666, 0.559368770785429, 0.5845149475525343, 0.0, 1.0, 0.2033105606411442], 
reward next is 0.7967, 
noisyNet noise sample is [array([-1.788817], dtype=float32), -2.141571]. 
=============================================
[2019-04-04 04:55:14,766] A3C_AGENT_WORKER-Thread-5 INFO:Local step 301000, global step 4814478: loss 0.0261
[2019-04-04 04:55:14,801] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 301000, global step 4814478: learning rate 0.0001
[2019-04-04 04:55:17,849] A3C_AGENT_WORKER-Thread-3 INFO:Local step 301000, global step 4815287: loss 0.0107
[2019-04-04 04:55:17,849] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 301000, global step 4815287: learning rate 0.0001
[2019-04-04 04:55:19,630] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.8914016e-29 1.9112786e-25 2.9674792e-27 3.1889675e-25 1.4168782e-25
 1.2072534e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:55:19,630] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1736
[2019-04-04 04:55:19,648] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 90.16666666666667, 0.0, 0.0, 26.0, 24.66058545935368, 0.233861359736086, 0.0, 1.0, 42695.75720097087], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2077800.0000, 
sim time next is 2078400.0000, 
raw observation next is [-4.5, 89.33333333333334, 0.0, 0.0, 26.0, 24.67047890838175, 0.2253900623257923, 0.0, 1.0, 42696.83207828485], 
processed observation next is [1.0, 0.043478260869565216, 0.3379501385041552, 0.8933333333333334, 0.0, 0.0, 0.6666666666666666, 0.5558732423651458, 0.5751300207752641, 0.0, 1.0, 0.20331824799183262], 
reward next is 0.7967, 
noisyNet noise sample is [array([-1.2505165], dtype=float32), 1.5954157]. 
=============================================
[2019-04-04 04:55:20,062] A3C_AGENT_WORKER-Thread-15 INFO:Local step 301000, global step 4815963: loss 0.0092
[2019-04-04 04:55:20,069] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 301000, global step 4815963: learning rate 0.0001
[2019-04-04 04:55:20,280] A3C_AGENT_WORKER-Thread-2 INFO:Local step 301000, global step 4816024: loss 0.0092
[2019-04-04 04:55:20,282] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 301000, global step 4816024: learning rate 0.0001
[2019-04-04 04:55:20,819] A3C_AGENT_WORKER-Thread-6 INFO:Local step 301000, global step 4816175: loss 0.0124
[2019-04-04 04:55:20,822] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 301000, global step 4816175: learning rate 0.0001
[2019-04-04 04:55:21,626] A3C_AGENT_WORKER-Thread-9 INFO:Local step 301000, global step 4816418: loss 0.0026
[2019-04-04 04:55:21,627] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 301000, global step 4816418: learning rate 0.0001
[2019-04-04 04:55:22,359] A3C_AGENT_WORKER-Thread-4 INFO:Local step 301000, global step 4816649: loss 0.0048
[2019-04-04 04:55:22,360] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 301000, global step 4816649: learning rate 0.0001
[2019-04-04 04:55:22,405] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.7958725e-29 9.6775839e-25 7.4868123e-27 5.2194701e-25 2.7394203e-25
 4.6969536e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:55:22,405] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5754
[2019-04-04 04:55:22,426] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.0, 86.33333333333334, 0.0, 0.0, 26.0, 24.15635758961614, 0.09977588330870964, 0.0, 1.0, 43682.06076511289], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2258400.0000, 
sim time next is 2259000.0000, 
raw observation next is [-8.1, 86.5, 0.0, 0.0, 26.0, 24.1736016960221, 0.0845978233380756, 0.0, 1.0, 43643.5409863981], 
processed observation next is [1.0, 0.13043478260869565, 0.23822714681440446, 0.865, 0.0, 0.0, 0.6666666666666666, 0.5144668080018416, 0.5281992744460252, 0.0, 1.0, 0.20782638564951475], 
reward next is 0.7922, 
noisyNet noise sample is [array([-0.5791487], dtype=float32), -0.12927212]. 
=============================================
[2019-04-04 04:55:22,445] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[82.48491]
 [82.52073]
 [82.50427]
 [82.51084]
 [82.57307]], R is [[82.38693237]
 [82.35505676]
 [82.32323456]
 [82.29167175]
 [82.25946808]].
[2019-04-04 04:55:23,542] A3C_AGENT_WORKER-Thread-8 INFO:Local step 301000, global step 4816959: loss 0.0082
[2019-04-04 04:55:23,542] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 301000, global step 4816959: learning rate 0.0001
[2019-04-04 04:55:27,136] A3C_AGENT_WORKER-Thread-10 INFO:Local step 301000, global step 4817965: loss 0.0150
[2019-04-04 04:55:27,138] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 301000, global step 4817965: learning rate 0.0001
[2019-04-04 04:55:27,430] A3C_AGENT_WORKER-Thread-20 INFO:Local step 301000, global step 4818064: loss 0.0119
[2019-04-04 04:55:27,433] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 301000, global step 4818064: learning rate 0.0001
[2019-04-04 04:55:28,784] A3C_AGENT_WORKER-Thread-18 INFO:Local step 301000, global step 4818494: loss 0.0118
[2019-04-04 04:55:28,785] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 301000, global step 4818495: learning rate 0.0001
[2019-04-04 04:55:32,427] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [9.92189649e-27 1.17388258e-22 1.74800335e-25 8.50107987e-24
 1.11737335e-23 1.04247932e-25 1.00000000e+00], sum to 1.0000
[2019-04-04 04:55:32,429] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5547
[2019-04-04 04:55:32,449] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.3, 60.0, 0.0, 0.0, 26.0, 23.63409912901425, -0.05864661478808251, 0.0, 1.0, 44267.82734936083], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2433000.0000, 
sim time next is 2433600.0000, 
raw observation next is [-8.4, 61.0, 0.0, 0.0, 26.0, 23.58951630925006, -0.06799011341064214, 0.0, 1.0, 44324.86639148896], 
processed observation next is [0.0, 0.17391304347826086, 0.2299168975069252, 0.61, 0.0, 0.0, 0.6666666666666666, 0.4657930257708385, 0.4773366288631193, 0.0, 1.0, 0.21107079234042364], 
reward next is 0.7889, 
noisyNet noise sample is [array([-0.8873446], dtype=float32), -0.756439]. 
=============================================
[2019-04-04 04:55:33,590] A3C_AGENT_WORKER-Thread-17 INFO:Local step 301500, global step 4819888: loss 0.3779
[2019-04-04 04:55:33,591] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 301500, global step 4819888: learning rate 0.0001
[2019-04-04 04:55:34,508] A3C_AGENT_WORKER-Thread-16 INFO:Local step 301000, global step 4820218: loss 0.0076
[2019-04-04 04:55:34,510] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 301000, global step 4820218: learning rate 0.0001
[2019-04-04 04:55:35,168] A3C_AGENT_WORKER-Thread-19 INFO:Local step 301500, global step 4820464: loss 0.3463
[2019-04-04 04:55:35,169] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 301500, global step 4820464: learning rate 0.0001
[2019-04-04 04:55:36,850] A3C_AGENT_WORKER-Thread-7 INFO:Local step 301500, global step 4821011: loss 0.3322
[2019-04-04 04:55:36,854] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 301500, global step 4821011: learning rate 0.0001
[2019-04-04 04:55:38,138] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.6784296e-31 5.4278399e-27 2.1151041e-29 3.1308890e-27 7.7978891e-28
 1.3989207e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 04:55:38,138] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5257
[2019-04-04 04:55:38,194] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.133333333333335, 74.66666666666667, 165.1666666666667, 48.16666666666667, 26.0, 25.70348639179576, 0.3455441688420498, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2283600.0000, 
sim time next is 2284200.0000, 
raw observation next is [-5.85, 73.0, 178.0, 50.0, 26.0, 25.7407690345856, 0.3466965341245802, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.30055401662049863, 0.73, 0.5933333333333334, 0.055248618784530384, 0.6666666666666666, 0.6450640862154667, 0.61556551137486, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.37110692], dtype=float32), -0.63058734]. 
=============================================
[2019-04-04 04:55:38,593] A3C_AGENT_WORKER-Thread-14 INFO:Local step 301500, global step 4821579: loss 0.3087
[2019-04-04 04:55:38,594] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 301500, global step 4821579: learning rate 0.0001
[2019-04-04 04:55:41,839] A3C_AGENT_WORKER-Thread-5 INFO:Local step 301500, global step 4822649: loss 0.2727
[2019-04-04 04:55:41,840] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 301500, global step 4822649: learning rate 0.0001
[2019-04-04 04:55:44,110] A3C_AGENT_WORKER-Thread-3 INFO:Local step 301500, global step 4823480: loss 0.2427
[2019-04-04 04:55:44,111] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 301500, global step 4823480: learning rate 0.0001
[2019-04-04 04:55:46,269] A3C_AGENT_WORKER-Thread-15 INFO:Local step 301500, global step 4824124: loss 0.2804
[2019-04-04 04:55:46,275] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 301500, global step 4824124: learning rate 0.0001
[2019-04-04 04:55:47,062] A3C_AGENT_WORKER-Thread-2 INFO:Local step 301500, global step 4824376: loss 0.2381
[2019-04-04 04:55:47,063] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 301500, global step 4824376: learning rate 0.0001
[2019-04-04 04:55:47,215] A3C_AGENT_WORKER-Thread-6 INFO:Local step 301500, global step 4824424: loss 0.2408
[2019-04-04 04:55:47,218] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 301500, global step 4824426: learning rate 0.0001
[2019-04-04 04:55:48,406] A3C_AGENT_WORKER-Thread-9 INFO:Local step 301500, global step 4824865: loss 0.1905
[2019-04-04 04:55:48,411] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 301500, global step 4824867: learning rate 0.0001
[2019-04-04 04:55:48,639] A3C_AGENT_WORKER-Thread-4 INFO:Local step 301500, global step 4824957: loss 0.1893
[2019-04-04 04:55:48,639] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 301500, global step 4824957: learning rate 0.0001
[2019-04-04 04:55:49,791] A3C_AGENT_WORKER-Thread-8 INFO:Local step 301500, global step 4825444: loss 0.1796
[2019-04-04 04:55:49,792] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 301500, global step 4825444: learning rate 0.0001
[2019-04-04 04:55:52,783] A3C_AGENT_WORKER-Thread-10 INFO:Local step 301500, global step 4826487: loss 0.1670
[2019-04-04 04:55:52,783] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 301500, global step 4826487: learning rate 0.0001
[2019-04-04 04:55:53,075] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.0509014e-29 4.0628190e-25 1.4225587e-26 4.6895488e-25 6.2849406e-25
 3.5931832e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:55:53,075] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1213
[2019-04-04 04:55:53,113] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.666666666666667, 71.0, 0.0, 0.0, 26.0, 24.60527887881784, 0.2497021991480936, 0.0, 1.0, 44430.08555026496], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2679600.0000, 
sim time next is 2680200.0000, 
raw observation next is [-8.0, 70.5, 0.0, 0.0, 26.0, 24.61944682449102, 0.2402973981713166, 0.0, 1.0, 44418.53293036347], 
processed observation next is [1.0, 0.0, 0.24099722991689754, 0.705, 0.0, 0.0, 0.6666666666666666, 0.5516205687075851, 0.5800991327237722, 0.0, 1.0, 0.2115168234779213], 
reward next is 0.7885, 
noisyNet noise sample is [array([0.14426441], dtype=float32), 2.2575533]. 
=============================================
[2019-04-04 04:55:53,455] A3C_AGENT_WORKER-Thread-20 INFO:Local step 301500, global step 4826685: loss 0.1680
[2019-04-04 04:55:53,457] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 301500, global step 4826686: learning rate 0.0001
[2019-04-04 04:55:54,726] A3C_AGENT_WORKER-Thread-18 INFO:Local step 301500, global step 4827184: loss 0.1786
[2019-04-04 04:55:54,727] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 301500, global step 4827184: learning rate 0.0001
[2019-04-04 04:55:55,919] A3C_AGENT_WORKER-Thread-17 INFO:Local step 302000, global step 4827659: loss 0.1031
[2019-04-04 04:55:55,919] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 302000, global step 4827659: learning rate 0.0001
[2019-04-04 04:55:57,692] A3C_AGENT_WORKER-Thread-19 INFO:Local step 302000, global step 4828319: loss 0.0999
[2019-04-04 04:55:57,692] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 302000, global step 4828319: learning rate 0.0001
[2019-04-04 04:55:58,809] A3C_AGENT_WORKER-Thread-7 INFO:Local step 302000, global step 4828646: loss 0.0803
[2019-04-04 04:55:58,810] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 302000, global step 4828646: learning rate 0.0001
[2019-04-04 04:56:00,329] A3C_AGENT_WORKER-Thread-16 INFO:Local step 301500, global step 4829182: loss 0.2015
[2019-04-04 04:56:00,343] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 301500, global step 4829182: learning rate 0.0001
[2019-04-04 04:56:01,107] A3C_AGENT_WORKER-Thread-14 INFO:Local step 302000, global step 4829485: loss 0.0811
[2019-04-04 04:56:01,110] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 302000, global step 4829485: learning rate 0.0001
[2019-04-04 04:56:04,121] A3C_AGENT_WORKER-Thread-5 INFO:Local step 302000, global step 4830577: loss 0.1059
[2019-04-04 04:56:04,124] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 302000, global step 4830579: learning rate 0.0001
[2019-04-04 04:56:04,763] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [9.0757065e-29 9.1688455e-25 2.2976797e-26 3.1391440e-25 2.3016033e-24
 1.5513576e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 04:56:04,763] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7692
[2019-04-04 04:56:04,814] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 89.5, 0.0, 0.0, 26.0, 25.14131986254537, 0.2856191298078692, 0.0, 1.0, 52355.48634639425], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2863800.0000, 
sim time next is 2864400.0000, 
raw observation next is [1.0, 90.66666666666667, 0.0, 0.0, 26.0, 25.0682496735421, 0.2725585609763369, 0.0, 1.0, 54115.38734188274], 
processed observation next is [1.0, 0.13043478260869565, 0.4903047091412743, 0.9066666666666667, 0.0, 0.0, 0.6666666666666666, 0.5890208061285082, 0.590852853658779, 0.0, 1.0, 0.2576923206756321], 
reward next is 0.7423, 
noisyNet noise sample is [array([-0.87994075], dtype=float32), 0.2580876]. 
=============================================
[2019-04-04 04:56:06,290] A3C_AGENT_WORKER-Thread-3 INFO:Local step 302000, global step 4831257: loss 0.0986
[2019-04-04 04:56:06,297] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 302000, global step 4831257: learning rate 0.0001
[2019-04-04 04:56:08,750] A3C_AGENT_WORKER-Thread-15 INFO:Local step 302000, global step 4832093: loss 0.0909
[2019-04-04 04:56:08,751] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 302000, global step 4832093: learning rate 0.0001
[2019-04-04 04:56:09,453] A3C_AGENT_WORKER-Thread-6 INFO:Local step 302000, global step 4832370: loss 0.0811
[2019-04-04 04:56:09,479] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 302000, global step 4832379: learning rate 0.0001
[2019-04-04 04:56:09,931] A3C_AGENT_WORKER-Thread-2 INFO:Local step 302000, global step 4832563: loss 0.1036
[2019-04-04 04:56:09,932] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 302000, global step 4832563: learning rate 0.0001
[2019-04-04 04:56:10,540] A3C_AGENT_WORKER-Thread-8 INFO:Local step 302000, global step 4832772: loss 0.0661
[2019-04-04 04:56:10,543] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 302000, global step 4832772: learning rate 0.0001
[2019-04-04 04:56:10,651] A3C_AGENT_WORKER-Thread-4 INFO:Local step 302000, global step 4832814: loss 0.0679
[2019-04-04 04:56:10,654] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 302000, global step 4832814: learning rate 0.0001
[2019-04-04 04:56:10,941] A3C_AGENT_WORKER-Thread-9 INFO:Local step 302000, global step 4832919: loss 0.0750
[2019-04-04 04:56:10,941] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 302000, global step 4832919: learning rate 0.0001
[2019-04-04 04:56:14,310] A3C_AGENT_WORKER-Thread-10 INFO:Local step 302000, global step 4834052: loss 0.0828
[2019-04-04 04:56:14,311] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 302000, global step 4834052: learning rate 0.0001
[2019-04-04 04:56:14,770] A3C_AGENT_WORKER-Thread-20 INFO:Local step 302000, global step 4834237: loss 0.0723
[2019-04-04 04:56:14,771] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 302000, global step 4834237: learning rate 0.0001
[2019-04-04 04:56:16,239] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.2332905e-31 1.4765202e-27 3.4788199e-29 3.2700599e-27 4.8830182e-27
 1.0708725e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 04:56:16,255] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7959
[2019-04-04 04:56:16,309] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.666666666666667, 41.0, 169.1666666666667, 698.0, 26.0, 25.40126778805134, 0.3302020995898049, 1.0, 1.0, 23354.15445852631], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2809200.0000, 
sim time next is 2809800.0000, 
raw observation next is [3.0, 39.5, 178.0, 685.0, 26.0, 24.47340653295673, 0.2944727889762415, 1.0, 1.0, 196217.9094192963], 
processed observation next is [1.0, 0.5217391304347826, 0.5457063711911359, 0.395, 0.5933333333333334, 0.7569060773480663, 0.6666666666666666, 0.5394505444130608, 0.5981575963254139, 1.0, 1.0, 0.9343709972347444], 
reward next is 0.0656, 
noisyNet noise sample is [array([-0.37917963], dtype=float32), 0.5248229]. 
=============================================
[2019-04-04 04:56:17,301] A3C_AGENT_WORKER-Thread-18 INFO:Local step 302000, global step 4835203: loss 0.0674
[2019-04-04 04:56:17,319] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 302000, global step 4835203: learning rate 0.0001
[2019-04-04 04:56:19,271] A3C_AGENT_WORKER-Thread-17 INFO:Local step 302500, global step 4835872: loss 0.0728
[2019-04-04 04:56:19,275] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 302500, global step 4835872: learning rate 0.0001
[2019-04-04 04:56:20,668] A3C_AGENT_WORKER-Thread-19 INFO:Local step 302500, global step 4836375: loss 0.0759
[2019-04-04 04:56:20,668] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 302500, global step 4836375: learning rate 0.0001
[2019-04-04 04:56:20,970] A3C_AGENT_WORKER-Thread-7 INFO:Local step 302500, global step 4836490: loss 0.0816
[2019-04-04 04:56:20,971] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 302500, global step 4836490: learning rate 0.0001
[2019-04-04 04:56:22,333] A3C_AGENT_WORKER-Thread-16 INFO:Local step 302000, global step 4837023: loss 0.0817
[2019-04-04 04:56:22,333] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 302000, global step 4837023: learning rate 0.0001
[2019-04-04 04:56:23,667] A3C_AGENT_WORKER-Thread-14 INFO:Local step 302500, global step 4837551: loss 0.0950
[2019-04-04 04:56:23,670] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 302500, global step 4837553: learning rate 0.0001
[2019-04-04 04:56:27,200] A3C_AGENT_WORKER-Thread-5 INFO:Local step 302500, global step 4838843: loss 0.1645
[2019-04-04 04:56:27,204] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 302500, global step 4838846: learning rate 0.0001
[2019-04-04 04:56:29,189] A3C_AGENT_WORKER-Thread-3 INFO:Local step 302500, global step 4839624: loss 0.2319
[2019-04-04 04:56:29,199] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 302500, global step 4839626: learning rate 0.0001
[2019-04-04 04:56:30,728] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.49068934e-30 3.59002008e-27 8.49946949e-29 1.11000926e-26
 1.07329179e-26 1.44129140e-30 1.00000000e+00], sum to 1.0000
[2019-04-04 04:56:30,734] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0458
[2019-04-04 04:56:30,747] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.666666666666667, 75.0, 50.66666666666667, 443.1666666666667, 26.0, 26.27142790388259, 0.7275690891285969, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3256800.0000, 
sim time next is 3257400.0000, 
raw observation next is [-3.833333333333333, 76.0, 42.33333333333334, 375.3333333333334, 26.0, 26.46779450256879, 0.5676619844707281, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.3564173591874424, 0.76, 0.14111111111111113, 0.4147329650092082, 0.6666666666666666, 0.7056495418807325, 0.6892206614902427, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.841972], dtype=float32), -0.9930677]. 
=============================================
[2019-04-04 04:56:30,898] A3C_AGENT_WORKER-Thread-15 INFO:Local step 302500, global step 4840324: loss 0.2306
[2019-04-04 04:56:30,899] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 302500, global step 4840324: learning rate 0.0001
[2019-04-04 04:56:31,211] A3C_AGENT_WORKER-Thread-6 INFO:Local step 302500, global step 4840433: loss 0.2314
[2019-04-04 04:56:31,229] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 302500, global step 4840434: learning rate 0.0001
[2019-04-04 04:56:32,040] A3C_AGENT_WORKER-Thread-2 INFO:Local step 302500, global step 4840743: loss 0.2161
[2019-04-04 04:56:32,044] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 302500, global step 4840743: learning rate 0.0001
[2019-04-04 04:56:32,986] A3C_AGENT_WORKER-Thread-9 INFO:Local step 302500, global step 4841169: loss 0.2932
[2019-04-04 04:56:32,987] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 302500, global step 4841170: learning rate 0.0001
[2019-04-04 04:56:33,174] A3C_AGENT_WORKER-Thread-8 INFO:Local step 302500, global step 4841263: loss 0.3071
[2019-04-04 04:56:33,175] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 302500, global step 4841264: learning rate 0.0001
[2019-04-04 04:56:33,596] A3C_AGENT_WORKER-Thread-4 INFO:Local step 302500, global step 4841463: loss 0.2960
[2019-04-04 04:56:33,599] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 302500, global step 4841464: learning rate 0.0001
[2019-04-04 04:56:35,222] A3C_AGENT_WORKER-Thread-10 INFO:Local step 302500, global step 4842174: loss 0.3103
[2019-04-04 04:56:35,222] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 302500, global step 4842174: learning rate 0.0001
[2019-04-04 04:56:36,160] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.5853578e-29 1.6450378e-25 1.7145780e-27 2.0513476e-25 7.5614703e-26
 1.9403364e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:56:36,160] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3541
[2019-04-04 04:56:36,176] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.45429022948466, 0.4932090624715694, 0.0, 1.0, 71281.22641698869], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3361800.0000, 
sim time next is 3362400.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.41009979984726, 0.5008420114672724, 0.0, 1.0, 77110.78655913717], 
processed observation next is [1.0, 0.9565217391304348, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6175083166539382, 0.6669473371557575, 0.0, 1.0, 0.36719422171017696], 
reward next is 0.6328, 
noisyNet noise sample is [array([-0.0998695], dtype=float32), 0.08258191]. 
=============================================
[2019-04-04 04:56:36,183] A3C_AGENT_WORKER-Thread-20 INFO:Local step 302500, global step 4842649: loss 0.3501
[2019-04-04 04:56:36,186] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 302500, global step 4842650: learning rate 0.0001
[2019-04-04 04:56:37,282] A3C_AGENT_WORKER-Thread-17 INFO:Local step 303000, global step 4843142: loss 0.0018
[2019-04-04 04:56:37,283] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 303000, global step 4843142: learning rate 0.0001
[2019-04-04 04:56:37,438] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [3.1062097e-31 2.7811868e-28 3.2072117e-29 6.1865985e-27 1.0272914e-27
 1.1659245e-31 1.0000000e+00], sum to 1.0000
[2019-04-04 04:56:37,439] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.9610
[2019-04-04 04:56:37,449] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.0, 100.0, 112.3333333333333, 811.6666666666666, 26.0, 27.31598668936194, 0.856802131636443, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3157800.0000, 
sim time next is 3158400.0000, 
raw observation next is [7.0, 100.0, 112.1666666666667, 808.8333333333334, 26.0, 27.33096829769233, 0.869076725621039, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.6565096952908588, 1.0, 0.373888888888889, 0.8937384898710866, 0.6666666666666666, 0.7775806914743608, 0.7896922418736797, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9732434], dtype=float32), -0.8979303]. 
=============================================
[2019-04-04 04:56:38,319] A3C_AGENT_WORKER-Thread-7 INFO:Local step 303000, global step 4843687: loss 0.0013
[2019-04-04 04:56:38,321] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 303000, global step 4843688: learning rate 0.0001
[2019-04-04 04:56:38,349] A3C_AGENT_WORKER-Thread-19 INFO:Local step 303000, global step 4843700: loss 0.0012
[2019-04-04 04:56:38,350] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 303000, global step 4843700: learning rate 0.0001
[2019-04-04 04:56:38,561] A3C_AGENT_WORKER-Thread-18 INFO:Local step 302500, global step 4843802: loss 0.3722
[2019-04-04 04:56:38,562] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 302500, global step 4843802: learning rate 0.0001
[2019-04-04 04:56:39,985] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [4.7415432e-29 2.9853650e-25 1.0641556e-26 3.2057138e-25 7.2897409e-25
 1.9027136e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:56:39,985] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.6360
[2019-04-04 04:56:40,005] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.5, 100.0, 0.0, 0.0, 26.0, 25.3199666838722, 0.4942803288789746, 0.0, 1.0, 40578.94688967164], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3216600.0000, 
sim time next is 3217200.0000, 
raw observation next is [-2.666666666666667, 100.0, 0.0, 0.0, 26.0, 25.32463025737698, 0.4891005788017895, 0.0, 1.0, 40585.49241177534], 
processed observation next is [1.0, 0.21739130434782608, 0.38873499538319484, 1.0, 0.0, 0.0, 0.6666666666666666, 0.610385854781415, 0.6630335262672632, 0.0, 1.0, 0.19326424957988256], 
reward next is 0.8067, 
noisyNet noise sample is [array([-0.3404756], dtype=float32), 1.3657371]. 
=============================================
[2019-04-04 04:56:40,368] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.4724716e-29 9.8698874e-25 1.3194011e-26 4.4142216e-25 3.1239370e-25
 4.5837056e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:56:40,370] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7689
[2019-04-04 04:56:40,383] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.15, 77.0, 0.0, 0.0, 26.0, 24.79025885486099, 0.2962520335436986, 0.0, 1.0, 43930.81620955041], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3294600.0000, 
sim time next is 3295200.0000, 
raw observation next is [-8.3, 77.0, 0.0, 0.0, 26.0, 24.72091390111856, 0.2919248180820392, 0.0, 1.0, 43946.60593165513], 
processed observation next is [1.0, 0.13043478260869565, 0.23268698060941828, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5600761584265467, 0.5973082726940131, 0.0, 1.0, 0.20926955205550063], 
reward next is 0.7907, 
noisyNet noise sample is [array([0.6587511], dtype=float32), -0.26540068]. 
=============================================
[2019-04-04 04:56:41,134] A3C_AGENT_WORKER-Thread-14 INFO:Local step 303000, global step 4845024: loss 0.0052
[2019-04-04 04:56:41,138] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 303000, global step 4845026: learning rate 0.0001
[2019-04-04 04:56:41,631] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0043286e-28 1.4357917e-24 3.8079524e-26 8.3263519e-25 2.5136814e-24
 1.5589094e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 04:56:41,634] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0733
[2019-04-04 04:56:41,647] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.47113300828001, 0.5411577272128539, 0.0, 1.0, 64440.0238633198], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3208200.0000, 
sim time next is 3208800.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.50037212914064, 0.5487866369355154, 0.0, 1.0, 36641.8767486308], 
processed observation next is [1.0, 0.13043478260869565, 0.4349030470914128, 1.0, 0.0, 0.0, 0.6666666666666666, 0.62503101076172, 0.6829288789785052, 0.0, 1.0, 0.17448512737443236], 
reward next is 0.8255, 
noisyNet noise sample is [array([1.1254905], dtype=float32), 0.04686053]. 
=============================================
[2019-04-04 04:56:43,343] A3C_AGENT_WORKER-Thread-16 INFO:Local step 302500, global step 4846061: loss 0.2917
[2019-04-04 04:56:43,343] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 302500, global step 4846061: learning rate 0.0001
[2019-04-04 04:56:44,555] A3C_AGENT_WORKER-Thread-5 INFO:Local step 303000, global step 4846618: loss 0.0137
[2019-04-04 04:56:44,557] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 303000, global step 4846618: learning rate 0.0001
[2019-04-04 04:56:45,860] A3C_AGENT_WORKER-Thread-3 INFO:Local step 303000, global step 4847244: loss 0.0119
[2019-04-04 04:56:45,861] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 303000, global step 4847244: learning rate 0.0001
[2019-04-04 04:56:47,784] A3C_AGENT_WORKER-Thread-15 INFO:Local step 303000, global step 4848164: loss 0.0120
[2019-04-04 04:56:47,784] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 303000, global step 4848164: learning rate 0.0001
[2019-04-04 04:56:48,005] A3C_AGENT_WORKER-Thread-6 INFO:Local step 303000, global step 4848273: loss 0.0114
[2019-04-04 04:56:48,007] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 303000, global step 4848273: learning rate 0.0001
[2019-04-04 04:56:48,770] A3C_AGENT_WORKER-Thread-2 INFO:Local step 303000, global step 4848597: loss 0.0087
[2019-04-04 04:56:48,772] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 303000, global step 4848597: learning rate 0.0001
[2019-04-04 04:56:49,807] A3C_AGENT_WORKER-Thread-9 INFO:Local step 303000, global step 4849070: loss 0.0196
[2019-04-04 04:56:49,808] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 303000, global step 4849071: learning rate 0.0001
[2019-04-04 04:56:49,924] A3C_AGENT_WORKER-Thread-8 INFO:Local step 303000, global step 4849130: loss 0.0169
[2019-04-04 04:56:49,925] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 303000, global step 4849130: learning rate 0.0001
[2019-04-04 04:56:49,959] A3C_AGENT_WORKER-Thread-4 INFO:Local step 303000, global step 4849143: loss 0.0154
[2019-04-04 04:56:49,960] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 303000, global step 4849144: learning rate 0.0001
[2019-04-04 04:56:51,823] A3C_AGENT_WORKER-Thread-10 INFO:Local step 303000, global step 4849981: loss 0.0133
[2019-04-04 04:56:51,832] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 303000, global step 4849981: learning rate 0.0001
[2019-04-04 04:56:53,029] A3C_AGENT_WORKER-Thread-20 INFO:Local step 303000, global step 4850633: loss 0.0144
[2019-04-04 04:56:53,029] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 303000, global step 4850633: learning rate 0.0001
[2019-04-04 04:56:53,479] A3C_AGENT_WORKER-Thread-17 INFO:Local step 303500, global step 4850877: loss 0.1006
[2019-04-04 04:56:53,481] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 303500, global step 4850877: learning rate 0.0001
[2019-04-04 04:56:54,037] A3C_AGENT_WORKER-Thread-7 INFO:Local step 303500, global step 4851156: loss 0.0859
[2019-04-04 04:56:54,039] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 303500, global step 4851156: learning rate 0.0001
[2019-04-04 04:56:54,634] A3C_AGENT_WORKER-Thread-19 INFO:Local step 303500, global step 4851494: loss 0.1153
[2019-04-04 04:56:54,636] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 303500, global step 4851494: learning rate 0.0001
[2019-04-04 04:56:55,121] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [5.4670911e-28 5.0240362e-25 6.0180723e-27 1.5478716e-25 3.2320031e-25
 1.3418610e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 04:56:55,121] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8389
[2019-04-04 04:56:55,136] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 67.33333333333333, 0.0, 0.0, 26.0, 25.41443848365369, 0.4020189582364753, 0.0, 1.0, 45464.70628726168], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3710400.0000, 
sim time next is 3711000.0000, 
raw observation next is [-2.5, 66.16666666666667, 0.0, 0.0, 26.0, 25.41028353834636, 0.3969843997724484, 0.0, 1.0, 44218.4853087352], 
processed observation next is [0.0, 0.9565217391304348, 0.39335180055401664, 0.6616666666666667, 0.0, 0.0, 0.6666666666666666, 0.61752362819553, 0.6323281332574828, 0.0, 1.0, 0.2105642157558819], 
reward next is 0.7894, 
noisyNet noise sample is [array([0.05123566], dtype=float32), -0.15098545]. 
=============================================
[2019-04-04 04:56:55,152] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[81.695175]
 [82.02872 ]
 [82.41182 ]
 [82.78581 ]
 [83.047745]], R is [[81.34895325]
 [81.3189621 ]
 [81.32821655]
 [81.35418701]
 [81.35095215]].
[2019-04-04 04:56:55,226] A3C_AGENT_WORKER-Thread-18 INFO:Local step 303000, global step 4851823: loss 0.0041
[2019-04-04 04:56:55,229] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 303000, global step 4851824: learning rate 0.0001
[2019-04-04 04:56:57,229] A3C_AGENT_WORKER-Thread-14 INFO:Local step 303500, global step 4852786: loss 0.1045
[2019-04-04 04:56:57,232] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 303500, global step 4852788: learning rate 0.0001
[2019-04-04 04:56:59,757] A3C_AGENT_WORKER-Thread-16 INFO:Local step 303000, global step 4854163: loss 0.0097
[2019-04-04 04:56:59,758] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 303000, global step 4854163: learning rate 0.0001
[2019-04-04 04:57:00,376] A3C_AGENT_WORKER-Thread-5 INFO:Local step 303500, global step 4854466: loss 0.0717
[2019-04-04 04:57:00,377] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 303500, global step 4854466: learning rate 0.0001
[2019-04-04 04:57:01,654] A3C_AGENT_WORKER-Thread-3 INFO:Local step 303500, global step 4855149: loss 0.0808
[2019-04-04 04:57:01,655] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 303500, global step 4855149: learning rate 0.0001
[2019-04-04 04:57:03,120] A3C_AGENT_WORKER-Thread-15 INFO:Local step 303500, global step 4856000: loss 0.0798
[2019-04-04 04:57:03,120] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 303500, global step 4856000: learning rate 0.0001
[2019-04-04 04:57:03,445] A3C_AGENT_WORKER-Thread-6 INFO:Local step 303500, global step 4856212: loss 0.0750
[2019-04-04 04:57:03,446] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 303500, global step 4856212: learning rate 0.0001
[2019-04-04 04:57:04,025] A3C_AGENT_WORKER-Thread-2 INFO:Local step 303500, global step 4856547: loss 0.0751
[2019-04-04 04:57:04,027] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 303500, global step 4856548: learning rate 0.0001
[2019-04-04 04:57:04,853] A3C_AGENT_WORKER-Thread-8 INFO:Local step 303500, global step 4857030: loss 0.0652
[2019-04-04 04:57:04,856] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 303500, global step 4857030: learning rate 0.0001
[2019-04-04 04:57:05,182] A3C_AGENT_WORKER-Thread-9 INFO:Local step 303500, global step 4857213: loss 0.0710
[2019-04-04 04:57:05,183] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 303500, global step 4857213: learning rate 0.0001
[2019-04-04 04:57:05,308] A3C_AGENT_WORKER-Thread-4 INFO:Local step 303500, global step 4857286: loss 0.0724
[2019-04-04 04:57:05,313] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 303500, global step 4857286: learning rate 0.0001
[2019-04-04 04:57:07,457] A3C_AGENT_WORKER-Thread-10 INFO:Local step 303500, global step 4858229: loss 0.0687
[2019-04-04 04:57:07,457] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 303500, global step 4858229: learning rate 0.0001
[2019-04-04 04:57:09,735] A3C_AGENT_WORKER-Thread-20 INFO:Local step 303500, global step 4858967: loss 0.0587
[2019-04-04 04:57:09,736] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 303500, global step 4858967: learning rate 0.0001
[2019-04-04 04:57:10,749] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.7936907e-28 2.7535959e-25 3.7591526e-27 5.2793441e-25 2.5030303e-25
 2.1840946e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:57:10,749] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7950
[2019-04-04 04:57:10,767] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.5, 55.5, 0.0, 0.0, 26.0, 24.99906137874251, 0.3508805457001007, 0.0, 1.0, 44084.79141663356], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3972600.0000, 
sim time next is 3973200.0000, 
raw observation next is [-9.666666666666666, 56.33333333333333, 0.0, 0.0, 26.0, 24.95050733763468, 0.3393485337110038, 0.0, 1.0, 44098.35326190494], 
processed observation next is [1.0, 1.0, 0.19482917820867962, 0.5633333333333332, 0.0, 0.0, 0.6666666666666666, 0.57920894480289, 0.6131161779036679, 0.0, 1.0, 0.2099921583900235], 
reward next is 0.7900, 
noisyNet noise sample is [array([-0.00276769], dtype=float32), -0.49202093]. 
=============================================
[2019-04-04 04:57:10,844] A3C_AGENT_WORKER-Thread-17 INFO:Local step 304000, global step 4859320: loss 0.0933
[2019-04-04 04:57:10,849] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 304000, global step 4859321: learning rate 0.0001
[2019-04-04 04:57:11,498] A3C_AGENT_WORKER-Thread-7 INFO:Local step 304000, global step 4859542: loss 0.1137
[2019-04-04 04:57:11,499] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 304000, global step 4859542: learning rate 0.0001
[2019-04-04 04:57:11,784] A3C_AGENT_WORKER-Thread-19 INFO:Local step 304000, global step 4859620: loss 0.1294
[2019-04-04 04:57:11,786] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 304000, global step 4859620: learning rate 0.0001
[2019-04-04 04:57:12,809] A3C_AGENT_WORKER-Thread-18 INFO:Local step 303500, global step 4859928: loss 0.0604
[2019-04-04 04:57:12,809] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 303500, global step 4859928: learning rate 0.0001
[2019-04-04 04:57:15,695] A3C_AGENT_WORKER-Thread-14 INFO:Local step 304000, global step 4860828: loss 0.0760
[2019-04-04 04:57:15,696] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 304000, global step 4860829: learning rate 0.0001
[2019-04-04 04:57:21,161] A3C_AGENT_WORKER-Thread-16 INFO:Local step 303500, global step 4862509: loss 0.0556
[2019-04-04 04:57:21,186] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 303500, global step 4862509: learning rate 0.0001
[2019-04-04 04:57:21,444] A3C_AGENT_WORKER-Thread-5 INFO:Local step 304000, global step 4862591: loss 0.0247
[2019-04-04 04:57:21,445] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 304000, global step 4862591: learning rate 0.0001
[2019-04-04 04:57:21,697] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.15848636e-29 1.11724806e-26 7.73992148e-28 8.49076507e-26
 5.88025917e-26 1.64670441e-29 1.00000000e+00], sum to 1.0000
[2019-04-04 04:57:21,698] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6554
[2019-04-04 04:57:21,761] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 76.0, 0.0, 0.0, 26.0, 25.49697682943904, 0.5091684355768097, 0.0, 1.0, 71478.01482979127], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3793800.0000, 
sim time next is 3794400.0000, 
raw observation next is [-3.0, 77.0, 0.0, 0.0, 26.0, 25.45429081399476, 0.510920921717458, 0.0, 1.0, 76027.90069229504], 
processed observation next is [1.0, 0.9565217391304348, 0.3795013850415513, 0.77, 0.0, 0.0, 0.6666666666666666, 0.62119090116623, 0.6703069739058193, 0.0, 1.0, 0.36203762234426207], 
reward next is 0.6380, 
noisyNet noise sample is [array([0.4449423], dtype=float32), -0.22264387]. 
=============================================
[2019-04-04 04:57:24,123] A3C_AGENT_WORKER-Thread-3 INFO:Local step 304000, global step 4863339: loss 0.0145
[2019-04-04 04:57:24,170] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 304000, global step 4863339: learning rate 0.0001
[2019-04-04 04:57:25,550] A3C_AGENT_WORKER-Thread-15 INFO:Local step 304000, global step 4863724: loss 0.0336
[2019-04-04 04:57:25,555] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 304000, global step 4863724: learning rate 0.0001
[2019-04-04 04:57:26,544] A3C_AGENT_WORKER-Thread-6 INFO:Local step 304000, global step 4863985: loss 0.0468
[2019-04-04 04:57:26,549] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 304000, global step 4863985: learning rate 0.0001
[2019-04-04 04:57:28,351] A3C_AGENT_WORKER-Thread-2 INFO:Local step 304000, global step 4864428: loss 0.0124
[2019-04-04 04:57:28,352] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 304000, global step 4864428: learning rate 0.0001
[2019-04-04 04:57:29,652] A3C_AGENT_WORKER-Thread-4 INFO:Local step 304000, global step 4864777: loss 0.0403
[2019-04-04 04:57:29,653] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 304000, global step 4864777: learning rate 0.0001
[2019-04-04 04:57:30,322] A3C_AGENT_WORKER-Thread-9 INFO:Local step 304000, global step 4864939: loss 0.0139
[2019-04-04 04:57:30,324] A3C_AGENT_WORKER-Thread-8 INFO:Local step 304000, global step 4864940: loss 0.0264
[2019-04-04 04:57:30,324] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 304000, global step 4864940: learning rate 0.0001
[2019-04-04 04:57:30,348] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 304000, global step 4864940: learning rate 0.0001
[2019-04-04 04:57:32,624] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.7593197e-28 3.9020381e-24 1.9004812e-26 3.5875594e-25 9.5509451e-25
 4.5727899e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:57:32,624] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4048
[2019-04-04 04:57:32,654] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.86919532449953, 0.2823700551186903, 0.0, 1.0, 43978.25098933714], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3818400.0000, 
sim time next is 3819000.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.82938412158337, 0.2788454864524402, 0.0, 1.0, 43989.55834713976], 
processed observation next is [1.0, 0.17391304347826086, 0.3518005540166205, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5691153434652808, 0.5929484954841467, 0.0, 1.0, 0.2094740873673322], 
reward next is 0.7905, 
noisyNet noise sample is [array([-0.4523352], dtype=float32), -0.933583]. 
=============================================
[2019-04-04 04:57:32,685] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[83.22005 ]
 [83.19192 ]
 [83.17719 ]
 [83.151474]
 [83.12646 ]], R is [[83.22813416]
 [83.18643188]
 [83.14533997]
 [83.10492706]
 [83.06521606]].
[2019-04-04 04:57:34,030] A3C_AGENT_WORKER-Thread-10 INFO:Local step 304000, global step 4866032: loss 0.0134
[2019-04-04 04:57:34,059] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 304000, global step 4866032: learning rate 0.0001
[2019-04-04 04:57:36,759] A3C_AGENT_WORKER-Thread-20 INFO:Local step 304000, global step 4866834: loss 0.0064
[2019-04-04 04:57:36,761] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 304000, global step 4866835: learning rate 0.0001
[2019-04-04 04:57:39,108] A3C_AGENT_WORKER-Thread-17 INFO:Local step 304500, global step 4867534: loss 0.5285
[2019-04-04 04:57:39,109] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 304500, global step 4867534: learning rate 0.0001
[2019-04-04 04:57:39,117] A3C_AGENT_WORKER-Thread-7 INFO:Local step 304500, global step 4867539: loss 0.5197
[2019-04-04 04:57:39,119] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 304500, global step 4867540: learning rate 0.0001
[2019-04-04 04:57:39,432] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.9452807e-28 1.1111537e-25 3.1454219e-27 1.5407786e-25 2.7681087e-25
 2.3368627e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:57:39,432] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1937
[2019-04-04 04:57:39,467] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.9, 58.0, 116.0, 655.0, 26.0, 25.47654040037177, 0.4641081440772945, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4289400.0000, 
sim time next is 4290000.0000, 
raw observation next is [6.933333333333334, 57.33333333333333, 108.3333333333333, 638.5, 26.0, 25.48446041010245, 0.4655822268135581, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.6546629732225301, 0.5733333333333333, 0.361111111111111, 0.7055248618784531, 0.6666666666666666, 0.6237050341752042, 0.6551940756045194, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.06963417], dtype=float32), 0.1996249]. 
=============================================
[2019-04-04 04:57:39,500] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[83.89171 ]
 [83.87947 ]
 [83.93121 ]
 [84.083626]
 [84.33643 ]], R is [[83.95036316]
 [84.11086273]
 [84.2697525 ]
 [84.42705536]
 [84.58278656]].
[2019-04-04 04:57:39,507] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [7.4843530e-28 1.3566813e-24 1.1027963e-26 3.7594577e-25 1.2820765e-24
 4.8285577e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:57:39,509] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.6760
[2019-04-04 04:57:39,539] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.333333333333334, 36.33333333333333, 0.0, 0.0, 26.0, 24.84516892751611, 0.2286926485071905, 0.0, 1.0, 40387.37840132401], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4077600.0000, 
sim time next is 4078200.0000, 
raw observation next is [-4.166666666666667, 35.16666666666667, 0.0, 0.0, 26.0, 24.78929686555816, 0.2317953975124562, 0.0, 1.0, 40362.70866115568], 
processed observation next is [1.0, 0.17391304347826086, 0.3471837488457987, 0.35166666666666674, 0.0, 0.0, 0.6666666666666666, 0.5657747387965134, 0.5772651325041521, 0.0, 1.0, 0.19220337457693182], 
reward next is 0.8078, 
noisyNet noise sample is [array([1.4600725], dtype=float32), -0.62108463]. 
=============================================
[2019-04-04 04:57:39,545] A3C_AGENT_WORKER-Thread-18 INFO:Local step 304000, global step 4867676: loss 0.0008
[2019-04-04 04:57:39,546] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 304000, global step 4867676: learning rate 0.0001
[2019-04-04 04:57:39,799] A3C_AGENT_WORKER-Thread-19 INFO:Local step 304500, global step 4867766: loss 0.5334
[2019-04-04 04:57:39,803] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 304500, global step 4867766: learning rate 0.0001
[2019-04-04 04:57:39,978] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0542622e-30 2.3630665e-27 7.2511473e-29 1.0590610e-26 8.0923956e-27
 8.3967226e-31 1.0000000e+00], sum to 1.0000
[2019-04-04 04:57:39,981] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4035
[2019-04-04 04:57:40,011] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6666666666666667, 31.33333333333334, 118.6666666666667, 800.3333333333334, 26.0, 26.7121346590459, 0.6381722488613221, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4101000.0000, 
sim time next is 4101600.0000, 
raw observation next is [-0.3333333333333334, 30.66666666666667, 119.8333333333333, 808.1666666666666, 26.0, 26.73894804780739, 0.646005992757353, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4533702677747, 0.3066666666666667, 0.3994444444444443, 0.8930018416206261, 0.6666666666666666, 0.7282456706506159, 0.7153353309191176, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2768984], dtype=float32), -0.72470593]. 
=============================================
[2019-04-04 04:57:43,134] A3C_AGENT_WORKER-Thread-14 INFO:Local step 304500, global step 4868806: loss 0.4902
[2019-04-04 04:57:43,135] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 304500, global step 4868806: learning rate 0.0001
[2019-04-04 04:57:43,963] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.8522306e-29 1.1929172e-25 2.9519774e-27 7.2395226e-26 3.0304003e-25
 4.7520820e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:57:43,967] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0803
[2019-04-04 04:57:43,977] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 37.0, 0.0, 0.0, 26.0, 25.1496286089317, 0.3628116850914495, 0.0, 1.0, 40721.00678499127], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4060200.0000, 
sim time next is 4060800.0000, 
raw observation next is [-6.0, 37.0, 0.0, 0.0, 26.0, 25.12617167407436, 0.3560757010546274, 0.0, 1.0, 40693.6913267208], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.37, 0.0, 0.0, 0.6666666666666666, 0.5938476395061967, 0.6186919003515424, 0.0, 1.0, 0.1937794825081943], 
reward next is 0.8062, 
noisyNet noise sample is [array([-1.4589311], dtype=float32), -2.263272]. 
=============================================
[2019-04-04 04:57:47,173] A3C_AGENT_WORKER-Thread-16 INFO:Local step 304000, global step 4870138: loss 0.0019
[2019-04-04 04:57:47,176] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 304000, global step 4870138: learning rate 0.0001
[2019-04-04 04:57:48,514] A3C_AGENT_WORKER-Thread-5 INFO:Local step 304500, global step 4870587: loss 0.5330
[2019-04-04 04:57:48,516] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 304500, global step 4870587: learning rate 0.0001
[2019-04-04 04:57:50,088] A3C_AGENT_WORKER-Thread-3 INFO:Local step 304500, global step 4871091: loss 0.4587
[2019-04-04 04:57:50,094] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 304500, global step 4871091: learning rate 0.0001
[2019-04-04 04:57:51,480] A3C_AGENT_WORKER-Thread-15 INFO:Local step 304500, global step 4871631: loss 0.4555
[2019-04-04 04:57:51,481] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 304500, global step 4871631: learning rate 0.0001
[2019-04-04 04:57:53,553] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.3530904e-29 6.1586075e-25 1.1106361e-26 1.3545035e-25 1.1033233e-24
 1.3350702e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 04:57:53,554] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0487
[2019-04-04 04:57:53,565] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.4, 70.33333333333333, 0.0, 0.0, 26.0, 25.48835197151182, 0.373227685589935, 0.0, 1.0, 18753.66731281949], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4340400.0000, 
sim time next is 4341000.0000, 
raw observation next is [3.35, 70.66666666666667, 0.0, 0.0, 26.0, 25.44948725428051, 0.3794028449939719, 0.0, 1.0, 42448.54698686583], 
processed observation next is [1.0, 0.21739130434782608, 0.5554016620498616, 0.7066666666666667, 0.0, 0.0, 0.6666666666666666, 0.6207906045233758, 0.6264676149979906, 0.0, 1.0, 0.20213593803269445], 
reward next is 0.7979, 
noisyNet noise sample is [array([2.5513525], dtype=float32), -1.3778217]. 
=============================================
[2019-04-04 04:57:53,570] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[83.223206]
 [83.2877  ]
 [83.32177 ]
 [83.25122 ]
 [83.19334 ]], R is [[83.2569809 ]
 [83.33511353]
 [83.41244507]
 [83.38781738]
 [83.23907471]].
[2019-04-04 04:57:53,902] A3C_AGENT_WORKER-Thread-6 INFO:Local step 304500, global step 4872545: loss 0.4330
[2019-04-04 04:57:53,903] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 304500, global step 4872545: learning rate 0.0001
[2019-04-04 04:57:54,355] A3C_AGENT_WORKER-Thread-2 INFO:Local step 304500, global step 4872715: loss 0.4288
[2019-04-04 04:57:54,357] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 304500, global step 4872715: learning rate 0.0001
[2019-04-04 04:57:55,567] A3C_AGENT_WORKER-Thread-9 INFO:Local step 304500, global step 4873196: loss 0.3603
[2019-04-04 04:57:55,570] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 304500, global step 4873198: learning rate 0.0001
[2019-04-04 04:57:55,630] A3C_AGENT_WORKER-Thread-4 INFO:Local step 304500, global step 4873219: loss 0.3426
[2019-04-04 04:57:55,631] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 304500, global step 4873219: learning rate 0.0001
[2019-04-04 04:57:56,346] A3C_AGENT_WORKER-Thread-8 INFO:Local step 304500, global step 4873476: loss 0.3499
[2019-04-04 04:57:56,372] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 304500, global step 4873480: learning rate 0.0001
[2019-04-04 04:57:57,698] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.9957034e-29 1.1059945e-25 2.5526876e-27 2.0439427e-25 1.2708764e-25
 9.4464625e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:57:57,712] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1869
[2019-04-04 04:57:57,737] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.4, 75.0, 0.0, 0.0, 26.0, 25.55480292959541, 0.4214073607138883, 0.0, 1.0, 18741.38038797822], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4316400.0000, 
sim time next is 4317000.0000, 
raw observation next is [4.416666666666667, 75.16666666666667, 0.0, 0.0, 26.0, 25.59554401059147, 0.4193999030019886, 0.0, 1.0, 18738.67630892201], 
processed observation next is [0.0, 1.0, 0.584949215143121, 0.7516666666666667, 0.0, 0.0, 0.6666666666666666, 0.6329620008826226, 0.6397999676673295, 0.0, 1.0, 0.08923179194724767], 
reward next is 0.9108, 
noisyNet noise sample is [array([-0.05084926], dtype=float32), 0.3030856]. 
=============================================
[2019-04-04 04:57:57,758] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[83.68963 ]
 [83.70792 ]
 [83.710045]
 [83.66692 ]
 [83.648544]], R is [[83.73982239]
 [83.81318665]
 [83.88579559]
 [83.91348267]
 [83.93753815]].
[2019-04-04 04:57:58,569] A3C_AGENT_WORKER-Thread-10 INFO:Local step 304500, global step 4874319: loss 0.3751
[2019-04-04 04:57:58,572] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 304500, global step 4874320: learning rate 0.0001
[2019-04-04 04:57:59,393] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.7969017e-29 8.4832168e-27 3.5599749e-28 4.2982347e-26 1.7314973e-26
 3.5220955e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 04:57:59,403] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0908
[2019-04-04 04:57:59,446] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 92.0, 149.0, 3.0, 26.0, 25.11843319524032, 0.5019339673561586, 1.0, 1.0, 35574.85346711096], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4453200.0000, 
sim time next is 4453800.0000, 
raw observation next is [0.0, 92.0, 164.6666666666667, 4.0, 26.0, 25.44752109312567, 0.5431178613526367, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.46260387811634357, 0.92, 0.5488888888888891, 0.004419889502762431, 0.6666666666666666, 0.6206267577604724, 0.6810392871175456, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.24810982], dtype=float32), -0.804762]. 
=============================================
[2019-04-04 04:58:00,301] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0717681e-29 2.8932266e-26 3.0722167e-28 2.0778008e-26 1.8462305e-26
 1.6724321e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:58:00,302] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5378
[2019-04-04 04:58:00,312] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.5, 59.0, 0.0, 0.0, 26.0, 25.72907871715509, 0.5621206292246536, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4570200.0000, 
sim time next is 4570800.0000, 
raw observation next is [1.333333333333333, 59.66666666666667, 0.0, 0.0, 26.0, 25.73867101464631, 0.556277167534783, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.4995383194829178, 0.5966666666666667, 0.0, 0.0, 0.6666666666666666, 0.6448892512205259, 0.6854257225115944, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.53527427], dtype=float32), -0.57109606]. 
=============================================
[2019-04-04 04:58:00,906] A3C_AGENT_WORKER-Thread-20 INFO:Local step 304500, global step 4875255: loss 0.3642
[2019-04-04 04:58:00,909] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 304500, global step 4875255: learning rate 0.0001
[2019-04-04 04:58:01,059] A3C_AGENT_WORKER-Thread-19 INFO:Local step 305000, global step 4875318: loss 0.0664
[2019-04-04 04:58:01,099] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 305000, global step 4875318: learning rate 0.0001
[2019-04-04 04:58:01,226] A3C_AGENT_WORKER-Thread-7 INFO:Local step 305000, global step 4875382: loss 0.0516
[2019-04-04 04:58:01,228] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 305000, global step 4875382: learning rate 0.0001
[2019-04-04 04:58:01,719] A3C_AGENT_WORKER-Thread-17 INFO:Local step 305000, global step 4875605: loss 0.0534
[2019-04-04 04:58:01,722] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 305000, global step 4875605: learning rate 0.0001
[2019-04-04 04:58:03,559] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.3638936e-29 1.6213388e-25 1.8822913e-27 1.4361909e-25 1.2389102e-25
 3.3966522e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:58:03,560] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9241
[2019-04-04 04:58:03,586] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.333333333333334, 65.66666666666667, 0.0, 0.0, 26.0, 25.78419513179138, 0.5807682704455714, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4412400.0000, 
sim time next is 4413000.0000, 
raw observation next is [6.216666666666666, 65.83333333333333, 0.0, 0.0, 26.0, 25.76098289987601, 0.5736131349239758, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.6348107109879964, 0.6583333333333333, 0.0, 0.0, 0.6666666666666666, 0.6467485749896674, 0.691204378307992, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4006991], dtype=float32), -0.19927943]. 
=============================================
[2019-04-04 04:58:03,635] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[83.86768 ]
 [83.978294]
 [84.05951 ]
 [84.24635 ]
 [84.40642 ]], R is [[83.95559692]
 [84.11604309]
 [84.27488708]
 [84.43213654]
 [84.58781433]].
[2019-04-04 04:58:03,813] A3C_AGENT_WORKER-Thread-14 INFO:Local step 305000, global step 4876355: loss 0.1657
[2019-04-04 04:58:03,815] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 305000, global step 4876355: learning rate 0.0001
[2019-04-04 04:58:03,949] A3C_AGENT_WORKER-Thread-18 INFO:Local step 304500, global step 4876407: loss 0.3820
[2019-04-04 04:58:03,952] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 304500, global step 4876408: learning rate 0.0001
[2019-04-04 04:58:04,845] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.3955236e-32 1.2043377e-27 2.2519289e-29 2.5281696e-27 4.5578302e-28
 1.1700950e-31 1.0000000e+00], sum to 1.0000
[2019-04-04 04:58:04,845] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6601
[2019-04-04 04:58:04,878] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 92.0, 180.3333333333333, 5.0, 26.0, 25.6379703121101, 0.5825591993241876, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4454400.0000, 
sim time next is 4455000.0000, 
raw observation next is [0.0, 92.0, 196.0, 6.0, 26.0, 25.93074202448354, 0.6042725059028456, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.46260387811634357, 0.92, 0.6533333333333333, 0.0066298342541436465, 0.6666666666666666, 0.6608951687069616, 0.7014241686342819, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.93623614], dtype=float32), -0.9713047]. 
=============================================
[2019-04-04 04:58:04,894] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[86.95676 ]
 [86.965515]
 [87.033646]
 [86.59695 ]
 [85.82593 ]], R is [[87.21154022]
 [87.33942413]
 [87.46603394]
 [87.42197418]
 [86.61300659]].
[2019-04-04 04:58:06,809] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.6538597e-32 1.6813877e-29 1.2248798e-30 1.8048647e-28 1.2789427e-28
 5.5855708e-33 1.0000000e+00], sum to 1.0000
[2019-04-04 04:58:06,809] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1008
[2019-04-04 04:58:06,820] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.56666666666667, 29.66666666666667, 115.5, 843.8333333333334, 26.0, 27.49134794789448, 0.9959441493611122, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4368000.0000, 
sim time next is 4368600.0000, 
raw observation next is [14.55, 30.0, 115.0, 842.0, 26.0, 27.89835232581437, 1.048946488107494, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.865650969529086, 0.3, 0.38333333333333336, 0.9303867403314917, 0.6666666666666666, 0.824862693817864, 0.8496488293691646, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.921314], dtype=float32), -1.6070645]. 
=============================================
[2019-04-04 04:58:09,236] A3C_AGENT_WORKER-Thread-5 INFO:Local step 305000, global step 4878204: loss 0.0998
[2019-04-04 04:58:09,239] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 305000, global step 4878207: learning rate 0.0001
[2019-04-04 04:58:10,798] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.5214996e-31 4.5135563e-27 4.0819498e-29 4.6363416e-27 2.2029228e-27
 6.7711926e-31 1.0000000e+00], sum to 1.0000
[2019-04-04 04:58:10,798] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4787
[2019-04-04 04:58:10,843] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.1666666666666667, 91.0, 211.3333333333333, 6.0, 26.0, 26.43042760141803, 0.5831849760562515, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4705800.0000, 
sim time next is 4706400.0000, 
raw observation next is [0.3333333333333333, 90.0, 212.1666666666667, 6.0, 26.0, 26.37139260456308, 0.5836467123036709, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4718374884579871, 0.9, 0.7072222222222224, 0.0066298342541436465, 0.6666666666666666, 0.6976160503802568, 0.6945489041012237, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2811326], dtype=float32), -0.21259888]. 
=============================================
[2019-04-04 04:58:11,179] A3C_AGENT_WORKER-Thread-16 INFO:Local step 304500, global step 4878738: loss 0.3079
[2019-04-04 04:58:11,179] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 304500, global step 4878738: learning rate 0.0001
[2019-04-04 04:58:12,019] A3C_AGENT_WORKER-Thread-3 INFO:Local step 305000, global step 4879022: loss 0.1046
[2019-04-04 04:58:12,023] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 305000, global step 4879023: learning rate 0.0001
[2019-04-04 04:58:13,769] A3C_AGENT_WORKER-Thread-15 INFO:Local step 305000, global step 4879592: loss 0.1033
[2019-04-04 04:58:13,769] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 305000, global step 4879592: learning rate 0.0001
[2019-04-04 04:58:15,531] A3C_AGENT_WORKER-Thread-2 INFO:Local step 305000, global step 4880190: loss 0.0624
[2019-04-04 04:58:15,532] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 305000, global step 4880190: learning rate 0.0001
[2019-04-04 04:58:16,058] A3C_AGENT_WORKER-Thread-6 INFO:Local step 305000, global step 4880377: loss 0.0770
[2019-04-04 04:58:16,059] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 305000, global step 4880377: learning rate 0.0001
[2019-04-04 04:58:16,357] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.4412135e-28 5.0244962e-25 6.8842420e-27 1.8453583e-25 3.0530963e-25
 2.7725536e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:58:16,360] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0233
[2019-04-04 04:58:16,377] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.833333333333333, 70.33333333333333, 0.0, 0.0, 26.0, 25.29566456900705, 0.3971516571523019, 0.0, 1.0, 37655.96869173537], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4596000.0000, 
sim time next is 4596600.0000, 
raw observation next is [-1.916666666666667, 70.66666666666667, 0.0, 0.0, 26.0, 25.28316720921138, 0.3959487888137015, 0.0, 1.0, 36614.00606938427], 
processed observation next is [1.0, 0.17391304347826086, 0.4095106186518929, 0.7066666666666667, 0.0, 0.0, 0.6666666666666666, 0.6069306007676151, 0.6319829296045671, 0.0, 1.0, 0.1743524098542108], 
reward next is 0.8256, 
noisyNet noise sample is [array([0.44563746], dtype=float32), 1.715069]. 
=============================================
[2019-04-04 04:58:16,761] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [7.9866182e-28 3.5317393e-24 3.2045262e-26 5.9335978e-25 1.0030595e-24
 9.2141727e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 04:58:16,762] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7055
[2019-04-04 04:58:16,810] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.31275900916256, 0.188153224704694, 0.0, 1.0, 41334.46556223487], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4771800.0000, 
sim time next is 4772400.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.27536292012545, 0.177194515650945, 0.0, 1.0, 41386.37760140264], 
processed observation next is [0.0, 0.21739130434782608, 0.296398891966759, 0.92, 0.0, 0.0, 0.6666666666666666, 0.5229469100104541, 0.559064838550315, 0.0, 1.0, 0.1970779885781078], 
reward next is 0.8029, 
noisyNet noise sample is [array([0.31195322], dtype=float32), -0.4812141]. 
=============================================
[2019-04-04 04:58:17,897] A3C_AGENT_WORKER-Thread-4 INFO:Local step 305000, global step 4881001: loss 0.0657
[2019-04-04 04:58:17,898] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 305000, global step 4881001: learning rate 0.0001
[2019-04-04 04:58:17,989] A3C_AGENT_WORKER-Thread-9 INFO:Local step 305000, global step 4881037: loss 0.0743
[2019-04-04 04:58:17,990] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 305000, global step 4881037: learning rate 0.0001
[2019-04-04 04:58:18,714] A3C_AGENT_WORKER-Thread-8 INFO:Local step 305000, global step 4881256: loss 0.1035
[2019-04-04 04:58:18,716] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 305000, global step 4881256: learning rate 0.0001
[2019-04-04 04:58:21,920] A3C_AGENT_WORKER-Thread-10 INFO:Local step 305000, global step 4882325: loss 0.0897
[2019-04-04 04:58:21,923] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 305000, global step 4882326: learning rate 0.0001
[2019-04-04 04:58:23,553] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.3270097e-29 6.8114466e-25 1.2759763e-26 1.1390758e-25 3.0513963e-25
 2.9458565e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:58:23,555] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6516
[2019-04-04 04:58:23,592] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 77.0, 0.0, 0.0, 26.0, 25.03445980696389, 0.3307139802111642, 0.0, 1.0, 36228.2776025174], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4604400.0000, 
sim time next is 4605000.0000, 
raw observation next is [-2.833333333333333, 76.0, 0.0, 0.0, 26.0, 24.98967209999191, 0.3218204432219289, 0.0, 1.0, 36279.69836040556], 
processed observation next is [1.0, 0.30434782608695654, 0.3841181902123731, 0.76, 0.0, 0.0, 0.6666666666666666, 0.5824726749993259, 0.6072734810739763, 0.0, 1.0, 0.17276046838288361], 
reward next is 0.8272, 
noisyNet noise sample is [array([0.00079701], dtype=float32), -0.7906642]. 
=============================================
[2019-04-04 04:58:23,615] A3C_AGENT_WORKER-Thread-20 INFO:Local step 305000, global step 4882907: loss 0.0658
[2019-04-04 04:58:23,616] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 305000, global step 4882907: learning rate 0.0001
[2019-04-04 04:58:23,649] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[81.753456]
 [81.739136]
 [81.725464]
 [81.72455 ]
 [81.74326 ]], R is [[81.81778717]
 [81.82709503]
 [81.83630371]
 [81.8453064 ]
 [81.85420227]].
[2019-04-04 04:58:25,413] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.1464122e-29 3.9445547e-25 4.9733096e-27 1.6228734e-25 1.6771633e-25
 1.4859218e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 04:58:25,413] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0645
[2019-04-04 04:58:25,444] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 77.0, 0.0, 0.0, 26.0, 25.03445980696389, 0.3307139802111642, 0.0, 1.0, 36228.2776025174], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4604400.0000, 
sim time next is 4605000.0000, 
raw observation next is [-2.833333333333333, 76.0, 0.0, 0.0, 26.0, 24.98967209999191, 0.3218204432219289, 0.0, 1.0, 36279.69836040556], 
processed observation next is [1.0, 0.30434782608695654, 0.3841181902123731, 0.76, 0.0, 0.0, 0.6666666666666666, 0.5824726749993259, 0.6072734810739763, 0.0, 1.0, 0.17276046838288361], 
reward next is 0.8272, 
noisyNet noise sample is [array([-0.6555141], dtype=float32), -0.24357586]. 
=============================================
[2019-04-04 04:58:25,453] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[83.49342]
 [83.47864]
 [83.46675]
 [83.46851]
 [83.48793]], R is [[83.54302979]
 [83.53508759]
 [83.52721405]
 [83.51930237]
 [83.51145935]].
[2019-04-04 04:58:26,682] A3C_AGENT_WORKER-Thread-19 INFO:Local step 305500, global step 4883791: loss 0.0204
[2019-04-04 04:58:26,683] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 305500, global step 4883791: learning rate 0.0001
[2019-04-04 04:58:26,884] A3C_AGENT_WORKER-Thread-18 INFO:Local step 305000, global step 4883860: loss 0.1092
[2019-04-04 04:58:26,888] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 305000, global step 4883860: learning rate 0.0001
[2019-04-04 04:58:26,896] A3C_AGENT_WORKER-Thread-7 INFO:Local step 305500, global step 4883865: loss 0.0142
[2019-04-04 04:58:26,898] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 305500, global step 4883865: learning rate 0.0001
[2019-04-04 04:58:27,891] A3C_AGENT_WORKER-Thread-17 INFO:Local step 305500, global step 4884156: loss 0.0129
[2019-04-04 04:58:27,894] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 305500, global step 4884156: learning rate 0.0001
[2019-04-04 04:58:29,012] A3C_AGENT_WORKER-Thread-14 INFO:Local step 305500, global step 4884498: loss 0.0142
[2019-04-04 04:58:29,012] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 305500, global step 4884498: learning rate 0.0001
[2019-04-04 04:58:29,855] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.7684171e-31 2.2054199e-27 4.8648485e-29 5.5367532e-27 6.6394794e-27
 5.4349564e-31 1.0000000e+00], sum to 1.0000
[2019-04-04 04:58:29,856] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1384
[2019-04-04 04:58:29,891] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 65.5, 164.0, 509.0, 26.0, 26.10797153804938, 0.5696292845503337, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4613400.0000, 
sim time next is 4614000.0000, 
raw observation next is [-0.6666666666666667, 63.66666666666667, 158.1666666666667, 552.0, 26.0, 26.24375816658005, 0.5837525168222993, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.44413665743305636, 0.6366666666666667, 0.5272222222222224, 0.6099447513812155, 0.6666666666666666, 0.6869798472150043, 0.6945841722740997, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1144873], dtype=float32), -0.81730115]. 
=============================================
[2019-04-04 04:58:29,914] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[90.2874  ]
 [90.299095]
 [90.2617  ]
 [90.16278 ]
 [90.05928 ]], R is [[89.94793701]
 [90.04846191]
 [90.14797974]
 [90.24649811]
 [90.34403229]].
[2019-04-04 04:58:30,555] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0160655e-30 4.9128755e-27 4.5519378e-29 1.9438946e-27 4.2835502e-27
 1.1161169e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 04:58:30,556] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8025
[2019-04-04 04:58:30,570] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 57.0, 169.5, 9.0, 26.0, 26.24015692760543, 0.5486584722232254, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4532400.0000, 
sim time next is 4533000.0000, 
raw observation next is [2.0, 55.5, 152.3333333333333, 5.999999999999998, 26.0, 26.24350694111394, 0.5482897284493885, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.518005540166205, 0.555, 0.5077777777777777, 0.006629834254143645, 0.6666666666666666, 0.6869589117594949, 0.6827632428164628, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.10213277], dtype=float32), 0.38681206]. 
=============================================
[2019-04-04 04:58:30,617] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[87.69718]
 [88.13935]
 [88.84661]
 [89.61359]
 [89.65316]], R is [[87.50112915]
 [87.62612152]
 [87.74986267]
 [87.87236786]
 [87.99364471]].
[2019-04-04 04:58:34,561] A3C_AGENT_WORKER-Thread-16 INFO:Local step 305000, global step 4886214: loss 0.1209
[2019-04-04 04:58:34,563] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 305000, global step 4886214: learning rate 0.0001
[2019-04-04 04:58:36,189] A3C_AGENT_WORKER-Thread-5 INFO:Local step 305500, global step 4886698: loss 0.0158
[2019-04-04 04:58:36,190] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 305500, global step 4886698: learning rate 0.0001
[2019-04-04 04:58:36,560] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.59217834e-29 1.38480696e-24 3.38056211e-27 1.02187926e-25
 5.55329974e-25 3.26995869e-28 1.00000000e+00], sum to 1.0000
[2019-04-04 04:58:36,561] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6098
[2019-04-04 04:58:36,576] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.6666666666666667, 48.33333333333334, 0.0, 0.0, 26.0, 25.50561980709081, 0.4389407700509442, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4825200.0000, 
sim time next is 4825800.0000, 
raw observation next is [0.5, 49.0, 0.0, 0.0, 26.0, 25.53019112753844, 0.4306057922326614, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.4764542936288089, 0.49, 0.0, 0.0, 0.6666666666666666, 0.62751592729487, 0.6435352640775538, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9956001], dtype=float32), 1.7850188]. 
=============================================
[2019-04-04 04:58:37,055] A3C_AGENT_WORKER-Thread-3 INFO:Local step 305500, global step 4886973: loss 0.0275
[2019-04-04 04:58:37,056] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 305500, global step 4886973: learning rate 0.0001
[2019-04-04 04:58:38,819] A3C_AGENT_WORKER-Thread-15 INFO:Local step 305500, global step 4887636: loss 0.0293
[2019-04-04 04:58:38,820] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 305500, global step 4887637: learning rate 0.0001
[2019-04-04 04:58:41,658] A3C_AGENT_WORKER-Thread-2 INFO:Local step 305500, global step 4888458: loss 0.0315
[2019-04-04 04:58:41,672] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 305500, global step 4888464: learning rate 0.0001
[2019-04-04 04:58:41,801] A3C_AGENT_WORKER-Thread-6 INFO:Local step 305500, global step 4888510: loss 0.0290
[2019-04-04 04:58:41,802] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 305500, global step 4888510: learning rate 0.0001
[2019-04-04 04:58:43,785] A3C_AGENT_WORKER-Thread-9 INFO:Local step 305500, global step 4889183: loss 0.0451
[2019-04-04 04:58:43,786] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 305500, global step 4889183: learning rate 0.0001
[2019-04-04 04:58:43,882] A3C_AGENT_WORKER-Thread-4 INFO:Local step 305500, global step 4889210: loss 0.0431
[2019-04-04 04:58:43,895] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 305500, global step 4889210: learning rate 0.0001
[2019-04-04 04:58:44,363] A3C_AGENT_WORKER-Thread-8 INFO:Local step 305500, global step 4889347: loss 0.0595
[2019-04-04 04:58:44,364] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 305500, global step 4889347: learning rate 0.0001
[2019-04-04 04:58:44,679] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:58:44,680] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:58:44,683] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run37
[2019-04-04 04:58:44,799] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:58:44,800] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:58:44,814] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run37
[2019-04-04 04:58:44,897] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:58:44,897] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:58:44,908] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run37
[2019-04-04 04:58:46,496] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [6.0324086e-29 9.0172268e-26 9.0706235e-28 2.3768003e-26 1.3388150e-25
 7.7708631e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 04:58:46,496] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.9591
[2019-04-04 04:58:46,512] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 39.33333333333334, 0.0, 0.0, 26.0, 25.47633888491412, 0.4026847363193207, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4911000.0000, 
sim time next is 4911600.0000, 
raw observation next is [1.0, 38.66666666666667, 0.0, 0.0, 26.0, 25.536672121215, 0.3992174026060463, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.4903047091412743, 0.3866666666666667, 0.0, 0.0, 0.6666666666666666, 0.6280560101012499, 0.6330724675353488, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3979611], dtype=float32), -0.7501462]. 
=============================================
[2019-04-04 04:58:46,706] A3C_AGENT_WORKER-Thread-10 INFO:Local step 305500, global step 4889945: loss 0.0463
[2019-04-04 04:58:46,708] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 305500, global step 4889945: learning rate 0.0001
[2019-04-04 04:58:47,797] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:58:47,797] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:58:47,801] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run37
[2019-04-04 04:58:49,797] A3C_AGENT_WORKER-Thread-20 INFO:Local step 305500, global step 4890674: loss 0.0421
[2019-04-04 04:58:49,799] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 305500, global step 4890675: learning rate 0.0001
[2019-04-04 04:58:50,584] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.1222557e-30 1.8992492e-26 7.4904044e-29 1.5328809e-26 7.1406156e-27
 1.5325972e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 04:58:50,584] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8082
[2019-04-04 04:58:50,604] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.666666666666667, 29.16666666666667, 118.6666666666667, 817.0, 26.0, 26.68705747517865, 0.6029313547549043, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4963800.0000, 
sim time next is 4964400.0000, 
raw observation next is [3.0, 29.0, 119.5, 824.0, 26.0, 26.67921111624005, 0.612651287952226, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.5457063711911359, 0.29, 0.3983333333333333, 0.9104972375690608, 0.6666666666666666, 0.7232675930200042, 0.7042170959840753, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.803424], dtype=float32), -0.24929497]. 
=============================================
[2019-04-04 04:58:53,359] A3C_AGENT_WORKER-Thread-18 INFO:Local step 305500, global step 4891683: loss 0.0705
[2019-04-04 04:58:53,360] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 305500, global step 4891683: learning rate 0.0001
[2019-04-04 04:58:54,309] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:58:54,309] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:58:54,316] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run37
[2019-04-04 04:58:55,314] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:58:55,315] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:58:55,318] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run37
[2019-04-04 04:58:57,505] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:58:57,505] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:58:57,523] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run37
[2019-04-04 04:58:59,788] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:58:59,788] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:58:59,792] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run37
[2019-04-04 04:59:00,241] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:59:00,253] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:59:00,256] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run37
[2019-04-04 04:59:00,967] A3C_AGENT_WORKER-Thread-16 INFO:Local step 305500, global step 4893314: loss 0.0286
[2019-04-04 04:59:00,999] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 305500, global step 4893314: learning rate 0.0001
[2019-04-04 04:59:02,313] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:59:02,313] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:59:02,317] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run37
[2019-04-04 04:59:03,833] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:59:03,833] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:59:03,836] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run37
[2019-04-04 04:59:04,793] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:59:04,793] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:59:04,796] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run37
[2019-04-04 04:59:09,854] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:59:09,854] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:59:09,894] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run37
[2019-04-04 04:59:12,120] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.6204275e-29 1.2005450e-26 9.3930695e-28 5.9192166e-26 9.8884620e-26
 6.0175809e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 04:59:12,144] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4123
[2019-04-04 04:59:12,158] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.0, 19.0, 0.0, 0.0, 26.0, 27.48734861998543, 0.9290641152306298, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5083200.0000, 
sim time next is 5083800.0000, 
raw observation next is [9.833333333333334, 19.0, 0.0, 0.0, 26.0, 27.40956879010525, 0.9148982539520637, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.7349953831948293, 0.19, 0.0, 0.0, 0.6666666666666666, 0.7841307325087709, 0.804966084650688, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7545975], dtype=float32), 0.4604776]. 
=============================================
[2019-04-04 04:59:14,921] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:59:14,922] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:59:15,025] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run37
[2019-04-04 04:59:18,949] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:59:18,949] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:59:18,952] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run37
[2019-04-04 04:59:27,852] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1078645e-26 1.5557700e-23 3.9031378e-25 5.4601828e-24 7.1751948e-24
 2.7144223e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 04:59:27,853] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7303
[2019-04-04 04:59:27,945] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 74.0, 0.0, 0.0, 26.0, 23.72958116474457, -0.001227074442508682, 0.0, 1.0, 44091.23331460002], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 104400.0000, 
sim time next is 105000.0000, 
raw observation next is [-5.283333333333333, 74.16666666666667, 0.0, 0.0, 26.0, 23.64676150861733, -0.01660814366891405, 0.0, 1.0, 44214.39083621311], 
processed observation next is [1.0, 0.21739130434782608, 0.31625115420129274, 0.7416666666666667, 0.0, 0.0, 0.6666666666666666, 0.47056345905144426, 0.494463952110362, 0.0, 1.0, 0.21054471826768148], 
reward next is 0.7895, 
noisyNet noise sample is [array([0.13093454], dtype=float32), 0.62784094]. 
=============================================
[2019-04-04 04:59:27,981] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[77.24129]
 [77.32135]
 [77.41365]
 [77.49599]
 [77.55574]], R is [[77.17562103]
 [77.19390106]
 [77.21252441]
 [77.23149109]
 [77.25075531]].
[2019-04-04 04:59:31,062] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:59:31,063] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:59:31,137] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run37
[2019-04-04 04:59:48,490] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0256742e-27 8.5703040e-24 1.6658644e-25 1.8928140e-24 8.2717146e-24
 9.3728871e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 04:59:48,490] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0584
[2019-04-04 04:59:48,544] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 26.0, 23.14419661879123, -0.1485595375287581, 0.0, 1.0, 44239.53867925959], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 186600.0000, 
sim time next is 187200.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 26.0, 23.08932940744918, -0.1513679740907117, 0.0, 1.0, 44266.14068746554], 
processed observation next is [1.0, 0.17391304347826086, 0.21606648199445982, 0.78, 0.0, 0.0, 0.6666666666666666, 0.42411078395409846, 0.44954400863642946, 0.0, 1.0, 0.21079114613078828], 
reward next is 0.7892, 
noisyNet noise sample is [array([0.93043536], dtype=float32), -0.10759947]. 
=============================================
[2019-04-04 04:59:49,897] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-04 04:59:49,901] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 04:59:49,901] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:59:49,912] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 04:59:49,912] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:59:49,914] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run50
[2019-04-04 04:59:49,999] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run50
[2019-04-04 04:59:50,060] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 04:59:50,060] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:59:50,063] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run50
[2019-04-04 04:59:59,144] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.343453], dtype=float32), 0.2604876]
[2019-04-04 04:59:59,145] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [7.198337799666667, 86.2148403, 58.21418845, 0.0, 26.0, 23.77210509674899, -0.008040386526799026, 0.0, 1.0, 65837.94919927148]
[2019-04-04 04:59:59,145] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 04:59:59,146] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.7325735e-27 7.2471563e-24 5.3538529e-26 2.0182438e-24 2.7378548e-24
 7.7630492e-27 1.0000000e+00], sampled 0.8734018178812202
[2019-04-04 05:00:45,450] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.343453], dtype=float32), 0.2604876]
[2019-04-04 05:00:45,450] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [7.716666666666667, 64.5, 26.0, 134.6666666666667, 26.0, 25.65112601159201, 0.3318036831431365, 1.0, 1.0, 0.0]
[2019-04-04 05:00:45,450] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 05:00:45,451] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [5.2808667e-28 2.8435850e-25 8.7120635e-27 9.9600178e-25 8.0790790e-25
 3.6570983e-28 1.0000000e+00], sampled 0.7576699140038621
[2019-04-04 05:02:52,382] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 05:03:11,706] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 05:03:14,899] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 05:03:15,924] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 4900000, evaluation results [4900000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 05:03:24,636] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [8.4545570e-28 4.4375737e-24 9.2959437e-26 5.7767022e-24 3.8569457e-24
 7.0112440e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 05:03:24,636] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.6428
[2019-04-04 05:03:24,675] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.800000000000001, 67.66666666666667, 0.0, 0.0, 26.0, 24.12653228574858, 0.06556779116206439, 0.0, 1.0, 45023.6349022385], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 263400.0000, 
sim time next is 264000.0000, 
raw observation next is [-6.9, 68.33333333333334, 0.0, 0.0, 26.0, 24.08343087015335, 0.0535814927209981, 0.0, 1.0, 45188.94413755413], 
processed observation next is [1.0, 0.043478260869565216, 0.27146814404432135, 0.6833333333333335, 0.0, 0.0, 0.6666666666666666, 0.5069525725127791, 0.517860497573666, 0.0, 1.0, 0.21518544827406727], 
reward next is 0.7848, 
noisyNet noise sample is [array([-0.0633577], dtype=float32), 1.1173795]. 
=============================================
[2019-04-04 05:03:24,698] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[76.750046]
 [76.94339 ]
 [77.26421 ]
 [77.60786 ]
 [77.944   ]], R is [[76.66790771]
 [76.68682861]
 [76.70640564]
 [76.72655487]
 [76.74707031]].
[2019-04-04 05:03:44,277] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.2454301e-28 5.2184347e-25 2.9191459e-26 1.5666738e-24 3.1020646e-24
 4.9936672e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 05:03:44,277] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0855
[2019-04-04 05:03:44,303] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-14.4, 68.5, 0.0, 0.0, 26.0, 23.2991804036696, -0.1038361271528673, 0.0, 1.0, 47630.78504656757], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 348600.0000, 
sim time next is 349200.0000, 
raw observation next is [-14.5, 69.0, 0.0, 0.0, 26.0, 23.26414369170375, -0.1081439439529799, 0.0, 1.0, 47675.46836704694], 
processed observation next is [1.0, 0.043478260869565216, 0.06094182825484763, 0.69, 0.0, 0.0, 0.6666666666666666, 0.43867864097531256, 0.46395201868234004, 0.0, 1.0, 0.22702603984308065], 
reward next is 0.7730, 
noisyNet noise sample is [array([-0.8721416], dtype=float32), -1.090373]. 
=============================================
[2019-04-04 05:03:45,064] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.3334431e-28 1.2534006e-23 5.9835328e-26 1.0274655e-24 3.0520149e-24
 2.9054464e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 05:03:45,064] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1467
[2019-04-04 05:03:45,141] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-15.3, 71.0, 0.0, 0.0, 26.0, 22.58875130518184, -0.2735284855423374, 0.0, 1.0, 49324.73093906156], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 358200.0000, 
sim time next is 358800.0000, 
raw observation next is [-15.4, 71.66666666666667, 0.0, 0.0, 26.0, 22.58653680374425, -0.2892020692586711, 0.0, 1.0, 49325.32520773841], 
processed observation next is [1.0, 0.13043478260869565, 0.03601108033240995, 0.7166666666666667, 0.0, 0.0, 0.6666666666666666, 0.3822114003120207, 0.4035993102471096, 0.0, 1.0, 0.23488250098923052], 
reward next is 0.7651, 
noisyNet noise sample is [array([0.2878317], dtype=float32), -1.0707244]. 
=============================================
[2019-04-04 05:03:47,590] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.4286010e-28 3.0792395e-24 1.3987100e-26 1.3002657e-24 2.5182787e-24
 2.2808765e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 05:03:47,590] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4305
[2019-04-04 05:03:47,632] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.383333333333333, 62.5, 0.0, 0.0, 26.0, 24.91004702799769, 0.2045418239621081, 0.0, 1.0, 48493.4925236972], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 673800.0000, 
sim time next is 674400.0000, 
raw observation next is [-2.466666666666667, 63.0, 0.0, 0.0, 26.0, 24.90344773352122, 0.2106948786595149, 0.0, 1.0, 122839.1925644915], 
processed observation next is [0.0, 0.8260869565217391, 0.39427516158818104, 0.63, 0.0, 0.0, 0.6666666666666666, 0.5752873111267682, 0.5702316262198383, 0.0, 1.0, 0.5849485360213881], 
reward next is 0.4151, 
noisyNet noise sample is [array([-2.2623646], dtype=float32), 0.6389175]. 
=============================================
[2019-04-04 05:03:48,664] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.3903753e-28 3.7445294e-25 7.3086399e-27 1.0054500e-24 8.6857915e-25
 1.4269331e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 05:03:48,664] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9141
[2019-04-04 05:03:48,721] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.2, 81.0, 128.8333333333333, 488.3333333333333, 26.0, 24.9946501321666, 0.3471977419198642, 0.0, 1.0, 32031.58099451198], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 570000.0000, 
sim time next is 570600.0000, 
raw observation next is [-1.2, 81.5, 127.0, 467.0, 26.0, 24.98975120024329, 0.3543634204015632, 0.0, 1.0, 34572.10916183866], 
processed observation next is [0.0, 0.6086956521739131, 0.42936288088642666, 0.815, 0.42333333333333334, 0.5160220994475138, 0.6666666666666666, 0.5824792666869408, 0.6181211401338543, 0.0, 1.0, 0.16462909124685077], 
reward next is 0.8354, 
noisyNet noise sample is [array([0.8460176], dtype=float32), 1.7143472]. 
=============================================
[2019-04-04 05:03:50,747] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [5.6316238e-29 3.1704820e-25 1.2664958e-26 5.6458321e-25 3.3220281e-25
 5.1914039e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 05:03:50,748] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.6966
[2019-04-04 05:03:50,783] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 76.0, 0.0, 0.0, 26.0, 24.17087384925158, 0.005562917802995925, 0.0, 1.0, 42035.92877027448], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 716400.0000, 
sim time next is 717000.0000, 
raw observation next is [-2.3, 76.0, 0.0, 0.0, 26.0, 24.10621444700531, 0.002680207362833429, 0.0, 1.0, 42077.33304301019], 
processed observation next is [1.0, 0.30434782608695654, 0.3988919667590028, 0.76, 0.0, 0.0, 0.6666666666666666, 0.5088512039171093, 0.5008934024542778, 0.0, 1.0, 0.20036825258576282], 
reward next is 0.7996, 
noisyNet noise sample is [array([-1.1875445], dtype=float32), -1.1049091]. 
=============================================
[2019-04-04 05:03:50,792] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[81.94588]
 [82.02481]
 [82.09599]
 [82.15321]
 [82.22202]], R is [[81.86425781]
 [81.84544373]
 [81.82707977]
 [81.809021  ]
 [81.79124451]].
[2019-04-04 05:04:05,035] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.0154493e-29 3.5589349e-25 2.8058041e-27 1.1042534e-25 1.2098508e-25
 5.7920231e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 05:04:05,035] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2103
[2019-04-04 05:04:05,062] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 24.41645935052396, 0.1437951009507819, 0.0, 1.0, 38639.94900970437], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 887400.0000, 
sim time next is 888000.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 24.43789615405709, 0.1560711570502064, 0.0, 1.0, 38573.80798759028], 
processed observation next is [1.0, 0.2608695652173913, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5364913461714241, 0.5520237190167355, 0.0, 1.0, 0.18368479994090608], 
reward next is 0.8163, 
noisyNet noise sample is [array([1.1718342], dtype=float32), -0.6821983]. 
=============================================
[2019-04-04 05:04:05,103] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[84.71901 ]
 [84.7382  ]
 [84.748695]
 [84.734604]
 [84.69697 ]], R is [[84.68454742]
 [84.65370178]
 [84.62296295]
 [84.59224701]
 [84.56147003]].
[2019-04-04 05:04:05,292] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1818764e-29 2.2758454e-26 3.0680709e-28 8.8302740e-27 3.2306085e-26
 1.0232190e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 05:04:05,293] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4732
[2019-04-04 05:04:05,308] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 80.0, 0.0, 0.0, 26.0, 24.63529496250853, 0.2140714654035001, 0.0, 1.0, 39912.65722973297], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 865200.0000, 
sim time next is 865800.0000, 
raw observation next is [-2.3, 80.0, 0.0, 0.0, 26.0, 24.65257446765129, 0.2233866981183018, 0.0, 1.0, 39782.90920181813], 
processed observation next is [1.0, 0.0, 0.3988919667590028, 0.8, 0.0, 0.0, 0.6666666666666666, 0.5543812056376076, 0.5744622327061006, 0.0, 1.0, 0.18944242477056253], 
reward next is 0.8106, 
noisyNet noise sample is [array([-0.6324282], dtype=float32), 0.09802575]. 
=============================================
[2019-04-04 05:04:18,371] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.9438864e-32 4.2878639e-28 1.0416821e-29 7.3968190e-28 4.9651370e-28
 2.1703542e-31 1.0000000e+00], sum to 1.0000
[2019-04-04 05:04:18,382] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4052
[2019-04-04 05:04:18,450] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.8, 93.0, 95.0, 0.0, 26.0, 24.75888959810454, 0.2824096606408779, 1.0, 1.0, 187256.1929940673], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 913200.0000, 
sim time next is 913800.0000, 
raw observation next is [3.8, 93.0, 94.0, 0.0, 26.0, 24.69889554600977, 0.3302281208009413, 1.0, 1.0, 145975.1824610198], 
processed observation next is [1.0, 0.5652173913043478, 0.5678670360110805, 0.93, 0.31333333333333335, 0.0, 0.6666666666666666, 0.5582412955008141, 0.6100760402669804, 1.0, 1.0, 0.6951199164810468], 
reward next is 0.3049, 
noisyNet noise sample is [array([-2.3626351], dtype=float32), -0.5501773]. 
=============================================
[2019-04-04 05:04:23,188] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2081816e-32 5.3596072e-29 7.2767436e-31 1.0269069e-28 1.1899702e-28
 1.9824336e-32 1.0000000e+00], sum to 1.0000
[2019-04-04 05:04:23,198] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3671
[2019-04-04 05:04:23,205] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 77.66666666666667, 0.0, 0.0, 26.0, 25.12155699967074, 0.4267865185489064, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1021800.0000, 
sim time next is 1022400.0000, 
raw observation next is [14.4, 77.0, 0.0, 0.0, 26.0, 25.03387725505881, 0.412729189198129, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.8614958448753465, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5861564379215674, 0.6375763963993764, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6317261], dtype=float32), 1.9056227]. 
=============================================
[2019-04-04 05:04:25,193] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.5242886e-33 1.9287945e-29 2.4429330e-31 2.5879629e-29 9.0373194e-30
 2.8702082e-33 1.0000000e+00], sum to 1.0000
[2019-04-04 05:04:25,195] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5299
[2019-04-04 05:04:25,236] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.31666666666667, 76.0, 46.33333333333334, 0.0, 26.0, 26.89543637496941, 0.7494416080042688, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1007400.0000, 
sim time next is 1008000.0000, 
raw observation next is [15.5, 75.0, 41.0, 0.0, 26.0, 26.97290538526313, 0.5170703458668701, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.8919667590027703, 0.75, 0.13666666666666666, 0.0, 0.6666666666666666, 0.747742115438594, 0.6723567819556234, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.35389447], dtype=float32), 0.14332819]. 
=============================================
[2019-04-04 05:04:25,249] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[92.06818 ]
 [92.030266]
 [92.08554 ]
 [92.181625]
 [92.17722 ]], R is [[91.8495636 ]
 [91.93106842]
 [92.0117569 ]
 [92.09163666]
 [92.17072296]].
[2019-04-04 05:04:25,724] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.5347602e-32 3.6256065e-29 5.8200126e-31 3.7065199e-29 5.3084943e-29
 5.6489116e-33 1.0000000e+00], sum to 1.0000
[2019-04-04 05:04:25,725] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6516
[2019-04-04 05:04:25,739] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 81.0, 0.0, 0.0, 26.0, 25.6141118760364, 0.5168688400659217, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1018200.0000, 
sim time next is 1018800.0000, 
raw observation next is [14.4, 81.0, 0.0, 0.0, 26.0, 25.58534340089899, 0.517725399667309, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.8614958448753465, 0.81, 0.0, 0.0, 0.6666666666666666, 0.6321119500749157, 0.6725751332224363, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4994254], dtype=float32), 0.6788652]. 
=============================================
[2019-04-04 05:04:37,717] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.7144587e-30 1.6711910e-26 4.1415698e-28 4.2301742e-27 1.6023816e-26
 1.4219001e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 05:04:37,721] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7300
[2019-04-04 05:04:37,734] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.366666666666667, 92.0, 0.0, 0.0, 26.0, 25.44898068025331, 0.5545259714312005, 0.0, 1.0, 41246.62693601829], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1309200.0000, 
sim time next is 1309800.0000, 
raw observation next is [2.283333333333333, 92.0, 0.0, 0.0, 26.0, 25.43767060049753, 0.548102755470448, 0.0, 1.0, 42227.61518725208], 
processed observation next is [1.0, 0.13043478260869565, 0.5258541089566021, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6198058833747941, 0.6827009184901494, 0.0, 1.0, 0.2010838818440575], 
reward next is 0.7989, 
noisyNet noise sample is [array([0.5996797], dtype=float32), -1.1386334]. 
=============================================
[2019-04-04 05:04:39,839] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.4326587e-29 4.3189605e-26 3.5087959e-28 1.9967994e-26 4.1956082e-26
 5.6866296e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 05:04:39,846] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2042
[2019-04-04 05:04:39,874] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.566666666666666, 96.0, 0.0, 0.0, 26.0, 24.68783654836951, 0.4439893302794873, 0.0, 1.0, 36757.15003903201], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1276800.0000, 
sim time next is 1277400.0000, 
raw observation next is [7.383333333333333, 96.0, 0.0, 0.0, 26.0, 24.68821687596179, 0.446131238242409, 0.0, 1.0, 34623.67201619082], 
processed observation next is [0.0, 0.782608695652174, 0.6671283471837489, 0.96, 0.0, 0.0, 0.6666666666666666, 0.5573514063301491, 0.6487104127474697, 0.0, 1.0, 0.16487462864852773], 
reward next is 0.8351, 
noisyNet noise sample is [array([0.9046544], dtype=float32), -1.0934076]. 
=============================================
[2019-04-04 05:04:43,283] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.1953924e-30 2.4724230e-27 1.0303050e-28 2.5329964e-27 5.3558317e-27
 3.3427787e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 05:04:43,285] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9014
[2019-04-04 05:04:43,300] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.433333333333334, 90.0, 0.0, 0.0, 26.0, 25.57926635995268, 0.5604650887250195, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1459200.0000, 
sim time next is 1459800.0000, 
raw observation next is [1.35, 90.5, 0.0, 0.0, 26.0, 25.59922113240688, 0.552990466845501, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.5000000000000001, 0.905, 0.0, 0.0, 0.6666666666666666, 0.6332684277005732, 0.6843301556151671, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.01311676], dtype=float32), -0.7066283]. 
=============================================
[2019-04-04 05:04:44,259] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.1472223e-32 9.9351475e-28 1.2500066e-29 9.1606164e-28 1.0476368e-27
 4.9395688e-31 1.0000000e+00], sum to 1.0000
[2019-04-04 05:04:44,259] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9737
[2019-04-04 05:04:44,303] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.3, 97.5, 46.0, 0.0, 26.0, 25.94719382034214, 0.5155366606126869, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1416600.0000, 
sim time next is 1417200.0000, 
raw observation next is [-0.2, 96.66666666666666, 50.33333333333333, 0.0, 26.0, 25.98647172532235, 0.5169708783875132, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4570637119113574, 0.9666666666666666, 0.16777777777777778, 0.0, 0.6666666666666666, 0.6655393104435291, 0.6723236261291711, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.92538077], dtype=float32), 1.4633893]. 
=============================================
[2019-04-04 05:05:02,795] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.8111626e-27 1.6479285e-23 6.1925279e-26 2.7082400e-24 4.1732970e-24
 1.5038288e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 05:05:02,795] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0829
[2019-04-04 05:05:02,829] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.283333333333333, 87.0, 0.0, 0.0, 26.0, 24.88535198898978, 0.3384855960510033, 0.0, 1.0, 43869.36444674501], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1750200.0000, 
sim time next is 1750800.0000, 
raw observation next is [-1.366666666666667, 87.0, 0.0, 0.0, 26.0, 24.85728994280371, 0.3330268283482395, 0.0, 1.0, 43916.59201584577], 
processed observation next is [0.0, 0.2608695652173913, 0.42474607571560485, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5714408285669759, 0.6110089427827465, 0.0, 1.0, 0.2091266286468846], 
reward next is 0.7909, 
noisyNet noise sample is [array([-1.3175222], dtype=float32), -0.56535023]. 
=============================================
[2019-04-04 05:05:03,881] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [6.3150580e-31 1.2436725e-27 9.8610610e-29 2.0685951e-26 4.9826146e-27
 2.8102882e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 05:05:03,882] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.9894
[2019-04-04 05:05:03,935] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.183333333333333, 86.83333333333334, 20.33333333333334, 0.0, 26.0, 25.49725234869285, 0.5073897304244125, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1702200.0000, 
sim time next is 1702800.0000, 
raw observation next is [1.1, 88.0, 15.5, 0.0, 26.0, 25.74591497964901, 0.5234089911970701, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.49307479224376743, 0.88, 0.051666666666666666, 0.0, 0.6666666666666666, 0.645492914970751, 0.6744696637323567, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3900785], dtype=float32), -0.57327247]. 
=============================================
[2019-04-04 05:05:10,975] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.6755556e-31 9.1148824e-28 4.5946447e-29 4.5790764e-27 8.8350249e-27
 5.4033276e-31 1.0000000e+00], sum to 1.0000
[2019-04-04 05:05:10,976] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0559
[2019-04-04 05:05:11,020] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.583333333333334, 65.16666666666666, 0.0, 0.0, 26.0, 26.24670260780302, 0.6895663290040058, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1623000.0000, 
sim time next is 1623600.0000, 
raw observation next is [9.4, 66.0, 0.0, 0.0, 26.0, 26.18656358282908, 0.6745704674234201, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.7229916897506927, 0.66, 0.0, 0.0, 0.6666666666666666, 0.6822136319024233, 0.7248568224744734, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5398351], dtype=float32), 0.06746117]. 
=============================================
[2019-04-04 05:05:16,845] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.3564772e-27 5.7237290e-24 6.4480591e-26 3.1359379e-24 1.6190466e-24
 3.5612671e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 05:05:16,847] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4487
[2019-04-04 05:05:16,936] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.7, 78.0, 27.0, 0.0, 26.0, 23.50548875717431, 0.05706513801021209, 0.0, 1.0, 203468.6302052165], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1845000.0000, 
sim time next is 1845600.0000, 
raw observation next is [-6.700000000000001, 78.0, 47.16666666666666, 15.66666666666666, 26.0, 24.1707305896439, 0.1630856528805252, 0.0, 1.0, 161283.0144685804], 
processed observation next is [0.0, 0.34782608695652173, 0.2770083102493075, 0.78, 0.15722222222222218, 0.017311233885819514, 0.6666666666666666, 0.5142275491369915, 0.5543618842935084, 0.0, 1.0, 0.7680143546122876], 
reward next is 0.2320, 
noisyNet noise sample is [array([-0.23652714], dtype=float32), 0.057004087]. 
=============================================
[2019-04-04 05:05:19,409] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [8.5400703e-27 2.8320264e-23 2.6868043e-25 1.2647355e-23 1.3198059e-23
 5.9461475e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 05:05:19,410] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.6932
[2019-04-04 05:05:19,464] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 81.66666666666667, 19.66666666666667, 0.0, 26.0, 25.03009228933469, 0.2506010356942625, 0.0, 1.0, 46018.72325876683], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1875000.0000, 
sim time next is 1875600.0000, 
raw observation next is [-4.5, 83.0, 15.0, 0.0, 26.0, 25.01208707985355, 0.2497854512403863, 0.0, 1.0, 54554.82386625389], 
processed observation next is [0.0, 0.7391304347826086, 0.3379501385041552, 0.83, 0.05, 0.0, 0.6666666666666666, 0.5843405899877959, 0.5832618170801288, 0.0, 1.0, 0.25978487555359], 
reward next is 0.7402, 
noisyNet noise sample is [array([0.22702122], dtype=float32), -1.3530833]. 
=============================================
[2019-04-04 05:05:23,455] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0332067e-26 6.1368945e-24 7.3572884e-26 5.9030548e-24 4.7724953e-24
 9.8331242e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 05:05:23,469] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7876
[2019-04-04 05:05:23,527] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.1, 87.0, 66.0, 0.0, 26.0, 25.0162134254107, 0.3453880413953148, 0.0, 1.0, 34211.48044602873], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1783800.0000, 
sim time next is 1784400.0000, 
raw observation next is [-3.2, 87.0, 59.66666666666666, 0.0, 26.0, 25.04321473768123, 0.3394310559125643, 0.0, 1.0, 26056.39880157877], 
processed observation next is [0.0, 0.6521739130434783, 0.37396121883656513, 0.87, 0.19888888888888887, 0.0, 0.6666666666666666, 0.5869345614734357, 0.6131436853041881, 0.0, 1.0, 0.12407808953132747], 
reward next is 0.8759, 
noisyNet noise sample is [array([0.06596532], dtype=float32), 0.76242596]. 
=============================================
[2019-04-04 05:05:34,298] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.3296331e-30 3.8349333e-27 1.5128147e-28 2.4038818e-26 5.7620361e-27
 4.9998379e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 05:05:34,298] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8387
[2019-04-04 05:05:34,352] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 78.33333333333334, 153.3333333333333, 0.0, 26.0, 25.32275660983174, 0.3100822329044441, 1.0, 1.0, 35237.52014046651], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2033400.0000, 
sim time next is 2034000.0000, 
raw observation next is [-4.5, 79.0, 152.0, 0.0, 26.0, 25.36736097517946, 0.2169405854829753, 1.0, 1.0, 32062.39217552042], 
processed observation next is [1.0, 0.5652173913043478, 0.3379501385041552, 0.79, 0.5066666666666667, 0.0, 0.6666666666666666, 0.6139467479316215, 0.5723135284943252, 1.0, 1.0, 0.15267805797866868], 
reward next is 0.8473, 
noisyNet noise sample is [array([1.5289004], dtype=float32), -0.5246053]. 
=============================================
[2019-04-04 05:05:34,357] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[82.94503]
 [83.01595]
 [83.09608]
 [83.22842]
 [83.31356]], R is [[82.75147247]
 [82.75615692]
 [82.72219849]
 [82.64463043]
 [82.8181839 ]].
[2019-04-04 05:05:44,808] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.7198385e-27 3.0955880e-23 1.4530483e-25 4.7493939e-24 6.6564556e-24
 1.2419711e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 05:05:44,808] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9051
[2019-04-04 05:05:44,870] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.2, 67.66666666666667, 0.0, 0.0, 26.0, 24.20917106169112, 0.1183870251932469, 0.0, 1.0, 41035.51773328586], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2352000.0000, 
sim time next is 2352600.0000, 
raw observation next is [-3.1, 67.0, 0.0, 0.0, 26.0, 24.19452350110534, 0.1134680641518058, 0.0, 1.0, 41092.6364387465], 
processed observation next is [0.0, 0.21739130434782608, 0.37673130193905824, 0.67, 0.0, 0.0, 0.6666666666666666, 0.5162102917587784, 0.537822688050602, 0.0, 1.0, 0.1956792211368881], 
reward next is 0.8043, 
noisyNet noise sample is [array([0.04583083], dtype=float32), 0.6707975]. 
=============================================
[2019-04-04 05:05:47,800] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4990376e-30 1.4159259e-27 1.8922099e-29 3.0047390e-27 1.7908610e-27
 4.1676343e-31 1.0000000e+00], sum to 1.0000
[2019-04-04 05:05:47,800] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9809
[2019-04-04 05:05:47,860] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.9166666666666667, 43.16666666666667, 132.3333333333333, 48.0, 26.0, 26.31915576063546, 0.3974217330195591, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2301000.0000, 
sim time next is 2301600.0000, 
raw observation next is [0.7333333333333335, 43.33333333333334, 135.1666666666667, 45.0, 26.0, 25.8458250894587, 0.4285893580609319, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.4829178208679595, 0.4333333333333334, 0.4505555555555557, 0.049723756906077346, 0.6666666666666666, 0.6538187574548916, 0.6428631193536439, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.04351132], dtype=float32), -1.950146]. 
=============================================
[2019-04-04 05:06:05,903] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.7795668e-29 8.1729675e-26 1.4030372e-28 1.3169674e-26 3.1877376e-26
 1.1633046e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 05:06:05,906] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1886
[2019-04-04 05:06:05,973] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.233333333333333, 28.5, 89.0, 840.6666666666666, 26.0, 24.91859079848704, 0.2696375241517449, 0.0, 1.0, 18719.87687657799], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2465400.0000, 
sim time next is 2466000.0000, 
raw observation next is [1.6, 28.0, 88.5, 838.5, 26.0, 24.94172266057019, 0.2738209550159147, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.5069252077562327, 0.28, 0.295, 0.9265193370165746, 0.6666666666666666, 0.5784768883808491, 0.5912736516719715, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.36526883], dtype=float32), -0.35833523]. 
=============================================
[2019-04-04 05:06:05,991] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[85.6832 ]
 [85.73312]
 [85.774  ]
 [85.73899]
 [85.71082]], R is [[85.80747223]
 [85.86026001]
 [85.87716675]
 [85.8621521 ]
 [85.8832016 ]].
[2019-04-04 05:06:14,037] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.9268741e-30 2.3588605e-27 5.4369941e-29 2.4941444e-27 2.9389997e-27
 4.1834993e-31 1.0000000e+00], sum to 1.0000
[2019-04-04 05:06:14,038] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9875
[2019-04-04 05:06:14,094] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.0, 68.0, 116.1666666666667, 691.8333333333334, 26.0, 26.08157597999655, 0.4731931516495056, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2716800.0000, 
sim time next is 2717400.0000, 
raw observation next is [-9.5, 66.0, 115.3333333333333, 709.6666666666666, 26.0, 26.07939661712038, 0.4789665873910186, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.1994459833795014, 0.66, 0.3844444444444443, 0.7841620626151012, 0.6666666666666666, 0.6732830514266984, 0.6596555291303395, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3272815], dtype=float32), 0.022604393]. 
=============================================
[2019-04-04 05:06:17,413] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.6472334e-29 4.3815908e-25 9.2063373e-27 3.3834942e-25 5.8652301e-25
 2.9158931e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 05:06:17,413] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4707
[2019-04-04 05:06:17,482] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 46.5, 0.0, 0.0, 26.0, 25.01493642069213, 0.1894419233600829, 0.0, 1.0, 38456.60243714591], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2518200.0000, 
sim time next is 2518800.0000, 
raw observation next is [-1.7, 47.33333333333333, 0.0, 0.0, 26.0, 25.03423804158064, 0.1827555784438874, 0.0, 1.0, 38425.79519834371], 
processed observation next is [1.0, 0.13043478260869565, 0.4155124653739613, 0.4733333333333333, 0.0, 0.0, 0.6666666666666666, 0.5861865034650533, 0.5609185261479624, 0.0, 1.0, 0.18297997713497005], 
reward next is 0.8170, 
noisyNet noise sample is [array([0.5895977], dtype=float32), -0.65122885]. 
=============================================
[2019-04-04 05:06:21,843] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.3344994e-30 1.8428671e-26 2.0014340e-28 8.2237840e-27 2.9235589e-26
 1.0314881e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 05:06:21,843] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0549
[2019-04-04 05:06:21,926] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 64.0, 130.0, 220.0, 26.0, 25.94020855822558, 0.382390931466859, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2797200.0000, 
sim time next is 2797800.0000, 
raw observation next is [-5.5, 62.5, 137.3333333333333, 224.3333333333333, 26.0, 25.8706307081135, 0.3805153157781169, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.3102493074792244, 0.625, 0.4577777777777776, 0.24788213627992628, 0.6666666666666666, 0.6558858923427916, 0.6268384385927056, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.21350543], dtype=float32), -1.5884198]. 
=============================================
[2019-04-04 05:06:30,575] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.9556846e-27 1.6041576e-23 2.8723449e-25 2.6651543e-24 2.1585888e-23
 1.4226076e-26 1.0000000e+00], sum to 1.0000
[2019-04-04 05:06:30,576] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1019
[2019-04-04 05:06:30,610] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.66666666666667, 74.83333333333333, 0.0, 0.0, 26.0, 24.20127929972993, 0.143383219044132, 0.0, 1.0, 44485.77999187382], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2685000.0000, 
sim time next is 2685600.0000, 
raw observation next is [-11.0, 76.0, 0.0, 0.0, 26.0, 24.11768160941596, 0.1426129540938375, 0.0, 1.0, 44489.67766652806], 
processed observation next is [1.0, 0.08695652173913043, 0.15789473684210528, 0.76, 0.0, 0.0, 0.6666666666666666, 0.5098068007846633, 0.5475376513646125, 0.0, 1.0, 0.21185560793584793], 
reward next is 0.7881, 
noisyNet noise sample is [array([0.03588805], dtype=float32), -1.3877705]. 
=============================================
[2019-04-04 05:06:42,455] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.4365138e-29 1.6894979e-26 1.5197101e-27 5.1132251e-26 1.3505101e-25
 8.5092949e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 05:06:42,455] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8920
[2019-04-04 05:06:42,486] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 50.0, 37.00000000000001, 320.3333333333334, 26.0, 25.90765175488876, 0.4586791579018502, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2739000.0000, 
sim time next is 2739600.0000, 
raw observation next is [-3.0, 50.0, 28.5, 253.5, 26.0, 25.59610119693251, 0.4543076756240847, 1.0, 1.0, 172573.9780704277], 
processed observation next is [1.0, 0.7391304347826086, 0.3795013850415513, 0.5, 0.095, 0.28011049723756903, 0.6666666666666666, 0.633008433077709, 0.6514358918746949, 1.0, 1.0, 0.8217808479544176], 
reward next is 0.1782, 
noisyNet noise sample is [array([1.1671591], dtype=float32), 1.2460475]. 
=============================================
[2019-04-04 05:06:44,257] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [9.9385968e-28 1.7513148e-24 1.8764206e-26 1.9303967e-24 1.4702387e-24
 2.1740625e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 05:06:44,257] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5276
[2019-04-04 05:06:44,330] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.083333333333333, 65.0, 0.0, 0.0, 26.0, 25.37707140951009, 0.3763357433908452, 0.0, 1.0, 38490.9249285835], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3010200.0000, 
sim time next is 3010800.0000, 
raw observation next is [-3.166666666666667, 65.0, 0.0, 0.0, 26.0, 25.36911401194112, 0.3730859642436187, 0.0, 1.0, 42218.90112181056], 
processed observation next is [0.0, 0.8695652173913043, 0.3748845798707295, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6140928343284268, 0.6243619880812062, 0.0, 1.0, 0.201042386294336], 
reward next is 0.7990, 
noisyNet noise sample is [array([-1.2995576], dtype=float32), 2.0300877]. 
=============================================
[2019-04-04 05:06:45,292] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.2502596e-29 2.6038206e-26 2.0918365e-27 1.3239090e-25 2.7853212e-25
 1.5209671e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 05:06:45,292] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4043
[2019-04-04 05:06:45,349] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 44.0, 0.0, 0.0, 26.0, 24.86941428174969, 0.3461467890452223, 0.0, 1.0, 187038.0894313329], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2838600.0000, 
sim time next is 2839200.0000, 
raw observation next is [2.0, 44.0, 0.0, 0.0, 26.0, 24.94182128764323, 0.3770508818699832, 0.0, 1.0, 113291.3222532294], 
processed observation next is [1.0, 0.8695652173913043, 0.518005540166205, 0.44, 0.0, 0.0, 0.6666666666666666, 0.5784851073036025, 0.6256836272899944, 0.0, 1.0, 0.53948248692014], 
reward next is 0.4605, 
noisyNet noise sample is [array([-0.94882506], dtype=float32), 0.36605787]. 
=============================================
[2019-04-04 05:06:49,253] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1033553e-27 5.8684333e-24 6.8153416e-26 2.2008263e-24 4.8894380e-24
 2.5102565e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 05:06:49,253] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7863
[2019-04-04 05:06:49,266] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.166666666666667, 65.0, 0.0, 0.0, 26.0, 25.36911401194112, 0.3730859642436187, 0.0, 1.0, 42218.90112181056], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3010800.0000, 
sim time next is 3011400.0000, 
raw observation next is [-3.25, 65.0, 0.0, 0.0, 26.0, 25.35505022859006, 0.3686186618435411, 0.0, 1.0, 41307.72582661316], 
processed observation next is [0.0, 0.8695652173913043, 0.3725761772853186, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6129208523825049, 0.6228728872811803, 0.0, 1.0, 0.19670345631720554], 
reward next is 0.8033, 
noisyNet noise sample is [array([0.34810328], dtype=float32), 0.47448248]. 
=============================================
[2019-04-04 05:07:01,911] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.7661359e-28 2.6574781e-25 2.0435274e-27 9.8901217e-26 3.2501405e-25
 1.5076643e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 05:07:01,911] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1787
[2019-04-04 05:07:01,943] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 55.0, 69.5, 570.5, 26.0, 25.14849951573077, 0.407805903577534, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2995200.0000, 
sim time next is 2995800.0000, 
raw observation next is [-1.0, 55.0, 65.0, 538.3333333333334, 26.0, 25.17184822680989, 0.3976545228936872, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.4349030470914128, 0.55, 0.21666666666666667, 0.5948434622467772, 0.6666666666666666, 0.597654018900824, 0.632551507631229, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2640195], dtype=float32), 0.66585505]. 
=============================================
[2019-04-04 05:07:04,995] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.4654427e-28 6.8880844e-25 6.0303440e-27 5.2210631e-25 9.4922124e-25
 3.3853351e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 05:07:04,995] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5155
[2019-04-04 05:07:05,048] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.166666666666667, 55.83333333333334, 22.66666666666666, 224.0, 26.0, 25.09202974464196, 0.3585795498795973, 0.0, 1.0, 27198.70935165413], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2999400.0000, 
sim time next is 3000000.0000, 
raw observation next is [-1.333333333333333, 56.66666666666667, 14.33333333333333, 161.5, 26.0, 25.07558730985142, 0.3484521427317866, 0.0, 1.0, 39517.52968874524], 
processed observation next is [0.0, 0.7391304347826086, 0.42566943674976926, 0.5666666666666668, 0.047777777777777766, 0.17845303867403314, 0.6666666666666666, 0.5896322758209518, 0.6161507142439289, 0.0, 1.0, 0.18817871280354878], 
reward next is 0.8118, 
noisyNet noise sample is [array([0.01799464], dtype=float32), -0.23018341]. 
=============================================
[2019-04-04 05:07:05,124] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[82.1557 ]
 [82.39164]
 [82.70586]
 [83.10492]
 [83.44786]], R is [[81.97652435]
 [82.02724457]
 [82.11161804]
 [82.11018372]
 [82.07287598]].
[2019-04-04 05:07:08,795] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.4810083e-29 7.6769764e-25 2.1891674e-27 1.6978453e-26 5.4252375e-25
 3.6868359e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 05:07:08,796] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0274
[2019-04-04 05:07:08,866] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.666666666666667, 58.16666666666667, 93.66666666666667, 543.0, 26.0, 25.39736984913916, 0.3248465214218544, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3057000.0000, 
sim time next is 3057600.0000, 
raw observation next is [-5.333333333333333, 57.33333333333334, 96.33333333333333, 589.0000000000001, 26.0, 25.37026755062598, 0.3244001449059064, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.3148661126500462, 0.5733333333333335, 0.32111111111111107, 0.650828729281768, 0.6666666666666666, 0.6141889625521649, 0.6081333816353022, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7167082], dtype=float32), 0.54562426]. 
=============================================
[2019-04-04 05:07:14,606] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.5415763e-29 6.9702892e-25 6.2986869e-27 1.5868768e-25 5.3580236e-25
 3.6596660e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 05:07:14,606] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5885
[2019-04-04 05:07:14,734] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 93.0, 0.0, 0.0, 26.0, 24.79532144103628, 0.2347997913178192, 0.0, 1.0, 51848.468990095], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2877000.0000, 
sim time next is 2877600.0000, 
raw observation next is [2.0, 93.0, 0.0, 0.0, 26.0, 24.80923560618925, 0.2532786808668571, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.518005540166205, 0.93, 0.0, 0.0, 0.6666666666666666, 0.5674363005157709, 0.584426226955619, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6791226], dtype=float32), -0.8615474]. 
=============================================
[2019-04-04 05:07:19,007] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.4705834e-28 7.9475769e-25 2.0843263e-26 4.9186375e-25 6.9666210e-25
 1.2474736e-27 1.0000000e+00], sum to 1.0000
[2019-04-04 05:07:19,008] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9327
[2019-04-04 05:07:19,054] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.0, 77.0, 0.0, 0.0, 26.0, 24.97817809194738, 0.3488945989233983, 0.0, 1.0, 43816.7033756431], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3291000.0000, 
sim time next is 3291600.0000, 
raw observation next is [-8.0, 77.0, 0.0, 0.0, 26.0, 24.93334948045548, 0.3466928504435836, 0.0, 1.0, 43831.6576442455], 
processed observation next is [1.0, 0.08695652173913043, 0.24099722991689754, 0.77, 0.0, 0.0, 0.6666666666666666, 0.57777912337129, 0.6155642834811945, 0.0, 1.0, 0.20872217925831188], 
reward next is 0.7913, 
noisyNet noise sample is [array([-1.4445528], dtype=float32), -0.2970503]. 
=============================================
[2019-04-04 05:07:25,478] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.0907125e-28 1.3285034e-24 9.7323657e-27 3.6521827e-25 6.8471633e-25
 8.7716508e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 05:07:25,478] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4486
[2019-04-04 05:07:25,509] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.633333333333333, 76.33333333333333, 0.0, 0.0, 26.0, 24.5582754343495, 0.228793626557963, 0.0, 1.0, 43773.94800161353], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3300000.0000, 
sim time next is 3300600.0000, 
raw observation next is [-9.816666666666666, 76.16666666666667, 0.0, 0.0, 26.0, 24.48966109779854, 0.2246383629734358, 0.0, 1.0, 43761.94857231114], 
processed observation next is [1.0, 0.17391304347826086, 0.19067405355494, 0.7616666666666667, 0.0, 0.0, 0.6666666666666666, 0.5408050914832115, 0.5748794543244786, 0.0, 1.0, 0.20839023129671974], 
reward next is 0.7916, 
noisyNet noise sample is [array([-0.34064847], dtype=float32), -1.1882643]. 
=============================================
[2019-04-04 05:07:27,237] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.8371988e-30 1.2590444e-25 1.6073335e-27 2.9278560e-26 6.7630266e-26
 3.5115123e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 05:07:27,238] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1019
[2019-04-04 05:07:27,284] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 61.66666666666667, 0.0, 0.0, 26.0, 24.6823504006484, 0.216654337809406, 0.0, 1.0, 42859.26255919712], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3392400.0000, 
sim time next is 3393000.0000, 
raw observation next is [-3.0, 62.5, 0.0, 0.0, 26.0, 24.68831369794104, 0.2071734665632334, 0.0, 1.0, 42846.83216407803], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.625, 0.0, 0.0, 0.6666666666666666, 0.5573594748284201, 0.5690578221877445, 0.0, 1.0, 0.20403253411465727], 
reward next is 0.7960, 
noisyNet noise sample is [array([1.3132277], dtype=float32), -0.05735289]. 
=============================================
[2019-04-04 05:07:27,334] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[85.21206 ]
 [85.15998 ]
 [85.0877  ]
 [84.93604 ]
 [84.762535]], R is [[85.15800476]
 [85.10233307]
 [85.04729462]
 [84.9931488 ]
 [84.94023132]].
[2019-04-04 05:07:28,182] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.0889453e-30 2.6329262e-26 4.3482828e-28 1.6478192e-26 1.5602904e-26
 7.7227195e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 05:07:28,182] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1138
[2019-04-04 05:07:28,202] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.333333333333333, 97.66666666666666, 0.0, 0.0, 26.0, 25.52024396892082, 0.6047160110945988, 0.0, 1.0, 18749.87550455641], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3199200.0000, 
sim time next is 3199800.0000, 
raw observation next is [1.166666666666667, 98.83333333333334, 0.0, 0.0, 26.0, 25.57084413782835, 0.6148879234403946, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.49492151431209613, 0.9883333333333334, 0.0, 0.0, 0.6666666666666666, 0.6309036781523624, 0.7049626411467983, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.13595282], dtype=float32), 1.7074682]. 
=============================================
[2019-04-04 05:07:29,574] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.2304282e-29 5.7764572e-25 4.9756624e-27 9.6188386e-26 2.4828125e-25
 7.4265342e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 05:07:29,574] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5583
[2019-04-04 05:07:29,615] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.0, 77.0, 0.0, 0.0, 26.0, 24.83977598718884, 0.3093039836941421, 0.0, 1.0, 43908.25010039363], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3294000.0000, 
sim time next is 3294600.0000, 
raw observation next is [-8.15, 77.0, 0.0, 0.0, 26.0, 24.79025885486099, 0.2962520335436986, 0.0, 1.0, 43930.81620955041], 
processed observation next is [1.0, 0.13043478260869565, 0.2368421052631579, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5658549045717493, 0.5987506778478996, 0.0, 1.0, 0.209194362902621], 
reward next is 0.7908, 
noisyNet noise sample is [array([-1.0914208], dtype=float32), -0.17170975]. 
=============================================
[2019-04-04 05:07:35,773] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.6273831e-29 2.1409143e-25 1.9489128e-28 1.4137300e-26 4.3011707e-26
 2.6364299e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 05:07:35,806] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0341
[2019-04-04 05:07:35,840] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.333333333333333, 51.66666666666666, 113.1666666666667, 815.1666666666667, 26.0, 25.09868088443422, 0.3575328473731237, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3069600.0000, 
sim time next is 3070200.0000, 
raw observation next is [-2.166666666666667, 50.83333333333334, 112.3333333333333, 813.3333333333334, 26.0, 25.10659254425747, 0.3569025924651283, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.4025854108956602, 0.5083333333333334, 0.37444444444444436, 0.8987108655616943, 0.6666666666666666, 0.5922160453547892, 0.6189675308217094, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.58556443], dtype=float32), 1.3834124]. 
=============================================
[2019-04-04 05:07:45,262] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.8274767e-31 1.0702004e-27 5.0000286e-30 3.7134707e-28 1.7106256e-27
 7.6131397e-32 1.0000000e+00], sum to 1.0000
[2019-04-04 05:07:45,262] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7761
[2019-04-04 05:07:45,287] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 47.0, 115.0, 804.0, 26.0, 26.61142676651667, 0.4194531454810574, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3411000.0000, 
sim time next is 3411600.0000, 
raw observation next is [3.0, 46.33333333333334, 115.3333333333333, 806.1666666666666, 26.0, 26.59255329158398, 0.6201942954823244, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.5457063711911359, 0.46333333333333343, 0.3844444444444443, 0.8907918968692449, 0.6666666666666666, 0.7160461076319983, 0.7067314318274415, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.8820393], dtype=float32), 0.08726533]. 
=============================================
[2019-04-04 05:07:46,347] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.3334041e-30 3.5339187e-26 5.0921780e-28 2.0087329e-26 3.4869963e-26
 9.4380648e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 05:07:46,347] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8089
[2019-04-04 05:07:46,418] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.5, 71.5, 3.0, 107.0, 26.0, 25.37705207028172, 0.4032276070388975, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3483000.0000, 
sim time next is 3483600.0000, 
raw observation next is [-0.6666666666666666, 71.33333333333333, 17.16666666666666, 155.6666666666667, 26.0, 25.49621443111022, 0.394000745408184, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.44413665743305636, 0.7133333333333333, 0.0572222222222222, 0.17200736648250467, 0.6666666666666666, 0.6246845359258518, 0.631333581802728, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5285802], dtype=float32), 0.86392486]. 
=============================================
[2019-04-04 05:07:50,012] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.8560217e-29 2.5979103e-24 4.1817844e-27 2.8220266e-25 3.6212736e-25
 5.9317554e-28 1.0000000e+00], sum to 1.0000
[2019-04-04 05:07:50,039] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5643
[2019-04-04 05:07:50,065] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.166666666666666, 70.0, 0.0, 0.0, 26.0, 24.36501381053674, 0.1993923217021012, 0.0, 1.0, 41341.72411718805], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3568200.0000, 
sim time next is 3568800.0000, 
raw observation next is [-6.333333333333333, 70.0, 0.0, 0.0, 26.0, 24.32524394910712, 0.1902726077945649, 0.0, 1.0, 41462.60630117857], 
processed observation next is [0.0, 0.30434782608695654, 0.28716528162511545, 0.7, 0.0, 0.0, 0.6666666666666666, 0.5271036624255933, 0.5634242025981883, 0.0, 1.0, 0.19744098238656463], 
reward next is 0.8026, 
noisyNet noise sample is [array([0.6212558], dtype=float32), 0.2589918]. 
=============================================
[2019-04-04 05:07:50,383] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.2816157e-30 2.9808215e-27 1.4425580e-28 2.8233603e-26 1.7192225e-26
 1.3233200e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 05:07:50,384] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9280
[2019-04-04 05:07:50,392] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.666666666666667, 61.0, 87.16666666666666, 715.8333333333334, 26.0, 26.51806901351702, 0.6613466718121495, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3424800.0000, 
sim time next is 3425400.0000, 
raw observation next is [2.5, 62.5, 84.0, 704.0, 26.0, 26.59393394823696, 0.6707037780802683, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5318559556786704, 0.625, 0.28, 0.7779005524861878, 0.6666666666666666, 0.7161611623530799, 0.7235679260267561, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.72682565], dtype=float32), -0.023242952]. 
=============================================
[2019-04-04 05:07:55,287] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.05403355e-29 8.04849883e-26 9.14769538e-28 4.54051605e-26
 5.78860832e-26 6.08106662e-29 1.00000000e+00], sum to 1.0000
[2019-04-04 05:07:55,294] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7779
[2019-04-04 05:07:55,322] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 50.0, 0.0, 0.0, 26.0, 25.41378827164085, 0.3905148059942955, 0.0, 1.0, 29685.99459014917], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3621600.0000, 
sim time next is 3622200.0000, 
raw observation next is [-2.166666666666667, 51.66666666666667, 0.0, 0.0, 26.0, 25.40787898411963, 0.3844421805686482, 0.0, 1.0, 35118.6726749068], 
processed observation next is [0.0, 0.9565217391304348, 0.4025854108956602, 0.5166666666666667, 0.0, 0.0, 0.6666666666666666, 0.6173232486766359, 0.6281473935228827, 0.0, 1.0, 0.16723177464241332], 
reward next is 0.8328, 
noisyNet noise sample is [array([1.4448256], dtype=float32), -1.157799]. 
=============================================
[2019-04-04 05:08:26,741] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.9925078e-29 7.3526865e-26 2.2768775e-27 4.6022318e-25 1.2875651e-25
 9.4944386e-29 1.0000000e+00], sum to 1.0000
[2019-04-04 05:08:26,741] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2450
[2019-04-04 05:08:26,772] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 56.66666666666667, 0.0, 0.0, 26.0, 25.55542665639526, 0.4528785516388698, 0.0, 1.0, 72485.67059545295], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3882000.0000, 
sim time next is 3882600.0000, 
raw observation next is [-1.0, 57.5, 0.0, 0.0, 26.0, 25.37166594154025, 0.4469849254585362, 0.0, 1.0, 150280.4522877796], 
processed observation next is [1.0, 0.9565217391304348, 0.4349030470914128, 0.575, 0.0, 0.0, 0.6666666666666666, 0.6143054951283542, 0.6489949751528453, 0.0, 1.0, 0.715621201370379], 
reward next is 0.2844, 
noisyNet noise sample is [array([-1.3331605], dtype=float32), 0.7293709]. 
=============================================
[2019-04-04 05:08:28,987] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.2427975e-30 4.5715714e-27 4.7965932e-29 5.6988833e-27 6.0530222e-27
 1.5511126e-30 1.0000000e+00], sum to 1.0000
[2019-04-04 05:08:28,993] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2959
[2019-04-04 05:08:29,034] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.833333333333333, 34.5, 110.6666666666667, 739.0, 26.0, 26.53381008076571, 0.5822239171785163, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4097400.0000, 
sim time next is 4098000.0000, 
raw observation next is [-1.666666666666667, 34.0, 112.3333333333333, 754.0, 26.0, 26.60038298783044, 0.5879698109746915, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4164358264081256, 0.34, 0.37444444444444436, 0.8331491712707182, 0.6666666666666666, 0.7166985823192032, 0.6959899369915639, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.98267347], dtype=float32), 1.1678678]. 
=============================================
[2019-04-04 05:08:29,061] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[88.54053 ]
 [88.65425 ]
 [88.7677  ]
 [89.043274]
 [89.40913 ]], R is [[88.51238251]
 [88.6272583 ]
 [88.74098969]
 [88.85358429]
 [88.96504974]].
[2019-04-04 05:08:30,723] A3C_AGENT_WORKER-Thread-5 INFO:Evaluating...
[2019-04-04 05:08:30,723] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 05:08:30,724] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:08:30,739] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 05:08:30,741] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:08:30,747] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run51
[2019-04-04 05:08:30,805] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 05:08:30,807] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:08:30,824] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run51
[2019-04-04 05:08:30,910] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run51
[2019-04-04 05:10:22,483] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.3466943], dtype=float32), 0.26655903]
[2019-04-04 05:10:22,483] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-7.3, 82.0, 0.0, 0.0, 26.0, 24.60097386680285, 0.2308735674121337, 0.0, 1.0, 42321.20865092586]
[2019-04-04 05:10:22,484] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 05:10:22,485] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.8102015e-28 5.3373791e-25 7.1244262e-27 3.6849012e-25 5.5821425e-25
 4.9830491e-28 1.0000000e+00], sampled 0.5392467409880117
[2019-04-04 05:11:36,432] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 05:11:55,885] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 05:11:58,767] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 05:11:59,790] A3C_AGENT_WORKER-Thread-5 INFO:Global step: 5000000, evaluation results [5000000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
